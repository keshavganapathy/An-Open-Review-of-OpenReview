,Unnamed: 0,Unnamed: 0.1,Unnamed: 0.1.1,Unnamed: 0.1.1.1,Unnamed: 0.1.1.1.1,Unnamed: 0.1.1.1.1.1,Unnamed: 0.1.1.1.1.1.1,conference,year,paper,authors,emails,ratings,confidences,decisions,cmt_before_review,cmt_between,cmt_after_decision,double_blinded,submission_date,institution,csranking,ranking,categories
0,0,0,0,0,0,0,0,ICLR,2017,Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy,Dougal J. Sutherland;Hsiao-Yu Tung;Heiko Strathmann;Soumyajit De;Aaditya Ramdas;Alex Smola;Arthur Gretton,dougal@gmail.com;htung@cs.cmu.edu;heiko.strathmann@gmail.com;soumyajitde.cse@gmail.com;aramdas@berkeley.edu;alex@smola.org;arthur.gretton@gmail.com,6;8;7,3;3;4,Accept (Poster),2,3,1.0,no,11/4/16,University College London;Carnegie Mellon University;University College London;;University of California Berkeley;Carnegie-Mellon University;University College London,45;1;45;-1;5;1;45,15;23;15;-1;10;23;15,5;4
1,1,1,1,1,1,1,1,ICLR,2017,A Simple but Tough-to-Beat Baseline for Sentence Embeddings,Sanjeev Arora;Yingyu Liang;Tengyu Ma,arora@cs.princeton.edu;yingyul@cs.princeton.edu;tengyu@cs.princeton.edu,7;7;8,4;4;3,Accept (Poster),8,4,0.0,no,11/4/16,Princeton University;Princeton University;Princeton University,32;32;32,7;7;7,3;5;6
2,2,2,2,2,2,2,2,ICLR,2017,A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks,Dan Hendrycks;Kevin Gimpel,dan@ttic.edu;kgimpel@ttic.edu,6;6;6,3;3;3,Accept (Poster),2,4,0.0,no,11/4/16,Toyota Technological Institute at Chicago;Toyota Technological Institute at Chicago,-1;-1,-1;-1,3;2
3,3,3,3,3,3,3,3,ICLR,2017,MS MARCO: A Human-Generated MAchine Reading COmprehension Dataset,Tri Nguyen;Mir Rosenberg;Xia Song;Jianfeng Gao;Saurabh Tiwary;Rangan Majumder;Li Deng,trnguye@microsoft.com;miriamr@microsoft.com;xiaso@microsoft.com;jfgao@microsoft.com;satiwary@microsoft.com;ranganm@microsoft.com;deng@microsoft.com,6;6;6,3;3,Reject,3,2,0.0,no,11/4/16,Microsoft;Microsoft;Microsoft;Microsoft;Microsoft;Microsoft;Microsoft,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,
4,4,4,4,4,4,4,4,ICLR,2017,Dynamic Steerable Frame Networks,Jörn-Henrik Jacobsen;Bert De Brabandere;Arnold W.M. Smeulders,j.jacobsen@uva.nl;bert.debrabandere@esat.kuleuven.be;a.w.m.smeulders@uva.nl,5;4;7,4;3;3,Reject,1,4,0.0,no,11/4/16,University of Amsterdam;KU Leuven;University of Amsterdam,161;105;161,63;40;63,
5,5,5,5,5,5,5,5,ICLR,2017,FILTER SHAPING FOR CONVOLUTIONAL NEURAL NETWORKS,Xingyi Li;Fuxin Li;Xiaoli Fern;Raviv Raich,lixin@eecs.oregonstate.edu;lif@eecs.oregonstate.edu;xfern@eecs.oregonstate.edu;raich@eecs.oregonstate.edu,6;7;7,4;3;4,Accept (Poster),2,5,0.0,no,11/5/16,Oregon State University;Oregon State University;Oregon State University;Oregon State University,75;75;75;75,316;316;316;316,
6,6,6,6,6,6,6,6,ICLR,2017, A Theoretical Framework for Robustness of (Deep) Classifiers against Adversarial Samples,Beilun Wang;Ji Gao;Yanjun Qi,bw4mw@virginia.edu;jg6yd@virginia.edu;yanjun@virginia.edu,5;3;5,2;4;4,Invite to Workshop Track,8,21,0.0,no,11/4/16,University of Virginia;University of Virginia;University of Virginia,67;67;67,123;123;123,4;1
7,7,7,7,7,7,7,7,ICLR,2017,Efficient Vector Representation for Documents through Corruption,Minmin Chen,m.chen@criteo.com,6;7;7,4;3;4,Accept (Poster),2,8,3.0,no,11/5/16,Criteo,-1,-1,3
8,8,8,8,8,8,8,8,ICLR,2017,Adversarial Training Methods for Semi-Supervised Text Classification,Takeru Miyato;Andrew M. Dai;Ian Goodfellow,takeru.miyato@gmail.com;adai@google.com;ian@openai.com,6;7;7,4;3;5,Accept (Poster),1,4,0.0,no,11/3/16,"Preferred Networks, Inc.;Google;OpenAI",-1;-1;-1,-1;-1;-1,3;4
9,9,9,9,9,9,9,9,ICLR,2017,Neural Architecture Search with Reinforcement Learning,Barret Zoph;Quoc Le,barretzoph@google.com;qvl@google.com,9;9;9,4;4;5,Accept (Oral),8,8,4.0,no,11/4/16,Google;Google,-1;-1,-1;-1,3
10,10,10,10,10,10,10,10,ICLR,2017,Decomposing Motion and Content for Natural Video Sequence Prediction,Ruben Villegas;Jimei Yang;Seunghoon Hong;Xunyu Lin;Honglak Lee,rubville@umich.edu;jimyang@adobe.com;maga33@postech.ac.kr;timelin@buaa.edu.cn;honglak@umich.edu,7;7;6,5;4;4,Accept (Poster),5,4,4.0,no,11/4/16,University of Michigan;Adobe Systems;POSTECH;Beihang University;University of Michigan,8;-1;113;129;8,21;-1;104;981;21,
11,11,11,11,11,11,11,11,ICLR,2017,Maximum Entropy Flow Networks,Gabriel Loaiza-Ganem *;Yuanjun Gao *;John P. Cunningham,gl2480@columbia.edu;yg2312@columbia.edu;jpc2181@columbia.edu,6;6;9,4;4;5,Accept (Poster),2,4,1.0,no,11/4/16,Columbia University;Columbia University;Columbia University,14;14;14,16;16;16,2
12,12,12,12,12,12,12,12,ICLR,2017,beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework,Irina Higgins;Loic Matthey;Arka Pal;Christopher Burgess;Xavier Glorot;Matthew Botvinick;Shakir Mohamed;Alexander Lerchner,irinah@google.com;lmatthey@google.com;arkap@google.com;cpburgess@google.com;glorotx@google.com;botvinick@google.com;shakir@google.com;lerchner@google.com,5;7;6,4;4;4,Accept (Poster),2,4,0.0,no,11/4/16,Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1,5
13,13,13,13,13,13,13,13,ICLR,2017,Soft Weight-Sharing for Neural Network Compression,Karen Ullrich;Edward Meeds;Max Welling,karen.ullrich@uva.nl;tmeeds@gmail.com;welling.max@gmail.com,7;7;7,3;4;3,Accept (Poster),3,2,1.0,no,11/4/16,University of Amsterdam;VU University Toronto;University of California - Irvine,161;15;36,63;22;99,
14,14,14,14,14,14,14,14,ICLR,2017,Support Regularized Sparse Coding and Its Fast Encoder,Yingzhen Yang;Jiahui Yu;Pushmeet Kohli;Jianchao Yang;Thomas S. Huang,superyyzg@gmail.com;jyu79@illinois.edu;pkohli@microsoft.com;jianchao.yang@snapchat.com;t-huang1@illinois.edu,6;7;7,3;4;4,Accept (Poster),1,4,0.0,no,11/4/16,"University of Illinois, Urbana-Champaign;University of Illinois, Urbana Champaign;Microsoft;Snap Inc.;University of Illinois, Urbana Champaign",4;4;-1;-1;4,36;36;-1;-1;36,
15,15,15,15,15,15,15,15,ICLR,2017,Stochastic Neural Networks for Hierarchical Reinforcement Learning,Carlos Florensa;Yan Duan;Pieter Abbeel,florensa@berkeley.edu;rocky@openai.com;pieter@openai.com,7;7;8,4;4;4,Accept (Poster),0,8,1.0,no,11/5/16,University of California Berkeley;OpenAI;OpenAI,5;-1;-1,10;-1;-1,
16,16,16,16,16,16,16,16,ICLR,2017,Combining policy gradient and Q-learning,Brendan O'Donoghue;Remi Munos;Koray Kavukcuoglu;Volodymyr Mnih,bodonoghue@google.com;munos@google.com;korayk@google.com;vmnih@google.com,9;7;7,5;3;4,Accept (Poster),15,4,0.0,no,11/4/16,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,
17,17,17,17,17,17,17,17,ICLR,2017,Recurrent Environment Simulators,Silvia Chiappa;Sébastien Racaniere;Daan Wierstra;Shakir Mohamed,csilvia@google.com;sracaniere@google.com;wierstra@google.com;shakir@google.com,7;5;8,5;4;4,Accept (Poster),3,5,0.0,no,11/4/16,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,
18,18,18,18,18,18,18,18,ICLR,2017,Multi-Agent Cooperation and the Emergence of (Natural) Language,Angeliki Lazaridou;Alexander Peysakhovich;Marco Baroni,angeliki.lazaridou@unitn.it;alexpeys@fb.com;marco.baroni@unitn.it,7;7;7,3;3;3,Accept (Oral),0,3,7.0,no,11/4/16,University of Trento;Facebook;University of Trento,15;-1;15,236;-1;236,3
19,19,19,19,19,19,19,19,ICLR,2017,Stick-Breaking Variational Autoencoders,Eric Nalisnick;Padhraic Smyth,enalisni@uci.edu;smyth@ics.uci.edu,4;8;8,4;4;5,Accept (Poster),6,7,0.0,no,11/4/16,"University of California, Irvine;University of California, Irvine",36;36,99;99,5;11
20,20,20,20,20,20,20,20,ICLR,2017,Sigma Delta Quantized Networks,Peter O'Connor;Max Welling,peter.ed.oconnor@gmail.com;max.welling@uva.nl,8;6;8,4;3;4,Accept (Poster),3,2,0.0,no,11/4/16,University of Amsterdam;University of Amsterdam,161;161,63;63,
21,21,21,21,21,21,21,21,ICLR,2017,Semi-supervised deep learning by metric embedding,Elad Hoffer;Nir Ailon,ehoffer@tx.technion.ac.il;nailon@cs.technion.ac.il,4;6,4;4,Invite to Workshop Track,1,3,0.0,no,11/4/16,Technion;Technion,24;24,301;301,
22,22,22,22,22,22,22,22,ICLR,2017,Adversarial Feature Learning,Jeff Donahue;Philipp Krähenbühl;Trevor Darrell,jdonahue@cs.berkeley.edu;philkr@utexas.edu;trevor@eecs.berkeley.edu,7;7;7,3;3;4,Accept (Poster),2,1,1.0,no,11/4/16,"University of California Berkeley;University of Texas, Austin;University of California Berkeley",5;20;5,10;50;10,5;4
23,23,23,23,23,23,23,23,ICLR,2017,Learning Visual Servoing with Deep Features and Fitted Q-Iteration,Alex X. Lee;Sergey Levine;Pieter Abbeel,alexlee_gk@cs.berkeley.edu;svlevine@cs.berkeley.edu;pabbeel@cs.berkeley.edu,8;7;7,3;4;3,Accept (Poster),3,0,0.0,no,11/5/16,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,10;10;10,8
24,24,24,24,24,24,24,24,ICLR,2017,Learning End-to-End Goal-Oriented Dialog,Antoine Bordes;Y-Lan Boureau;Jason Weston,abordes@fb.com;ylan@fb.com;jase@fb.com,7;8;8,4;5;4,Accept (Oral),3,5,0.0,no,11/4/16,Facebook;Facebook;Facebook,-1;-1;-1,-1;-1;-1,
25,25,25,25,25,25,25,25,ICLR,2017,Learning Graphical State Transitions,Daniel D. Johnson,ddjohnson@hmc.edu,9;7;9,3;2;3,Accept (Oral),6,6,0.0,no,10/29/16,Harvey Mudd College,462,981,10
26,26,26,26,26,26,26,26,ICLR,2017,Revisiting Classifier Two-Sample Tests,David Lopez-Paz;Maxime Oquab,dlp@fb.com;qas@fb.com,7;8;7,3;5;4,Accept (Poster),6,6,0.0,no,11/4/16,Facebook;Facebook,-1;-1,-1;-1,5;4
27,27,27,27,27,27,27,27,ICLR,2017,Mollifying Networks,Caglar Gulcehre;Marcin Moczulski;Francesco Visin;Yoshua Bengio,gulcehrc@iro.umontreal.ca;marcin-m@post.pl;fvisin@gmail.com;yoshua.umontreal@gmail.com,7;6;6,5;4;4,Accept (Poster),3,3,0.0,no,11/4/16,University of Montreal;University of Oxford;Politecnico di Milano;University of Montreal,113;50;140;113,103;1;981;103,4;9
28,28,28,28,28,28,28,28,ICLR,2017,Introspection:Accelerating Neural Network Training By Learning Weight Evolution,Abhishek Sinha;Aahitagni Mukherjee;Mausoom Sarkar;Balaji Krishnamurthy,abhishek.sinha94@gmail.com;ahitagnimukherjeeam@gmail.com;msarkar@adobe.com;kbalaji@adobe.com,8;7;9,5;4;5,Accept (Poster),3,10,0.0,no,11/4/16,Adobe Systems;IIT Kanpur;Adobe Systems;Adobe Systems,-1;140;-1;-1,-1;498;-1;-1,
29,29,29,29,29,29,29,29,ICLR,2017,Neural Program Lattices,Chengtao Li;Daniel Tarlow;Alexander L. Gaunt;Marc Brockschmidt;Nate Kushman,ctli@mit.edu;dtarlow@microsoft.com;algaunt@microsoft.com;mabrocks@microsoft.com;nkushman@microsoft.com,4;7;7,4;4;5,Accept (Poster),3,5,0.0,no,11/4/16,Massachusetts Institute of Technology;Microsoft;Microsoft;Microsoft;Microsoft,2;-1;-1;-1;-1,5;-1;-1;-1;-1,
30,30,30,30,30,30,30,30,ICLR,2017,Highway and Residual Networks learn Unrolled Iterative Estimation,Klaus Greff;Rupesh K. Srivastava;Jürgen Schmidhuber,klaus@idsia.ch;rupesh@idsia.ch;juergen@idsia.ch,7;8;6,4;4;5,Accept (Poster),0,3,0.0,no,11/5/16,IDSIA;IDSIA;IDSIA,-1;-1;-1,-1;-1;-1,
31,31,31,31,31,31,31,31,ICLR,2017,Hadamard Product for Low-rank Bilinear Pooling,Jin-Hwa Kim;Kyoung-Woon On;Woosang Lim;Jeonghee Kim;Jung-Woo Ha;Byoung-Tak Zhang,jnhwkim@snu.ac.kr;kwon@bi.snu.ac.kr;quasar17@kaist.ac.kr;jeonghee.kim@navercorp.com;jungwoo.ha@navercorp.com;btzhang@bi.snu.ac.kr,7;7;6,3;5;3,Accept (Poster),2,13,0.0,no,10/17/16,Seoul National University;Seoul National University;Korea Advanced Institute of Science and Technology;NAVER;NAVER;Seoul National University,50;50;462;-1;-1;50,72;72;88;-1;-1;72,2
32,32,32,32,32,32,32,32,ICLR,2017,Machine Comprehension Using Match-LSTM and Answer Pointer,Shuohang Wang;Jing Jiang,shwang.2014@phdis.smu.edu.sg;jingjiang@smu.edu.sg,7;6;6,3;4;3,Accept (Poster),4,5,0.0,no,11/4/16,Singapore Management University;Singapore Management University,87;87,981;981,3
33,33,33,33,33,33,33,33,ICLR,2017,Unsupervised Cross-Domain Image Generation,Yaniv Taigman;Adam Polyak;Lior Wolf,yaniv@fb.com;adampolyak@fb.com;wolf@fb.com,7;6;7,3;3;4,Accept (Poster),18,8,1.0,no,11/4/16,Facebook;Facebook;Facebook,-1;-1;-1,-1;-1;-1,5
34,34,34,34,34,34,34,34,ICLR,2017,Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling,Hakan Inan;Khashayar Khosravi;Richard Socher,inanh@stanford.edu;khosravi@stanford.edu;rsocher@salesforce.com,6;7;8,4;4;4,Accept (Poster),6,4,0.0,no,11/4/16,Stanford University;Stanford University;SalesForce.com,3;3;-1,3;3;-1,3
35,35,35,35,35,35,35,35,ICLR,2017,Making Neural Programming Architectures Generalize via Recursion,Jonathon Cai;Richard Shin;Dawn Song,jonathon@cs.berkeley.edu;ricshin@cs.berkeley.edu;dawnsong@cs.berkeley.edu,8;8;9,4;3;5,Accept (Oral),1,5,1.0,no,11/5/16,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,10;10;10,1
36,36,36,36,36,36,36,36,ICLR,2017,Generative Multi-Adversarial Networks,Ishan Durugkar;Ian Gemp;Sridhar Mahadevan,idurugkar@cs.umass.edu;imgemp@cs.umass.edu;mahadeva@cs.umass.edu,7;6;7,4;4;3,Accept (Poster),7,6,0.0,no,11/4/16,"University of Massachusetts, Amherst;University of Massachusetts, Amherst;University of Massachusetts, Amherst",25;25;25,166;166;166,5;4
37,37,37,37,37,37,37,37,ICLR,2017,Variational Recurrent Adversarial Deep Domain Adaptation,Sanjay Purushotham;Wilka Carvalho;Tanachat Nilanon;Yan Liu,spurusho@usc.edu;wcarvalh@usc.edu;nilanon@usc.edu;yanliu.cs@usc.edu,6;5;6,4;4;4,Accept (Poster),3,4,0.0,no,11/4/16,University of Southern California;University of Southern California;University of Southern California;University of Southern California,31;31;31;31,60;60;60;60,4
38,38,38,38,38,38,38,38,ICLR,2017,Data Noising as Smoothing in Neural Network Language Models,Ziang Xie;Sida I. Wang;Jiwei Li;Daniel Lévy;Aiming Nie;Dan Jurafsky;Andrew Y. Ng,zxie@cs.stanford.edu;sidaw@cs.stanford.edu;jiweil@stanford.edu;danilevy@cs.stanford.edu;anie@cs.stanford.edu;jurafsky@stanford.edu;ang@cs.stanford.edu,6;6;8,4;4,Accept (Poster),2,3,0.0,no,11/4/16,Stanford University;Stanford University;Stanford University;Stanford University;Stanford University;Stanford University;Stanford University,3;3;3;3;3;3;3,3;3;3;3;3;3;3,3
39,39,39,39,39,39,39,39,ICLR,2017,Deep Probabilistic Programming,Dustin Tran;Matthew D. Hoffman;Rif A. Saurous;Eugene Brevdo;Kevin Murphy;David M. Blei,dustin@cs.columbia.edu;mathoffm@adobe.com;rif@google.com;ebrevdo@google.com;kpmurphy@google.com;david.blei@columbia.edu,5;8;7,4;4;4,Accept (Poster),2,4,0.0,no,11/4/16,Columbia University;Adobe Systems;Google;Google;Google;Columbia University,14;-1;-1;-1;-1;14,16;-1;-1;-1;-1;16,5;4
40,40,40,40,40,40,40,40,ICLR,2017,Hierarchical Multiscale Recurrent Neural Networks,Junyoung Chung;Sungjin Ahn;Yoshua Bengio,junyoung.chung@umontreal.ca;sungjin.ahn@umontreal.ca;yoshua.bengio@umontreal.ca,8;7;8,4;3;4,Accept (Poster),2,3,0.0,no,10/29/16,University of Montreal;University of Montreal;University of Montreal,113;113;113,103;103;103,3
41,41,41,41,41,41,41,41,ICLR,2017,Programming With a Differentiable Forth Interpreter,Matko Bošnjak;Tim Rocktäschel;Jason Naradowsky;Sebastian Riedel,m.bosnjak@cs.ucl.ac.uk;t.rocktaschel@cs.ucl.ac.uk;j.narad@cs.ucl.ac.uk;s.riedel@cs.ucl.ac.uk,7;5;6,2;2;4,Invite to Workshop Track,1,4,1.0,no,11/4/16,University College London;University College London;University College London;University College London,45;45;45;45,15;15;15;15,10
42,42,42,42,42,42,42,42,ICLR,2017,Learning to Play in a Day: Faster Deep Reinforcement Learning by Optimality Tightening,Frank S.He;Yang Liu;Alexander G. Schwing;Jian Peng,frankheshibi@gmail.com;liu301@illinois.edu;aschwing@illinois.edu;jianpeng@illinois.edu,9;4;9,3;4;4,Accept (Poster),10,9,0.0,no,11/4/16,"Zhejiang University;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",129;4;4;4,981;36;36;36,
43,43,43,43,43,43,43,43,ICLR,2017,Energy-based Generative Adversarial Networks,Junbo Zhao;Michael Mathieu;Yann LeCun,jakezhao@cs.nyu.edu;mathieu@cs.nyu.edu;yann@cs.nyu.edu,8;7;7,3;5;3,Accept (Poster),4,5,0.0,no,11/4/16,New York University;New York University;New York University,25;25;25,32;32;32,5;4
44,44,44,44,44,44,44,44,ICLR,2017,Normalizing the Normalizers: Comparing and Extending Network Normalization Schemes,Mengye Ren;Renjie Liao;Raquel Urtasun;Fabian H. Sinz;Richard S. Zemel,mren@cs.toronto.edu;rjliao@cs.toronto.edu;urtasun@cs.toronto.edu;fabian.sinz@epagoge.de;zemel@cs.toronto.edu,7;5;9,4;4;5,Accept (Poster),0,7,1.0,no,11/5/16,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Baylor College of Medicine;Department of Computer Science, University of Toronto",15;15;15;-1;15,22;22;22;-1;22,3
45,45,45,45,45,45,45,45,ICLR,2017,Learning through Dialogue Interactions by Asking Questions,Jiwei Li;Alexander H. Miller;Sumit Chopra;Marc'Aurelio Ranzato;Jason Weston,jiwel@fb.com;ahm@fb.com;spchopra@fb.com;ranzato@fb.com;jase@fb.com,7;8;7,3;5;3,Accept (Poster),4,9,0.0,no,11/4/16,Facebook;Facebook;Facebook;Facebook;Facebook,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,
46,46,46,46,46,46,46,46,ICLR,2017,Third Person Imitation Learning,Bradly C Stadie;Pieter Abbeel;Ilya Sutskever,bstadie@openai.com;pieter@openai.com;ilyasu@openai.com,6;5;6,4;3;4,Accept (Poster),2,4,0.0,no,11/4/16,OpenAI;OpenAI;OpenAI,-1;-1;-1,-1;-1;-1,
47,47,47,47,47,47,47,47,ICLR,2017,The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables,Chris J. Maddison;Andriy Mnih;Yee Whye Teh,cmaddis@stats.ox.ac.uk;amnih@google.com;y.w.teh@stats.ox.ac.uk,7;9;8,3;5;4,Accept (Poster),4,1,0.0,no,11/4/16,University of Oxford;Google;University of Oxford,50;-1;50,1;-1;1,10
48,48,48,48,48,48,48,48,ICLR,2017,Pruning Convolutional Neural Networks for Resource Efficient Inference,Pavlo Molchanov;Stephen Tyree;Tero Karras;Timo Aila;Jan Kautz,pmolchanov@nvidia.com;styree@nvidia.com;tkarras@nvidia.com;taila@nvidia.com;jkautz@nvidia.com,7;6;9,4;4;4,Accept (Poster),5,6,0.0,no,11/4/16,NVIDIA;NVIDIA;NVIDIA;NVIDIA;NVIDIA,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,6;8
49,49,49,49,49,49,49,49,ICLR,2017,Adaptive Feature Abstraction for Translating Video to Language,Yunchen Pu;Martin Renqiang Min;Zhe Gan;Lawrence Carin,yunchen.pu@duke.edu;renqiang@nec-labs.com;zhe.gan@duke.edu;lcarin@duke.edu,4;4;7,5;4;4,Invite to Workshop Track,6,7,0.0,no,11/4/16,Duke University;NEC-Labs;Duke University;Duke University,42;-1;42;42,18;-1;18;18,
50,50,50,50,50,50,50,50,ICLR,2017,Learning Recurrent Representations for Hierarchical Behavior Modeling,Eyrun Eyjolfsdottir;Kristin Branson;Yisong Yue;Pietro Perona,eeyjolfs@caltech.edu;bransonk@janelia.hhmi.org;yyue@caltech.edu;perona@caltech.edu,7;6;7,3;4;4,Accept (Poster),3,7,0.0,no,11/3/16,California Institute of Technology;HHMI Janelia Research Campus;California Institute of Technology;California Institute of Technology,129;-1;129;129,2;-1;2;2,5;7
51,51,51,51,51,51,51,51,ICLR,2017,Generalizing Skills with Semi-Supervised Reinforcement Learning,Chelsea Finn;Tianhe Yu;Justin Fu;Pieter Abbeel;Sergey Levine,cbfinn@eecs.berkeley.edu;tianhe.yu@berkeley.edu;justinfu@eecs.berkeley.edu;pabbeel@eecs.berkeley.edu;svlevine@eecs.berkeley.edu,6;7;8,5;3;4,Accept (Poster),2,0,0.0,no,11/4/16,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5;5,10;10;10;10;10,8
52,52,52,52,52,52,52,52,ICLR,2017,Entropy-SGD: Biasing Gradient Descent Into Wide Valleys,Pratik Chaudhari;Anna Choromanska;Stefano Soatto;Yann LeCun;Carlo Baldassi;Christian Borgs;Jennifer Chayes;Levent Sagun;Riccardo Zecchina,pratikac@ucla.edu;achoroma@cims.nyu.edu;soatto@cs.ucla.edu;yann@cs.nyu.edu;carlo.baldassi@polito.it;borgs@microsoft.com;jchayes@microsoft.com;sagun@cims.nyu.edu;riccardo.zecchina@polito.it,7;8;9,4;4;3,Accept (Poster),4,13,0.0,no,11/4/16,"University of California, Los Angeles;New York University;University of California, Los Angeles;New York University;Politecnico di Torino;Microsoft;Microsoft;New York University;Politecnico di Torino",20;25;20;25;462;-1;-1;25;462,14;32;14;32;384;-1;-1;32;384,8
53,53,53,53,53,53,53,53,ICLR,2017,Tree-structured decoding with doubly-recurrent neural networks,David Alvarez-Melis;Tommi S. Jaakkola,dalvmel@mit.edu;tommi@csail.mit.edu,7;6;6,4;4;4,Accept (Poster),3,10,0.0,no,11/4/16,Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2,5;5,
54,54,54,54,54,54,54,54,ICLR,2017,A Compositional Object-Based Approach to Learning Physical Dynamics,Michael Chang;Tomer Ullman;Antonio Torralba;Joshua Tenenbaum,mbchang@mit.edu;tomeru@mit.edu;torralba@mit.edu;jbt@mit.edu,9;6;7,4;4;4,Accept (Poster),6,6,0.0,no,11/4/16,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2,5;5;5;5,
55,55,55,55,55,55,55,55,ICLR,2017,"Attend, Adapt and Transfer: Attentive Deep Architecture for Adaptive Transfer from multiple sources in the same domain",Janarthanan Rajendran;Aravind Lakshminarayanan;Mitesh M. Khapra;Prasanna P;Balaraman Ravindran,rjana@umich.edu;aravindsrinivas@gmail.com;miteshk@cse.iitm.ac.in;prasanna.p@cs.mcgill.ca;ravi@cse.iitm.ac.in,7;7;7,4;3;4,Accept (Poster),1,5,0.0,no,11/4/16,University of Michigan;University of Montreal;Indian Institute of Technology Madras;McGill University;Indian Institute of Technology Madras,8;113;140;80;140,21;103;472;42;472,
56,56,56,56,56,56,56,56,ICLR,2017,Multilayer Recurrent Network Models of Primate Retinal Ganglion Cell Responses,Eleanor Batty;Josh Merel;Nora Brackbill;Alexander Heitman;Alexander Sher;Alan Litke;E.J. Chichilnisky;Liam Paninski,erb2180@columbia.edu;jsmerel@gmail.com;nbrack@stanford.edu;alexkenheitmen@gmail.com;sashake3@uscs.edu;Alan.Litke@cern.ch;ej@stanford.edu;liam@stat.columbia.edu,8;7;4,5;4;4,Accept (Poster),2,5,0.0,no,11/5/16,Columbia University;;Stanford University;;;CERN;Stanford University;Columbia University,14;-1;3;-1;-1;-1;3;14,16;-1;3;-1;-1;-1;3;16,5
57,57,57,57,57,57,57,57,ICLR,2017,LR-GAN: Layered Recursive Generative Adversarial Networks for Image Generation,Jianwei Yang;Anitha Kannan;Dhruv Batra;Devi Parikh,jw2yang@vt.edu;akannan@fb.com;dbatra@gatech.edu;parikh@gatech.edu,6;6;7,3;4,Accept (Poster),5,5,0.0,no,11/4/16,Virginia Tech;Facebook;Georgia Institute of Technology;Georgia Institute of Technology,80;-1;12;12,286;-1;33;33,5;4
58,58,58,58,58,58,58,58,ICLR,2017,Learning to Optimize,Ke Li;Jitendra Malik,ke.li@eecs.berkeley.edu;malik@eecs.berkeley.edu,7;6;7,4;4;4,Accept (Poster),5,5,1.0,no,11/4/16,University of California Berkeley;University of California Berkeley,5;5,10;10,
59,59,59,59,59,59,59,59,ICLR,2017,Variational Lossy Autoencoder,Xi Chen;Diederik P. Kingma;Tim Salimans;Yan Duan;Prafulla Dhariwal;John Schulman;Ilya Sutskever;Pieter Abbeel,peter@openai.com;dpkingma@openai.com;tim@openai.com;rocky@openai.com;prafulla@mit.edu;joschu@openai.com;ilyasu@openai.com;pieter@openai.com,7;7;6,4;4;4,Accept (Poster),8,8,0.0,no,11/4/16,OpenAI;OpenAI;OpenAI;OpenAI;Massachusetts Institute of Technology;OpenAI;OpenAI;OpenAI,-1;-1;-1;-1;2;-1;-1;-1,-1;-1;-1;-1;5;-1;-1;-1,5
60,60,60,60,60,60,60,60,ICLR,2017,Improving Generative Adversarial Networks with Denoising Feature Matching,David Warde-Farley;Yoshua Bengio,d.warde.farley@gmail.com;yoshua.umontreal@gmail.com,7;6;7,4;2;5,Accept (Poster),5,4,0.0,no,11/5/16,Google;University of Montreal,-1;113,-1;103,5;4
61,61,61,61,61,61,61,61,ICLR,2017,Emergence of foveal image sampling from learning to attend in visual scenes,Brian Cheung;Eric Weiss;Bruno Olshausen,bcheung@berkeley.edu;eaweiss@berkeley.edu;baolshausen@berkeley.edu,6;5;6,4;4;5,Accept (Poster),3,3,0.0,no,11/5/16,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,10;10;10,
62,62,62,62,62,62,62,62,ICLR,2017,Words or Characters? Fine-grained Gating for Reading Comprehension,Zhilin Yang;Bhuwan Dhingra;Ye Yuan;Junjie Hu;William W. Cohen;Ruslan Salakhutdinov,zhiliny@cs.cmu.edu;bdhingra@andrew.cmu.edu;yey1@andrew.cmu.edu;junjieh@cmu.edu;wcohen@cs.cmu.edu;rsalakhu@cs.cmu.edu,7;6;7,4;4;3,Accept (Poster),2,0,2.0,no,11/4/16,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1;1;1,23;23;23;23;23;23,
63,63,63,63,63,63,63,63,ICLR,2017,Transfer Learning for Sequence Tagging with Hierarchical Recurrent Networks,Zhilin Yang;Ruslan Salakhutdinov;William W. Cohen,zhiliny@cs.cmu.edu;rsalakhu@cs.cmu.edu;wcohen@cs.cmu.edu,7;5;8,4;4;4,Accept (Poster),2,5,0.0,no,11/4/16,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1,23;23;23,6
64,64,64,64,64,64,64,64,ICLR,2017,Bidirectional Attention Flow for Machine Comprehension,Minjoon Seo;Aniruddha Kembhavi;Ali Farhadi;Hannaneh Hajishirzi,minjoon@cs.washington.edu;anik@allenai.org;alif@allenai.org;hannaneh@cs.washington.edu,7;8;8,5;4;5,Accept (Poster),10,7,2.0,no,11/4/16,University of Washington;Allen Institute for Artificial Intelligence;Allen Institute for Artificial Intelligence;University of Washington,6;-1;-1;6,25;-1;-1;25,
65,65,65,65,65,65,65,65,ICLR,2017,Query-Reduction Networks for Question Answering,Minjoon Seo;Sewon Min;Ali Farhadi;Hannaneh Hajishirzi,minjoon@cs.washington.edu;shmsw25@snu.ac.kr;ali@cs.washington.edu;hannaneh@cs.washington.edu,7;7;7,3;4;4,Accept (Poster),7,2,1.0,no,11/4/16,University of Washington;Seoul National University;University of Washington;University of Washington,6;50;6;6,25;72;25;25,
66,66,66,66,66,66,66,66,ICLR,2017,Training Compressed Fully-Connected Networks with a Density-Diversity Penalty,Shengjie Wang;Haoran Cai;Jeff Bilmes;William Noble,wangsj@cs.washington.edu;haoran@uw.edu;bilmes@uw.edu;william-noble@u.washington.edu,9;6;6,4;4;2,Accept (Poster),5,10,0.0,no,11/4/16,"University of Washington;University of Washington, Seattle;University of Washington, Seattle;University of Washington",6;6;6;6,25;25;25;25,
67,67,67,67,67,67,67,67,ICLR,2017,Do Deep Convolutional Nets Really Need to be Deep and Convolutional?,Gregor Urban;Krzysztof J. Geras;Samira Ebrahimi Kahou;Ozlem Aslan;Shengjie Wang;Abdelrahman Mohamed;Matthai Philipose;Matt Richardson;Rich Caruana,gurban@uci.edu;k.j.geras@sms.ed.ac.uk;samira.ebrahimi-kahou@polymtl.ca;ozlem@cs.ualberta.ca;wangsj@cs.washington.edu;asamir@microsoft.com;matthaip@microsoft.com;mattri@microsoft.com;rcaruana@microsoft.com,7;7,3;4,Accept (Poster),2,4,0.0,no,11/4/16,"University of California, Irvine;University of Edinburgh;Polytechnique Montreal;University of Alberta;University of Washington;Microsoft;Microsoft;Microsoft;Microsoft",36;33;351;94;6;-1;-1;-1;-1,99;27;981;106;25;-1;-1;-1;-1,
68,68,68,68,68,68,68,68,ICLR,2017,Recurrent Hidden Semi-Markov Model,Hanjun Dai;Bo Dai;Yan-Ming Zhang;Shuang Li;Le Song,hanjundai@gatech.edu;bodai@gatech.edu;ymzhang@nlpr.ia.ac.cn;sli370@gatech.edu;lsong@cc.gatech.edu,7;7;7,4;3;4,Accept (Poster),2,3,0.0,no,11/4/16,"Georgia Institute of Technology;Georgia Institute of Technology;Institute of automation, Chinese academy of science, Chinese Academy of Sciences;Georgia Institute of Technology;Georgia Institute of Technology",12;12;67;12;12,33;33;981;33;33,5;2
69,69,69,69,69,69,69,69,ICLR,2017,Lie-Access Neural Turing Machines,Greg Yang;Alexander Rush,gyang@college.harvard.edu;srush@seas.harvard.edu,6;7;6;8,4;4;3;4,Accept (Poster),4,6,0.0,no,11/4/16,Harvard University;Harvard University,36;36,6;6,8
70,70,70,70,70,70,70,70,ICLR,2017,What does it take to generate natural textures?,Ivan Ustyuzhaninov *;Wieland Brendel *;Leon Gatys;Matthias Bethge,ivan.ustyuzhaninov@bethgelab.org;wieland.brendel@bethgelab.org;leon.gatys@bethgelab.org;matthias.bethge@bethgelab.org,7;8;8,4;3;5,Accept (Poster),2,3,0.0,no,11/5/16,"Centre for Integrative Neuroscience, AG Bethge;Centre for Integrative Neuroscience, AG Bethge;Centre for Integrative Neuroscience, AG Bethge;Centre for Integrative Neuroscience, AG Bethge",-1;-1;-1;-1,-1;-1;-1;-1,
71,71,71,71,71,71,71,71,ICLR,2017,Tracking the World State with Recurrent Entity Networks,Mikael Henaff;Jason Weston;Arthur Szlam;Antoine Bordes;Yann LeCun,mbh305@nyu.edu;jase@fb.com;azslam@fb.com;abordes@fb.com;yann@fb.com,7;7;7,3;5;4,Accept (Poster),6,4,0.0,no,11/4/16,New York University;Facebook;Facebook;Facebook;Facebook,25;-1;-1;-1;-1,32;-1;-1;-1;-1,
72,72,72,72,72,72,72,72,ICLR,2017,Training Agent for First-Person Shooter Game with Actor-Critic Curriculum Learning,Yuxin Wu;Yuandong Tian,ppwwyyxx@gmail.com;yuandong@fb.com,6;4;7,4;5;4,Accept (Poster),1,4,3.0,no,11/4/16,Carnegie Mellon University;Facebook,1;-1,23;-1,
73,73,73,73,73,73,73,73,ICLR,2017,Deep Biaffine Attention for Neural Dependency Parsing,Timothy Dozat;Christopher D. Manning,tdozat@stanford.edu;manning@stanford.edu,5;5;6,4;4;4,Accept (Poster),9,1,2.0,no,11/4/16,Stanford University;Stanford University,3;3,3;3,10
74,74,74,74,74,74,74,74,ICLR,2017,Learning Curve Prediction with Bayesian Neural Networks,Aaron Klein;Stefan Falkner;Jost Tobias Springenberg;Frank Hutter,kleinaa@cs.uni-freiburg.de;sfalkner@cs.uni-freiburg.de;springj@cs.uni-freiburg.de;fh@cs.uni-freiburg.de,7;7;7,4;4;5,Accept (Poster),3,3,0.0,no,11/4/16,Universität Freiburg;Universität Freiburg;Universität Freiburg;Universität Freiburg,113;113;113;113,95;95;95;95,11
75,75,75,75,75,75,75,75,ICLR,2017,Deep Information Propagation,Samuel S. Schoenholz;Justin Gilmer;Surya Ganguli;Jascha Sohl-Dickstein,schsam@google.com;gilmer@google.com;sganguli@stanford.edu;jaschasd@google.com,8;9;8,2;4;3,Accept (Poster),0,3,0.0,no,11/4/16,Google;Google;Stanford University;Google,-1;-1;3;-1,-1;-1;3;-1,
76,76,76,76,76,76,76,76,ICLR,2017,Pruning Filters for Efficient ConvNets,Hao Li;Asim Kadav;Igor Durdanovic;Hanan Samet;Hans Peter Graf,haoli@cs.umd.edu;asim@nec-labs.com;igord@nec-labs.com;hjs@cs.umd.edu;hpg@nec-labs.com,6;7;7;7,5;4;5;4,Accept (Poster),0,6,0.0,no,11/5/16,"University of Maryland, College Park;NEC-Labs;NEC-Labs;University of Maryland, College Park;NEC-Labs",12;-1;-1;12;-1,67;-1;-1;67;-1,
77,77,77,77,77,77,77,77,ICLR,2017,SGDR: Stochastic Gradient Descent with Warm Restarts,Ilya Loshchilov;Frank Hutter,ilya@cs.uni-freiburg.de;fh@cs.uni-freiburg.de,7;7;7,3;4;5,Accept (Poster),2,2,0.0,no,11/4/16,Universität Freiburg;Universität Freiburg,113;113,95;95,9
78,78,78,78,78,78,78,78,ICLR,2017,Program Synthesis for Character Level Language Modeling,Pavol Bielik;Veselin Raychev;Martin Vechev,pavol.bielik@inf.ethz.ch;veselin.raychev@inf.ethz.ch;martin.vechev@inf.ethz.ch,5;8;8,3;2;4,Accept (Poster),2,4,0.0,no,11/4/16,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,9;9;9,9;9;9,3
79,79,79,79,79,79,79,79,ICLR,2017,Learning Invariant Feature Spaces to Transfer Skills with Reinforcement Learning,Abhishek Gupta;Coline Devin;YuXuan Liu;Pieter Abbeel;Sergey Levine,abhigupta@berkeley.edu;coline@berkeley.edu;yuxuanliu@berkeley.edu;pabbeel@cs.berkeley.edu;svlevine@eecs.berkeley.edu,7;6;6,3;5;4,Accept (Poster),3,6,0.0,no,11/5/16,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5;5,10;10;10;10;10,6
80,80,80,80,80,80,80,80,ICLR,2017,On the Quantitative Analysis of Decoder-Based Generative Models,Yuhuai Wu;Yuri Burda;Ruslan Salakhutdinov;Roger Grosse,ywu@cs.toronto.edu;yburda@openai.com;rsalakhu@cs.cmu.edu;rgrosse@cs.toronto.edu,7;7;6,4;4;5,Accept (Poster),3,9,0.0,no,11/4/16,"Department of Computer Science, University of Toronto;OpenAI;Carnegie Mellon University;Department of Computer Science, University of Toronto",15;-1;1;15,22;-1;23;22,5;4
81,81,81,81,81,81,81,81,ICLR,2017,Tighter bounds lead to improved classifiers,Nicolas Le Roux,nicolas@le-roux.name,6;8;4,4;5;4,Accept (Poster),0,3,1.0,no,11/2/16,Criteo,-1,-1,1
82,82,82,82,82,82,82,82,ICLR,2017,Topology and Geometry of Half-Rectified Network Optimization,C. Daniel Freeman;Joan Bruna,daniel.freeman@berkeley.edu;bruna@cims.nyu.edu,2;7;8,5;3;3,Accept (Poster),5,5,0.0,no,11/4/16,University of California Berkeley;New York University,5;25,10;32,1
83,83,83,83,83,83,83,83,ICLR,2017,Quasi-Recurrent Neural Networks,James Bradbury;Stephen Merity;Caiming Xiong;Richard Socher,james.bradbury@salesforce.com;smerity@salesforce.com;cxiong@salesforce.com;rsocher@salesforce.com,6;7;7,4;4;4,Accept (Poster),2,5,0.0,no,11/4/16,SalesForce.com;SalesForce.com;SalesForce.com;SalesForce.com,-1;-1;-1;-1,-1;-1;-1;-1,3
84,84,84,84,84,84,84,84,ICLR,2017,Pointer Sentinel Mixture Models,Stephen Merity;Caiming Xiong;James Bradbury;Richard Socher,smerity@salesforce.com;cxiong@salesforce.com;james.bradbury@salesforce.com;rsocher@salesforce.com,7;8;8,4;4;4,Accept (Poster),2,2,0.0,no,11/3/16,SalesForce.com;SalesForce.com;SalesForce.com;SalesForce.com,-1;-1;-1;-1,-1;-1;-1;-1,3
85,85,85,85,85,85,85,85,ICLR,2017,Learning to Generate Samples from Noise through Infusion Training,Florian Bordes;Sina Honari;Pascal Vincent,florian.bordes@umontreal.ca;sina.honari@umontreal.ca;pascal.vincent@umontreal.ca,8;7;6,5;5;4,Accept (Poster),2,6,0.0,no,11/5/16,University of Montreal;University of Montreal;University of Montreal,113;113;113,103;103;103,5;4
86,86,86,86,86,86,86,86,ICLR,2017,EPOpt: Learning Robust Neural Network Policies Using Model Ensembles,Aravind Rajeswaran;Sarvjeet Ghotra;Balaraman Ravindran;Sergey Levine,aravraj@cs.washington.edu;sarvjeet.13it236@nitk.edu.in;ravi@cse.iitm.ac.in;svlevine@eecs.berkeley.edu,8;7;7,4;4,Accept (Poster),2,1,0.0,no,11/4/16,University of Washington;National Institute of Technology Karnataka;Indian Institute of Technology Madras;University of California Berkeley,6;462;140;5,25;981;472;10,4;11
87,87,87,87,87,87,87,87,ICLR,2017,Steerable CNNs,Taco S. Cohen;Max Welling,taco.cohen@gmail.com;m.welling@uva.nl,6;7;8,3;4;3,Accept (Poster),6,4,0.0,no,11/4/16,University of Amsterdam;University of Amsterdam,161;161,63;63,
88,88,88,88,88,88,88,88,ICLR,2017,Learning to Perform Physics Experiments via Deep Reinforcement Learning,Misha Denil;Pulkit Agrawal;Tejas D Kulkarni;Tom Erez;Peter Battaglia;Nando de Freitas,mdenil@google.com;pulkitag@berkeley.edu;tkulkarni@google.com;etom@google.com;peterbattaglia@google.com;nandodefreitas@google.com,7;7;6;7,4;3;3;3,Accept (Poster),3,6,0.0,no,11/4/16,Google;University of California Berkeley;Google;Google;Google;Google,-1;5;-1;-1;-1;-1,-1;10;-1;-1;-1;-1,3
89,89,89,89,89,89,89,89,ICLR,2017,The Neural Noisy Channel,Lei Yu;Phil Blunsom;Chris Dyer;Edward Grefenstette;Tomas Kocisky,lei.yu@cs.ox.ac.uk;pblunsom@google.com;cdyer@google.com;etg@google.com;tkocisky@google.com,7;7;6,4;4;4,Accept (Poster),3,4,0.0,no,11/4/16,University of Oxford;Google;Google;Google;Google,50;-1;-1;-1;-1,1;-1;-1;-1;-1,3
90,90,90,90,90,90,90,90,ICLR,2017,Learning to Compose Words into Sentences with Reinforcement Learning,Dani Yogatama;Phil Blunsom;Chris Dyer;Edward Grefenstette;Wang Ling,dyogatama@google.com;pblunsom@google.com;cdyer@google.com;etg@google.com;lingwang@google.com,6;7;8,3;5;4,Accept (Poster),15,1,0.0,no,11/4/16,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,3
91,91,91,91,91,91,91,91,ICLR,2017,Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data,Nicolas Papernot;Martín Abadi;Úlfar Erlingsson;Ian Goodfellow;Kunal Talwar,ngp5056@cse.psu.edu;abadi@google.com;ulfar@google.com;ian@openai.com;kunal@google.com,9;7;9,4;3;4,Accept (Oral),6,9,0.0,no,11/2/16,Pennsylvania State University;Google;Google;OpenAI;Google,39;-1;-1;-1;-1,68;-1;-1;-1;-1,5
92,92,92,92,92,92,92,92,ICLR,2017,Diet Networks: Thin Parameters for Fat Genomics,Adriana Romero;Pierre Luc Carrier;Akram Erraqabi;Tristan Sylvain;Alex Auvolat;Etienne Dejoie;Marc-André Legault;Marie-Pierre Dubé;Julie G. Hussin;Yoshua Bengio,adriana.romero.soriano@umontreal.ca;pierre-luc.carrier@umontreal.ca;akram.er-raqabi@umontreal.ca;Tristan.sylvain@umontreal.ca;alexis211@gmail.com;etiennedejoie@gmail.com;marc-andre.legault.1@umontreal.ca;julieh@well.ox.ac.uk;yoshua.umontreal@gmail.com,6;7;8,4;3;3,Accept (Poster),2,12,0.0,no,11/4/16,University of Montreal;University of Montreal;University of Montreal;University of Montreal;;;University of Montreal;University of Oxford;University of Montreal,113;113;113;113;-1;-1;113;50;113,103;103;103;103;-1;-1;103;1;103,
93,93,93,93,93,93,93,93,ICLR,2017,Autoencoding Variational Inference For Topic Models,Akash Srivastava;Charles Sutton,akash.srivastava@ed.ac.uk;csutton@inf.ed.ac.uk,6;7;6,4;3;5,Accept (Poster),9,8,0.0,no,11/4/16,University of Edinburgh;University of Edinburgh,33;33,27;27,
94,94,94,94,94,94,94,94,ICLR,2017,Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data,Maximilian Karl;Maximilian Soelch;Justin Bayer;Patrick van der Smagt,karlma@in.tum.de;m.soelch@tum.de;bayer.justin@googlemail.com;smagt@brml.org,6;7;6,4;3;3,Accept (Poster),2,5,0.0,no,11/4/16,"Technical University Munich;Technical University Munich;Data Lab, Volkswagen Group;TU Munich",54;54;-1;54,46;46;-1;30,
95,95,95,95,95,95,95,95,ICLR,2017,Why Deep Neural Networks for Function Approximation?,Shiyu Liang;R. Srikant,sliang26@illinois.edu;rsrikant@illinois.edu,7;7;7,4;4;3,Accept (Poster),2,3,0.0,no,11/1/16,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",4;4,36;36,
96,96,96,96,96,96,96,96,ICLR,2017,Reinforcement Learning with Unsupervised Auxiliary Tasks,Max Jaderberg;Volodymyr Mnih;Wojciech Marian Czarnecki;Tom Schaul;Joel Z Leibo;David Silver;Koray Kavukcuoglu,jaderberg@google.com;vmnih@google.com;lejlot@google.com;schaul@google.com;jzl@google.com;davidsilver@google.com;korayk@google.com,7;8;8,4;4;4,Accept (Oral),12,4,0.0,no,11/4/16,Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,
97,97,97,97,97,97,97,97,ICLR,2017,Improving Policy Gradient by Exploring Under-appreciated Rewards,Ofir Nachum;Mohammad Norouzi;Dale Schuurmans,ofirnachum@google.com;mnorouzi@google.com;schuurmans@google.com,7;8;7,4;4;4,Accept (Poster),5,4,3.0,no,11/4/16,Google;Google;Google,-1;-1;-1,-1;-1;-1,
98,98,98,98,98,98,98,98,ICLR,2017,Mode Regularized Generative Adversarial Networks,Tong Che;Yanran Li;Athul Jacob;Yoshua Bengio;Wenjie Li,tong.che@umontreal.ca;csyli@comp.polyu.edu.hk;ap.jacob@umontreal.ca;yoshua.bengio@umontreal.ca;cswjli@comp.polyu.edu.hk,4;7;7,4;4;4,Accept (Poster),9,11,0.0,no,11/5/16,University of Montreal;The Hong Kong Polytechnic University;University of Montreal;University of Montreal;The Hong Kong Polytechnic University,113;194;113;113;194,103;192;103;103;192,5;4
99,99,99,99,99,99,99,99,ICLR,2017,An Actor-Critic Algorithm for Sequence Prediction,Dzmitry Bahdanau;Philemon Brakel;Kelvin Xu;Anirudh Goyal;Ryan Lowe;Joelle Pineau;Aaron Courville;Yoshua Bengio,dimabgv@gmail.com;pbpop3@gmail.com;iamkelvinxu@gmail.com;anirudhgoyal9119@gmail.com;lowe.ryan.t@gmail.com;jpineau@cs.mcgill.ca;aaron.courville@gmail.com;yoshua.bengio@gmail.com,4;8;8,4;5;4,Accept (Poster),5,9,0.0,no,11/2/16,University of Montreal;University of Montreal;Google;University of Montreal;McGill University;McGill University;University of Montreal;,113;113;-1;113;80;80;113;-1,103;103;-1;103;42;42;103;-1,3
100,100,100,100,100,100,100,100,ICLR,2017,Trusting SVM for Piecewise Linear CNNs,Leonard Berrada;Andrew Zisserman;M. Pawan Kumar,lberrada@robots.ox.ac.uk;az@robots.ox.ac.uk;pawan@robots.ox.ac.uk,5;4;6,4;4;4,Accept (Poster),1,4,0.0,no,11/4/16,University of Oxford;University of Oxford;University of Oxford,50;50;50,1;1;1,
101,101,101,101,101,101,101,101,ICLR,2017,End-to-end Optimized Image Compression,Johannes Ballé;Valero Laparra;Eero P. Simoncelli,johannes.balle@nyu.edu;valero.laparra@uv.es;eero.simoncelli@nyu.edu,8;9;8;8,4;4;4;3,Accept (Oral),3,3,0.0,no,11/5/16,New York University;Universitat de València;New York University,25;-1;25,32;-1;32,5
102,102,102,102,102,102,102,102,ICLR,2017,"Snapshot Ensembles: Train 1, Get M for Free",Gao Huang;Yixuan Li;Geoff Pleiss;Zhuang Liu;John E. Hopcroft;Kilian Q. Weinberger,gh349@cornell.edu;yl2363@cornell.edu;geoff@cs.cornell.edu;liuzhuangthu@gmail.com;jeh@cs.cornell.edu;kqw4@cornell.edu,9;7;8,4;5;3,Accept (Poster),8,17,0.0,no,11/4/16,Cornell University;Cornell University;Cornell University;Tsinghua University;Cornell University;Cornell University,7;7;7;11;7;7,19;19;19;35;19;19,
103,103,103,103,103,103,103,103,ICLR,2017,Learning Features of Music From Scratch,John Thickstun;Zaid Harchaoui;Sham Kakade,thickstn@cs.washington.edu;sham@cs.washington.edu;zaid@uw.edu,8;6;6,4;4;4,Accept (Poster),3,3,0.0,no,11/4/16,"University of Washington;University of Washington;University of Washington, Seattle",6;6;6,25;25;25,
104,104,104,104,104,104,104,104,ICLR,2017,DeepCoder: Learning to Write Programs,Matej Balog;Alexander L. Gaunt;Marc Brockschmidt;Sebastian Nowozin;Daniel Tarlow,matej.balog@gmail.com;t-algaun@microsoft.com;mabrocks@microsoft.com;Sebastian.Nowozin@microsoft.com;dtarlow@microsoft.com,6;6;7,4;4;2,Accept (Poster),7,7,0.0,no,11/4/16,University of Cambridge;Microsoft;Microsoft;Microsoft;Microsoft,67;-1;-1;-1;-1,4;-1;-1;-1;-1,4
105,105,105,105,105,105,105,105,ICLR,2017,A Compare-Aggregate Model for Matching Text Sequences,Shuohang Wang;Jing Jiang,shwang.2014@phdis.smu.edu.sg;jingjiang@smu.edu.sg,6;7;8,4;5;5,Accept (Poster),4,4,0.0,no,11/4/16,Singapore Management University;Singapore Management University,87;87,981;981,3
106,106,106,106,106,106,106,106,ICLR,2017,Automatic Rule Extraction from Long Short Term Memory Networks,W. James Murdoch;Arthur Szlam,jmurdoch@berkeley.edu;aszlam@fb.com,7;7;7,4;3;3,Accept (Poster),1,5,2.0,no,11/4/16,University of California Berkeley;Facebook,5;-1,10;-1,3
107,107,107,107,107,107,107,107,ICLR,2017,Capacity and Trainability in Recurrent Neural Networks,Jasmine Collins;Jascha Sohl-Dickstein;David Sussillo,jlcollins@google.com;jaschasd@google.com;sussillo@google.com,8;7;8,4;4;4,Accept (Poster),1,5,0.0,no,11/4/16,Google;Google;Google,-1;-1;-1,-1;-1;-1,1
108,108,108,108,108,108,108,108,ICLR,2017,Learning to Repeat: Fine Grained Action Repetition for Deep Reinforcement Learning,Sahil Sharma;Aravind S. Lakshminarayanan;Balaraman Ravindran,ssahil08@gmail.com;aravindsrinivas@gmail.com;ravi@cse.iitm.ac.in,8;7;8,5;3;4,Accept (Poster),4,6,1.0,no,11/4/16,Indian Institute of Technology Madras;University of Montreal;Indian Institute of Technology Madras,140;113;140,472;103;472,
109,109,109,109,109,109,109,109,ICLR,2017,Multi-view Recurrent Neural Acoustic Word Embeddings,Wanjia He;Weiran Wang;Karen Livescu,wanjia@ttic.edu;weiranwang@ttic.edu;klivescu@ttic.edu,5;6;6,4;4;3,Accept (Poster),2,1,0.0,no,11/4/16,Toyota Technological Institute at Chicago;Toyota Technological Institute at Chicago;Toyota Technological Institute at Chicago,-1;-1;-1,-1;-1;-1,3
110,110,110,110,110,110,110,110,ICLR,2017,Neuro-Symbolic Program Synthesis,Emilio Parisotto;Abdel-rahman Mohamed;Rishabh Singh;Lihong Li;Dengyong Zhou;Pushmeet Kohli,eparisot@andrew.cmu.edu;asamir@microsoft.com;risin@microsoft.com;lihongli@microsoft.com;denzho@microsoft.com;pkohli@microsoft.com,5;7;8,4;3;4,Accept (Poster),2,4,0.0,no,11/4/16,Carnegie Mellon University;Microsoft;Microsoft;Microsoft;Microsoft;Microsoft,1;-1;-1;-1;-1;-1,23;-1;-1;-1;-1;-1,
111,111,111,111,111,111,111,111,ICLR,2017,Learning to Remember Rare Events,Lukasz Kaiser;Ofir Nachum;Aurko Roy;Samy Bengio,lukaszkaiser@google.com;ofirnachum@google.com;aurko@gatech.edu;bengio@google.com,6;8;7,5;3;4,Accept (Poster),4,3,0.0,no,11/4/16,Google;Google;Georgia Institute of Technology;Google,-1;-1;12;-1,-1;-1;33;-1,3
112,112,112,112,112,112,112,112,ICLR,2017,Discrete Variational Autoencoders,Jason Tyler Rolfe,jrolfe@dwavesys.com,8;9;8,2;4;4,Accept (Poster),9,8,0.0,no,11/4/16,D-Wave Systems,-1,-1,5
113,113,113,113,113,113,113,113,ICLR,2017,Understanding Trainable Sparse Coding with Matrix Factorization,Thomas Moreau;Joan Bruna,thomas.moreau@cmla.ens-cachan.fr;joan.bruna@berkeley.edu,6;8;5,3;4;2,Accept (Poster),1,3,0.0,no,11/2/16,ENS Paris-Saclay;University of California Berkeley,462;5,475;10,1
114,114,114,114,114,114,114,114,ICLR,2017,"Learning to Query, Reason, and Answer Questions On Ambiguous Texts",Xiaoxiao Guo;Tim Klinger;Clemens Rosenbaum;Joseph P. Bigus;Murray Campbell;Ban Kawas;Kartik Talamadupula;Gerry Tesauro;Satinder   Singh,tklinger@us.ibm.com;guoxiao@umich.edu;cgbr@cs.umass.edu;jbigus@us.ibm.com;mcam@us.ibm.com;bkawas@us.ibm.com;krtalamad@us.ibm.com;gtesauro@us.ibm.com;baveja@umich.edu,6;7;7,3;4;3,Accept (Poster),5,2,0.0,no,11/4/16,"International Business Machines;University of Michigan;University of Massachusetts, Amherst;International Business Machines;International Business Machines;International Business Machines;International Business Machines;International Business Machines;University of Michigan",-1;8;25;-1;-1;-1;-1;-1;8,-1;21;166;-1;-1;-1;-1;-1;21,
115,115,115,115,115,115,115,115,ICLR,2017,Variable Computation in Recurrent Neural Networks,Yacine Jernite;Edouard Grave;Armand Joulin;Tomas Mikolov,yacine.jernite@nyu.edu;egrave@fb.com;ajoulin@fb.com;tmikolov@fb.com,7;7;4,4;4;5,Accept (Poster),2,6,0.0,no,11/4/16,New York University;Facebook;Facebook;Facebook,25;-1;-1;-1,32;-1;-1;-1,
116,116,116,116,116,116,116,116,ICLR,2017,Learning and Policy Search in Stochastic Dynamical Systems with Bayesian Neural Networks,Stefan Depeweg;José Miguel Hernández-Lobato;Finale Doshi-Velez;Steffen Udluft,stefan.depeweg@siemens.com;jmh233@cam.ac.uk;finale@seas.harvard.edu;steffen.udluft@siemens.com,7;6;7,3;3;3,Accept (Poster),2,4,0.0,no,11/4/16,Siemens Corporate Research;University of Cambridge;Harvard University;Siemens Corporate Research,-1;67;36;-1,-1;4;6;-1,11
117,117,117,117,117,117,117,117,ICLR,2017,Reinforcement Learning through Asynchronous Advantage Actor-Critic on a GPU,Mohammad Babaeizadeh;Iuri Frosio;Stephen Tyree;Jason Clemons;Jan Kautz,mb2@uiuc.edu;ifrosio@nvidia.com;styree@nvidia.com;jclemons@nvidia.com;jkautz@nvidia.com,5;7;8,5;5;3,Accept (Poster),7,2,0.0,no,11/4/16,"University of Illinois, Urbana-Champaign;NVIDIA;NVIDIA;NVIDIA;NVIDIA",4;-1;-1;-1;-1,36;-1;-1;-1;-1,
118,118,118,118,118,118,118,118,ICLR,2017,Reasoning with Memory Augmented Neural Networks for Language Comprehension,Tsendsuren Munkhdalai;Hong Yu,tsendsuren.munkhdalai@umassmed.edu;hong.yu@umassmed.edu,7;6;7,2;3;4,Accept (Poster),1,6,0.0,no,11/3/16,UMass;UMass,25;25,166;166,
119,119,119,119,119,119,119,119,ICLR,2017,Learning a Natural Language Interface with Neural Programmer,Arvind Neelakantan;Quoc V. Le;Martin Abadi;Andrew McCallum;Dario Amodei,arvind@cs.umass.edu;qvl@google.com;abadi@google.com;mccallum@cs.umass.edu;damodei@openai.com,7;6;6,3;3;4,Accept (Poster),3,6,1.0,no,11/4/16,"University of Massachusetts, Amherst;Google;Google;University of Massachusetts, Amherst;OpenAI",25;-1;-1;25;-1,166;-1;-1;166;-1,3
120,120,120,120,120,120,120,120,ICLR,2017,Loss-aware Binarization of Deep Networks,Lu Hou;Quanming Yao;James T. Kwok,lhouab@cse.ust.hk;qyaoaa@cse.ust.hk;jamesk@cse.ust.hk,7;7;7,3;4;3,Accept (Poster),1,14,0.0,no,11/4/16,The Hong Kong University of Science and Technology;The Hong Kong University of Science and Technology;The Hong Kong University of Science and Technology,39;39;39,48;48;48,
121,121,121,121,121,121,121,121,ICLR,2017,Hyperband: Bandit-Based Configuration Evaluation for Hyperparameter Optimization,Lisha Li;Kevin Jamieson;Giulia DeSalvo;Afshin Rostamizadeh;Ameet Talwalkar,lishal@cs.ucla.edu;kjamieson@berkeley.edu;desalvo@cims.nyu.edu;rostami@google.com;ameet@cs.ucla.edu,8;7;7,4;5;4,Accept (Poster),3,4,1.0,no,11/4/16,"University of California, Los Angeles;University of California Berkeley;New York University;Google;University of California, Los Angeles",20;5;25;-1;20,14;10;32;-1;14,11
122,122,122,122,122,122,122,122,ICLR,2017,Nonparametric Neural Networks,George Philipp;Jaime G. Carbonell,george.philipp@email.de;jgc@cs.cmu.edu,7;7;5,3;4;4,Accept (Poster),2,9,0.0,no,11/5/16,Carnegie Mellon University;Carnegie Mellon University,1;1,23;23,1
123,123,123,123,123,123,123,123,ICLR,2017,Improving Neural Language Models with a Continuous Cache,Edouard Grave;Armand Joulin;Nicolas Usunier,egrave@fb.com;ajoulin@fb.com;usunier@fb.com,7;5;9,5;4;5,Accept (Poster),6,1,0.0,no,11/4/16,Facebook;Facebook;Facebook,-1;-1;-1,-1;-1;-1,3
124,124,124,124,124,124,124,124,ICLR,2017,Episodic Exploration for Deep Deterministic Policies for StarCraft Micromanagement,Nicolas Usunier;Gabriel Synnaeve;Zeming Lin;Soumith Chintala,usunier@fb.com;gab@fb.com;zlin@fb.com;soumith@fb.com,7;7;8,4;4;4,Accept (Poster),2,0,0.0,no,11/4/16,Facebook;Facebook;Facebook;Facebook,-1;-1;-1;-1,-1;-1;-1;-1,
125,125,125,125,125,125,125,125,ICLR,2017,Lossy Image Compression with Compressive Autoencoders,Lucas Theis;Wenzhe Shi;Andrew Cunningham;Ferenc Huszár,ltheis@twitter.com;wshi@twitter.com;acunningham@twitter.com;fhuszar@twitter.com,8;7;5,5;3;4,Accept (Poster),7,12,2.0,no,11/4/16,Twitter;Twitter;Twitter;Twitter,-1;-1;-1;-1,-1;-1;-1;-1,
126,126,126,126,126,126,126,126,ICLR,2017,Temporal Ensembling for Semi-Supervised Learning,Samuli Laine;Timo Aila,slaine@nvidia.com;taila@nvidia.com,7;9;8,4;4;5,Accept (Poster),3,3,1.0,no,11/4/16,NVIDIA;NVIDIA,-1;-1,-1;-1,
127,127,127,127,127,127,127,127,ICLR,2017,Optimization as a Model for Few-Shot Learning,Sachin Ravi;Hugo Larochelle,sachinr@twitter.com;hugo@twitter.com;sachinr@princeton.edu,8;9;6,4;5;4,Accept (Oral),4,5,0.0,no,11/4/16,Twitter;Twitter;Princeton University,-1;-1;32,-1;-1;7,6
128,128,128,128,128,128,128,128,ICLR,2017,Towards a Neural Statistician,Harrison Edwards;Amos Storkey,h.l.edwards@sms.ed.ac.uk;amos.storkey@ed.ac.uk,8;8;6,2;4;4,Accept (Poster),2,3,0.0,no,11/4/16,University of Edinburgh;University of Edinburgh,33;33,27;27,5
129,129,129,129,129,129,129,129,ICLR,2017,Faster CNNs with Direct Sparse Convolutions and Guided Pruning,Jongsoo Park;Sheng Li;Wei Wen;Ping Tak Peter Tang;Hai Li;Yiran Chen;Pradeep Dubey,jongsoo.park@intel.com;sheng.r.li@intel.com;peter.tang@intel.com;weiwen.web@gmail.com;HAL66@pitt.edu;yic52@pitt.edu;pradeep.dubey@intel.com,6;7;6,3;3;3,Accept (Poster),0,3,0.0,no,11/4/16,Intel;Intel;Intel;University of Pittsburgh;University of Pittsburgh;University of Pittsburgh;Intel,-1;-1;-1;78;78;78;-1,-1;-1;-1;80;80;80;-1,
130,130,130,130,130,130,130,130,ICLR,2017,Boosted Generative Models,Aditya Grover;Stefano Ermon,adityag@cs.stanford.edu;ermon@cs.stanford.edu,5;6;5,3;3;3,Reject,1,4,0.0,no,11/5/16,Stanford University;Stanford University,3;3,3;3,5
131,131,131,131,131,131,131,131,ICLR,2017,Deep Predictive Coding Networks for Video Prediction and Unsupervised Learning,William Lotter;Gabriel Kreiman;David Cox,lotter@fas.harvard.edu;gabriel.kreiman@tch.harvard.edu;davidcox@fas.harvard.edu,6;8;8,3;4;5,Accept (Poster),6,0,0.0,no,11/4/16,Harvard University;Harvard University;Harvard University,36;36;36,6;6;6,
132,132,132,132,132,132,132,132,ICLR,2017, Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer,Noam Shazeer;*Azalia Mirhoseini;*Krzysztof Maziarz;Andy Davis;Quoc Le;Geoffrey Hinton;Jeff Dean,noam@google.com;azalia@google.com;krzysztof.maziarz@student.uj.edu.pl;andydavis@google.com;qvl@google.com;geoffhinton@google.com;jeff@google.com,7;6;7,4;4;4,Accept (Poster),4,10,0.0,no,11/4/16,Google;Google;Jagiellonian University;Google;Google;Google;Google,-1;-1;462;-1;-1;-1;-1,-1;-1;625;-1;-1;-1;-1,3
133,133,133,133,133,133,133,133,ICLR,2017,Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic,Shixiang Gu;Timothy Lillicrap;Zoubin Ghahramani;Richard E. Turner;Sergey Levine,sg717@cam.ac.uk;countzero@google.com;zoubin@eng.cam.ac.uk;ret26@cam.ac.uk;svlevine@eecs.berkeley.edu,7;7;7;8,4;5;4;3,Accept (Oral),1,11,9.0,no,11/4/16,University of Cambridge;Google;University of Cambridge;University of Cambridge;University of California Berkeley,67;-1;67;67;5,4;-1;4;4;10,
134,134,134,134,134,134,134,134,ICLR,2017,Recurrent Batch Normalization,Tim Cooijmans;Nicolas Ballas;César Laurent;Çağlar Gülçehre;Aaron Courville,tim.cooijmans@umontreal.ca;nicolas.ballas@umontreal.ca;cesar.laurent@umontreal.ca;caglar.gulcehre@umontreal.ca;aaron.courville@umontreal.ca,7;8;7,4;4;4,Accept (Poster),3,4,0.0,no,11/4/16,University of Montreal;University of Montreal;University of Montreal;University of Montreal;University of Montreal,113;113;113;113;113,103;103;103;103;103,3;8
135,135,135,135,135,135,135,135,ICLR,2017,Inductive Bias of Deep Convolutional Networks through Pooling Geometry,Nadav Cohen;Amnon Shashua,cohennadav@cs.huji.ac.il;shashua@cs.huji.ac.il,7;7;6,5;3;3,Accept (Poster),1,3,0.0,no,11/3/16,Hebrew University of Jerusalem;Hebrew University of Jerusalem,57;57,186;186,2
136,136,136,136,136,136,136,136,ICLR,2017,Incremental Network Quantization: Towards Lossless CNNs with Low-precision Weights,Aojun Zhou;Anbang Yao;Yiwen Guo;Lin Xu;Yurong Chen,aojun.zhou@intel.com;anbang.yao@intel.com;yiwen.guo@intel.com;lin.x.xu@intel.com;yurong.chen@intel.com,7;8;7,3;4;4,Accept (Poster),2,2,0.0,no,11/4/16,Intel;Intel;Intel;Intel;Intel,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,
137,137,137,137,137,137,137,137,ICLR,2017,,,,4;7;7,4;3;3,Reject,0,0,1.0,no,11/4/16,,,,
138,138,138,138,138,138,138,138,ICLR,2017,DSD: Dense-Sparse-Dense Training for Deep Neural Networks,Song Han;Jeff Pool;Sharan Narang;Huizi Mao;Enhao Gong;Shijian Tang;Erich Elsen;Peter Vajda;Manohar Paluri;John Tran;Bryan Catanzaro;William J. Dally,songhan@stanford.edu;jpool@nvidia.com;sharan@baidu.com;huizi@stanford.edu;enhaog@stanford.edu;sjtang@stanford.edu;eriche@google.com;vajdap@fb.com;mano@fb.com;johntran@nvidia.com;bcatanzaro@nvidia.com;dally@stanford.edu,8;8;5,3;3;4,Accept (Poster),3,3,0.0,no,11/4/16,Stanford University;NVIDIA;Baidu;Stanford University;Stanford University;Stanford University;Google;Facebook;Facebook;NVIDIA;NVIDIA;Stanford University,3;-1;-1;3;3;3;-1;-1;-1;-1;-1;3,3;-1;-1;3;3;3;-1;-1;-1;-1;-1;3,
139,139,139,139,139,139,139,139,ICLR,2017,TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency,Adji B. Dieng;Chong Wang;Jianfeng Gao;John Paisley,abd2141@columbia.edu;chowang@microsoft.com;jfgao@microsoft.com;jpaisley@columbia.edu,6;7;8,3;4;4,Accept (Poster),2,7,2.0,no,11/4/16,Columbia University;Microsoft;Microsoft;Columbia University,14;-1;-1;14,16;-1;-1;16,3
140,140,140,140,140,140,140,140,ICLR,2017,Efficient Representation of Low-Dimensional Manifolds using Deep Networks,Ronen Basri;David W. Jacobs,ronen.basri@weizmann.ac.il;djacobs@cs.umd.edu,6;5;7,3;3;5,Accept (Poster),1,3,0.0,no,11/3/16,"Weizmann Institute;University of Maryland, College Park",105;12,981;67,
141,141,141,141,141,141,141,141,ICLR,2017,Training deep neural-networks using a noise adaptation layer,Jacob Goldberger;Ehud Ben-Reuven,jacob.goldberger@biu.ac.il;udi.benreuven@gmail.com,5;7;5,4;5;5,Accept (Poster),2,3,0.0,no,11/4/16,Bar Ilan University;,87;-1,489;-1,
142,142,142,142,142,142,142,142,ICLR,2017,SampleRNN: An Unconditional End-to-End Neural Audio Generation Model,Soroush Mehri;Kundan Kumar;Ishaan Gulrajani;Rithesh Kumar;Shubham Jain;Jose Sotelo;Aaron Courville;Yoshua Bengio,soroush.mehri@umontreal.ca;kundankumar2510@gmail.com;igul222@gmail.com;ritheshkumar.95@gmail.com;shubhamjain1310@gmail.com;rdz.sotelo@gmail.com;aaron.courville@umontreal.ca;yoshua.bengio@umontreal.ca,9;8;8,4;4;3,Accept (Poster),4,5,0.0,no,11/4/16,University of Montreal;University of Montreal;University of Montreal;Sri Sivasubramaniya Nadar College Of Engineering;;;University of Montreal;University of Montreal,113;113;113;-1;-1;-1;113;113,103;103;103;-1;-1;-1;103;103,
143,143,143,143,143,143,143,143,ICLR,2017,FractalNet: Ultra-Deep Neural Networks without Residuals,Gustav Larsson;Michael Maire;Gregory Shakhnarovich,larsson@cs.uchicago.edu;mmaire@ttic.edu;greg@ttic.edu,6;6;5,5;4;5,Accept (Poster),2,2,0.0,no,11/4/16,University of Chicago;Toyota Technological Institute at Chicago;Toyota Technological Institute at Chicago,45;-1;-1,10;-1;-1,
144,144,144,144,144,144,144,144,ICLR,2017,Towards the Limit of Network Quantization,Yoojin Choi;Mostafa El-Khamy;Jungwon Lee,yoojin.c@samsung.com;mostafa.e@samsung.com;jungwon2.lee@samsung.com,7;7;7,3;3;4,Accept (Poster),3,5,0.0,no,11/4/16,Samsung;Samsung;Samsung,-1;-1;-1,-1;-1;-1,
145,145,145,145,145,145,145,145,ICLR,2017,Understanding deep learning requires rethinking generalization,Chiyuan Zhang;Samy Bengio;Moritz Hardt;Benjamin Recht;Oriol Vinyals,chiyuan@mit.edu;bengio@google.com;mrtz@google.com;brecht@berkeley.edu;vinyals@google.com,10;9;10,4;3;4,Accept (Oral),2,33,9.0,no,11/4/16,Massachusetts Institute of Technology;Google;Google;University of California Berkeley;Google,2;-1;-1;5;-1,5;-1;-1;10;-1,8
146,146,146,146,146,146,146,146,ICLR,2017,Optimal Binary Autoencoding with Pairwise Correlations,Akshay Balsubramani,abalsubr@stanford.edu,7;7;6,3;2;4,Accept (Poster),0,2,0.0,no,11/4/16,Stanford University,3,3,9
147,147,147,147,147,147,147,147,ICLR,2017,Metacontrol for Adaptive Imagination-Based Optimization,Jessica B. Hamrick;Andrew J. Ballard;Razvan Pascanu;Oriol Vinyals;Nicolas Heess;Peter W. Battaglia,jhamrick@berkeley.edu;aybd@google.com;razp@google.com;vinyals@google.com;heess@google.com;peterbattaglia@google.com,8;8;7;8,3;3;3;3,Accept (Poster),0,4,0.0,no,11/4/16,University of California Berkeley;Google;Google;Google;Google;Google,5;-1;-1;-1;-1;-1,10;-1;-1;-1;-1;-1,
148,148,148,148,148,148,148,148,ICLR,2017,Sparsely-Connected Neural Networks: Towards Efficient VLSI Implementation of Deep Neural Networks,Arash Ardakani;Carlo Condo;Warren J. Gross,arash.ardakani@mail.mcgill.ca;carlo.condo@mail.mcgill.ca;warren.gross@mcgill.ca,6;7;6,4;3;3,Accept (Poster),2,6,0.0,no,11/3/16,McGill University;McGill University;McGill University,80;80;80,42;42;42,
149,149,149,149,149,149,149,149,ICLR,2017,Calibrating Energy-based Generative Adversarial Networks,Zihang Dai;Amjad Almahairi;Philip Bachman;Eduard Hovy;Aaron Courville,zander.dai@gmail.com;amjadmahayri@gmail.com;phil.bachman@gmail.com;hovy@cmu.edu;aaron.courville@gmail.com,8;7;8,4;5;4,Accept (Poster),0,7,0.0,no,11/4/16,Carnegie Mellon University;University of Montreal;Maluuba;Carnegie Mellon University;University of Montreal,1;113;-1;1;113,23;103;-1;23;103,5;4;1
150,150,150,150,150,150,150,150,ICLR,2017,Designing Neural Network Architectures using Reinforcement Learning,Bowen Baker;Otkrist Gupta;Nikhil Naik;Ramesh Raskar,bowen@mit.edu;otkrist@mit.edu;naik@mit.edu;raskar@mit.edu,6;6;6,4;3;4,Accept (Poster),4,6,0.0,no,11/4/16,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2,5;5;5;5,
151,151,151,151,151,151,151,151,ICLR,2017,Categorical Reparameterization with Gumbel-Softmax,Eric Jang;Shixiang Gu;Ben Poole,ejang@google.com;sg717@cam.ac.uk;poole@cs.stanford.edu,7;6;6,5;4;3,Accept (Poster),4,3,0.0,no,11/4/16,Google;University of Cambridge;Stanford University,-1;67;3,-1;4;3,5
152,152,152,152,152,152,152,152,ICLR,2017,PixelVAE: A Latent Variable Model for Natural Images,Ishaan Gulrajani;Kundan Kumar;Faruk Ahmed;Adrien Ali Taiga;Francesco Visin;David Vazquez;Aaron Courville,igul222@gmail.com;kundankumar2510@gmail.com;faruk.ahmed.91@gmail.com;adrien.alitaiga@gmail.com;francesco.visin@polimi.it;dvazquez@cvc.uab.es;aaron.courville@gmail.com,7;6;7,4;3;3,Accept (Poster),3,3,0.0,no,11/4/16,"University of Montreal;University of Montreal;University of Montreal;University of Montreal;Politecnico di Milano;Computer Vision Center, Universitat Autònoma de Barcelona;University of Montreal",113;113;113;113;140;462;113,103;103;103;103;981;164;103,5
153,153,153,153,153,153,153,153,ICLR,2017,Transfer of View-manifold Learning to Similarity Perception of Novel Objects,Xingyu Lin;Hao Wang;Zhihao Li;Yimeng Zhang;Alan Yuille;Tai Sing Lee,sean.linxingyu@pku.edu.cn;hao.wang@pku.edu.cn;zhihaol@andrew.cmu.edu;yimengzh@andrew.cmu.edu;alan.yuille@jhu.edu;tai@cnbc.cmu.edu,5;6;7,5;4;3,Accept (Poster),2,4,0.0,no,11/5/16,Peking University;Peking University;Carnegie Mellon University;Carnegie Mellon University;Johns Hopkins University;Carnegie Mellon University,25;25;1;1;67;1,29;29;23;23;17;23,
154,154,154,154,154,154,154,154,ICLR,2017,Sample Efficient Actor-Critic with  Experience Replay,Ziyu Wang;Victor Bapst;Nicolas Heess;Volodymyr Mnih;Remi Munos;Koray Kavukcuoglu;Nando de Freitas,ziyu@google.com;vbapst@google.com;heess@google.com;vmnih@google.com;Munos@google.com;korayk@google.com;nandodefreitas@google.com,7;6;6,3;3;4,Accept (Poster),3,8,1.0,no,11/4/16,Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,
155,155,155,155,155,155,155,155,ICLR,2017,Distributed Second-Order Optimization using Kronecker-Factored Approximations,Jimmy Ba;Roger Grosse;James Martens,jimmy@psi.toronto.edu;rgrosse@cs.toronto.edu;jmartens@cs.toronto.edu,7;6,4;3,Accept (Poster),6,3,0.0,no,11/5/16,"University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",15;15;15,22;22;22,
156,156,156,156,156,156,156,156,ICLR,2017,Learning to Navigate in Complex Environments,Piotr Mirowski;Razvan Pascanu;Fabio Viola;Hubert Soyer;Andy Ballard;Andrea Banino;Misha Denil;Ross Goroshin;Laurent Sifre;Koray Kavukcuoglu;Dharshan Kumaran;Raia Hadsell,piotrmirowski@google.com;razp@google.com;fviola@google.com;soyer@google.com;aybd@google.com;abanino@google.com;mdenil@google.com;goroshin@google.com;sifre@google.com;korayk@google.com;dkumaran@google.com;raia@google.com,7;5;7,5;4;3,Accept (Poster),6,9,0.0,no,11/4/16,Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,
157,157,157,157,157,157,157,157,ICLR,2017,Learning to superoptimize programs,Rudy Bunel;Alban Desmaison;M. Pawan Kumar;Philip H.S. Torr;Pushmeet Kohli,rudy@robots.ox.ac.uk;alban@robots.ox.ac.uk;pawan@robots.ox.ac.uk;philip.torr@eng.ox.ac.uk;pkohli@microsoft.com,6;8;7,5;4;4,Accept (Poster),2,7,0.0,no,11/4/16,University of Oxford;University of Oxford;University of Oxford;University of Oxford;Microsoft,50;50;50;50;-1,1;1;1;1;-1,3
158,158,158,158,158,158,158,158,ICLR,2017,Trained Ternary Quantization,Chenzhuo Zhu;Song Han;Huizi Mao;William J. Dally,zhucz13@mails.tsinghua.edu.cn;songhan@stanford.edu;huizi@stanford.edu;dally@stanford.edu,3;7;7;8,3;5;3;5,Accept (Poster),0,10,0.0,no,11/4/16,Tsinghua University;Stanford University;Stanford University;Stanford University,11;3;3;3,35;3;3;3,
159,159,159,159,159,159,159,159,ICLR,2017,Online Bayesian Transfer Learning for Sequential Data Modeling,Priyank Jaini;Zhitang Chen;Pablo Carbajal;Edith Law;Laura Middleton;Kayla Regan;Mike Schaekermann;George Trimponias;James Tung;Pascal Poupart,pjaini@uwaterloo.ca;chenzhitang2@huawei.com;pablo@veedata.io;edith.law@uwaterloo.ca;lmiddlet@uwaterloo.ca;kregan@uwaterloo.ca;mschaekermann@uwaterloo.ca;g.trimponias@huawei.com;james.tung@uwaterloo.ca;ppoupart@uwaterloo.ca,7;6;6,3;3;3,Accept (Poster),2,7,0.0,no,11/4/16,University of Waterloo;Huawei Technologies Ltd.;;University of Waterloo;University of Waterloo;University of Waterloo;University of Waterloo;Huawei Technologies Ltd.;University of Waterloo;University of Waterloo,25;-1;-1;25;25;25;25;-1;25;25,174;-1;-1;174;174;174;174;-1;174;174,11;6
160,160,160,160,160,160,160,160,ICLR,2017,Deep Variational Information Bottleneck,Alexander A. Alemi;Ian Fischer;Joshua V. Dillon;Kevin Murphy,alemi@google.com;iansf@google.com;jvdillon@google.com;kpmurphy@google.com,7;6;6,4;4;3,Accept (Poster),2,8,0.0,no,11/4/16,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,4;1;8
161,161,161,161,161,161,161,161,ICLR,2017,Exploring Sparsity in Recurrent Neural Networks,Sharan Narang;Greg Diamos;Shubho Sengupta;Erich Elsen,sharan@baidu.com;gdiamos@baidu.com;ssengupta@baidu.com;eriche@google.com,7;6,3;4,Accept (Poster),6,6,0.0,no,11/4/16,Baidu;Baidu;Baidu;Google,-1;-1;-1;-1,-1;-1;-1;-1,
162,162,162,162,162,162,162,162,ICLR,2017,Neural Photo Editing with Introspective Adversarial Networks,Andrew Brock;Theodore Lim;J.M. Ritchie;Nick Weston,ajb5@hw.ac.uk;t.lim@hw.ac.uk;j.m.ritchie@hw.ac.uk;Nick.Weston@renishaw.com,5;6;6,3;4;4,Accept (Poster),4,5,0.0,no,10/29/16,Heriot-Watt University;Heriot-Watt University;Heriot-Watt University;Renishaw,275;275;275;-1,429;429;429;-1,5;4;8
163,163,163,163,163,163,163,163,ICLR,2017,Semi-Supervised Classification with Graph Convolutional Networks,Thomas N. Kipf;Max Welling,T.N.Kipf@uva.nl;M.Welling@uva.nl,7;7;7,3;4;4,Accept (Poster),1,1,1.0,no,11/3/16,University of Amsterdam;University of Amsterdam,161;161,63;63,10
164,164,164,164,164,164,164,164,ICLR,2017,Deep Learning with Dynamic Computation Graphs,Moshe Looks;Marcello Herreshoff;DeLesley Hutchins;Peter Norvig,madscience@google.com;marcelloh@google.com;delesley@google.com;pnorvig@google.com,8;7;8,3;5;3,Accept (Poster),1,5,1.0,no,11/4/16,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,3;10
165,165,165,165,165,165,165,165,ICLR,2017,A recurrent neural network without chaos,Thomas Laurent;James von Brecht,tlaurent@lmu.edu;james.vonbrecht@csulb.edu,8;7;7,3;4;4,Accept (Poster),6,3,0.0,no,11/4/16,"Loyola Marymount University;California State University, Long Beach",462;462,981;981,3;1
166,166,166,166,166,166,166,166,ICLR,2017,Adversarially Learned Inference,Vincent Dumoulin;Ishmael Belghazi;Ben Poole;Alex Lamb;Martin Arjovsky;Olivier Mastropietro;Aaron Courville,vincent.dumoulin@umontreal.ca;ishmael.belghazi@gmail.com;poole@cs.stanford.edu;alex6200@gmail.com;martinarjovsky@gmail.com;oli.mastro@gmail.com;aaron.courville@gmail.com,7;7;8,4;3;4,Accept (Poster),4,6,0.0,no,11/4/16,University of Montreal;University of Montreal;Stanford University;University of Montreal;New York University;University of Montreal;University of Montreal,113;113;3;113;25;113;113,103;103;3;103;32;103;103,5;4
167,167,167,167,167,167,167,167,ICLR,2017,Amortised MAP Inference for Image Super-resolution,Casper Kaae Sønderby;Jose Caballero;Lucas Theis;Wenzhe Shi;Ferenc Huszár,casperkaae@gmail.com;jcaballero@twitter.com;ltheis@twitter.com;wshi@twitter.com;fhuszar@twitter.com,8;9;7,5;3;2,Accept (Oral),2,5,0.0,no,11/1/16,University of Copenhagen;Twitter;Twitter;Twitter;Twitter,105;-1;-1;-1;-1,120;-1;-1;-1;-1,5;4
168,168,168,168,168,168,168,168,ICLR,2017,A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING,Zhouhan Lin;Minwei Feng;Cicero Nogueira dos Santos;Mo Yu;Bing Xiang;Bowen Zhou;Yoshua Bengio,lin.zhouhan@gmail.com;mfeng@us.ibm.com;cicerons@us.ibm.com;yum@us.ibm.com;bingxia@us.ibm.com;zhou@us.ibm.com;yoshua.bengio@umontreal.ca,6;5;8,5;4;4,Accept (Poster),4,5,0.0,no,11/4/16,University of Montreal;International Business Machines;International Business Machines;International Business Machines;International Business Machines;International Business Machines;University of Montreal,113;-1;-1;-1;-1;-1;113,103;-1;-1;-1;-1;-1;103,
169,169,169,169,169,169,169,169,ICLR,2017,Geometry of Polysemy,Jiaqi Mu;Suma Bhat;Pramod Viswanath,jiaqimu2@illinois.edu;spbhat2@illinois.edu;pramodv@illinois.edu,7;7;7,3;4;4,Accept (Poster),4,6,0.0,no,11/4/16,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",4;4;4,36;36;36,3
170,170,170,170,170,170,170,170,ICLR,2017,On Detecting Adversarial Perturbations,Jan Hendrik Metzen;Tim Genewein;Volker Fischer;Bastian Bischoff,JanHendrik.Metzen@de.bosch.com;Tim.Genewein@de.bosch.com;Volker.Fischer@de.bosch.com;Bastian.Bischoff@de.bosch.com,7;7;5,4;4;3,Accept (Poster),3,3,0.0,no,11/4/16,Bosch;Bosch;Bosch;Bosch,-1;-1;-1;-1,433;433;433;433,4
171,171,171,171,171,171,171,171,ICLR,2017,Frustratingly Short Attention Spans in Neural Language Modeling,Michał Daniluk;Tim Rocktäschel;Johannes Welbl;Sebastian Riedel,michal.daniluk.15@ucl.ac.uk;t.rocktaschel@cs.ucl.ac.uk;j.welbl@cs.ucl.ac.uk;s.riedel@cs.ucl.ac.uk,7;7;7,4;4;4,Accept (Poster),3,3,1.0,no,11/4/16,University College London;University College London;University College London;University College London,45;45;45;45,15;15;15;15,3
172,172,172,172,172,172,172,172,ICLR,2017,Learning Invariant Representations Of Planar Curves ,Gautam Pai;Aaron Wetzler;Ron Kimmel,paigautam@cs.technion.ac.il;twerd@cs.technion.ac.il;ron@cs.technion.ac.il,5;6;8,2;5;3,Accept (Poster),3,4,0.0,no,11/4/16,Technion;Technion;Technion,24;24;24,301;301;301,
173,173,173,173,173,173,173,173,ICLR,2017,Deep Multi-task Representation Learning: A Tensor Factorisation Approach,Yongxin Yang;Timothy M. Hospedales,yongxin.yang@qmul.ac.uk;t.hospedales@qmul.ac.uk,5;7;8,3;4;4,Accept (Poster),1,5,0.0,no,11/4/16,Queen Mary University London;Queen Mary University London,275;275,113;113,
174,174,174,174,174,174,174,174,ICLR,2017,Unrolled Generative Adversarial Networks,Luke Metz;Ben Poole;David Pfau;Jascha Sohl-Dickstein,lmetz@google.com;poole@cs.stanford.edu;pfau@google.com;jaschasd@google.com,7;7;9,5;5;5,Accept (Poster),3,8,0.0,no,11/4/16,Google;Stanford University;Google;Google,-1;3;-1;-1,-1;3;-1;-1,5;4
175,175,175,175,175,175,175,175,ICLR,2017,Structured Attention Networks,Yoon Kim;Carl Denton;Luong Hoang;Alexander M. Rush,yoonkim@seas.harvard.edu;carldenton@college.harvard.edu;lhoang@g.harvard.edu;srush@seas.harvard.edu,8;8;8,4;5;3,Accept (Poster),3,2,0.0,no,11/4/16,Harvard University;Harvard University;Harvard University;Harvard University,36;36;36;36,6;6;6;6,3;2;10
176,176,176,176,176,176,176,176,ICLR,2017,HyperNetworks,David Ha;Andrew M. Dai;Quoc V. Le,hadavid@google.com;adai@google.com;qvl@google.com,8;7;6,4;4;5,Accept (Poster),2,5,0.0,no,10/27/16,Google;Google;Google,-1;-1;-1,-1;-1;-1,3
177,177,177,177,177,177,177,177,ICLR,2017,Dropout with Expectation-linear Regularization,Xuezhe Ma;Yingkai Gao;Zhiting Hu;Yaoliang Yu;Yuntian Deng;Eduard Hovy,xuezhem@cs.cmu.edu;yingkaig@cs.cmu.edu;zhitinghu@cs.cmu.edu;yaoliang@cs.cmu.edu;dengyuntian@gmail.com;hovy@cmu.edu,8;7;8,3;4;3,Accept (Poster),1,4,0.0,no,10/28/16,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;;Carnegie Mellon University,1;1;1;1;-1;1,23;23;23;23;-1;23,1
178,178,178,178,178,178,178,178,ICLR,2017,Dynamic Coattention Networks For Question Answering,Caiming Xiong;Victor Zhong;Richard Socher,cxiong@salesforce.com;vzhong@salesforce.com;rsocher@salesforce.com,8;8;8,3;4;4,Accept (Poster),12,9,2.0,no,11/4/16,SalesForce.com;SalesForce.com;SalesForce.com,-1;-1;-1,-1;-1;-1,
179,179,179,179,179,179,179,179,ICLR,2017,Visualizing Deep Neural Network Decisions: Prediction Difference Analysis,Luisa M Zintgraf;Taco S Cohen;Tameem Adel;Max Welling,lmzintgraf@gmail.com;t.s.cohen@uva.nl;tameem.hesham@gmail.com;m.welling@uva.nl,6;9;6,4;4;5,Accept (Poster),1,6,0.0,no,11/4/16,Vrije Universiteit Brussel;University of Amsterdam;University of Cambridge;University of Amsterdam,-1;161;67;161,-1;63;4;63,
180,180,180,180,180,180,180,180,ICLR,2017,Density estimation using Real NVP,Laurent Dinh;Jascha Sohl-Dickstein;Samy Bengio,dinh.laurent@gmail.com;jaschasd@google.com;bengio@google.com,8;8;7,4;4;4,Accept (Poster),4,2,0.0,no,11/4/16,Google;Google;Google,-1;-1;-1,-1;-1;-1,
181,181,181,181,181,181,181,181,ICLR,2017,Dialogue Learning With Human-in-the-Loop,Jiwei Li;Alexander H. Miller;Sumit Chopra;Marc'Aurelio Ranzato;Jason Weston,jiwel@fb.com;ahm@fb.com;spchopra@fb.com;ranzato@fb.com;jase@fb.com,5;6;7,4;3;4,Accept (Poster),4,5,0.0,no,11/4/16,Facebook;Facebook;Facebook;Facebook;Facebook,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,
182,182,182,182,182,182,182,182,ICLR,2017,Learning to Act by Predicting the Future,Alexey Dosovitskiy;Vladlen Koltun,adosovitskiy@gmail.com;vkoltun@gmail.com,7;8;8,4;4;4,Accept (Oral),4,1,0.0,no,11/4/16,Universität Freiburg;Intel,113;-1,95;-1,
183,183,183,183,183,183,183,183,ICLR,2017,"Offline bilingual word vectors, orthogonal transformations and the inverted softmax",Samuel L. Smith;David H. P. Turban;Steven Hamblin;Nils Y. Hammerla,samuel.smith@babylonhealth.com;dt382@cam.ac.uk;steven.hamblin@babylonhealth.com;nils.hammerla@babylonhealth.com,7;8;6,5;5;3,Accept (Poster),2,6,1.0,no,11/4/16,babylon health;University of Cambridge;babylon health;babylon health,-1;67;-1;-1,-1;4;-1;-1,1
184,184,184,184,184,184,184,184,ICLR,2017,Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer,Sergey Zagoruyko;Nikos Komodakis,sergey.zagoruyko@enpc.fr;nikos.komodakis@enpc.fr,6;6;6,4;4;4,Accept (Poster),4,7,0.0,no,11/5/16,ENPC;ENPC,462;462,373;373,3;2
185,185,185,185,185,185,185,185,ICLR,2017,Recurrent Mixture Density Network for Spatiotemporal Visual Attention,Loris Bazzani;Hugo Larochelle;Lorenzo Torresani,loris.bazzani@gmail.com;hugo.larochelle@usherbrooke.ca;lt@dartmouth.edu,7;6;6,4;4;4,Accept (Poster),4,7,0.0,no,11/3/16,Amazon;Université de Sherbrooke;Dartmouth College,-1;351;140,-1;567;82,2
186,186,186,186,186,186,186,186,ICLR,2017,Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations,David Krueger;Tegan Maharaj;Janos Kramar;Mohammad Pezeshki;Nicolas Ballas;Nan Rosemary Ke;Anirudh  Goyal;Yoshua Bengio;Aaron Courville;Christopher Pal,davidscottkrueger@gmail.com;tegan.jrm@gmail.com;ballas.n@gmail.com,7;8;8,4;4;5,Accept (Poster),3,3,0.0,no,11/4/16,University of Montreal;Polytechnique Montreal;University of Montreal,113;351;113,103;981;103,3;8
187,187,187,187,187,187,187,187,ICLR,2017,Adversarial Machine Learning at Scale,Alexey Kurakin;Ian J. Goodfellow;Samy Bengio,kurakin@google.com;ian@openai.com;bengio@google.com,7;6;6,3;4;4,Accept (Poster),3,3,0.0,no,11/3/16,Google;OpenAI;Google,-1;-1;-1,-1;-1;-1,4
188,188,188,188,188,188,188,188,ICLR,2017,A Learned Representation For Artistic Style,Vincent Dumoulin;Jonathon Shlens;Manjunath Kudlur,vi.dumoulin@gmail.com;shlens@google.com;keveman@google.com,7;8;8,3;5;5,Accept (Poster),6,10,0.0,no,10/26/16,University of Montreal;Google;Google,113;-1;-1,103;-1;-1,
189,189,189,189,189,189,189,189,ICLR,2017,Batch Policy Gradient  Methods for  Improving Neural Conversation Models,Kirthevasan Kandasamy;Yoram Bachrach;Ryota Tomioka;Daniel Tarlow;David Carter,kandasamy@cmu.edu;yorambac@gmail.com;ryoto@microsoft.com;dtarlow@microsoft.com;dacart@microsoft.com,6;7;8,3;3;3,Accept (Poster),2,5,0.0,no,11/4/16,Carnegie Mellon University;Microsoft;Microsoft;Microsoft;Microsoft,1;-1;-1;-1;-1,23;-1;-1;-1;-1,3
190,190,190,190,190,190,190,190,ICLR,2017,Incorporating long-range consistency in CNN-based texture generation,Guillaume Berger;Roland Memisevic,guillaume.berger@umontreal.ca;memisevr@iro.umontreal.ca,5;7;7,5;5;5,Accept (Poster),2,1,0.0,no,11/4/16,University of Montreal;University of Montreal,113;113,103;103,
191,191,191,191,191,191,191,191,ICLR,2017,Paleo: A Performance Model for Deep Neural Networks,Hang Qi;Evan R. Sparks;Ameet Talwalkar,hangqi@cs.ucla.edu;sparks@cs.berkeley.edu;ameet@cs.ucla.edu,6;7;6,4;4;4,Accept (Poster),2,3,0.0,no,11/4/16,"University of California, Los Angeles;University of California Berkeley;University of California, Los Angeles",20;5;20,14;10;14,
192,192,192,192,192,192,192,192,ICLR,2017,On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima,Nitish Shirish Keskar;Dheevatsa Mudigere;Jorge Nocedal;Mikhail Smelyanskiy;Ping Tak Peter Tang,keskar.nitish@u.northwestern.edu;dheevatsa.mudigere@intel.com;j-nocedal@northwestern.edu;mikhail.smelyanskiy@intel.com;peter.tang@intel.com,8;10;6,3;3;4,Accept (Oral),2,5,2.0,no,11/3/16,Northwestern University;Intel;Northwestern University;Intel;Intel,45;-1;45;-1;-1,20;-1;20;-1;-1,8
193,193,193,193,193,193,193,193,ICLR,2017,Predicting Medications from Diagnostic Codes with Recurrent Neural Networks,Jacek M. Bajor;Thomas A. Lasko,jacek.m.bajor@vanderbilt.edu;tom.lasko@vanderbilt.edu,6;8;7,3;4;5,Accept (Poster),3,3,0.0,no,11/3/16,Vanderbilt University;Vanderbilt University,230;230,108;108,
194,194,194,194,194,194,194,194,ICLR,2017,An Information-Theoretic Framework for Fast and Robust Unsupervised Learning via Neural Population Infomax,Wentao Huang;Kechen Zhang,whuang21@jhmi.edu;kzhang4@jhmi.edu,8;5;7,2;2;3,Accept (Poster),1,6,0.0,no,11/5/16,;Johns Hopkins University,-1;67,-1;17,
195,195,195,195,195,195,195,195,ICLR,2017,Regularizing CNNs with Locally Constrained Decorrelations,Pau Rodríguez;Jordi Gonzàlez;Guillem Cucurull;Josep M. Gonfaus;Xavier Roca,pau.rodriguez@cvc.uab.es;poal@cvc.uab.es;pep.gonfaus@visual-tagging.com;xavier.roca@visual-tagging.com,7;7;7,3;4;4,Accept (Poster),4,4,0.0,no,11/4/16,"Computer Vision Center, Universitat Autònoma de Barcelona;Computer Vision Center, Universitat Autònoma de Barcelona;Visual-tagging;Visual-tagging",462;462;-1;-1,164;164;-1;-1,
196,196,196,196,196,196,196,196,ICLR,2017,Generating Long and Diverse Responses with Neural Conversation Models,Louis Shao;Stephan Gouws;Denny Britz;Anna Goldie;Brian Strope;Ray Kurzweil,overmind@google.com;sgouws@google.com;dennybritz@google.com;agoldie@google.com;bps@google.com;raykurzweil@google.com,5;7;7,3;4;3,Reject,4,5,0.0,no,11/5/16,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,3
197,197,197,197,197,197,197,197,ICLR,2017,DeepDSL: A Compilation-based Domain-Specific Language for Deep Learning,Tian Zhao;Xiao Bing Huang;Yu Cao,tzhao@uwm.edu;xiaobing@uwm.edu;ycao@cs.uml.edu,6;7;8,4;4;3,Accept (Poster),3,2,0.0,no,11/4/16,"College of William and Mary;College of William and Mary;University of Massachusetts, Lowell",161;161;-1,280;280;-1,
198,198,198,198,198,198,198,198,ICLR,2017,Delving into Transferable Adversarial Examples and Black-box Attacks,Yanpei Liu;Xinyun Chen;Chang Liu;Dawn Song,resodo.liu@gmail.com;jungyhuk@gmail.com;liuchang@eecs.berkeley.edu;dawnsong@cs.berkeley.edu,6;7;5,3;3;3,Accept (Poster),7,5,0.0,no,11/4/16,Shanghai Jiao Tong University;Shanghai Jiao Tong University;University of California Berkeley;University of California Berkeley,61;61;5;5,214;214;10;10,4
199,199,199,199,199,199,199,199,ICLR,2017,Central Moment Discrepancy (CMD) for Domain-Invariant Representation Learning,Werner Zellinger;Thomas Grubinger;Edwin Lughofer;Thomas Natschläger;Susanne Saminger-Platz,werner.zellinger@jku.at;thomas.grubinger@scch.at;edwin.lughofer@jku.at;thomas.natschlaeger@scch.at;susanne.saminger-platz@jku.at,6;7;9,4;4;5,Accept (Poster),3,3,0.0,no,11/4/16,Johannes Kepler University Linz;Software Competence Center Hagenberg GmbH;Johannes Kepler University Linz;Software Competence Center Hagenberg GmbH;Johannes Kepler University Linz,462;-1;462;-1;462,498;-1;498;-1;498,4;1
200,200,200,200,200,200,200,200,ICLR,2017,Towards Deep Interpretability (MUS-ROVER II): Learning Hierarchical Representations of Tonal Music,Haizi Yu;Lav R. Varshney,haiziyu7@illinois.edu;varshney@illinois.edu,6;8;6,3;4;3,Accept (Poster),5,1,0.0,no,11/4/16,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",4;4,36;36,3
201,201,201,201,201,201,201,201,ICLR,2017,PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications,Tim Salimans;Andrej Karpathy;Xi Chen;Diederik P. Kingma,tim@openai.com;karpathy@openai.com;peter@openai.com;dpkingma@openai.com,6;7;7,3;4;5,Accept (Poster),5,5,0.0,no,11/5/16,OpenAI;OpenAI;OpenAI;OpenAI,-1;-1;-1;-1,-1;-1;-1;-1,5
202,202,202,202,202,202,202,202,ICLR,2017,Latent Sequence Decompositions,William Chan;Yu Zhang;Quoc Le;Navdeep Jaitly,williamchan@cmu.edu;yzhang87@mit.edu;qvl@google.com;ndjaitly@google.com,7;8;7,4;4;5,Accept (Poster),2,3,0.0,no,11/4/16,Carnegie Mellon University;Massachusetts Institute of Technology;Google;Google,1;2;-1;-1,23;5;-1;-1,
203,203,203,203,203,203,203,203,ICLR,2017,HolStep: A Machine Learning Dataset for Higher-order Logic Theorem Proving,Cezary Kaliszyk;François Chollet;Christian Szegedy,cezary.kaliszyk@uibk.ac.at;fchollet@google.com;szegedy@google.com,6;8;7,3;3;3,Accept (Poster),2,2,0.0,no,11/2/16,University of Innsbruck;Google;Google,462;-1;-1,307;-1;-1,1
204,204,204,204,204,204,204,204,ICLR,2017,Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks,Yossi Adi;Einat Kermany;Yonatan Belinkov;Ofer Lavi;Yoav Goldberg,yossiadidrum@gmail.com;einatke@il.ibm.com;belinkov@mit.edu;oferl@il.ibm.com;yoav.goldberg@gmail.com,8;8;8,5;4;4,Accept (Poster),5,3,0.0,no,11/3/16,Bar Ilan University;International Business Machines;Massachusetts Institute of Technology;International Business Machines;Bar-Ilan University,87;-1;2;-1;87,489;-1;5;-1;489,
205,205,205,205,205,205,205,205,ICLR,2017,Learning similarity preserving representations with neural similarity and context encoders,Franziska Horn;Klaus-Robert Müller,franziska.horn@campus.tu-berlin.de;klaus-robert.mueller@tu-berlin.de,3;2;3,4;5;4,Reject,3,1,0.0,no,11/3/16,TU Berlin;TU Berlin,105;105,82;82,3
206,206,206,206,206,206,206,206,ICLR,2017,Memory-augmented Attention Modelling for Videos,Rasool Fakoor;Abdel-rahman Mohamed;Margaret Mitchell;Sing Bing Kang;Pushmeet Kohli,rasool.fakoor@mavs.uta.edu;asamir@microsoft.com;margarmitchell@gmail.com;SingBing.Kang@microsoft.com;pkohli@microsoft.com,4;4;4,4;5,Reject,9,3,0.0,no,11/4/16,"University of Texas, Arlington;Microsoft;;Microsoft;Microsoft",113;-1;-1;-1;-1,570;-1;-1;-1;-1,
207,207,207,207,207,207,207,207,ICLR,2017,Neural Causal Regularization under the Independence of Mechanisms Assumption,Mohammad Taha Bahadori;Krzysztof Chalupka;Edward Choi;Robert Chen;Walter F. Stewart;Jimeng Sun,bahadori@gatech.edu;kjchalup@caltech.edu;mp2893@gatech.edu;rchen87@gatech.edu;StewarWF@sutterhealth.org;jsun@cc.gatech.edu,5;4;6,4;4;5,Reject,7,5,0.0,no,11/4/16,Georgia Institute of Technology;California Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;;Georgia Institute of Technology,12;129;12;12;-1;12,33;2;33;33;-1;33,
208,208,208,208,208,208,208,208,ICLR,2017,New Learning Approach By Genetic Algorithm In A Convolutional Neural Network For Pattern Recognition,Mohammad Ali Mehrolhassani;Majid Mohammadi,Alimehrolhassani@yahoo.com;Mohammadi@uk.ac.ir,3;3;2,5;5;5,Reject,6,7,0.0,no,11/1/16,;University of Tehran,-1;462,-1;747,
209,209,209,209,209,209,209,209,ICLR,2017,Bit-Pragmatic Deep Neural Network Computing,Jorge Albericio;Patrick Judd;Alberto Delmas;Sayeh Sharify;Andreas Moshovos,jorge.albericio@gmail.com;judd@ece.utoronto.ca;delmas1@ece.utoronto.ca;sayeh@ece.utoronto.ca;moshovos@ece.utoronto.ca,6;7;5,3;2;2,Invite to Workshop Track,1,6,0.0,no,11/4/16,University of Toronto;Toronto University;Toronto University;Toronto University;Toronto University,15;15;15;15;15,22;22;22;22;22,10
210,210,210,210,210,210,210,210,ICLR,2017,Learning Identity Mappings with Residual Gates,Pedro H. P. Savarese;Leonardo O. Mazza;Daniel R. Figueiredo,savarese@land.ufrj.br;leonardomazza@poli.ufrj.br;daniel@land.ufrj.br,5;5;6,5;5;4,Reject,3,9,0.0,no,11/4/16,Federal University of Rio de Janeiro - UFRJ;Federal University of Rio de Janeiro - UFRJ;Federal University of Rio de Janeiro - UFRJ,461;461;461,693;693;693,
211,211,211,211,211,211,211,211,ICLR,2017,Investigating Different Context Types and Representations for Learning Word Embeddings,Bofang Li;Tao Liu;Zhe Zhao;Buzhou Tang;Xiaoyong Du,libofang@ruc.edu.cn;tliu@ruc.edu.cn;helloworld@ruc.edu.cn;tangbuzhou@gmail.com;duyong@ruc.edu.cn,4;6;4,5;4;3,Reject,4,5,0.0,no,11/4/16,"University of Illinois, Urbana-Champaign;University of Illinois, Urbana-Champaign;University of Illinois, Urbana-Champaign;;University of Illinois, Urbana-Champaign",4;4;4;-1;4,36;36;36;-1;36,3
212,212,212,212,212,212,212,212,ICLR,2017,Neural Combinatorial Optimization with Reinforcement Learning,Irwan Bello*;Hieu Pham*;Quoc V. Le;Mohammad Norouzi;Samy Bengio,ibello@google.com;hyhieu@google.com;qvl@google.com;mnorouzi@google.com;bengio@google.com,6;6;6,4;4;4,Reject,16,8,0.0,no,11/4/16,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,10
213,213,213,213,213,213,213,213,ICLR,2017,A Differentiable Physics Engine for Deep Learning in Robotics,Jonas Degrave;Michiel Hermans;Joni Dambre;Francis wyffels,Jonas.Degrave@UGent.be;x@UGent.be;Joni.Dambre@UGent.be;Francis.wyffels@UGent.be,5;5;6,2;4;4,Invite to Workshop Track,3,4,0.0,no,11/3/16,;;;,-1;-1;-1;-1,-1;-1;-1;-1,
214,214,214,214,214,214,214,214,ICLR,2017,Two Methods for Wild Variational Inference,Qiang Liu;Yihao Feng,qiang.liu@dartmouth.edu;yihao.feng.gr@dartmouth.edu,3;3;3,4;4;4,Reject,1,1,0.0,no,11/4/16,Dartmouth College;Dartmouth College,140;140,82;82,
215,215,215,215,215,215,215,215,ICLR,2017,Sample Importance in Training Deep Neural Networks,Tianxiang Gao;Vladimir Jojic,tgao@cs.unc.edu;vjojic@cs.unc.edu,2;7;3,4;4;4,Reject,6,4,0.0,no,11/4/16,"University of North Carolina, Chapel Hill;University of North Carolina, Chapel Hill",73;73,56;56,
216,216,216,216,216,216,216,216,ICLR,2017,Neural Code Completion,Chang Liu;Xin Wang;Richard Shin;Joseph E. Gonzalez;Dawn Song,xinw@eecs.berkeley.edu;liuchang@eecs.berkeley.edu;ricshin@berkeley.edu;jegonzal@berkeley.edu;dawnsong@cs.berkeley.edu,5;5;4,4;4;4,Reject,3,7,0.0,no,11/4/16,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5;5,10;10;10;10;10,
217,217,217,217,217,217,217,217,ICLR,2017,Demystifying ResNet,Sihan Li;Jiantao Jiao;Yanjun Han;Tsachy Weissman,lisihan13@mails.tsinghua.edu.cn;jiantao@stanford.edu;yjhan@stanford.edu;tsachy@stanford.edu,4;5;4,4;3;5,Reject,7,3,0.0,no,11/3/16,Tsinghua University;Stanford University;Stanford University;Stanford University,11;3;3;3,35;3;3;3,9
218,218,218,218,218,218,218,218,ICLR,2017,Adjusting for Dropout Variance in Batch Normalization and Weight Initialization,Dan Hendrycks;Kevin Gimpel,dan@ttic.edu;kgimpel@ttic.edu,5;7;6,4;4;4,Reject,2,5,0.0,no,11/4/16,Toyota Technological Institute at Chicago;Toyota Technological Institute at Chicago,-1;-1,-1;-1,
219,219,219,219,219,219,219,219,ICLR,2017,Tartan: Accelerating Fully-Connected and Convolutional Layers in Deep Learning Networks by Exploiting Numerical Precision Variability,Alberto Delmás Lascorz;Sayeh Sharify;Patrick Judd;Andreas Moshovos,delmasl1@ece.utoronto.ca;sayeh@ece.utoronto.ca;moshovos@ece.utoronto.ca,5;5;6;4;4,5;5;2;1;3,Reject,2,8,0.0,no,11/4/16,Toronto University;Toronto University;Toronto University,15;15;15,22;22;22,
220,220,220,220,220,220,220,220,ICLR,2017,Cooperative Training of Descriptor and Generator Networks,Jianwen Xie;Yang Lu;Ruiqi Gao;Song-Chun Zhu;Ying Nian Wu,jianwen@ucla.edu;yanglv@ucla.edu;ruiqigao@ucla.edu;sczhu@stat.ucla.edu;ywu@stat.ucla.edu,4;3;6,3;4;4,Reject,2,2,0.0,no,11/4/16,"University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles",20;20;20;20;20,14;14;14;14;14,
221,221,221,221,221,221,221,221,ICLR,2017,Regularizing Neural Networks by Penalizing Confident Output Distributions,Gabriel Pereyra;George Tucker;Jan Chorowski;Lukasz Kaiser;Geoffrey Hinton,pereyra@google.com;gjt@google.com;chorowski@google.com;lukaszkaiser@google.com;geoffhinton@google.com,6;5;5,4;4;4,Reject,4,3,0.0,no,11/4/16,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,3
222,222,222,222,222,222,222,222,ICLR,2017,Song From PI: A Musically Plausible Network for Pop Music Generation,Hang Chu;Raquel Urtasun;Sanja Fidler,chuhang1122@cs.toronto.edu;urtasun@cs.toronto.edu;fidler@cs.toronto.edu,4;6;7,3;4;3,Invite to Workshop Track,3,1,0.0,no,11/4/16,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",15;15;15,22;22;22,
223,223,223,223,223,223,223,223,ICLR,2017,Dataset Augmentation in Feature Space,Terrance DeVries;Graham W. Taylor,terrance@uoguelph.ca;gwtaylor@uoguelph.ca,4;7;6,5;4;5,Invite to Workshop Track,2,4,0.0,no,11/4/16,University of Guelph;University of Guelph,275;275,398;398,
224,224,224,224,224,224,224,224,ICLR,2017,Recurrent Neural Networks for Multivariate Time Series with Missing Values,Zhengping Che;Sanjay Purushotham;Kyunghyun Cho;David Sontag;Yan Liu,zche@usc.edu;spurusho@usc.edu;kyunghyun.cho@nyu.edu;dsontag@cs.nyu.edu;yanliu.cs@usc.edu,6;5;6,4;3;3,Reject,4,4,0.0,no,11/4/16,University of Southern California;University of Southern California;New York University;New York University;University of Southern California,31;31;25;25;31,60;60;32;32;60,
225,225,225,225,225,225,225,225,ICLR,2017,Counterpoint by Convolution,Cheng-Zhi Anna Huang;Tim Cooijmans;Adam Roberts;Aaron Courville;Douglas Eck,chengzhiannahuang@gmail.com;tim.cooijmans@umontreal.ca;adarob@google.com;aaron.courville@umontreal.ca;deck@google.com,6;5;6,5;4;3,Reject,2,4,0.0,no,11/4/16,Harvard University;University of Montreal;Google;University of Montreal;Google,36;113;-1;113;-1,6;103;-1;103;-1,5
226,226,226,226,226,226,226,226,ICLR,2017,Fast Chirplet Transform to Enhance CNN Machine Listening - Validation on Animal calls and Speech,Herve Glotin;Julien Ricard;Randall Balestriero,glotin@univ-tln.fr;julien.ricard@gmail.com;randallbalestriero@gmail.com,6;6;4,3;5;4,Invite to Workshop Track,1,8,0.0,no,11/4/16,CNRS university Toulon;;Rice University,462;-1;80,981;-1;87,6
227,227,227,227,227,227,227,227,ICLR,2017,Finding a Jack-of-All-Trades: An Examination of Semi-supervised Learning in Reading Comprehension,Rudolf Kadlec;Ondřej Bajgar;Peter Hrincar;Jan Kleindienst,rudolf_kadlec@cz.ibm.com;obajgar@cz.ibm.com;phrincar@cz.ibm.com;jankle@cz.ibm.com,6;4;3,4;4;4,Reject,5,5,0.0,no,11/4/16,International Business Machines;International Business Machines;International Business Machines;International Business Machines,-1;-1;-1;-1,-1;-1;-1;-1,3;6
228,228,228,228,228,228,228,228,ICLR,2017,Efficient Summarization with Read-Again and Copy Mechanism,Wenyuan Zeng;Wenjie Luo;Sanja Fidler;Raquel Urtasun,cengwy13@mails.tsinghua.edu.cn;wenjie@cs.toronto.edu;fidler@cs.toronto.edu;urtasun@cs.toronto.edu,5;6;5,4;4;5,Reject,4,4,0.0,no,11/4/16,"Tsinghua University;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",11;15;15;15,35;22;22;22,
229,229,229,229,229,229,229,229,ICLR,2017,The Predictron: End-To-End Learning and Planning,David Silver;Hado van Hasselt;Matteo Hessel;Tom Schaul;Arthur Guez;Tim Harley;Gabriel Dulac-Arnold;David Reichert;Neil Rabinowitz;Andre Barreto;Thomas Degris,davidsilver@google.com;hado@google.com;mtthss@google.com;schaul@google.com;aguez@google.com;tharley@google.com;dulacarnold@google.com;reichert@google.com;ncr@google.com;andrebarreto@google.com;degris@google.com,4;6;9,4;5;2,Reject,2,10,0.0,no,11/4/16,Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,
230,230,230,230,230,230,230,230,ICLR,2017,Training Group Orthogonal Neural Networks with Privileged Information,Yunpeng Chen;Xiaojie Jin;Jiashi Feng;Shuicheng Yan,chenyunpeng@u.nus.edu;xiaojie.jin@u.nus.edu;elefjia@nus.edu.sg;yanshuicheng@360.cn,6;5;6,4;4;4,Reject,2,4,0.0,no,11/3/16,National University of Singapore;National University of Singapore;National University of Singapore;Qihoo 360 Technology Co. Ltd,15;15;15;-1,24;24;24;-1,2;8
231,231,231,231,231,231,231,231,ICLR,2017,Neural Data Filter for Bootstrapping Stochastic Gradient Descent,Yang Fan;Fei Tian;Tao Qin;Tie-Yan Liu,v-yanfa@microsoft.com;fetia@microsoft.com;taoqin@microsoft.com;tie-yan.liu@microsoft.com,4;6;7,5;4,Invite to Workshop Track,2,8,0.0,no,11/4/16,Microsoft;Microsoft;Microsoft;Microsoft,-1;-1;-1;-1,-1;-1;-1;-1,
232,232,232,232,232,232,232,232,ICLR,2017,Surprise-Based Intrinsic Motivation for Deep Reinforcement Learning,Joshua Achiam;Shankar Sastry,jachiam@berkeley.edu;sastry@coe.berkeley.edu,6;6;6,3;3;4,Invite to Workshop Track,1,3,0.0,no,11/4/16,University of California Berkeley;University of California Berkeley,5;5,10;10,
233,233,233,233,233,233,233,233,ICLR,2017,Eigenvalues of the Hessian in Deep Learning: Singularity and Beyond,Levent Sagun;Leon Bottou;Yann LeCun,leventsagun@gmail.com;leon@bottou.org;yann@cs.nyu.edu,3;4;4,4;5;5,Reject,0,8,0.0,no,11/4/16,New York University;Facebook;New York University,25;-1;25,32;-1;32,
234,234,234,234,234,234,234,234,ICLR,2017,Extensions and Limitations of the Neural GPU,Eric Price;Wojciech Zaremba;Ilya Sutskever,ecprice@cs.utexas.edu;woj@openai.com;ilyasu@openai.com,5;4;5,3;4;4,Reject,0,2,0.0,no,11/4/16,"University of Texas, Austin;OpenAI;OpenAI",20;-1;-1,50;-1;-1,4;8
235,235,235,235,235,235,235,235,ICLR,2017,Options Discovery with Budgeted Reinforcement Learning,Aurelia Léon;Ludovic Denoyer,aurelia.leon@lip6.fr;ludovic.denoyer@lip6.fr,5;4;5;4,4;5;5;4,Reject,2,3,0.0,no,11/4/16,LIP6;LIP6,-1;-1,-1;-1,
236,236,236,236,236,236,236,236,ICLR,2017,On orthogonality and learning recurrent networks with long term dependencies,Eugene Vorontsov;Chiheb Trabelsi;Samuel Kadoury;Chris Pal,eugene.vorontsov@gmail.com;chiheb.trabelsi@polymtl.ca;samuel.kadoury@polymtl.ca;christopher.pal@polymtl.ca,7;5;5,4;4;5,Reject,5,4,0.0,no,11/5/16,Polytechnique Montreal;Polytechnique Montreal;Polytechnique Montreal;Polytechnique Montreal,351;351;351;351,981;981;981;981,1
237,237,237,237,237,237,237,237,ICLR,2017,Learning Continuous Semantic Representations of Symbolic Expressions,Miltiadis Allamanis;Pankajan Chanthirasegaran;Pushmeet Kohli;Charles Sutton,m.allamanis@ed.ac.uk;pankajan.chanthirasegaran@ed.ac.uk;pkohli@microsoft.com;csutton@ed.ac.uk,7;5;6,3;3;4,Invite to Workshop Track,2,5,0.0,no,11/4/16,University of Edinburgh;University of Edinburgh;Microsoft;University of Edinburgh,33;33;-1;33,27;27;-1;27,
238,238,238,238,238,238,238,238,ICLR,2017,Fuzzy paraphrases in learning word representations with a lexicon,Yuanzhi Ke;Masafumi Hagiwara,enshika8811.a6@keio.jp;hagiwara@keio.jp,5;3;6,3;4;4,Reject,1,9,0.0,no,11/3/16,Keio University;Keio University,351;351,603;603,
239,239,239,239,239,239,239,239,ICLR,2017,Unsupervised Deep Learning of State Representation Using Robotic Priors ,Timothee LESORT;David FILLIAT,timothee.lesort@ensta-paristech.fr;david.filliat@ensta-paristech.fr,3;3;3,5;4;4,Reject,2,2,0.0,no,11/4/16,ENSTA ParisTech;ENSTA ParisTech,-1;-1,-1;-1,
240,240,240,240,240,240,240,240,ICLR,2017,LSTM-Based System-Call Language Modeling and Ensemble Method for Host-Based Intrusion Detection,Gyuwan Kim;Hayoon Yi;Jangho Lee;Yunheung Paek;Sungroh Yoon,kgwmath@snu.ac.kr;hyyi@snu.ac.kr;ubuntu@snu.ac.kr;ypaek@snu.ac.kr;sryoon@snu.ac.kr,5;5;8,3;4;3,Reject,1,4,0.0,no,11/4/16,Seoul National University;Seoul National University;Seoul National University;Seoul National University;Seoul National University,50;50;50;50;50,72;72;72;72;72,3
241,241,241,241,241,241,241,241,ICLR,2017,Epitomic Variational Autoencoders,Serena Yeung;Anitha Kannan;Yann Dauphin;Li Fei-Fei,serena@cs.stanford.edu;akannan@fb.com;ynd@fb.com;feifeili@cs.stanford.edu,5;4;8,5;5;5,Reject,1,4,0.0,no,11/4/16,Stanford University;Facebook;Facebook;Stanford University,3;-1;-1;3,3;-1;-1;3,5
242,242,242,242,242,242,242,242,ICLR,2017,Layer Recurrent Neural Networks,Weidi Xie;Alison Noble;Andrew Zisserman,weidi.xie@eng.ox.ac.uk;alison.noble@eng.ox.ac.uk;az@robots.ox.ac.uk,7;6;5,4;4;5,Reject,4,5,0.0,no,11/4/16,University of Oxford;University of Oxford;University of Oxford,50;50;50,1;1;1,2
243,243,243,243,243,243,243,243,ICLR,2017,Communicating Hierarchical Neural Controllers for Learning Zero-shot Task Generalization,Junhyuk Oh;Satinder Singh;Honglak Lee;Pushmeet Kohli,junhyuk@umich.edu;baveja@umich.edu;honglak@umich.edu;pkohli@microsoft.com,4;7;5;3,3;5;4,Reject,2,6,0.0,no,11/4/16,University of Michigan;University of Michigan;University of Michigan;Microsoft,8;8;8;-1,21;21;21;-1,3;8
244,244,244,244,244,244,244,244,ICLR,2017,NEWSQA: A MACHINE COMPREHENSION DATASET,Adam Trischler;Tong Wang;Xingdi Yuan;Justin Harris;Alessandro Sordoni;Philip Bachman;Kaheer Suleman,adam.trischler@maluuba.com;tong.wang@maluuba.com;eric.yuan@maluuba.com;justin.harris@maluuba.com;alessandro.sordoni@maluuba.com;phil.bachman@maluuba.com;k.suleman@maluuba.com,6;6;6,4;4;3,Reject,3,5,0.0,no,11/4/16,Maluuba;Maluuba;Maluuba;Maluuba;Maluuba;Maluuba;Maluuba,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,3
245,245,245,245,245,245,245,245,ICLR,2017,Coarse Pruning of Convolutional Neural Networks with Random Masks,Sajid Anwar;Wonyong Sung,sajid@dsp.snu.ac.kr;wysung@snu.ac.kr,6;4;5,3;4;4,Reject,0,0,0.0,no,11/4/16,Seoul National University;Seoul National University,50;50,72;72,4
246,246,246,246,246,246,246,246,ICLR,2017,Universality in halting time,Levent Sagun;Thomas Trogdon;Yann LeCun,leventsagun@gmail.com;tom.trogdon@gmail.com;yann@cs.nyu.edu,5;5;2,4;3;4,Reject,3,4,0.0,no,11/4/16,New York University;;New York University,25;-1;25,32;-1;32,
247,247,247,247,247,247,247,247,ICLR,2017,Generating Interpretable Images with Controllable Structure,Scott Reed;Aäron van den Oord;Nal Kalchbrenner;Victor Bapst;Matt Botvinick;Nando de Freitas,reedscot@google.com;avdnoord@google.com;nalk@google.com;vbapst@google.com;botvinick@google.com;nandodefreitas@google.com,7;5;6,3;3;3,Invite to Workshop Track,2,1,0.0,no,11/4/16,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,2
248,248,248,248,248,248,248,248,ICLR,2017,Representing inferential uncertainty in deep neural networks through sampling,Patrick McClure;Nikolaus Kriegeskorte,Patrick.McClure@mrc-cbu.cam.ac.uk;Nikolaus.Kriegeskorte@mrc-cbu.cam.ac.uk,4;4;5,4;4;4,Reject,1,5,0.0,no,11/4/16,University of Cambridge;University of Cambridge,67;67,4;4,11
249,249,249,249,249,249,249,249,ICLR,2017,Online Structure Learning for Sum-Product Networks with Gaussian Leaves,Wilson Hsu;Agastya Kalra;Pascal Poupart,wwhsu@uwaterloo.ca;a6kalra@uwaterloo.ca;ppoupart@uwaterloo.ca,6;4;4,3;2;1,Invite to Workshop Track,2,11,0.0,no,11/4/16,University of Waterloo;University of Waterloo;University of Waterloo,25;25;25,174;174;174,10
250,250,250,250,250,250,250,250,ICLR,2017,A Convolutional Encoder Model for Neural Machine Translation,Jonas Gehring;Michael Auli;David Grangier;Yann N. Dauphin,jgehring@fb.com;michaelauli@fb.com;grangier@fb.com;ynd@fb.com,7;6;6,3;5;4,Reject,4,5,0.0,no,11/4/16,Facebook;Facebook;Facebook;Facebook,-1;-1;-1;-1,-1;-1;-1;-1,3
251,251,251,251,251,251,251,251,ICLR,2017,Reference-Aware Language Models,Zichao Yang;Phil Blunsom;Chris Dyer;Wang Ling,zichaoy@cs.cmu.edu;pblunsom@google.com;cdyer@google.com;lingwang@google.com,6;5;5,4;4;4,Reject,4,3,0.0,no,11/4/16,Carnegie Mellon University;Google;Google;Google,1;-1;-1;-1,23;-1;-1;-1,3
252,252,252,252,252,252,252,252,ICLR,2017,Deep Learning with Sets and Point Clouds,Siamak Ravanbakhsh;Jeff Schneider;Barnabas Poczos,mravanba@cs.cmu.edu;bapoczos@cs.cmu.edu;jeff.schneider@cs.cmu.edu,5;7;5,4;1;4,Invite to Workshop Track,2,7,0.0,no,11/4/16,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1,23;23;23,
253,253,253,253,253,253,253,253,ICLR,2017,Exponential Machines,Alexander Novikov;Mikhail Trofimov;Ivan Oseledets,novikov@bayesgroup.ru;mikhail.trofimov@phystech.edu;i.oseledets@skoltech.ru,5;6;7;6,4;4;3;4,Invite to Workshop Track,4,8,0.0,no,11/4/16,Higher School of Economics;Moscow Institute of Physics and Technology;Skolkovo Institute of Science and Technology,462;462;-1,456;313;-1,
254,254,254,254,254,254,254,254,ICLR,2017,Adversarial examples in the physical world,Alexey Kurakin;Ian J. Goodfellow;Samy Bengio,kurakin@google.com;ian@openai.com;bengio@google.com,5;6;6,4;3;3,Invite to Workshop Track,2,3,0.0,no,11/2/16,Google;OpenAI;Google,-1;-1;-1,-1;-1;-1,4
255,255,255,255,255,255,255,255,ICLR,2017,Gated Multimodal Units for Information Fusion,John Arevalo;Thamar Solorio;Manuel Montes-y-Gómez;Fabio A. González,jearevaloo@unal.edu.co;solorio@cs.uh.edu;smmontesg@inaoep.mx;fagonzalezo@unal.edu.co,6;7;4,3;5;4,Invite to Workshop Track,4,5,0.0,no,11/5/16,Universidad Nacional de Colombia;University of Houston;;Universidad Nacional de Colombia,462;161;-1;462,981;365;-1;981,
256,256,256,256,256,256,256,256,ICLR,2017,Warped Convolutions: Efficient Invariance to Spatial Transformations,Joao F. Henriques;Andrea Vedaldi,joao@robots.ox.ac.uk;vedaldi@robots.ox.ac.uk,6;7;6,5;4;4,Reject,5,6,0.0,no,11/4/16,University of Oxford;University of Oxford,50;50,1;1,
257,257,257,257,257,257,257,257,ICLR,2017,Human perception in computer vision,Ron Dekel,ron.dekel@weizmann.ac.il,7;6;6,3;4;3,Reject,1,4,0.0,no,11/4/16,Weizmann Institute,105,981,2;8
258,258,258,258,258,258,258,258,ICLR,2017,Binary Paragraph Vectors,Karol Grzegorczyk;Marcin Kurdziel,kgr@agh.edu.pl;kurdziel@agh.edu.pl,6;5;6,2;5;3,Reject,1,5,0.0,no,11/4/16,"AGH University of Science and Technology, Krakow, Poland;AGH University of Science and Technology, Krakow, Poland",462;462,708;708,6
259,259,259,259,259,259,259,259,ICLR,2017,Modularized Morphing of Neural Networks,Tao Wei;Changhu Wang;Chang Wen Chen,taowei@buffalo.edu;chw@microsoft.com;chencw@buffalo.edu,7;6;7;5,4;5;5;4,Invite to Workshop Track,1,5,0.0,no,11/4/16,"State University of New York, Buffalo;Microsoft;State University of New York, Buffalo",94;-1;94,263;-1;263,1;10
260,260,260,260,260,260,260,260,ICLR,2017,NEUROGENESIS-INSPIRED DICTIONARY LEARNING: ONLINE MODEL ADAPTION IN A CHANGING WORLD,Sahil Garg;Irina Rish;Guillermo Cecchi;Aurelie Lozano,sahilgar@usc.edu;rish@us.ibm.com;gcecchi@us.ibm.com;aclozano@us.ibm.com,7;5;5,4;3;4,Reject,1,4,0.0,no,11/4/16,University of Southern California;International Business Machines;International Business Machines;International Business Machines,31;-1;-1;-1,60;-1;-1;-1,
261,261,261,261,261,261,261,261,ICLR,2017,Low-rank passthrough neural networks,Antonio Valerio Miceli Barone,amiceli@inf.ed.ac.uk,4;5;6,4;4,Reject,3,10,0.0,no,11/4/16,University of Edinburgh,33,27,
262,262,262,262,262,262,262,262,ICLR,2017,Towards Understanding the Invertibility of Convolutional Neural Networks,Anna C. Gilbert;Yi Zhang;Kibok Lee;Yuting Zhang;Honglak Lee,annacg@umich.edu;yeezhang@umich.edu;kibok@umich.edu;yutingzh@umich.edu;honglak@umich.edu,5;7;4,4;3;4,Reject,2,3,1.0,no,11/4/16,University of Michigan;University of Michigan;University of Michigan;University of Michigan;University of Michigan,8;8;8;8;8,21;21;21;21;21,
263,263,263,263,263,263,263,263,ICLR,2017,Recurrent Normalization Propagation,César Laurent;Nicolas Ballas;Pascal Vincent,cesar.laurent@umontreal.ca;nicolas.ballas@umontreal.ca;pascal.vincent@umontreal.ca,4;6;6,4;4;3,Invite to Workshop Track,3,3,0.0,no,11/4/16,University of Montreal;University of Montreal;University of Montreal,113;113;113,103;103;103,3;5
264,264,264,264,264,264,264,264,ICLR,2017,Learning in Implicit Generative Models,Shakir Mohamed;Balaji Lakshminarayanan,shakir@google.com;balajiln@google.com,6;8;7,4;4;3,Invite to Workshop Track,0,5,1.0,no,11/4/16,Google;Google,-1;-1,-1;-1,5;4;11
265,265,265,265,265,265,265,265,ICLR,2017,Intelligible Language Modeling with Input Switched Affine Networks,Jakob Foerster;Justin Gilmer;Jan Chorowski;Jascha Sohl-dickstein;David Sussillo,jakob.foerster@cs.ox.ac.uk;gilmer@google.com;jan.chorowski@cs.uni.wroc.pl;jaschasd@google.com;sussillo@google.com,6;7;6,4;3;4,Reject,1,1,0.0,no,11/5/16,University of Oxford;Google;University of Wroclaw;Google;Google,50;-1;230;-1;-1,1;-1;981;-1;-1,3
266,266,266,266,266,266,266,266,ICLR,2017,Lifelong Perceptual Programming By Example,Alexander L. Gaunt;Marc Brockschmidt;Nate Kushman;Daniel Tarlow,t-algaun@microsoft.com;mabrocks@microsoft.com;nkushman@microsoft.com;dtarlow@microsoft.com,2;8;4,5;4;4,Invite to Workshop Track,1,8,0.0,no,11/4/16,Microsoft;Microsoft;Microsoft;Microsoft,-1;-1;-1;-1,-1;-1;-1;-1,
267,267,267,267,267,267,267,267,ICLR,2017,Alternating Direction Method of Multipliers for Sparse Convolutional Neural Networks,Farkhondeh Kiaee;Christian Gagné;and Mahdieh Abbasi,farkhondeh.kiaee.1@ulaval.ca;christian.gagne@gel.ulaval.ca;mahdieh.abbasi.1@ulaval.ca,7;7;6;5,3;5;4;3,Reject,1,7,1.0,no,11/4/16,Laval university;Laval university;Laval university,462;462;462,265;265;265,8
268,268,268,268,268,268,268,268,ICLR,2017,Sentence Ordering using Recurrent Neural Networks,Lajanugen Logeswaran;Honglak Lee;Dragomir Radev,llajan@umich.edu;honglak@eecs.umich.edu;radev@umich.edu,6;6;7,4;4;3,Reject,3,1,0.0,no,11/3/16,University of Michigan;University of Michigan;University of Michigan,8;8;8,21;21;21,3
269,269,269,269,269,269,269,269,ICLR,2017,Towards an automatic Turing test: Learning to evaluate dialogue responses,Ryan Lowe;Michael Noseworthy;Iulian V. Serban;Nicolas Angelard-Gontier;Yoshua Bengio;Joelle Pineau,rlowe1@cs.mcgill.ca;michael.noseworthy@mail.mcgill.ca;julianserban@gmail.com;nicolas.angelard-gontier@mail.mcgill.ca;yoshua.umontreal@gmail.com;jpineau@cs.mcgill.ca,5;4;7,4;4;4,Invite to Workshop Track,4,5,0.0,no,11/5/16,McGill University;McGill University;University College London;McGill University;University of Montreal;McGill University,80;80;45;80;113;80,42;42;15;42;103;42,
270,270,270,270,270,270,270,270,ICLR,2017,Inference and Introspection in Deep Generative Models of Sparse Data,Rahul G. Krishnan;Matthew Hoffman,rahul@cs.nyu.edu;matthoffm@adobe.com,6;7;5;5,4;3;3;4,Reject,1,5,0.0,no,11/4/16,New York University;Adobe Systems,25;-1,32;-1,5
271,271,271,271,271,271,271,271,ICLR,2017,TreNet: Hybrid Neural Networks for Learning the Local Trend in Time Series,Tao Lin;Tian Guo;Karl Aberer,tao.lin@epfl.ch;tian.guo@epfl.ch;karl.aberer@epfl.ch,6;5;4,4;5;4,Reject,4,5,0.0,no,11/5/16,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne,25;25;25,30;30;30,
272,272,272,272,272,272,272,272,ICLR,2017,Compositional Kernel Machines,Robert Gens;Pedro Domingos,rcg@cs.washington.edu;pedrod@cs.washington.edu,5;5;6;5,3;4;4;4,Invite to Workshop Track,1,4,0.0,no,11/3/16,University of Washington;University of Washington,6;6,25;25,2
273,273,273,273,273,273,273,273,ICLR,2017,Introducing Active Learning for CNN under the light of Variational Inference,Melanie Ducoffe;Frederic Precioso,ducoffe@i3s.unice.fr;precioso@i3s.unice.fr,6;6;6,1;2;2,Reject,6,4,0.0,no,11/5/16,Université Côte d'Azur;Université Côte d'Azur,-1;-1,-1;-1,
274,274,274,274,274,274,274,274,ICLR,2017,L-SR1: A Second Order Optimization Method for Deep Learning,Vivek Ramamurthy;Nigel Duffy,vivek.ramamurthy@sentient.ai;nigel.duffy@sentient.ai,4;4;5,4;3;3,Reject,3,6,0.0,no,11/4/16,"Sentient Technologies, Inc.;Sentient Technologies, Inc.",-1;-1,-1;-1,
275,275,275,275,275,275,275,275,ICLR,2017,Adversarial examples for generative models,Jernej Kos;Ian Fischer;Dawn Song,jernej@kos.mx;iansf@google.com;dawnsong.travel@gmail.com,5;6;5,4;3;4,Reject,3,3,0.0,no,11/5/16,National University of Singapore;Google;University of California Berkeley,15;-1;5,24;-1;10,5;4
276,276,276,276,276,276,276,276,ICLR,2017,Hierarchical compositional feature learning,Miguel Lazaro-Gredilla;Yi Liu;D. Scott Phoenix;Dileep George,miguel@vicarious.com;yi@vicarious.com;scott@vicarious.com;dileep@vicarious.com,5;5;4,4;4;4,Reject,1,3,0.0,no,11/3/16,Vicarious Inc.;Vicarious Inc.;Vicarious Inc.;Vicarious Inc.,-1;-1;-1;-1,-1;-1;-1;-1,5
277,277,277,277,277,277,277,277,ICLR,2017,Unsupervised Perceptual Rewards for Imitation Learning,Pierre Sermanet;Kelvin Xu;Sergey Levine,sermanet@google.com;kelvinxx@google.com;slevine@google.com,4;6;6,4;4;5,Invite to Workshop Track,2,3,0.0,no,11/4/16,Google;Google;Google,-1;-1;-1,-1;-1;-1,
278,278,278,278,278,278,278,278,ICLR,2017,Submodular Sum-product Networks for Scene Understanding,Abram L. Friesen;Pedro Domingos,afriesen@cs.washington.edu;pedrod@cs.washington.edu,5;4;4,4;3;3,Reject,1,4,0.0,no,11/4/16,University of Washington;University of Washington,6;6,25;25,10
279,279,279,279,279,279,279,279,ICLR,2017,Semi-Supervised Detection of Extreme Weather Events in Large Climate Datasets,Evan Racah;Christopher Beckham;Tegan Maharaj;Prabhat;Christopher Pal,eracah@lbl.gov;christopher.beckham@polymtl.ca;tegan.maharaj@polymtl.ca;prabhat@lbl.gov;christopher.pal@polymtl.ca,6;4;6,3;4;4,Reject,2,3,0.0,no,11/4/16,Lawrence Berkeley National Lab;Polytechnique Montreal;Polytechnique Montreal;Lawrence Berkeley National Lab;Polytechnique Montreal,-1;351;351;-1;351,-1;981;981;-1;981,
280,280,280,280,280,280,280,280,ICLR,2017,Progressive Attention Networks for Visual Attribute Prediction,Paul Hongsuck Seo;Zhe Lin;Scott Cohen;Xiaohui Shen;Bohyung Han,hsseo@postech.ac.kr;zlin@adobe.com;scohen@adobe.com;xshen@adobe.com;bhhan@postech.ac.kr,7;4;6,4;5;3,Reject,2,5,0.0,no,11/3/16,POSTECH;Adobe Systems;Adobe Systems;Adobe Systems;POSTECH,113;-1;-1;-1;113,104;-1;-1;-1;104,
281,281,281,281,281,281,281,281,ICLR,2017,Fast Adaptation in Generative Models with Generative Matching Networks,Sergey Bartunov;Dmitry P. Vetrov,sbos.net@gmail.com;vetrovd@yandex.ru,4;5;7,4;3;4,Reject,2,4,0.0,no,11/4/16,Google;Higher School of Economics,-1;462,-1;456,5;6;8
282,282,282,282,282,282,282,282,ICLR,2017,Classless Association using Neural Networks,Federico Raue;Sebastian Palacio;Andreas Dengel;Marcus Liwicki,federico.raue@dfki.de;sebastian.palacio@dfki.de;andreas.dengel@dfki.de;liwicki@cs.uni-kl.de,5;5;6,4;3;3,Reject,1,4,0.0,no,11/4/16,German Research Center for AI;German Research Center for AI;German Research Center for AI;TU Kaiserslautern,-1;-1;-1;140,-1;-1;-1;398,
283,283,283,283,283,283,283,283,ICLR,2017,Recurrent Coevolutionary Feature Embedding Processes for Recommendation,Hanjun Dai*;Yichen Wang*;Rakshit Trivedi;Le Song,hanjundai@gatech.edu;yichen.wang@gatech.edu;rstrivedi@gatech.edu;lsong@cc.gatech.edu,6;6;6,4;4;3,Reject,4,4,0.0,no,11/4/16,Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology,12;12;12;12,33;33;33;33,
284,284,284,284,284,284,284,284,ICLR,2017,Unsupervised Learning of State Representations for Multiple Tasks,Antonin Raffin;Sebastian Höfer;Rico Jonschkowski;Oliver Brock;Freek Stulp,antonin.raffin@ensta-paristech.fr;sebastian.hoefer@tu-berlin.de;rico.jonschkowski@tu-berlin.de;oliver.brock@tu-berlin.de;freek.stulp@dlr.de,6;5;6,4;4;3,Reject,2,1,0.0,no,11/4/16,ENSTA ParisTech;TU Berlin;TU Berlin;TU Berlin;German Aerospace Center (DLR),-1;105;105;105;-1,-1;82;82;82;-1,
285,285,285,285,285,285,285,285,ICLR,2017,A Neural Stochastic Volatility Model,Rui Luo;Xiaojun Xu;Weinan Zhang;Jun Wang,r.luo@cs.ucl.ac.uk;xuxj@apex.sjtu.edu.cn;wnzhang@apex.sjtu.edu.cn;j.wang@cs.ucl.ac.uk,6;5;5,4;4;3,Reject,3,1,0.0,no,11/4/16,University College London;Shanghai Jiao Tong University;Shanghai Jiao Tong University;University College London,45;61;61;45,15;214;214;15,5
286,286,286,286,286,286,286,286,ICLR,2017,Playing SNES in the Retro Learning Environment,Nadav Bhonker;Shai Rozenberg;Itay Hubara,nadavbh@tx.technion.ac.il;shairoz@tx.technion.ac.il;itayhubara@gmail.com,5;4;7,4;4;4,Reject,3,3,0.0,no,11/4/16,Technion;Technion;,24;24;-1,301;301;-1,
287,287,287,287,287,287,287,287,ICLR,2017,Linear Time Complexity Deep Fourier Scattering Network and Extension to Nonlinear Invariants,Randall Balestriero;Herve Glotin,randallbalestriero@gmail.com;glotin@univ-tln.fr,4;5;4,3;3;5,Reject,1,4,0.0,no,11/4/16,Rice University;CNRS university Toulon,80;462,87;981,
288,288,288,288,288,288,288,288,ICLR,2017,Boosting Image Captioning with Attributes,Ting Yao;Yingwei Pan;Yehao Li;Zhaofan Qiu;Tao Mei,tiyao@microsoft.com;v-yipan@microsoft.com;v-yehl@microsoft.com;v-zhqiu@microsoft.com;tmei@microsoft.com,4;5;6,5;4;5,Reject,3,3,0.0,no,11/4/16,Microsoft;Microsoft;Microsoft;Microsoft;Microsoft,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,3;2
289,289,289,289,289,289,289,289,ICLR,2017,Recursive Regression with Neural Networks: Approximating the HJI PDE Solution,Vicenç Rubies Royo;Claire Tomlin,vrubies@berkeley.edu;tomlin@berkeley.edu,7;3;5,3;5;1,Invite to Workshop Track,0,4,0.0,no,11/4/16,University of California Berkeley;University of California Berkeley,5;5,10;10,
290,290,290,290,290,290,290,290,ICLR,2017,Deep Unsupervised Clustering with Gaussian Mixture Variational Autoencoders,Nat Dilokthanakul;Pedro A. M. Mediano;Marta Garnelo;Matthew C.H. Lee;Hugh Salimbeni;Kai Arulkumaran;Murray Shanahan,n.dilokthanakul14@imperial.ac.uk;pmediano@imperial.ac.uk;m.garnelo-abellanas13@imperial.ac.uk;matthew.lee13@imperial.ac.uk;h.salimbeni15@imperial.ac.uk;kailash.arulkumaran13@imperial.ac.uk;m.shanahan@imperial.ac.uk,8;4;4,4;4;4,Reject,6,5,0.0,no,11/3/16,Imperial College London;Imperial College London;Imperial College London;Imperial College London;Imperial College London;Imperial College London;Imperial College London,75;75;75;75;75;75;75,8;8;8;8;8;8;8,5
291,291,291,291,291,291,291,291,ICLR,2017,Taming the waves: sine as activation function in deep neural networks,Giambattista Parascandolo;Heikki Huttunen;Tuomas Virtanen,giambattista.parascandolo@tut.fi;heikki.huttunen@tut.fi;tuomas.virtanen@tut.fi,4;4;4,4;4;4,Reject,1,5,0.0,no,11/4/16,Tampere University of Technology;Tampere University of Technology;Tampere University of Technology,462;462;462,552;552;552,
292,292,292,292,292,292,292,292,ICLR,2017,Significance of Softmax-Based Features over Metric Learning-Based Features,Shota Horiguchi;Daiki Ikami;Kiyoharu Aizawa,horiguchi@hal.t.u-tokyo.ac.jp;ikami@hal.t.u-tokyo.ac.jp;aizawa@hal.t.u-tokyo.ac.jp,5;7;4,4;4;5,Reject,3,10,0.0,no,10/31/16,The University of Tokyo;The University of Tokyo;The University of Tokyo,50;50;50,39;39;39,2
293,293,293,293,293,293,293,293,ICLR,2017,Deep Generalized Canonical Correlation Analysis,Adrian Benton;Huda Khayrallah;Biman Gujral;Drew Reisinger;Sheng Zhang;Raman Arora,adrian@cs.jhu.edu;huda@jhu.edu;bgujral1@jhu.edu;reisinger@cogsci.jhu.edu;zsheng2@jhu.edu;arora@cs.jhu.edu,6;5;7,4;5;3,Reject,0,4,0.0,no,11/4/16,Johns Hopkins University;Johns Hopkins University;Johns Hopkins University;Johns Hopkins University;Johns Hopkins University;Johns Hopkins University,67;67;67;67;67;67,17;17;17;17;17;17,
294,294,294,294,294,294,294,294,ICLR,2017,Learning Recurrent Span Representations for Extractive Question Answering,Kenton Lee;Tom Kwiatkowksi;Ankur Parikh;Dipanjan Das,kentonl@cs.washington.edu;tomkwiat@google.com;aparikh@google.com;dipanjand@google.com,7;6;6,4;5;3,Reject,1,1,0.0,no,11/4/16,University of Washington;Google;Google;Google,6;-1;-1;-1,25;-1;-1;-1,3
295,295,295,295,295,295,295,295,ICLR,2017,GRAM: Graph-based Attention Model for Healthcare Representation Learning,Edward Choi;Mohammad Taha Bahadori;Le Song;Walter F. Stewart;Jimeng Sun,mp2893@gatech.edu;bahadori@gatech.edu;lsong@cc.gatech.edu;stewarwf@sutterhealth.org;jsun@cc.gatech.edu,6;6;6,4;4;3,Reject,2,6,0.0,no,11/4/16,Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;;Georgia Institute of Technology,12;12;12;-1;12,33;33;33;-1;33,10
296,296,296,296,296,296,296,296,ICLR,2017,Improving Sampling from Generative Autoencoders with Markov Chains,Antonia Creswell;Kai Arulkumaran;Anil Anthony Bharath,ac2211@imperial.ac.uk;ka709@imperial.ac.uk;aab01@imperial.ac.uk,3;3;3,5;4;4,Reject,5,4,0.0,no,10/31/16,Imperial College London;Imperial College London;Imperial College London,75;75;75,8;8;8,5;4
297,297,297,297,297,297,297,297,ICLR,2017,Towards Principled Methods for Training Generative Adversarial Networks,Martin Arjovsky;Leon Bottou,martinarjovsky@gmail.com;leonb@fb.com,8;10;7,3;5;4,Accept (Oral),4,16,0.0,no,11/4/16,New York University;Facebook,25;-1,32;-1,5;4
298,298,298,298,298,298,298,298,ICLR,2017,Learning Disentangled Representations in Deep Generative Models,N. Siddharth;Brooks Paige;Alban Desmaison;Jan-Willem van de Meent;Frank Wood;Noah D. Goodman;Pushmeet Kohli;Philip H.S. Torr,nsid@robots.ox.ac.uk;brooks@robots.ox.ac.uk;alban@robots.ox.ac.uk;j.vandemeent@northeastern.edu;fwood@robots.ox.ac.uk;ngoodman@stanford.edu;pkohli@microsoft.com;philip.torr@eng.ox.ac.uk,6;6;5,5;4;3,Reject,2,4,0.0,no,11/4/16,University of Oxford;University of Oxford;University of Oxford;Northeastern University;University of Oxford;Stanford University;Microsoft;University of Oxford,50;50;50;18;50;3;-1;50,1;1;1;778;1;3;-1;1,5;10
299,299,299,299,299,299,299,299,ICLR,2017,Local minima in training of deep networks,Grzegorz Swirszcz;Wojciech Marian Czarnecki;Razvan Pascanu,swirszcz@google.com;lejlot@google.com;razp@google.com,5;5;3,3;4;5,Reject,3,4,0.0,no,11/4/16,Google;Google;Google,-1;-1;-1,-1;-1;-1,1
300,300,300,300,300,300,300,300,ICLR,2017,Compact Embedding of Binary-coded Inputs and Outputs using Bloom Filters,Joan Serrà;Alexandros Karatzoglou,joan.serra@telefonica.com;alexandros.karatzoglou@telefonica.com,3;6;6,3;4;4,Reject,2,6,0.0,no,11/3/16,Telefonica Research;Telefonica Research,-1;-1,-1;-1,10
301,301,301,301,301,301,301,301,ICLR,2017,RenderGAN: Generating Realistic Labeled Data,Leon Sixt;Benjamin Wild;Tim Landgraf,leon.sixt@fu-berlin.de;benjamin.wild@fu-berlin.de;tim.landgraf@fu-berlin.de,5;6;6,3;4;4,Invite to Workshop Track,3,6,0.0,no,11/4/16,Freie Universität Berlin;Freie Universität Berlin;Freie Universität Berlin,-1;-1;-1,-1;-1;-1,5;4;2
302,302,302,302,302,302,302,302,ICLR,2017,BIOACOUSTIC SEGMENTATION BY HIERARCHICAL DIRICHLET PROCESS HIDDEN MARKOV MODEL,Vincent Roger;Marius Bartcus;Faicel Chamroukhi;Hervé Glotin,vincent-roger@etud.univ-tln.fr;marius.bartcus@gmail.com;faicel.chamroukhi@unicaen.fr;glotin@univ-tln.fr,5;4;5,3;3;5,Reject,2,0,0.0,no,11/4/16,CNRS university Toulon;;University of Caen Normandie;CNRS university Toulon,462;-1;-1;462,981;-1;-1;981,11;2
303,303,303,303,303,303,303,303,ICLR,2017,Tuning Recurrent Neural Networks with Reinforcement Learning,Natasha Jaques;Shixiang Gu;Richard E. Turner;Douglas Eck,jaquesn@mit.edu;sg717@cam.ac.uk;ret26@cam.ack.uk;deck@google.com,5;6;5,5;3;5,Invite to Workshop Track,7,5,1.0,no,11/4/16,Massachusetts Institute of Technology;University of Cambridge;University of Cambridge;Google,2;67;67;-1,5;4;4;-1,
304,304,304,304,304,304,304,304,ICLR,2017,An Analysis of Feature Regularization for Low-shot Learning,Zhuoyuan Chen;Han Zhao;Xiao Liu;Wei Xu,chenzhuoyuan@baidu.com;liuxiao12@baidu.com;wei.xu@baidu.com;han.zhao@cs.cmu.edu,5;6;6,4;3;3,Reject,3,4,0.0,no,11/4/16,Baidu;Baidu;Baidu;Carnegie Mellon University,-1;-1;-1;1,-1;-1;-1;23,
305,305,305,305,305,305,305,305,ICLR,2017,Gradients of Counterfactuals,Mukund Sundararajan;Ankur Taly;Qiqi Yan,mukunds@google.com;ataly@google.com;qiqiyan@google.com,3;3;5,4;4;4,Reject,0,4,0.0,no,11/4/16,Google;Google;Google,-1;-1;-1,-1;-1;-1,3
306,306,306,306,306,306,306,306,ICLR,2017,Encoding and Decoding Representations with Sum- and Max-Product Networks,Antonio Vergari;Robert Peharz;Nicola Di Mauro;Floriana Esposito,antonio.vergari@uniba.it;robert.peharz@medunigraz.at;nicola.dimauro@uniba.it;floriana.esposito@uniba.it,6;3;6,4;3;4,Reject,3,8,0.0,no,11/4/16,University of Bari;Medical University of Graz;University of Bari;University of Bari,194;462;194;194,110;452;110;110,5;1
307,307,307,307,307,307,307,307,ICLR,2017,Skip-graph: Learning graph embeddings with an encoder-decoder model,John Boaz Lee;Xiangnan Kong,jtlee@wpi.edu;xkong@wpi.edu,5;6;7,4;1;3,Reject,3,5,0.0,no,11/4/16,Worcester Polytechnic Institute;Worcester Polytechnic Institute,194;194,981;981,3;10
308,308,308,308,308,308,308,308,ICLR,2017,Information Dropout: learning optimal representations through noise,Alessandro Achille;Stefano Soatto,achille@cs.ucla.edu;soatto@cs.ucla.edu,4;6;6,4;4;4,Reject,2,3,0.0,no,11/4/16,"University of California, Los Angeles;University of California, Los Angeles",20;20,14;14,5;8
309,309,309,309,309,309,309,309,ICLR,2017,Deep unsupervised learning through spatial contrasting,Elad Hoffer;Itay Hubara;Nir Ailon,ehoffer@tx.technion.ac.il;itayh@tx.technion.ac.il;nailon@cs.technion.ac.il,5;6;7,4;4;4,Reject,3,3,0.0,no,10/19/16,Technion;Technion;Technion,24;24;24,301;301;301,
310,310,310,310,310,310,310,310,ICLR,2017,Dynamic Partition Models,Marc Goessling;Yali Amit,goessling@uchicago.edu,3;6;3,4;3;4,Reject,2,4,0.0,no,11/2/16,University of Chicago,45,10,
311,311,311,311,311,311,311,311,ICLR,2017,End-to-End Answer Chunk Extraction and Ranking for Reading Comprehension,Yang Yu;Wei Zhang;Bowen Zhou;Kazi Hasan;Mo Yu;Bing Xiang,yu@us.ibm.com;zhangwei@us.ibm.com;zhou@us.ibm.com;kshasan@us.ibm.com;yum@us.ibm.com;bingxia@us.ibm.com,4;5;6,3;3;3,Reject,3,1,0.0,no,11/4/16,International Business Machines;International Business Machines;International Business Machines;International Business Machines;International Business Machines;International Business Machines,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,
312,312,312,312,312,312,312,312,ICLR,2017,#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning,Haoran Tang;Rein Houthooft;Davis Foote;Adam Stooke;Xi Chen;Yan Duan;John Schulman;Filip De Turck;Pieter Abbeel,hrtang.alex@berkeley.edu;rein.houthooft@ugent.be;djfoote@berkeley.edu;adam.stooke@berkeley.edu;peter@openai.com;rocky@openai.com;joschu@openai.com;filip.deturck@ugent.be;pieter@openai.com,6;4;7,4;3;4,Reject,4,4,0.0,no,11/5/16,University of California Berkeley;Ghent University;University of California Berkeley;University of California Berkeley;OpenAI;OpenAI;OpenAI;Ghent University;OpenAI,5;462;5;5;-1;-1;-1;462;-1,10;118;10;10;-1;-1;-1;118;-1,8
313,313,313,313,313,313,313,313,ICLR,2017,Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models,Ashwin K Vijayakumar;Michael Cogswell;Ramprasaath R. Selvaraju;Qing Sun;Stefan Lee;David Crandall;Dhruv Batra,ashwinkv@vt.edu;cogswell@vt.edu;ram21@vt.edu;sunqing@vt.edu;steflee@vt.edu;djcran@indiana.edu;dbatra@vt.edu,6;6;4,4;4;4,Reject,2,5,0.0,no,11/4/16,Virginia Tech;Virginia Tech;Virginia Tech;Virginia Tech;Virginia Tech;University of Arizona;Virginia Tech,80;80;80;80;80;161;80,286;286;286;286;286;156;286,3
314,314,314,314,314,314,314,314,ICLR,2017,Extrapolation and learning equations,Georg Martius;Christoph H. Lampert,gmartius@ist.ac.at;chl@ist.ac.at,7;3;6,4;4;3,Invite to Workshop Track,3,4,0.0,no,11/2/16,Institute of Science and Technology Austria;Institute of Science and Technology Austria,94;94,981;981,
315,315,315,315,315,315,315,315,ICLR,2017,Learning Python Code Suggestion with a Sparse Pointer Network,Avishkar Bhoopchand;Tim Rocktäschel;Earl Barr;Sebastian Riedel,avishkar.bhoopchand.15@ucl.ac.uk;t.rocktaschel@cs.ucl.ac.uk;e.barr@cs.ucl.ac.uk;s.riedel@cs.ucl.ac.uk,6;6;5,4;4;4,Reject,4,5,0.0,no,11/3/16,University College London;University College London;University College London;University College London,45;45;45;45,15;15;15;15,3
316,316,316,316,316,316,316,316,ICLR,2017,Evaluation of Defensive Methods for DNNs against Multiple Adversarial Evasion Models,Xinyun Chen;Bo Li;Yevgeniy Vorobeychik,jungyhuk@gmail.com;bbbli@umich.edu;yevgeniy.vorobeychik@vanderbilt.edu,5;5;4,4;3;3,Reject,4,4,0.0,no,11/4/16,Shanghai Jiao Tong University;University of Michigan;Vanderbilt University,61;8;230,214;21;108,4;8
317,317,317,317,317,317,317,317,ICLR,2017,Representation Stability as a Regularizer for Improved Text Analytics Transfer Learning,Matthew Riemer;Elham Khabiri;Richard Goodwin,mdriemer@us.ibm.com;ekhabiri@us.ibm.com;rgoodwin@us.ibm.com,5;7;6,4;4;3,Reject,2,3,0.0,no,11/4/16,International Business Machines;International Business Machines;International Business Machines,-1;-1;-1,-1;-1;-1,6;8
318,318,318,318,318,318,318,318,ICLR,2017,Exploring LOTS in Deep Neural Networks,Andras Rozsa;Manuel Gunther;Terrance E. Boult,andras.rozsa@yahoo.com;siebenkopf@googlemail.com;tboult@vast.uccs.edu,6;6;6,4;4;4,Reject,3,7,2.0,no,11/4/16,"University of Colorado, Colorado Springs;University of Colorado, Colorado Springs;University of Colorado, Colorado Springs",462;462;462,981;981;981,4
319,319,319,319,319,319,319,319,ICLR,2017,Development of JavaScript-based deep learning platform and application to distributed training,Masatoshi Hidaka;Ken Miura;Tatsuya Harada,hidaka@mi.t.u-tokyo.ac.jp;miura@mi.t.u-tokyo.ac.jp;harada@mi.t.u-tokyo.ac.jp,6;4;7,4;2;3,Invite to Workshop Track,2,3,2.0,no,11/4/16,The University of Tokyo;The University of Tokyo;The University of Tokyo,50;50;50,39;39;39,
320,320,320,320,320,320,320,320,ICLR,2017,Annealing Gaussian into ReLU: a New Sampling Strategy for Leaky-ReLU RBM,Chun-Liang Li;Siamak Ravanbakhsh;Barnabas Poczos,chunlial@cs.cmu.edu;mravanba@cs.cmu.edu;bapoczos@cs.cmu.edu,5;5;6,5;4;4,Reject,3,5,0.0,no,11/4/16,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1,23;23;23,5;10
321,321,321,321,321,321,321,321,ICLR,2017,Out-of-class novelty generation: an experimental foundation,Mehdi Cherti;Balázs Kégl;Akın Kazakçı,mehdicherti@gmail.com;balazskegl@gmail.com;akin.kazakci@mines-paristech.fr,7;6;4;5,4;3;4;3,Reject,3,4,0.0,no,11/5/16,Linear accelerator Laboratory;;Mines ParisTech,-1;-1;462,-1;-1;265,5
322,322,322,322,322,322,322,322,ICLR,2017,End-to-End Learnable Histogram Filters,Rico Jonschkowski;Oliver Brock,rico.jonschkowski@tu-berlin.de;oliver.brock@tu-berlin.de,4;4;3,3;3;3,Reject,3,4,0.0,no,11/5/16,TU Berlin;TU Berlin,105;105,82;82,
323,323,323,323,323,323,323,323,ICLR,2017,Character-aware Attention Residual Network for Sentence Representation,Xin Zheng;Zhenzhou Wu,xzheng008@e.ntu.edu.sg;zhenzhou.wu@sap.com,4;4;4,4;4;5,Reject,3,3,0.0,no,11/5/16,National Taiwan University;SAP,87;351,195;257,
324,324,324,324,324,324,324,324,ICLR,2017,ReasoNet: Learning to Stop Reading in Machine Comprehension,Yelong Shen;Po-Sen Huang;Jianfeng Gao;Weizhu Chen,yeshen@microsoft.com;pshuang@microsoft.com;jfgao@microsoft.com;wzchen@microsoft.com,5;5;6,3;3;5,Reject,4,2,0.0,no,11/4/16,Microsoft;Microsoft;Microsoft;Microsoft,-1;-1;-1;-1,-1;-1;-1;-1,10
325,325,325,325,325,325,325,325,ICLR,2017,Implicit ReasoNet: Modeling Large-Scale Structured Relationships with Shared Memory,Yelong Shen*;Po-Sen Huang*;Ming-Wei Chang;Jianfeng Gao,yeshen@microsoft.com;pshuang@microsoft.com;minchang@microsoft.com;jfgao@microsoft.com,6;6;6,4;4;4,Reject,6,4,0.0,no,11/4/16,Microsoft;Microsoft;Microsoft;Microsoft,-1;-1;-1;-1,-1;-1;-1;-1,
326,326,326,326,326,326,326,326,ICLR,2017,Efficient iterative policy optimization,Nicolas Le Roux,nicolas@le-roux.name,3;7;5,2;3;4,Reject,1,3,0.0,no,11/4/16,Criteo,-1,-1,
327,327,327,327,327,327,327,327,ICLR,2017,A hybrid network: Scattering and Convnet,Edouard Oyallon,edouard.oyallon@ens.fr,7;5;7,4;4;3,Reject,5,4,0.0,no,11/4/16,Ecole Normale Superieure,94,66,
328,328,328,328,328,328,328,328,ICLR,2017,Cortical-Inspired Open-Bigram Representation for Handwritten Word Recognition,Théodore Bluche;Christopher Kermorvant;Claude Touzet;Hervé Glotin,tb@a2ia.com;kermorvant@teklia.com;claude.touzet@univ-amu.fr;glotin@univ-tln.fr,5;4;7,5;4;5,Reject,2,3,0.0,no,11/4/16,A2iA SAS;TEKLIA;Aix Marseille Univ;CNRS university Toulon,-1;-1;462;462,-1;-1;313;981,
329,329,329,329,329,329,329,329,ICLR,2017,Learning Locomotion Skills Using DeepRL: Does the Choice of Action Space Matter?,Xue Bin Peng;Michiel van de Panne,xbpeng@cs.ubc.ca;van@cs.ubc.ca,6;6;6,4;4;3,Reject,2,3,0.0,no,11/2/16,University of British Columbia;University of British Columbia,34;34,36;36,
330,330,330,330,330,330,330,330,ICLR,2017,Learning to Discover Sparse Graphical Models,Eugene Belilovsky;Kyle Kastner;Gael Varoquaux;Matthew B. Blaschko,eugene.belilovsky@inria.fr;kyle.kastner@umontreal.ca;gael.varoquaux@inria.fr;matthew.blaschko@esat.kuleuven.be,6;7;5,3;3;2,Invite to Workshop Track,0,4,0.0,no,11/4/16,INRIA;University of Montreal;INRIA;KU Leuven,-1;113;-1;105,-1;103;-1;40,10
331,331,331,331,331,331,331,331,ICLR,2017,Efficient Communications in Training Large Scale Neural Networks,Linnan Wang;Wei Wu;George Bosilca;Richard Vuduc;Zenglin Xu,linnan.wang@gatech.edu;wwu12@vols.utk.edu;bosilca@icl.utk.edu;richie@cc.gatech.edu;zlxu@uestc.edu.cn,5;5,3;5,Reject,5,6,0.0,no,11/2/16,"Georgia Institute of Technology;University of Tennessee, Knoxville;University of Tennessee, Knoxville;Georgia Institute of Technology;University of Electronic Science and Technology of China",12;161;161;12;462,33;286;286;33;933,
332,332,332,332,332,332,332,332,ICLR,2017,Semantic Noise Modeling for Better Representation Learning,Hyo-Eun Kim;Sangheum Hwang;Kyunghyun Cho,hekim@lunit.io;shwang@lunit.io;kyunghyun.cho@nyu.edu,4;3;2,4;4;4,Reject,1,7,0.0,no,11/2/16,Lunit Inc.;Lunit Inc.;New York University,-1;-1;25,-1;-1;32,8
333,333,333,333,333,333,333,333,ICLR,2017,Attentive Recurrent Comparators,Pranav Shyam;Ambedkar Dukkipati,pranavm.cs13@rvce.edu.in;ad@csa.iisc.ernet.in,4;5;3,5;2;5,Reject,0,0,0.0,no,11/5/16,"R V College of Engineering;Indian Institute of Science Bangalore., Indian institute of science, Bangalore",462;461,981;247,11;8
334,334,334,334,334,334,334,334,ICLR,2017,Combating Deep Reinforcement Learning's Sisyphean Curse with Intrinsic Fear,Zachary C. Lipton;Jianfeng Gao;Lihong Li;Jianshu Chen;Li Deng,zlipton@cs.ucsd.edu;jfgao@microsoft.com;lihongli.cs@gmail.com;jianshuc@microsoft.com;deng@microsoft.com,5;4;4,2;3;4,Reject,2,5,0.0,no,11/3/16,"University of California, San Diego;Microsoft;Microsoft;Microsoft;Microsoft",9;-1;-1;-1;-1,41;-1;-1;-1;-1,
335,335,335,335,335,335,335,335,ICLR,2017,Shift Aggregate Extract Networks,Francesco Orsini;Daniele Baracchi;Paolo Frasconi,francesco.orsini@kuleuven.be;daniele.baracchi@unifi.it;paolo.frasconi@unifi.it,5;5;3,3;3;2,Invite to Workshop Track,1,3,0.0,no,11/4/16,KU Leuven;University of Florence;University of Florence,105;462;462,40;439;439,10
336,336,336,336,336,336,336,336,ICLR,2017,Unsupervised Pretraining for Sequence to Sequence Learning,Prajit Ramachandran;Peter J. Liu;Quoc V. Le,prajitram@gmail.com;peterjliu@google.com;qvl@google.com,6;7;5,4;5;5,Reject,3,4,0.0,no,11/4/16,"University of Illinois, Urbana Champaign;Google;Google",4;-1;-1,36;-1;-1,3;8
337,337,337,337,337,337,337,337,ICLR,2017,Revisiting Batch Normalization For Practical Domain Adaptation,Yanghao Li;Naiyan Wang;Jianping Shi;Jiaying Liu;Xiaodi Hou,lyttonhao@pku.edu.cn;winsty@gmail.com;shijianping5000@gmail.com;liujiaying@pku.edu.cn;xiaodi.hou@gmail.com,4;6;5,4;3;4,Reject,1,7,1.0,no,11/4/16,Peking University;;;Peking University;TUSIMPLE LLC,25;-1;-1;25;-1,29;-1;-1;29;-1,2;8
338,338,338,338,338,338,338,338,ICLR,2017,A Neural Knowledge Language Model,Sungjin Ahn;Heeyoul Choi;Tanel Parnamaa;Yoshua Bengio,sjn.ahn@gmail.com;heeyoul@gmail.com;tanel.parnamaa@gmail.com;yoshua.bengio@umontreal.ca,6;6;6,4;4;3,Reject,2,5,0.0,no,11/3/16,University of Montreal;Handong Global University;;University of Montreal,113;462;-1;113,103;981;-1;103,3;10
339,339,339,339,339,339,339,339,ICLR,2017,Dynamic Neural Turing Machine with Continuous and Discrete Addressing Schemes,Caglar Gulcehre;Sarath Chandar;Kyunghyun Cho;Yoshua Bengio,gulcehrc@iro.umontreal.ca;apsarathchandar@gmail.com;kyunghyun.cho@nyu.edu;yoshua.umontreal@gmail.com,6;4;7,4;4;4,Reject,3,6,0.0,no,11/4/16,University of Montreal;University of Montreal;New York University;University of Montreal,113;113;25;113,103;103;32;103,
340,340,340,340,340,340,340,340,ICLR,2017,Gated-Attention Readers for Text Comprehension,Bhuwan Dhingra;Hanxiao Liu;Zhilin Yang;William W. Cohen;Ruslan Salakhutdinov,bdhingra@cs.cmu.edu;hanxiaol@cs.cmu.edu;zhiliny@cs.cmu.edu;wcohen@cs.cmu.edu;rsalakhu@cs.cmu.edu,6;6;7,3;3,Reject,15,5,0.0,no,11/4/16,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1;1,23;23;23;23;23,
341,341,341,341,341,341,341,341,ICLR,2017,Deep Character-Level Neural Machine Translation By Learning Morphology,Shenjian Zhao;Zhihua Zhang,sword.york@gmail.com;zhzhang@math.pku.edu.cn,5;6;7,5;4;4,Reject,2,9,0.0,no,11/4/16,Shanghai Jiao Tong University;Peking University,61;25,214;29,3
342,342,342,342,342,342,342,342,ICLR,2017,Automated Generation of Multilingual Clusters for the Evaluation of Distributed Representations,Philip Blair;Yuval Merhav;Joel Barry,pblair@basistech.com;yuval@basistech.com;joelb@basistech.com,5;8;6,3;4;3,Invite to Workshop Track,3,2,0.0,no,11/4/16,Northeastern University;Basistech;Basistech,18;-1;-1,778;-1;-1,3;10
343,343,343,343,343,343,343,343,ICLR,2017,Adding Gradient Noise Improves Learning for Very Deep Networks,Arvind Neelakantan;Luke Vilnis;Quoc V. Le;Lukasz Kaiser;Karol Kurach;Ilya Sutskever;James Martens,arvind@cs.umass.edu;luke@cs.umass.edu;qvl@google.com;lukaszkaiser@google.com;kkurach@google.com;ilyasu@openai.com;jmartens@cs.toronto.edu,4;4;7,5;4;5,Reject,1,4,0.0,no,11/4/16,"University of Massachusetts, Amherst;University of Massachusetts, Amherst;Google;Google;Google;OpenAI;Department of Computer Science, University of Toronto",25;25;-1;-1;-1;-1;15,166;166;-1;-1;-1;-1;22,
344,344,344,344,344,344,344,344,ICLR,2017,Knowledge Adaptation: Teaching to Adapt,Sebastian Ruder;Parsa Ghaffari;John G. Breslin,sebastian.ruder@insight-centre.org;parsa@aylien.com;john.breslin@insight-centre.org,6;7;5,4;3;4,Reject,2,6,0.0,no,11/3/16,Insight Centre for Data Analytics;Aylien;Insight Centre for Data Analytics,-1;-1;-1,-1;-1;-1,4
345,345,345,345,345,345,345,345,ICLR,2017,Neural Taylor Approximations: Convergence and Exploration in Rectifier Networks,David Balduzzi;Brian McWilliams;Tony Butler-Yeoman,david.balduzzi@vuw.ac.nz;brian@disneyresearch.com;butlertony@ecs.vuw.ac.nz,3;7;7,4;3;2,Invite to Workshop Track,1,3,0.0,no,11/4/16,"Victoria University Wellington;Disney Research, Disney;Victoria University Wellington",275;-1;275,362;-1;362,1;9
346,346,346,346,346,346,346,346,ICLR,2017,On the Expressive Power of Deep Neural Networks,Maithra Raghu;Ben Poole;Jon Kleinberg;Surya Ganguli;Jascha Sohl-Dickstein,maithrar@gmail.com;benmpoole@gmail.com;kleinber@cs.cornell.edu;sganguli@stanford.edu;jaschasd@google.com,3;6;5,3;5;3,Reject,4,5,0.0,no,11/4/16,Cornell University;Stanford University;Cornell University;Stanford University;Google,7;3;7;3;-1,19;3;19;3;-1,
347,347,347,347,347,347,347,347,ICLR,2017,The loss surface of residual networks: Ensembles and the role of batch normalization,Etai Littwin;Lior Wolf,etai.littwin@gmail.com;liorwolf@gmail.com,7;3;7,3;5;3,Reject,3,6,0.0,no,11/4/16,Tel Aviv University;Tel Aviv University,36;36,216;216,
348,348,348,348,348,348,348,348,ICLR,2017,Understanding trained CNNs by indexing neuron selectivity,Ivet Rafegas;Maria Vanrell;Luís A. Alexandre,ivet.rafegas@uab.cat;maria.vanrell@uab.cat;lfbaa@ubi.pt,7;3;7,4;5;3,Reject,3,4,0.0,no,11/4/16,Universitat Autonoma de Barcelona;Universitat Autonoma de Barcelona;Universidade da Beira Interior,-1;-1;-1,-1;-1;-1,
349,349,349,349,349,349,349,349,ICLR,2017,Riemannian Optimization for Skip-Gram Negative Sampling,Alexander Fonarev;Alexey Grinchuk;Gleb Gusev;Pavel Serdyukov;Ivan Oseledets,newo@newo.su;oleksii.hrinchuk@skolkovotech.ru;gleb57@yandex-team.ru;pavser@yandex-team.ru;ioseledets@skoltech.ru,4;5;6,4;3;3,Reject,0,4,0.0,no,11/4/16,Skolkovo Institute of Science and Technology;Skolkovo Institute of Science and Technology;Yandex;Yandex;Skolkovo Institute of Science and Technology,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,3
350,350,350,350,350,350,350,350,ICLR,2017,Exploring the Application of Deep Learning for Supervised Learning Problems,Jose Rozanec;Gilad Katz;Eui Chul Richard Shin;Dawn Song,jmrozanec@gmail.com;giladk@berkeley.edu;ricshin@berkeley.edu;dawnsong@eecs.berkeley.edu,4;3;5,3;5;4,Reject,2,2,0.0,no,11/5/16,;University of California Berkeley;University of California Berkeley;University of California Berkeley,-1;5;5;5,-1;10;10;10,6
351,351,351,351,351,351,351,351,ICLR,2017,A Way out of the Odyssey: Analyzing and Combining Recent Insights for LSTMs,Shayne Longpre;Sabeek Pradhan;Caiming Xiong;Richard Socher,slongpre@cs.stanford.edu;sabeekp@cs.stanford.edu;cxiong@salesforce.com;rsocher@salesforce.com,5;5;5,4;4;4,Reject,2,0,0.0,no,11/4/16,Stanford University;Stanford University;SalesForce.com;SalesForce.com,3;3;-1;-1,3;3;-1;-1,3
352,352,352,352,352,352,352,352,ICLR,2017,LipNet: End-to-End Sentence-level Lipreading,Yannis M. Assael;Brendan Shillingford;Shimon Whiteson;Nando de Freitas,yannis.assael@cs.ox.ac.uk;brendan.shillingford@cs.ox.ac.uk;shimon.whiteson@cs.ox.ac.uk;nando.de.freitas@cs.ox.ac.uk,4;6;4,4;3;4,Reject,6,14,1.0,no,11/4/16,University of Oxford;University of Oxford;University of Oxford;University of Oxford,50;50;50;50,1;1;1;1,
353,353,353,353,353,353,353,353,ICLR,2017,Distributed Transfer Learning for Deep Convolutional Neural Networks by Basic Probability Assignment,Arash Shahriari,arash.shahriari@csiro.au,3;3;4,3;4;4,Reject,2,3,0.0,no,11/4/16,CSIRO,-1,-1,6
354,354,354,354,354,354,354,354,ICLR,2017,DeepRebirth: A General Approach for Accelerating Deep Neural Network Execution on Mobile Devices,Dawei Li;Xiaolong Wang;Deguang Kong;Mooi Choo Chuah,dal312@lehigh.edu;visionxiaolong@gmail.com;doogkong@gmail.com,4;4;4,4;4;4,Reject,0,6,0.0,no,11/4/16,Lehigh University;Samsung;Yahoo,230;-1;-1,441;-1;-1,
355,355,355,355,355,355,355,355,ICLR,2017,Modelling Relational Time Series using Gaussian Embeddings,Ludovic Dos Santos;Ali Ziat;Ludovic Denoyer;Benjamin Piwowarski;Patrick Gallinari,ludovic.dossantos@lip6.fr;ali.ziat@vedecom.fr;ludovic.denoyer@lip6.fr;benjamin.piwowarski@lip6.fr;patrick.gallinari@lip6.fr,4;4;4,3;5;4,Reject,2,0,0.0,no,11/3/16,LIP6;;LIP6;LIP6;LIP6,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,
356,356,356,356,356,356,356,356,ICLR,2017,Higher Order Recurrent Neural Networks,Rohollah Soltani;Hui Jiang,rsoltani@cse.yorku.ca;hj@cse.yorku.ca,4;6;3,4;4;4,Reject,3,0,0.0,no,11/5/16,York University;York University,161;161,316;316,3
357,357,357,357,357,357,357,357,ICLR,2017,Multi-task learning with deep model based reinforcement learning,Asier Mujika,asierm@student.ethz.ch,2;4;4,5;4;4,Reject,1,3,0.0,no,11/4/16,Swiss Federal Institute of Technology,9,9,
358,358,358,358,358,358,358,358,ICLR,2017,Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units,Dan Hendrycks;Kevin Gimpel,dan@ttic.edu;kgimpel@ttic.edu,4;5;5,4;4;4,Reject,3,1,0.0,no,11/4/16,Toyota Technological Institute at Chicago;Toyota Technological Institute at Chicago,-1;-1,-1;-1,
359,359,359,359,359,359,359,359,ICLR,2017,Iterative Refinement for Machine Translation,Roman Novak;Michael Auli;David Grangier,roman.novak@polytechnique.edu;michaelauli@fb.com;grangier@fb.com,5;5;7;4,3;5;3;4,Reject,0,0,0.0,no,11/2/16,Ecole polytechnique;Facebook;Facebook,462;-1;-1,116;-1;-1,3
360,360,360,360,360,360,360,360,ICLR,2017,Machine Solver for Physics Word Problems,Megan Leszczynski;Jose Moreira,mel255@cornell.edu;jmoreira@us.ibm.com,4;4;5,4;4;4,Reject,1,4,0.0,no,11/4/16,Cornell University;International Business Machines,7;-1,19;-1,
361,361,361,361,361,361,361,361,ICLR,2017,b-GAN: Unified Framework of Generative Adversarial Networks,Masatosi Uehara;Issei Sato;Masahiro Suzuki;Kotaro Nakayama;Yutaka Matsuo,uehara-masatoshi136@g.ecc.u-tokyo.ac.jp;sato@k.u-tokyo.ac.jp;masa@weblab.t.u-tokyo.ac.jp;nakayama@weblab.t.u-tokyo.ac.jp;matsuo@weblab.t.u-tokyo.ac.jp,5;6;4,3;4;3,Reject,0,4,0.0,no,11/5/16,The University of Tokyo;The University of Tokyo;The University of Tokyo;The University of Tokyo;The University of Tokyo,50;50;50;50;50,39;39;39;39;39,5;4
362,362,362,362,362,362,362,362,ICLR,2017,Structured Sequence Modeling with Graph Convolutional Recurrent Networks,Youngjoo Seo;Michaël Defferrard;Pierre Vandergheynst;Xavier Bresson,youngjoo.seo@epfl.ch;michael.defferrard@epfl.ch;pierre.vandergheynst@epfl.ch;xavier.bresson@gmail.com,4;4;4,4;4;4,Reject,3,3,0.0,no,11/4/16,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;National Taiwan University,25;25;25;87,30;30;30;195,3;10;8
363,363,363,363,363,363,363,363,ICLR,2017,Revisiting Denoising Auto-Encoders,Luis Gonzalo Sanchez Giraldo,lgsanchez@cs.miami.edu,4;4;5,5;4;4,Reject,4,0,0.0,no,11/5/16,University of Miami,351,183,
364,364,364,364,364,364,364,364,ICLR,2017,Revisiting Distributed Synchronous SGD,Jianmin Chen*;Xinghao Pan*;Rajat Monga;Samy Bengio;Rafal Jozefowicz,jmchen@google.com;xinghao@google.com;rajatmonga@google.com;bengio@google.com;rafal@openai.com,6;6;5,4;4;3,Reject,9,3,0.0,no,11/1/16,Google;Google;Google;Google;OpenAI,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,
365,365,365,365,365,365,365,365,ICLR,2017,Here's My Point: Argumentation Mining with Pointer Networks,Peter Potash;Alexey Romanov;Anna Rumshisky,ppotash@cs.uml.edu;aromanov@cs.uml.edu;arum@cs.uml.edu,4;5;5,3;4;4,Reject,2,5,0.0,no,11/4/16,"University of Massachusetts, Lowell;University of Massachusetts, Lowell;University of Massachusetts, Lowell",-1;-1;-1,-1;-1;-1,
366,366,366,366,366,366,366,366,ICLR,2017,DyVEDeep: Dynamic Variable Effort Deep Neural Networks,Sanjay Ganapathy;Swagath Venkataramani;Balaraman Ravindran;Anand Raghunathan,sanjaygana@gmail.com;venkata0@purdue.edu;ravi@cse.iitm.ac.in;raghunathan@purdue.edu,6;7;6,3;3;4,Reject,1,3,0.0,no,11/4/16,;Purdue University;Indian Institute of Technology Madras;Purdue University,-1;25;140;25,-1;70;472;70,
367,367,367,367,367,367,367,367,ICLR,2017,Improved Architectures for Computer Go,Tristan Cazenave,Tristan.Cazenave@dauphine.fr,3;7;4;3,4;5;4;4,Reject,2,0,0.0,no,11/3/16,Univeristé Paris-Dauphine,462,981,
368,368,368,368,368,368,368,368,ICLR,2017,Deep Variational Canonical Correlation Analysis,Weiran Wang;Xinchen Yan;Honglak Lee;Karen Livescu,weiranwang@ttic.edu;xcyan@umich.edu;honglak@umich.edu;klivescu@ttic.edu,5;5;7,4;4;4,Reject,3,4,0.0,no,11/4/16,Toyota Technological Institute at Chicago;University of Michigan;University of Michigan;Toyota Technological Institute at Chicago,-1;8;8;-1,-1;21;21;-1,5;1
369,369,369,369,369,369,369,369,ICLR,2017,Charged Point Normalization: An Efficient Solution to the Saddle Point Problem,Armen Aghajanyan,armen.ag@live.com,5;4;4,4;4;3,Invite to Workshop Track,6,3,0.0,no,10/18/16,Dimensional Mechanics,-1,-1,9
370,370,370,370,370,370,370,370,ICLR,2017,Multi-label learning with semantic embeddings,Liping Jing;MiaoMiao Cheng;Liu Yang;Alex Gittens;Michael W. Mahoney,lpjing@bjtu.edu.cn;15112085@bjtu.edu.cn;11112191@bjtu.edu.cn;gittens@icsi.berkeley.edu;mmahoney@stat.berkeley.edu,4;4;5,4;4;4,Reject,3,0,0.0,no,10/31/16,Beijing jiaotong univercity;Beijing jiaotong univercity;Beijing jiaotong univercity;University of California Berkeley;University of California Berkeley,462;462;462;5;5,981;981;981;10;10,
371,371,371,371,371,371,371,371,ICLR,2017,Emergent Predication Structure in Vector Representations of Neural Readers,Hai Wang;Takeshi Onishi;Kevin Gimpel;David McAllester,haiwang@ttic.edu;tonishi@ttic.edu;kgimpel@ttic.edu;mcallester@ttic.edu,6;6;5,5;4;3,Reject,3,2,0.0,no,11/4/16,Toyota Technological Institute at Chicago;Toyota Technological Institute at Chicago;Toyota Technological Institute at Chicago;Toyota Technological Institute at Chicago,-1;-1;-1;-1,-1;-1;-1;-1,
372,372,372,372,372,372,372,372,ICLR,2017,An Actor-critic Algorithm for Learning Rate Learning,Chang Xu;Tao Qin;Gang Wang;Tie-Yan Liu,changxu@nbjl.nankai.edu.cn;taoqin@microsoft.com;wgzwp@nbjl.nankai.edu.cn;tie-yan.liu@microsoft.com,3;5;4,5;4;4,Reject,3,0,0.0,no,11/3/16,Nankai University;Microsoft;Nankai University;Microsoft,462;-1;462;-1,905;-1;905;-1,
373,373,373,373,373,373,373,373,ICLR,2017,Joint Multimodal Learning with Deep Generative Models,Masahiro Suzuki;Kotaro Nakayama;Yutaka Matsuo,masa@weblab.t.u-tokyo.ac.jp;k-nakayama@weblab.t.u-tokyo.ac.jp;matsuo@weblab.t.u-tokyo.ac.jp,5;3;5,5;4;3,Reject,4,0,0.0,no,11/4/16,The University of Tokyo;The University of Tokyo;The University of Tokyo,50;50;50,39;39;39,5
374,374,374,374,374,374,374,374,ICLR,2017,Energy-Based Spherical Sparse Coding,Bailey Kong;Charless C. Fowlkes,bhkong@ics.uci.edu;fowlkes@ics.uci.edu,5;5;6,4;4;4,Reject,3,3,0.0,no,11/5/16,"University of California, Irvine;University of California, Irvine",36;36,99;99,
375,375,375,375,375,375,375,375,ICLR,2017,Improving Invariance and Equivariance Properties of Convolutional Neural Networks,Christopher Tensmeyer;Tony Martinez,tensmeyer@byu.edu;martinez@cs.byu.edu,4;5;4,4;3;5,Reject,3,1,0.0,no,11/5/16,Brigham Young University;Brigham Young University,-1;-1,-1;-1,
376,376,376,376,376,376,376,376,ICLR,2017,Efficient Softmax Approximation for GPUs,Édouard Grave;Armand Joulin;Moustapha Cissé;David Grangier;Hervé Jégou,egrave@fb.com;ajoulin@fb.com;moustaphacisse@fb.com;grangier@fb.com;rvj@fb.com,7;6;7,3;5;4,Invite to Workshop Track,1,5,0.0,no,11/4/16,Facebook;Facebook;Facebook;Facebook;Facebook,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,3;10
377,377,377,377,377,377,377,377,ICLR,2017,The Preimage of Rectifier Network Activities,Stefan Carlsson;Hossein Azizpour;Ali Razavian,stefanc@kth.se;azizpour@kth.se;razavian@kth.se,4;4;4,4;5,Reject,2,1,0.0,no,11/4/16,"KTH Royal Institute of Technology, Stockholm, Sweden;KTH Royal Institute of Technology, Stockholm, Sweden;KTH Royal Institute of Technology, Stockholm, Sweden",129;129;129,160;160;160,
378,378,378,378,378,378,378,378,ICLR,2017,Deep Convolutional Neural Network Design Patterns,Leslie N. Smith;Nicholay Topin,leslie.smith@nrl.navy.mil;ntopin@umbc.edu,3;3;4,4;4;3,Reject,2,1,0.0,no,11/4/16,US Naval Research Laboratory;Boston College,-1;275,-1;212,
379,379,379,379,379,379,379,379,ICLR,2017,OMG: Orthogonal Method of Grouping With Application of K-Shot Learning,Haoqi Fan;Yu Zhang;Kris M. Kitani,haoqif@andrew.cmu.edu;kkitani@cs.cmu.edu,4;4;4,4;4;5,Reject,2,0,0.0,no,11/4/16,Carnegie Mellon University;Carnegie Mellon University,1;1,23;23,
380,380,380,380,380,380,380,380,ICLR,2017,Spatio-Temporal Abstractions in Reinforcement Learning Through Neural Encoding,Nir Baram;Tom Zahavy;Shie Mannor,nirb@campus.technion.ac.il;tomzahavy@campus.technion.ac.il;shie@ee.technion.ac.il,4;4;4,5;5;4,Reject,2,1,0.0,no,11/4/16,Technion;Technion;Technion,24;24;24,301;301;301,
381,381,381,381,381,381,381,381,ICLR,2017,Making Stochastic Neural Networks from Deterministic Ones,Kimin Lee;Jaehyung Kim;Song Chong;Jinwoo Shin,kiminlee@kaist.ac.kr;jaehyungkim@kaist.ac.kr;songchong@kaist.edu;jinwoos@kaist.ac.kr,5;6,5;4,Reject,7,3,0.0,no,11/4/16,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;KAIST;Korea Advanced Institute of Science and Technology,462;462;23;462,88;88;88;88,
382,382,382,382,382,382,382,382,ICLR,2017,Chess Game Concepts Emerge under Weak Supervision: A Case Study of Tic-tac-toe,Hao Zhao;Ming Lu;Anbang Yao;Yurong Chen;Li Zhang,zhao-h13@mails.tsinghua.edu.cn;lu-m13@mails.tsinghua.edu.cn;anbang.yao@intel.com;yurong.chen@intel.com;chinazhangli@mail.tsinghua.edu.cn,3;3;3,5;3;2,Reject,3,3,0.0,no,11/4/16,Tsinghua University;Tsinghua University;Intel;Intel;Tsinghua University,11;11;-1;-1;11,35;35;-1;-1;35,
383,383,383,383,383,383,383,383,ICLR,2017,Generative Adversarial Parallelization,Daniel Jiwoong Im;He Ma;Chris Dongjoo Kim;Graham Taylor,daniel.im@aifounded.com;hma02@uoguelph.ca;ckim07@uoguelph.ca;gwtaylor@uoguelph.ca,4;4;4,3;4;4,Reject,2,4,0.0,no,11/5/16,Aifounded;University of Guelph;University of Guelph;University of Guelph,-1;275;275;275,-1;398;398;398,5;4;1
384,384,384,384,384,384,384,384,ICLR,2017,Investigating Recurrence and Eligibility Traces in Deep Q-Networks,Jean Harb;Doina Precup,jharb@cs.mcgill.ca;dprecup@cs.mcgill.ca,4;4;3,4;5;5,Reject,3,0,0.0,no,11/5/16,McGill University;McGill University,80;80,42;42,
385,385,385,385,385,385,385,385,ICLR,2017,Discovering objects and their relations from entangled scene representations,David Raposo;Adam Santoro;David Barrett;Razvan Pascanu;Timothy Lillicrap;Peter Battaglia,draposo@google.com;adamsantoro@google.com;barrettdavid@google.com;razp@google.com;countzero@google.com;peterbattaglia@google.com,7;3;7,4;5;4,Invite to Workshop Track,0,5,0.0,no,11/4/16,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,5
386,386,386,386,386,386,386,386,ICLR,2017,Generalizable Features From Unsupervised Learning,Mehdi Mirza;Aaron Courville;Yoshua Bengio,memirzamo@gmail.com;aaron.courville@gmail.com;yoshua.umontreal@gmail.com,5;5;3,4;4;4,Invite to Workshop Track,2,0,0.0,no,11/4/16,Google;University of Montreal;University of Montreal,-1;113;113,-1;103;103,8
387,387,387,387,387,387,387,387,ICLR,2017,RL^2: Fast Reinforcement Learning via Slow Reinforcement Learning,Yan Duan;John Schulman;Xi Chen;Peter L. Bartlett;Ilya Sutskever;Pieter Abbeel,rocky@openai.com;joschu@openai.com;peter@openai.com;peter@berkeley.edu;ilyasu@openai.com;pieter@openai.com,4;3;3,3;4;4,Reject,2,3,4.0,no,11/4/16,OpenAI;OpenAI;OpenAI;University of California Berkeley;OpenAI;OpenAI,-1;-1;-1;5;-1;-1,-1;-1;-1;10;-1;-1,
388,388,388,388,388,388,388,388,ICLR,2017,Opening the vocabulary of  neural language models with character-level word representations,Matthieu Labeau;Alexandre Allauzen,labeau@limsi.fr;allauzen@limsi.fr,3;2;4,4;5;4,Reject,8,0,0.0,no,11/4/16,LIMSI-CNRS / Université Paris-Sud;LIMSI-CNRS / Université Paris-Sud,462;462,180;180,3
389,389,389,389,389,389,389,389,ICLR,2017,Multi-modal Variational Encoder-Decoders,Iulian V. Serban;Alexander G. Ororbia II;Joelle Pineau;Aaron Courville,julianserban@gmail.com;ago109@psu.edu;jpineau@cs.mcgill.ca;aaron.courville@umontreal.ca,3;4;4,4;5;4,Reject,14,5,0.0,no,11/4/16,University College London;Pennsylvania State University;McGill University;University of Montreal,45;39;80;113,15;68;42;103,3;5;10
390,390,390,390,390,390,390,390,ICLR,2017,Transformational Sparse Coding,Dimitrios C. Gklezakos;Rajesh P. N. Rao,gklezd@cs.washington.edu;rao@cs.washington.edu,5;4;4,4;4;4,Reject,7,3,0.0,no,11/4/16,University of Washington;University of Washington,6;6,25;25,
391,391,391,391,391,391,391,391,ICLR,2017,Convolutional Neural Networks Generalization Utilizing the Data Graph Structure,Yotam Hechtlinger;Purvasha Chakravarti;Jining Qin,yhechtli@andrew.cmu.edu;pchakrav@andrew.cmu.edu;jiningq@andrew.cmu.edu,6;3;3,3;3,Reject,2,3,0.0,no,11/4/16,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1,23;23;23,10;8
392,392,392,392,392,392,392,392,ICLR,2017,Learning Approximate Distribution-Sensitive Data Structures,Zenna Tavares;Armando Solar-Lezama,zenna@mit.edu;asolar@csail.mit.edu,4;4;3,3;3;4,Reject,1,0,0.0,no,11/4/16,Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2,5;5,
393,393,393,393,393,393,393,393,ICLR,2017,Sampling Generative Networks,Tom White,tom.white@vuw.ac.nz,5;5;6,4;3;3,Reject,6,3,0.0,no,11/2/16,Victoria University Wellington,275,362,5;4
394,394,394,394,394,394,394,394,ICLR,2017,Collaborative Deep Embedding via Dual Networks,Yilei Xiong;Dahua Lin;Haoying Niu;JIefeng Cheng;Zhenguo Li,xy014@ie.cuhk.edu.hk;dhlin@ie.cuhk.edu.hk;niu.haoying@huawei.com;cheng.jiefeng@huawei.com;li.zhenguo@huawei.com,5;5;4,3;4;4,Reject,2,3,0.0,no,11/4/16,The Chinese University of Hong Kong;The Chinese University of Hong Kong;Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.,61;61;-1;-1;-1,43;43;-1;-1;-1,
395,395,395,395,395,395,395,395,ICLR,2017,An Empirical Analysis of Deep Network Loss Surfaces,Daniel Jiwoong Im;Michael Tao;Kristin Branson,daniel.im@aifounded.com;mtao@dgp.toronto.edu;bransonk@janelia.hhmi.org,6;4;4,4;4;4,Reject,3,0,0.0,no,11/5/16,Aifounded;University of Toronto;HHMI Janelia Research Campus,-1;15;-1,-1;22;-1,
396,396,396,396,396,396,396,396,ICLR,2017,Parametric Exponential Linear Unit for Deep Convolutional Neural Networks,Ludovic Trottier;Philippe Giguère;Brahim Chaib-draa,ludovic.trottier.1@ulaval.ca;philippe.giguere@ift.ulaval.ca;brahim.chaib-draa@ift.ulaval.ca,5;7;4;6,4;5;4;4,Reject,2,1,0.0,no,11/4/16,Laval university;Laval university;Laval university,462;462;462,265;265;265,
397,397,397,397,397,397,397,397,ICLR,2017,CAN AI GENERATE LOVE ADVICE?: TOWARD NEURAL ANSWER GENERATION FOR NON-FACTOID QUESTIONS,Makoto Nakatsuji;Hisashi Ito;Naruhiro Ikeda;Shota Sagara;Akihisa Fujita,nakatuji@nttr.co.jp;h-ito@nttr.co.jp;nikeda@nttr.co.jp;s-sagara@nttr.co.jp;akihisa@nttr.co.jp,4;4;4,4;4,Reject,1,0,0.0,no,11/5/16,;;;;,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,3
398,398,398,398,398,398,398,398,ICLR,2017,Multiagent System for Layer Free Network,Hiroki Kurotaki;Kotaro Nakayama;Yutaka Matsuo,kurotaki@weblab.t.u-tokyo.ac.jp;nakayama@weblab.t.u-tokyo.ac.jp;matsuo@weblab.t.u-tokyo.ac.jp,3;1;2,3;5;5,Reject,2,0,0.0,no,11/4/16,The University of Tokyo;The University of Tokyo;The University of Tokyo,50;50;50,39;39;39,
399,399,399,399,399,399,399,399,ICLR,2017,Boosted Residual Networks,Alan Mosca;George D. Magoulas,a.mosca@dcs.bbk.ac.uk;gmagoulas@dcs.bbk.ac.uk,3;4;3,5;5;5,Reject,2,0,0.0,no,11/4/16,Birkbeck;Birkbeck,230;230,251;251,
400,400,400,400,400,400,400,400,ICLR,2017,Multi-view Generative Adversarial Networks,Mickaël Chen;Ludovic Denoyer,mickael.chen@lip6.fr;ludovic.denoyer@lip6.fr,3;5;6,3;3;4,Reject,2,0,0.0,no,11/4/16,LIP6;LIP6,-1;-1,-1;-1,
401,401,401,401,401,401,401,401,ICLR,2017,Modular Multitask Reinforcement Learning with Policy Sketches,Jacob Andreas;Dan Klein;Sergey Levine,jda@cs.berkeley.edu;klein@cs.berkeley.edu;svlevine@eecs.berkeley.edu,4;5;3,5;5;4,Invite to Workshop Track,2,5,0.0,no,11/4/16,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,10;10;10,
402,402,402,402,402,402,402,402,ICLR,2017,Prototypical Networks for Few-shot Learning,Jake Snell;Kevin Swersky;Richard Zemel,jsnell@cs.toronto.edu;kswersky@twitter.com;zemel@cs.toronto.edu,5;6;4,3;4;5,Reject,1,3,0.0,no,11/5/16,"Department of Computer Science, University of Toronto;Twitter;Department of Computer Science, University of Toronto",15;-1;15,22;-1;22,6
403,403,403,403,403,403,403,403,ICLR,2017,Towards Information-Seeking Agents,Philip Bachman;Alessandro Sordoni;Adam Trischler,phil.bachman@maluuba.com;alessandro.sordoni@maluuba.com;adam.trischler@maluuba.com,6;4;4,4;4;4,Reject,1,1,0.0,no,11/5/16,Maluuba;Maluuba;Maluuba,-1;-1;-1,-1;-1;-1,
404,404,404,404,404,404,404,404,ICLR,2017,Beyond Bilingual: Multi-sense Word Embeddings using Multilingual Context,Shyam Upadhyay;Kai-Wei Chang;James Zou;Matt Taddy;Adam Kalai,upadhya3@illinois.edu;kwchang@virginia.edu;jamesz@stanford.edu;taddy@microsoft.com;adum@microsoft.com,4;4;5,3;4;3,Reject,2,3,0.0,no,11/4/16,"University of Illinois, Urbana Champaign;University of Virginia;Stanford University;Microsoft;Microsoft",4;67;3;-1;-1,36;123;3;-1;-1,3;11
405,405,405,405,405,405,405,405,ICLR,2017,A Context-aware Attention Network for Interactive Question Answering,Huayu Li;Martin Renqiang Min;Yong Ge;Asim Kadav,hli38@uncc.edu;renqiang@nec-labs.com;yongge@email.arizona.edu;asim@nec-labs.com,5;4;4,3;4;4,Reject,3,3,0.0,no,11/4/16,"University of North Carolina, Charlotte;NEC-Labs;University of Arizona;NEC-Labs",73;-1;161;-1,981;-1;156;-1,
406,406,406,406,406,406,406,406,ICLR,2017,Sequence to Sequence Transduction with Hard Monotonic Attention,Roee Aharoni;Yoav Goldberg,roee.aharoni@gmail.com;yoav.goldberg@gmail.com,5;5;4,4;3;5,Reject,5,0,0.0,no,11/4/16,Bar Ilan University;Bar-Ilan University,87;87,489;489,
407,407,407,407,407,407,407,407,ICLR,2017,SoftTarget Regularization: An Effective Technique to Reduce Over-Fitting in Neural Networks,Armen Aghajanyan,armen.ag@live.com,3;4;4,5;5;5,Reject,7,2,0.0,no,10/18/16,Dimensional Mechanics,-1,-1,
408,408,408,408,408,408,408,408,ICLR,2017,Parallel Stochastic Gradient Descent with Sound Combiners,Saeed Maleki;Madanlal Musuvathi;Todd Mytkowicz;Yufei Ding,saemal@microsoft.com;madanm@microsoft.com;toddm@microsoft.com;yding8@ncsu.edu,4;6;4,5;4;5,Reject,3,3,0.0,no,11/4/16,Microsoft;Microsoft;Microsoft;North Carolina State University,-1;-1;-1;87,-1;-1;-1;249,9
409,409,409,409,409,409,409,409,ICLR,2017,Inefficiency of stochastic gradient descent with larger mini-batches (and more learners),Onkar Bhardwaj;Guojing Cong,onkar.bhardwaj@gmail.com;gcong@us.ibm.com,6;4;5,4;4;3,Reject,4,3,0.0,no,11/4/16,International Business Machines;International Business Machines,-1;-1,-1;-1,
410,410,410,410,410,410,410,410,ICLR,2017,The Power of Sparsity in Convolutional Neural Networks,Soravit Changpinyo;Mark Sandler;Andrey Zhmoginov,schangpi@usc.edu;sandler@google.com;azhmogin@google.com,7;5;4,3;4;4,Reject,6,3,0.0,no,11/4/16,University of Southern California;Google;Google,31;-1;-1,60;-1;-1,
411,411,411,411,411,411,411,411,ICLR,2017,The Variational Walkback Algorithm,Anirudh Goyal;Nan Rosemary Ke;Alex Lamb;Yoshua Bengio,anirudhgoyal9119@gmail.com;rosemary.nan.ke@gmail.com;lambalex@iro.umontreal.ca;yoshua.umontreal@gmail.com,4;5;4,4;5;5,Reject,2,1,0.0,no,11/4/16,University of Montreal;Polytechnique Montreal;University of Montreal;University of Montreal,113;351;113;113,103;981;103;103,5;1;10
412,412,412,412,412,412,412,412,ICLR,2017,Incremental Sequence Learning,Edwin D. de Jong,edwin.webmail@gmail.com,5;3;5,3;4;4,Reject,6,4,0.0,no,11/4/16,Utrecht University,275,86,5;6;8
413,413,413,413,413,413,413,413,ICLR,2017,Learning to Draw Samples: With Application to Amortized MLE for Generative Adversarial Learning,Dilin Wang;Qiang Liu,dilin.wang.gr@dartmouth.edu;qiang.liu@dartmouth.edu,4;4;4,3;4;3,Invite to Workshop Track,2,1,0.0,no,11/4/16,Dartmouth College;Dartmouth College,140;140,82;82,4
414,414,414,414,414,414,414,414,ICLR,2017,Gaussian Attention Model and Its Application to Knowledge Base Embedding and Question Answering,Liwen Zhang;John Winn;Ryota Tomioka,liwenz@cs.uchicago.edu;jwinn@microsoft.com;ryoto@microsoft.com,5;4;4,4;4;3,Reject,2,1,0.0,no,11/4/16,University of Chicago;Microsoft;Microsoft,45;-1;-1,10;-1;-1,
415,415,415,415,415,415,415,415,ICLR,2017,Recurrent Inference Machines for Solving Inverse Problems,Patrick Putzky;Max Welling,patrick.putzky@gmail.com;welling.max@gmail.com,5;4;7,4;4;3,Reject,2,3,0.0,no,11/4/16,University of Amsterdam;University of California - Irvine,161;36,63;99,
416,416,416,416,416,416,416,416,ICLR,2017,Learning a Static Analyzer: A Case Study on a Toy Language,Manzil Zaheer;Jean-Baptiste Tristan;Michael L. Wick;Guy L. Steele Jr.,manzil.zaheer@cmu.edu;jean.baptiste.tristan@oracle.com;michael.wick@oracle.com;guy.steele@oracle.com,4;3;3,4;4;5,Reject,5,0,0.0,no,11/3/16,Carnegie Mellon University;Oracle;Oracle;Oracle,1;-1;-1;-1,23;-1;-1;-1,3
417,417,417,417,417,417,417,417,ICLR,2017,The Incredible Shrinking Neural Network: New Perspectives on Learning Representations Through The Lens of Pruning,Nikolas Wolfe;Aditya Sharma;Lukas Drude;Bhiksha Raj,nwolfe@cs.cmu.edu;adityasharma@cmu.edu;drude@nt.upb.de;bhiksha@cs.cmu.edu,3;3;3,4;4;4,Reject,7,1,0.0,no,11/4/16,Carnegie Mellon University;Carnegie Mellon University;;Carnegie Mellon University,1;1;-1;1,23;23;-1;23,8
418,418,418,418,418,418,418,418,ICLR,2017,Neural Machine Translation with Latent Semantic of Image and Text,Joji Toyama;Masanori Misono;Masahiro Suzuki;Kotaro Nakayama;Yutaka Matsuo,toyama@weblab.t.u-tokyo.ac.jp;misono@weblab.t.u-tokyo.ac.jp;masa@weblab.t.u-tokyo.ac.jp;k-nakayama@weblab.t.u-tokyo.ac.jp;matsuo@weblab.t.u-tokyo.ac.jp,3;4;3,4;5;5,Reject,7,0,0.0,no,11/4/16,The University of Tokyo;The University of Tokyo;The University of Tokyo;The University of Tokyo;The University of Tokyo,50;50;50;50;50,39;39;39;39;39,3
419,419,419,419,419,419,419,419,ICLR,2017,CONTENT2VEC: SPECIALIZING JOINT REPRESENTATIONS OF PRODUCT IMAGES AND TEXT FOR THE TASK OF PRODUCT RECOMMENDATION,Thomas Nedelec;Elena Smirnova;Flavian Vasile,t.nedelec@criteo.com;e.smirnova@criteo.com;f.vasile@criteo.com,3;3;5,3;3;3,Reject,1,1,0.0,no,11/5/16,Criteo;Criteo;Criteo,-1;-1;-1,-1;-1;-1,
420,420,420,420,420,420,420,420,ICLR,2017,Group Sparse CNNs for Question Sentence Classification with Answer Sets,Mingbo Ma;Liang Huang;Bing Xiang;Bowen Zhou,mam@oregonstate.edu;liang.huang@oregonstate.edu;bingxia@us.ibm.com;zhou@us.ibm.com,4;5;6,4;4;4,Reject,1,0,0.0,no,11/4/16,Oregon State University;Oregon State University;International Business Machines;International Business Machines,75;75;-1;-1,316;316;-1;-1,
421,421,421,421,421,421,421,421,ICLR,2017,Multi-label learning with the RNNs for Fashion Search,Taewan Kim,taey.16@navercorp.com,4;3;3,4;4;3,Reject,3,0,0.0,no,11/5/16,NAVER,-1,-1,2
422,422,422,422,422,422,422,422,ICLR,2017,A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks,Kazuma Hashimoto;Caiming Xiong;Yoshimasa Tsuruoka;Richard Socher,hassy@logos.t.u-tokyo.ac.jp;cxiong@salesforce.com;tsuruoka@logos.t.u-tokyo.ac.jp;rsocher@salesforce.com,6;3;5,4;4;4,Reject,3,3,0.0,no,11/4/16,The University of Tokyo;SalesForce.com;The University of Tokyo;SalesForce.com,50;-1;50;-1,39;-1;39;-1,3
423,423,423,423,423,423,423,423,ICLR,2017,An Analysis of Deep Neural Network Models for Practical Applications,Alfredo Canziani;Adam Paszke;Eugenio Culurciello,canziani@purdue.edu;a.paszke@students.mimuw.edu.pl;euge@purdue.edu,4;4;5,3;3;4,Reject,3,5,0.0,no,11/4/16,"Purdue University;University of Washington, Seattle;Purdue University",25;6;25,70;25;70,1;2
424,424,424,424,424,424,424,424,ICLR,2017,Improving Stochastic Gradient Descent with Feedback,Jayanth Koushik;Hiroaki Hayashi,jkoushik@cs.cmu.edu;hiroakih@cs.cmu.edu,5;6;5,4;4;4,Reject,5,2,0.0,no,11/4/16,Carnegie Mellon University;Carnegie Mellon University,1;1,23;23,3
425,425,425,425,425,425,425,425,ICLR,2017,Understanding intermediate layers using linear classifier probes,Guillaume Alain;Yoshua Bengio,guillaume.alain.umontreal@gmail.com;yoshua.bengio@gmail.com,5;4;4,3;4;4,Reject,2,2,0.0,no,11/5/16,University of Montreal;,113;-1,103;-1,
426,426,426,426,426,426,426,426,ICLR,2017,Classify or Select: Neural Architectures for Extractive Document Summarization,Ramesh Nallapati;Bowen Zhou;Mingbo Ma,nallapati@us.ibm.com;zhou@us.ibm.com;mam@oregonstate.edu,6;4;4,4;4;4,Reject,7,3,0.0,no,11/4/16,International Business Machines;International Business Machines;Oregon State University,-1;-1;75,-1;-1;316,
427,427,427,427,427,427,427,427,ICLR,2017,Joint Training of Ratings and Reviews with Recurrent Recommender Networks,Chao-Yuan Wu;Amr Ahmed;Alex Beutel;Alexander J. Smola,cywu@cs.utexas.edu;amra@google.com;alexbeutel@google.com;alex@smola.org,6;6;6,4;3;4,Reject,4,6,0.0,no,11/4/16,"University of Texas, Austin;Google;Google;Carnegie-Mellon University",20;-1;-1;1,50;-1;-1;23,3
428,428,428,428,428,428,428,428,ICLR,2017,Divide and Conquer with Neural Networks,Alex Nowak;Joan Bruna,anv273@nyu.edu;bruna@cims.nyu.edu,4;4;3,2;4;2,Reject,4,1,0.0,no,11/4/16,New York University;New York University,25;25,32;32,10;8
429,429,429,429,429,429,429,429,ICLR,2017,Is a picture worth a thousand words? A Deep Multi-Modal Fusion Architecture for Product Classification in e-commerce,Tom Zahavy;Alessandro Magnani;Abhinandan Krishnan;Shie Mannor,tomzahavy@tx.technion.ac.il;AMagnani@walmartlabs.com;AKrishnan@walmartlabs.com;shie@ee.technion.ac.il,5;5;4,4;4;4,Reject,2,2,0.0,no,11/4/16,Technion;Walmartlabs;Walmartlabs;Technion,24;-1;-1;24,301;-1;-1;301,
430,430,430,430,430,430,430,430,ICLR,2017,Deep Symbolic Representation Learning for Heterogeneous Time-series Classification,Shengdong Zhang;Soheil Bahrampour;Naveen Ramakrishnan;Mohak Shah,zhangshengdongofgz@gmail.com;Soheil.Bahrampour@us.bosch.com;Naveen.Ramakrishnan@us.bosch.com;mohak@mohakshah.com,3;5;4,4;4;3,Reject,4,2,0.0,no,11/4/16,Simon Fraser University;Bosch;Bosch;Mohakshah,-1;-1;-1;-1,-1;433;433;-1,
431,431,431,431,431,431,431,431,ICLR,2017,Enforcing constraints on outputs with unconstrained inference,Jay Yoon Lee;Michael L. Wick;Jean-Baptiste Tristan,lee.jayyoon@gmail.com;michael.wick@oracle.com;jean.baptiste.tristan@oracle.com,4;3;3,5;4;4,Reject,2,0,0.0,no,11/3/16,Carnegie Mellon University;Oracle;Oracle,1;-1;-1,23;-1;-1,3
432,432,432,432,432,432,432,432,ICLR,2017,Sequence generation with a physiologically plausible model of handwriting and Recurrent Mixture Density Networks,Daniel Berio;Memo Akten;Frederic Fol Leymarie;Mick Grierson;Réjean Plamondon,d.berio@gold.ac.uk;m.akten@ac.uk;ffl@gold.ac.uk;m.grierson@gold.ac.uk;rejean.plamondon@polymtl.ca,3;3;3,3;5;3,Reject,3,1,0.0,no,11/5/16,;;;;Polytechnique Montreal,-1;-1;-1;-1;351,-1;-1;-1;-1;981,
433,433,433,433,433,433,433,433,ICLR,2017,Deep Neural Networks and the Tree of Life,Yan Wang;Kun He;John E. Hopcroft;Yu Sun,yanwang@hust.edu.cn;brooklet60@hust.edu.cn;jeh@cs.cornell.edu;ys646@cornell.edu,3;4;4,5;4;4,Reject,0,3,0.0,no,11/5/16,Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Cornell University;Cornell University,39;39;7;7,48;48;19;19,2
434,434,434,434,434,434,434,434,ICLR,2017,Hierarchical Memory Networks,Sarath Chandar;Sungjin Ahn;Hugo Larochelle;Pascal Vincent;Gerald Tesauro;Yoshua Bengio,apsarathchandar@gmail.com;sjn.ahn@gmail.com;hugo@twitter.com;vincentp@iro.umontreal.ca;gtesauro@us.ibm.com;yoshua.bengio@umontreal.ca,5;5;4,3;5;4,Reject,3,3,0.0,no,11/4/16,University of Montreal;University of Montreal;Twitter;University of Montreal;International Business Machines;University of Montreal,113;113;-1;113;-1;113,103;103;-1;103;-1;103,
435,435,435,435,435,435,435,435,ICLR,2017,Near-Data Processing for Machine Learning,Hyeokjun Choe;Seil Lee;Hyunha Nam;Seongsik Park;Seijoon Kim;Eui-Young Chung;Sungroh Yoon,genesis1104@snu.ac.kr;lees231@dsl.snu.ac.kr;godqhr825@snu.ac.kr;pss015@snu.ac.kr;hokiespa@snu.ac.kr;eychung@yonsei.ac.kr;sryoon@snu.ac.kr,4;6;5,4;2;2,Reject,2,3,0.0,no,11/5/16,Seoul National University;Seoul National University;Seoul National University;Seoul National University;Seoul National University;Yonsei University;Seoul National University,50;50;50;50;50;462;50,72;72;72;72;72;293;72,
436,436,436,436,436,436,436,436,ICLR,2017,Rotation Plane Doubly Orthogonal Recurrent Neural Networks,Zoe McCarthy;Andrew Bai;Xi Chen;Pieter Abbeel,zmccarthy@berkeley.edu;xiaoyang.bai@berkeley.edu;c.xi@berkeley.edu;pabbeel@berkeley.edu,4;4;5,4;4;3,Reject,0,0,0.0,no,11/5/16,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5,10;10;10;10,
437,437,437,437,437,437,437,437,ICLR,2017,FastText.zip: Compressing text classification models,Armand Joulin;Edouard Grave;Piotr Bojanowski;Matthijs Douze;Herve Jegou;Tomas Mikolov,ajoulin@fb.com;egrave@fb.com;bojanowski@fb.com;matthijs@fb.com;rvj@fb.com;tmikolov@fb.com,5;6;6,4;4;3,Reject,2,2,0.0,no,11/4/16,Facebook;Facebook;Facebook;Facebook;Facebook;Facebook,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,3
438,438,438,438,438,438,438,438,ICLR,2017,Symmetry-Breaking Convergence Analysis of Certain Two-layered Neural Networks with ReLU nonlinearity,Yuandong Tian,yuandong@fb.com,4;8;4,3;4;4,Invite to Workshop Track,1,3,0.0,no,11/4/16,Facebook,-1,-1,1;9
439,439,439,439,439,439,439,439,ICLR,2017,Training Long Short-Term Memory With Sparsified Stochastic Gradient Descent,Maohua Zhu;Minsoo Rhu;Jason Clemons;Stephen W. Keckler;Yuan Xie,maohuazhu@ece.ucsb.edu;mrhu@nvidia.com;jclemons@nvidia.com;skeckler@nvidia.com;yuanxie@ece.ucsb.edu,4;5;4,4;3;4,Reject,2,0,0.0,no,11/4/16,UC Santa Barbara;NVIDIA;NVIDIA;NVIDIA;UC Santa Barbara,39;-1;-1;-1;39,48;-1;-1;-1;48,
440,440,440,440,440,440,440,440,ICLR,2017,Unsupervised Learning Using Generative Adversarial Training And Clustering,Vittal Premachandran;Alan L. Yuille,vittalp@jhu.edu;ayuille1@jhu.edu,3;3;3,4;5;4,Reject,0,0,0.0,no,11/5/16,Johns Hopkins University;Johns Hopkins University,67;67,17;17,5;4
441,441,441,441,441,441,441,441,ICLR,2017,Inverse Problems in Computer Vision using  Adversarial  Imagination Priors,Hsiao-Yu Fish Tung;Katerina Fragkiadaki,htung@cs.cmu.edu;katef@cs.cmu.edu,3;5;6,4;3;3,Reject,0,0,0.0,no,11/4/16,Carnegie Mellon University;Carnegie Mellon University,1;1,23;23,4;2;10
442,442,442,442,442,442,442,442,ICLR,2017,Generative Adversarial Networks as Variational Training of Energy Based Models,Shuangfei Zhai;Yu Cheng;Rogerio Feris;Zhongfei Zhang,szhai2@binghamton.edu;chengyu@us.ibm.com;rsferis@us.ibm.com;zhongfei@cs.binghamton.edu,4;4;4,3;5;5,Reject,4,5,0.0,no,11/5/16,"State University of New York, Binghamton;International Business Machines;International Business Machines;State University of New York, Binghamton",140;-1;-1;140,394;-1;-1;394,5;4;1
443,443,443,443,443,443,443,443,ICLR,2017,An Information Retrieval Approach for Finding Dependent Subspaces of Multiple Views,Ziyuan Lin;Jaakko Peltonen,ziyuan.lin@aalto.fi;jaakko.peltonen@uta.fi,4;4;4,4;4;4,Reject,2,0,0.0,no,11/5/16,Aalto University;University of Tampere,161;462,228;296,
444,444,444,444,444,444,444,444,ICLR,2017,Differentiable Canonical Correlation Analysis,Matthias Dorfer;Jan Schlüter;Gerhard Widmer,matthias.dorfer@jku.at;jan.schlueter@ofai.at;gerhard.widmer@jku.at,3;4;3,4;4;4,Reject,1,1,0.0,no,11/5/16,Johannes Kepler University Linz;Austrian Research Institute for Artificial Intelligence;Johannes Kepler University Linz,462;-1;462,498;-1;498,
445,445,445,445,445,445,445,445,ICLR,2017,Generative Adversarial Networks for Image Steganography,Denis Volkhonskiy;Boris Borisenko;Evgeny Burnaev,dvolkhonskiy@gmail.com;bborisenko@hse.ru;e.burnaev@skoltech.ru,5;6;4,3;4;3,Reject,2,3,0.0,no,11/4/16,Skolkovo Institute of Science and Technology;Higher School of Economics;Skolkovo Institute of Science and Technology,-1;462;-1,-1;456;-1,5;4
446,446,446,446,446,446,446,446,ICLR,2017,A Deep Learning Approach for Joint Video Frame and Reward Prediction in Atari Games,Felix Leibfried;Nate Kushman;Katja Hofmann,felix.leibfried@gmail.com;nkushman@microsoft.com;katja.hofmann@microsoft.com,4;4;4,4;4;4,Reject,2,1,0.0,no,11/3/16,Max-Planck Institute;Microsoft;Microsoft,-1;-1;-1,-1;-1;-1,
447,447,447,447,447,447,447,447,ICLR,2017,Wav2Letter: an End-to-End ConvNet-based Speech Recognition System,Ronan Collobert;Christian Puhrsch;Gabriel Synnaeve,locronan@fb.com;cpuhrsch@fb.com;gab@fb.com,6;7,4;5,Reject,7,1,0.0,no,11/4/16,Facebook;Facebook;Facebook,-1;-1;-1,-1;-1;-1,2;10
448,448,448,448,448,448,448,448,ICLR,2017,Tree-Structured Variational Autoencoder,Richard Shin;Alexander A. Alemi;Geoffrey Irving;Oriol Vinyals,ricshin@cs.berkeley.edu;alemi@google.com;geoffreyi@google.com;vinyals@google.com,3;3;4,4;4;4,Reject,0,0,0.0,no,11/4/16,University of California Berkeley;Google;Google;Google,5;-1;-1;-1,10;-1;-1;-1,3;5;1
449,449,449,449,449,449,449,449,ICLR,2017,Transformation-based Models of Video Sequences,Joost van Amersfoort;Anitha Kannan;Marc'Aurelio Ranzato;Arthur Szlam;Du Tran;Soumith Chintala,joost@joo.st;akannan@fb.com;ranzato@fb.com;aszlam@fb.com;trandu@fb.com;soumith@fb.com,5;6;3,3;4;3,Reject,6,0,0.0,no,11/4/16,Twitter;Facebook;Facebook;Facebook;Facebook;Facebook,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,
450,450,450,450,450,450,450,450,ICLR,2017,Nonparametrically Learning Activation Functions in Deep Neural Nets,Carson Eisenach;Zhaoran Wang;Han Liu,eisenach@princeton.edu;zhaoran@princeton.edu;hanliu@princeton.edu,7;6;5,5;4;4,Invite to Workshop Track,1,3,0.0,no,11/4/16,Princeton University;Princeton University;Princeton University,32;32;32,7;7;7,8
451,451,451,451,451,451,451,451,ICLR,2017,Leveraging Asynchronicity in Gradient Descent for Scalable Deep Learning,Jeff Daily;Abhinav Vishnu;Charles Siegel,jeff.daily@pnnl.gov;abhinav.vishnu@pnnl.gov;charles.siegel@pnnl.gov,5;3;3,4;4;5,Reject,0,1,0.0,no,11/4/16,Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory,-1;-1;-1,-1;-1;-1,
452,452,452,452,452,452,452,452,ICLR,2017,Rule Mining in Feature Space,Stefano Teso;Andrea Passerini,teso@disi.unitn.it;passerini@disi.unitn.it,4;3;4,4;4;4,Reject,0,0,0.0,no,11/4/16,University of Trento;University of Trento,15;15,236;236,
453,453,453,453,453,453,453,453,ICLR,2017,ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation,Adam Paszke;Abhishek Chaurasia;Sangpil Kim;Eugenio Culurciello,a.paszke@students.mimuw.edu.pl;aabhish@purdue.edu;sangpilkim@purdue.edu;euge@purdue.edu,4;4;5;3,4;4;4;4,Reject,9,0,0.0,no,11/4/16,"University of Washington, Seattle;Purdue University;Purdue University;Purdue University",6;25;25;25,25;70;70;70,2
454,454,454,454,454,454,454,454,ICLR,2017,Efficient Calculation of Polynomial Features on Sparse Matrices,Andrew Nystrom;John Hughes,awnystrom@gmail.com;jfh@cs.brown.edu,3;3;3,3;3;1,Reject,0,1,0.0,no,11/4/16,Google;Brown University,-1;61,-1;51,
455,455,455,455,455,455,455,455,ICLR,2017,Learning Word-Like Units from Joint Audio-Visual Analylsis,David Harwath;James R. Glass,dharwath@mit.edu;glass@mit.edu,6;5;5,4;5;4,Reject,1,1,0.0,no,11/4/16,Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2,5;5,
456,456,456,456,456,456,456,456,ICLR,2017,Rethinking Numerical Representations for Deep Neural Networks,Parker Hill;Babak Zamirai;Shengshuo Lu;Yu-Wei Chao;Michael Laurenzano;Mehrzad Samadi;Marios Papaefthymiou;Scott Mahlke;Thomas Wenisch;Jia Deng;Lingjia Tang;Jason Mars,parkerhh@umich.edu;zamirai@umich.edu;luss@umich.edu;ywchao@umich.edu;mlaurenz@umich.edu;mehrzads@umich.edu;marios@umich.edu;mahlke@umich.edu;twenisch@umich.edu;jiadeng@umich.edu;lingjia@umich.edu;profmars@umich.edu,6;5;5,3;5;2,Reject,2,3,0.0,no,11/4/16,University of Michigan;University of Michigan;University of Michigan;University of Michigan;University of Michigan;University of Michigan;University of Michigan;University of Michigan;University of Michigan;University of Michigan;University of Michigan;University of Michigan,8;8;8;8;8;8;8;8;8;8;8;8,21;21;21;21;21;21;21;21;21;21;21;21,
457,457,457,457,457,457,457,457,ICLR,2017,Semi-Supervised Learning with Context-Conditional Generative Adversarial Networks,Emily Denton;Sam Gross;Rob Fergus,denton@cs.nyu.edu;sgross@fb.com;robfergus@fb.com,5;6;6,4;4;4,Reject,6,3,0.0,no,11/4/16,New York University;Facebook;Facebook,25;-1;-1,32;-1;-1,5;4
458,458,458,458,458,458,458,458,ICLR,2017,Neural Graph Machines: Learning Neural Networks Using Graphs,Thang D. Bui;Sujith Ravi;Vivek Ramavajjala,tdb40@cam.ac.uk;sravi@google.com;vramavaj@google.com,3;4;3,4;4;4,Reject,3,1,0.0,no,11/4/16,University of Cambridge;Google;Google,67;-1;-1,4;-1;-1,10
459,459,459,459,459,459,459,459,ICLR,2017,Perception Updating Networks: On architectural constraints for interpretable video generative models,Eder Santana;Jose C Principe,edercsjr@gmail.com;principe@cnel.ufl.edu,4;4;4,3;4;4,Invite to Workshop Track,1,4,1.0,no,11/4/16,University of Florida;University of Florida,129;129,135;135,10
460,460,460,460,460,460,460,460,ICLR,2017,DRAGNN: A Transition-Based Framework for Dynamically Connected Neural Networks,Lingpeng Kong;Chris Alberti;Daniel Andor;Ivan Bogatyy;David Weiss,lingpenk@cs.cmu.edu;chrisalberti@google.com;andor@google.com;bogatyy@google.com;djweiss@google.com,6;7;5,3;4;4,Reject,2,1,0.0,no,11/4/16,Carnegie Mellon University;Google;Google;Google;Google,1;-1;-1;-1;-1,23;-1;-1;-1;-1,10
461,461,461,461,461,461,461,461,ICLR,2017,A Simple yet Effective Method to Prune Dense Layers of Neural Networks,Mohammad Babaeizadeh;Paris Smaragdis;Roy H. Campbell,mb2@illinois.edu.edu;paris@illinois.edu.edu;rhc@illinois.edu.edu,5;5;3,4;3;4,Reject,1,1,0.0,no,11/4/16,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",4;4;4,36;36;36,1
462,462,462,462,462,462,462,462,ICLR,2017,Beyond Fine Tuning: A Modular Approach to Learning on Small Data,Aryk Anderson;Kyle Shaffer;Artem Yankov;Court Corley;Nathan Hodas,aryk.anderson@eagles.ewu.edu;kyle.shaffer@pnnl.gov;artem.yankov@pnnl.gov;court@pnnl.gov;nathan.hodas@pnnl.gov,4;6;6,4;2;5,Reject,3,0,0.0,no,11/4/16,Eastern Washington University;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,6
463,463,463,463,463,463,463,463,ICLR,2017,HFH: Homologically Functional Hashing for Compressing Deep Neural Networks,Lei Shi;Shikun Feng;Zhifan Zhu,shilei06@baidu.com;fengshikun01@baidu.com;zhuzhifan@baidu.com,5;6;4,5;4,Reject,1,2,0.0,no,11/4/16,Baidu;Baidu;Baidu,-1;-1;-1,-1;-1;-1,
464,464,464,464,464,464,464,464,ICLR,2017,Learning to Understand: Incorporating Local Contexts with Global Attention for Sentiment Classification,Zhigang Yuan;Yuting Hu;Yongfeng Huang,yuanzg14@mails.tsinghua.edu.cn;hu-yt12@mails.tsinghua.edu.cn;yfhuang@tsinghua.edu.cn,3;3;4,4;4;4,Reject,0,0,0.0,no,11/4/16,Tsinghua University;Tsinghua University;Tsinghua University,11;11;11,35;35;35,
465,465,465,465,465,465,465,465,ICLR,2017,Multiplicative LSTM for sequence modelling,Ben Krause;Iain Murray;Steve Renals;Liang Lu,ben.krause@ed.ac.uk;i.murray@ed.ac.uk;s.renals@ed.ac.uk;llu@ttic.edu,4;4;6,4;5;4,Invite to Workshop Track,3,4,0.0,no,11/4/16,University of Edinburgh;University of Edinburgh;University of Edinburgh;Toyota Technological Institute at Chicago,33;33;33;-1,27;27;27;-1,
466,466,466,466,466,466,466,466,ICLR,2017,Tensorial Mixture Models,Or Sharir;Ronen Tamari;Nadav Cohen;Amnon Shashua,or.sharir@cs.huji.ac.il;ronent@cs.huji.ac.il;cohennadav@cs.huji.ac.il;shashua@cs.huji.ac.il,4;7;5,4;3;3,Reject,3,3,0.0,no,11/4/16,Hebrew University of Jerusalem;Hebrew University of Jerusalem;Hebrew University of Jerusalem;Hebrew University of Jerusalem,57;57;57;57,186;186;186;186,5
467,467,467,467,467,467,467,467,ICLR,2017,Neural Functional Programming,John K. Feser;Marc Brockschmidt;Alexander L. Gaunt;Daniel Tarlow,feser@csail.mit.edu;mabrocks@microsoft.com;t-algaun@microsoft.com;dtarlow@microsoft.com,5;4;7;5;6,3;3;2;2;3,Invite to Workshop Track,4,2,0.0,no,11/4/16,Massachusetts Institute of Technology;Microsoft;Microsoft;Microsoft,2;-1;-1;-1,5;-1;-1;-1,
468,468,468,468,468,468,468,468,ICLR,2017,Deep Error-Correcting Output Codes,Guoqiang Zhong;Yuchen Zheng;Peng Zhang;Mengqi Li;Junyu Dong,gqzhong@ouc.edu.cn;ouczyc@outlook.com;sdrzbruce@163.com;enri9615@outlook.com;dongjunyu@ouc.edu.cn,3;3;3,4;5;5,Reject,1,3,0.0,no,11/4/16,"University of Illinois, Urbana-Champaign;University of Illinois, Urbana-Champaign;163;;University of Illinois, Urbana-Champaign",4;4;-1;-1;4,36;36;-1;-1;36,
469,469,469,469,469,469,469,469,ICLR,2017,Marginal Deep Architectures: Deep learning for Small and Middle Scale Applications,Yuchen Zheng;Guoqiang Zhong;Junyu Dong,ouczyc@outlook.com;gqzhong@ouc.edu.cn;dongjunyu@ouc.edu.cn,3;4;4,4;4;4,Reject,3,3,0.0,no,11/4/16,"University of Illinois, Urbana-Champaign;University of Illinois, Urbana-Champaign;University of Illinois, Urbana-Champaign",4;4;4,36;36;36,
470,470,470,470,470,470,470,470,ICLR,2017,Generative Paragraph Vector,Ruqing Zhang;Jiafeng Guo;Yanyan Lan;Jun Xu;Xueqi Cheng,zhangruqing@software.ict.ac.cn;guojiafeng@ict.ac.cn;lanyanyan@ict.ac.cn;junxu@ict.ac.cn;cxq@ict.ac.cn,4;3;2,4;4;5,Reject,1,0,0.0,no,11/4/16,"Institute of Computing Technology, Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences",67;67;67;67;67,981;981;981;981;981,5
471,471,471,471,471,471,471,471,ICLR,2017,Simple Black-Box Adversarial Perturbations for Deep Networks,Nina Narodytska;Shiva Kasiviswanathan,n.narodytska@gmail.com;kaivisw@gmail.com,4;4;4,3;4;4,Reject,2,3,0.0,no,11/4/16,;,-1;-1,-1;-1,4;2
472,472,472,472,472,472,472,472,ICLR,2017,Short and Deep: Sketching and Neural Networks,Amit Daniely;Nevena Lazic;Yoram Singer;Kunal Talwar,amitdaniely@google.com;nevena@google.com;singer@google.com;kunal@google.com,4;5;5,2;4;2,Invite to Workshop Track,5,4,0.0,no,11/4/16,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,
473,473,473,473,473,473,473,473,ICLR,2017,Cat2Vec: Learning Distributed Representation of Multi-field Categorical Data,Ying Wen;Jun Wang;Tianyao Chen;Weinan Zhang,ying.wen@cs.ucl.ac.uk;jun.wang@cs.ucl.ac.uk;tychen@apex.sjtu.edu.cn;wnzhang@apex.sjtu.edu.cn,4;5;4,4;5;4,Reject,2,0,0.0,no,11/4/16,University College London;University College London;Shanghai Jiao Tong University;Shanghai Jiao Tong University,45;45;61;61,15;15;214;214,3
474,474,474,474,474,474,474,474,ICLR,2017,On Robust Concepts and Small Neural Nets,Amit Deshpande;Sushrut Karmalkar,amitdesh@microsoft.com;sushrutk@cs.utexas.edu,5;5;6,4;4;2,Invite to Workshop Track,3,3,0.0,no,11/4/16,"Microsoft;University of Texas, Austin",-1;20,-1;50,1;8
475,475,475,475,475,475,475,475,ICLR,2017,Identity Matters in Deep Learning,Moritz Hardt;Tengyu Ma,m@mrtz.org;tengyu@cs.princeton.edu,8;5;6,3;4;5,Accept (Poster),2,4,0.0,no,11/4/16,University of California - Berkeley;Princeton University,5;32,10;7,1
476,476,476,476,476,476,476,476,ICLR,2017,SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size,Forrest N. Iandola;Song Han;Matthew W. Moskewicz;Khalid Ashraf;William J. Dally;Kurt Keutzer,forresti@eecs.berkeley.edu;songhan@stanford.edu;moskewcz@eecs.berkeley.edu;kashraf@eecs.berkeley.edu;dally@stanford.edu;keutzer@eecs.berkeley.edu,7;5;7,3;4;4,Reject,3,3,0.0,no,11/4/16,University of California Berkeley;Stanford University;University of California Berkeley;University of California Berkeley;Stanford University;University of California Berkeley,5;3;5;5;3;5,10;3;10;10;3;10,
477,477,477,477,477,477,477,477,ICLR,2017,Non-linear Dimensionality Regularizer for Solving Inverse Problems,Ravi Garg;Anders Eriksson;Ian Reid,ravi.garg@adelaide.edu.au;anders.eriksson@qut.edu.au;ian.reid@adelaide.edu.au,4;3;4,4;5;4,Reject,0,2,0.0,no,11/4/16,The University of Adelaide;South China University of Technology;The University of Adelaide,113;462;113,143;613;143,2
478,478,478,478,478,478,478,478,ICLR,2017,"ParMAC: distributed optimisation of nested functions, with application to binary autoencoders",Miguel A. Carreira-Perpinan;Mehdi Alizadeh,mcarreira-perpinan@ucmerced.edu;malizadeh@ucmerced.edu,4;5;6;6,2;4;4;4,Reject,1,5,0.0,no,11/4/16,University of California at Merced;University of California at Merced,462;462,981;981,
479,479,479,479,479,479,479,479,ICLR,2017,Filling in the details: Perceiving from low fidelity visual input,Farahnaz A. Wick;Michael L. Wick;Marc Pomplun,fwick@cs.umb.edu;mwick@cs.umass.edu;mpomplun@gmail.com,4;6;5,3;4;5,Reject,3,4,0.0,no,11/3/16,"University of Massachusetts, Boston;University of Massachusetts, Amherst;",351;25;-1,166;166;-1,5
480,480,480,480,480,480,480,480,ICLR,2017,Pedestrian Detection Based On Fast R-CNN and Batch Normalization ,Zhong-Qiu Zhao;Haiman Bian;Donghui Hu;Herve Glotin,z.zhao@hfut.edu.cn;bhm2164@163.com;hudh@hfut.edu.cn;h.glotin@gmail.com,3;3;2;3,5;5;5;5,Reject,0,0,0.0,no,11/3/16,South China University of Technology;163;South China University of Technology;CNRS university Toulon,462;-1;462;462,613;-1;613;981,8
481,481,481,481,481,481,481,481,ICLR,2017,Ternary Weight Decomposition and Binary Activation Encoding for Fast and Compact Neural Network,Mitsuru Ambai;Takuya Matsumoto;Takayoshi Yamashita;Hironobu Fujiyoshi,manbai@d-itlab.co.jp;tmatsumoto@d-itlab.co.jp;yamashita@cs.chubu.ac.jp;hf@cs.chubu.ac.jp,6;5;4,3;3;4,Reject,1,0,0.0,no,11/2/16,;;chubu university;chubu university,-1;-1;462;462,-1;-1;630;630,2
482,482,482,482,482,482,482,482,ICLR,2017,What Is the Best Practice for CNNs Applied to Visual Instance Retrieval?,Jiedong Hao;Jing Dong;Wei Wang;Tieniu Tan,jiedong.hao@cripac.ia.ac.cn;jdong@nlpr.ia.ac.cn;wwang@nlpr.ia.ac.cn;tnt@nlpr.ia.ac.cn,3;6;3,5;4;5,Reject,4,0,0.0,no,11/3/16,"Institute of automation, Chinese academy of science, Chinese Academy of Sciences;Institute of automation, Chinese academy of science, Chinese Academy of Sciences;Institute of automation, Chinese academy of science, Chinese Academy of Sciences;Institute of automation, Chinese academy of science, Chinese Academy of Sciences",67;67;67;67,981;981;981;981,
483,483,483,483,483,483,483,483,ICLR,2017,Rectified Factor Networks for Biclustering,Djork-Arné Clevert;Thomas Unterthiner;Sepp Hochreiter,okko@bioinf.jku.at;unterthiner@bioinf.jku.at;hochreit@bioinf.jku.at,4;5;5,4;2;2,Reject,1,0,0.0,no,11/2/16,Johannes Kepler University Linz;Johannes Kepler University Linz;Johannes Kepler University Linz,462;462;462,498;498;498,5
484,484,484,484,484,484,484,484,ICLR,2017,Vocabulary Selection Strategies for Neural Machine Translation,Gurvan L'Hostis;David Grangier;Michael Auli,gurvan.lhostis@polytechnique.edu;grangier@fb.com;michaelauli@fb.com,5;4;4;5,3;5;4;3,Reject,0,0,0.0,no,11/2/16,Ecole polytechnique;Facebook;Facebook,462;-1;-1,116;-1;-1,3
485,485,485,485,485,485,485,485,ICLR,2017,Learning Efficient Algorithms with Hierarchical Attentive Memory,Marcin Andrychowicz;Karol Kurach,marcin@openai.com;kkurach@google.com,3;5;5,4;5;4,Reject,2,0,0.0,no,11/1/16,OpenAI;Google,-1;-1,-1;-1,
486,486,486,486,486,486,486,486,ICLR,2017,PREDICTION OF POTENTIAL HUMAN INTENTION USING SUPERVISED COMPETITIVE LEARNING,Masayoshi Ishikawa;Mariko Okude;Takehisa Nishida;Kazuo Muto,masayoshi.ishikawa.gv@hitachi.com;mariko.okude.uh@hitachi.com;takehisa.nishida.cu@hitachi.com;kazuo.muto.ny@hitachi.com,2;2;4,4;4;4,Reject,1,0,0.0,no,11/1/16,University of Iceland;Hitachi;Hitachi;Hitachi,462;-1;-1;-1,243;-1;-1;-1,
487,487,487,487,487,487,487,487,ICLR,2017,Conditional Image Synthesis With Auxiliary Classifier GANs,Augustus Odena;Christopher Olah;Jonathon Shlens,augustusodena@google.com;colah@google.com;shlens@google.com,3;6;6,4;5;4,Reject,3,5,0.0,no,11/1/16,Google;Google;Google,-1;-1;-1,-1;-1;-1,5;4
488,488,488,488,488,488,488,488,ICLR,2017,Learning to Protect Communications with Adversarial Neural Cryptography,Martín Abadi;David G. Andersen,abadi@google.com;dga@google.com,5;6;4,4;3;2,Reject,3,3,0.0,no,10/21/16,Google;Google,-1;-1,-1;-1,4
489,489,489,489,489,489,489,489,ICLR,2017,Surprisal-Driven Feedback in Recurrent Networks,Kamil Rocki,kmrocki@us.ibm.com,4;3;3,4;5;5,Reject,5,1,0.0,no,10/18/16,International Business Machines,-1,-1,3
490,490,490,490,490,490,490,490,ICLR,2018,Certifying Some Distributional Robustness with Principled Adversarial Training,Aman Sinha;Hongseok Namkoong;John Duchi,amans@stanford.edu;hnamk@stanford.edu;jduchi@stanford.edu,9;9;9,5;4;4,Accept (Oral),0,14,3.0,yes,10/27/17,Stanford University;Stanford University;Stanford University,4;4;4,3;3;3,4
491,491,491,491,491,491,491,491,ICLR,2018,Parametric Information Bottleneck to Optimize Stochastic Neural Networks,Thanh T. Nguyen;Jaesik Choi,thanhnguyen2792@gmail.com;jaesik@unist.ac.kr,4;6;4,4;4;4,Reject,0,0,0.0,yes,10/27/17,Ulsan National Institute of Science and Technology;Ulsan National Institute of Science and Technology,468;468,230;230,8
492,492,492,492,492,492,492,492,ICLR,2018,Towards Neural Phrase-based Machine Translation,Po-Sen Huang;Chong Wang;Sitao Huang;Dengyong Zhou;Li Deng,huang.person@gmail.com;chongw@google.com;shuang91@illinois.edu;dennyzhou@gmail.com;l.deng@ieee.org,6;6;8,3;4;5,Accept (Poster),0,3,0.0,yes,10/26/17,"Microsoft;Google;University of Illinois, Urbana Champaign;Google;",-1;-1;3;-1;-1,-1;-1;37;-1;-1,3;2
493,493,493,493,493,493,493,493,ICLR,2018,Weightless: Lossy Weight Encoding For Deep Neural Network Compression,Brandon Reagen;Udit Gupta;Robert Adolf;Michael Mitzenmacher;Alexander Rush;Gu-Yeon Wei;David Brooks,reagen@fas.harvard.edu;ugupta@g.harvard.edu;rdadolf@seas.harvard.edu;michaelm@eecs.harvard.edu;srush@seas.harvard.edu;gywei@g.harvard.edu;dbrooks@eecs.harvard.edu,6;6;4,4;4;4,Invite to Workshop Track,0,3,0.0,yes,10/27/17,Harvard University;Harvard University;Harvard University;Harvard University;Harvard University;Harvard University;Harvard University,37;37;37;37;37;37;37,6;6;6;6;6;6;6,
494,494,494,494,494,494,494,494,ICLR,2018,Interactive Grounded Language Acquisition and Generalization in a 2D World,Haonan Yu;Haichao Zhang;Wei Xu,haonanyu@baidu.com;zhanghaichao@baidu.com;wei.xu@baidu.com,7;6;6,4;4;4,Accept (Poster),2,7,0.0,yes,10/27/17,Baidu;Baidu;Baidu,-1;-1;-1,-1;-1;-1,6;8
495,495,495,495,495,495,495,495,ICLR,2018,A Boo(n) for Evaluating Architecture Performance,Ondrej Bajgar;Rudolf Kadlec;and Jan Kleindienst,ondrej@bajgar.org;rudolf_kadlec@cz.ibm.com;jankle@cz.ibm.com,4;6;4,4;4;4,Reject,0,4,0.0,yes,10/27/17,;International Business Machines;International Business Machines,-1;-1;-1,-1;-1;-1,
496,496,496,496,496,496,496,496,ICLR,2018,Countering Adversarial Images using Input Transformations,Chuan Guo;Mayank Rana;Moustapha Cisse;Laurens van der Maaten,cg563@cornell.edu;mayankrana@fb.com;moustaphacisse@fb.com;lvdmaaten@gmail.com,4;8;7,3;4;3,Accept (Poster),12,16,0.0,yes,10/27/17,Cornell University;Facebook;Facebook;Facebook,7;-1;-1;-1,19;-1;-1;-1,4
497,497,497,497,497,497,497,497,ICLR,2018,A Neural Representation of Sketch Drawings,David Ha;Douglas Eck,hadavid@google.com;deck@google.com,8;8;5,4;4;4,Accept (Poster),0,3,0.0,yes,10/26/17,Google;Google,-1;-1,-1;-1,5
498,498,498,498,498,498,498,498,ICLR,2018,The Kanerva Machine: A Generative Distributed Memory,Yan Wu;Greg Wayne;Alex Graves;Timothy Lillicrap,yanwu@google.com;gregwayne@google.com;gravesa@google.com;countzero@google.com,6;7;7,4;3;2,Accept (Poster),0,3,0.0,yes,10/27/17,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,5;11
499,499,499,499,499,499,499,499,ICLR,2018,Minimax Curriculum Learning: Machine Teaching with Desirable Difficulties and Scheduled Diversity,Tianyi Zhou;Jeff Bilmes,tianyi.david.zhou@gmail.com;bilmes@uw.edu,5;6;6,3;4;3,Accept (Poster),0,4,0.0,yes,10/27/17,"University of Washington;University of Washington, Seattle",6;6,25;25,
500,500,500,500,500,500,500,500,ICLR,2018,Multi-View Data Generation Without View Supervision,Mickael Chen;Ludovic Denoyer;Thierry Artières,mickael.chen@lip6.fr;ludovic.denoyer@lip6.fr;thierry.artieres@lif.univ-mrs.fr,7;5;7,3;4;5,Accept (Poster),0,3,0.0,yes,10/27/17,LIP6;LIP6;Aix Marseille University,-1;-1;468,-1;-1;297,5;4
501,501,501,501,501,501,501,501,ICLR,2018,Towards Binary-Valued Gates for Robust LSTM Training ,Zhuohan Li;Di He;Fei Tian;Wei Chen;Tao Qin;Liwei Wang;Tie-Yan Liu,lizhuohan@pku.edu.cn;di_he@pku.edu.cn;fetia@microsoft.com;wche@microsoft.com;taoqin@microsoft.com;wanglw@cis.pku.edu.cn;tyliu@microsoft.com,6;4;6,3;4;4,Reject,0,4,0.0,yes,10/26/17,Peking University;Peking University;Microsoft;Microsoft;Microsoft;Peking University;Microsoft,24;24;-1;-1;-1;24;-1,27;27;-1;-1;-1;27;-1,3;8
502,502,502,502,502,502,502,502,ICLR,2018,On the importance of single directions for generalization,Ari S. Morcos;David G.T. Barrett;Neil C. Rabinowitz;Matthew Botvinick,arimorcos@google.com;barrettdavid@google.com;ncr@google.com;botvinick@google.com,7;5;9,3;4;3,Accept (Poster),0,5,1.0,yes,10/27/17,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,8
503,503,503,503,503,503,503,503,ICLR,2018,Variational Continual Learning,Cuong V. Nguyen;Yingzhen Li;Thang D. Bui;Richard E. Turner,vcn22@cam.ac.uk;yl494@cam.ac.uk;tdb40@cam.ac.uk;ret26@cam.ac.uk,6;6;6,3;4;2,Accept (Poster),0,4,0.0,yes,10/27/17,University of Cambridge;University of Cambridge;University of Cambridge;University of Cambridge,71;71;71;71,2;2;2;2,5
504,504,504,504,504,504,504,504,ICLR,2018,Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning,Rajarshi Das;Shehzaad Dhuliawala;Manzil Zaheer;Luke Vilnis;Ishan Durugkar;Akshay Krishnamurthy;Alex Smola;Andrew McCallum,rajarshi@cs.umass.edu;sdhuliawala@cs.umass.edu;manzil@cmu.edu;luke@cs.umass.edu;ishand@cs.utexas.edu;akshay@cs.umass.edu;alex@smola.org;mccallum@cs.umass.edu,7;6;5,4;4;4,Accept (Poster),8,16,0.0,yes,10/27/17,"University of Massachusetts, Amherst;University of Massachusetts, Amherst;Carnegie Mellon University;University of Massachusetts, Amherst;University of Texas, Austin;University of Massachusetts, Amherst;Carnegie-Mellon University;University of Massachusetts, Amherst",30;30;1;30;21;30;1;30,191;191;24;191;49;191;24;191,10
505,505,505,505,505,505,505,505,ICLR,2018,Learning to Represent Programs with Graphs,Miltiadis Allamanis;Marc Brockschmidt;Mahmoud Khademi,miallama@microsoft.com;mabrocks@microsoft.com;mkhademi@sfu.ca,8;8;8,4;4;4,Accept (Oral),0,10,0.0,yes,10/27/17,Microsoft;Microsoft;Simon Fraser University,-1;-1;57,-1;-1;253,3;10
506,506,506,506,506,506,506,506,ICLR,2018,Variational image compression with a scale hyperprior,Johannes Ballé;David Minnen;Saurabh Singh;Sung Jin Hwang;Nick Johnston,jballe@google.com;dminnen@google.com;saurabhsingh@google.com;sjhwang@google.com;nickj@google.com,7;7;7,4;5;5,Accept (Poster),1,9,1.0,yes,10/27/17,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,5
507,507,507,507,507,507,507,507,ICLR,2018,Simulating Action Dynamics with Neural Process Networks,Antoine Bosselut;Omer Levy;Ari Holtzman;Corin Ennis;Dieter Fox;Yejin Choi,antoineb@cs.washington.edu;omerlevy@cs.washington.edu;ahai@cs.washington.edu;corin123@uw.edu;fox@cs.washington.edu;yejin@cs.washington.edu,6;9;8,4;4;4,Accept (Poster),0,5,0.0,yes,10/27/17,"University of Washington;University of Washington;University of Washington;University of Washington, Seattle;University of Washington;University of Washington",6;6;6;6;6;6,25;25;25;25;25;25,
508,508,508,508,508,508,508,508,ICLR,2018,Evidence Aggregation for Answer Re-Ranking in Open-Domain Question Answering,Shuohang Wang;Mo Yu;Jing Jiang;Wei Zhang;Xiaoxiao Guo;Shiyu Chang;Zhiguo Wang;Tim Klinger;Gerald Tesauro;Murray Campbell,shwang.2014@phdis.smu.edu.sg;yum@us.ibm.com;jingjiang@smu.edu.sg;zhangwei@us.ibm.com;xiaoxiao.guo@ibm.com;shiyu.chang@ibm.com;zhigwang@us.ibm.com;tklinger@us.ibm.com;gtesauro@us.ibm.com;mcam@us.ibm.com,6;8;6,2;3;4,Accept (Poster),0,3,0.0,yes,10/27/17,Singapore Management University;International Business Machines;Singapore Management University;International Business Machines;International Business Machines;International Business Machines;International Business Machines;International Business Machines;International Business Machines;International Business Machines,90;-1;90;-1;-1;-1;-1;-1;-1;-1,1103;-1;1103;-1;-1;-1;-1;-1;-1;-1,
509,509,509,509,509,509,509,509,ICLR,2018,QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension,Adams Wei Yu;David Dohan;Minh-Thang Luong;Rui Zhao;Kai Chen;Mohammad Norouzi;Quoc V. Le,weiyu@cs.cmu.edu;ddohan@google.com;thangluong@google.com;rzhao@google.com;kaichen@google.com;mnorouzi@google.com;qvl@google.com,8;5;6,5;4;3,Accept (Poster),2,11,1.0,yes,10/27/17,Carnegie Mellon University;Google;Google;Google;Google;Google;Google,1;-1;-1;-1;-1;-1;-1,24;-1;-1;-1;-1;-1;-1,3
510,510,510,510,510,510,510,510,ICLR,2018,Measuring the Intrinsic Dimension of Objective Landscapes,Chunyuan Li;Heerad Farkhoor;Rosanne Liu;Jason Yosinski,chunyuan.li@duke.edu;heerad@uber.com;rosanne@uber.com;jason@yosinski.com,7;7;6,3;4;2,Accept (Poster),0,4,0.0,yes,10/27/17,Duke University;Uber;Uber;University of Montreal,46;-1;-1;124,17;-1;-1;108,1
511,511,511,511,511,511,511,511,ICLR,2018,Residual Loss Prediction: Reinforcement Learning With No Incremental Feedback,Hal Daumé III;John Langford;Amr Sharaf,hal@umiacs.umd.edu;jl@hunch.net;amr@cs.umd.edu,7;7;6,5;2;4,Accept (Poster),0,10,2.0,yes,10/27/17,"University of Maryland, College Park;;University of Maryland, College Park",12;-1;12,69;-1;69,
512,512,512,512,512,512,512,512,ICLR,2018,Reinforcement Learning Algorithm Selection,Romain Laroche;Raphael Feraud,romain.laroche@gmail.com;raphael.feraud@orange.com,6;6;7,5;3;4,Accept (Poster),0,0,0.0,yes,10/26/17,Microsoft;General Electric,-1;-1,-1;-1,
513,513,513,513,513,513,513,513,ICLR,2018,Feature Incay for Representation Regularization,Yuhui Yuan;Kuiyuan Yang;Jianyuan Guo;Jingdong Wang;Chao Zhang,yuyua@microsoft.com;kuiyuanyang@deepmotion.ai;1701214082@pku.edu.cn;jingdw@microsoft.com;chzhang@cis.pku.edu.cn,6;6;6,3;2;4,Invite to Workshop Track,0,3,1.0,yes,10/25/17,Microsoft;DeepMotion;Peking University;Microsoft;Peking University,-1;-1;24;-1;24,-1;-1;27;-1;27,
514,514,514,514,514,514,514,514,ICLR,2018,Zero-Shot Visual Imitation,Deepak Pathak;Parsa Mahmoudieh;Guanghao Luo;Pulkit Agrawal;Dian Chen;Yide Shentu;Evan Shelhamer;Jitendra Malik;Alexei A. Efros;Trevor Darrell,pathak@berkeley.edu;parsa.m@berkeley.edu;michaelluo@berkeley.edu;pulkitag@berkeley.edu;dianchen@berkeley.edu;fredshentu@berkeley.edu;shelhamer@cs.berkeley.edu;malik@eecs.berkeley.edu;efros@eecs.berkeley.edu;trevor@eecs.berkeley.edu,8;8;7,4;3;5,Accept (Oral),0,8,0.0,yes,10/27/17,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5;5;5;5;5;5;5,18;18;18;18;18;18;18;18;18;18,6
515,515,515,515,515,515,515,515,ICLR,2018,"Emergent Communication in a Multi-Modal, Multi-Step Referential Game",Katrina Evtimova;Andrew Drozdov;Douwe Kiela;Kyunghyun Cho,kve216@nyu.edu;apd283@nyu.edu;dkiela@fb.com;kyunghyun.cho@nyu.edu,7;7;7,3;4;4,Accept (Poster),0,3,0.0,yes,10/27/17,New York University;New York University;Facebook;New York University,26;26;-1;26,27;27;-1;27,3;8
516,516,516,516,516,516,516,516,ICLR,2018,Distributed Fine-tuning of Language Models on Private Data,Vadim Popov;Mikhail Kudinov;Irina Piontkovskaya;Petr Vytovtov;Alex Nevidomsky,v.popov@samsung.com;m.kudinov@samsung.com;p.irina@samsung.com;p.vytovtov@partner.samsung.com;a.nevidomsky@samsung.com,5;4;4,4;3;4,Accept (Poster),0,6,0.0,yes,10/24/17,Samsung;Samsung;Samsung;Samsung;Samsung,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,3
517,517,517,517,517,517,517,517,ICLR,2018,Preliminary theoretical troubleshooting in Variational Autoencoder,Shiqi Liu;Qian Zhao;Xiangyong Cao;Deyu Meng;Zilu Ma;Tao Yu,liushiqi@stu.xjtu.edu.cn;dymeng@mail.xjtu.edu.cn;timmy.zhaoqian@gmail.com;460376821@qq.com;1030884089@qq.com;602077855@qq.com,5;3;2,4;4;4,Reject,0,5,0.0,yes,10/27/17,Xi'an Jiaotong University;Xi'an Jiaotong University;Xi'an Jiaotong University;;;,468;468;468;-1;-1;-1,565;565;565;-1;-1;-1,5;1
518,518,518,518,518,518,518,518,ICLR,2018,On the Convergence of Adam and Beyond,Sashank J. Reddi;Satyen Kale;Sanjiv Kumar,sashank@google.com;satyenkale@google.com;sanjivk@google.com,9;8;8,5;4;3,Accept (Oral),2,10,13.0,yes,10/27/17,Google;Google;Google,-1;-1;-1,-1;-1;-1,9
519,519,519,519,519,519,519,519,ICLR,2018,Spatially Transformed Adversarial Examples,Chaowei Xiao;Jun-Yan Zhu;Bo Li;Warren He;Mingyan Liu;Dawn Song,xiaocw@umich.edu;junyanzhu89@gmail.com;lxbosky@gmail.com;_w@eecs.berkeley.edu;mingyan@umich.edu;dawnsong.travel@gmail.com,7;9;7,4;5;4,Accept (Poster),7,13,1.0,yes,10/27/17,University of Michigan;NAVER;University of California Berkeley;University of California Berkeley;University of Michigan;University of California Berkeley,8;-1;5;5;8;5,21;-1;18;18;21;18,4
520,520,520,520,520,520,520,520,ICLR,2018,Parametrized Hierarchical Procedures for Neural Programming,Roy Fox;Richard Shin;Sanjay Krishnan;Ken Goldberg;Dawn Song;Ion Stoica,roy.d.fox@gmail.com;shin.richard@gmail.com;sanjay@eecs.berkeley.edu;goldberg@berkeley.edu;dawnsong.travel@gmail.com;istoica@cs.berkeley.edu,6;6;6,3;1;2,Accept (Poster),0,8,0.0,yes,10/27/17,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5;5;5,18;18;18;18;18;18,
521,521,521,521,521,521,521,521,ICLR,2018,Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality,Xingjun Ma;Bo Li;Yisen Wang;Sarah M. Erfani;Sudanthi Wijewickrema;Grant Schoenebeck;Dawn Song;Michael E. Houle;James Bailey,xingjunm@student.unimelb.edu.au;crystalboli@berkeley.edu;wangys14@mails.tsinghua.edu.cn;sarah.erfani@unimelb.edu.au;sudanthi.wijewickrema@unimelb.edu.au;schoeneb@umich.edu;dawnsong.travel@gmail.com;meh@nii.ac.jp;baileyj@unimelb.edu.au,8;6;7,3;1;4,Accept (Oral),0,11,2.0,yes,10/24/17,The University of Melbourne;University of California Berkeley;Tsinghua University;The University of Melbourne;The University of Melbourne;University of Michigan;University of California Berkeley;Meiji University;The University of Melbourne,124;5;10;124;124;8;5;468;124,32;18;30;32;32;21;18;334;32,4;1
522,522,522,522,522,522,522,522,ICLR,2018,Generating Adversarial Examples with Adversarial Networks,Chaowei Xiao;Bo Li;Jun-Yan Zhu;Warren He;Mingyan Liu;Dawn Song,xiaocw@umich.edu;lxbosky@gmail.com;junyanz@berkeley.edu;_w@eecs.berkeley.edu;mingyan@umich.edu;dawnsong.travel@gmail.com,4;6;7,4;4;3,Reject,1,16,1.0,yes,10/27/17,University of Michigan;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of Michigan;University of California Berkeley,8;5;5;5;8;5,21;18;18;18;21;18,5;4
523,523,523,523,523,523,523,523,ICLR,2018,Decision Boundary Analysis of Adversarial Examples,Warren He;Bo Li;Dawn Song,_w@eecs.berkeley.edu;lxbosky@gmail.com;dawnsong.travel@gmail.com,6;6;6,3;2;3,Accept (Poster),0,4,0.0,yes,10/27/17,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,18;18;18,4
524,524,524,524,524,524,524,524,ICLR,2018,Learning Sparse Latent Representations with the Deep Copula Information Bottleneck,Aleksander Wieczorek*;Mario Wieser*;Damian Murezzan;Volker Roth,aleksander.wieczorek@unibas.ch;mario.wieser@unibas.ch;d.murezzan@unibas.ch;volker.roth@unibas.ch,5;6;6;6,4;3;3;1,Accept (Poster),0,2,0.0,yes,10/27/17,University of Basel;University of Basel;University of Basel;University of Basel,364;364;364;364,95;95;95;95,
525,525,525,525,525,525,525,525,ICLR,2018,Demystifying MMD GANs,Mikołaj Bińkowski;Dougal J. Sutherland;Michael Arbel;Arthur Gretton,mikbinkowski@gmail.com;dougal@gmail.com;michael.n.arbel@gmail.com;arthur.gretton@gmail.com,4;7;6,4;4;2,Accept (Poster),1,6,0.0,yes,10/27/17,Imperial College London;University College London;University College London;,74;46;46;-1,8;16;16;-1,5;4
526,526,526,526,526,526,526,526,ICLR,2018,Fix your classifier: the marginal value of training the last weight layer,Elad Hoffer;Itay Hubara;Daniel Soudry,elad.hoffer@gmail.com;itayhubara@gmail.com;daniel.soudry@gmail.com,6;6;6,4;5;3,Accept (Poster),2,4,3.0,yes,10/27/17,Technion;;Technion,24;-1;24,327;-1;327,
527,527,527,527,527,527,527,527,ICLR,2018,Flipout: Efficient Pseudo-Independent Weight Perturbations on Mini-Batches,Yeming Wen;Paul Vicol;Jimmy Ba;Dustin Tran;Roger Grosse,wenyemin@cs.toronto.edu;pvicol@cs.toronto.edu;jimmy@psi.toronto.edu;trandustin@google.com;rgrosse@cs.toronto.edu,6;8;6,4;3;4,Accept (Poster),0,6,0.0,yes,10/27/17,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;University of Toronto;Google;Department of Computer Science, University of Toronto",17;17;17;-1;17,22;22;22;-1;22,11
528,528,528,528,528,528,528,528,ICLR,2018,Tree-to-tree Neural Networks for Program Translation,Xinyun Chen;Chang Liu;Dawn Song,xinyun.chen@berkeley.edu;liuchang@eecs.berkeley.edu;dawnsong.travel@gmail.com,6;4;4,4;3;4,Invite to Workshop Track,0,6,0.0,yes,10/27/17,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,18;18;18,
529,529,529,529,529,529,529,529,ICLR,2018,Towards Synthesizing Complex Programs From Input-Output Examples,Xinyun Chen;Chang Liu;Dawn Song,xinyun.chen@berkeley.edu;liuchang@eecs.berkeley.edu;dawnsong.travel@gmail.com,8;7;5,3;4;2,Accept (Poster),0,9,0.0,yes,10/27/17,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,18;18;18,
530,530,530,530,530,530,530,530,ICLR,2018,A DIRT-T Approach to Unsupervised Domain Adaptation,Rui Shu;Hung Bui;Hirokazu Narui;Stefano Ermon,ruishu@stanford.edu;buih@google.com;hirokaz2@stanford.edu;ermon@cs.stanford.edu,8;7;7,4;4;2,Accept (Poster),0,5,0.0,yes,10/27/17,Stanford University;Google;Stanford University;Stanford University,4;-1;4;4,3;-1;3;3,5;4
531,531,531,531,531,531,531,531,ICLR,2018,SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data,Alon Brutzkus;Amir Globerson;Eran Malach;Shai Shalev-Shwartz,alonbrutzkus@mail.tau.ac.il;amir.globerson@gmail.com;eran.malach@mail.huji.ac.il;shais@cs.huji.ac.il,7;7;8,3;3;4,Accept (Poster),0,1,0.0,yes,10/27/17,Tel Aviv University;Tel Aviv University;Hebrew University of Jerusalem;Hebrew University of Jerusalem,37;37;62;62,217;217;205;205,1;9;8
532,532,532,532,532,532,532,532,ICLR,2018,NerveNet: Learning Structured Policy with Graph Neural Networks,Tingwu Wang;Renjie Liao;Jimmy Ba;Sanja Fidler,tingwuwang@cs.toronto.edu;rjliao@cs.toronto.edu;jimmy@psi.toronto.edu;fidler@cs.toronto.edu,7;6;7,3;3;3,Accept (Poster),0,7,0.0,yes,10/27/17,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;University of Toronto;Department of Computer Science, University of Toronto",17;17;17;17,22;22;22;22,6;10
533,533,533,533,533,533,533,533,ICLR,2018,Matrix capsules with EM routing,Geoffrey E Hinton;Sara Sabour;Nicholas Frosst,geoffhinton@google.com;sasabour@google.com;frosst@google.com,7;6;4,3;3;2,Accept (Poster),10,10,18.0,yes,10/27/17,Google;Google;Google,-1;-1;-1,-1;-1;-1,4
534,534,534,534,534,534,534,534,ICLR,2018,Towards better understanding of gradient-based attribution methods for Deep Neural Networks,Marco Ancona;Enea Ceolini;Cengiz Öztireli;Markus Gross,marco.ancona@inf.ethz.ch;enea.ceolini@ini.uzh.ch;cengizo@inf.ethz.ch;grossm@inf.ethz.ch,7;6;7,3;5;4,Accept (Poster),0,3,0.0,yes,10/26/17,Swiss Federal Institute of Technology;University of Zurich;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,9;139;9;9,10;136;10;10,1
535,535,535,535,535,535,535,535,ICLR,2018,Linearly Constrained Weights: Resolving the Vanishing Gradient Problem by Reducing Angle Bias,Takuro Kutsuna,kutsuna@mosk.tytlabs.co.jp,5;5;4,4;4;4,Reject,0,6,0.0,yes,10/24/17,Toyota Central R&D Labs. Inc.,-1,-1,
536,536,536,536,536,536,536,536,ICLR,2018,Understanding Short-Horizon Bias in Stochastic Meta-Optimization,Yuhuai Wu;Mengye Ren;Renjie Liao;Roger Grosse.,ywu@cs.toronto.edu;mren@cs.toronto.edu;rjliao@cs.toronto.edu;rgrosse@cs.toronto.edu,7;6;8,4;4;3,Accept (Poster),0,5,0.0,yes,10/27/17,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",17;17;17;17,22;22;22;22,
537,537,537,537,537,537,537,537,ICLR,2018,Memorization Precedes Generation: Learning Unsupervised GANs with Memory Networks,Youngjin Kim;Minjung Kim;Gunhee Kim,youngjin.kim@vision.snu.ac.kr;minjung.kim1994@gmail.com;gunhee@snu.ac.kr,6;6;7,4;4;4,Accept (Poster),0,5,0.0,yes,10/26/17,Seoul National University;;Seoul National University,46;-1;46,74;-1;74,5;4
538,538,538,538,538,538,538,538,ICLR,2018,Universal Agent for Disentangling Environments and Tasks,Jiayuan Mao;Honghua Dong;Joseph J. Lim,mjy14@mails.tsinghua.edu.cn;dhh14@mails.tsinghua.edu.cn;limjj@usc.edu,6;7;6,3;4;3,Accept (Poster),0,0,0.0,yes,10/27/17,Tsinghua University;Tsinghua University;University of Southern California,10;10;31,30;30;66,
539,539,539,539,539,539,539,539,ICLR,2018,Breaking the Softmax Bottleneck: A High-Rank RNN Language Model,Zhilin Yang;Zihang Dai;Ruslan Salakhutdinov;William W. Cohen,zhiliny@cs.cmu.edu;zander.dai@gmail.com;rsalakhu@cs.cmu.edu;wcohen@cs.cmu.edu,7;7;8,5;4;4,Accept (Oral),7,12,0.0,yes,10/27/17,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1,24;24;24;24,3
540,540,540,540,540,540,540,540,ICLR,2018,Meta-Learning for Semi-Supervised Few-Shot Classification,Mengye Ren;Eleni Triantafillou;Sachin Ravi;Jake Snell;Kevin Swersky;Joshua B. Tenenbaum;Hugo Larochelle;Richard S. Zemel,mren@cs.toronto.edu;eleni@cs.toronto.edu;sachinr@princeton.edu;jsnell@cs.toronto.edu;kswersky@google.com;jbt@mit.edu;hugolarochelle@google.com;zemel@cs.toronto.edu,6;6;6,5;4;4,Accept (Poster),0,4,0.0,yes,10/27/17,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Princeton University;Department of Computer Science, University of Toronto;Google;Massachusetts Institute of Technology;Google;Department of Computer Science, University of Toronto",17;17;31;17;-1;2;-1;17,22;22;7;22;-1;5;-1;22,6
541,541,541,541,541,541,541,541,ICLR,2018,Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields,Thomas Unterthiner;Bernhard Nessler;Calvin Seward;Günter Klambauer;Martin Heusel;Hubert Ramsauer;Sepp Hochreiter,unterthiner@bioinf.jku.at;nessler@bioinf.jku.at;seward@bioinf.jku.at;klambauer@bioinf.jku.at;mheusel@gmail.com;ramsauer@bioinf.jku.at;hochreit@bioinf.jku.at,5;7;7,4;3;2,Accept (Poster),6,4,0.0,yes,10/27/17,Johannes Kepler University Linz;Johannes Kepler University Linz;Johannes Kepler University Linz;Johannes Kepler University Linz;Johannes Kepler University Linz;Johannes Kepler University Linz;Johannes Kepler University Linz,468;468;468;468;468;468;468,538;538;538;538;538;538;538,5;4;1
542,542,542,542,542,542,542,542,ICLR,2018,Memory Architectures in Recurrent Neural Network Language Models,Dani Yogatama;Yishu Miao;Gabor Melis;Wang Ling;Adhiguna Kuncoro;Chris Dyer;Phil Blunsom,dyogatama@google.com;yishu.miao@cs.ox.ac.uk;melisgl@google.com;lingwang@google.com;akuncoro@google.com;cdyer@google.com;pblunsom@google.com,6;5;8,3;5;5,Accept (Poster),2,4,0.0,yes,10/27/17,Google;University of Oxford;Google;Google;Google;Google;Google,-1;51;-1;-1;-1;-1;-1,-1;1;-1;-1;-1;-1;-1,3;8
543,543,543,543,543,543,543,543,ICLR,2018,Mitigating Adversarial Effects Through Randomization,Cihang Xie;Jianyu Wang;Zhishuai Zhang;Zhou Ren;Alan Yuille,cihangxie306@gmail.com;wjyouch@gmail.com;zhshuai.zhang@gmail.com;zhou.ren@snapchat.com;alan.l.yuille@gmail.com,6;7;6,4;4;3,Accept (Poster),5,4,0.0,yes,10/27/17,Johns Hopkins University;Baidu;Johns Hopkins University;Snap Inc.;Johns Hopkins University,71;-1;71;-1;71,13;-1;13;-1;13,4
544,544,544,544,544,544,544,544,ICLR,2018,Learning Awareness Models,Brandon Amos;Laurent Dinh;Serkan Cabi;Thomas Rothörl;Sergio Gómez Colmenarejo;Alistair Muldal;Tom Erez;Yuval Tassa;Nando de Freitas;Misha Denil,bamos@cs.cmu.edu;dinh.laurent@gmail.com;cabi@google.com;tcr@google.com;sergomez@google.com;alimuldal@google.com;etom@google.com;tassa@google.com;nandodefreitas@google.com;mdenil@google.com,7;4;4,4;4;5,Accept (Poster),0,7,0.0,yes,10/27/17,Carnegie Mellon University;Google;Google;Google;Google;Google;Google;Google;Google;Google,1;-1;-1;-1;-1;-1;-1;-1;-1;-1,24;-1;-1;-1;-1;-1;-1;-1;-1;-1,
545,545,545,545,545,545,545,545,ICLR,2018,Generating Wikipedia by Summarizing Long Sequences,Peter J. Liu*;Mohammad Saleh*;Etienne Pot;Ben Goodrich;Ryan Sepassi;Lukasz Kaiser;Noam Shazeer,peterjliu@google.com;msaleh@google.com;epot@google.com;bgoodrich@google.com;rsepassi@google.com;lukaszkaiser@google.com;noam@google.com,7;8;7,5;3;4,Accept (Poster),0,5,0.0,yes,10/27/17,Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,
546,546,546,546,546,546,546,546,ICLR,2018,MaskGAN: Better Text Generation via Filling in the _______,William Fedus;Ian Goodfellow;Andrew M. Dai,liam.fedus@gmail.com;goodfellow@google.com;adai@google.com,7;7;7,4;3;5,Accept (Poster),0,8,0.0,yes,10/27/17,University of Montreal;Google;Google,124;-1;-1,108;-1;-1,3;4;5
547,547,547,547,547,547,547,547,ICLR,2018,Detecting Statistical Interactions from Neural Network Weights,Michael Tsang;Dehua Cheng;Yan Liu,tsangm@usc.edu;dehuache@usc.edu;yanliu.cs@usc.edu,7;7;7,4;4;5,Accept (Poster),0,7,0.0,yes,10/26/17,University of Southern California;University of Southern California;University of Southern California,31;31;31,66;66;66,
548,548,548,548,548,548,548,548,ICLR,2018,Model-Ensemble Trust-Region Policy Optimization,Thanard Kurutach;Ignasi Clavera;Yan Duan;Aviv Tamar;Pieter Abbeel,thanard.kurutach@berkeley.edu;iclavera@berkeley.edu;rockyduan@eecs.berkeley.edu;avivt@berkeley.edu;pabbeel@cs.berkeley.edu,7;6;7,4;3;5,Accept (Poster),13,4,0.0,yes,10/27/17,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5;5,18;18;18;18;18,
549,549,549,549,549,549,549,549,ICLR,2018,Compositional Obverter Communication Learning from Raw Visual Input,Edward Choi;Angeliki Lazaridou;Nando de Freitas,mp2893@gatech.edu;angeliki@google.com;nandodefreitas@google.com,9;3;6,4;4;3,Accept (Poster),0,7,0.0,yes,10/27/17,Georgia Institute of Technology;Google;Google,13;-1;-1,33;-1;-1,6
550,550,550,550,550,550,550,550,ICLR,2018,Quantitatively Evaluating GANs With Divergences Proposed for Training,Daniel Jiwoong Im;He Ma;Graham W. Taylor;Kristin Branson,daniel.im@aifounded.com;hma02@uoguelph.ca;gwtaylor@uoguelph.ca;kristinbranson@gmail.com,7;4;7,5;3;4,Accept (Poster),3,7,0.0,yes,10/27/17,Aifounded;University of Guelph;University of Guelph;Janelia Farm Research Campus- HHMI,-1;291;291;-1,-1;1103;1103;-1,5;4
551,551,551,551,551,551,551,551,ICLR,2018,Apprentice: Using Knowledge Distillation Techniques To Improve Low-Precision Network Accuracy,Asit Mishra;Debbie Marr,asit.k.mishra@intel.com;debbie.marr@intel.com,7;7;8,4;4;4,Accept (Poster),0,5,1.0,yes,10/27/17,Intel;Intel,-1;-1,-1;-1,2
552,552,552,552,552,552,552,552,ICLR,2018,Semi-parametric topological memory for navigation,Nikolay Savinov;Alexey Dosovitskiy;Vladlen Koltun,nikolay.savinov@inf.ethz.ch;adosovitskiy@gmail.com;vkoltun@gmail.com,7;3;7,5;4;4,Accept (Poster),0,11,0.0,yes,10/27/17,Swiss Federal Institute of Technology;Intel;Intel,9;-1;-1,10;-1;-1,10
553,553,553,553,553,553,553,553,ICLR,2018,Automatically Inferring Data Quality for Spatiotemporal Forecasting,Sungyong Seo;Arash Mohegh;George Ban-Weiss;Yan Liu,sungyons@usc.edu;mohegh@usc.edu;banweiss@usc.edu;yanliu.cs@usc.edu,6;6;8,3;4;4,Accept (Poster),0,9,0.0,yes,10/24/17,University of Southern California;University of Southern California;University of Southern California;University of Southern California,31;31;31;31,66;66;66;66,10
554,554,554,554,554,554,554,554,ICLR,2018,"Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks",Pratik Chaudhari;Stefano Soatto,pratikac@ucla.edu;soatto@ucla.edu,8;5;6,5;4;4,Accept (Poster),0,6,0.0,yes,10/27/17,"University of California, Los Angeles;University of California, Los Angeles",20;20,15;15,1
555,555,555,555,555,555,555,555,ICLR,2018,Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning,Tianmin Shu;Caiming Xiong;Richard Socher,tianmin.shu@ucla.edu;cxiong@salesforce.com;richard@socher.org,6;6;6,4;3;3,Accept (Poster),0,5,0.0,yes,10/27/17,"University of California, Los Angeles;SalesForce.com;SalesForce.com",20;-1;-1,15;-1;-1,
556,556,556,556,556,556,556,556,ICLR,2018,Not-So-Random Features,Brian Bullins;Cyril Zhang;Yi Zhang,bbullins@cs.princeton.edu;cyril.zhang@cs.princeton.edu;y.zhang@cs.princeton.edu,7;6;4,3;5;5,Accept (Poster),0,6,0.0,yes,10/27/17,Princeton University;Princeton University;Princeton University,31;31;31,7;7;7,8
557,557,557,557,557,557,557,557,ICLR,2018,Learning a Generative Model for Validity in Complex Discrete Structures,Dave Janz;Jos van der Westhuizen;Brooks Paige;Matt Kusner;José Miguel Hernández-Lobato,david.janz93@gmail.com;josvdwest@gmail.com;tbpaige@gmail.com;matt.kusner@gmail.com;jmh233@cam.ac.uk,6;7;7,4;3;3,Accept (Poster),0,11,0.0,yes,10/27/17,University of Cambridge;;Alan Turing Institute;Alan Turing Institute;University of Cambridge,71;-1;-1;-1;71,2;-1;-1;-1;2,5
558,558,558,558,558,558,558,558,ICLR,2018,Learning to Multi-Task by Active Sampling,Sahil Sharma*;Ashutosh Kumar Jha*;Parikshit S Hegde;Balaraman Ravindran,sahil@cse.iitm.ac.in;me14b148@smail.iitm.ac.in;ee14b123@ee.iitm.ac.in;ravi@cse.iitm.ac.in,5;7;7,3;3;5,Accept (Poster),3,5,0.0,yes,10/27/17,Indian Institute of Technology Madras;Indian Institute of Technology Madras;Indian Institute of Technology Madras;Indian Institute of Technology Madras,153;153;153;153,625;625;625;625,6
559,559,559,559,559,559,559,559,ICLR,2018,Consequentialist conditional cooperation in social dilemmas with imperfect information,Alexander Peysakhovich;Adam Lerer,alexpeys@gmail.com;alerer@fb.com,7;5;6,3;4;4,Accept (Poster),0,4,0.0,yes,10/17/17,Facebook;Facebook,-1;-1,-1;-1,
560,560,560,560,560,560,560,560,ICLR,2018,MGAN: Training Generative Adversarial Nets with Multiple Generators,Quan Hoang;Tu Dinh Nguyen;Trung Le;Dinh Phung,qhoang@umass.edu;tu.nguyen@deakin.edu.au;trung.l@deakin.edu.au;dinh.phung@deakin.edu.au,5;7;6,4;5;3,Accept (Poster),0,10,15.0,yes,10/25/17,"University of Massachusetts, Amherst;Deakin University;Deakin University;Deakin University",30;468;468;468,191;334;334;334,5;4;1
561,561,561,561,561,561,561,561,ICLR,2018,Residual Connections Encourage Iterative Inference,Stanisław Jastrzebski;Devansh Arpit;Nicolas Ballas;Vikas Verma;Tong Che;Yoshua Bengio,staszek.jastrzebski@gmail.com;devansharpit@gmail.com;ballas.n@gmail.com;vikasverma.iitm@gmail.com;tongcheprivate@gmail.com;yoshua.umontreal@gmail.com,6;5;7,3;5;4,Accept (Poster),0,5,0.0,yes,10/27/17,University of Montreal;University of Montreal;University of Montreal;;University of Montreal;University of Montreal,124;124;124;-1;124;124,108;108;108;-1;108;108,8
562,562,562,562,562,562,562,562,ICLR,2018,Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis,Rudy Bunel;Matthew Hausknecht;Jacob Devlin;Rishabh Singh;Pushmeet Kohli,rudy@robots.ox.ac.uk;mahauskn@microsoft.com;jacobdevlin@google.com;risin@microsoft.com;pushmeet@google.com,5;6;7,3;3;3,Accept (Poster),0,5,2.0,yes,10/26/17,University of Oxford;Microsoft;Google;Microsoft;Google,51;-1;-1;-1;-1,1;-1;-1;-1;-1,3
563,563,563,563,563,563,563,563,ICLR,2018,Stochastic Activation Pruning for Robust Adversarial Defense,Guneet S. Dhillon;Kamyar Azizzadenesheli;Zachary C. Lipton;Jeremy D. Bernstein;Jean Kossaifi;Aran Khanna;Animashree Anandkumar,guneetdhillon@utexas.edu;kazizzad@uci.edu;zlipton@cmu.edu;bernstein@caltech.edu;jean.kossaifi@gmail.com;arankhan@amazon.com;animakumar@gmail.com,6;7;6,3;4;4,Accept (Poster),0,4,0.0,yes,10/27/17,"University of Texas, Austin;University of California, Irvine;Carnegie Mellon University;California Institute of Technology;Imperial College London;Amazon;University of California-Irvine",21;36;1;139;74;-1;36,49;99;24;3;8;-1;99,4
564,564,564,564,564,564,564,564,ICLR,2018,Online Learning Rate Adaptation with Hypergradient Descent,Atilim Gunes Baydin;Robert Cornish;David Martinez Rubio;Mark Schmidt;Frank Wood,gunes@robots.ox.ac.uk;rcornish@robots.ox.ac.uk;david.martinez2@wadh.ox.ac.uk;schmidtm@cs.ubc.ca;fwood@robots.ox.ac.uk,6;7;7,4;3;4,Accept (Poster),1,8,0.0,yes,10/27/17,University of Oxford;University of Oxford;University of Oxford;University of British Columbia;University of Oxford,51;51;51;34;51,1;1;1;34;1,9
565,565,565,565,565,565,565,565,ICLR,2018,Temporal Difference Models: Model-Free Deep RL for Model-Based Control,Vitchyr Pong*;Shixiang Gu*;Murtaza Dalal;Sergey Levine,vitchyr@berkeley.edu;sg717@cam.ac.uk;mdalal@berkeley.edu;svlevine@eecs.berkeley.edu,7;4;7,4;4;3,Accept (Poster),0,3,2.0,yes,10/27/17,University of California Berkeley;University of Cambridge;University of California Berkeley;University of California Berkeley,5;71;5;5,18;2;18;18,
566,566,566,566,566,566,566,566,ICLR,2018,Generative Models of Visually Grounded Imagination,Ramakrishna Vedantam;Ian Fischer;Jonathan Huang;Kevin Murphy,vrama@gatech.edu;iansf@google.com;jonathanhuang@google.com;murphyk@gmail.com,7;7;7,3;4;3,Accept (Poster),0,6,0.0,yes,10/27/17,Georgia Institute of Technology;Google;Google;Google,13;-1;-1;-1,33;-1;-1;-1,5
567,567,567,567,567,567,567,567,ICLR,2018,Spherical CNNs,Taco S. Cohen;Mario Geiger;Jonas Köhler;Max Welling,taco.cohen@gmail.com;geiger.mario@gmail.com;jonas.koehler.ks@gmail.com;m.welling@uva.nl,8;7;9,4;3;4,Accept (Oral),0,7,3.0,yes,10/27/17,University of Amsterdam;Swiss Federal Institute of Technology Lausanne;;University of Amsterdam,181;468;-1;181,59;38;-1;59,1
568,568,568,568,568,568,568,568,ICLR,2018,Neural Speed Reading via Skim-RNN,Minjoon Seo;Sewon Min;Ali Farhadi;Hannaneh Hajishirzi,minjoon@cs.washington.edu;shmsw25@snu.ac.kr;ali@cs.washington.edu;hannaneh@washington.edu,7;7;8,3;3;3,Accept (Poster),3,10,0.0,yes,10/27/17,University of Washington;Seoul National University;University of Washington;University of Washington,6;46;6;6,25;74;25;25,3
569,569,569,569,569,569,569,569,ICLR,2018,On Unifying Deep Generative Models,Zhiting Hu;Zichao Yang;Ruslan Salakhutdinov;Eric P. Xing,zhitinghu@gmail.com;yangtze2301@gmail.com;rsalakhu@cs.cmu.edu;epxing@cs.cmu.edu,7;6;7,4;4;3,Accept (Poster),0,4,0.0,yes,10/27/17,Carnegie Mellon University;;Carnegie Mellon University;Carnegie Mellon University,1;-1;1;1,24;-1;24;24,5;4
570,570,570,570,570,570,570,570,ICLR,2018,Cascade Adversarial Machine Learning Regularized with a Unified Embedding,Taesik Na;Jong Hwan Ko;Saibal Mukhopadhyay,taesik.na@gatech.edu;jonghwan.ko@gatech.edu;saibal.mukhopadhyay@ece.gatech.edu,6;6;5,4;4;4,Accept (Poster),0,5,0.0,yes,10/24/17,Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology,13;13;13,33;33;33,4
571,571,571,571,571,571,571,571,ICLR,2018,Variance Reduction for Policy Gradient with Action-Dependent Factorized Baselines,Cathy Wu;Aravind Rajeswaran;Yan Duan;Vikash Kumar;Alexandre M Bayen;Sham Kakade;Igor Mordatch;Pieter Abbeel,cathywu@eecs.berkeley.edu;aravraj@cs.washington.edu;rockyduan@eecs.berkeley.edu;vikash@cs.washington.edu;bayen@berkeley.edu;sham@cs.washington.edu;igor.mordatch@gmail.com;pabbeel@cs.berkeley.edu,7;8;6,4;3;4,Accept (Oral),0,4,2.0,yes,10/27/17,University of California Berkeley;University of Washington;University of California Berkeley;University of Washington;University of California Berkeley;University of Washington;University of Washington;University of California Berkeley,5;6;5;6;5;6;6;5,18;25;18;25;18;25;25;18,
572,572,572,572,572,572,572,572,ICLR,2018,Lifelong Learning with Dynamically Expandable Networks,Jaehong Yoon;Eunho Yang;Jeongtae Lee;Sung Ju Hwang,mmvc98@unist.ac.kr;eunhoy@kaist.ac.kr;jtlee@unist.ac.kr;sjhwang82@kaist.ac.kr,7;6;8,3;3;2,Accept (Poster),0,4,0.0,yes,10/27/17,Ulsan National Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Ulsan National Institute of Science and Technology;Korea Advanced Institute of Science and Technology,468;21;468;21,230;95;230;95,
573,573,573,573,573,573,573,573,ICLR,2018,A Simple Neural Attentive Meta-Learner,Nikhil Mishra;Mostafa Rohaninejad;Xi Chen;Pieter Abbeel,nmishra@berkeley.edu;rohaninejadm@berkeley.edu;adslcx@gmail.com;pabbeel@gmail.com,7;6;6,4;3;3,Accept (Poster),1,7,0.0,yes,10/27/17,University of California Berkeley;University of California Berkeley;covariant.ai;University of California-Berkeley,5;5;-1;5,18;18;-1;18,6;8
574,574,574,574,574,574,574,574,ICLR,2018,Emergence of grid-like representations by training recurrent neural networks to perform spatial localization,Christopher J. Cueva;Xue-Xin Wei,ccueva@gmail.com;weixxpku@gmail.com,8;9;8,4;4;4,Accept (Poster),0,4,0.0,yes,10/27/17,Columbia University;Columbia University,15;15,14;14,
575,575,575,575,575,575,575,575,ICLR,2018,Certified Defenses against Adversarial Examples ,Aditi Raghunathan;Jacob Steinhardt;Percy Liang,aditir@stanford.edu;jsteinhardt@cs.stanford.edu;pliang@cs.stanford.edu,8;8;5,4;4;3,Accept (Poster),4,5,1.0,yes,10/27/17,Stanford University;Stanford University;Stanford University,4;4;4,3;3;3,4
576,576,576,576,576,576,576,576,ICLR,2018,Emergence of Linguistic Communication from  Referential Games with Symbolic and Pixel Input,Angeliki Lazaridou;Karl Moritz Hermann;Karl Tuyls;Stephen Clark,angeliki@google.com;kmh@google.com;karltuyls@google.com;clarkstephen@google.com,7;9;5,4;5;4,Accept (Oral),0,6,0.0,yes,10/27/17,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,
577,577,577,577,577,577,577,577,ICLR,2018,Unsupervised Representation Learning by Predicting Image Rotations,Spyros Gidaris;Praveer Singh;Nikos Komodakis,spyros.gidaris@enpc.fr;praveer.singh@enpc.fr;nikos.komodakis@enpc.fr,6;6;6,5;4;3,Accept (Poster),0,7,0.0,yes,10/27/17,ENPC;ENPC;ENPC,468;468;468,280;280;280,2
578,578,578,578,578,578,578,578,ICLR,2018,Learn to Pay Attention,Saumya Jetley;Nicholas A. Lord;Namhoon Lee;Philip H. S. Torr,saumya.jetley@stx.ox.ac.uk;nicklord@robots.ox.ac.uk;namhoon.lee@eng.ox.ac.uk;philip.torr@eng.ox.ac.uk,5;6;6,4;4;4,Accept (Poster),6,8,0.0,yes,10/27/17,University of Oxford;University of Oxford;University of Oxford;University of Oxford,51;51;51;51,1;1;1;1,4;2
579,579,579,579,579,579,579,579,ICLR,2018,Auto-Encoding Sequential Monte Carlo,Tuan Anh Le;Maximilian Igl;Tom Rainforth;Tom Jin;Frank Wood,tuananh@robots.ox.ac.uk;maximilian.igl@gmail.com;twgr@robots.ox.ac.uk;tom@jin.me.uk;fwood@robots.ox.ac.uk,7;3;7,3;2;4,Accept (Poster),0,7,0.0,yes,10/27/17,University of Oxford;University of Oxford;University of Oxford;University of Oxford;University of Oxford,51;51;51;51;51,1;1;1;1;1,5;1
580,580,580,580,580,580,580,580,ICLR,2018,Divide-and-Conquer Reinforcement Learning,Dibya Ghosh;Avi Singh;Aravind Rajeswaran;Vikash Kumar;Sergey Levine,dibya.ghosh@berkeley.edu;avisingh@cs.berkeley.edu;aravraj@cs.washington.edu;vikash@cs.washington.edu;svlevine@eecs.berkeley.edu,7;7;4,4;4;4,Accept (Poster),0,4,1.0,yes,10/27/17,University of California Berkeley;University of California Berkeley;University of Washington;University of Washington;University of California Berkeley,5;5;6;6;5,18;18;25;25;18,
581,581,581,581,581,581,581,581,ICLR,2018,SMASH: One-Shot Model Architecture Search through HyperNetworks,Andrew Brock;Theo Lim;J.M. Ritchie;Nick Weston,ajb5@hw.ac.uk;t.lim@hw.ac.uk;j.m.ritchie@hw.ac.uk;nick.weston@renishaw.com,7;7;6,3;4;2,Accept (Poster),0,2,0.0,yes,9/29/17,Heriot-Watt University;Heriot-Watt University;Heriot-Watt University;Renishaw,291;291;291;-1,363;363;363;-1,
582,582,582,582,582,582,582,582,ICLR,2018,Recasting Gradient-Based Meta-Learning as Hierarchical Bayes,Erin Grant;Chelsea Finn;Sergey Levine;Trevor Darrell;Thomas Griffiths,eringrant@berkeley.edu;cbfinn@eecs.berkeley.edu;svlevine@eecs.berkeley.edu;trevor@eecs.berkeley.edu;tom_griffiths@berkeley.edu,6;7;7,3;3;3,Accept (Poster),1,8,0.0,yes,10/26/17,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5;5,18;18;18;18;18,11;6
583,583,583,583,583,583,583,583,ICLR,2018,Neural Sketch Learning for Conditional Program Generation,Vijayaraghavan Murali;Letao Qi;Swarat Chaudhuri;Chris Jermaine,vijay@rice.edu;letao.qi@rice.edu;swarat@rice.edu;cmj4@rice.edu,7;8;7,2;4;3,Accept (Oral),0,4,0.0,yes,10/27/17,Rice University;Rice University;Rice University;Rice University,85;85;85;85,86;86;86;86,
584,584,584,584,584,584,584,584,ICLR,2018,Synthetic and Natural Noise Both Break Neural Machine Translation,Yonatan Belinkov;Yonatan Bisk,belinkov@mit.edu;ybisk@yonatanbisk.com,7;7;8,4;4;4,Accept (Oral),0,7,0.0,yes,10/27/17,Massachusetts Institute of Technology;University of Washington,2;6,5;25,3
585,585,585,585,585,585,585,585,ICLR,2018,Policy Optimization by Genetic Distillation ,Tanmay Gangwani;Jian Peng,gangwan2@illinois.edu;jianpeng@illinois.edu,8;6;3,5;4;4,Accept (Poster),0,9,0.0,yes,10/27/17,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",3;3,37;37,
586,586,586,586,586,586,586,586,ICLR,2018,Active Neural Localization,Devendra Singh Chaplot;Emilio Parisotto;Ruslan Salakhutdinov,chaplot@cs.cmu.edu;eparisot@andrew.cmu.edu;rsalakhu@cs.cmu.edu,6;8;7,4;5;4,Accept (Poster),0,4,1.0,yes,10/25/17,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1,24;24;24,
587,587,587,587,587,587,587,587,ICLR,2018,Reinforcement Learning on Web Interfaces using Workflow-Guided Exploration,Evan Zheran Liu;Kelvin Guu;Panupong Pasupat;Tianlin Shi;Percy Liang,evzliu@gmail.com;kguu@stanford.edu;ppasupat@cs.stanford.edu;tianlins@cs.stanford.edu;pliang@cs.stanford.edu,7;6;7,4;3;3,Accept (Poster),0,8,0.0,yes,10/27/17,Stanford University;Stanford University;Stanford University;Stanford University;Stanford University,4;4;4;4;4,3;3;3;3;3,
588,588,588,588,588,588,588,588,ICLR,2018,Training GANs with Optimism,Constantinos Daskalakis;Andrew Ilyas;Vasilis Syrgkanis;Haoyang Zeng,costis@mit.edu;ailyas@mit.edu;vasy@microsoft.com;haoyangz@mit.edu,7;8;6,4;4;4,Accept (Poster),3,6,0.0,yes,10/27/17,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Microsoft;Massachusetts Institute of Technology,2;2;-1;2,5;5;-1;5,5;4;1
589,589,589,589,589,589,589,589,ICLR,2018,"A Compressed Sensing View of Unsupervised Text Embeddings, Bag-of-n-Grams, and LSTMs",Sanjeev Arora;Mikhail Khodak;Nikunj Saunshi;Kiran Vodrahalli,arora@cs.princeton.edu;mkhodak@princeton.edu;nsaunshi@cs.princeton.edu;kiran.vodrahalli@columbia.edu,7;7;6,3;1;4,Accept (Poster),0,6,0.0,yes,10/27/17,Princeton University;Princeton University;Princeton University;Columbia University,31;31;31;15,7;7;7;14,1
590,590,590,590,590,590,590,590,ICLR,2018,Scalable Private Learning with PATE,Nicolas Papernot;Shuang Song;Ilya Mironov;Ananth Raghunathan;Kunal Talwar;Ulfar Erlingsson,ngp5056@cse.psu.edu;shs037@eng.ucsd.edu;mironov@google.com;pseudorandom@google.com;kunal@google.com;ulfar@google.com,6;6;7,1;4;3,Accept (Poster),0,3,0.0,yes,10/27/17,"Pennsylvania State University;University of California, San Diego;Google;Google;Google;Google",40;11;-1;-1;-1;-1,77;31;-1;-1;-1;-1,1
591,591,591,591,591,591,591,591,ICLR,2018,Deep Bayesian Bandits Showdown:  An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling,Carlos Riquelme;George Tucker;Jasper Snoek,rikel@google.com;gjt@google.com;jsnoek@google.com,5;7;6,5;4;4,Accept (Poster),0,9,0.0,yes,10/27/17,Google;Google;Google,-1;-1;-1,-1;-1;-1,11
592,592,592,592,592,592,592,592,ICLR,2018,Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training,Yujun Lin;Song Han;Huizi Mao;Yu Wang;Bill Dally,yujunlin@stanford.edu;songhan@stanford.edu;huizi@stanford.edu;yu-wang@mail.tsinghua.edu.cn;dally@stanford.edu,7;6;7,4;5;4,Accept (Poster),6,24,5.0,yes,10/27/17,Stanford University;Stanford University;Stanford University;Tsinghua University;Stanford University,4;4;4;10;4,3;3;3;30;3,3
593,593,593,593,593,593,593,593,ICLR,2018,The power of deeper networks for expressing natural functions,David Rolnick;Max Tegmark,drolnick@mit.edu;tegmark@mit.edu,7;6;6,4;4;4,Accept (Poster),0,3,0.0,yes,10/27/17,Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2,5;5,1
594,594,594,594,594,594,594,594,ICLR,2018,Auto-Conditioned Recurrent Networks for Extended Complex Human Motion Synthesis,Yi Zhou;Zimo Li;Shuangjiu Xiao;Chong He;Zeng Huang;Hao Li,zhou859@usc.edu;zimoli@usc.edu;xsjiu99@sjtu.edu.cn;sal@sjtu.edu.cn;zenghuang@usc.edu;hao@hao-li.com,7;7;6,3;5;5,Accept (Poster),0,1,0.0,yes,10/27/17,University of Southern California;University of Southern California;Shanghai Jiao Tong University;Shanghai Jiao Tong University;University of Southern California;Hao-li,31;31;57;57;31;-1,66;66;188;188;66;-1,
595,595,595,595,595,595,595,595,ICLR,2018,A Hierarchical Model for Device Placement,Azalia Mirhoseini;Anna Goldie;Hieu Pham;Benoit Steiner;Quoc V. Le;Jeff Dean,azalia@google.com;agoldie@google.com;hyhieu@cmu.edu;bsteiner@google.com;qvl@google.com;jeff@google.com,5;5;8,4;4;5,Accept (Poster),0,6,1.0,yes,10/27/17,Google;Google;Carnegie Mellon University;Google;Google;Google,-1;-1;1;-1;-1;-1,-1;-1;24;-1;-1;-1,3;2;10
596,596,596,596,596,596,596,596,ICLR,2018,Improving GANs Using Optimal Transport,Tim Salimans;Han Zhang;Alec Radford;Dimitris Metaxas,tim@openai.com;han.zhang@cs.rutgers.edu;alec@openai.com;dnm@cs.rutgers.edu,8;6;6,4;2;3,Accept (Poster),0,4,0.0,yes,10/26/17,OpenAI;Rutgers University;OpenAI;Rutgers University,-1;34;-1;34,-1;172;-1;172,5;4
597,597,597,597,597,597,597,597,ICLR,2018,Improving GAN Training via Binarized Representation Entropy (BRE) Regularization,Yanshuai Cao;Gavin Weiguang Ding;Kry Yik-Chau Lui;Ruitong Huang,yanshuai.cao@borealisai.com;gavin.ding@borealisai.com;yikchau.y.lui@borealisai.com;ruitong.huang@borealisai.com,6;7;4,3;4;3,Accept (Poster),0,5,1.0,yes,10/27/17,Borealis AI;Borealis AI;Borealis AI;Borealis AI,-1;-1;-1;-1,-1;-1;-1;-1,5;4
598,598,598,598,598,598,598,598,ICLR,2018,Hierarchical Density Order Embeddings,Ben Athiwaratkun;Andrew Gordon Wilson,pa338@cornell.edu;andrew@cornell.edu,4;6;8,3;4;5,Accept (Poster),0,5,1.0,yes,10/27/17,Cornell University;Cornell University,7;7,19;19,3;10
599,599,599,599,599,599,599,599,ICLR,2018,Stabilizing Adversarial Nets with Prediction Methods,Abhay Yadav;Sohil Shah;Zheng Xu;David Jacobs;Tom Goldstein,jaiabhay@cs.umd.edu;sohilas@umd.edu;xuzh@cs.umd.edu;djacobs@umiacs.umd.edu;tomg@cs.umd.edu,4;9;7,4;4;4,Accept (Poster),0,15,0.0,yes,10/27/17,"University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park",12;12;12;12;12,69;69;69;69;69,4
600,600,600,600,600,600,600,600,ICLR,2018,Boosting the Actor with Dual Critic,Bo Dai;Albert Shaw;Niao He;Lihong Li;Le Song,bohr.dai@gmail.com;ashaw596@gatech.edu;niaohe@illinois.edu;lihongli.cs@gmail.com;lsong@cc.gatech.edu,7;6;5,4;3;4,Accept (Poster),0,4,0.0,yes,10/27/17,"Georgia Institute of Technology;Georgia Institute of Technology;University of Illinois, Urbana Champaign;Google;Georgia Institute of Technology",13;13;3;-1;13,33;33;37;-1;33,
601,601,601,601,601,601,601,601,ICLR,2018,On the Information Bottleneck Theory of Deep Learning,Andrew Michael Saxe;Yamini Bansal;Joel Dapello;Madhu Advani;Artemy Kolchinsky;Brendan Daniel Tracey;David Daniel Cox,asaxe@fas.harvard.edu;ybansal@g.harvard.edu;dapello@g.harvard.edu;madvani@fas.harvard.edu;artemyk@gmail.com;tracey.brendan@gmail.com;davidcox@fas.harvard.edu,6;7;7,2;3;3,Accept (Poster),7,8,2.0,yes,10/27/17,Harvard University;Harvard University;Harvard University;Harvard University;Santa Fe Institute;;Harvard University,37;37;37;37;-1;-1;37,6;6;6;6;-1;-1;6,8
602,602,602,602,602,602,602,602,ICLR,2018,Syntax-Directed Variational Autoencoder for Structured Data,Hanjun Dai;Yingtao Tian;Bo Dai;Steven Skiena;Le Song,hanjundai@gatech.edu;yittian@cs.stonybrook.edu;bohr.dai@gmail.com;skiena@cs.stonybrook.edu;lsong@cc.gatech.edu,3;5;7,2;1;3,Accept (Poster),0,11,0.0,yes,10/27/17,"Georgia Institute of Technology;State University of New York, Stony Brook;Georgia Institute of Technology;State University of New York, Stony Brook;Georgia Institute of Technology",13;42;13;42;13,33;258;33;258;33,5
603,603,603,603,603,603,603,603,ICLR,2018,Do GANs learn the distribution? Some Theory and Empirics,Sanjeev Arora;Andrej Risteski;Yi Zhang,arora@cs.princeton.edu;risteski@cs.princeton.edu;y.zhang@cs.princeton.edu,7;6;7,4;4;3,Accept (Poster),0,4,0.0,yes,10/27/17,Princeton University;Princeton University;Princeton University,31;31;31,7;7;7,5;4;1
604,604,604,604,604,604,604,604,ICLR,2018,TRUNCATED HORIZON POLICY SEARCH: COMBINING REINFORCEMENT LEARNING & IMITATION LEARNING,Wen Sun;J. Andrew Bagnell;Byron Boots,wensun@cs.cmu.edu;dbagnell@cs.cmu.edu;bboots@cc.gatech.edu,7;6;3,3;4;5,Accept (Poster),0,5,0.0,yes,10/27/17,Carnegie Mellon University;Carnegie Mellon University;Georgia Institute of Technology,1;1;13,24;24;33,
605,605,605,605,605,605,605,605,ICLR,2018,Understanding Deep Neural Networks with Rectified Linear Units,Raman Arora;Amitabh Basu;Poorya Mianjy;Anirbit Mukherjee,arora@cs.jhu.edu;basu.amitabh@jhu.edu;mianjy@jhu.edu;amukhe14@jhu.edu,6;6;7,4;5;4,Accept (Poster),0,0,0.0,yes,10/27/17,Johns Hopkins University;Johns Hopkins University;Johns Hopkins University;Johns Hopkins University,71;71;71;71,13;13;13;13,1
606,606,606,606,606,606,606,606,ICLR,2018,Deep Neural Networks as Gaussian Processes,Jaehoon Lee;Yasaman Bahri;Roman Novak;Samuel S. Schoenholz;Jeffrey Pennington;Jascha Sohl-Dickstein,jaehlee@google.com;yasamanb@google.com;romann@google.com;schsam@google.com;jpennin@google.com;jaschasd@google.com,4;6;7,4;4;3,Accept (Poster),0,8,0.0,yes,10/27/17,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,11
607,607,607,607,607,607,607,607,ICLR,2018,Deep Complex Networks,Chiheb Trabelsi;Olexa Bilaniuk;Ying Zhang;Dmitriy Serdyuk;Sandeep Subramanian;Joao Felipe Santos;Soroush Mehri;Negar Rostamzadeh;Yoshua Bengio;Christopher J Pal,chiheb.trabelsi@polymtl.ca;olexa.bilaniuk@umontreal.ca;ying.zhang@umontreal.ca;serdyuk@iro.umontreal.ca;sandeep.subramanian.1@umontreal.ca;jfsantos@emt.inrs.ca;soroush.mehri@microsoft.com;negar@elementai.com;yoshua.bengio@umontreal.ca;christopher.pal@polymtl.ca,7;8;4,4;4;4,Accept (Poster),0,5,0.0,yes,10/27/17,Polytechnique Montreal;University of Montreal;University of Montreal;University of Montreal;University of Montreal;Institut national de la recherche scientifique;Microsoft;Element AI;University of Montreal;Polytechnique Montreal,364;124;124;124;124;-1;-1;-1;124;364,108;108;108;108;108;-1;-1;-1;108;108,2
608,608,608,608,608,608,608,608,ICLR,2018,Non-Autoregressive Neural Machine Translation,Jiatao Gu;James Bradbury;Caiming Xiong;Victor O.K. Li;Richard Socher,jiataogu@eee.hku.hk;james.bradbury@salesforce.com;cxiong@salesforce.com;vli@eee.hku.hk;rsocher@salesforce.com,7;7;6,4;4;4,Accept (Poster),2,3,0.0,yes,10/27/17,The University of Hong Kong;SalesForce.com;SalesForce.com;The University of Hong Kong;SalesForce.com,90;-1;-1;90;-1,40;-1;-1;40;-1,3
609,609,609,609,609,609,609,609,ICLR,2018,Depthwise Separable Convolutions for Neural Machine Translation,Lukasz Kaiser;Aidan N. Gomez;Francois Chollet,lukaszkaiser@google.com;aidan.n.gomez@gmail.com;fchollet@google.com,5;7;7,4;4;3,Accept (Poster),0,4,0.0,yes,10/27/17,"Google;Department of Computer Science, University of Toronto;Google",-1;17;-1,-1;22;-1,3
610,610,610,610,610,610,610,610,ICLR,2018,Learning a neural response metric for retinal prosthesis,Nishal P Shah;Sasidhar Madugula;EJ Chichilnisky;Yoram Singer;Jonathon Shlens,nishalps@stanford.edu;sasidhar@stanford.edu;ej@stanford.edu;singer@google.com;shlens@google.com,5;6;7,4;3;4,Accept (Poster),0,0,0.0,yes,10/27/17,Stanford University;Stanford University;Stanford University;Google;Google,4;4;4;-1;-1,3;3;3;-1;-1,5
611,611,611,611,611,611,611,611,ICLR,2018,Distributed Distributional Deterministic Policy Gradients,Gabriel Barth-Maron;Matthew W. Hoffman;David Budden;Will Dabney;Dan Horgan;Dhruva TB;Alistair Muldal;Nicolas Heess;Timothy Lillicrap,gabrielbm@google.com;mwhoffman@google.com;budden@google.com;wdabney@google.com;horgan@google.com;dhruvat@google.com;alimuldal@google.com;heess@google.com;countzero@google.com,9;6;5,4;5;4,Accept (Poster),2,6,0.0,yes,10/27/17,Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1,
612,612,612,612,612,612,612,612,ICLR,2018,Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples,Ashwin Kalyan;Abhishek Mohta;Oleksandr Polozov;Dhruv Batra;Prateek Jain;Sumit Gulwani,ashwinkv@gatech.edu;t-abmoht@microsoft.com;polozov@microsoft.com;dbatra@gatech.edu;prajain@microsoft.com;sumitg@microsoft.com,6;6;8,3;4;3,Accept (Poster),0,5,0.0,yes,10/27/17,Georgia Institute of Technology;Microsoft;Microsoft;Georgia Institute of Technology;Microsoft;Microsoft,13;-1;-1;13;-1;-1,33;-1;-1;33;-1;-1,8
613,613,613,613,613,613,613,613,ICLR,2018,Learning an Embedding Space for Transferable Robot Skills,Karol Hausman;Jost Tobias Springenberg;Ziyu Wang;Nicolas Heess;Martin Riedmiller,hausmankarol@gmail.com;springenberg@google.com;ziyu@google.com;heess@google.com;riedmiller@google.com,7;7;7,4;4;5,Accept (Poster),0,6,0.0,yes,10/27/17,University of Southern California;Google;Google;Google;Google,31;-1;-1;-1;-1,66;-1;-1;-1;-1,
614,614,614,614,614,614,614,614,ICLR,2018,Multi-Mention Learning for Reading Comprehension with Neural Cascades,Swabha Swayamdipta;Ankur P. Parikh;Tom Kwiatkowski,swabha@cs.cmu.edu;aparikh@google.com;tomkwiat@google.com,7;5;6,4;4;4,Accept (Poster),0,6,0.0,yes,10/27/17,Carnegie Mellon University;Google;Google,1;-1;-1,24;-1;-1,
615,615,615,615,615,615,615,615,ICLR,2018,Learning Differentially Private Recurrent Language Models,H. Brendan McMahan;Daniel Ramage;Kunal Talwar;Li Zhang,mcmahan@google.com;dramage@google.com;kunal@google.com;liqzhang@google.com,7;7;8,4;2;4,Accept (Poster),0,3,0.0,yes,10/27/17,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,3
616,616,616,616,616,616,616,616,ICLR,2018,Learning Approximate Inference Networks for Structured Prediction,Lifu Tu;Kevin Gimpel,lifu@ttic.edu;kgimpel@ttic.edu,7;5;9,5;3;4,Accept (Poster),0,5,0.0,yes,10/27/17,Toyota Technological Institute at Chicago;Toyota Technological Institute at Chicago,-1;-1,-1;-1,3
617,617,617,617,617,617,617,617,ICLR,2018,Emergent Communication through Negotiation,Kris Cao;Angeliki Lazaridou;Marc Lanctot;Joel Z Leibo;Karl Tuyls;Stephen Clark,kc391@cam.ac.uk;angeliki@google.com;lanctot@google.com;jzl@google.com;karltuyls@google.com;clarkstephen@google.com,6;7;5,3;4;4,Accept (Poster),7,5,0.0,yes,10/27/17,University of Cambridge;Google;Google;Google;Google;Google,71;-1;-1;-1;-1;-1,2;-1;-1;-1;-1;-1,
618,618,618,618,618,618,618,618,ICLR,2018,Ensemble Adversarial Training: Attacks and Defenses,Florian Tramèr;Alexey Kurakin;Nicolas Papernot;Ian Goodfellow;Dan Boneh;Patrick McDaniel,tramer@cs.stanford.edu;alexey@kurakin.me;ngp5056@cse.psu.edu;goodfellow@google.com;dabo@cs.stanford.edu;mcdaniel@cse.psu.edu,6;6;6,2;4;4,Accept (Poster),0,4,1.0,yes,10/27/17,Stanford University;Google;Pennsylvania State University;Google;Stanford University;Pennsylvania State University,4;-1;40;-1;4;40,3;-1;77;-1;3;77,4
619,619,619,619,619,619,619,619,ICLR,2018,Maximum a Posteriori Policy Optimisation,Abbas Abdolmaleki;Jost Tobias Springenberg;Yuval Tassa;Remi Munos;Nicolas Heess;Martin Riedmiller,abbas.abdolmaleky@gmail.com;springenberg@google.com;heess@google.com;tassa@google.com;munos@google.com,7;6;5,5;1;4,Accept (Poster),4,5,2.0,yes,10/27/17,;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,
620,620,620,620,620,620,620,620,ICLR,2018,Global Optimality Conditions for Deep Neural Networks,Chulhee Yun;Suvrit Sra;Ali Jadbabaie,chulheey@mit.edu;suvrit@mit.edu;jadbabai@mit.edu,5;7;8,5;4;5,Accept (Poster),0,3,0.0,yes,10/27/17,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2,5;5;5,1;9
621,621,621,621,621,621,621,621,ICLR,2018,Leave no Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning,Benjamin Eysenbach;Shixiang Gu;Julian Ibarz;Sergey Levine,eysenbach@google.com;sg717@cam.ac.uk;julianibarz@google.com;slevine@google.com,7;6;5;7,4;5;4;4,Accept (Poster),0,10,0.0,yes,10/27/17,Google;University of Cambridge;Google;Google,-1;71;-1;-1,-1;2;-1;-1,
622,622,622,622,622,622,622,622,ICLR,2018,Sensitivity and Generalization in Neural Networks: an Empirical Study,Roman Novak;Yasaman Bahri;Daniel A. Abolafia;Jeffrey Pennington;Jascha Sohl-Dickstein,romann@google.com;yasamanb@google.com;danabo@google.com;jpennin@google.com;jaschasd@google.com,8;5;4,4;3;5,Accept (Poster),0,9,4.0,yes,10/27/17,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,8
623,623,623,623,623,623,623,623,ICLR,2018,Combining Symbolic Expressions and Black-box Function Evaluations in Neural Programs,Forough Arabshahi;Sameer Singh;Animashree Anandkumar,farabsha@uci.edu;sameer@uci.edu;animakumar@gmail.com,6;8;5,4;3;4,Accept (Poster),0,7,0.0,yes,10/27/17,"University of California, Irvine;University of California, Irvine;University of California-Irvine",36;36;36,99;99;99,8
624,624,624,624,624,624,624,624,ICLR,2018,Dynamic Neural Program Embeddings for Program Repair,Ke Wang;Rishabh Singh;Zhendong Su,kbwang@ucdavis.edu;risin@microsoft.com;su@cs.ucdavis.edu,6;7;7,2;3;4,Accept (Poster),0,8,0.0,yes,10/27/17,"University of California, Davis;Microsoft;University of California, Davis",78;-1;78,54;-1;54,
625,625,625,625,625,625,625,625,ICLR,2018,Generating Natural Adversarial Examples,Zhengli Zhao;Dheeru Dua;Sameer Singh,zhengliz@uci.edu;ddua@uci.edu;sameer@uci.edu,6;7;6,3;4;3,Accept (Poster),3,5,0.0,yes,10/27/17,"University of California, Irvine;University of California, Irvine;University of California, Irvine",36;36;36,99;99;99,3;4;5
626,626,626,626,626,626,626,626,ICLR,2018,mixup: Beyond Empirical Risk Minimization,Hongyi Zhang;Moustapha Cisse;Yann N. Dauphin;David Lopez-Paz,hongyiz@mit.edu;moustaphacisse@fb.com;ynd@fb.com;dlp@fb.com,6;7;6,4;4;4,Accept (Poster),1,6,0.0,yes,10/27/17,Massachusetts Institute of Technology;Facebook;Facebook;Facebook,2;-1;-1;-1,5;-1;-1;-1,5;4;8
627,627,627,627,627,627,627,627,ICLR,2018,Backpropagation through the Void: Optimizing control variates for black-box gradient estimation,Will Grathwohl;Dami Choi;Yuhuai Wu;Geoff Roeder;David Duvenaud,wgrathwohl@cs.toronto.edu;choidami@cs.toronto.edu;ywu@cs.toronto.edu;roeder@cs.toronto.edu;duvenaud@cs.toronto.edu,8;7;6,4;3;2,Accept (Poster),0,11,0.0,yes,10/27/17,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",17;17;17;17;17,22;22;22;22;22,
628,628,628,628,628,628,628,628,ICLR,2018,"Don't Decay the Learning Rate, Increase the Batch Size",Samuel L. Smith;Pieter-Jan Kindermans;Chris Ying;Quoc V. Le,slsmith@google.com;pikinder@google.com;chrisying@google.com;qvl@google.com,6;7;6,4;4;4,Accept (Poster),2,6,0.0,yes,10/27/17,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,
629,629,629,629,629,629,629,629,ICLR,2018,Fidelity-Weighted Learning,Mostafa Dehghani;Arash Mehrjou;Stephan Gouws;Jaap Kamps;Bernhard Schölkopf,dehghani@uva.nl;amehrjou@tuebingen.mpg.de;sgouws@google.com;kamps@uva.nl;bs@tuebingen.mpg.de,7;5;6,4;4;3,Accept (Poster),0,8,0.0,yes,10/27/17,"University of Amsterdam;Max Planck Institute for Intelligent Systems, Max-Planck Institute;Google;University of Amsterdam;Max Planck Institute for Intelligent Systems, Max-Planck Institute",181;-1;-1;181;-1,59;-1;-1;59;-1,3
630,630,630,630,630,630,630,630,ICLR,2018,Emergent Complexity via Multi-Agent Competition,Trapit Bansal;Jakub Pachocki;Szymon Sidor;Ilya Sutskever;Igor Mordatch,tbansal@cs.umass.edu;jakub@openai.com;szymon@openai.com;ilyasu@openai.com;mordatch@openai.com,3;9;7,3;5;4,Accept (Poster),0,5,0.0,yes,10/27/17,"University of Massachusetts, Amherst;OpenAI;OpenAI;OpenAI;OpenAI",30;-1;-1;-1;-1,191;-1;-1;-1;-1,
631,631,631,631,631,631,631,631,ICLR,2018,A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks,Behnam Neyshabur;Srinadh Bhojanapalli;Nathan Srebro,bneyshabur@ttic.edu;srinadh@ttic.edu;nati@ttic.edu,9;6;7,4;3;4,Accept (Poster),0,0,0.0,yes,10/27/17,Toyota Technological Institute at Chicago;Toyota Technological Institute at Chicago;Toyota Technological Institute at Chicago,-1;-1;-1,-1;-1;-1,1;8
632,632,632,632,632,632,632,632,ICLR,2018,N2N learning: Network to Network Compression via Policy Gradient Reinforcement Learning,Anubhav Ashok;Nicholas Rhinehart;Fares Beainy;Kris M. Kitani,anubhava@andrew.cmu.edu;nrhineha@cs.cmu.edu;fares.beainy@volvo.com;kkitani@cs.cmu.edu,5;9;4,4;4;4,Accept (Poster),0,4,0.0,yes,10/27/17,Carnegie Mellon University;Carnegie Mellon University;Volvo Trucks;Carnegie Mellon University,1;1;-1;1,24;24;-1;24,6;2
633,633,633,633,633,633,633,633,ICLR,2018,AmbientGAN: Generative models from lossy measurements,Ashish Bora;Eric Price;Alexandros G. Dimakis,ashish.bora@utexas.edu;ecprice@cs.utexas.edu;dimakis@austin.utexas.edu,7;7;8,4;4;4,Accept (Oral),0,2,1.0,yes,10/27/17,"University of Texas, Austin;University of Texas, Austin;University of Texas, Austin",21;21;21,49;49;49,5;4
634,634,634,634,634,634,634,634,ICLR,2018,Eigenoption Discovery through the Deep Successor Representation,Marlos C. Machado;Clemens Rosenbaum;Xiaoxiao Guo;Miao Liu;Gerald Tesauro;Murray Campbell,machado@ualberta.ca;crosenbaum@umass.edu;xiaoxiao.guo@ibm.com;miao.liu1@ibm.com;gtesauro@us.ibm.com;mcam@us.ibm.com,6;9;7,3;5;4,Accept (Poster),0,3,0.0,yes,10/27/17,"University of Alberta;University of Massachusetts, Amherst;International Business Machines;International Business Machines;International Business Machines;International Business Machines",99;30;-1;-1;-1;-1,119;191;-1;-1;-1;-1,
635,635,635,635,635,635,635,635,ICLR,2018,Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning,Sandeep Subramanian;Adam Trischler;Yoshua Bengio;Christopher J Pal,sandeep.subramanian.1@umontreal.ca;adam.trischler@microsoft.com;yoshua.umontreal@gmail.com;christopher.pal@polymtl.ca,8;8;4,5;5;5,Accept (Poster),1,15,0.0,yes,10/27/17,University of Montreal;Microsoft;University of Montreal;Polytechnique Montreal,124;-1;124;364,108;-1;108;108,3;6
636,636,636,636,636,636,636,636,ICLR,2018,Communication Algorithms via Deep Learning,Hyeji Kim;Yihan Jiang;Ranvir B. Rana;Sreeram Kannan;Sewoong Oh;Pramod Viswanath,hyejikim@illinois.edu;yihanrogerjiang@gmail.com;rbrana2@illinois.edu;ksreeram@uw.edu;sewoong79@gmail.com;pramodv@illinois.edu,2;6;9,4;4;5,Accept (Poster),8,18,1.0,yes,10/27/17,"University of Illinois, Urbana Champaign;University of Washington, Seattle;University of Illinois, Urbana Champaign;University of Washington, Seattle;University of Illinois at Urbana-Champaign;University of Illinois, Urbana Champaign",3;6;3;6;3;3,37;25;37;25;37;37,
637,637,637,637,637,637,637,637,ICLR,2018,Variational Inference of Disentangled Latent Concepts from Unlabeled Observations,Abhishek Kumar;Prasanna Sattigeri;Avinash Balakrishnan,abhishk@us.ibm.com;psattig@us.ibm.com;avinash.bala@us.ibm.com,6;7;7,5;4;4,Accept (Poster),0,9,0.0,yes,10/27/17,International Business Machines;International Business Machines;International Business Machines,-1;-1;-1,-1;-1;-1,5
638,638,638,638,638,638,638,638,ICLR,2018,Variational Message Passing with Structured Inference Networks,Wu Lin;Nicolas Hubacher;Mohammad Emtiyaz Khan,wlin2018@cs.ubc.ca;nicolas.hubacher@outlook.com;emtiyaz@gmail.com,7;7;7,3;4;2,Accept (Poster),0,6,0.0,yes,10/27/17,University of British Columbia;;RIKEN,34;-1;-1,34;-1;-1,5;10
639,639,639,639,639,639,639,639,ICLR,2018,On the insufficiency of existing momentum schemes for Stochastic Optimization,Rahul Kidambi;Praneeth Netrapalli;Prateek Jain;Sham M. Kakade,rkidambi@uw.edu;praneeth@microsoft.com;prajain@microsoft.com;sham@cs.washington.edu,7;7;8,4;3;5,Accept (Oral),0,4,5.0,yes,10/27/17,"University of Washington, Seattle;Microsoft;Microsoft;University of Washington",6;-1;-1;6,25;-1;-1;25,
640,640,640,640,640,640,640,640,ICLR,2018,CausalGAN: Learning Causal Implicit Generative Models with Adversarial Training,Murat Kocaoglu;Christopher Snyder;Alexandros G. Dimakis;Sriram Vishwanath,mkocaoglu@utexas.edu;22csnyder@gmail.com;dimakis@austin.utexas.edu;sriram@austin.utexas.edu,6;7;9,3;3;3,Accept (Poster),0,5,2.0,yes,10/27/17,"University of Texas, Austin;University of Texas, Galveston;University of Texas, Austin;University of Texas, Austin",21;468;21;21,49;1103;49;49,5;4;10
641,641,641,641,641,641,641,641,ICLR,2018,Unsupervised Neural Machine Translation,Mikel Artetxe;Gorka Labaka;Eneko Agirre;Kyunghyun Cho,mikel.artetxe@ehu.eus;gorka.labaka@ehu.eus;e.agirre@ehu.eus;kyunghyun.cho@nyu.edu,6;5;7,4;4;5,Accept (Poster),0,12,0.0,yes,10/27/17,University of the Basque Country;University of the Basque Country;University of the Basque Country;New York University,468;468;468;26,613;613;613;27,3
642,642,642,642,642,642,642,642,ICLR,2018,Understanding image motion with group representations ,Andrew Jaegle;Stephen Phillips;Daphne Ippolito;Kostas Daniilidis,ajaegle@upenn.edu;stephi@seas.upenn.edu;daphnei@seas.upenn.edu;kostas@seas.upenn.edu,7;5;4,3;4;4,Accept (Poster),0,4,0.0,yes,10/27/17,University of Pennsylvania;University of Pennsylvania;University of Pennsylvania;University of Pennsylvania,19;19;19;19,10;10;10;10,
643,643,643,643,643,643,643,643,ICLR,2018,Deep Learning with Logged Bandit Feedback,Thorsten Joachims;Adith Swaminathan;Maarten de Rijke,tj@cs.cornell.edu;adswamin@microsoft.com;derijke@uva.nl,7;8;6,4;3;3,Accept (Poster),0,6,0.0,yes,10/27/17,Cornell University;Microsoft;University of Amsterdam,7;-1;181,19;-1;59,
644,644,644,644,644,644,644,644,ICLR,2018,FearNet: Brain-Inspired Model for Incremental Learning,Ronald Kemker;Christopher Kanan,rmk6217@rit.edu;kanan@rit.edu,7;7;6,2;4;2,Accept (Poster),0,5,0.0,yes,10/27/17,Rochester Institute of Technology;Rochester Institute of Technology,139;139,666;666,5
645,645,645,645,645,645,645,645,ICLR,2018,On the Discrimination-Generalization Tradeoff in GANs,Pengchuan Zhang;Qiang Liu;Dengyong Zhou;Tao Xu;Xiaodong He,penzhan@microsoft.com;qiang.liu@dartmouth.edu;dennyzhou@google.com;tax313@lehigh.edu;xiaohe@microsoft.com,6;3;7,4;4;4,Accept (Poster),0,9,0.0,yes,10/27/17,Microsoft;Dartmouth College;Google;Lehigh University;Microsoft,-1;153;-1;244;-1,-1;89;-1;533;-1,5;4;1;8
646,646,646,646,646,646,646,646,ICLR,2018,Learning Latent Permutations with Gumbel-Sinkhorn Networks,Gonzalo Mena;David Belanger;Scott Linderman;Jasper Snoek,gem2131@columbia.edu;dbelanger@google.com;scott.linderman@gmail.com;jsnoek@google.com,8;7;6,4;4;2,Accept (Poster),0,6,0.0,yes,10/27/17,Columbia University;Google;Columbia University;Google,15;-1;15;-1,14;-1;14;-1,
647,647,647,647,647,647,647,647,ICLR,2018,Towards Deep Learning Models Resistant to Adversarial Attacks,Aleksander Madry;Aleksandar Makelov;Ludwig Schmidt;Dimitris Tsipras;Adrian Vladu,madry@mit.edu;amakelov@mit.edu;ludwigs@mit.edu;tsipras@mit.edu;avladu@mit.edu,7;6;7,4;3;4,Accept (Poster),3,6,1.0,yes,10/27/17,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2;2,5;5;5;5;5,4
648,648,648,648,648,648,648,648,ICLR,2018,Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via Ranking,Aleksandar Bojchevski;Stephan Günnemann,a.bojchevski@in.tum.de;guennemann@in.tum.de,7;6;7,4;4;3,Accept (Poster),2,5,0.0,yes,10/27/17,Technical University Munich;Technical University Munich,55;55,41;41,10
649,649,649,649,649,649,649,649,ICLR,2018,Predicting Floor-Level for 911 Calls with Neural Networks and Smartphone Sensor Data,William Falcon;Henning Schulzrinne,waf2107@columbia.edu;hgs@cs.columbia.edu,6;7;6,4;5;3,Accept (Poster),0,14,1.0,yes,10/27/17,Columbia University;Columbia University,15;15,14;14,
650,650,650,650,650,650,650,650,ICLR,2018,Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples,Kimin Lee;Honglak Lee;Kibok Lee;Jinwoo Shin,kiminlee@kaist.ac.kr;honglak@eecs.umich.edu;kibok@umich.edu;jinwoos@kaist.ac.kr,6;7;6,4;3;3,Accept (Poster),0,4,0.0,yes,10/27/17,Korea Advanced Institute of Science and Technology;University of Michigan;University of Michigan;Korea Advanced Institute of Science and Technology,21;8;8;21,95;21;21;95,5
651,651,651,651,651,651,651,651,ICLR,2018,Ask the Right Questions: Active Question Reformulation with Reinforcement Learning,Christian Buck;Jannis Bulian;Massimiliano Ciaramita;Wojciech Gajewski;Andrea Gesmundo;Neil Houlsby;Wei Wang.,cbuck@google.com;jbulian@google.com;massi@google.com;wgaj@google.com;agesmundo@google.com;neilhoulsby@google.com;wangwe@google.com,7;6;8,5;4;3,Accept (Oral),0,5,0.0,yes,10/27/17,Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,3
652,652,652,652,652,652,652,652,ICLR,2018,Towards Image Understanding from Deep Compression Without Decoding,Robert Torfason;Fabian Mentzer;Eirikur Agustsson;Michael Tschannen;Radu Timofte;Luc Van Gool,robertto@student.ethz.ch;mentzerf@vision.ee.ethz.ch;aeirikur@vision.ee.ethz.ch;michaelt@nari.ee.ethz.ch;radu.timofte@vision.ee.ethz.ch;vangool@vision.ee.ethz.ch,6;9;6,4;5;3,Accept (Poster),0,3,0.0,yes,10/27/17,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,9;9;9;9;9;9,10;10;10;10;10;10,2
653,653,653,653,653,653,653,653,ICLR,2018,The Implicit Bias of Gradient Descent on Separable Data,Daniel Soudry;Elad Hoffer;Mor Shpigel Nacson;Nathan Srebro,daniel.soudry@gmail.com;elad.hoffer@gmail.com;mor.shpigel@gmail.com;nati@ttic.edu,5;7;8,5;4;4,Accept (Poster),0,3,0.0,yes,10/27/17,Technion;Technion;Technion;Toyota Technological Institute at Chicago,24;24;24;-1,327;327;327;-1,8
654,654,654,654,654,654,654,654,ICLR,2018,Distributed Prioritized Experience Replay,Dan Horgan;John Quan;David Budden;Gabriel Barth-Maron;Matteo Hessel;Hado van Hasselt;David Silver,horgan@google.com;johnquan@google.com;budden@google.com;gabrielbm@google.com;mtthss@google.com;hado@google.com;davidsilver@google.com,9;7;6,4;4;3,Accept (Poster),0,6,0.0,yes,10/27/17,Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,
655,655,655,655,655,655,655,655,ICLR,2018,Learning One-hidden-layer Neural Networks with Landscape Design,Rong Ge;Jason D. Lee;Tengyu Ma,rongge@cs.duke.edu;jasondlee88@gmail.com;tengyuma@cs.stanford.edu,6;9;7,3;3;3,Accept (Poster),0,4,1.0,yes,10/27/17,Duke University;University of Southern California;Stanford University,46;31;4,17;66;3,1
656,656,656,656,656,656,656,656,ICLR,2018,Learning to cluster in order to transfer across domains and tasks,Yen-Chang Hsu;Zhaoyang Lv;Zsolt Kira,yenchang.hsu@gatech.edu;zhaoyang.lv@gatech.edu;zkira@gatech.edu,7;5;9,4;4;5,Accept (Poster),2,6,0.0,yes,10/20/17,Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology,13;13;13,33;33;33,6
657,657,657,657,657,657,657,657,ICLR,2018,Mixed Precision Training of Convolutional Neural Networks using Integer Operations,Dipankar Das;Naveen Mellempudi;Dheevatsa Mudigere;Dhiraj Kalamkar;Sasikanth Avancha;Kunal Banerjee;Srinivas Sridharan;Karthik Vaidyanathan;Bharat Kaul;Evangelos Georganas;Alexander Heinecke;Pradeep Dubey;Jesus Corbal;Nikita Shustrov;Roma Dubtsov;Evarist Fomenko;Vadim Pirogov,dipankar.das@intel.com;naveen.k.mellempudi@intel.com;dheevatsa.mudigere@intel.com;dhiraj.d.kalamkar@intel.com;sasikanth.avancha@intel.com;kunal.banerjee@intel.com;srinivas.sridharan@intel.com;karthikeyan.vaidyanathan@intel.com;bharat.kaul@intel.com;evangelos.georganas@intel.com;alexander.heinecke@intel.com;pradeep.dubey@intel.com;jesus.corbal@intel.com;nikita.a.shustrov@intel.com;roman.s.dubtsov@intel.com;evarist.m.fomenko@intel.com;vadim.o.pirogov@intel.com,7;7;6,4;3;3,Accept (Poster),0,4,0.0,yes,10/27/17,Intel;Intel;Intel;Intel;Intel;Intel;Intel;Intel;Intel;Intel;Intel;Intel;Intel;Intel;Intel;Intel;Intel,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,
658,658,658,658,658,658,658,658,ICLR,2018,Generative networks as inverse problems with Scattering transforms,Tomás Angles;Stéphane Mallat,tomas.angles@ens.fr;stephane.mallat@ens.fr,7;8;6,4;4;4,Accept (Poster),0,5,0.0,yes,10/27/17,Ecole Normale Superieure;Ecole Normale Superieure,99;99,72;72,5;4
659,659,659,659,659,659,659,659,ICLR,2018,Sparse Persistent RNNs: Squeezing Large Recurrent Networks On-Chip,Feiwen Zhu;Jeff Pool;Michael Andersch;Jeremy Appleyard;Fung Xie,mzhu@nvidia.com;jpool@nvidia.com;mandersch@nvidia.com;jappleyard@nvidia.com;ftse@nvidia.com,6;6;6,2;4;2,Accept (Poster),0,5,0.0,yes,10/27/17,NVIDIA;NVIDIA;NVIDIA;NVIDIA;NVIDIA,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,
660,660,660,660,660,660,660,660,ICLR,2018,Robustness of Classifiers to Universal Perturbations: A Geometric Perspective,Seyed-Mohsen Moosavi-Dezfooli;Alhussein Fawzi;Omar Fawzi;Pascal Frossard;Stefano Soatto,seyed.moosavi@epfl.ch;fawzi@cs.ucla.edu;omar.fawzi@ens-lyon.fr;pascal.frossard@epfl.ch;soatto@cs.ucla.edu,6;5;7,4;3;3,Accept (Poster),0,3,0.0,yes,10/26/17,"Swiss Federal Institute of Technology Lausanne;University of California, Los Angeles;Ecole Normale Supérieure de Lyon;Swiss Federal Institute of Technology Lausanne;University of California, Los Angeles",468;20;181;468;20,38;15;182;38;15,1
661,661,661,661,661,661,661,661,ICLR,2018,Deep Learning as a Mixed Convex-Combinatorial Optimization Problem,Abram L. Friesen;Pedro Domingos,afriesen@cs.washington.edu;pedrod@cs.washington.edu,7;7;7,4;4;3,Accept (Poster),0,2,0.0,yes,10/27/17,University of Washington;University of Washington,6;6,25;25,9
662,662,662,662,662,662,662,662,ICLR,2018,Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments,Maruan Al-Shedivat;Trapit Bansal;Yura Burda;Ilya Sutskever;Igor Mordatch;Pieter Abbeel,alshedivat@cs.cmu.edu;tbansal@cs.umass.edu;yburda@openai.com;ilyasu@openai.com;mordatch@openai.com;pabbeel@cs.berkeley.edu,8;7;9,4;4;2,Accept (Oral),0,4,3.0,yes,10/27/17,"Carnegie Mellon University;University of Massachusetts, Amherst;OpenAI;OpenAI;OpenAI;University of California Berkeley",1;30;-1;-1;-1;5,24;191;-1;-1;-1;18,4;6
663,663,663,663,663,663,663,663,ICLR,2018,Unsupervised Cipher Cracking Using Discrete GANs,Aidan N. Gomez;Sicong Huang;Ivan Zhang;Bryan M. Li;Muhammad Osama;Lukasz Kaiser,aidan.n.gomez@gmail.com;huang@cs.toronto.edu;ivan@for.ai;bryan@for.ai;osama@for.ai;lukaszkaiser@google.com,7;7;8,4;1;4,Accept (Poster),0,5,0.0,yes,10/27/17,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Google",17;17;-1;17;17;-1,22;22;-1;22;22;-1,5;1
664,664,664,664,664,664,664,664,ICLR,2018,Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play,Sainbayar Sukhbaatar;Zeming Lin;Ilya Kostrikov;Gabriel Synnaeve;Arthur Szlam;Rob Fergus,sainbar@cs.nyu.edu;zlin@fb.com;kostrikov@cs.nyu.edu;gab@fb.com;aszlam@fb.com;fergus@cs.nyu.edu,8;5;8,4;3;4,Accept (Poster),0,2,0.0,yes,10/27/17,New York University;Facebook;New York University;Facebook;Facebook;New York University,26;-1;26;-1;-1;26,27;-1;27;-1;-1;27,
665,665,665,665,665,665,665,665,ICLR,2018,Mastering the Dungeon: Grounded Language Learning by Mechanical Turker Descent,Zhilin Yang;Saizheng Zhang;Jack Urbanek;Will Feng;Alexander Miller;Arthur Szlam;Douwe Kiela;Jason Weston,zhiliny@cs.cmu.edu;saizheng.zhang@umontreal.ca;jju@fb.com;willfeng@fb.com;ahm@fb.com;aszlam@fb.com;dkiela@fb.com;jase@fb.com,7;7;8,4;4;5,Accept (Poster),0,4,0.0,yes,10/27/17,Carnegie Mellon University;University of Montreal;Facebook;Facebook;Facebook;Facebook;Facebook;Facebook,1;124;-1;-1;-1;-1;-1;-1,24;108;-1;-1;-1;-1;-1;-1,3
666,666,666,666,666,666,666,666,ICLR,2018,Gradient Estimators for Implicit Models,Yingzhen Li;Richard E. Turner,yl494@cam.ac.uk;ret26@cam.ac.uk,7;7;6,2;4;2,Accept (Poster),1,9,1.0,yes,10/27/17,University of Cambridge;University of Cambridge,71;71,2;2,5;4;6
667,667,667,667,667,667,667,667,ICLR,2018, Neural Map: Structured Memory for Deep Reinforcement Learning,Emilio Parisotto;Ruslan Salakhutdinov,eparisot@andrew.cmu.edu;rsalakhu@cs.cmu.edu,7;9;6,4;5;5,Accept (Poster),0,9,0.0,yes,10/27/17,Carnegie Mellon University;Carnegie Mellon University,1;1,24;24,
668,668,668,668,668,668,668,668,ICLR,2018,Memory-based Parameter Adaptation,Pablo Sprechmann;Siddhant M. Jayakumar;Jack W. Rae;Alexander Pritzel;Adria Puigdomenech Badia;Benigno Uria;Oriol Vinyals;Demis Hassabis;Razvan Pascanu;Charles Blundell,psprechmann@google.com;sidmj@google.com;jwrae@google.com;apritzel@google.com;adriap@google.com;buria@google.com;vinyals@google.com;dhcontact@google.com;razp@google.com;cblundell@google.com,6;6;8,4;5;4,Accept (Poster),1,13,1.0,yes,10/27/17,Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,3
669,669,669,669,669,669,669,669,ICLR,2018,Model compression via distillation and quantization,Antonio Polino;Razvan Pascanu;Dan Alistarh,antonio.polino1@gmail.com;razp@google.com;d.alistarh@gmail.com,7;6;8,2;4;5,Accept (Poster),0,13,3.0,yes,10/27/17,;Google;Institute of Science and Technology Austria,-1;-1;99,-1;-1;1103,
670,670,670,670,670,670,670,670,ICLR,2018,Learning Wasserstein Embeddings,Nicolas Courty;Rémi Flamary;Mélanie Ducoffe,ncourty@irisa.fr;remi.flamary@unice.fr;ducoffe@i3s.unice.fr,7;7;7,3;3;4,Accept (Poster),0,10,0.0,yes,10/26/17,"IRISA, Université Bretagne Sud;Université Côte d'Azur;Université Côte d'Azur",468;-1;-1,1103;-1;-1,5
671,671,671,671,671,671,671,671,ICLR,2018,Loss-aware Weight Quantization of Deep Networks,Lu Hou;James T. Kwok,lhouab@cse.ust.hk;jamesk@cse.ust.hk,8;6;6,3;4;4,Accept (Poster),0,3,0.0,yes,10/27/17,The Hong Kong University of Science and Technology;The Hong Kong University of Science and Technology,40;40,44;44,
672,672,672,672,672,672,672,672,ICLR,2018,Kernel Implicit Variational Inference,Jiaxin Shi;Shengyang Sun;Jun Zhu,shijx15@mails.tsinghua.edu.cn;ssy@cs.toronto.edu;dcszj@tsinghua.edu.cn,5;7;7,4;3;4,Accept (Poster),0,7,0.0,yes,10/27/17,"Tsinghua University;Department of Computer Science, University of Toronto;Tsinghua University",10;17;10,30;22;30,11
673,673,673,673,673,673,673,673,ICLR,2018,Adversarial Dropout Regularization,Kuniaki Saito;Yoshitaka Ushiku;Tatsuya Harada;Kate Saenko,k-saito@mi.t.u-tokyo.ac.jp;ushiku@mi.t.u-tokyo.ac.jp;harada@mi.t.u-tokyo.ac.jp;saenko@bu.edu,5;7;8,4;3;5,Accept (Poster),0,11,0.0,yes,10/27/17,The University of Tokyo;The University of Tokyo;The University of Tokyo;Boston University,52;52;52;69,45;45;45;70,4;2
674,674,674,674,674,674,674,674,ICLR,2018,Hierarchical Subtask Discovery with Non-Negative Matrix Factorization,Adam C. Earle;Andrew M. Saxe;Benjamin Rosman,adam.earle@ymail.com;asaxe@fas.harvard.edu;benjros@gmail.com,6;5;7,2;2;3,Accept (Poster),0,5,0.0,yes,10/27/17,University of the Witwatersrand;Harvard University;University of the Witwatersrand,468;37;468,293;6;293,
675,675,675,675,675,675,675,675,ICLR,2018,A Scalable Laplace Approximation for Neural Networks,Hippolyt Ritter;Aleksandar Botev;David Barber,j.ritter@cs.ucl.ac.uk;botevmg@gmail.com;d.barber@cs.ucl.ac.uk,9;6;6,4;4;4,Accept (Poster),0,5,0.0,yes,10/27/17,University College London;University College London;University College London,46;46;46,16;16;16,4
676,676,676,676,676,676,676,676,ICLR,2018,TreeQN and ATreeC: Differentiable Tree-Structured Models for Deep Reinforcement Learning,Gregory Farquhar;Tim Rocktäschel;Maximilian Igl;Shimon Whiteson,gregory.farquhar@cs.ox.ac.uk;tim.rocktaeschel@gmail.com;maximilian.igl@gmail.com;shimon.whiteson@gmail.com,4;8;5,5;5;3,Accept (Poster),3,5,0.0,yes,10/27/17,University of Oxford;University of Oxford;University of Oxford;University of Oxford,51;51;51;51,1;1;1;1,
677,677,677,677,677,677,677,677,ICLR,2018,Training wide residual networks for deployment using a single bit for each weight,Mark D. McDonnell,mark.mcdonnell@unisa.edu.au,6;6;6,4;3;4,Accept (Poster),0,8,0.0,yes,10/26/17,University of South Australia,468,248,
678,678,678,678,678,678,678,678,ICLR,2018,Can Neural Networks Understand Logical Entailment?,Richard Evans;David Saxton;David Amos;Pushmeet Kohli;Edward Grefenstette,richardevans@google.com;saxton@google.com;davidamos@google.com;pushmeet@google.com;etg@google.com,7;7;4,3;3;4,Accept (Poster),6,3,0.0,yes,10/27/17,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,
679,679,679,679,679,679,679,679,ICLR,2018,Learning Sparse Neural Networks through L_0 Regularization,Christos Louizos;Max Welling;Diederik P. Kingma,c.louizos@uva.nl;m.welling@uva.nl;dpkingma@openai.com,6;6;7,3;3;4,Accept (Poster),0,5,0.0,yes,10/27/17,University of Amsterdam;University of Amsterdam;OpenAI,181;181;-1,59;59;-1,8
680,680,680,680,680,680,680,680,ICLR,2018,Unsupervised Learning of Goal Spaces for Intrinsically Motivated Goal Exploration,Alexandre Péré;Sébastien Forestier;Olivier Sigaud;Pierre-Yves Oudeyer,alexandre.pere@inria.fr;sebastien.forestier@inria.fr;olivier.sigaud@upmc.fr;pierre-yves.oudeyer@inria.fr,7;6;7,4;2;4,Accept (Poster),0,6,0.0,yes,10/26/17,"INRIA;INRIA;Computer Science Lab  - Pierre and Marie Curie University, Paris, France;INRIA",-1;-1;468;-1,-1;-1;123;-1,
681,681,681,681,681,681,681,681,ICLR,2018,Learning From Noisy Singly-labeled Data,Ashish Khetan;Zachary C. Lipton;Animashree Anandkumar,khetan2@illinois.edu;zlipton@cmu.edu;anima@amazon.com,7;6;7,4;3;4,Accept (Poster),0,8,1.0,yes,10/27/17,"University of Illinois, Urbana Champaign;Carnegie Mellon University;Amazon",3;1;-1,37;24;-1,1;8
682,682,682,682,682,682,682,682,ICLR,2018,SEARNN: Training RNNs with global-local losses,Rémi Leblond;Jean-Baptiste Alayrac;Anton Osokin;Simon Lacoste-Julien,remi.leblond@inria.fr;jean-baptiste.alayrac@inria.fr;aosokin@hse.ru;slacoste@iro.umontreal.ca,8;5;7,4;5;3,Accept (Poster),0,12,0.0,yes,10/27/17,INRIA;INRIA;Higher School of Economics;University of Montreal,-1;-1;468;124,-1;-1;377;108,3
683,683,683,683,683,683,683,683,ICLR,2018,"Progressive Growing of GANs for Improved Quality, Stability, and Variation",Tero Karras;Timo Aila;Samuli Laine;Jaakko Lehtinen,tkarras@nvidia.com;taila@nvidia.com;slaine@nvidia.com;jlehtinen@nvidia.com,8;1;8,4;4;4,Accept (Oral),2,5,4.0,yes,10/27/17,NVIDIA;NVIDIA;NVIDIA;NVIDIA,-1;-1;-1;-1,-1;-1;-1;-1,5;4
684,684,684,684,684,684,684,684,ICLR,2018,Graph Attention Networks,Petar Veličković;Guillem Cucurull;Arantxa Casanova;Adriana Romero;Pietro Liò;Yoshua Bengio,petar.velickovic@cst.cam.ac.uk;gcucurull@gmail.com;ar.casanova.8@gmail.com;adriana.romsor@gmail.com;pietro.lio@cst.cam.ac.uk;yoshua.umontreal@gmail.com,6;7;5,4;5;4,Accept (Poster),13,5,0.0,yes,10/26/17,University of Cambridge;Universitat Autonoma de Barcelona;University of Montreal;Facebook;University of Cambridge;University of Montreal,71;468;124;-1;71;124,2;146;108;-1;2;108,10
685,685,685,685,685,685,685,685,ICLR,2018,i-RevNet: Deep Invertible Networks,Jörn-Henrik Jacobsen;Arnold W.M. Smeulders;Edouard Oyallon,joern.jacobsen@bethgelab.org;a.w.m.smeulders@uva.nl;edouard.oyallon@ens.fr,8;9;8,4;4;4,Accept (Poster),0,9,7.0,yes,10/27/17,"Centre for Integrative Neuroscience, AG Bethge;University of Amsterdam;Ecole Normale Superieure",-1;181;99,-1;59;72,
686,686,686,686,686,686,686,686,ICLR,2018,On the regularization of Wasserstein GANs,Henning Petzka;Asja Fischer;Denis Lukovnikov,henning.petzka@iais.fraunhofer.de;asja.fischer@gmail.com;lukovnik@cs.uni-bonn.de,7;2;6,4;2;5,Accept (Poster),0,8,0.0,yes,10/27/17,Fraunhofer IIS;University of Bonn;University of Bonn,-1;124;124,-1;100;100,5;4
687,687,687,687,687,687,687,687,ICLR,2018,Initialization matters: Orthogonal Predictive State Recurrent Neural Networks,Krzysztof Choromanski;Carlton Downey;Byron Boots,kchoro@google.com;cmdowney@cs.cmu.edu;bboots@cc.gatech.edu,4;8;7,5;4;2,Accept (Poster),0,0,0.0,yes,10/27/17,Google;Carnegie Mellon University;Georgia Institute of Technology,-1;1;13,-1;24;33,3
688,688,688,688,688,688,688,688,ICLR,2018,Adaptive Quantization of Neural Networks,Soroosh Khoram;Jing Li,khoram@wisc.edu;jli@ece.wisc.edu,6;6;6,4;4;3,Accept (Poster),0,4,1.0,yes,10/27/17,University of Southern California;University of Southern California,31;31,66;66,
689,689,689,689,689,689,689,689,ICLR,2018,Large Scale Optimal Transport and Mapping Estimation,Vivien Seguy;Bharath Bhushan Damodaran;Remi Flamary;Nicolas Courty;Antoine Rolet;Mathieu Blondel,vivienseguy@gmail.com;bharath-bhushan.damodaran@irisa.fr;remi.flamary@unice.fr;courty@univ-ubs.fr;antoine.rolet@iip.ist.i.kyoto-u.ac.jp;mblondel@gmail.com,7;6;6;8,3;3;3;3,Accept (Poster),0,8,1.0,yes,10/27/17,"Meiji University;IRISA, Université Bretagne Sud;Université Côte d'Azur;University of Bretagne Sud;Meiji University;NTT",468;468;-1;468;468;-1,334;1103;-1;1103;334;-1,5;1;8
690,690,690,690,690,690,690,690,ICLR,2018,Generalizing Across Domains via Cross-Gradient Training,Shiv Shankar*;Vihari Piratla*;Soumen Chakrabarti;Siddhartha Chaudhuri;Preethi Jyothi;Sunita Sarawagi,shivshankariitb@gmail.com;viharipiratla@gmail.com;soumen@cse.iitb.ac.in;sidch@cse.iitb.ac.in;pjyothi@cse.iitb.ac.in;sunita@iitb.ac.in,7;7;8;7,5;4;4;5,Accept (Poster),0,7,0.0,yes,10/27/17,Indian Institute of Technology Bombay;Indian Institute of Technology Bombay;Indian Institute of Technology Bombay;Indian Institute of Technology Bombay;Indian Institute of Technology Bombay;Indian Institute of Technology Bombay,115;115;115;115;115;115,367;367;367;367;367;367,4;11;8
691,691,691,691,691,691,691,691,ICLR,2018,Action-dependent Control Variates for Policy Optimization via Stein Identity,Hao Liu*;Yihao Feng*;Yi Mao;Dengyong Zhou;Jian Peng;Qiang Liu,uestcliuhao@gmail.com;yihao@cs.utexas.edu;maoyi@microsoft.com;dennyzhou@google.com;jianpeng@illinois.edu;lqiang@cs.utexas.edu,7;7;7,3;3;4,Accept (Poster),3,9,1.0,yes,10/27/17,"University of California Berkeley;University of Texas, Austin;Microsoft;Google;University of Illinois, Urbana Champaign;University of Texas, Austin",5;21;-1;-1;3;21,18;49;-1;-1;37;49,
692,692,692,692,692,692,692,692,ICLR,2018,Bi-Directional Block Self-Attention for Fast and Memory-Efficient Sequence Modeling,Tao Shen;Tianyi Zhou;Guodong Long;Jing Jiang;Chengqi Zhang,tao.shen@student.uts.edu.au;tianyizh@uw.edu;guodong.long@uts.edu.au;jing.jiang@uts.edu.au;chengqi.zhang@uts.edu.au,6;9;6,4;4;4,Accept (Poster),0,7,3.0,yes,10/27/17,"University of Technology Sydney;University of Washington, Seattle;University of Technology Sydney;University of Technology Sydney;University of Technology Sydney",115;6;115;115;115,216;25;216;216;216,3
693,693,693,693,693,693,693,693,ICLR,2018,Generalizing Hamiltonian Monte Carlo with Neural Networks,Daniel Levy;Matt D. Hoffman;Jascha Sohl-Dickstein,danilevy@cs.stanford.edu;mhoffman@google.com;jaschasd@google.com,7;6;8,4;3;2,Accept (Poster),0,13,0.0,yes,10/26/17,Stanford University;Google;Google,4;-1;-1,3;-1;-1,5
694,694,694,694,694,694,694,694,ICLR,2018,Beyond Word Importance:  Contextual Decomposition to Extract Interactions from LSTMs,W. James Murdoch;Peter J. Liu;Bin Yu,jmurdoch@berkeley.edu;peterjliu@google.com;binyu@berkeley.edu,7;7;7,3;4;2,Accept (Oral),0,8,1.0,yes,10/27/17,University of California Berkeley;Google;University of California Berkeley,5;-1;5,18;-1;18,
695,695,695,695,695,695,695,695,ICLR,2018,Temporally Efficient Deep Learning with Spikes,Peter O'Connor;Efstratios Gavves;Matthias Reisser;Max Welling,peter.ed.oconnor@gmail.com;e.gavves@uva.nl;reisser.matthias@gmail.com;m.welling@uva.nl,7;6;8,5;4;4,Accept (Poster),0,1,1.0,yes,10/27/17,;University of Amsterdam;University of Amsterdam;University of Amsterdam,-1;181;181;181,-1;59;59;59,
696,696,696,696,696,696,696,696,ICLR,2018,Learning Deep Mean Field Games for Modeling Large Population Behavior,Jiachen Yang;Xiaojing Ye;Rakshit Trivedi;Huan Xu;Hongyuan Zha,yjiachen@gmail.com;xye@gsu.edu;rstrivedi@gatech.edu;huan.xu@isye.gatech.edu;zha@cc.gatech.edu,8;8;10,4;3;5,Accept (Oral),0,6,0.0,yes,10/27/17,Lawrence Livermore National Labs;SUN YAT-SEN UNIVERSITY;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology,-1;468;13;13;13,-1;352;33;33;33,
697,697,697,697,697,697,697,697,ICLR,2018,Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models,Pouya Samangouei;Maya Kabkab;Rama Chellappa,pouya@umiacs.umd.edu;mayak@umiacs.umd.edu;rama@umiacs.umd.edu,6;6;8,3;3;4,Accept (Poster),1,12,0.0,yes,10/27/17,"University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park",12;12;12,69;69;69,5;4
698,698,698,698,698,698,698,698,ICLR,2018,Learning from Between-class Examples for Deep Sound Recognition,Yuji Tokozume;Yoshitaka Ushiku;Tatsuya Harada,tokozume@mi.t.u-tokyo.ac.jp;ushiku@mi.t.u-tokyo.ac.jp;harada@mi.t.u-tokyo.ac.jp,9;4;8,4;4;4,Accept (Poster),0,9,2.0,yes,10/27/17,The University of Tokyo;The University of Tokyo;The University of Tokyo,52;52;52,45;45;45,
699,699,699,699,699,699,699,699,ICLR,2018,META LEARNING SHARED HIERARCHIES,Kevin Frans;Jonathan Ho;Xi Chen;Pieter Abbeel;John Schulman,kevinfrans2@gmail.com;jonathanho@berkeley.edu;c.xi@eecs.berkeley.edu;pabbeel@cs.berkeley.edu;joschu@openai.com,6;4;7,3;4;3,Accept (Poster),0,13,0.0,yes,10/27/17,OpenAI;University of California Berkeley;University of California Berkeley;University of California Berkeley;OpenAI,-1;5;5;5;-1,-1;18;18;18;-1,
700,700,700,700,700,700,700,700,ICLR,2018,Stochastic Variational Video Prediction,Mohammad Babaeizadeh;Chelsea Finn;Dumitru Erhan;Roy H. Campbell;Sergey Levine,mb2@uiuc.edu;cbfinn@eecs.berkeley.edu;dumitru@google.com;rhc@illinois.edu;svlevine@eecs.berkeley.edu,7;7;7,5;4;4,Accept (Poster),0,9,0.0,yes,10/27/17,"University of Illinois, Urbana-Champaign;University of California Berkeley;Google;University of Illinois, Urbana Champaign;University of California Berkeley",3;5;-1;3;5,37;18;-1;37;18,
701,701,701,701,701,701,701,701,ICLR,2018,An efficient framework for learning sentence representations,Lajanugen Logeswaran;Honglak Lee,llajan@umich.edu;honglak@eecs.umich.edu,6;8;6,5;4;4,Accept (Poster),8,2,1.0,yes,10/27/17,University of Michigan;University of Michigan,8;8,21;21,3
702,702,702,702,702,702,702,702,ICLR,2018,Evaluating the Robustness of Neural Networks: An Extreme Value Theory Approach,Tsui-Wei Weng*;Huan Zhang*;Pin-Yu Chen;Jinfeng Yi;Dong Su;Yupeng Gao;Cho-Jui Hsieh;Luca Daniel,twweng@mit.edu;ecezhang@ucdavis.edu;pin-yu.chen@ibm.com;jinfengyi.ustc@gmail.com;dong.su@ibm.com;yupeng.gao@ibm.com;chohsieh@ucdavis.edu;dluca@mit.edu,7;7;7,3;3;1,Accept (Poster),0,7,2.0,yes,10/27/17,"Massachusetts Institute of Technology;University of California, Davis;International Business Machines;JD AI Research;International Business Machines;International Business Machines;University of California, Davis;Massachusetts Institute of Technology",2;78;-1;-1;-1;-1;78;2,5;54;-1;-1;-1;-1;54;5,4
703,703,703,703,703,703,703,703,ICLR,2018,Thermometer Encoding: One Hot Way To Resist Adversarial Examples,Jacob Buckman;Aurko Roy;Colin Raffel;Ian Goodfellow,buckman@google.com;aurkor@google.com;craffel@google.com;goodfellow@google.com,6;6;6,2;4;4,Accept (Poster),3,4,0.0,yes,10/27/17,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,4;8
704,704,704,704,704,704,704,704,ICLR,2018,Simulated+Unsupervised Learning With Adaptive Data Generation and Bidirectional Mappings,Kangwook Lee;Hoon Kim;Changho Suh,kw1jjang@gmail.com;gnsrla12@kaist.ac.kr;chsuh@kaist.ac.kr,6;6;3,3;4;4,Accept (Poster),0,4,0.0,yes,10/27/17,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,21;21;21,95;95;95,
705,705,705,705,705,705,705,705,ICLR,2018,Interpretable Counting for Visual Question Answering,Alexander Trott;Caiming Xiong;Richard Socher,atrott@salesforce.com;cxiong@salesforce.com;rsocher@salesforce.com,6;7;7,3;4;4,Accept (Poster),0,5,1.0,yes,10/27/17,SalesForce.com;SalesForce.com;SalesForce.com,-1;-1;-1,-1;-1;-1,
706,706,706,706,706,706,706,706,ICLR,2018,Hierarchical Representations for Efficient Architecture Search,Hanxiao Liu;Karen Simonyan;Oriol Vinyals;Chrisantha Fernando;Koray Kavukcuoglu,hanxiaol@cs.cmu.edu;simonyan@google.com;vinyals@google.com;chrisantha@google.com;korayk@google.com,6;6;8,3;4;4,Accept (Poster),0,3,0.0,yes,10/27/17,Carnegie Mellon University;Google;Google;Google;Google,1;-1;-1;-1;-1,24;-1;-1;-1;-1,
707,707,707,707,707,707,707,707,ICLR,2018,Multi-Scale Dense Networks for Resource Efficient Image Classification,Gao Huang;Danlu Chen;Tianhong Li;Felix Wu;Laurens van der Maaten;Kilian Weinberger,gh349@cornell.edu;taineleau@gmail.com;lth14@mails.tsinghua.edu.cn;fw245@cornell.edu;lvdmaaten@fb.com;kqw4@cornell.edu,8;7;10,4;4;4,Accept (Oral),0,3,2.0,yes,10/27/17,Cornell University;Fudan University;Tsinghua University;Cornell University;Facebook;Cornell University,7;78;10;7;-1;7,19;116;30;19;-1;19,
708,708,708,708,708,708,708,708,ICLR,2018,Sobolev GAN,Youssef Mroueh;Chun-Liang Li;Tom Sercu;Anant Raj;Yu Cheng,mroueh@us.ibm.com;chunlial@cs.cmu.edu;tom.sercu1@ibm.com;anant.raj@tuebingen.mpg.de;chengyu@us.ibm.com,8;6;6;7,4;3;4;3,Accept (Poster),0,16,0.0,yes,10/27/17,"International Business Machines;Carnegie Mellon University;International Business Machines;Max Planck Institute for Intelligent Systems, Max-Planck Institute;International Business Machines",-1;1;-1;-1;-1,-1;24;-1;-1;-1,5;4
709,709,709,709,709,709,709,709,ICLR,2018,Compositional Attention Networks for Machine Reasoning,Drew A. Hudson;Christopher D. Manning,dorarad@cs.stanford.edu;manning@cs.stanford.edu,7;7;6,4;3;4,Accept (Poster),3,7,1.0,yes,10/27/17,Stanford University;Stanford University,4;4,3;3,8
710,710,710,710,710,710,710,710,ICLR,2018,DCN+: Mixed Objective And Deep Residual Coattention for Question Answering,Caiming Xiong;Victor Zhong;Richard Socher,cxiong@salesforce.com;richard@socher.org;victor@victorzhong.com,7;6;8,4;4;2,Accept (Poster),1,13,0.0,yes,10/27/17,SalesForce.com;SalesForce.com;SalesForce.com,-1;-1;-1,-1;-1;-1,
711,711,711,711,711,711,711,711,ICLR,2018,Empirical Risk Landscape Analysis for Understanding Deep Neural Networks,Pan Zhou;Jiashi Feng,pzhou@u.nus.edu;elefjia@nus.edu.sg;panzhou3@gmail.com,3;7;7,3;3;3,Accept (Poster),0,3,0.0,yes,10/23/17,National University of Singapore;National University of Singapore;National University of Singapore,16;16;16,22;22;22,1;9;8
712,712,712,712,712,712,712,712,ICLR,2018,Monotonic Chunkwise Attention,Chung-Cheng Chiu*;Colin Raffel*,chungchengc@google.com;craffel@gmail.com,7;6;8,5;4;4,Accept (Poster),0,4,0.0,yes,10/27/17,Google;Google,-1;-1,-1;-1,
713,713,713,713,713,713,713,713,ICLR,2018,Compressing Word Embeddings via Deep Compositional Code Learning,Raphael Shu;Hideki Nakayama,shu@nlab.ci.i.u-tokyo.ac.jp;nakayama@ci.i.u-tokyo.ac.jp,8;6;7,4;4;4,Accept (Poster),0,8,0.0,yes,10/27/17,The University of Tokyo;The University of Tokyo,52;52,45;45,3;2
714,714,714,714,714,714,714,714,ICLR,2018,A Framework for the Quantitative Evaluation of Disentangled Representations,Cian Eastwood;Christopher K. I. Williams,s1668298@ed.ac.uk;ckiw@inf.ed.ac.uk,7;6;6,5;5;4,Accept (Poster),0,4,0.0,yes,10/27/17,University of Edinburgh;University of Edinburgh,33;33,27;27,
715,715,715,715,715,715,715,715,ICLR,2018,Adaptive Dropout with Rademacher Complexity Regularization,Ke Zhai;Huan Wang,zhaikedavy@gmail.com;joyousprince@gmail.com,6;6;7,3;3;5,Accept (Poster),0,17,1.0,yes,10/26/17,;,-1;-1,-1;-1,1
716,716,716,716,716,716,716,716,ICLR,2018,An Online Learning Approach to Generative Adversarial Networks,Paulina Grnarova;Kfir Y Levy;Aurelien Lucchi;Thomas Hofmann;Andreas Krause,paulina.grnarova@inf.ethz.ch;yehuda.levy@inf.ethz.ch;aurelien.lucchi@inf.ethz.ch;thomas.hofmann@inf.ethz.ch;krausea@ethz.ch,7;8;5,4;5;4,Accept (Poster),0,8,0.0,yes,10/26/17,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,9;9;9;9;9,10;10;10;10;10,5;4
717,717,717,717,717,717,717,717,ICLR,2018,Learning Robust Rewards with Adverserial Inverse Reinforcement Learning,Justin Fu;Katie Luo;Sergey Levine,justinjfu@eecs.berkeley.edu;katieluo@berkeley.edu;svlevine@eecs.berkeley.edu,6;6;7,4;2;3,Accept (Poster),4,11,1.0,yes,10/27/17,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,18;18;18,4
718,718,718,718,718,718,718,718,ICLR,2018,Variational Network Quantization,Jan Achterhold;Jan Mathias Koehler;Anke Schmeink;Tim Genewein,mail@janachterhold.de;jan.koehler@de.bosch.com;anke.schmeink@rwth-aachen.de;tim.genewein@gmail.com,7;7;7,5;4;3,Accept (Poster),1,5,1.0,yes,10/27/17,;Bosch;RWTH Aachen University;Bosch,-1;-1;99;-1,-1;-1;79;-1,11
719,719,719,719,719,719,719,719,ICLR,2018,Identifying Analogies Across Domains,Yedid Hoshen;Lior Wolf,yedidh@fb.com;wolf@fb.com,7;5;4,4;4;3,Accept (Poster),0,8,0.0,yes,10/27/17,Facebook;Facebook,-1;-1,-1;-1,
720,720,720,720,720,720,720,720,ICLR,2018,Improving the Improved Training of Wasserstein GANs: A Consistency Term and Its Dual Effect,Xiang Wei;Boqing Gong;Zixia Liu;Wei Lu;Liqiang Wang,yqweixiang@knights.ucf.edu;boqinggo@outlook.com;zixia@knights.ucf.edu;luwei@bjtu.edu.cn;lwang@cs.ucf.edu,4;6;7,4;5;4,Accept (Poster),16,11,0.0,yes,10/27/17,University of Central Florida;International Computer Science Institute;University of Central Florida;Beijing jiaotong univercity;University of Central Florida,81;-1;81;468;81,1103;-1;1103;854;1103,5;4
721,721,721,721,721,721,721,721,ICLR,2018,Unbiased Online Recurrent Optimization,Corentin Tallec;Yann Ollivier,corentin.tallec@polytechnique.edu;yann@yann-ollivier.org,6;7;8,4;4;5,Accept (Poster),0,5,0.0,yes,10/27/17,Ecole polytechnique;Facebook,468;-1,115;-1,10
722,722,722,722,722,722,722,722,ICLR,2018,Can recurrent neural networks warp time?,Corentin Tallec;Yann Ollivier,corentin.tallec@polytechnique.edu;yol@fb.com,8;8;8,4;4;4,Accept (Poster),0,9,0.0,yes,10/27/17,Ecole polytechnique;Facebook,468;-1,115;-1,1
723,723,723,723,723,723,723,723,ICLR,2018,Twin Networks: Matching the Future for Sequence Generation,Dmitriy Serdyuk;Nan Rosemary Ke;Alessandro Sordoni;Adam Trischler;Chris Pal;Yoshua Bengio,serdyuk.dmitriy@gmail.com;rosemary.nan.ke@gmail.com;alessandro.sordoni@gmail.com;adam.trischler@microsoft.com;chris.j.pal@gmail.com;yoshua.umontreal@gmail.com,6;7;8,4;4;4,Accept (Poster),0,10,0.0,yes,10/27/17,Element AI;Polytechnique Montreal;Microsoft;Microsoft;Ecole Polytechnique de Montreal;University of Montreal,-1;364;-1;-1;365;124,-1;108;-1;-1;1103;108,5
724,724,724,724,724,724,724,724,ICLR,2018,Semantically Decomposing the Latent Spaces of Generative Adversarial Networks,Chris Donahue;Zachary C. Lipton;Akshay Balsubramani;Julian McAuley,cdonahue@ucsd.edu;zlipton@cmu.edu;abalsubr@stanford.edu;jmcauley@cs.ucsd.edu,6;6;7,4;4;4,Accept (Poster),0,4,0.0,yes,10/27/17,"University of California, San Diego;Carnegie Mellon University;Stanford University;University of California, San Diego",11;1;4;11,31;24;3;31,5;4
725,725,725,725,725,725,725,725,ICLR,2018,Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting,Yaguang Li;Rose Yu;Cyrus Shahabi;Yan Liu,yaguang@usc.edu;rose@caltech.edu;shahabi@usc.edu;yanliu.cs@usc.edu,5;4;9,3;5;5,Accept (Poster),0,4,1.0,yes,10/27/17,University of Southern California;California Institute of Technology;University of Southern California;University of Southern California,31;139;31;31,66;3;66;66,10
726,726,726,726,726,726,726,726,ICLR,2018,PixelNN: Example-based Image Synthesis,Aayush Bansal;Yaser Sheikh;Deva Ramanan,aayushb@cs.cmu.edu;yaser@cs.cmu.edu;deva@cs.cmu.edu,8;6;7,4;4;3,Accept (Poster),0,3,0.0,yes,10/27/17,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1,24;24;24,5
727,727,727,727,727,727,727,727,ICLR,2018,Rethinking the Smaller-Norm-Less-Informative Assumption in Channel Pruning of Convolution Layers,Jianbo Ye;Xin Lu;Zhe Lin;James Z. Wang,jxy198@ist.psu.edu;xinl@adobe.com;zlin@adobe.com;jwang@ist.psu.edu,5;7;6,5;3;5,Accept (Poster),0,6,1.0,yes,10/25/17,Pennsylvania State University;Adobe Systems;Adobe Systems;Pennsylvania State University,40;-1;-1;40,77;-1;-1;77,10
728,728,728,728,728,728,728,728,ICLR,2018,A Deep Reinforced Model for Abstractive Summarization,Romain Paulus;Caiming Xiong;Richard Socher,rpaulus@salesforce.com;cxiong@salesforce.com;richard@socher.org,8;7;6,3;5;4,Accept (Poster),0,3,0.0,yes,10/27/17,SalesForce.com;SalesForce.com;SalesForce.com,-1;-1;-1,-1;-1;-1,
729,729,729,729,729,729,729,729,ICLR,2018,The Role of Minimal Complexity Functions in Unsupervised Learning of Semantic Mappings,Tomer Galanti;Lior Wolf;Sagie Benaim,tomer22g@gmail.com;liorwolf@gmail.com;sagiebenaim@gmail.com,7;7;6,4;2;4,Accept (Poster),0,6,0.0,yes,10/27/17,Tel Aviv University;Tel Aviv University;Tel Aviv University,37;37;37,217;217;217,
730,730,730,730,730,730,730,730,ICLR,2018,Emergent Translation in Multi-Agent Communication,Jason Lee;Kyunghyun Cho;Jason Weston;Douwe Kiela,jason@cs.nyu.edu;kyunghyun.cho@nyu.edu;jase@fb.com;dkiela@fb.com,8;7;5,5;3;5,Accept (Poster),0,5,0.0,yes,10/27/17,New York University;New York University;Facebook;Facebook,26;26;-1;-1,27;27;-1;-1,3
731,731,731,731,731,731,731,731,ICLR,2018,Few-shot Autoregressive Density Estimation: Towards Learning to Learn Distributions,Scott Reed;Yutian Chen;Thomas Paine;Aäron van den Oord;S. M. Ali Eslami;Danilo Rezende;Oriol Vinyals;Nando de Freitas,reedscot@google.com;yutianc@google.com;tpaine@google.com;avdnoord@google.com;aeslami@google.com;danilor@google.com;vinyals@google.com;nandodefreitas@google.com,6;7;6,5;4;4,Accept (Poster),1,1,0.0,yes,10/27/17,Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1,6
732,732,732,732,732,732,732,732,ICLR,2018,Kronecker-factored Curvature Approximations for Recurrent Neural Networks,James Martens;Jimmy Ba;Matt Johnson,james.martens@gmail.com;jimmy@psi.toronto.edu;mattjj@csail.mit.edu,7;5;7,4;4;3,Accept (Poster),1,9,0.0,yes,10/27/17,Google;University of Toronto;Massachusetts Institute of Technology,-1;17;2,-1;22;5,10
733,733,733,733,733,733,733,733,ICLR,2018,DORA The Explorer: Directed Outreaching Reinforcement Action-Selection,Lior Fox;Leshem Choshen;Yonatan Loewenstein,lior.fox@mail.huji.ac.il;leshem.choshen@mail.huji.ac.il;yonatan.loewenstein@mail.huji.ac.il,7;6;6,3;4;4,Accept (Poster),0,5,0.0,yes,10/27/17,Hebrew University of Jerusalem;Hebrew University of Jerusalem;Hebrew University of Jerusalem,62;62;62,205;205;205,8
734,734,734,734,734,734,734,734,ICLR,2018,The Reactor: A fast and sample-efficient Actor-Critic agent for  Reinforcement Learning,Audrunas Gruslys;Will Dabney;Mohammad Gheshlaghi Azar;Bilal Piot;Marc Bellemare;Remi Munos,audrunas@google.com;wdabney@google.com;mazar@google.com;piot@google.com;bellemare@google.com;munos@google.com,7;7;7,4;2;4,Accept (Poster),0,6,0.0,yes,10/27/17,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,
735,735,735,735,735,735,735,735,ICLR,2018,Gaussian Process Behaviour in Wide Deep Neural Networks,Alexander G. de G. Matthews;Jiri Hron;Mark Rowland;Richard E. Turner;Zoubin Ghahramani,am554@cam.ac.uk;jh2084@cam.ac.uk;mr504@cam.ac.uk;ret26@cam.ac.uk;zoubin@eng.cam.ac.uk,6;6;6,4;4;4,Accept (Poster),0,3,1.0,yes,10/27/17,University of Cambridge;University of Cambridge;University of Cambridge;University of Cambridge;University of Cambridge,71;71;71;71;71,2;2;2;2;2,11;9
736,736,736,736,736,736,736,736,ICLR,2018,On the Expressive Power of Overlapping Architectures of Deep Learning,Or Sharir;Amnon Shashua,or.sharir@cs.huji.ac.il;shashua@cs.huji.ac.il,6;8;6,4;3;4,Accept (Poster),0,3,0.0,yes,10/27/17,Hebrew University of Jerusalem;Hebrew University of Jerusalem,62;62,205;205,
737,737,737,737,737,737,737,737,ICLR,2018,An image representation based convolutional network for DNA classification,Bojian Yin;Marleen Balvert;Davide Zambrano;Alexander Schoenhuth;Sander Bohte,yinbojian93@gmail.com;m.balvert@cwi.nl;d.zambrano@cwi.nl;a.schoenhuth@cwi.nl;s.m.bohte@cwi.nl,7;7;7,5;3;5,Accept (Poster),0,11,0.0,yes,10/27/17,cwi;Centrum voor Wiskunde en Informatica;Centrum voor Wiskunde en Informatica;Centrum voor Wiskunde en Informatica;Centrum voor Wiskunde en Informatica,210;-1;-1;-1;-1,1103;-1;-1;-1;-1,
738,738,738,738,738,738,738,738,ICLR,2018,Deep Learning and Quantum Entanglement: Fundamental Connections with Implications to Network Design,Yoav Levine;David Yakira;Nadav Cohen;Amnon Shashua,yoavlevine@cs.huji.ac.il;davidyakira@cs.huji.ac.il;cohennadav@cs.huji.ac.il;shashua@cs.huji.ac.il,6;7;8;6,4;3;5;2,Accept (Poster),0,6,0.0,yes,10/26/17,Hebrew University of Jerusalem;Hebrew University of Jerusalem;Hebrew University of Jerusalem;Hebrew University of Jerusalem,62;62;62;62,205;205;205;205,10
739,739,739,739,739,739,739,739,ICLR,2018,Critical Percolation as a Framework to Analyze the Training of Deep Networks,Zohar Ringel;Rodrigo Andrade de Bem,zoharahoz@gmail.com;rodrigo.bem@gmail.com,7;7;6,3;3;1,Accept (Poster),0,3,0.0,yes,10/26/17,Hebrew University of Jerusalem;University of Oxford,62;51,205;1,10
740,740,740,740,740,740,740,740,ICLR,2018,Parallelizing Linear Recurrent Neural Nets Over Sequence Length,Eric Martin;Chris Cundy,eric@ericmart.in;chris.j.cundy@gmail.com,6;7;7,3;2;4,Accept (Poster),0,3,0.0,yes,10/27/17,California Institute of Technology;University of Cambridge,139;71,3;2,
741,741,741,741,741,741,741,741,ICLR,2018,Guide Actor-Critic for Continuous Control,Voot Tangkaratt;Abbas Abdolmaleki;Masashi Sugiyama,voot.tangkaratt@riken.jp;abbas.a@ua.pt;masashi.sugiyama@riken.jp,6;4;7,2;4;4,Accept (Poster),0,5,0.0,yes,10/27/17,RIKEN;;RIKEN,-1;-1;-1,-1;-1;-1,
742,742,742,742,742,742,742,742,ICLR,2018,Multi-Task Learning for Document Ranking and Query Suggestion,Wasi Uddin Ahmad;Kai-Wei Chang;Hongning Wang,wasiahmad@cs.ucla.edu;kwchang@cs.ucla.edu;hw5x@virginia.edu,4;6;7,4;4;4,Accept (Poster),0,5,0.0,yes,10/27/17,"University of California, Los Angeles;University of California, Los Angeles;University of Virginia",20;20;62,15;15;113,
743,743,743,743,743,743,743,743,ICLR,2018,Hyperparameter optimization: a spectral approach,Elad Hazan;Adam Klivans;Yang Yuan,ehazan@cs.princeton.edu;klivans@cs.utexas.edu;yangyuan@cs.cornell.edu,6;6;9,4;3;5,Accept (Poster),0,3,0.0,yes,10/27/17,"Princeton University;University of Texas, Austin;Cornell University",31;21;7,7;49;19,11
744,744,744,744,744,744,744,744,ICLR,2018,Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence Learning,Wei Ping;Kainan Peng;Andrew Gibiansky;Sercan O. Arik;Ajay Kannan;Sharan Narang;Jonathan Raiman;John Miller,pingwei01@baidu.com;pengkainan@baidu.com;gibianskyandrew@baidu.com;sercanarik@baidu.com;kannanajay@baidu.com;sharan@baidu.com;raiman@openai.com;miller_john@berkeley.edu,7;6;6,4;5;3,Accept (Poster),0,4,0.0,yes,10/24/17,Baidu;Baidu;Baidu;Baidu;Baidu;Baidu;OpenAI;University of California Berkeley,-1;-1;-1;-1;-1;-1;-1;5,-1;-1;-1;-1;-1;-1;-1;18,
745,745,745,745,745,745,745,745,ICLR,2018,Divide and Conquer Networks,Alex Nowak;David Folqué;Joan Bruna,alexnowakvila@gmail.com;david.folque@gmail.com;bruna@cims.nyu.edu,6;7;7,3;3;3,Accept (Poster),0,2,0.0,yes,10/27/17,Ecole Normale Superieure;Universitat Politècnica de Catalunya;New York University,99;468;26,72;1103;27,8
746,746,746,746,746,746,746,746,ICLR,2018,When is a Convolutional Filter Easy to Learn?,Simon S. Du;Jason D. Lee;Yuandong Tian,ssdu@cs.cmu.edu;jasonlee@marshall.usc.edu;yuandong@fb.com,6;9;8,3;4;3,Accept (Poster),0,5,0.0,yes,10/25/17,Carnegie Mellon University;University of Southern California;Facebook,1;31;-1,24;66;-1,1;9
747,747,747,747,747,747,747,747,ICLR,2018,SpectralNet: Spectral Clustering using Deep Neural Networks,Uri Shaham;Kelly Stanton;Henry Li;Ronen Basri;Boaz Nadler;Yuval Kluger,uri.shaham@yale.edu;kelly.stanton@yale.edu;henry.li@yale.edu;ronen.basri@gmail.com;boaz.nadler@gmail.com;yuval.kluger@yale.edu,6;4;7,3;4;5,Accept (Poster),2,7,1.0,yes,10/26/17,Yale University;Yale University;Yale University;Weizmann Institute;;Yale University,62;62;62;104;-1;62,12;12;12;1103;-1;12,10;1;8
748,748,748,748,748,748,748,748,ICLR,2018,Boundary Seeking GANs,R Devon Hjelm;Athul Paul Jacob;Adam Trischler;Gerry Che;Kyunghyun Cho;Yoshua Bengio,erroneus@gmail.com;apjacob@uwaterloo.ca;adam.trischler@microsoft.com;tong.che@umontreal.ca;kyunghyun.cho@nyu.edu;yoshua.bengio@umontreal.ca,7;7;4,4;3;3,Accept (Poster),0,12,0.0,yes,10/27/17,University of Montreal;University of Waterloo;Microsoft;University of Montreal;New York University;University of Montreal,124;26;-1;124;26;124,108;207;-1;108;27;108,3;4;5
749,749,749,749,749,749,749,749,ICLR,2018,Natural Language Inference over Interaction Space,Yichen Gong;Heng Luo;Jian Zhang,yichen.gong@nyu.edu;heng.luo@hobot.cc;jian.zhang@hobot.cc,5;6;6,5;4;4,Accept (Poster),0,7,2.0,yes,10/19/17,New York University;Horizon Robotics Inc.;Horizon Robotics Inc.,26;-1;-1,27;-1;-1,3
750,750,750,750,750,750,750,750,ICLR,2018,Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks,Shiyu Liang;Yixuan Li;R. Srikant,sliang26@illinois.edu;yli@cs.cornell.edu;rsrikant@illinois.edu;liangshiyu@icloud.com;yl2363@cornell.edu,6;6;9,4;3;3,Accept (Poster),0,6,1.0,yes,10/27/17,"University of Illinois, Urbana Champaign;Cornell University;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;Cornell University",3;7;3;3;7,37;19;37;37;19,
751,751,751,751,751,751,751,751,ICLR,2018,Deep Autoencoding Gaussian Mixture Model for Unsupervised Anomaly Detection,Bo Zong;Qi Song;Martin Renqiang Min;Wei Cheng;Cristian Lumezanu;Daeki Cho;Haifeng Chen,bzong@nec-labs.com;qsong@nec-labs.com;renqiang@nec-labs.com;weicheng@nec-labs.com;lume@nec-labs.com;dkcho@nec-labs.com;haifeng@nec-labs.com,8;8;8,5;4;4,Accept (Poster),0,8,0.0,yes,10/27/17,NEC-Labs;NEC-Labs;NEC-Labs;NEC-Labs;NEC-Labs;NEC-Labs;NEC-Labs,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,
752,752,752,752,752,752,752,752,ICLR,2018,Smooth Loss Functions for Deep Top-k Classification,Leonard Berrada;Andrew Zisserman;M. Pawan Kumar,lberrada@robots.ox.ac.uk;az@robots.ox.ac.uk;pawan@robots.ox.ac.uk,6;7;8,5;4;4,Accept (Poster),0,4,0.0,yes,10/27/17,University of Oxford;University of Oxford;University of Oxford,51;51;51,1;1;1,2
753,753,753,753,753,753,753,753,ICLR,2018,Active Learning for Convolutional Neural Networks: A Core-Set Approach,Ozan Sener;Silvio Savarese,ozansener@cs.stanford.edu;ssilvio@stanford.edu,7;7;7,4;4;3,Accept (Poster),0,10,2.0,yes,10/27/17,Stanford University;Stanford University,4;4,3;3,
754,754,754,754,754,754,754,754,ICLR,2018,Debiasing Evidence Approximations: On Importance-weighted Autoencoders and Jackknife Variational Inference,Sebastian Nowozin,sebastian.nowozin@microsoft.com,7;6;7,4;3;3,Accept (Poster),2,6,1.0,yes,10/27/17,Microsoft,-1,-1,5;1
755,755,755,755,755,755,755,755,ICLR,2018,Unsupervised Machine Translation Using Monolingual Corpora Only,Guillaume Lample;Alexis Conneau;Ludovic Denoyer;Marc'Aurelio Ranzato,glample@fb.com;aconneau@fb.com;ludovic.denoyer@lip6.fr;ranzato@fb.com,8;7;7,5;5;4,Accept (Poster),3,8,0.0,yes,10/27/17,Facebook;Facebook;LIP6;Facebook,-1;-1;-1;-1,-1;-1;-1;-1,3
756,756,756,756,756,756,756,756,ICLR,2018,Word translation without parallel data,Guillaume Lample;Alexis Conneau;Marc'Aurelio Ranzato;Ludovic Denoyer;Hervé Jégou,glample@fb.com;aconneau@fb.com;ranzato@fb.com;ludovic.denoyer@upmc.fr;rvj@fb.com,9;3;8,4;5;3,Accept (Poster),2,10,0.0,yes,10/11/17,"Facebook;Facebook;Facebook;Computer Science Lab  - Pierre and Marie Curie University, Paris, France;Facebook",-1;-1;-1;468;-1,-1;-1;-1;123;-1,3;4
757,757,757,757,757,757,757,757,ICLR,2018,Wavelet Pooling for Convolutional Neural Networks,Travis Williams;Robert Li,tlwilli3@aggies.ncat.edu;eeli@ncat.edu,7;9;4,4;3;4,Accept (Poster),0,5,3.0,yes,10/27/17,North Carolina A&T State University;North Carolina A&T State University,468;468,1103;1103,
758,758,758,758,758,758,758,758,ICLR,2018,WRPN: Wide Reduced-Precision Networks,Asit Mishra;Eriko Nurvitadhi;Jeffrey J Cook;Debbie Marr,asit.k.mishra@intel.com;eriko.nurvitadhi@intel.com;jeffrey.j.cook@intel.com;debbie.marr@intel.com,5;9;5,3;4;4,Accept (Poster),0,8,0.0,yes,10/27/17,Intel;Intel;Intel;Intel,-1;-1;-1;-1,-1;-1;-1;-1,2
759,759,759,759,759,759,759,759,ICLR,2018,Skip Connections Eliminate Singularities,Emin Orhan;Xaq Pitkow,aeminorhan@gmail.com;xaq@rice.edu,8;8;6,3;3;4,Accept (Poster),0,4,0.0,yes,10/27/17,Rice University;Rice University,85;85,86;86,
760,760,760,760,760,760,760,760,ICLR,2018,Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step,William Fedus*;Mihaela Rosca*;Balaji Lakshminarayanan;Andrew M. Dai;Shakir Mohamed;Ian Goodfellow,liam.fedus@gmail.com;mihaelacr@google.com;balajiln@google.com;adai@google.com;shakir@google.com;goodfellow@google.com,8;4;7,5;4;3,Accept (Poster),6,7,0.0,yes,10/27/17,University of Montreal;Google;Google;Google;Google;Google,124;-1;-1;-1;-1;-1,108;-1;-1;-1;-1;-1,5;4
761,761,761,761,761,761,761,761,ICLR,2018,YellowFin and the Art of Momentum Tuning,Jian Zhang;Ioannis Mitliagkas;Christopher Re,zjian@cs.stanford.edu;ioannis@iro.umontreal.ca;chrismre@cs.stanford.edu,4;4;6,3;5;1,Reject,0,11,0.0,yes,10/27/17,Stanford University;University of Montreal;Stanford University,4;124;4,3;108;3,3
762,762,762,762,762,762,762,762,ICLR,2018,HexaConv,Emiel Hoogeboom;Jorn W.T. Peters;Taco S. Cohen;Max Welling,e.hoogeboom@gmail.com;jornpeters@gmail.com;taco.cohen@gmail.com;welling.max@gmail.com,7;7;7,4;4;4,Accept (Poster),0,5,0.0,yes,10/27/17,University of Amsterdam;University of Amsterdam;University of Amsterdam;University of California - Irvine,181;181;181;36,59;59;59;99,
763,763,763,763,763,763,763,763,ICLR,2018,Few-Shot Learning with Graph Neural Networks,Victor Garcia Satorras;Joan Bruna Estrach,vgsatorras@gmail.com;bruna@cims.nyu.edu,7;7;7,4;4;4,Accept (Poster),0,13,0.0,yes,10/27/17,New York University;New York University,26;26,27;27,6;10
764,764,764,764,764,764,764,764,ICLR,2018,Proximal Backpropagation,Thomas Frerix;Thomas Möllenhoff;Michael Moeller;Daniel Cremers,thomas.frerix@tum.de;thomas.moellenhoff@in.tum.de;michael.moeller@uni-siegen.de;cremers@tum.de,6;5;7,4;4;4,Accept (Poster),0,5,0.0,yes,10/27/17,Technical University Munich;Technical University Munich;University of Siegen;Technical University Munich,55;55;364;55,41;41;431;41,1
765,765,765,765,765,765,765,765,ICLR,2018,Wasserstein Auto-Encoders,Ilya Tolstikhin;Olivier Bousquet;Sylvain Gelly;Bernhard Schoelkopf,iliya.tolstikhin@gmail.com;obousquet@gmail.com;sylvain.gelly@gmail.com;bs@tuebingen.mpg.de,8;8;8,3;3;4,Accept (Oral),0,9,8.0,yes,10/27/17,"Max-Planck Institute;Google;Google;Max Planck Institute for Intelligent Systems, Max-Planck Institute",-1;-1;-1;-1,-1;-1;-1;-1,5;4;8
766,766,766,766,766,766,766,766,ICLR,2018,The High-Dimensional Geometry of Binary Neural Networks,Alexander G. Anderson;Cory P. Berg,aga@berkeley.edu;cberg500@berkeley.edu,7;4;7,4;3;4,Accept (Poster),0,8,0.0,yes,10/27/17,University of California Berkeley;University of California Berkeley,5;5,18;18,
767,767,767,767,767,767,767,767,ICLR,2018,Critical Points of Linear Neural Networks: Analytical Forms and Landscape Properties,Yi Zhou;Yingbin Liang,zhou.1172@osu.edu;liang.889@osu.edu,7;7;6,3;5;4,Accept (Poster),0,6,0.0,yes,10/27/17,Ohio State University;Ohio State University,-1;-1,-1;-1,
768,768,768,768,768,768,768,768,ICLR,2018,Overcoming Catastrophic Interference using Conceptor-Aided Backpropagation,Xu He;Herbert Jaeger,x.he@jacobs-university.de;h.jaeger@jacobs-university.de,7;7;7,5;3;3,Accept (Poster),0,4,0.0,yes,10/27/17,Jacobs University Bremen;Jacobs University Bremen,364;364,1103;1103,
769,769,769,769,769,769,769,769,ICLR,2018,Self-ensembling for visual domain adaptation,Geoff French;Michal Mackiewicz;Mark Fisher,g.french@uea.ac.uk;m.mackiewicz@uea.ac.uk;m.fisher@uea.ac.uk,7;7;7,4;3;5,Accept (Poster),1,5,2.0,yes,10/27/17,;;,-1;-1;-1,-1;-1;-1,
770,770,770,770,770,770,770,770,ICLR,2018,Synthesizing realistic neural population activity patterns using Generative Adversarial Networks,Manuel Molano-Mazon;Arno Onken;Eugenio Piasini*;Stefano Panzeri*,manuel.molano@iit.it;aonken@inf.ed.ac.uk;epiasini@sas.upenn.edu;stefano.panzeri@iit.it,8;4;6,5;4;3,Accept (Poster),1,4,0.0,yes,10/27/17,Istituto Italiano di Tecnologia;University of Edinburgh;University of Pennsylvania;Istituto Italiano di Tecnologia,468;33;19;468,1103;27;10;1103,5;4
771,771,771,771,771,771,771,771,ICLR,2018,Minimal-Entropy Correlation Alignment for Unsupervised Deep Domain Adaptation,Pietro Morerio;Jacopo Cavazza;Vittorio Murino,pietro.morerio@iit.it;jacopo.cavazza@iit.it;vittorio.murino@iit.it,6;7;8,5;5;4,Accept (Poster),0,5,1.0,yes,10/27/17,Istituto Italiano di Tecnologia;Istituto Italiano di Tecnologia;Istituto Italiano di Tecnologia,468;468;468,1103;1103;1103,
772,772,772,772,772,772,772,772,ICLR,2018,Activation Maximization Generative Adversarial Nets,Zhiming Zhou;Han Cai;Shu Rong;Yuxuan Song;Kan Ren;Weinan Zhang;Jun Wang;Yong Yu,heyohai@apex.sjtu.edu.cn;hcai@apex.sjtu.edu.cn;shu.rong@yitu-inc.com;songyuxuan@apex.sjtu.edu.cn;kren@apex.sjtu.edu.cn;wnzhang@sjtu.edu.cn;j.wang@cs.ucl.ac.uk;yyu@apex.sjtu.edu.cn,5;7;8,4;4;4,Accept (Poster),3,5,0.0,yes,10/27/17,Shanghai Jiao Tong University;Shanghai Jiao Tong University;YiTu Technology co. ltd;Shanghai Jiao Tong University;Shanghai Jiao Tong University;Shanghai Jiao Tong University;University College London;Shanghai Jiao Tong University,57;57;-1;57;57;57;46;57,188;188;-1;188;188;188;16;188,5;4
773,773,773,773,773,773,773,773,ICLR,2018,Efficient Sparse-Winograd Convolutional Neural Networks,Xingyu Liu;Jeff Pool;Song Han;William J. Dally,xyl@stanford.edu;jpool@nvidia.com;songhan@stanford.edu;dally@stanford.edu,7;7;8,3;4;4,Accept (Poster),0,6,1.0,yes,10/26/17,Stanford University;NVIDIA;Stanford University;Stanford University,4;-1;4;4,3;-1;3;3,
774,774,774,774,774,774,774,774,ICLR,2018,Neural Language Modeling by Jointly Learning Syntax and Lexicon,Yikang Shen;Zhouhan Lin;Chin-wei Huang;Aaron Courville,yikang.shn@gmail.com;lin.zhouhan@gmail.com;chin-wei.huang@umontreal.ca;aaron.courville@gmail.com,7;8;7,3;4;4,Accept (Poster),2,5,7.0,yes,10/27/17,University of Montreal;University of Montreal;University of Montreal;University of Montreal,124;124;124;124,108;108;108;108,3
775,775,775,775,775,775,775,775,ICLR,2018,Imitation Learning from Visual Data with Multiple Intentions,Aviv Tamar;Khashayar Rohanimanesh;Yinlam Chow;Chris Vigorito;Ben Goodrich;Michael Kahane;Derik Pridmore,avivt@berkeley.edu;khash@osaro.com;yldick.chow@gmail.com;chris@osaro.com;ben@osaro.com;mk@osaro.com;derik@osaro.com,6;4;6,4;3;4,Accept (Poster),0,1,0.0,yes,10/27/17,University of California Berkeley;Osaro Inc.;Google;Osaro Inc.;Osaro Inc.;Osaro Inc.;Osaro Inc.,5;-1;-1;-1;-1;-1;-1,18;-1;-1;-1;-1;-1;-1,
776,776,776,776,776,776,776,776,ICLR,2018,Neumann Optimizer: A Practical Optimization Algorithm for Deep Neural Networks,Shankar Krishnan;Ying Xiao;Rif. A. Saurous,skrishnan@google.com;yingxiao@google.com;rif@google.com,6;6;6,4;3;3,Accept (Poster),1,9,0.0,yes,10/27/17,Google;Google;Google,-1;-1;-1,-1;-1;-1,
777,777,777,777,777,777,777,777,ICLR,2018,TD or not TD: Analyzing the Role of Temporal Differencing in Deep Reinforcement Learning,Artemij Amiranashvili;Alexey Dosovitskiy;Vladlen Koltun;Thomas Brox,amiranas@cs.uni-freiburg.de;adosovitskiy@gmail.com;vkoltun@gmail.com;brox@cs.uni-freiburg.de,7;7;7,4;4;4,Accept (Poster),0,3,0.0,yes,10/27/17,Universität Freiburg;Intel;Intel;Universität Freiburg,115;-1;-1;115,82;-1;-1;82,
778,778,778,778,778,778,778,778,ICLR,2018,All-but-the-Top: Simple and Effective Postprocessing for Word Representations,Jiaqi Mu;Pramod Viswanath,jiaqimu2@illinois.edu;pramodv@illinois.edu,6;7;7,5;4;4,Accept (Poster),0,3,0.0,yes,10/26/17,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",3;3,37;37,3
779,779,779,779,779,779,779,779,ICLR,2018,Spectral Normalization for Generative Adversarial Networks,Takeru Miyato;Toshiki Kataoka;Masanori Koyama;Yuichi Yoshida,miyato@preferred.jp;kataoka@preferred.jp;koyama.masanori@gmail.com;yyoshida@nii.ac.jp,7;8;7,4;3;2,Accept (Oral),8,15,4.0,yes,10/23/17,"Preferred Networks, Inc.;Preferred Networks, Inc.;Preferred Networks, Inc.;Meiji University",-1;-1;-1;468,-1;-1;-1;334,5;4
780,780,780,780,780,780,780,780,ICLR,2018,Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models,Wieland Brendel *;Jonas Rauber *;Matthias Bethge,wieland.brendel@bethgelab.org;jonas.rauber@bethgelab.org;matthias.bethge@bethgelab.org,7;7;8,4;4;3,Accept (Poster),2,9,0.0,yes,10/27/17,"Centre for Integrative Neuroscience, AG Bethge;Centre for Integrative Neuroscience, AG Bethge;Centre for Integrative Neuroscience, AG Bethge",-1;-1;-1,-1;-1;-1,4;2
781,781,781,781,781,781,781,781,ICLR,2018,Relational Neural Expectation Maximization: Unsupervised Discovery of Objects and their Interactions,Sjoerd van Steenkiste;Michael Chang;Klaus Greff;Jürgen Schmidhuber,sjoerd@idsia.ch;mbchang@berkeley.edu;klaus@idsia.ch;juergen@idsia.ch,8;7;7,5;4;3,Accept (Poster),0,6,0.0,yes,10/27/17,IDSIA;University of California Berkeley;IDSIA;IDSIA,-1;5;-1;-1,-1;18;-1;-1,
782,782,782,782,782,782,782,782,ICLR,2018,cGANs with Projection Discriminator,Takeru Miyato;Masanori Koyama,miyato@preferred.jp;koyama.masanori@gmail.com,6;7;6,4;5;4,Accept (Poster),0,9,1.0,yes,10/27/17,"Preferred Networks, Inc.;Preferred Networks, Inc.",-1;-1,-1;-1,5
783,783,783,783,783,783,783,783,ICLR,2018,Latent Constraints: Learning to Generate Conditionally from Unconditional Generative Models,Jesse Engel;Matthew Hoffman;Adam Roberts,jesseengel@google.com;mhoffman@google.com;adarob@google.com,7;7;7,4;3;3,Accept (Poster),0,4,0.0,yes,10/27/17,Google;Google;Google,-1;-1;-1,-1;-1;-1,5;6
784,784,784,784,784,784,784,784,ICLR,2018,Learning Intrinsic Sparse Structures within Long Short-Term Memory,Wei Wen;Yuxiong He;Samyam Rajbhandari;Minjia Zhang;Wenhan Wang;Fang Liu;Bin Hu;Yiran Chen;Hai Li,wei.wen@duke.edu;yuxhe@microsoft.com;samyamr@microsoft.com;minjiaz@microsoft.com;wenhanw@microsoft.com;fangliu@microsoft.com;binhu@microsoft.com;yiran.chen@duke.edu;hai.li@duke.edu,7;6;7,4;4;4,Accept (Poster),1,6,1.0,yes,10/1/17,Duke University;Microsoft;Microsoft;Microsoft;Microsoft;Microsoft;Microsoft;Duke University;Duke University,46;-1;-1;-1;-1;-1;-1;46;46,17;-1;-1;-1;-1;-1;-1;17;17,3
785,785,785,785,785,785,785,785,ICLR,2018,Coupled Ensembles of Neural Networks,Anuvabh Dutt;Denis Pellerin;Georges Quénot,anuvabh.dutt@univ-grenoble-alpes.fr;denis.pellerin@gipsa-lab.grenoble-inp.fr;georges.quenot@imag.fr,6;6;6,4;4;4,Reject,0,4,0.0,yes,10/3/17,University of Grenoble-Alpes;University of Grenoble-Alpes;Imag Montpellier Université,468;468;-1,321;321;-1,
786,786,786,786,786,786,786,786,ICLR,2018,Exploring Sentence Vectors Through Automatic Summarization,Adly Templeton;Jugal Kalita,at7@williams.edu;jkalita@uccs.edu,2;2;3,5;5;5,Reject,0,0,0.0,yes,10/10/17,"Williams College;University of Colorado, Colorado Springs",468;468,1103;1103,3
787,787,787,787,787,787,787,787,ICLR,2018,Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates,Leslie N. Smith;Nicholay Topin,leslie.smith@nrl.navy.mil;ntopin1@umbc.edu,4;4;4,3;4;3,Reject,2,4,0.0,yes,10/11/17,US Naval Research Laboratory;Boston College,-1;291,-1;309,1;8
788,788,788,788,788,788,788,788,ICLR,2018,DOUBLY STOCHASTIC ADVERSARIAL AUTOENCODER,Mahdi Azarafrooz,mazarafrooz@cylance.com,3;3;2,4;5;5,Reject,0,3,0.0,yes,10/12/17,Cylance,-1,-1,5;4
789,789,789,789,789,789,789,789,ICLR,2018,Incremental Learning through Deep Adaptation,Amir Rosenfeld;John K. Tsotsos,amir.rosenfeld@gmail.com,6;4;5,4;4;4,Reject,0,3,0.0,yes,10/13/17,York University,153,350,6
790,790,790,790,790,790,790,790,ICLR,2018,Spontaneous Symmetry Breaking in Deep Neural Networks,Ricky Fok;Aijun An;Xiaogang Wang,ricky.fok3@gmail.com,3;3;3,4;3;3,Reject,0,0,0.0,yes,10/18/17,York University,153,350,
791,791,791,791,791,791,791,791,ICLR,2018,Distributed non-parametric deep and wide networks,Biswa Sengupta;Yu Qian,biswasengupta@yahoo.com;yu.qian@cortexica.com,3;3;3,4;5;4,Reject,0,0,0.0,yes,10/18/17,Imperial College London;,74;-1,8;-1,
792,792,792,792,792,792,792,792,ICLR,2018,Ensemble Methods as a Defense to Adversarial Perturbations Against Deep Neural Networks,Thilo Strauss;Markus Hanselmann;Andrej Junginger;Holger Ulmer,thilo.strauss@etas.com;markus.hanselmann@etas.com;andrej.junginger@etas.com;holger.ulmer@etas.com,7;5;4,3;3;4,Reject,0,4,0.0,yes,10/18/17,ETAS GmbH;ETAS GmbH;ETAS GmbH;ETAS GmbH,-1;-1;-1;-1,-1;-1;-1;-1,4
793,793,793,793,793,793,793,793,ICLR,2018,Learning Less-Overlapping Representations,Hongbao Zhang;Pengtao Xie;Eric Xing,hongbao.zhang@petuum.com;pengtaox@cs.cmu.edu;eric.xing@petuum.com,5;4;3,4;4;5,Reject,0,0,0.0,yes,10/19/17,Petuum Inc.;Carnegie Mellon University;Petuum Inc.,-1;1;-1,-1;24;-1,8
794,794,794,794,794,794,794,794,ICLR,2018,Reward Design in Cooperative Multi-agent Reinforcement Learning for Packet Routing,Hangyu Mao;Zhibo Gong;Zhen Xiao,pku.hy.mao@gmail.com;gongzhibo@huawei.com;gtxaio@gmail.com,5;2;5,3;4;2,Reject,0,3,0.0,yes,10/19/17,Peking University;Huawei Technologies Ltd.;,24;-1;-1,27;-1;-1,
795,795,795,795,795,795,795,795,ICLR,2018,Iterative temporal differencing with fixed random feedback alignment support spike-time dependent plasticity in vanilla backpropagation for deep learning,Aras Dargazany;Kunal Mankodiya,arasdar@uri.edu,3;2;2,4;5;5,Reject,0,0,0.0,yes,10/19/17,University of Rhode Island,468,1103,
796,796,796,796,796,796,796,796,ICLR,2018,Reinforcement Learning via Replica Stacking of Quantum Measurements for the Training of Quantum Boltzmann Machines,Anna Levit; Daniel Crawford;Navid Ghadermarzy;Jaspreet S. Oberoi;Ehsan Zahedinejad;Pooya Ronagh,anna.levit@1qbit.com;daniel.crawford@1qbit.com;navid.ghadermarzy@1qbit.com;jaspreet.oberoi@1qbit.com;ehsan.zahedinejad@1qbit.com;pooya.ronagh@1qbit.com,4;6;4,3;4;3,Reject,0,4,0.0,yes,10/19/17,1qbit;1qbit;1qbit;1qbit;1qbit;1qbit,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,
797,797,797,797,797,797,797,797,ICLR,2018,Pixel Deconvolutional Networks,Hongyang Gao;Hao Yuan;Zhengyang Wang;Shuiwang Ji,hongyang.gao@wsu.edu;hao.yuan@wsu.edu;zwang6@eecs.wsu.edu;sji@eecs.wsu.edu,5;5;6,5;4;4,Reject,0,3,0.0,yes,10/19/17,SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY,468;468;468;468,352;352;352;352,5;2
798,798,798,798,798,798,798,798,ICLR,2018,Building effective deep neural networks one feature at a time,Martin Mundt;Tobias Weis;Kishore Konda;Visvanathan Ramesh,mundt@fias.uni-frankfurt.de;weis@ccc.cs.uni-frankfurt.de;kishore.konda@insofe.edu.in;ramesh@fias.uni-frankfurt.de,4;8;5,5;4;4,Reject,2,5,0.0,yes,10/19/17,Goethe University;Goethe University;;Goethe University,210;210;-1;210,293;293;-1;293,
799,799,799,799,799,799,799,799,ICLR,2018,Adversary A3C for Robust Reinforcement Learning,Zhaoyuan Gu;Zhenzhong Jia;Howie Choset,guzhaoyuan14@gmail.com;zhenzhong.jia@gmail.com;choset@cs.cmu.edu,4;4;4,4;4;4,Reject,0,0,0.0,yes,10/20/17,Tsinghua University;;Carnegie Mellon University,10;-1;1,30;-1;24,4
800,800,800,800,800,800,800,800,ICLR,2018,Deep Function Machines: Generalized Neural Networks for Topological Layer Expression,William H. Guss,wguss@cs.cmu.edu,7;3;4,1;4;3,Reject,0,3,0.0,yes,10/20/17,Carnegie Mellon University,1,24,2;8
801,801,801,801,801,801,801,801,ICLR,2018,Enhance Word Representation for Out-of-Vocabulary on Ubuntu Dialogue Corpus,JIANXIONG DONG;Jim Huang,jdongca2003@gmail.com;ccjimhuang@gmail.com,6;3;5,3;5;4,Reject,0,15,0.0,yes,10/20/17,;,-1;-1,-1;-1,
802,802,802,802,802,802,802,802,ICLR,2018,On Optimality Conditions for Auto-Encoder Signal Recovery,Devansh Arpit;Yingbo Zhou;Hung Q. Ngo;Nils Napp;Venu Govindaraju,devansharpit@gmail.com;zybzmhhj@gmail.com;hungngo@buffalo.edu;nnapp@buffalo.edu;venu@cubs.buffalo.edu,4;5;5,3;4;4,Reject,0,3,0.0,yes,10/20/17,"University of Montreal;;State University of New York, Buffalo;State University of New York, Buffalo;State University of New York, Buffalo",124;-1;85;85;85,108;-1;270;270;270,
803,803,803,803,803,803,803,803,ICLR,2018,Multi-label Learning for Large Text Corpora using Latent Variable Model with Provable Gurantees,Sayantan Dasgupta,sayandg@umich.edu,4;3;4,5;4;5,Reject,0,0,0.0,yes,10/21/17,University of Michigan,8,21,
804,804,804,804,804,804,804,804,ICLR,2018,Egocentric Spatial Memory Network,Mengmi Zhang;Keng Teck Ma;Joo Hwee Lim;Shih-Cheng Yen;Qi Zhao;Jiashi Feng,a0091624@u.nus.edu;makt@i2r.a-star.edu.sg;joohwee@i2r.a-star.edu.sg;shihcheng@nus.edu.sg;qzhao@cs.umn.edu;elefjia@nus.edu.sg,5;3;4,4;4;4,Reject,0,0,0.0,yes,10/21/17,"National University of Singapore;A*STAR;A*STAR;National University of Singapore;University of Minnesota, Minneapolis;National University of Singapore",16;-1;-1;16;55;16,22;-1;-1;22;56;22,
805,805,805,805,805,805,805,805,ICLR,2018,Entropy-SGD optimizes the prior of a PAC-Bayes bound: Data-dependent PAC-Bayes priors via differential privacy,Gintare Karolina Dziugaite;Daniel M. Roy,gkd22@cam.ac.uk;droy@utstat.toronto.edu,6;6;6,3;3;3,Reject,0,13,0.0,yes,10/21/17,University of Cambridge;University of Toronto,71;17,2;22,1;8
806,806,806,806,806,806,806,806,ICLR,2018,Decoupling the Layers in Residual Networks,Ricky Fok;Aijun An;Zana Rashidi;Xiaogang Wang,ricky.fok3@gmail.com;aan@cse.yorku.ca;rashidi.zana@gmail.com;stevenw@mathstat.yorku.ca,6;7;6,3;3;3,Accept (Poster),0,22,0.0,yes,10/21/17,York University;York University;York University;York University,153;153;153;153,350;350;350;350,
807,807,807,807,807,807,807,807,ICLR,2018,ENRICHMENT OF FEATURES FOR CLASSIFICATION USING AN OPTIMIZED LINEAR/NON-LINEAR COMBINATION OF INPUT FEATURES,Mehran Taghipour-Gorjikolaie;Seyyed Mohammad Razavi;Javad Sadri,mehran.tg.88@gmail.com;razavism@gmail.com;j_sadri@encs.concordia.ca,1;3;2,5;4;3,Reject,0,0,0.0,yes,10/21/17,";;Concordia University, Montreal",-1;-1;291,-1;-1;560,
808,808,808,808,808,808,808,808,ICLR,2018,Thinking like a machine — generating visual rationales through latent space optimization,Jarrel Seah;Jennifer Tang;Andy Kitchen;Jonathan Seah,jarrelscy@gmail.com,4;8;7,3;2;4,Reject,0,10,0.0,yes,10/21/17,Alfred Health,-1,-1,5;8
809,809,809,809,809,809,809,809,ICLR,2018,AANN: Absolute Artificial Neural Network,Animesh Karnewar,animeshsk3@gmail.com,2;3;6,3;5;4,Reject,0,2,0.0,yes,10/21/17,Google,-1,-1,
810,810,810,810,810,810,810,810,ICLR,2018,Dependent Bidirectional RNN with Extended-long Short-term Memory,Yuanhang Su;Yuzhong Huang;C.-C. Jay Kuo,yuanhans@usc.edu;yuzhongh@usc.edu;cckuo@sipi.usc.edu,3;4;4,4;4;4,Reject,0,10,1.0,yes,10/22/17,University of Southern California;University of Southern California;University of Southern California,31;31;31,66;66;66,
811,811,811,811,811,811,811,811,ICLR,2018,Make SVM great again with Siamese kernel for  few-shot learning,Bence Tilk,bence.tilk@gmail.com,5;3;4,4;4;5,Reject,0,3,0.0,yes,10/22/17,BUDAPEST UNIVERSITY OF TECHNOLOGY AND ECONOMICS,468,807,6;8
812,812,812,812,812,812,812,812,ICLR,2018,Some Considerations on Learning to Explore via Meta-Reinforcement Learning,Bradly Stadie;Ge Yang;Rein Houthooft;Xi Chen;Yan Duan;Yuhuai Wu;Pieter Abbeel;Ilya Sutskever,bstadie@berkeley.edu;yangge1987@gmail.com;rein.hh@gmail.com;adslcx@gmail.com;dementrock@gmail.com;ywu@cs.toronto.edu;pabbeel@gmail.com;ilyasu@openai.com,7;6;4,4;5;4,Invite to Workshop Track,0,8,0.0,yes,10/22/17,"University of California Berkeley;University of Chicago;;covariant.ai;University of California Berkeley;Department of Computer Science, University of Toronto;University of California-Berkeley;OpenAI",5;46;-1;-1;5;17;5;-1,18;9;-1;-1;18;22;18;-1,
813,813,813,813,813,813,813,813,ICLR,2018,Learning non-linear transform with discriminative and minimum information loss priors,Dimche Kostadinov;Slava Voloshynovskiy,dimche.kostadinov@unige.ch;svolos@unige.ch,5;4;5,2;2;1,Reject,0,6,0.0,yes,10/22/17,"University of Geneva, Switzerland;University of Geneva, Switzerland",468;468,130;130,
814,814,814,814,814,814,814,814,ICLR,2018,Continuous Convolutional Neural Networks for Image Classification,Vitor Guizilini;Fabio Ramos,vitor.guizilini@sydney.edu.au;fabio.ramos@sydney.edu.au,5;6;4,3;2;4,Reject,1,6,0.0,yes,10/22/17,University of Sydney;University of Sydney,90;90,61;61,9
815,815,815,815,815,815,815,815,ICLR,2018,Learning Representations and Generative Models for 3D Point Clouds,Panos Achlioptas;Olga Diamanti;Ioannis Mitliagkas;Leonidas Guibas,optas@cs.stanford.edu;diamanti@stanford.edu;ioannis@iro.umontreal.ca;guibas@cs.stanford.edu,6;8;5,5;5;4,Invite to Workshop Track,0,10,0.0,yes,10/23/17,Stanford University;Stanford University;University of Montreal;Stanford University,4;4;124;4,3;3;108;3,5;8
816,816,816,816,816,816,816,816,ICLR,2018,Siamese Survival Analysis with Competing Risks,Anton Nemchenko;Kartik Ahuja;Mihaela Van Der Schaar,santon834@g.ucla.edu;ahujak@ucla.edu;mihaela@ee.ucla.edu,4;4;4,4;4;5,Reject,0,5,0.0,yes,10/23/17,"University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles",20;20;20,15;15;15,
817,817,817,817,817,817,817,817,ICLR,2018,Training and Inference with Integers in Deep Neural Networks,Shuang Wu;Guoqi Li;Feng Chen;Luping Shi,wus15@mails.tsinghua.edu.cn;liguoqi@mail.tsinghua.edu.cn;chenfeng@mail.tsinghua.edu.cn;lpshi@mail.tsinghua.edu.cn,7;7;8,4;3;4,Accept (Oral),0,3,5.0,yes,10/23/17,Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University,10;10;10;10,30;30;30;30,
818,818,818,818,818,818,818,818,ICLR,2018,Associative Conversation Model: Generating Visual Information from Textual Information,Yoichi Ishibashi;Hisashi Miyamori,g1445539@cc.kyoto-su.ac.jp;miya@cc.kyoto-su.ac.jp,4;3;3,5;4;5,Reject,0,1,0.0,yes,10/23/17,Meiji University;Meiji University,468;468,334;334,3
819,819,819,819,819,819,819,819,ICLR,2018,The Principle of Logit Separation,Gil Keren;Sivan Sabato;Björn Schuller,cruvadom@gmail.com;sivan.sabato@gmail.com;bjoern.schuller@imperial.ac.uk,6;3;4,3;4;4,Reject,0,2,0.0,yes,10/23/17,University of Passau;Ben-Gurion University of the Negev;Imperial College London,291;181;74,243;627;8,
820,820,820,820,820,820,820,820,ICLR,2018,Improving image generative models with human interactions,Andrew Kyle Lampinen;David So;Douglas Eck;Fred Bertsch,lampinen@stanford.edu;davidso@google.com;deck@google.com;fredbertsch@google.com,4;5;4,5;3;4,Reject,0,3,0.0,yes,10/23/17,Stanford University;Google;Google;Google,4;-1;-1;-1,3;-1;-1;-1,5;1
821,821,821,821,821,821,821,821,ICLR,2018,Large Batch Training of Convolutional Networks with Layer-wise Adaptive Rate Scaling,Boris Ginsburg;Igor Gitman;Yang You,bginsburg@nvidia.com;igitman@andrew.cmu.edu;youyang@cs.berkeley.edu,5;4;5,3;4;5,Reject,0,3,0.0,yes,10/24/17,NVIDIA;Carnegie Mellon University;University of California Berkeley,-1;1;5,-1;24;18,
822,822,822,822,822,822,822,822,ICLR,2018,ResBinNet: Residual Binary Neural Network,Mohammad Ghasemzadeh;Mohammad Samragh;Farinaz Koushanfar,mghasemzadeh@ucsd.edu;msamragh@ucsd.edu;farinaz@ucsd.edu,4;4;4,4;4;4,Reject,2,0,0.0,yes,10/24/17,"University of California, San Diego;University of California, San Diego;University of California, San Diego",11;11;11,31;31;31,9
823,823,823,823,823,823,823,823,ICLR,2018,Exponentially vanishing sub-optimal local minima in multilayer neural networks,Daniel Soudry;Elad Hoffer,daniel.soudry@gmail.com;elad.hoffer@gmail.com,5;7;6,3;2;3,Invite to Workshop Track,0,5,0.0,yes,10/24/17,Technion;Technion,24;24,327;327,1
824,824,824,824,824,824,824,824,ICLR,2018,A New Method of Region Embedding for Text Classification,chao qiao;bo huang;guocheng niu;daren li;daxiang dong;wei he;dianhai yu;hua wu,chao.qiao@outlook.com;bohuang0321@gmail.com;niuguocheng@baidu.com;lidaren@baidu.com;dongdaxiang@baidu.com;hewei06@baidu.com;yudianhai@baidu.com;wu_hua@baidu.com,6;6;6,4;5;3,Accept (Poster),4,18,0.0,yes,10/24/17,;Baidu;Baidu;Baidu;Baidu;Baidu;Baidu;Baidu,-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1,
825,825,825,825,825,825,825,825,ICLR,2018,Predicting Auction Price of Vehicle License Plate with Deep Recurrent Neural Network,Vinci Chow,vincichow@cuhk.edu.hk,6;4;4,5;4;4,Reject,0,3,0.0,yes,10/24/17,The Chinese University of Hong Kong,57,40,3
826,826,826,826,826,826,826,826,ICLR,2018,Machine Learning by Two-Dimensional Hierarchical Tensor Networks: A Quantum Information Theoretic Perspective on Deep Architectures,Ding Liu;Shi-Ju Ran;Peter Wittek;Cheng Peng;Raul Blázquez García;Gang Su;Maciej Lewenstein,dingliu_thu@126.com;shi-ju.ran@icfo.eu;peter.wittek@icfo.eu;pengcheng12@mails.ucas.ac.cn;raulbzga@gmail.com;gsu@ucas.ac.cn;maciej.lewenstein@icfo.eu,6;4;3,3;3;2,Reject,0,3,0.0,yes,10/24/17,ICFO-Institut de Ciencies Fotoniques;ICFO-Institut de Ciencies Fotoniques;ICFO-Institut de Ciencies Fotoniques;Chinese Academy of Sciences;;Chinese Academy of Sciences;ICFO-Institut de Ciencies Fotoniques,-1;-1;-1;62;-1;62;-1,-1;-1;-1;1103;-1;1103;-1,
827,827,827,827,827,827,827,827,ICLR,2018,Learning to play slot cars and Atari 2600 games in just minutes,Lionel Cordesses;Omar Bentahar;Julien Page,lionel.cordesses@renault.com;omar.bentahar@renault.com;ju.page@hotmail.com,3;2;3,2;5;1,Reject,0,4,0.0,yes,10/24/17,"Blaise Pascal University, France;Renault;",468;-1;-1,565;-1;-1,
828,828,828,828,828,828,828,828,ICLR,2018,Learning how to explain neural networks: PatternNet and PatternAttribution,Pieter-Jan Kindermans;Kristof T. Schütt;Maximilian Alber;Klaus-Robert Müller;Dumitru Erhan;Been Kim;Sven Dähne,pikinder@google.com;kristof.schuett@tu-berlin.de;maximilian.aber@tu-berlin.de;klaus-robert.mueller@tu-berlin.de;dumitru@google.com;beenkim@google.com;sven.daehne@tu-berlin.de,8;8;6,3;4;4,Accept (Poster),3,5,0.0,yes,10/24/17,Google;TU Berlin;TU Berlin;TU Berlin;Google;Google;TU Berlin,-1;104;104;104;-1;-1;104,-1;92;92;92;-1;-1;92,8
829,829,829,829,829,829,829,829,ICLR,2018,Memory Augmented Control Networks,Arbaaz Khan;Clark Zhang;Nikolay Atanasov;Konstantinos Karydis;Vijay Kumar;Daniel D. Lee,arbaazk@seas.upenn.edu;clarkz@seas.upenn.edu;natanasov@ucsd.edu;konstantinos.karydis@ucr.edu;vijay.kumar@seas.upenn.edu;ddlee@seas.upenn.edu,4;6;9,5;2;4,Accept (Poster),0,6,0.0,yes,10/24/17,"University of Pennsylvania;University of Pennsylvania;University of California, San Diego;University of California, Riverside;University of Pennsylvania;University of Pennsylvania",19;19;11;62;19;19,10;10;31;197;10;10,
830,830,830,830,830,830,830,830,ICLR,2018,Trace norm regularization and faster inference for embedded speech recognition RNNs,Markus Kliegl;Siddharth Goyal;Kexin Zhao;Kavya Srinet;Mohammad Shoeybi,mkliegl@gmail.com;goyalsiddharth@baidu.com;zhaokexin01@baidu.com;srinetkavya@baidu.com;shoeybim@gmail.com,4;5;5,3;5;3,Reject,0,5,0.0,yes,10/24/17,Baidu;Baidu;Baidu;Baidu;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,
831,831,831,831,831,831,831,831,ICLR,2018,Exploring the Hidden Dimension in Accelerating Convolutional Neural Networks,Zhihao Jia;Sina Lin;Charles R. Qi;Alex Aiken,zhihao@cs.stanford.edu;silin@microsoft.com;rqi@stanford.edu;aiken@cs.stanford.edu,4;5;7,5;4;4,Reject,0,6,0.0,yes,10/25/17,Stanford University;Microsoft;Stanford University;Stanford University,4;-1;4;4,3;-1;3;3,
832,832,832,832,832,832,832,832,ICLR,2018,Boosting Dilated Convolutional Networks with Mixed Tensor Decompositions,Nadav Cohen;Ronen Tamari;Amnon Shashua,cohennadav@ias.edu;ronent@cs.huji.ac.il;shashua@cs.huji.ac.il,7;9;8,4;4;3,Accept (Oral),0,3,0.0,yes,10/25/17,"Institue for Advanced Study, Princeton;Hebrew University of Jerusalem;Hebrew University of Jerusalem",-1;62;62,-1;205;205,1
833,833,833,833,833,833,833,833,ICLR,2018,Model Distillation with Knowledge Transfer from Face Classification to Alignment and Verification,Chong Wang;Xipeng Lan;Yangang Zhang,chongwang.nlpr@gmail.com;xipeng.lan@gmail.com;caveman1984@gmail.com,3;5;3,4;5;4,Reject,0,0,0.0,yes,10/25/17,;;,-1;-1;-1,-1;-1;-1,2
834,834,834,834,834,834,834,834,ICLR,2018,Distribution Regression Network,Connie Kou;Hwee Kuan Lee;Teck Khim Ng,koukl@comp.nus.edu.sg;leehk@bii.a-star.edu.sg;ngtk@comp.nus.edu.sg,5;7;7,4;2;4,Reject,0,5,0.0,yes,10/25/17,National University of Singapore;A*STAR;National University of Singapore,16;-1;16,22;-1;22,
835,835,835,835,835,835,835,835,ICLR,2018,Graph Classification with 2D Convolutional Neural Networks,Antoine J.-P. Tixier;Giannis Nikolentzos;Polykarpos Meladianos;Michalis Vazirgiannis,antoine.tixier-1@colorado.edu;giannisnik@hotmail.com;p.meladianos@gmail.com;mvazirg@lix.polytechnique.fr,3;4;7,5;3;3,Reject,0,0,0.0,yes,10/25/17,"University of Colorado, Boulder;;;Ecole Polytechnique, France",42;-1;-1;468,100;-1;-1;115,10
836,836,836,836,836,836,836,836,ICLR,2018,Unsupervised Deep Structure Learning by Recursive Dependency Analysis,Raanan Y. Yehezkel Rohekar;Guy Koren;Shami Nisimov;Gal Novik,raanan.y.yehezkel.rohekar@intel.com;guy.koren@intel.com;shami.nisimov@intel.com;gal.novik@intel.com,4;5;5,4;2;3,Reject,0,3,0.0,yes,10/25/17,Intel;Intel;Intel;Intel,-1;-1;-1;-1,-1;-1;-1;-1,5;1;10
837,837,837,837,837,837,837,837,ICLR,2018,Dynamic Integration of Background Knowledge in Neural NLU Systems,Dirk Weissenborn;Tomas Kocisky;Chris Dyer,dirk.weissenborn@dfki.de;tkocisky@google.com;cdyer@google.com,5;5;6,3;4;4,Reject,0,4,0.0,yes,10/25/17,German Research Center for AI;Google;Google,-1;-1;-1,-1;-1;-1,3
838,838,838,838,838,838,838,838,ICLR,2018,Post-training for Deep Learning,Thomas Moreau;Julien Audiffren,thomas.moreau@cmla.ens-cachan.fr;julien.audiffren@cmla.ens-cachan.fr,5;3;4,4;5;4,Reject,1,3,0.0,yes,10/25/17,ENS Paris-Saclay;ENS Paris-Saclay,468;468,603;603,
839,839,839,839,839,839,839,839,ICLR,2018,Image Segmentation by Iterative Inference from Conditional Score Estimation,Adriana Romero;Michal Drozdzal;Akram Erraqabi;Simon Jégou;Yoshua Bengio,adriana.romsor@gmail.com;michal.drozdzal@gmail.com;akram.er-raqabi@umontreal.ca;simon.jegou@gmail.com;yoshua.umontreal@gmail.com,5;4;4,5;4;4,Reject,0,0,0.0,yes,10/25/17,Facebook;University of Montreal;University of Montreal;;University of Montreal,-1;124;124;-1;124,-1;108;108;-1;108,2
840,840,840,840,840,840,840,840,ICLR,2018,Towards Safe Deep Learning: Unsupervised Defense Against Generic Adversarial Attacks,Bita Darvish Rouhani;Mohammad Samragh;Tara Javidi;Farinaz Koushanfar,bita@ucsd.edu;msamragh@ucsd.edu;tjavidi@ucsd.edu;farinaz@ucsd.edu,5;7;3,3;3;5,Reject,0,5,0.0,yes,10/25/17,"University of California, San Diego;University of California, San Diego;University of California, San Diego;University of California, San Diego",11;11;11;11,31;31;31;31,4;1
841,841,841,841,841,841,841,841,ICLR,2018,GeoSeq2Seq: Information Geometric Sequence-to-Sequence Networks,Alessandro Bay;Biswa Sengupta,alessandro.bay@cortexica.com;biswasengupta@yahoo.com,5;4;5,2;4;4,Invite to Workshop Track,0,3,0.0,yes,10/25/17,;Imperial College London,-1;74,-1;8,2;10
842,842,842,842,842,842,842,842,ICLR,2018,Hybed: Hyperbolic Neural Graph Embedding,Benjamin Paul Chamberlain;James R Clough;Marc Peter Deisenroth,benjamin.chamberlain@gmail.com;james.clough@kcl.ac.uk;m.deisenroth@imperial.ac.uk,4;7;5;4,3;2;3;3,Reject,0,16,0.0,yes,10/25/17,Imperial College London;King's College London;Imperial College London,74;181;74,8;36;8,3;10
843,843,843,843,843,843,843,843,ICLR,2018,Image Quality Assessment Techniques Improve Training and Evaluation of Energy-Based Generative Adversarial Networks,Michael O. Vertolli;Jim Davies,michaelvertolli@gmail.com;jim@jimdavies.org,5;5;6,3;3;3,Reject,0,3,0.0,yes,10/25/17,Carleton University;,153;-1,545;-1,5;4
844,844,844,844,844,844,844,844,ICLR,2018,Regularizing and Optimizing LSTM Language Models,Stephen Merity;Nitish Shirish Keskar;Richard Socher,smerity@smerity.com;keskar.nitish@u.northwestern.edu;richard@socher.org,7;7;7,4;4;5,Accept (Poster),0,3,0.0,yes,10/25/17,Smerity;Northwestern University;SalesForce.com,-1;42;-1,-1;20;-1,3
845,845,845,845,845,845,845,845,ICLR,2018,Dynamic Evaluation of Neural Sequence Models,Ben Krause;Emmanuel Kahembwe;Iain Murray;Steve Renals,ben.krause@ed.ac.uk;e.kahembwe@ed.ac.uk;i.murray@ed.ac.uk;s.renals@ed.ac.uk,7;7;3,4;4;3,Reject,0,4,0.0,yes,10/25/17,University of Edinburgh;University of Edinburgh;University of Edinburgh;University of Edinburgh,33;33;33;33,27;27;27;27,
846,846,846,846,846,846,846,846,ICLR,2018,Taking Apart Autoencoders: How do They Encode Geometric Shapes ?,Alasdair Newson;Andres Almansa;Yann Gousseau;Said Ladjal,alasdairnewson@gmail.com;andres.almansa@parisdescartes.fr;yann.gousseau@telecom-paristech.fr;said.ladjal@telecom-paristech.fr,4;4;4,5;4;3,Reject,0,4,0.0,yes,10/25/17,Télécom ParisTech;University Paris Descartes;Télécom ParisTech;Télécom ParisTech,468;468;468;468,1103;1103;1103;1103,5
847,847,847,847,847,847,847,847,ICLR,2018,Diffusing Policies : Towards Wasserstein Policy Gradient Flows,Pierre H. Richemond;Brendan Maginnis,phr17@imperial.ac.uk;b.maginnis@imperial.ac.uk,4;5;4,3;3;4,Reject,0,3,0.0,yes,10/25/17,Imperial College London;Imperial College London,74;74,8;8,
848,848,848,848,848,848,848,848,ICLR,2018,"To Prune, or Not to Prune: Exploring the Efficacy of Pruning for Model Compression",Michael H. Zhu;Suyog Gupta,mhzhu@cs.stanford.edu;suyoggupta@google.com,5;5;5,4;4;5,Invite to Workshop Track,1,2,0.0,yes,10/25/17,Stanford University;Google,4;-1,3;-1,
849,849,849,849,849,849,849,849,ICLR,2018,Training Deep AutoEncoders for Recommender Systems,Oleksii Kuchaiev;Boris Ginsburg,kuchaev@gmail.com;boris.ginsburg@gmail.com,4;3;6,5;4;4,Reject,1,3,0.0,yes,10/25/17,NVIDIA;NVIDIA,-1;-1,-1;-1,
850,850,850,850,850,850,850,850,ICLR,2018,Neural Networks with Block Diagonal Inner Product Layers,Amy Nesky;Quentin Stout,anesky@umich.edu;qstout@umich.edu,5;6;4,4;4;3,Reject,2,4,0.0,yes,10/26/17,University of Michigan;University of Michigan,8;8,21;21,
851,851,851,851,851,851,851,851,ICLR,2018,Balanced and Deterministic Weight-sharing Helps Network Performance,Oscar Chang;Hod Lipson,oscar.chang@columbia.edu;hod.lipson@columbia.edu,4;4;4,4;4;4,Reject,0,0,0.0,yes,10/26/17,Columbia University;Columbia University,15;15,14;14,
852,852,852,852,852,852,852,852,ICLR,2018,Learning Discrete Weights Using the Local Reparameterization Trick,Oran Shayer;Dan Levi;Ethan Fetaya,oran.sh@gmail.com;dan.levi@gm.com;ethanf@cs.toronto.edu,6;7;6,4;3;3,Accept (Poster),0,1,0.0,yes,10/26/17,"Technion;General Motors;Department of Computer Science, University of Toronto",24;-1;17,327;-1;22,2
853,853,853,853,853,853,853,853,ICLR,2018,End-to-End Abnormality Detection in Medical Imaging,Dufan Wu;Kyungsang Kim;Bin Dong;Quanzheng Li,dwu6@mgh.harvard.edu;kkim24@mgh.harvard.edu;dongbin@math.pku.edu.cn;li.quanzheng@mgh.harvard.edu,4;5;6,4;4;3,Reject,0,0,0.0,yes,10/26/17,Harvard University;Harvard University;Peking University;Harvard University,37;37;24;37,6;6;27;6,2
854,854,854,854,854,854,854,854,ICLR,2018,Understanding Deep Learning Generalization by Maximum Entropy,Guanhua Zheng;Jitao Sang;Changsheng Xu,zhenggh@mail.ustc.edu.cn;jtsang@bjtu.edu.cn;csxu@nlpr.ia.ac.cn,2;3;6,3;3;2,Reject,0,0,0.0,yes,10/26/17,"University of Science and Technology of China;Beijing jiaotong univercity;Institute of automation, Chinese academy of science, Chinese Academy of Sciences",115;468;62,132;854;1103,1;8
855,855,855,855,855,855,855,855,ICLR,2018,Compact Encoding of Words for Efficient Character-level Convolutional Neural Networks Text Classification,Wemerson Marinho;Luis Marti;Nayat Sanchez-pi,wemerson_marinho@id.uff.br;lmarti@ic.uff.br;nayat@ime.uerj.br,4;3;2,5;5;5,Reject,0,12,0.0,yes,10/26/17,;;Universidade do Estado do Rio de Janeiro,-1;-1;-1,-1;-1;-1,
856,856,856,856,856,856,856,856,ICLR,2018,Rotational Unit of Memory ,Rumen Dangovski;Li Jing;Marin Soljacic,rumenrd@mit.edu;ljing@mit.edu;soljacic@mit.edu,4;6;5,4;3;4,Invite to Workshop Track,0,11,0.0,yes,10/26/17,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2,5;5;5,3
857,857,857,857,857,857,857,857,ICLR,2018,Style Memory: Making a Classifier Network Generative,Rey Wiyatno;Jeff Orchard,rrwiyatn@uwaterloo.ca;jorchard@uwaterloo.ca,3;3;4,5;5;3,Reject,0,4,0.0,yes,10/26/17,University of Waterloo;University of Waterloo,26;26,207;207,5
858,858,858,858,858,858,858,858,ICLR,2018,CrescendoNet: A Simple Deep Convolutional Neural Network with Ensemble Behavior,Xiang Zhang;Nishant Vishwamitra;Hongxin Hu;Feng Luo,xzhang7@clemson.edu;nvishwa@clemson.edu;luofeng@clemson.edu;hongxih@clemson.edu,4;5;4,5;4;5,Reject,4,11,0.0,yes,10/26/17,Clemson University;Clemson University;Clemson University;Clemson University,210;210;210;210,1103;1103;1103;1103,
859,859,859,859,859,859,859,859,ICLR,2018,Anytime Neural Network: a Versatile Trade-off Between Computation and Accuracy,Hanzhang Hu;Debadeepta Dey;Martial Hebert;J. Andrew Bagnell,hanzhang@cs.cmu.edu;dedey@microsoft.com;hebert@ri.cmu.edu;dbagnell@ri.cmu.edu,7;5;5,2;3;4,Reject,0,0,0.0,yes,10/26/17,Carnegie Mellon University;Microsoft;Carnegie Mellon University;Carnegie Mellon University,1;-1;1;1,24;-1;24;24,
860,860,860,860,860,860,860,860,ICLR,2018,Adversarial Policy Gradient for Alternating Markov Games,Chao Gao;Martin Mueller;Ryan Hayward,cgao3@ualberta.ca;mmueller@ualberta.ca;hayward@ualberta.ca,5;5;5,2;4;4,Invite to Workshop Track,0,6,0.0,yes,10/26/17,University of Alberta;University of Alberta;University of Alberta,99;99;99,119;119;119,4
861,861,861,861,861,861,861,861,ICLR,2018,THE EFFECTIVENESS OF A TWO-LAYER NEURAL NETWORK FOR RECOMMENDATIONS,Oleg Rybakov;Vijai Mohan;Avishkar Misra;Scott LeGrand;Rejith Joseph;Kiuk Chung;Siddharth Singh;Qian You;Eric Nalisnick;Leo Dirac;Runfei Luo,rybakovo@amazon.com;vijaim@amazon.com;avishkar@gmail.com;slegrand@a9.com;rgeorgej@amazon.com;kiuk@amazon.com;singsidd@amazon.com;qian.you@snapchat.com;enalisni@uci.edu;leodirac@amazon.com;rluo@pstat.ucsb.edu,6;6;7,3;4;3,Invite to Workshop Track,0,4,0.0,yes,10/26/17,"Amazon;Amazon;;A9;Amazon;Amazon;Amazon;Snap Inc.;University of California, Irvine;Amazon;UC Santa Barbara",-1;-1;-1;-1;-1;-1;-1;-1;36;-1;37,-1;-1;-1;-1;-1;-1;-1;-1;99;-1;53,
862,862,862,862,862,862,862,862,ICLR,2018,Normalized Direction-preserving Adam,Zijun Zhang;Lin Ma;Zongpeng Li;Chuan Wu,zijun.zhang@ucalgary.ca;linmawhu@gmail.com;zongpeng@ucalgary.ca;cwu@cs.hku.hk,5;5;4,4;5;4,Reject,2,9,0.0,yes,10/26/17,University of Calgary;;University of Calgary;The University of Hong Kong,181;-1;181;90,210;-1;210;40,9;8
863,863,863,863,863,863,863,863,ICLR,2018,Rethinking generalization requires revisiting old ideas: statistical mechanics approaches and complex learning behavior,Charles H. Martin;Michael W. Mahoney,charles@calculationconsulting.com;mmahoney@stat.berkeley.edu,3;6;7,3;5;4,Reject,0,9,0.0,yes,10/26/17,Calculationconsulting;University of California Berkeley,-1;5,-1;18,8
864,864,864,864,864,864,864,864,ICLR,2018,Learning Non-Metric Visual Similarity for Image Retrieval,Noa Garcia;George Vogiatzis,garciadn@aston.ac.uk;g.vogiatzis@aston.ac.uk,7;4;3,5;4;5,Reject,0,4,0.0,yes,10/26/17,Aston University;Aston University,69;69,358;358,
865,865,865,865,865,865,865,865,ICLR,2018,Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks,Víctor Campos;Brendan Jou;Xavier Giró-i-Nieto;Jordi Torres;Shih-Fu Chang,victor.campos@bsc.es;bjou@google.com;xavier.giro@upc.edu;jordi.torres@bsc.es;shih.fu.chang@columbia.edu,6;6;6,4;4;4,Accept (Poster),1,6,0.0,yes,10/26/17,Barcelona Supercomputing Center;Google;Universitat Politècnica de Catalunya;Barcelona Supercomputing Center;Columbia University,-1;-1;468;-1;15,-1;-1;1103;-1;14,10
866,866,866,866,866,866,866,866,ICLR,2018,Iterative Deep Compression : Compressing Deep Networks for Classification and Semantic Segmentation,Sugandha Doda;Vitor Fortes Rey;Dr. Nadereh Hatami;Prof. Dr. Paul Lukowicz,sugandhadoda672@gmail.com;vitor.fortes@dfki.uni-kl.de;nadereh.hatamimazinani@de.bosch.com,6;4;5,4;4;4,Reject,0,6,0.0,yes,10/26/17,Bosch;TU Kaiserslautern;Bosch,-1;139;-1,-1;452;-1,3;2
867,867,867,867,867,867,867,867,ICLR,2018,Do Deep Reinforcement Learning Algorithms really Learn to Navigate?,Shurjo Banerjee;Vikas Dhiman;Brent Griffin;Jason J. Corso,shurjo@umich.edu;dhiman@umich.edu;griffb@umich.edu;jjcorso@umich.edu,7;3;3,4;5;4,Reject,0,4,0.0,yes,10/26/17,University of Michigan;University of Michigan;University of Michigan;University of Michigan,8;8;8;8,21;21;21;21,
868,868,868,868,868,868,868,868,ICLR,2018,Adversarial Learning for Semi-Supervised Semantic Segmentation,Wei-Chih Hung;Yi-Hsuan Tsai;Yan-Ting Liou;Yen-Yu Lin;Ming-Hsuan Yang,whung8@ucmerced.edu;ytsai@nec-labs.com;lyt@csie.ntu.edu.tw;yylin@citi.sinica.edu.tw;mhyang@ucmerced.edu,5;5;5,5;4;4,Reject,5,6,0.0,yes,10/26/17,University of California at Merced;NEC-Labs;National Taiwan University;Academia Sinica;University of California at Merced,468;-1;85;-1;468,1103;-1;197;-1;1103,4;2
869,869,869,869,869,869,869,869,ICLR,2018,Towards Interpretable Chit-chat: Open Domain Dialogue Generation with Dialogue Acts,Wei Wu;Can Xu;Yu Wu;Zhoujun Li,wuwei@microsoft.com;can.xu@microsoft.com;wumark@126.com;lizj@buaa.edu.cn,7;4;7,3;5;4,Reject,0,9,0.0,yes,10/26/17,Microsoft;Microsoft;Beihang University;Beihang University,-1;-1;124;124,-1;-1;658;658,
870,870,870,870,870,870,870,870,ICLR,2018,PDE-Net: Learning PDEs from Data,Zichao Long;Yiping Lu;Xianzhong Ma;Bin Dong,zlong@pku.edu.cn;luyiping9712@pku.edu.cn;xianzhongma@pku.edu.cn;dongbin@math.pku.edu.cn,7;8;5,4;4;4,Invite to Workshop Track,0,4,0.0,yes,10/26/17,Peking University;Peking University;Peking University;Peking University,24;24;24;24,27;27;27;27,2
871,871,871,871,871,871,871,871,ICLR,2018,Unleashing the Potential of CNNs for Interpretable Few-Shot Learning,Boyang Deng;Qing Liu;Siyuan Qiao;Alan Yuille,billydeng@buaa.edu.cn;qingliu@jhu.edu;siyuan.qiao@jhu.edu;alan.yuille@jhu.edu,5;7;4,4;4;5,Reject,0,8,0.0,yes,10/26/17,Beihang University;Johns Hopkins University;Johns Hopkins University;Johns Hopkins University,124;71;71;71,658;13;13;13,6;2
872,872,872,872,872,872,872,872,ICLR,2018,Clipping Free Attacks Against Neural Networks,Boussad ADDAD,boussad.addad@thalesgroup.com;boussad83@yahoo.fr,3;4;5,3;3;2,Reject,0,4,0.0,yes,10/26/17,Thalesgroup;Thales,-1;-1,-1;-1,3;4;2
873,873,873,873,873,873,873,873,ICLR,2018,Noisy Networks For Exploration,Meire Fortunato;Mohammad Gheshlaghi Azar;Bilal Piot;Jacob Menick;Matteo Hessel;Ian Osband;Alex Graves;Volodymyr Mnih;Remi Munos;Demis Hassabis;Olivier Pietquin;Charles Blundell;Shane Legg,meirefortunato@google.com;mazar@google.com;piot@google.com;jmenick@google.com;mtthss@google.com;iosband@google.com;gravesa@google.com;vmnih@google.com;munos@google.com;dhcontact@google.com;pietquin@google.com;cblundell@google.com;legg@google.com,5;7;6,3;4;4,Accept (Poster),1,10,0.0,yes,10/26/17,Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,
874,874,874,874,874,874,874,874,ICLR,2018,The Set Autoencoder: Unsupervised Representation Learning for Sets,Malte Probst,malte.probst@honda-ri.de,4;5;5,5;4;4,Reject,0,2,0.0,yes,10/26/17,Honda Research Institute,-1,-1,3
875,875,875,875,875,875,875,875,ICLR,2018,Learning to Count Objects in Natural Images for Visual Question Answering,Yan Zhang;Jonathon Hare;Adam Prügel-Bennett,yz5n12@ecs.soton.ac.uk;jsh2@ecs.soton.ac.uk;apb@ecs.soton.ac.uk,6;6;4,3;3;4,Accept (Poster),0,12,1.0,yes,10/26/17,University of Southampton;University of Southampton;University of Southampton,181;181;181,126;126;126,
876,876,876,876,876,876,876,876,ICLR,2018,Sensor Transformation Attention Networks,Stefan Braun;Daniel Neil;Enea Ceolini;Jithendar Anumula;Shih-Chii Liu,brauns@ethz.ch;daniel.l.neil@gmail.com;enea.ceolini@ini.uzh.ch;anumula@ini.uzh.ch;shih@ini.ethz.ch,7;4;3,4;4;4,Reject,0,1,0.0,yes,10/26/17,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;University of Zurich;University of Zurich;Swiss Federal Institute of Technology,9;9;139;139;9,10;10;136;136;10,
877,877,877,877,877,877,877,877,ICLR,2018,CAYLEYNETS: SPECTRAL GRAPH CNNS WITH COMPLEX RATIONAL FILTERS,Ron Levie;Federico Monti;Xavier Bresson;Michael M. Bronstein,ronlevie@gmail.com;federico.monti@usi.ch;xavier.bresson@gmail.com;michael.bronstein@gmail.com,6;4;8,3;3;3,Reject,0,15,0.0,yes,10/26/17,;Università della Svizzera Italiana;National Taiwan University;Tel Aviv University,-1;139;85;37,-1;1103;197;217,10
878,878,878,878,878,878,878,878,ICLR,2018,Convolutional Normalizing Flows,Guoqing Zheng;Yiming Yang;Jaime Carbonell,gzheng@cs.cmu.edu;yiming@cs.cmu.edu;jgc@cs.cmu.edu,3;5;3,5;4;4,Reject,2,4,0.0,yes,10/26/17,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1,24;24;24,11
879,879,879,879,879,879,879,879,ICLR,2018,Baseline-corrected space-by-time non-negative matrix factorization for decoding single trial population spike trains,Arezoo Alizadeh;Marion Mutter;Thomas Münch;Arno Onken;Stefano Panzeri,arezoo.alizadehkhajehiem@iit.it;marion.mutter@gmx.de;thomas.muench@cin.uni-tuebingen.de;aonken@inf.ed.ac.uk;stefano.panzeri@iit.it,4;6;6,4;3;3,Reject,0,3,0.0,yes,10/26/17,Istituto Italiano di Tecnologia;;University of Tuebingen;University of Edinburgh;Istituto Italiano di Tecnologia,468;-1;153;33;468,1103;-1;94;27;1103,5
880,880,880,880,880,880,880,880,ICLR,2018,DNA-GAN: Learning Disentangled Representations from Multi-Attribute Images,Taihong Xiao;Jiapeng Hong;Jinwen Ma,xiaotaihong@pku.edu.cn;jphong@pku.edu.cn;jwma@math.pku.edu.cn,4;5;6,4;4;5,Invite to Workshop Track,1,9,0.0,yes,10/26/17,Peking University;Peking University;Peking University,24;24;24,27;27;27,4
881,881,881,881,881,881,881,881,ICLR,2018,Avoiding Catastrophic States with Intrinsic Fear,Zachary C. Lipton;Kamyar Azizzadenesheli;Abhishek Kumar;Lihong Li;Jianfeng Gao;Li Deng,zlipton@cmu.edu;kazizzad@uci.edu;abkumar@ucsd.edu;lihongli.cs@gmail.com;jfgao@microsoft.com;l.deng@ieee.org,5;5;7,4;5;3,Reject,1,12,0.0,yes,10/26/17,"Carnegie Mellon University;University of California, Irvine;University of California, San Diego;Google;Microsoft;",1;36;11;-1;-1;-1,24;99;31;-1;-1;-1,
882,882,882,882,882,882,882,882,ICLR,2018,DDRprog: A CLEVR Differentiable Dynamic Reasoning Programmer,Joseph Suarez;Justin Johnson;Fei-Fei Li,joseph15@stanford.edu;jcjohns@cs.stanford.edu;feifeili@cs.stanford.edu,5;5;6,2;2;2,Reject,0,6,0.0,yes,10/26/17,Stanford University;Stanford University;Stanford University,4;4;4,3;3;3,
883,883,883,883,883,883,883,883,ICLR,2018,Espresso: Efficient Forward Propagation for Binary Deep Neural Networks,Fabrizio Pedersoli;George Tzanetakis;Andrea Tagliasacchi,fpeder@uvic.ca;gtzan@uvic.ca;ataiya@uvic.ca,6;7;7,3;4;1,Accept (Poster),0,0,0.0,yes,10/26/17,University of Victoria;University of Victoria;University of Victoria,181;181;181,346;346;346,
884,884,884,884,884,884,884,884,ICLR,2018,LatentPoison -- Adversarial Attacks On The Latent Space,Antonia Creswell;Biswa Sengupta;Anil A. Bharath,ac2211@ic.ac.uk;b.sengupta@imperial.ac.uk;a.bharath@imperial.ac.uk,5;3;4,3;4;4,Reject,0,3,0.0,yes,10/26/17,Imperial College London;Imperial College London;Imperial College London,74;74;74,8;8;8,5;4
885,885,885,885,885,885,885,885,ICLR,2018,PACT: Parameterized Clipping Activation for Quantized Neural Networks,Jungwook Choi;Zhuo Wang;Swagath Venkataramani;Pierce I-Jen Chuang;Vijayalakshmi Srinivasan;Kailash Gopalakrishnan,choij@us.ibm.com,5;5;5,4;5;4,Reject,0,8,0.0,yes,10/26/17,International Business Machines,-1,-1,
886,886,886,886,886,886,886,886,ICLR,2018,Understanding Grounded Language Learning Agents,Felix Hill;Karl Moritz Hermann;Phil Blunsom;Stephen Clark,felixhill@google.com;kmh@google.com;pblunsom@google.com;clarkstephen@google.com,4;5;7,5;3;4,Reject,0,6,0.0,yes,10/26/17,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,
887,887,887,887,887,887,887,887,ICLR,2018,APPLICATION OF DEEP CONVOLUTIONAL NEURAL NETWORK TO PREVENT ATM FRAUD BY FACIAL DISGUISE IDENTIFICATION,Suraj Nandkishor Kothawade;Sumit Baburao Tamgale,kothawadesuraj@sggs.ac.in;tamgalesumit@sggs.ac.in,1;2;3,5;4;5,Reject,0,0,0.0,yes,10/26/17,;,-1;-1,-1;-1,
888,888,888,888,888,888,888,888,ICLR,2018,Discrete Sequential Prediction of Continuous Actions for Deep RL,Luke Metz;Julian Ibarz;Navdeep Jaitly;James Davidson,lmetz@google.com;julianibarz@google.com;njaitly@google.com;jcdavidson@google.com,7;5;4,5;1;5,Reject,0,4,0.0,yes,10/26/17,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,
889,889,889,889,889,889,889,889,ICLR,2018,An Ensemble of Retrieval-Based and Generation-Based Human-Computer Conversation Systems.,Yiping Song;Rui Yan;Cheng-Te Li;Jian-Yun Nie;Ming Zhang;Dongyan Zhao,songyiping@pku.edu.cn;ruiyan@pku.edu.cn;chengte@mail.ncku.edu.tw;nie@iro.umontreal.ca;mzhang_cs@pku.edu.cn;zhaody@pku.edu.cn,5;5;6,3;3;3,Reject,0,0,1.0,yes,10/26/17,Peking University;Peking University;Peking University;University of Montreal;Peking University;Peking University,24;24;24;124;24;24,27;27;27;108;27;27,3;5
890,890,890,890,890,890,890,890,ICLR,2018,Multi-Advisor Reinforcement Learning,Romain Laroche;Mehdi Fatemi;Joshua Romoff;Harm van Seijen,romain.laroche@gmail.com;mehdi.fatemi@microsoft.com;joshua.romoff@mail.mcgill.ca;havansei@microsoft.com,4;4;4,4;4;5,Reject,0,1,0.0,yes,10/26/17,Microsoft;Microsoft;McGill University;Microsoft,-1;-1;81;-1,-1;-1;42;-1,
891,891,891,891,891,891,891,891,ICLR,2018,Progressive Reinforcement Learning with Distillation for Multi-Skilled Motion Control,Glen Berseth;Cheng Xie;Paul Cernek;Michiel Van de Panne,gberseth@gmail.com;cheng.k.xie@gmail.com;pcernek@cs.ubc.ca;van@cs.ubc.ca,7;7;5,4;4;3,Accept (Poster),0,6,0.0,yes,10/26/17,University of British Columbia;University of British Columbia;University of British Columbia;University of British Columbia,34;34;34;34,34;34;34;34,6
892,892,892,892,892,892,892,892,ICLR,2018,UNSUPERVISED METRIC LEARNING VIA NONLINEAR FEATURE SPACE TRANSFORMATIONS,Pin Zhang;Bibo Shi;JundongLiu,pz335412@ohio.edu;bibo.shi@duke.edu;liuj1@ohio.edu,6;4;4,4;5;4,Reject,0,0,0.0,yes,10/26/17,Ohio University;Duke University;Ohio University,364;46;364,627;17;627,
893,893,893,893,893,893,893,893,ICLR,2018,Learning Deep Generative Models With Discrete Latent Variables,Hengyuan Hu;Ruslan Salakhutdinov,hengyuah@andrew.cmu.edu;rsalakhu@cs.cmu.edu,4;5;4,4;4;4,Reject,2,3,0.0,yes,10/26/17,Carnegie Mellon University;Carnegie Mellon University,1;1,24;24,5;10
894,894,894,894,894,894,894,894,ICLR,2018,Novelty Detection with GAN,Mark Kliger;Shachar Fleishman,mark.kliger@gmail.com;shacharfl@gmail.com,4;5;6,4;4;4,Reject,0,3,0.0,yes,10/26/17,;,-1;-1,-1;-1,5;4
895,895,895,895,895,895,895,895,ICLR,2018,A Self-Organizing Memory Network,Callie Federer;Joel Zylberberg,callie.federer@ucdenver.edu;joel.zylberberg@ucdenver.edu,3;4;4,2;4;4,Reject,0,0,0.0,yes,10/26/17,"University of Colorado, Denver;University of Colorado, Denver",468;468,286;286,
896,896,896,896,896,896,896,896,ICLR,2018,The Cramer Distance as a Solution to Biased Wasserstein Gradients,Marc G. Bellemare;Ivo Danihelka;Will Dabney;Shakir Mohamed;Balaji Lakshminarayanan;Stephan Hoyer;Remi Munos,bellemare@google.com;danihelka@google.com;shakir@google.com;balajiln@google.com;shoyer@google.com;munos@google.com,5;4;7,3;5;2,Reject,1,4,0.0,yes,10/26/17,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,5;4
897,897,897,897,897,897,897,897,ICLR,2018,DEEP DENSITY NETWORKS AND UNCERTAINTY IN RECOMMENDER SYSTEMS,Yoel Zeldes;Stavros Theodorakis;Efrat Solodnik;Aviv Rotman;Gil Chamiel;Dan Friedman,yoel.z@taboola.com;sth@deeplab.ai;efrat.s@taboola.com;aviv.r@taboola.com;gil.c@taboola.com;dan.f@taboola.com,4;3;4,5;4;3,Reject,0,0,0.0,yes,10/26/17,Taboola;DeepLab;Taboola;Taboola;Taboola;Taboola,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,
898,898,898,898,898,898,898,898,ICLR,2018,PrivyNet: A Flexible Framework for Privacy-Preserving Deep Neural Network Training,Meng Li;Liangzhen Lai;Naveen Suda;Vikas Chandra;David Z. Pan,meng_li@utexas.edu;liangzhen.lai@arm.com;naveen.suda@arm.com;vikas.chandra@arm.com;dpan@ece.utexas.edu,6;5;3,5;3;3,Reject,1,0,0.0,yes,10/26/17,"University of Texas, Austin;arm;arm;arm;University of Texas, Austin",21;-1;-1;-1;21,49;-1;-1;-1;49,
899,899,899,899,899,899,899,899,ICLR,2018,Simple Fast Convolutional Feature Learning,David Macêdo;Cleber Zanchettin;Teresa Ludermir,dlm@cin.ufpe.br;cz@cin.ufpe.br;tbl@cin.ufpe.br,3;3;2,4;4;4,Reject,0,5,0.0,yes,10/26/17,Federal University of Pernambuco;Federal University of Pernambuco;Federal University of Pernambuco,468;468;468,958;958;958,
900,900,900,900,900,900,900,900,ICLR,2018,Enhancing Batch Normalized Convolutional Networks using Displaced Rectifier Linear Units: A Systematic Comparative Study,David Macêdo;Cleber Zanchettin;Adriano L. I. Oliveira;Teresa Ludermir,dlm@cin.ufpe.br;cz@cin.ufpe.br;alio@cin.ufpe.br;tbl@cin.ufpe.br,3;4;5,5;4;5,Reject,0,4,0.0,yes,10/26/17,Federal University of Pernambuco;Federal University of Pernambuco;Federal University of Pernambuco;Federal University of Pernambuco,468;468;468;468,958;958;958;958,2
901,901,901,901,901,901,901,901,ICLR,2018,ANALYSIS ON GRADIENT PROPAGATION IN BATCH NORMALIZED RESIDUAL NETWORKS,Abhishek Panigrahi;Yueru Chen;C.-C. Jay Kuo,abhishekpanigrahi@iitkgp.ac.in;yueruche@usc.edu;cckuo@sipi.usc.edu,4;1;4,4;5;5,Reject,0,0,0.0,yes,10/26/17,Indian Institute of Technology Kharagpur;University of Southern California;University of Southern California,291;31;31,506;66;66,
902,902,902,902,902,902,902,902,ICLR,2018,Deterministic Policy Imitation Gradient Algorithm,Fumihiro Sasaki;Atsuo Kawaguchi,fumihiro.fs.sasaki@nts.ricoh.co.jp;atsuo.kawaguchi@nts.ricoh.co.jp,6;5;5,4;4;3,Reject,0,3,0.0,yes,10/26/17,"Ricoh Company, Ltd.;Ricoh Company, Ltd.",-1;-1,-1;-1,5;4
903,903,903,903,903,903,903,903,ICLR,2018,Data augmentation instead of explicit regularization,Alex Hernández-García;Peter König,alexhg15@gmail.com;pkoenig@uos.de,5;5;5,4;4;4,Reject,0,5,0.0,yes,9/25/19,University of Osnabrück;University of Osnabrück,364;364,1103;1103,8
904,904,904,904,904,904,904,904,ICLR,2018,Neural Clustering By Predicting And Copying Noise,Sam Coope;Andrej Zukov-Gregoric;Yoram Bachrach,sam@digitalgenius.com;andrej@digitalgenius.com;yoram@digitalgenius.com,5;5;5,4;3;4,Reject,3,4,0.0,yes,10/26/17,DigitalGenius Ltd.;DigitalGenius Ltd.;DigitalGenius Ltd.,-1;-1;-1,-1;-1;-1,
905,905,905,905,905,905,905,905,ICLR,2018,Generative Adversarial Networks using Adaptive Convolution,Nhat M. Nguyen;Nilanjan Ray,nmnguyen@ualberta.ca;nray1@ualberta.ca,4;4;4,5;4;5,Reject,0,0,0.0,yes,10/26/17,University of Alberta;University of Alberta,99;99,119;119,5
906,906,906,906,906,906,906,906,ICLR,2018,Deep Rewiring: Training very sparse deep networks,Guillaume Bellec;David Kappel;Wolfgang Maass;Robert Legenstein,bellec@igi.tugraz.at;kappel@igi.tugraz.at;maass@igi.tugraz.at;legenstein@igi.tugraz.at,8;5;6,4;5;4,Accept (Poster),0,0,1.0,yes,10/26/17,Graz University of Technology;Graz University of Technology;Graz University of Technology;Graz University of Technology,104;104;104;104,443;443;443;443,
907,907,907,907,907,907,907,907,ICLR,2018,Network Iterative Learning for Dynamic Deep Neural Networks via Morphism,Tao Wei;Changhu Wang;Chang Wen Chen,taowei@buffalo.edu;wangchanghu@toutiao.com;chencw@buffalo.edu,7;5;5,4;2;3,Reject,0,0,0.0,yes,10/26/17,"State University of New York, Buffalo;Toutiao AI Lab;State University of New York, Buffalo",85;-1;85,270;-1;270,
908,908,908,908,908,908,908,908,ICLR,2018,Generalized Graph Embedding Models,Qiao Liu;Xiaohui Yang;Rui Wan;Shouzhong Tu;Zufeng Wu,qliu@uestc.edu.cn;yangxhui@uestc.std.edu.cn;rwan@uestc.std.edu.cn;tusz11@mails.tsinghua.edu.cn;wuzufeng@uestc.edu.cn,6;4;3,4;4;4,Reject,0,0,0.0,yes,10/26/17,University of Electronic Science and Technology of China;Tsinghua University;Tsinghua University;Tsinghua University;University of Electronic Science and Technology of China,468;10;10;10;468,843;30;30;30;843,10
909,909,909,909,909,909,909,909,ICLR,2018,Discriminative k-shot learning using probabilistic models,Matthias Bauer;Mateo Rojas-Carulla;Jakub Bartłomiej Świątkowski;Bernhard Schölkopf;Richard E. Turner,msb55@cam.ac.uk;mrojascarulla@gmail.com;kuba.swiatkowski@gmail.com;bs@tuebingen.mpg.de;ret26@cam.ac.uk,5;5;5,3;3;3,Reject,0,4,0.0,yes,10/26/17,"University of Cambridge;University of Cambridge;University of Cambridge;Max Planck Institute for Intelligent Systems, Max-Planck Institute;University of Cambridge",71;71;71;-1;71,2;2;2;-1;2,
910,910,910,910,910,910,910,910,ICLR,2018,Link Weight Prediction with Node Embeddings,Yuchen Hou;Lawrence B. Holder,yuchen.hou@wsu.edu;holder@wsu.edu,3;3;4,4;5;3,Reject,0,0,0.0,yes,10/26/17,SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY,468;468,352;352,3;10
911,911,911,911,911,911,911,911,ICLR,2018,Learning Independent Causal Mechanisms,Giambattista Parascandolo;Mateo Rojas Carulla;Niki Kilbertus;Bernhard Schoelkopf,gparascandolo@tue.mpg.de;mrojascarulla@gmail.com;nkilbertus@tue.mpg.de;bs@tue.mpg.de,6;5;5,4;4;4,Reject,0,6,0.0,yes,10/26/17,"Max Planck Institute for Intelligent Systems, Max-Planck Institute;University of Cambridge;Max Planck Institute for Intelligent Systems, Max-Planck Institute;Max Planck Institute for Intelligent Systems, Max-Planck Institute",-1;71;-1;-1,-1;2;-1;-1,5
912,912,912,912,912,912,912,912,ICLR,2018,DNN Feature Map Compression using Learned Representation over GF(2),Denis A. Gudovskiy;Alec Hodgkinson;Luca Rigazio,denis.gudovskiy@us.panasonic.com;alec.hodgkinson@us.panasonic.com;luca.rigazio@us.panasonic.com,7;5;4,3;4;4,Reject,0,4,0.0,yes,10/26/17,Us.panasonic;Rensselaer Polytechnic Institute;Us.panasonic,-1;153;-1,-1;304;-1,2
913,913,913,913,913,913,913,913,ICLR,2018,A Bayesian Perspective on Generalization and Stochastic Gradient Descent,Samuel L. Smith;Quoc V. Le,slsmith@google.com;qvl@google.com,3;7;7,4;4;3,Accept (Poster),2,12,0.0,yes,10/26/17,Google;Google,-1;-1,-1;-1,11;8
914,914,914,914,914,914,914,914,ICLR,2018,Mixed Precision Training,Paulius Micikevicius;Sharan Narang;Jonah Alben;Gregory Diamos;Erich Elsen;David Garcia;Boris Ginsburg;Michael Houston;Oleksii Kuchaiev;Ganesh Venkatesh;Hao Wu,pauliusm@nvidia.com;sharan@baidu.com;alben@nvidia.com;gdiamos@baidu.com;eriche@google.com;dagarcia@nvidia.com;bginsburg@nvidia.com;mhouston@nvidia.com;okuchaiev@nvidia.com;gavenkatesh@nvidia.com;skyw@nvidia.com,8;5;7,4;3;4,Accept (Poster),0,6,0.0,yes,10/26/17,NVIDIA;Baidu;NVIDIA;Baidu;Google;NVIDIA;NVIDIA;NVIDIA;NVIDIA;NVIDIA;NVIDIA,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,
915,915,915,915,915,915,915,915,ICLR,2018,Multiscale Hidden Markov Models For Covariance Prediction,João Sedoc;Jordan Rodu;Dean Foster;Lyle Ungar,joao@cis.upenn.edu;jsr6q@virginia.edu;dean@foster.net;ungar@cis.upenn.edu,5;6;6,3;4;4,Reject,0,3,0.0,yes,10/26/17,University of Pennsylvania;University of Virginia;University of Pennsylvania;University of Pennsylvania,19;62;19;19,10;113;10;10,5
916,916,916,916,916,916,916,916,ICLR,2018,Latent Topic Conversational Models,Tsung-Hsien Wen;Minh-Thang Luong,thw28@cam.ac.uk;thangluong@google.com,4;5;6,3;4;4,Reject,0,8,0.0,yes,10/26/17,University of Cambridge;Google,71;-1,2;-1,
917,917,917,917,917,917,917,917,ICLR,2018,Recurrent Auto-Encoder Model for Multidimensional Time Series Representation,Timothy Wong;Zhiyuan Luo,timothy.wong@centrica.com;zhiyuan.luo@cs.rhul.ac.uk,2;2;4,4;5;4,Reject,0,4,0.0,yes,10/26/17,"Royal Holloway, University of London;Royal Holloway, University of London",181;181,195;195,
918,918,918,918,918,918,918,918,ICLR,2018,Sparse-Complementary Convolution for Efficient Model Utilization on CNNs,Chun-Fu (Richard) Chen;Jinwook Oh;Quanfu Fan;Marco Pistoia;Gwo Giun (Chris) Lee,chenrich@us.ibm.com;ohj@us.ibm.com;qfan@us.ibm.com;pistoia@us.ibm.com;clee@mail.ncku.edu.tw,6;5;5,5;4;4,Reject,0,5,0.0,yes,10/26/17,International Business Machines;International Business Machines;International Business Machines;International Business Machines;Peking University,-1;-1;-1;-1;24,-1;-1;-1;-1;27,
919,919,919,919,919,919,919,919,ICLR,2018,Achieving morphological agreement with Concorde,Daniil Polykovskiy;Dmitry Soloviev,daniil.polykovskiy@gmail.com;d.soloviev@corp.mail.ru,2;5;6,5;4;5,Reject,0,0,0.0,yes,10/26/17,Lomonosov Moscow State University;,468;-1,193;-1,
920,920,920,920,920,920,920,920,ICLR,2018,Discrete-Valued Neural Networks Using Variational Inference,Wolfgang Roth;Franz Pernkopf,roth@tugraz.at;pernkopf@tugraz.at,6;5;5,1;4;4,Reject,0,3,0.0,yes,10/27/17,Graz University of Technology;Graz University of Technology,104;104,443;443,11
921,921,921,921,921,921,921,921,ICLR,2018,Improving Deep Learning by Inverse Square Root Linear Units (ISRLUs),Brad Carlile;Guy Delamarter;Paul Kinney;Akiko Marti;Brian Whitney,bradcarlile@yahoo.com;info@aiperf.com,4;5;3,4;4;4,Reject,3,1,0.0,yes,10/27/17,AI Perf Eng;Aiperf,-1;-1,-1;-1,8
922,922,922,922,922,922,922,922,ICLR,2018,Feat2Vec:  Dense Vector Representation for Data with Arbitrary Features,Luis Armona;José P. González-Brenes;Ralph Edezhath,luisarmona@gmail.com;jgonzalez@chegg.com;ralph.angelus@gmail.com,7;2;7,3;2;5,Reject,0,6,0.0,yes,10/27/17,Stanford University;Chegg;,4;-1;-1,3;-1;-1,
923,923,923,923,923,923,923,923,ICLR,2018,Jointly Learning Sentence Embeddings and Syntax with Unsupervised Tree-LSTMs,Jean Maillard;Stephen Clark;Dani Yogatama,jean@maillard.it;sc609@cam.ac.uk;dyogatama@google.com,5;6;4,4;4;4,Reject,0,0,0.0,yes,10/27/17,University of Cambridge;University of Cambridge;Google,71;71;-1,2;2;-1,3
924,924,924,924,924,924,924,924,ICLR,2018,COLD FUSION: TRAINING SEQ2SEQ MODELS TOGETHER WITH LANGUAGE MODELS,Anuroop Sriram;Heewoo Jun;Sanjeev Satheesh;Adam Coates,anuroop.sriram@gmail.com;junheewoo@baidu.com;sanjeevsatheesh@baidu.com;adamcoates@baidu.com,5;6;5,5;5;5,Invite to Workshop Track,0,0,0.0,yes,10/27/17,;Baidu;Baidu;Baidu,-1;-1;-1;-1,-1;-1;-1;-1,3;8
925,925,925,925,925,925,925,925,ICLR,2018,Long Short-Term Memory as a Dynamically Computed Element-wise Weighted Sum,Omer Levy;Kenton Lee;Nicholas FitzGerald;Luke Zettlemoyer,omerlevy@gmail.com;kentonl@cs.washington.edu;nfitz@cs.washington.edu;lsz@cs.washington.edu,6;5;7,4;5;3,Reject,8,5,0.0,yes,10/27/17,University of Washington;University of Washington;University of Washington;University of Washington,6;6;6;6,25;25;25;25,3
926,926,926,926,926,926,926,926,ICLR,2018,AUTOMATED DESIGN USING NEURAL NETWORKS AND GRADIENT DESCENT,Oliver Hennigh,loliverhennigh101@gmail.com,5;7;4,4;4;5,Invite to Workshop Track,0,0,0.0,yes,10/27/17,,,,
927,927,927,927,927,927,927,927,ICLR,2018,Tensor Contraction & Regression Networks,Jean Kossaifi;Zack Chase Lipton;Aran Khanna;Tommaso Furlanello;Anima Anandkumar,jean.kossaifi@gmail.com;zlipton@cmu.edu;arankhan@amazon.com;tfurlanello@gmail.com;animakumar@gmail.com,4;6;4,4;4;3,Reject,0,5,0.0,yes,10/27/17,Imperial College London;Carnegie Mellon University;Amazon;University of Southern California;University of California-Irvine,74;1;-1;31;36,8;24;-1;66;99,
928,928,928,928,928,928,928,928,ICLR,2018,Fixing Weight Decay Regularization in Adam,Ilya Loshchilov;Frank Hutter,ilya.loshchilov@gmail.com;fh@cs.uni-freiburg.de,7;4;8,4;4;3,Reject,0,18,1.0,yes,10/27/17,Universität Freiburg;Universität Freiburg,115;115,82;82,8
929,929,929,929,929,929,929,929,ICLR,2018,Exploring the Space of Black-box Attacks on Deep Neural Networks,Arjun Nitin Bhagoji;Warren He;Bo Li;Dawn Song,abhagoji@princeton.edu;_w@eecs.berkeley.edu;lxbosky@gmail.com;dawnsong@gmail.com,5;6;7,4;3;4,Reject,3,6,0.0,yes,10/27/17,Princeton University;University of California Berkeley;University of California Berkeley;University of California Berkeley,31;5;5;5,7;18;18;18,4
930,930,930,930,930,930,930,930,ICLR,2018,Code Synthesis with Priority Queue Training,Daniel A. Abolafia;Quoc V. Le;Mohammad Norouzi,danabo@google.com;qvl@google.com;mnorouzi@google.com,6;5;6,4;3;4,Reject,0,8,2.0,yes,10/27/17,Google;Google;Google,-1;-1;-1,-1;-1;-1,
931,931,931,931,931,931,931,931,ICLR,2018,Implicit Causal Models for Genome-wide Association Studies,Dustin Tran;David M. Blei,dustin@cs.columbia.edu;david.blei@columbia.edu,5;6;6,5;5;5,Accept (Poster),0,12,0.0,yes,10/27/17,Columbia University;Columbia University,15;15,14;14,5;11
932,932,932,932,932,932,932,932,ICLR,2018,Learning what to learn in a neural program,Richard Shin;Dawn Song,ricshin@berkeley.edu;dawnsong.travel@gmail.com,5;4;5,4;4;2,Reject,3,3,0.0,yes,10/27/17,University of California Berkeley;University of California Berkeley,5;5,18;18,8
933,933,933,933,933,933,933,933,ICLR,2018,Generative Models for Alignment and Data Efficiency in Language,Dustin Tran;Yura Burda;Ilya Sutskever,dustin@cs.columbia.edu;yburda@openai.com;ilyasu@openai.com,5;4;2,3;3;3,Reject,0,0,0.0,yes,10/27/17,Columbia University;OpenAI;OpenAI,15;-1;-1,14;-1;-1,3
934,934,934,934,934,934,934,934,ICLR,2018,Maintaining cooperation in complex social dilemmas using deep reinforcement learning,Alexander Peysakhovich;Adam Lerer,alex.peys@gmail.com;alerer@fb.com,4;4;3,4;4;5,Reject,0,15,0.0,yes,10/27/17,Facebook;Facebook,-1;-1,-1;-1,
935,935,935,935,935,935,935,935,ICLR,2018,TRAINING GENERATIVE ADVERSARIAL NETWORKS VIA PRIMAL-DUAL SUBGRADIENT METHODS: A LAGRANGIAN PERSPECTIVE ON GAN,Xu Chen;Jiang Wang;Hao Ge,chenxugz@gmail.com;wangjiangb@gmail.com;haoge2013@u.northwestern.edu,7;6;7,4;4;3,Accept (Poster),0,14,0.0,yes,10/27/17,Northwestern University;;Northwestern University,42;-1;42,20;-1;20,5;4;1;9
936,936,936,936,936,936,936,936,ICLR,2018,Stochastic Training of Graph Convolutional Networks,Jianfei Chen;Jun Zhu,chenjian14@mails.tsinghua.edu.cn;dcszj@mail.tsinghua.edu.cn,3;4;7,4;4;3,Reject,0,9,0.0,yes,10/27/17,Tsinghua University;Tsinghua University,10;10,30;30,10
937,937,937,937,937,937,937,937,ICLR,2018,Transformation Autoregressive Networks,Junier Oliva;Avinava Dubey;Barnabás Póczos;Eric P. Xing;Jeff Schneider,joliva@cs.cmu.edu;akdubey@cs.cmu.edu;bapoczos@cs.cmu.edu;epxing@cs.cmu.edu;schneide@cs.cmu.edu,5;5;8,3;2;4,Reject,0,6,0.0,yes,10/27/17,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1;1,24;24;24;24;24,
938,938,938,938,938,938,938,938,ICLR,2018,SQLNet: Generating Structured Queries From Natural Language Without Reinforcement Learning,Xiaojun Xu;Chang Liu;Dawn Song,xuxiaojun1005@gmail.com;liuchang@eecs.berkeley.edu;dawnsong@cs.berkeley.edu,4;7;5,5;4;5,Reject,0,10,0.0,yes,10/27/17,Shanghai Jiao Tong University;University of California Berkeley;University of California Berkeley,57;5;5,188;18;18,3;10
939,939,939,939,939,939,939,939,ICLR,2018,Nearest Neighbour Radial Basis Function Solvers for Deep Neural Networks,Benjamin J. Meyer;Ben Harwood;Tom Drummond,benjamin.meyer@monash.edu;ben.harwood@monash.edu;tom.drummond@monash.edu,5;3;4,4;4;4,Reject,0,3,0.0,yes,10/27/17,Monash University;Monash University;Monash University,124;124;124,80;80;80,
940,940,940,940,940,940,940,940,ICLR,2018,Deep Learning Inferences with Hybrid Homomorphic Encryption,Anthony Meehan;Ryan K L Ko;Geoff Holmes,anthonymeehan@anthonymeehan.com;ryan.ko@waikato.ac.nz;geoff@waikato.ac.nz,4;4;4,4;5;5,Reject,0,4,0.0,yes,10/27/17,Anthonymeehan;The University of Waikato;The University of Waikato,-1;291;291,-1;393;393,
941,941,941,941,941,941,941,941,ICLR,2018,Adversarial reading networks for machine comprehension,Quentin Grail;Julien Perez,julien.perez@naverlabs.com,4;5;5,5;5;4,Reject,0,1,0.0,yes,10/27/17,Naver Labs Europe,-1,-1,3;4
942,942,942,942,942,942,942,942,ICLR,2018,Log-DenseNet: How to Sparsify a DenseNet,Hanzhang Hu;Debadeepta Dey;Allie Del Giorno;Martial Hebert;J. Andrew Bagnell,hanzhang@cs.cmu.edu;dedey@microsoft.com;adelgior@ri.cmu.edu;hebert@ri.cmu.edu;dbagnell@ri.cmu.edu,6;6;5,4;4;4,Reject,0,0,0.0,yes,10/27/17,Carnegie Mellon University;Microsoft;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;-1;1;1;1,24;-1;24;24;24,2
943,943,943,943,943,943,943,943,ICLR,2018,Contextual memory bandit for pro-active dialog engagement,julien perez;Tomi Silander,julien.perez@naverlabs.com,2;3;3,5;4;4,Reject,0,0,0.0,yes,10/27/17,Naver Labs Europe,-1,-1,
944,944,944,944,944,944,944,944,ICLR,2018,Sequence Transfer Learning for Neural Decoding,Venkatesh Elango*;Aashish N Patel*;Kai J Miller;Vikash Gilja,velango@eng.ucsd.edu;anp054@eng.ucsd.edu;kai.miller@stanford.edu;vgilja@eng.ucsd.edu,4;6;3,5;5;4,Reject,0,5,0.0,yes,10/27/17,"University of California, San Diego;University of California, San Diego;Stanford University;University of California, San Diego",11;11;4;11,31;31;3;31,6
945,945,945,945,945,945,945,945,ICLR,2018,Unbiased scalable softmax optimization,Francois Fagan;Garud Iyengar,ff2316@columbia.edu;garud@ieor.columbia.edu,5;5;5,4;3;4,Reject,0,4,0.0,yes,10/27/17,Columbia University;Columbia University,15;15,14;14,3
946,946,946,946,946,946,946,946,ICLR,2018,BLOCK-DIAGONAL HESSIAN-FREE OPTIMIZATION FOR TRAINING NEURAL NETWORKS,Huishuai Zhang;Caiming Xiong;James Bradbury;Richard Socher,hzhan23@syr.edu;cxiong@salesforce.com;james.bradbury@salesforce.com;richard@socher.org,6;4;6,4;4;4,Reject,0,4,0.0,yes,10/27/17,Syracuse University;SalesForce.com;SalesForce.com;SalesForce.com,244;-1;-1;-1,275;-1;-1;-1,8
947,947,947,947,947,947,947,947,ICLR,2018,Polar Transformer Networks,Carlos Esteves;Christine Allen-Blanchette;Xiaowei Zhou;Kostas Daniilidis,machc@seas.upenn.edu;allec@seas.upenn.edu;xiaowz@seas.upenn.edu;kostas@seas.upenn.edu,7;7;8,4;4;3,Accept (Poster),0,5,2.0,yes,10/27/17,University of Pennsylvania;University of Pennsylvania;University of Pennsylvania;University of Pennsylvania,19;19;19;19,10;10;10;10,
948,948,948,948,948,948,948,948,ICLR,2018,Softmax Q-Distribution Estimation for Structured Prediction: A Theoretical Interpretation for RAML,Xuezhe Ma;Pengcheng Yin;Jingzhou Liu;Graham Neubig;Eduard Hovy,xuezhem@cs.cmu.edu;pcyin@cs.cmu.edu;liujingzhou@cs.cmu.edu;gneubig@cs.cmu.edu;hovy@cs.cmu.edu,5;5;6,2;4;3,Reject,0,4,0.0,yes,10/27/17,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1;1,24;24;24;24;24,3;11
949,949,949,949,949,949,949,949,ICLR,2018,Learning to Optimize Neural Nets,Ke Li;Jitendra Malik,ke.li@eecs.berkeley.edu;jitendram@google.com,5;6;6,3;4;3,Reject,0,3,0.0,yes,10/27/17,University of California Berkeley;Google,5;-1,18;-1,
950,950,950,950,950,950,950,950,ICLR,2018,Character Level Based Detection of DGA Domain Names,Bin Yu;Jie Pan;Jiaming Hu;Anderson Nascimento;Martine De Cock,biny@infoblox.com;jiep@uw.edu;huj22@uw.edu;andclay@uw.edu;mdecock@uw.edu,7;5;4,4;3;4,Reject,0,4,0.0,yes,10/27/17,"Infoblox;University of Washington, Seattle;University of Washington, Seattle;University of Washington, Seattle;University of Washington, Seattle",-1;6;6;6;6,-1;25;25;25;25,
951,951,951,951,951,951,951,951,ICLR,2018,Data Augmentation by Pairing Samples for Images Classification,Hiroshi Inoue,inouehrs@jp.ibm.com,4;5;6,5;4;4,Reject,1,20,0.0,yes,10/27/17,International Business Machines,-1,-1,
952,952,952,952,952,952,952,952,ICLR,2018,Weighted Transformer Network for Machine Translation,Karim Ahmed;Nitish Shirish Keskar;Richard Socher,karim.mmm@gmail.com;keskar.nitish@u.northwestern.edu;richard@socher.org,9;4;6,4;4;5,Reject,2,5,0.0,yes,10/27/17,Dartmouth College;Northwestern University;SalesForce.com,153;42;-1,89;20;-1,3
953,953,953,953,953,953,953,953,ICLR,2018,Principled Hybrids of Generative and Discriminative Domain Adaptation,Han Zhao;Zhenyao Zhu;Junjie Hu;Adam Coates;Geoff Gordon,han.zhao@cs.cmu.edu;zhenyaozhu@baidu.com;junjieh@cmu.edu;adamcoates@baidu.com;ggordon@cs.cmu.edu,6;5;5,3;4;4,Reject,0,5,0.0,yes,10/27/17,Carnegie Mellon University;Baidu;Carnegie Mellon University;Baidu;Carnegie Mellon University,1;-1;1;-1;1,24;-1;24;-1;24,5;1
954,954,954,954,954,954,954,954,ICLR,2018,Integrating Episodic Memory into a Reinforcement Learning Agent Using Reservoir Sampling,Kenny J. Young;Shuo Yang;Richard S. Sutton,kjyoung@ualberta.ca;rsutton@ualberta.ca;s-yan14@mails.tsinghua.edu.cn,4;4;4,4;3;3,Reject,0,4,0.0,yes,10/27/17,University of Alberta;University of Alberta;Tsinghua University,99;99;10,119;119;30,
955,955,955,955,955,955,955,955,ICLR,2018,Relational Multi-Instance Learning for Concept Annotation from Medical Time Series,Sanjay Purushotham;Zhengping Che;Bo Jiang;Tanachat Nilanon;Yan Liu,spurusho@usc.edu;zche@usc.edu;boj@usc.edu;nilanon@usc.edu;yanliu.cs@usc.edu,6;3;3,4;3;5,Reject,0,3,0.0,yes,10/27/17,University of Southern California;University of Southern California;University of Southern California;University of Southern California;University of Southern California,31;31;31;31;31,66;66;66;66;66,3
956,956,956,956,956,956,956,956,ICLR,2018,VOCABULARY-INFORMED VISUAL FEATURE AUGMENTATION FOR ONE-SHOT LEARNING,jianqi ma;hangyu lin;yinda zhang;yanwei fu;xiangyang xue,14302010017@fudan.edu.cn;16210240036@fudan.edu.cn;yindaz@cs.princeton.edu;y.fu@qmul.ac.uk,4;6;5,3;4;4,Reject,0,0,0.0,yes,10/27/17,Fudan University;Fudan University;Princeton University;Queen Mary University London,78;78;31;244,116;116;7;121,
957,957,957,957,957,957,957,957,ICLR,2018,Understanding Local Minima in Neural Networks by Loss Surface Decomposition,Hanock Kwak;Byoung-Tak Zhang,hnkwak@bi.snu.ac.kr;btzhang@bi.snu.ac.kr,4;5;4,4;4;4,Reject,0,0,0.0,yes,10/27/17,Seoul National University;Seoul National University,46;46,74;74,1
958,958,958,958,958,958,958,958,ICLR,2018,Viterbi-based Pruning for Sparse Matrix with Fixed and High Index Compression Ratio,Dongsoo Lee;Daehyun Ahn;Taesu Kim;Pierce I. Chuang;Jae-Joon Kim,dslee3@gmail.com;daehyun.ahn@postech.ac.kr;taesukim@postech.ac.kr;pchuang@us.ibm.com;jaejoon@postech.ac.kr,6;6;7,4;4;3,Accept (Poster),0,4,0.0,yes,10/27/17,Samsung;POSTECH;POSTECH;International Business Machines;POSTECH,-1;115;115;-1;115,-1;137;137;-1;137,
959,959,959,959,959,959,959,959,ICLR,2018,Toward learning better metrics for sequence generation training with policy gradient,Joji Toyama;Yusuke Iwasawa;Kotaro Nakayama;Yutaka Matsuo,toyama@weblab.t.u-tokyo.ac.jp;iwasawa@weblab.t.u-tokyo.ac.jp;nakayama@weblab.t.u-tokyo.ac.jp;matsuo@weblab.t.u-tokyo.ac.jp,7;4;7,1;3;3,Reject,0,7,0.0,yes,10/27/17,The University of Tokyo;The University of Tokyo;The University of Tokyo;The University of Tokyo,52;52;52;52,45;45;45;45,5;4
960,960,960,960,960,960,960,960,ICLR,2018,Classifier-to-Generator Attack: Estimation of Training Data Distribution from Classifier,Kosuke Kusano;Jun Sakuma,cocuh@mdl.cs.tsukuba.ac.jp;jun@cs.tsukuba.ac.jp,7;4;4,3;3;3,Reject,0,4,0.0,yes,10/27/17,University of Tsukuba;University of Tsukuba,468;468,1103;1103,5;4;2
961,961,961,961,961,961,961,961,ICLR,2018,Large scale distributed neural network training through online distillation,Rohan Anil;Gabriel Pereyra;Alexandre Passos;Robert Ormandi;George E. Dahl;Geoffrey E. Hinton,rohananil@google.com;pereyra@google.com;apassos@google.com;ormandi@google.com;gdahl@google.com;geoffhinton@google.com,8;4;6,4;3;3,Accept (Poster),0,6,0.0,yes,10/27/17,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,3
962,962,962,962,962,962,962,962,ICLR,2018,Better Generalization by Efficient Trust Region Method,Xuanqing Liu;Jason D. Lee;Cho-Jui Hsieh,xqliu@ucdavis.edu;jasondlee88@gmail.com;chohsieh@ucdavis.edu,6;5;6,5;2;3,Reject,0,3,0.0,yes,10/27/17,"University of California, Davis;University of Southern California;University of California, Davis",78;31;78,54;66;54,9;8
963,963,963,963,963,963,963,963,ICLR,2018,Enhancing the Transferability of Adversarial Examples with Noise Reduced Gradient,Lei Wu;Zhanxing Zhu;Cheng Tai;Weinan E,leiwu@pku.edu.cn;zhanxing.zhu@pku.edu.cn;chengt@math.princeton.edu;weinan@math.princeton.edu,4;5;5,4;3;4,Reject,0,0,0.0,yes,10/27/17,Peking University;Peking University;Princeton University;Princeton University,24;24;31;31,27;27;7;7,4
964,964,964,964,964,964,964,964,ICLR,2018,Discovering Order in Unordered Datasets: Generative Markov Networks,Yao-Hung Hubert Tsai;Han Zhao;Nebojsa Jojic;Ruslan Salakhutdinov,yaohungt@cs.cmu.edu;han.zhao@cs.cmu.edu;jojic@microsoft.com;rsalakhu@cs.cmu.edu,4;4;4,4;4;4,Reject,0,3,0.0,yes,10/27/17,Carnegie Mellon University;Carnegie Mellon University;Microsoft;Carnegie Mellon University,1;1;-1;1,24;24;-1;24,5
965,965,965,965,965,965,965,965,ICLR,2018,Kronecker Recurrent Units,Cijo Jose;Moustapha Cisse;Francois Fleuret,cijo.jose@idiap.ch;moustaphacisse@fb.com;francois.fleuret@idiap.ch,6;5;7,5;4;3,Invite to Workshop Track,0,1,0.0,yes,10/27/17,Idiap Research Institute;Facebook;Idiap Research Institute,-1;-1;-1,-1;-1;-1,
966,966,966,966,966,966,966,966,ICLR,2018,Characterizing Sparse Connectivity Patterns in Neural Networks,Sourya Dey;Kuan-Wen Huang;Peter A. Beerel;Keith M. Chugg,souryade@usc.edu;kuanwenh@usc.edu;pabeerel@usc.edu;chugg@usc.edu,4;5;4,3;3;3,Reject,0,5,0.0,yes,10/27/17,University of Southern California;University of Southern California;University of Southern California;University of Southern California,31;31;31;31,66;66;66;66,1
967,967,967,967,967,967,967,967,ICLR,2018,Multi-level Residual Networks from Dynamical Systems View,Bo Chang;Lili Meng;Eldad Haber;Frederick Tung;David Begert,bchang@stat.ubc.ca;lilimeng1103@gmail.com;haber@math.ubc.ca;ftung@sfu.ca;david@xtract.ai,7;7;7,4;3;4,Accept (Poster),0,5,0.0,yes,10/27/17,University of British Columbia;University of British Columbia;University of British Columbia;Simon Fraser University;Xtract AI,34;34;34;57;-1,34;34;34;253;-1,3;2
968,968,968,968,968,968,968,968,ICLR,2018,Pointing Out SQL Queries From Text,Chenglong Wang;Marc Brockschmidt;Rishabh Singh,clwang@cs.washington.edu;mabrocks@microsoft.com;risin@microsoft.com,4;3;7,4;4;4,Reject,0,3,0.0,yes,10/27/17,University of Washington;Microsoft;Microsoft,6;-1;-1,25;-1;-1,3
969,969,969,969,969,969,969,969,ICLR,2018,Learning Graph Convolution Filters from Data Manifold,Guokun Lai;Hanxiao Liu;Yiming Yang,guokun@cs.cmu.edu;hanxiaol@cs.cmu.edu;yiming@cs.cmu.edu,4;6;5,5;4;3,Reject,0,4,0.0,yes,10/27/17,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1,24;24;24,2;10
970,970,970,970,970,970,970,970,ICLR,2018,Interpreting Deep Classification Models With Bayesian Inference,Hanshu Yan;Jiashi Feng,eleyanh@nus.edu.sg;elefjia@nus.edu.sg,3;5;3,3;3;4,Reject,0,0,0.0,yes,10/27/17,National University of Singapore;National University of Singapore,16;16,22;22,5;11
971,971,971,971,971,971,971,971,ICLR,2018,Learning Latent Representations in Neural Networks for Clustering through Pseudo Supervision and Graph-based Activity Regularization,Ozsel Kilinc;Ismail Uysal,ozsel@mail.usf.edu;iuysal@usf.edu,6;7;7,5;4;3,Accept (Poster),2,9,0.0,yes,10/27/17,University of South Florida;University of South Florida,291;291,255;255,10
972,972,972,972,972,972,972,972,ICLR,2018,Parametric Manifold Learning Via Sparse Multidimensional Scaling,Gautam Pai;Ronen Talmon;Ron Kimmel,paigautam@cs.technion.ac.il;ronen@ef.technion.ac.il;ron@cs.technion.ac.il,5;4;3,5;4;4,Reject,0,3,0.0,yes,10/27/17,Technion;Technion;Technion,24;24;24,327;327;327,8
973,973,973,973,973,973,973,973,ICLR,2018,Fast and Accurate Inference with Adaptive Ensemble Prediction for Deep Networks,Hiroshi Inoue,inouehrs@jp.ibm.com,6;5;5,3;4;4,Reject,0,3,0.0,yes,10/27/17,International Business Machines,-1,-1,
974,974,974,974,974,974,974,974,ICLR,2018,Counterfactual Image Networks,Deniz Oktay;Carl Vondrick;Antonio Torralba,denizokt@mit.edu;vondrick@google.com;torralba@mit.edu,4;5;4,4;4;4,Reject,0,1,0.0,yes,10/27/17,Massachusetts Institute of Technology;Google;Massachusetts Institute of Technology,2;-1;2,5;-1;5,5;2
975,975,975,975,975,975,975,975,ICLR,2018,Massively Parallel Hyperparameter Tuning,Lisha Li;Kevin Jamieson;Afshin Rostamizadeh;Katya Gonina;Moritz Hardt;Benjamin Recht;Ameet Talwalkar,lishal@cs.ucla.edu;jamieson@cs.washington.edu;rostami@google.com;kgonina@google.com;hardt@berkeley.edu;brecht@berkeley.edu;talwalkar@cmu.edu,5;6;5,5;3;5,Reject,0,5,0.0,yes,9/27/18,"University of California, Los Angeles;University of Washington;Google;Google;University of California Berkeley;University of California Berkeley;Carnegie Mellon University",20;6;-1;-1;5;5;1,15;25;-1;-1;18;18;24,
976,976,976,976,976,976,976,976,ICLR,2018,Hallucinating brains with artificial brains,Peiye Zhuang;Alexander G. Schwing;Oluwasanmi Koyejo,py_zhuang@bupt.edu.cn;aschwing@illinois.edu;sanmi@illinois.edu,8;6;5,5;4;3,Reject,0,4,0.0,yes,10/27/17,"Beijing University of Post and Telecommunication;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",468;3;3,1103;37;37,5;4
977,977,977,977,977,977,977,977,ICLR,2018,Still not systematic after all these years: On the compositional skills of sequence-to-sequence recurrent networks,Brenden Lake;Marco Baroni,brenden@nyu.edu;marco.baroni@unitn.it,6;7;6,3;4;5,Invite to Workshop Track,0,3,0.0,yes,10/27/17,New York University;University of Trento,26;17,27;258,8;3;1;6
978,978,978,978,978,978,978,978,ICLR,2018,FusionNet: Fusing via Fully-aware Attention with Application to Machine Comprehension,Hsin-Yuan Huang;Chenguang Zhu;Yelong Shen;Weizhu Chen,momohuang@gmail.com;chezhu@microsoft.com;yeshen@microsoft.com;wzchen@microsoft.com,7;8;7,5;3;4,Accept (Poster),4,17,3.0,yes,10/27/17,National Taiwan University;Microsoft;Microsoft;Microsoft,85;-1;-1;-1,197;-1;-1;-1,4;8
979,979,979,979,979,979,979,979,ICLR,2018,Multi-task Learning on MNIST Image Datasets,Po-Chen Hsieh;Chia-Ping Chen,st70712@gmail.com;cpchen@cse.nsysu.edu.tw,5;4;5,4;5;3,Reject,0,15,0.0,yes,10/27/17,Purdue University;SUN YAT-SEN UNIVERSITY,28;468,60;352,
980,980,980,980,980,980,980,980,ICLR,2018,A closer look at the word analogy problem,Siddharth Krishna Kumar,siddharthkumar@upwork.com,2;3;3,5;4;4,Reject,0,0,0.0,yes,10/27/17,Upwork,-1,-1,5
981,981,981,981,981,981,981,981,ICLR,2018,Learning Deep ResNet Blocks Sequentially using Boosting Theory,Furong Huang;Jordan T. Ash;John Langford;Robert E. Schapire,furongh@cs.umd.edu;jordantash@gmail.com;jcl@microsoft.com;schapire@microsoft.com,5;4;5,4;4;3,Reject,0,7,0.0,yes,10/27/17,"University of Maryland, College Park;Princeton University;Microsoft;Microsoft",12;31;-1;-1,69;7;-1;-1,1;8
982,982,982,982,982,982,982,982,ICLR,2018,A Flexible Approach to Automated RNN Architecture Generation,Martin Schrimpf;Stephen Merity;James Bradbury;Richard Socher,msch@mit.edu;smerity@smerity.com;james.bradbury@salesforce.com;richard@socher.org,6;4;5,4;4;4,Invite to Workshop Track,0,9,0.0,yes,10/27/17,Massachusetts Institute of Technology;Smerity;SalesForce.com;SalesForce.com,2;-1;-1;-1,5;-1;-1;-1,3
983,983,983,983,983,983,983,983,ICLR,2018,Large Scale Multi-Domain Multi-Task Learning with MultiModel,Lukasz Kaiser;Aidan N. Gomez;Noam Shazeer;Ashish Vaswani;Niki Parmar;Llion Jones;Jakob Uszkoreit,lukaszkaiser@google.com;aidan.n.gomez@gmail.com;noam@google.com;avaswani@google.com;nikip@google.com;llion@google.com;usz@google.com,6;3;6,3;5;4,Reject,0,6,0.0,yes,10/27/17,"Google;Department of Computer Science, University of Toronto;Google;Google;Google;Google;Google",-1;17;-1;-1;-1;-1;-1,-1;22;-1;-1;-1;-1;-1,6
984,984,984,984,984,984,984,984,ICLR,2018,Learning Generative Models with Locally Disentangled Latent Factors,Brady Neal;Alex Lamb;Sherjil Ozair;Devon Hjelm;Aaron Courville;Yoshua Bengio;Ioannis Mitliagkas,nealb@seas.upenn.edu;alex6200@gmail.com;sherjilozair@gmail.com;erroneus@gmail.com;aaron.courville@gmail.com;yoshua.umontreal@gmail.com;imitliagkas@gmail.com,4;6;3,4;3;4,Reject,0,3,0.0,yes,10/27/17,University of Pennsylvania;University of Montreal;University of Montreal;University of Montreal;University of Montreal;University of Montreal;University of Montreal,19;124;124;124;124;124;124,10;108;108;108;108;108;108,5
985,985,985,985,985,985,985,985,ICLR,2018,Towards Reverse-Engineering Black-Box Neural Networks,Seong Joon Oh;Max Augustin;Mario Fritz;Bernt Schiele,joon@mpi-inf.mpg.de;maxaug@mpi-inf.mpg.de;mfritz@mpi-inf.mpg.de;schiele@mpi-inf.mpg.de,7;7;5,3;4;4,Accept (Poster),0,8,0.0,yes,10/27/17,"Saarland Informatics Campus, Max-Planck Institute;Saarland Informatics Campus, Max-Planck Institute;Saarland Informatics Campus, Max-Planck Institute;Saarland Informatics Campus, Max-Planck Institute",-1;-1;-1;-1,-1;-1;-1;-1,4
986,986,986,986,986,986,986,986,ICLR,2018,A Deep Predictive Coding Network for Learning Latent Representations,Shirin Dora;Cyriel Pennartz;Sander Bohte,shirin.dora@gmail.com;c.m.a.pennartz@uva.nl;s.m.bohte@cwi.nl,4;3;3,4;4;5,Reject,0,0,0.0,yes,10/27/17,University of Amsterdam;University of Amsterdam;Centrum voor Wiskunde en Informatica,181;181;-1,59;59;-1,5
987,987,987,987,987,987,987,987,ICLR,2018,On Batch Adaptive Training for Deep Learning: Lower Loss and Larger Step Size,Runyao Chen;Kun Wu;Ping Luo,chenrunyao14@mails.ucas.ac.cn;WuKun14@mails.ucas.ac.cn;luop@ict.ac.cn,5;5;4,3;4;3,Reject,0,4,0.0,yes,10/27/17,"Chinese Academy of Sciences;Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences",62;62;62,1103;1103;1103,3
988,988,988,988,988,988,988,988,ICLR,2018,Training with Growing Sets: A Simple Alternative to Curriculum Learning and Self Paced Learning,Melike Nur Mermer;Mehmet Fatih Amasyali,melike.mermer@izu.edu.tr;mfatih@ce.yildiz.edu.tr,4;6;4,4;3;4,Reject,0,3,0.0,yes,10/27/17,Yildiz Technical University;Yildiz Technical University,468;468,904;904,
989,989,989,989,989,989,989,989,ICLR,2018,Learning to Mix n-Step Returns: Generalizing Lambda-Returns for Deep Reinforcement Learning,Sahil Sharma;Girish Raguvir J *;Srivatsan Ramesh *;Balaraman Ravindran,sahil@cse.iitm.ac.in;girishraguvir@gmail.com;sriramesh4@gmail.com;ravi@cse.iitm.ac.in,5;6;5,3;4;4,Reject,0,3,0.0,yes,10/27/17,Indian Institute of Technology Madras;;Indian Institute of Technology Madras;Indian Institute of Technology Madras,153;-1;153;153,625;-1;625;625,8
990,990,990,990,990,990,990,990,ICLR,2018,Piecewise Linear Neural Networks verification: A comparative study,Rudy Bunel;Ilker Turkaslan;Philip H.S. Torr;Pushmeet Kohli;M. Pawan Kumar,rudy@robots.ox.ac.uk;ilker.turkaslan@lmh.ox.ac.uk;philip.torr@eng.ox.ac.uk;pushmeet@google.com;pawan@robots.ox.ac.uk,6;5;3,3;4;5,Reject,0,7,0.0,yes,10/27/17,University of Oxford;University of Oxford;University of Oxford;Google;University of Oxford,51;51;51;-1;51,1;1;1;-1;1,
991,991,991,991,991,991,991,991,ICLR,2018,Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms,Tom Zahavy;Bingyi Kang;Alex Sivak;Jiashi Feng;Huan Xu;Shie Mannor,tomzahavy@gmail.com;bingykang@gmail.com;silex@campus.technion.ac.il;jshfeng@gmail.com;huan.xu@isye.gatech.edu;shiemannor@gmail.com,4;4;8,3;5;4,Invite to Workshop Track,0,10,0.0,yes,10/27/17,Technion;National University of Singapore;Technion;NUS;Georgia Institute of Technology;Technion,24;16;24;-1;13;24,327;22;327;-1;33;327,4;8
992,992,992,992,992,992,992,992,ICLR,2018,Heterogeneous Bitwidth Binarization in Convolutional Neural Networks,Josh Fromm;Matthai Philipose;Shwetak Patel,jwfromm@uw.edu;matthaip@microsoft.com;shwetak@cs.washington.edu,6;5;4,4;4;4,Reject,0,8,0.0,yes,10/27/17,"University of Washington, Seattle;Microsoft;University of Washington",6;-1;6,25;-1;25,
993,993,993,993,993,993,993,993,ICLR,2018,Residual Gated Graph ConvNets,Xavier Bresson;Thomas Laurent,xbresson@ntu.edu.sg;tlaurent@lmu.edu,7;3;6,4;4;3,Reject,0,5,0.0,yes,10/27/17,National Taiwan University;Loyola Marymount University,85;468,197;1103,5;10
994,994,994,994,994,994,994,994,ICLR,2018,WHAT ARE GANS USEFUL FOR?,Pablo M. Olmos;Briland Hitaj;Paolo Gasti;Giuseppe Ateniese;Fernando Perez-Cruz,olmos@tsc.uc3m.es;bhitaj@stevens.edu;pgasti@nyit.edu;gatenies@stevens.edu;fernando.perezcruz@sdsc.ethz.ch,3;3;3,5;4;5,Reject,0,0,0.0,yes,10/27/17,Universidad Carlos III de Madrid;Stevens Institute of Technology;New York Institute of Technology;Stevens Institute of Technology;Swiss Federal Institute of Technology,-1;153;468;153;9,-1;512;1103;512;10,5
995,995,995,995,995,995,995,995,ICLR,2018,Learning objects from pixels,David Saxton,saxton@google.com,3;4;4,4;3;4,Reject,0,0,0.0,yes,10/27/17,Google,-1,-1,8
996,996,996,996,996,996,996,996,ICLR,2018,Learning to Compute Word Embeddings On the Fly,Dzmitry Bahdanau;Tom Bosc;Stanisław Jastrzębski;Edward Grefenstette;Pascal Vincent;Yoshua Bengio,dimabgv@gmail.com;bosc.tom@gmail.com;staszek.jastrzebski@gmail.com;etg@google.com;pascal.vincent@umontreal.ca;yoshua.umontreal@gmail.com,5;7;5,4;3;4,Reject,0,1,0.0,yes,10/27/17,University of Montreal;University of Montreal;University of Montreal;Google;University of Montreal;University of Montreal,124;124;124;-1;124;124,108;108;108;-1;108;108,3
997,997,997,997,997,997,997,997,ICLR,2018,SIC-GAN: A Self-Improving Collaborative GAN for Decoding Sketch RNNs,Chi-Chun Chuang;Zheng-Xin Weng;Shan-Hung Wu,ccchuang@datalab.cs.nthu.edu.tw;zxweng@datalab.cs.nthu.edu.tw;shwu@cs.nthu.edu.tw,5;4;7,3;5;3,Reject,0,3,0.0,yes,10/27/17,National Tsing Hua University;National Tsing Hua University;National Tsing Hua University,181;181;181,323;323;323,5
998,998,998,998,998,998,998,998,ICLR,2018,On the Construction and Evaluation of Color Invariant Networks,Konrad Groh,konrad.groh@de.bosch.com,4;3;3,4;4;4,Reject,0,0,0.0,yes,10/27/17,Bosch,-1,-1,8
999,999,999,999,999,999,999,999,ICLR,2018,The Mutual Autoencoder: Controlling Information in Latent Code Representations,Mary Phuong;Max Welling;Nate Kushman;Ryota Tomioka;Sebastian Nowozin,bphuong@ist.ac.at;m.welling@uva.nl;nkushman@microsoft.com;ryoto@microsoft.com;sebastian.nowozin@microsoft.com,4;5;4,5;4;4,Reject,0,1,0.0,yes,10/27/17,Institute of Science and Technology Austria;University of Amsterdam;Microsoft;Microsoft;Microsoft,99;181;-1;-1;-1,1103;59;-1;-1;-1,5;1
1000,1000,1000,1000,1000,1000,1000,1000,ICLR,2018,Avoiding degradation in deep feed-forward networks by phasing out skip-connections,Ricardo Pio Monti;Sina Tootoonian;Robin Cao,r.monti@ucl.ac.uk;sina@gatsby.ucl.ac.uk;robin.cao@ucl.ac.uk,6;6;6,4;4;5,Reject,4,9,1.0,yes,10/27/17,University College London;University College London;University College London,46;46;46,16;16;16,
1001,1001,1001,1001,1001,1001,1001,1001,ICLR,2018,Capturing Human Category Representations by Sampling in Deep Feature Spaces,Joshua Peterson;Krishan Aghi;Jordan Suchow;Alexander Ku;Tom Griffiths,peterson.c.joshua@gmail.com;kaghi@berkeley.edu;suchow@berkeley.edu;alexku@berkeley.edu;tom_griffiths@berkeley.edu,6;5;5,4;5;4,Invite to Workshop Track,0,4,0.0,yes,10/27/17,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5;5,18;18;18;18;18,5;1;2
1002,1002,1002,1002,1002,1002,1002,1002,ICLR,2018,Deep Sensing: Active Sensing using Multi-directional Recurrent Neural Networks,Jinsung Yoon;William R. Zame;Mihaela van der Schaar,jsyoon0823@gmail.com;zame@econ.ucla.edu;mihaela.vanderschaar@oxford-man.ox.ac.uk,8;6;7,4;3;4,Accept (Poster),0,6,0.0,yes,10/27/17,"University of California, Los Angeles;University of California, Los Angeles;University of Oxford",20;20;51,15;15;1,
1003,1003,1003,1003,1003,1003,1003,1003,ICLR,2018,"Universality, Robustness, and Detectability of Adversarial Perturbations under Adversarial Training",Jan Hendrik Metzen,janhendrik.metzen@de.bosch.com,3;6;6,4;3;3,Reject,2,3,0.0,yes,10/27/17,Bosch,-1,-1,4
1004,1004,1004,1004,1004,1004,1004,1004,ICLR,2018,Autoregressive Generative Adversarial Networks,Yasin Yazici;Kim-Hui Yap;Stefan Winkler,yasin001@e.ntu.edu.sg;ekhyap@ntu.edu.sg;stefan.winkler@adsc.com.sg,5;3;5,4;5;5,Invite to Workshop Track,0,3,0.0,yes,10/27/17,National Taiwan University;National Taiwan University;Advanced Digital Sciences Center,85;85;-1,197;197;-1,5;4
1005,1005,1005,1005,1005,1005,1005,1005,ICLR,2018,"Learning to Select: Problem, Solution, and Applications",Heechang Ryu;Donghyun Kim;Hayong Shin,rhc93@kaist.ac.kr;dhk618@kaist.ac.kr;hyshin@kaist.ac.kr,4;4;4,4;4;5,Reject,0,0,0.0,yes,10/27/17,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,21;21;21,95;95;95,10
1006,1006,1006,1006,1006,1006,1006,1006,ICLR,2018,Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update,Su Young Lee;Sungik Choi;Sae-Young Chung,sy9424@kaist.ac.kr;si_choi@kaist.ac.kr;schung@kaist.ac.kr,4;6;5,4;4;4,Reject,0,15,0.0,yes,10/27/17,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,21;21;21,95;95;95,
1007,1007,1007,1007,1007,1007,1007,1007,ICLR,2018,Policy Gradient For Multidimensional Action Spaces: Action Sampling and Entropy Bonus,Vuong Ho Quan;Yiming Zhang;Kenny Song;Xiao-Yue Gong;Keith W. Ross,quan.hovuong@gmail.com;yiming.zhang@nyu.edu;kenny.song@nyu.edu;xygong@mit.edu;keithwross@nyu.edu,6;5;5,3;4;5,Reject,0,4,0.0,yes,10/27/17,New York University;New York University;New York University;Massachusetts Institute of Technology;New York University,26;26;26;2;26,27;27;27;5;27,
1008,1008,1008,1008,1008,1008,1008,1008,ICLR,2018,GENERATIVE LOW-SHOT NETWORK EXPANSION,Adi Hayat;Mark Kliger;Shachar Fleishman;Daniel Cohen-Or,adi.hayat3@gmail.com;mark.kliger@gmail.com;shacharfl@gmail.com;cohenor@gmail.com,6;4;4,4;3;4,Reject,0,3,0.0,yes,10/27/17,Tel Aviv University;;;,37;-1;-1;-1,217;-1;-1;-1,5
1009,1009,1009,1009,1009,1009,1009,1009,ICLR,2018,Cheap DNN Pruning with Performance Guarantees ,Konstantinos Pitas;Mike Davies;Pierre Vandergheynst,konstantinos.pitas@epfl.ch;mike.davies@ed.ac.uk;pierre.vandergheynst@epfl.ch,6;5;5,3;3;4,Reject,0,4,0.0,yes,10/27/17,Swiss Federal Institute of Technology Lausanne;University of Edinburgh;Swiss Federal Institute of Technology Lausanne,468;33;468,38;27;38,
1010,1010,1010,1010,1010,1010,1010,1010,ICLR,2018,Alternating Multi-bit Quantization for Recurrent Neural Networks,Chen Xu;Jianqiang Yao;Zhouchen Lin;Wenwu Ou;Yuanbin Cao;Zhirong Wang;Hongbin Zha,xuen@pku.edu.cn;tianduo@taobao.com;zlin@pku.edu.cn;santong.oww@taobao.com;lingzun.cyb@alibaba-inc.com;qingfeng@taobao.com;zha@cis.pku.edu.cn,8;7;7,4;4;2,Accept (Poster),0,12,0.0,yes,10/27/17,Peking University;Taobao;Peking University;Taobao;Alibaba Group;Taobao;Peking University,24;-1;24;-1;-1;-1;24,27;-1;27;-1;-1;-1;27,3
1011,1011,1011,1011,1011,1011,1011,1011,ICLR,2018,Learning to Generate Filters for Convolutional Neural Networks,Wei Shen;Rujie Liu,shenwei@cn.fujitsu.com;rjliu@cn.fujitsu.com,4;5;4,4;4;5,Reject,2,0,0.0,yes,10/27/17,Fujitsu Laboratories Ltd.;Fujitsu Laboratories Ltd.,-1;-1,-1;-1,
1012,1012,1012,1012,1012,1012,1012,1012,ICLR,2018,Theoretical properties of the global optimizer of two-layer Neural Network,Digvijay Boob;Guanghui Lan,digvijaybb40@gatech.edu;george.lan@isye.gatech.edu,4;7;7,5;5;4,Reject,4,6,0.0,yes,10/27/17,Georgia Institute of Technology;Georgia Institute of Technology,13;13,33;33,
1013,1013,1013,1013,1013,1013,1013,1013,ICLR,2018,Regularization for Deep Learning: A Taxonomy,Jan Kukačka;Vladimir Golkov;Daniel Cremers,jan.kukacka@tum.de;vladimir.golkov@tum.de;cremers@tum.de,5;4;4,5;4;5,Reject,0,4,0.0,yes,10/27/17,Technical University Munich;Technical University Munich;Technical University Munich,55;55;55,41;41;41,
1014,1014,1014,1014,1014,1014,1014,1014,ICLR,2018,Kernel Graph Convolutional Neural Nets,Giannis Nikolentzos;Polykarpos Meladianos;Antoine J-P Tixier;Konstantinos Skianis;Michalis Vazirgiannis,giannisnik@hotmail.com;pmeladianos@aueb.gr;antoine.tixier-1@colorado.edu;kskianis@lix.polytechnique.fr;mvazirg@lix.polytechnique.fr,5;5;4,5;4;5,Reject,0,0,0.0,yes,10/27/17,";;University of Colorado, Boulder;Ecole Polytechnique, France;Ecole Polytechnique, France",-1;-1;42;468;468,-1;-1;100;115;115,10
1015,1015,1015,1015,1015,1015,1015,1015,ICLR,2018,Improving Search Through A3C Reinforcement Learning Based Conversational Agent,Milan Aggarwal;Aarushi Arora;Shagun Sodhani;Balaji Krishnamurthy,milan.ag1994@gmail.com;aarushi.arora043@gmail.com;sshagunsodhani@gmail.com;kbalaji@adobe.com,5;2;3,4;5;5,Reject,0,7,0.0,yes,10/27/17,Indian Institute of Technology Delhi;;University of Montreal;Adobe Systems,-1;-1;124;-1,-1;-1;108;-1,
1016,1016,1016,1016,1016,1016,1016,1016,ICLR,2018,Now I Remember! Episodic Memory For Reinforcement Learning,Ricky Loynd;Matthew Hausknecht;Lihong Li;Li Deng,riloynd@microsoft.com;mahauskn@microsoft.com;lihongli.cs@gmail.com;l.deng@ieee.org,4;4;4,5;4;5,Reject,0,0,0.0,yes,10/27/17,Microsoft;Microsoft;Google;,-1;-1;-1;-1,-1;-1;-1;-1,
1017,1017,1017,1017,1017,1017,1017,1017,ICLR,2018,An Out-of-the-box Full-network Embedding for Convolutional Neural Networks,Dario Garcia-Gasulla;Armand Vilalta;Ferran Parés;Jonatan Moreno;Eduard Ayguadé;Jesús Labarta;Ulises Cortés;Toyotaro Suzumura,dario.garcia@bsc.es;armand.vilalta@bsc.es;ferran.pares@bsc.es;jonatan.moreno@bsc.es;eduard.ayguade@bsc.es;jesus.labarta@bsc.es;ia@cs.upc.edu;suzumurat@gmail.com,3;4;4,4;5;5,Reject,0,0,0.0,yes,10/27/17,Barcelona Supercomputing Center;Barcelona Supercomputing Center;Barcelona Supercomputing Center;Barcelona Supercomputing Center;Barcelona Supercomputing Center;Barcelona Supercomputing Center;Universitat Politècnica de Catalunya;,-1;-1;-1;-1;-1;-1;468;-1,-1;-1;-1;-1;-1;-1;1103;-1,6
1018,1018,1018,1018,1018,1018,1018,1018,ICLR,2018,Parameter Space Noise for Exploration,Matthias Plappert;Rein Houthooft;Prafulla Dhariwal;Szymon Sidor;Richard Y. Chen;Xi Chen;Tamim Asfour;Pieter Abbeel;Marcin Andrychowicz,matthiasplappert@me.com;rein.houthooft@openai.com;prafulla@openai.com;szymon@openai.com;richardchen@openai.com;peter@openai.com;asfour@kit.edu;pabbeel@cs.berkeley.edu;marcin@openai.com,6;7;7,4;4;5,Accept (Poster),0,8,0.0,yes,10/27/17,OpenAI;OpenAI;OpenAI;OpenAI;OpenAI;OpenAI;Karlsruhe Institute of Technology;University of California Berkeley;OpenAI,-1;-1;-1;-1;-1;-1;153;5;-1,-1;-1;-1;-1;-1;-1;133;18;-1,
1019,1019,1019,1019,1019,1019,1019,1019,ICLR,2018,Global Convergence of Policy Gradient Methods for Linearized  Control Problems,Maryam Fazel;Rong Ge;Sham M. Kakade;Mehran Mesbahi,mfazel@uw.edu;rongge@cs.duke.edu;sham@cs.washington.edu;mesbahi@aa.washington.edu,6;5;5,4;3;3,Reject,0,3,0.0,yes,10/27/17,"University of Washington, Seattle;Duke University;University of Washington;University of Washington",6;46;6;6,25;17;25;25,9
1020,1020,1020,1020,1020,1020,1020,1020,ICLR,2018,A Bayesian Nonparametric Topic Model with Variational Auto-Encoders,Xuefei Ning;Yin Zheng;Zhuxi Jiang;Yu Wang;Huazhong Yang;Junzhou Huang,foxdoraame@gmail.com;yzheng3xg@gmail.com;zjiang9310@gmail.com;yu-wang@mail.tsinghua.edu.cn;yanghz@tsinghua.edu.cn;joehhuang@tencent.com,7;3;5,4;4;2,Reject,0,8,0.0,yes,10/27/17,Tsinghua University;Tencent AI Lab;BIT;Tsinghua University;Tsinghua University;Tencent AI Lab,10;-1;-1;10;10;-1,30;-1;-1;30;30;-1,5;11
1021,1021,1021,1021,1021,1021,1021,1021,ICLR,2018,DropMax: Adaptive Stochastic Softmax,Hae Beom Lee;Juho Lee;Eunho Yang;Sung Ju Hwang,hblee@unist.ac.kr;stonecold@postech.ac.kr;yangeh@gmail.com;sjhwang82@gmail.com,6;6;4,3;4;3,Invite to Workshop Track,0,8,0.0,yes,10/27/17,Ulsan National Institute of Science and Technology;POSTECH;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,468;115;21;21,230;137;95;95,
1022,1022,1022,1022,1022,1022,1022,1022,ICLR,2018,Graph Partition Neural Networks for Semi-Supervised Classification,Renjie Liao;Marc Brockschmidt;Daniel Tarlow;Alexander Gaunt;Raquel Urtasun;Richard S. Zemel,rjliao@cs.toronto.edu;mabrocks@microsoft.com;dtarlow@google.com;algaunt@microsoft.com;urtasun@cs.toronto.edu;zemel@cs.toronto.edu,6;5;6,3;3;3,Invite to Workshop Track,0,3,0.0,yes,10/27/17,"Department of Computer Science, University of Toronto;Microsoft;Google;Microsoft;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",17;-1;-1;-1;17;17,22;-1;-1;-1;22;22,10
1023,1023,1023,1023,1023,1023,1023,1023,ICLR,2018,Semi-supervised Outlier Detection using Generative And Adversary Framework,Jindong Gu;Matthias Schubert;Volker Tresp,jindong.gu@siemens.com;schubert@dbs.ifi.lmu.de;volker.tresp@siemens.com,4;4;3,3;4;5,Reject,0,4,0.0,yes,10/27/17,Siemens Corporate Research;Institut für Informatik;Siemens Corporate Research,-1;-1;-1,-1;-1;-1,5;4
1024,1024,1024,1024,1024,1024,1024,1024,ICLR,2018,Deep Asymmetric Multi-task Feature Learning,Hae Beom Lee;Eunho Yang;Sung Ju Hwang,hblee@unist.ac.kr;yangeh@gmail.com;sjhwang@unist.ac.kr,6;3;5,4;4;4,Reject,0,0,0.0,yes,10/27/17,Ulsan National Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Ulsan National Institute of Science and Technology,468;21;468,230;95;230,
1025,1025,1025,1025,1025,1025,1025,1025,ICLR,2018,Cross-Corpus Training with TreeLSTM for the Extraction of Biomedical Relationships from Text,Legrand Joël;Yannick Toussaint;Chedy Raïssi;Adrien Coulet,joel.legrand@loria.fr;yannick.toussaint@loria.fr;chedy.raissi@inria.fr;adrien.coulet@loria.fr,4;5;3,4;4;5,Invite to Workshop Track,0,0,0.0,yes,10/27/17,University of Lorraine;University of Lorraine;INRIA;University of Lorraine,468;468;-1;468,535;535;-1;535,
1026,1026,1026,1026,1026,1026,1026,1026,ICLR,2018,Learning to navigate by distilling visual information and natural language instructions,Abhishek Sinha;Akilesh B;Mausoom Sarkar;Balaji Krishnamurthy,abhsinha@adobe.com;akb@adobe.com;msarkar@adobe.com;kbalaji@adobe.com,4;4;5,4;5;3,Reject,0,16,0.0,yes,10/27/17,Adobe Systems;Adobe Systems;Adobe Systems;Adobe Systems,-1;-1;-1;-1,-1;-1;-1;-1,3;6;8
1027,1027,1027,1027,1027,1027,1027,1027,ICLR,2018,Statestream: A toolbox to explore layerwise-parallel deep neural networks,Volker Fischer,volker.fischer@de.bosch.com,3;5;5,4;4;3,Reject,0,3,0.0,yes,10/27/17,Bosch,-1,-1,10
1028,1028,1028,1028,1028,1028,1028,1028,ICLR,2018,Beyond Finite Layer Neural Networks: Bridging Deep Architectures and Numerical Differential Equations,Yiping Lu;Aoxiao Zhong;Quanzheng Li;Bin Dong,luyiping9712@pku.edu.cn;zhongaoxiao@gmail.com;quanzhengli5@gmail.com;dongbin@math.pku.edu.cn,7;6;5;5,4;1;3;1,Invite to Workshop Track,0,4,0.0,yes,10/27/17,Peking University;Zhejiang University;;Peking University,24;124;-1;24,27;658;-1;27,8
1029,1029,1029,1029,1029,1029,1029,1029,ICLR,2018,Training RNNs as Fast as CNNs,Tao Lei;Yu Zhang;Yoav Artzi,tao@asapp.com;yzhang87@csail.mit.edu;yoav@cs.cornell.edu,7;8;4,4;5;5,Reject,8,10,0.0,yes,10/27/17,ASAPP Inc;Massachusetts Institute of Technology;Cornell University,-1;2;7,-1;5;19,3
1030,1030,1030,1030,1030,1030,1030,1030,ICLR,2018,Revisiting Bayes by Backprop,Meire Fortunato;Charles Blundell;Oriol Vinyals,meirefortunato@google.com;cblundell@google.com;vinyals@google.com,5;6;6,4;4;5,Reject,0,5,0.0,yes,10/27/17,Google;Google;Google,-1;-1;-1,-1;-1;-1,3;11
1031,1031,1031,1031,1031,1031,1031,1031,ICLR,2018,PARAMETRIZED DEEP Q-NETWORKS LEARNING: PLAYING ONLINE BATTLE ARENA WITH DISCRETE-CONTINUOUS HYBRID ACTION SPACE,Jiechao Xiong;Qing Wang;Zhuoran Yang;Peng Sun;Yang Zheng;Lei Han;Haobo Fu;Xiangru Lian;Carson Eisenach;Haichuan Yang;Emmanuel Ekwedike;Bei Peng;Haoyue Gao;Tong Zhang;Ji Liu;Han Liu,jcxiong@tencent.com;drwang@tencent.com;pythonsun@tencent.com;zakzheng@tencent.com;lxhan@tencent.com;haobofu@tencent.com;tongzhang@tongzhang-ml.org;ji.liu.uwisc@gmail.com,5;5;4,4;3;4,Reject,0,0,0.0,yes,10/27/17,Tencent AI Lab;Tencent AI Lab;Tencent AI Lab;Tencent AI Lab;Tencent AI Lab;Tencent AI Lab;;University of Rochester,-1;-1;-1;-1;-1;-1;-1;104,-1;-1;-1;-1;-1;-1;-1;153,
1032,1032,1032,1032,1032,1032,1032,1032,ICLR,2018,Towards Building Affect sensitive Word Distributions,Kushal Chawla;Sopan Khosla;Niyati Chhaya;Kokil Jaidka,kchawla@adobe.com;skhosla@adobe.com;nchhaya@adobe.com;jaidka@sas.upenn.edu,6;4;4,3;5;4,Reject,0,4,0.0,yes,10/27/17,Adobe Systems;Adobe Systems;Adobe Systems;University of Pennsylvania,-1;-1;-1;19,-1;-1;-1;10,3
1033,1033,1033,1033,1033,1033,1033,1033,ICLR,2018,Unsupervised Learning of Entailment-Vector Word Embeddings,James Henderson,james.henderson@idiap.ch,3;7;3,5;3;5,Reject,0,3,0.0,yes,10/27/17,Idiap Research Institute,-1,-1,3
1034,1034,1034,1034,1034,1034,1034,1034,ICLR,2018,Overcoming the vanishing gradient problem in plain recurrent networks,Yuhuang Hu;Adrian Huber;Shih-Chii Liu,yuhuang.hu@ini.uzh.ch;huberad@ini.uzh.ch;shih@ini.uzh.ch,4;2;7,5;4;4,Reject,0,5,0.0,yes,10/27/17,University of Zurich;University of Zurich;University of Zurich,139;139;139,136;136;136,
1035,1035,1035,1035,1035,1035,1035,1035,ICLR,2018,VoiceLoop: Voice Fitting and Synthesis via a Phonological Loop,Yaniv Taigman;Lior Wolf;Adam Polyak;Eliya Nachmani,yaniv@fb.com;wolf@fb.com;adampolyak@fb.com;enk100@gmail.com,8;5;6,4;4;4,Accept (Poster),1,6,0.0,yes,10/27/17,Facebook;Facebook;Facebook;Facebook,-1;-1;-1;-1,-1;-1;-1;-1,
1036,1036,1036,1036,1036,1036,1036,1036,ICLR,2018,Graph Topological Features via GAN,Weiyi Liu;Hal Cooper;Min-Hwan Oh,weiyiliu@us.ibm.com;hal.cooper@columbia.edu;m.oh@columbia.edu,3;4;4,4;4;5,Reject,0,0,0.0,yes,10/27/17,International Business Machines;Columbia University;Columbia University,-1;15;15,-1;14;14,5;4;10
1037,1037,1037,1037,1037,1037,1037,1037,ICLR,2018,Improve Training Stability of Semi-supervised Generative Adversarial Networks with Collaborative Training,Dalei Wu;Xiaohua Liu,daleiwu@gmail.com;dalei.wu@huawei.com;liuxh3@huawei.com,3;2;3,4;4;5,Reject,0,0,0.0,yes,10/27/17,;Huawei Technologies Ltd.;Huawei Technologies Ltd.,-1;-1;-1,-1;-1;-1,5;4
1038,1038,1038,1038,1038,1038,1038,1038,ICLR,2018,Disentangled activations in deep networks,Mikael Kågebäck;Olof Mogren,kageback@chalmers.se;olof@mogren.one,6;5;4,3;4;3,Reject,0,3,0.0,yes,10/27/17,Chalmers University;Chalmers University,153;153,240;240,
1039,1039,1039,1039,1039,1039,1039,1039,ICLR,2018,On the State of the Art of Evaluation in Neural Language Models,Gábor Melis;Chris Dyer;Phil Blunsom,melisgl@google.com;cdyer@cs.cmu.edu;phil.blunsom@cs.ox.ac.uk,7;5;8,2;5;3,Accept (Poster),0,16,0.0,yes,10/27/17,Google;Carnegie Mellon University;University of Oxford,-1;1;51,-1;24;1,3
1040,1040,1040,1040,1040,1040,1040,1040,ICLR,2018,Interpretable Classification via Supervised Variational Autoencoders and Differentiable Decision Trees,Eleanor Quint;Garrett Wirka;Jacob Williams;Stephen Scott;N.V. Vinodchandran,pquint@cse.unl.edu;gwirka@cse.unl.edu;jwilliam@cse.unl.edu;sscott@cse.unl.edu;vinod@cse.unl.edu,3;4;5,5;4;4,Reject,0,2,0.0,yes,10/27/17,"University of Nebraska, Lincoln;University of Nebraska, Lincoln;University of Nebraska, Lincoln;University of Nebraska, Lincoln;University of Nebraska, Lincoln",244;244;244;244;244,337;337;337;337;337,5
1041,1041,1041,1041,1041,1041,1041,1041,ICLR,2018,Zero-shot Cross Language Text Classification,Dan Svenstrup;Jonas Meinertz Hansen;Ole Winther,dsve@dtu.dk;jonas@meinertz.org;olwi@dtu.dk,4;2;3,3;4;4,Reject,0,0,0.0,yes,10/27/17,Technical University of Denmark;;Technical University of Denmark,210;-1;210,153;-1;153,
1042,1042,1042,1042,1042,1042,1042,1042,ICLR,2018,Long Term Memory Network for Combinatorial Optimization Problems,Hazem A. A. Nomer;Abdallah Aboutahoun;Ashraf Elsayed,hazemahmed@alexu.edu.eg;abdallah_aboutahoun@alexu.edu.eg;ashraf.elsayed@alexu.edu.eg,4;4;3,1;2;4,Reject,3,0,0.0,yes,10/27/17,Alexandria University;Alexandria University;Alexandria University,468;468;468,896;896;896,
1043,1043,1043,1043,1043,1043,1043,1043,ICLR,2018,Autoregressive Convolutional Neural Networks for Asynchronous Time Series,Mikolaj Binkowski;Gautier Marti;Philippe Donnat,mikbinkowski@gmail.com;gautier.marti@gmail.com;pdonnat@helleborecapital.com,4;5;5,5;3;4,Reject,0,1,0.0,yes,10/27/17,Imperial College London;;Hellebore Capital Ltd.,74;-1;-1,8;-1;-1,
1044,1044,1044,1044,1044,1044,1044,1044,ICLR,2018,Towards Unsupervised Classification with Deep Generative Models,Dimitris Kalatzis;Konstantia Kotta;Ilias Kalamaras;Anastasios Vafeiadis;Andrew Rawstron;Dimitris Tzovaras;Kostas Stamatopoulos,dkal@iti.gr;ntina_kotta@yahoo.com;kalamar@iti.gr;anasvaf@iti.gr;a.c.rawstron@leeds.ac.uk;dimitrios.tzovaras@iti.gr;kostas.stamatopoulos@gmail.com,4;4;4,4;4;5,Reject,0,1,0.0,yes,10/27/17,CERTH/ITI;;CERTH/ITI;CERTH/ITI;University of Leeds;CERTH/ITI;,-1;-1;-1;-1;244;-1;-1,-1;-1;-1;-1;139;-1;-1,5
1045,1045,1045,1045,1045,1045,1045,1045,ICLR,2018,Topic-Based Question Generation,Wenpeng Hu;Bing Liu;Rui Yan;Dongyan Zhao;Jinwen Ma,wenpeng.hu@pku.edu.cn;liub@cs.uic.edu;ruiyan@pku.edu.cn;zhaody@pku.edu.cn;jwma@math.pku.edu.cn,3;4;8,5;4;3,Invite to Workshop Track,0,3,0.0,yes,10/27/17,"Peking University;University of Illinois, Chicago;Peking University;Peking University;Peking University",24;57;24;24;24,27;255;27;27;27,
1046,1046,1046,1046,1046,1046,1046,1046,ICLR,2018,Variance Regularized Counterfactual Risk Minimization via Variational Divergence Minimization,Hang Wu,hwu340@gatech.edu;hangwu@gatech.edu,4;5;7,4;5;3,Reject,0,6,0.0,yes,10/27/17,Georgia Institute of Technology;Georgia Institute of Technology,13;13,33;33,8
1047,1047,1047,1047,1047,1047,1047,1047,ICLR,2018,Lifelong Learning with Output Kernels,Keerthiram Murugesan;Jaime Carbonell,kmuruges@cs.cmu.edu;jgc@cs.cmu.edu,3;2;4,4;5;4,Reject,0,4,0.0,yes,10/27/17,Carnegie Mellon University;Carnegie Mellon University,1;1,24;24,1
1048,1048,1048,1048,1048,1048,1048,1048,ICLR,2018,Deep Continuous Clustering,Sohil Atul Shah;Vladlen Koltun,sohilas@umd.edu;vkoltun@gmail.com,6;3;7,3;5;4,Reject,0,10,0.0,yes,10/27/17,"University of Maryland, College Park;Intel",12;-1,69;-1,
1049,1049,1049,1049,1049,1049,1049,1049,ICLR,2018,Label Embedding Network: Learning Label Representation for Soft Training of Deep Networks,Xu Sun;Bingzhen Wei;Xuancheng Ren;Shuming Ma,xusun@pku.edu.cn;weibz@pku.edu.cn;renxc@pku.edu.cn;shumingma@pku.edu.cn,4;4;3,5;3;4,Reject,0,1,0.0,yes,10/27/17,Peking University;Peking University;Peking University;Peking University,24;24;24;24,27;27;27;27,
1050,1050,1050,1050,1050,1050,1050,1050,ICLR,2018,Semantic Code Repair using Neuro-Symbolic Transformation Networks,Jacob Devlin;Jonathan  Uesato;Rishabh Singh;Pushmeet Kohli,jacobdevlin@google.com;juesato@gmail.com;risin@microsoft.com;pushmeet@google.com,4;6;6,4;4;4,Invite to Workshop Track,0,4,0.0,yes,10/27/17,Google;;Microsoft;Google,-1;-1;-1;-1,-1;-1;-1;-1,
1051,1051,1051,1051,1051,1051,1051,1051,ICLR,2018,Simple and efficient architecture search for Convolutional Neural Networks,Thomas Elsken;Jan Hendrik Metzen;Frank Hutter,thomas.elsken@de.bosch.com;janhendrik.metzen@de.bosch.com;fh@cs.uni-freiburg.de,6;5;4,4;5;4,Invite to Workshop Track,0,4,0.0,yes,10/27/17,Bosch;Bosch;Universität Freiburg,-1;-1;115,-1;-1;82,
1052,1052,1052,1052,1052,1052,1052,1052,ICLR,2018,Natural Language Inference with External Knowledge,Qian Chen;Xiaodan Zhu;Zhen-Hua Ling;Diana Inkpen,cq1231@mail.ustc.edu.cn;xiaodan.zhu@queensu.ca;zhling@ustc.edu.cn;diana.inkpen@uottawa.ca,6;5;3;7,5;4;5;4,Invite to Workshop Track,3,4,0.0,yes,10/27/17,University of Science and Technology of China;Queens University;University of Science and Technology of China;University of Ottawa,115;244;115;291,132;261;132;233,3
1053,1053,1053,1053,1053,1053,1053,1053,ICLR,2018,AirNet: a machine learning dataset for air quality forecasting,Songgang Zhao;Xingyuan Yuan;Da Xiao;Jianyuan Zhang;Zhouyuan Li,gfgkmn@gmail.com;yuan@caiyunapp.com;xiaoda99@gmail.com;littletree@caiyunapp.com;joeyzhouyuanli@caiyunapp.com,5;4;4,4;4;4,Reject,0,4,0.0,yes,10/27/17,;ColorfulClouds Tech.;Beijing University of Post and Telecommunication;ColorfulClouds Tech.;ColorfulClouds Tech.,-1;-1;468;-1;-1,-1;-1;1103;-1;-1,
1054,1054,1054,1054,1054,1054,1054,1054,ICLR,2018,Recursive Binary Neural Network Learning Model  with 2-bit/weight Storage Requirement,Tianchan Guan;Xiaoyang Zeng;Mingoo Seok,tg2569@columbia.edu;xyzeng@fudan.edu;ms4415@columbia.edu,6;7;5,3;4;3,Reject,0,3,0.0,yes,10/27/17,Columbia University;Fudan University;Columbia University,15;78;15,14;116;14,
1055,1055,1055,1055,1055,1055,1055,1055,ICLR,2018,Learning Dynamic State Abstractions for Model-Based Reinforcement Learning,Lars Buesing;Theophane Weber;Sebastien Racaniere;S. M. Ali Eslami;Danilo Rezende;David Reichert;Fabio Viola;Frederic Besse;Karol Gregor;Demis Hassabis;Daan Wierstra,lbuesing@google.com;theophane@google.com;sracaniere@google.com;aeslami@google.com;danilor@google.com;reichert@google.com;fviola@google.com;fbesse@google.com;demishassabis@google.com;wierstra@google.com,6;5;8,4;4;4,Reject,0,6,1.0,yes,10/27/17,Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,
1056,1056,1056,1056,1056,1056,1056,1056,ICLR,2018,Towards Effective GANs for Data Distributions with Diverse Modes,Sanchit Agrawal;Gurneet Singh;Mitesh Khapra,sanchit@cse.iitm.ac.in;garry@cse.iitm.ac.in;miteshk@cse.iitm.ac.in,6;4;4,5;3;3,Invite to Workshop Track,0,3,0.0,yes,10/27/17,Indian Institute of Technology Madras;Indian Institute of Technology Madras;Indian Institute of Technology Madras,153;153;153,625;625;625,5;4
1057,1057,1057,1057,1057,1057,1057,1057,ICLR,2018,Unsupervised Adversarial Anomaly  Detection using One-Class Support Vector Machines,Prameesha Sandamal Weerasinghe;Tansu Alpcan;Sarah Monazam Erfani;Christopher Leckie,pweerasinghe@student.unimelb.edu.au;tansu.alpcan@unimelb.edu.au;sarah.erfani@unimelb.edu.au;caleckie@unimelb.edu.au,4;4;4,4;4;4,Reject,0,0,0.0,yes,10/27/17,The University of Melbourne;The University of Melbourne;The University of Melbourne;The University of Melbourne,124;124;124;124,32;32;32;32,4
1058,1058,1058,1058,1058,1058,1058,1058,ICLR,2018,Estimation of cross-lingual news similarities using text-mining methods,Zhouhao Wang;Enda Liu;Hiroki Sakaji;Tomoki Ito;Kiyoshi Izumi;Kota Tsubouchi;Tatsuo Yamashita,wangzhouhao94@gmail.com;m2015eliu@socsim.org;sakaji@sys.t.u-tokyo.ac.jp;m2015titoh@socsim.org;izumi@sys.t.u-tokyo.ac.jp;ktsubouc@yahoo-corp.jp;tayamash@yahoo-corp.jp,2;6;2,5;4;4,Reject,0,0,0.0,yes,10/27/17,The University of Tokyo;;The University of Tokyo;;The University of Tokyo;;,52;-1;52;-1;52;-1;-1,45;-1;45;-1;45;-1;-1,
1059,1059,1059,1059,1059,1059,1059,1059,ICLR,2018,Data-driven Feature Sampling for Deep Hyperspectral Classification and Segmentation,William M. Severa;Jerilyn A. Timlin;Suraj Kholwadwala;Conrad D. James;James B. Aimone,wmsever@sandia.gov;jatimli@sandia.gov;skholwadwala@gmail.com;cdjame@sandia.gov;jbaimon@sandia.gov,3;6;4,5;5;5,Reject,0,0,0.0,yes,10/27/17,Sandia National Laboratories;Sandia National Laboratories;;Sandia National Laboratories;Sandia National Laboratories,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,2
1060,1060,1060,1060,1060,1060,1060,1060,ICLR,2018,Grouping-By-ID: Guarding Against Adversarial Domain Shifts,Christina Heinze-Deml;Nicolai Meinshausen,heinzedeml@stat.math.ethz.ch;meinshausen@stat.math.ethz.ch,7;4;5,3;5;4,Reject,0,4,0.0,yes,10/27/17,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,9;9,10;10,4;6;7;10
1061,1061,1061,1061,1061,1061,1061,1061,ICLR,2018,"MACH: Embarrassingly parallel $K$-class classification in $O(d\log{K})$ memory and $O(K\log{K} + d\log{K})$ time, instead of $O(Kd)$",Qixuan Huang;Anshumali Shrivastava;Yiqiu Wang,qh5@rice.edu;anshumali@rice.edu;yiqiu.wang@rice.edu,6;6;6,4;4;4,Reject,0,7,0.0,yes,10/27/17,Rice University;Rice University;Rice University,85;85;85,86;86;86,
1062,1062,1062,1062,1062,1062,1062,1062,ICLR,2018,SHADE: SHAnnon DEcay Information-Based Regularization for Deep Learning,Michael Blot;Thomas Robert;Nicolas Thome;Matthieu Cord,michael.blot@lip6.fr;thomas.robert@lip6.fr;nicolas.thome@lip6.fr;matthieu.cord@lip6.fr,4;5;7;4,3;4;3;3,Reject,0,7,0.0,yes,10/27/17,LIP6;LIP6;LIP6;LIP6,-1;-1;-1;-1,-1;-1;-1;-1,
1063,1063,1063,1063,1063,1063,1063,1063,ICLR,2018,Lifelong Generative Modeling,Jason Ramapuram;Magda Gregorova;Alexandros Kalousis,jason.ramapuram@etu.unige.ch;magda.gregorova@unige.ch;alexandros.kalousis@hesge.ch,9;4;4,5;5;2,Reject,0,4,0.0,yes,10/27/17,"University of Geneva, Switzerland;University of Geneva, Switzerland;Geneva School of Business Administration, HES-SO University of Applied Sciences of Western Switzerland;",468;468;468;-1,130;130;1103;-1,5;6
1064,1064,1064,1064,1064,1064,1064,1064,ICLR,2018,A Self-Training Method for Semi-Supervised GANs,Alan Do-Omri;Dalei Wu;Xiaohua Liu,alan.do-omri@mail.mcgill.ca;daleiwu@gmail.com;liuxiaohua3@huawei.com,3;4;3,5;4;4,Reject,0,4,0.0,yes,10/27/17,McGill University;;Huawei Technologies Ltd.,81;-1;-1,42;-1;-1,5;4
1065,1065,1065,1065,1065,1065,1065,1065,ICLR,2018,Neighbor-encoder,Chin-Chia Michael Yeh;Yan Zhu;Evangelos E. Papalexakis;Abdullah Mueen;Eamonn Keogh,myeh003@ucr.edu;yzhu015@ucr.edu;epapalex@cs.ucr.edu;mueen@unm.edu;eamonn@cs.ucr.edu,5;6;4,5;4;4,Reject,0,3,2.0,yes,10/27/17,"University of California, Riverside;University of California, Riverside;University of California, Riverside;University of New Mexico;University of California, Riverside",62;62;62;210;62,197;197;197;1103;197,8
1066,1066,1066,1066,1066,1066,1066,1066,ICLR,2018,Benefits of Depth for Long-Term Memory of Recurrent Networks,Yoav Levine;Or Sharir;Amnon Shashua,yoavlevine@cs.huji.ac.il;or.sharir@cs.huji.ac.il;shashua@cs.huji.ac.il,5;7;6,2;3;3,Invite to Workshop Track,0,6,0.0,yes,10/27/17,Hebrew University of Jerusalem;Hebrew University of Jerusalem;Hebrew University of Jerusalem,62;62;62,205;205;205,1
1067,1067,1067,1067,1067,1067,1067,1067,ICLR,2018,A Neural Method for Goal-Oriented Dialog Systems to interact with Named Entities,Janarthanan Rajendran;Jatin Ganhotra;Xiaoxiao Guo;Mo Yu;Satinder Singh,rjana@umich.edu;jatinganhotra@us.ibm.com;xiaoxiao.guo@ibm.com;yum@us.ibm.com;baveja@umich.edu,6;4;3,3;3;3,Reject,0,4,0.0,yes,10/27/17,University of Michigan;International Business Machines;International Business Machines;International Business Machines;University of Michigan,8;-1;-1;-1;8,21;-1;-1;-1;21,
1068,1068,1068,1068,1068,1068,1068,1068,ICLR,2018,Loss Functions for Multiset Prediction,Sean Welleck;Zixin Yao;Yu Gai;Jialin Mao;Zheng Zhang;Kyunghyun Cho,wellecks@nyu.edu;zy566@nyu.edu;yg1246@nyu.edu;jm5830@nyu.edu;zz@nyu.edu;kyunghyun.cho@nyu.edu,5;7;4,4;3;3,Reject,0,3,0.0,yes,10/27/17,New York University;New York University;New York University;New York University;New York University;New York University,26;26;26;26;26;26,27;27;27;27;27;27,
1069,1069,1069,1069,1069,1069,1069,1069,ICLR,2018,Improving the Universality and Learnability of Neural Programmer-Interpreters with Combinator Abstraction,Da Xiao;Jo-Yu Liao;Xingyuan Yuan,xiaoda99@gmail.com;liaoruoyu@caiyunapp.com;yuan@caiyunapp.com,3;7;7,4;4;4,Accept (Poster),0,10,0.0,yes,10/27/17,Beijing University of Post and Telecommunication;ColorfulClouds Tech.;ColorfulClouds Tech.,468;-1;-1,1103;-1;-1,
1070,1070,1070,1070,1070,1070,1070,1070,ICLR,2018,Efficiently applying attention to sequential data with the Recurrent Discounted Attention unit,Brendan Maginnis;Pierre Richemond,brendan.maginnis@gmail.com;pierre.richemond@gmail.com,4;6;3,5;4;4,Reject,0,0,0.0,yes,10/27/17,Imperial College London;Imperial College London,74;74,8;8,
1071,1071,1071,1071,1071,1071,1071,1071,ICLR,2018,Comparison of Paragram and GloVe Results for Similarity Benchmarks,Jakub Dutkiewicz;Czesław Jędrzejek,jakub.dutkiewicz@put.poznan.pl;czeslaw.jedrzejek@put.poznan.pl,2;4;3,4;5;4,Reject,0,1,0.0,yes,10/27/17,Poznan University of Technology;Poznan University of Technology,468;468,1103;1103,3
1072,1072,1072,1072,1072,1072,1072,1072,ICLR,2018,Domain Adaptation for Deep Reinforcement Learning in Visually Distinct Games,Dino S. Ratcliffe;Luca Citi;Sam Devlin;Udo Kruschwitz,d.ratcliffe@qmul.ac.uk;lciti@essex.ac.uk;sam.devlin@york.ac.uk;udo@essex.ac.uk,3;2;4,3;4;5,Reject,0,0,0.0,yes,10/27/17,Queen Mary University London;University of Sussex;University of York;University of Sussex,244;244;210;244,121;146;137;146,10
1073,1073,1073,1073,1073,1073,1073,1073,ICLR,2018,Flexible Prior Distributions for Deep Generative Models,Yannic Kilcher;Aurelien Lucchi;Thomas Hofmann,yannic.kilcher@inf.ethz.ch;aurelien.lucchi@inf.ethz.ch;thomas.hofmann@inf.ethz.ch,6;6;5,4;3;4,Reject,0,3,0.0,yes,10/27/17,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,9;9;9,10;10;10,5;8
1074,1074,1074,1074,1074,1074,1074,1074,ICLR,2018,Ego-CNN: An Ego Network-based Representation of Graphs Detecting Critical Structures,Ruo-Chun Tzeng;Shan-Hung Wu,rctzeng@datalab.cs.nthu.edu.tw;shwu@cs.nthu.edu.tw,7;4;4,3;4;4,Reject,1,4,0.0,yes,10/27/17,National Tsing Hua University;National Tsing Hua University,181;181,323;323,10
1075,1075,1075,1075,1075,1075,1075,1075,ICLR,2018,Lifelong Learning by Adjusting Priors,Ron Amit;Ron Meir,ronamit@campus.technion.ac.il;rmeir@ee.technion.ac.il,6;6;6,4;4;4,Reject,0,9,0.0,yes,10/27/17,Technion;Technion,24;24,327;327,6;8
1076,1076,1076,1076,1076,1076,1076,1076,ICLR,2018,A Simple Fully Connected Network for Composing Word Embeddings from Characters,Michael Traynor;Thomas Trappenberg,mike.sk.traynor@gmail.com;trappenberg@gmail.com,3;5;4,4;4;5,Reject,0,0,0.0,yes,10/27/17,Dalhousie University;,291;-1,289;-1,3
1077,1077,1077,1077,1077,1077,1077,1077,ICLR,2018,"Dissecting Adam: The Sign, Magnitude and Variance of Stochastic Gradients",Lukas Balles;Philipp Hennig,lukas.balles@tuebingen.mpg.de;ph@tue.mpg.de,4;4;6,4;4;3,Reject,1,6,0.0,yes,10/27/17,"Max Planck Institute for Intelligent Systems, Max-Planck Institute;Max Planck Institute for Intelligent Systems, Max-Planck Institute",-1;-1,-1;-1,
1078,1078,1078,1078,1078,1078,1078,1078,ICLR,2018,Censoring Representations with Multiple-Adversaries over Random Subspaces,Yusuke Iwasawa;Kotaro Nakayama;Yutaka Matsuo,iwasawa@weblab.t.u-tokyo.ac.jp;nakayama@weblab.t.u-tokyo.ac.jp;matsuo@weblab.t.u-tokyo.ac.jp,6;5;6,3;4;4,Reject,0,5,0.0,yes,10/27/17,The University of Tokyo;The University of Tokyo;The University of Tokyo,52;52;52,45;45;45,4
1079,1079,1079,1079,1079,1079,1079,1079,ICLR,2018,Evolutionary Expectation Maximization for Generative Models with Binary Latents,Enrico Guiraud;Jakob Drefs;Joerg Luecke,enrico.guiraud@cern.ch;jakob.heinrich.drefs@uni-oldenburg.de;joerg.luecke@uni-oldenburg.de,4;4;4,4;4;4,Reject,0,6,0.0,yes,10/27/17,CERN;University of Oldenburg;University of Oldenburg,-1;364;364,-1;1103;1103,5;1
1080,1080,1080,1080,1080,1080,1080,1080,ICLR,2018,Learnability of Learned Neural Networks,Rahul Anand Sharma;Navin Goyal;Monojit Choudhury;Praneeth Netrapalli,t-rahsha@microsoft.com;navingo@microsoft.com;monojitc@microsoft.com;praneeth@microsoft.com,7;6;4,4;4;4,Reject,0,6,0.0,yes,10/27/17,Microsoft;Microsoft;Microsoft;Microsoft,-1;-1;-1;-1,-1;-1;-1;-1,8
1081,1081,1081,1081,1081,1081,1081,1081,ICLR,2018,Compact Neural Networks based on the Multiscale Entanglement Renormalization Ansatz,Andrew Hallam;Edward Grant;Vid Stojevic;Simone Severini;Andrew G. Green,andrew.hallam.10@ucl.ac.uk;edward.grant.16@ucl.ac.uk;vstojevic@gtn.ai;s.severini@ucl.ac.uk;andrew.green@ucl.ac.uk,5;5;4,3;4;4,Reject,0,3,0.0,yes,10/27/17,University College London;University College London;;University College London;University College London,46;46;-1;46;46,16;16;-1;16;16,
1082,1082,1082,1082,1082,1082,1082,1082,ICLR,2018,Online Hyper-Parameter Optimization,Damien Vincent;Sylvain Gelly;Nicolas Le Roux;Olivier Bousquet,damienv@google.com;sylvain.gelly@gmail.com;nicolas@le-roux.name;obousquet@gmail.com,4;5;4,3;3;3,Reject,0,1,0.0,yes,10/27/17,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,
1083,1083,1083,1083,1083,1083,1083,1083,ICLR,2018,Learning to Teach,Yang Fan;Fei Tian;Tao Qin;Xiang-Yang Li;Tie-Yan Liu,fyabc@mail.ustc.edu.cn;fetia@microsoft.com;taoqin@microsoft.com;tyliu@microsoft.com,8;9;5,4;3;4,Accept (Poster),0,10,0.0,yes,10/27/17,University of Science and Technology of China;Microsoft;Microsoft;Microsoft,115;-1;-1;-1,132;-1;-1;-1,
1084,1084,1084,1084,1084,1084,1084,1084,ICLR,2018,Word2net: Deep Representations of Language,Maja Rudolph;Francisco Ruiz;David Blei,marirudolph@gmail.com;f.ruiz@columbia.edu;david.blei@columbia.edu,5;4;4,5;4;4,Reject,0,3,0.0,yes,10/27/17,Columbia University;Columbia University;Columbia University,15;15;15,14;14;14,3
1085,1085,1085,1085,1085,1085,1085,1085,ICLR,2018,The loss surface and expressivity of deep convolutional neural networks,Quynh Nguyen;Matthias Hein,quynh@cs.uni-saarland.de;hein@cs.uni-saarland.de,4;7;5;6,4;2;2;3,Invite to Workshop Track,0,4,0.0,yes,10/27/17,Saarland University;Saarland University,90;90,1103;1103,
1086,1086,1086,1086,1086,1086,1086,1086,ICLR,2018,Combination of Supervised and Reinforcement Learning For Vision-Based Autonomous Control,Dmitry Kangin;Nicolas Pugeault,d.kangin@exeter.ac.uk;n.pugeault@exeter.ac.uk,4;5;3,5;3;4,Reject,0,4,0.0,yes,10/27/17,University of Exeter;University of Exeter,364;364,130;130,
1087,1087,1087,1087,1087,1087,1087,1087,ICLR,2018,Gating out sensory noise in a spike-based Long Short-Term Memory network,Davide Zambrano;Isabella Pozzi;Roeland Nusselder;Sander Bohte,d.zambrano@cwi.nl;isabella.pozzi@cwi.nl;roeland.nusselder@gmail.com;s.m.bohte@cwi.nl,5;5;4,4;3;4,Reject,0,0,0.0,yes,10/27/17,Centrum voor Wiskunde en Informatica;Centrum voor Wiskunde en Informatica;;Centrum voor Wiskunde en Informatica,-1;-1;-1;-1,-1;-1;-1;-1,
1088,1088,1088,1088,1088,1088,1088,1088,ICLR,2018,Learning to Infer Graphics Programs from Hand-Drawn Images,Kevin Ellis;Daniel Ritchie;Armando Solar-Lezama;Joshua B. Tenenbaum,ellisk@mit.edu;daniel_richie@brown.edu;asolar@csail.mit.edu;jbt@mit.edu,4;6;4,4;4;2,Reject,0,4,0.0,yes,10/27/17,Massachusetts Institute of Technology;Brown University;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;62;2;2,5;50;5;5,10
1089,1089,1089,1089,1089,1089,1089,1089,ICLR,2018,Federated Learning: Strategies for Improving Communication Efficiency,Jakub Konečný;H. Brendan McMahan;Felix X. Yu;Ananda Theertha Suresh;Dave Bacon;Peter Richtárik,konkey@google.com;mcmahan@google.com;felixyu@google.com;theertha@google.com;dabacon@google.com;peter.richtarik@kaust.edu.sa,5;7;5,3;5;5,Reject,0,4,0.0,yes,10/27/17,Google;Google;Google;Google;Google;KAUST,-1;-1;-1;-1;-1;124,-1;-1;-1;-1;-1;1103,
1090,1090,1090,1090,1090,1090,1090,1090,ICLR,2018,Convolutional Sequence Modeling Revisited,Shaojie Bai;J. Zico Kolter;Vladlen Koltun,shaojieb@cs.cmu.edu;zkolter@cs.cmu.edu;vkoltun@gmail.com,8;5;4,4;4;3,Invite to Workshop Track,1,9,4.0,yes,10/27/17,Carnegie Mellon University;Carnegie Mellon University;Intel,1;1;-1,24;24;-1,
1091,1091,1091,1091,1091,1091,1091,1091,ICLR,2018,Searching for Activation Functions,Prajit Ramachandran;Barret Zoph;Quoc V. Le,prajitram@gmail.com;barretzoph@google.com;qvl@google.com,4;5;7,4;5;5,Invite to Workshop Track,5,8,0.0,yes,10/27/17,"University of Illinois, Urbana Champaign;Google;Google",3;-1;-1,37;-1;-1,
1092,1092,1092,1092,1092,1092,1092,1092,ICLR,2018,On Convergence and Stability of GANs,Naveen Kodali;James Hays;Jacob Abernethy;Zsolt Kira,nkodali3@gatech.edu;hays@gatech.edu;prof@gatech.edu;zkira@gatech.edu,5;4;3,2;5;3,Reject,17,6,0.0,yes,10/27/17,Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology,13;13;13;13,33;33;33;33,5
1093,1093,1093,1093,1093,1093,1093,1093,ICLR,2018,Training Autoencoders by Alternating Minimization,Sneha Kudugunta;Adepu Shankar;Surya Chavali;Vineeth Balasubramanian;Purushottam Kar,cs14btech11020@iith.ac.in;cs14resch11001@iith.ac.in;cs13b1028@iith.ac.in;vineethnb@iith.ac.in;purushot@cse.iitk.ac.in,6;4;7,4;4;5,Reject,0,3,0.0,yes,10/27/17,Indian Institute of Technology Hyderabad;Indian Institute of Technology Hyderabad;Indian Institute of Technology Hyderabad;Indian Institute of Technology Hyderabad;IIT Kanpur,210;210;210;210;139,1103;1103;1103;1103;578,9
1094,1094,1094,1094,1094,1094,1094,1094,ICLR,2018,Learning temporal evolution of probability distribution with Recurrent Neural Network,Kyongmin Yeo;Igor Melnyk;Nam Nguyen;Eun Kyung Lee,kyeo@us.ibm.com;igor.melnyk@ibm.com;nnguyen@us.ibm.com;eunkyung.lee@us.ibm.com,6;5;6,2;4;4,Reject,0,3,0.0,yes,10/27/17,International Business Machines;International Business Machines;International Business Machines;International Business Machines,-1;-1;-1;-1,-1;-1;-1;-1,
1095,1095,1095,1095,1095,1095,1095,1095,ICLR,2018,LEARNING TO ORGANIZE KNOWLEDGE WITH N-GRAM MACHINES,Fan Yang;Jiazhong Nie;William W. Cohen;Ni Lao,fanyang1@cs.cmu.edu;niejiazhong@google.com;wcohen@cs.cmu.edu;nlao@google.com,4;5;4,4;4;3,Invite to Workshop Track,0,3,0.0,yes,10/27/17,Carnegie Mellon University;Google;Carnegie Mellon University;Google,1;-1;1;-1,24;-1;24;-1,3
1096,1096,1096,1096,1096,1096,1096,1096,ICLR,2018,Stabilizing GAN Training with Multiple Random Projections,Behnam Neyshabur;Srinadh Bhojanapalli;Ayan Chakrabarti,bneyshabur@ttic.edu;srinadh@ttic.edu;ayan@wustl.edu,5;3;8,4;5;4,Reject,0,4,0.0,yes,10/27/17,"Toyota Technological Institute at Chicago;Toyota Technological Institute at Chicago;Washington University, St. Louis",-1;-1;104,-1;-1;50,5;4
1097,1097,1097,1097,1097,1097,1097,1097,ICLR,2018,Meta-Learning and Universality: Deep Representations and Gradient Descent can Approximate any Learning Algorithm,Chelsea Finn;Sergey Levine,cbfinn@eecs.berkeley.edu;svlevine@eecs.berkeley.edu,6;6;7,3;1;1,Accept (Poster),0,6,0.0,yes,10/27/17,University of California Berkeley;University of California Berkeley,5;5,18;18,6
1098,1098,1098,1098,1098,1098,1098,1098,ICLR,2018,TCAV: Relative concept importance testing with Linear Concept Activation Vectors,Been Kim;Justin Gilmer;Martin Wattenberg;Fernanda Viégas,beenkim@google.com;viegas@google.com;wattenberg@google.com;gilmer@google.com,4;4;5;3,4;3;2;5,Reject,0,6,0.0,yes,10/27/17,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,7
1099,1099,1099,1099,1099,1099,1099,1099,ICLR,2018,Shifting Mean Activation Towards Zero with Bipolar Activation Functions,Lars Hiller Eidnes;Arild Nøkland,larseidnes@gmail.com;arild.nokland@gmail.com,4;5;5,4;5;3,Invite to Workshop Track,0,4,2.0,yes,10/27/17,Itema;Norges teknisk-naturvitenskapelige universitet,-1;-1,-1;-1,3
1100,1100,1100,1100,1100,1100,1100,1100,ICLR,2018,Continuous-Time Flows for Efficient Inference and Density Estimation,Changyou Chen;Chunyuan Li;Liqun Chen;Wenlin Wang;Yunchen Pu;Lawrence Carin,cchangyou@gmail.com;chunyuan.li@duke.edu;lc267@duke.edu;wenlin.wang@duke.edu;yunchen.pu@duke.edu;lcarin@duke.edu,3;6;6,3;4;4,Reject,0,3,0.0,yes,10/27/17,"State University of New York, Buffalo;Duke University;Duke University;Duke University;Duke University;Duke University",85;46;46;46;46;46,270;17;17;17;17;17,5;4
1101,1101,1101,1101,1101,1101,1101,1101,ICLR,2018,Large-scale Cloze Test Dataset Designed by Teachers,Qizhe Xie;Guokun Lai;Zihang Dai;Eduard Hovy,qizhex@gmail.com;guokun@cs.cmu.edu;zander.dai@gmail.com;hovy@cs.cmu.edu,4;7;4,4;4;4,Reject,0,4,0.0,yes,10/27/17,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1,24;24;24;24,
1102,1102,1102,1102,1102,1102,1102,1102,ICLR,2018,Deep Learning is Robust to Massive Label Noise,David Rolnick;Andreas Veit;Serge Belongie;Nir Shavit,drolnick@mit.edu;av443@cornell.edu;sjb344@cornell.edu;shanir@csail.mit.edu,5;4;5,4;5;5,Reject,2,4,0.0,yes,10/27/17,Massachusetts Institute of Technology;Cornell University;Cornell University;Massachusetts Institute of Technology,2;7;7;2,5;19;19;5,
1103,1103,1103,1103,1103,1103,1103,1103,ICLR,2018,Unsupervised Hierarchical Video Prediction,Nevan Wichers;Dumitru Erhan;Honglak Lee,wichersn@google.com;dumitru@google.com;honglak@google.com,4;4;4,4;4;4,Reject,0,2,0.0,yes,10/27/17,Google;Google;Google,-1;-1;-1,-1;-1;-1,
1104,1104,1104,1104,1104,1104,1104,1104,ICLR,2018,One-shot and few-shot learning of word embeddings,Andrew Kyle Lampinen;James Lloyd McClelland,lampinen@stanford.edu;mcclelland@stanford.edu,4;3;4,4;4;4,Reject,3,3,0.0,yes,10/27/17,Stanford University;Stanford University,4;4,3;3,3;6
1105,1105,1105,1105,1105,1105,1105,1105,ICLR,2018,Unbiasing Truncated Backpropagation Through Time,Corentin Tallec;Yann Ollivier,corentin.tallec@polytechnique.edu;yann@yann-ollivier.org,6;5;5,3;4;4,Reject,0,0,0.0,yes,10/27/17,Ecole polytechnique;Facebook,468;-1,115;-1,3;10
1106,1106,1106,1106,1106,1106,1106,1106,ICLR,2018,Joint autoencoders: a flexible meta-learning framework,Baruch Epstein;Ron Meir;Tomer Michaeli,baruch.epstein@gmail.com;rmeir@ee.technion.ac.il;tomer.m@ee.technion.ac.il,4;5;5,4;3;4,Reject,0,4,0.0,yes,10/27/17,Technion;Technion;Technion,24;24;24,327;327;327,6
1107,1107,1107,1107,1107,1107,1107,1107,ICLR,2018,On the Use of Word Embeddings Alone to Represent Natural Language Sequences,Dinghan Shen;Guoyin Wang;Wenlin Wang;Martin Renqiang Min;Qinliang Su;Yizhe Zhang;Ricardo Henao;Lawrence Carin,dinghan.shen@duke.edu;guoyin.wang@duke.edu;wenlin.wang@duke.edu;renqiang@nec-labs.com;qinliang.su@duke.edu;yizhe.zhang@duke.edu;ricardo.henao@duke.edu;lcarin@duke.edu,7;5;6,4;4;5,Reject,0,14,0.0,yes,10/27/17,Duke University;Duke University;Duke University;NEC-Labs;Duke University;Duke University;Duke University;Duke University,46;46;46;-1;46;46;46;46,17;17;17;-1;17;17;17;17,3
1108,1108,1108,1108,1108,1108,1108,1108,ICLR,2018,Byte-Level Recursive Convolutional Auto-Encoder for Text,Xiang Zhang;Yann LeCun,xiang@cs.nyu.edu;yann@cs.nyu.edu,7;5;5,4;3;5,Reject,0,0,1.0,yes,10/27/17,New York University;New York University,26;26,27;27,
1109,1109,1109,1109,1109,1109,1109,1109,ICLR,2018,Phase Conductor on Multi-layered Attentions for Machine Comprehension,Rui Liu;Wei Wei;Weiguang Mao;Maria Chikina,ult.rui.liu@gmail.com;weiwei@cs.cmu.edu;mwg10.thu@gmail.com;mchikina@gmail.com,8;5;5,3;5;4,Reject,0,4,0.0,yes,10/27/17,Carnegie Mellon University;Carnegie Mellon University;University of Pittsburgh;,1;1;78;-1,24;24;100;-1,3
1110,1110,1110,1110,1110,1110,1110,1110,ICLR,2018,Gaussian Prototypical Networks for Few-Shot Learning on Omniglot,Stanislav Fort,sfort1@stanford.edu,4;3;3,4;4;4,Reject,0,0,0.0,yes,10/27/17,Stanford University,4,3,6
1111,1111,1111,1111,1111,1111,1111,1111,ICLR,2018,Anomaly Detection with Generative Adversarial Networks,Lucas Deecke;Robert Vandermeulen;Lukas Ruff;Stephan Mandt;Marius Kloft,ldeecke@gmail.com;vandermeulen@cs.uni-kl.de;contact@lukasruff.com;stephan.mandt@disneyresearch.com;kloft@cs.uni-kl.de,4;6;4,5;4;4,Reject,0,3,0.0,yes,10/27/17,"Freie Universität Berlin;TU Kaiserslautern;Hasso Plattner Institute;Disney Research, Disney;TU Kaiserslautern",-1;139;-1;-1;139,-1;452;-1;-1;452,5;4
1112,1112,1112,1112,1112,1112,1112,1112,ICLR,2018,Learning Gaussian Policies from Smoothed Action Value Functions,Ofir Nachum;Mohammad Norouzi;George Tucker;Dale Schuurmans,ofirnachum@google.com;mnorouzi@google.com;gjt@google.com;schuurmans@google.com,6;6;5,4;4;3,Reject,0,3,0.0,yes,10/27/17,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,
1113,1113,1113,1113,1113,1113,1113,1113,ICLR,2018,Spectral Graph Wavelets for Structural Role Similarity in Networks,Claire Donnat;Marinka Zitnik;David Hallac;Jure Leskovec,cdonnat@stanford.edu;marinka@cs.stanford.edu;hallac@stanford.edu;jure@cs.stanford.edu,5;5;3,5;4;4,Reject,0,3,0.0,yes,10/27/17,Stanford University;Stanford University;Stanford University;Stanford University,4;4;4;4,3;3;3;3,1;10
1114,1114,1114,1114,1114,1114,1114,1114,ICLR,2018,Adversarial Spheres,Justin Gilmer;Luke Metz;Fartash Faghri;Sam Schoenholz;Maithra Raghu;Martin Wattenberg;Ian Goodfellow,gilmer@google.com;lmetz@google.com;fartash.faghri@google.com;schsam@google.com;maithra@google.com;goodfellow@google.com,4;5;3,4;3;3,Invite to Workshop Track,0,2,0.0,yes,10/27/17,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,4;1;2
1115,1115,1115,1115,1115,1115,1115,1115,ICLR,2018,Modifying memories in a Recurrent Neural Network Unit,Vlad Velici;Adam Prügel-Bennett,vsv1g12@soton.ac.uk;apb@soton.ac.uk,4;4;3,3;3;4,Reject,1,1,0.0,yes,10/27/17,University of Southampton;University of Southampton,181;181,126;126,
1116,1116,1116,1116,1116,1116,1116,1116,ICLR,2018,Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,Tuomas Haarnoja;Aurick Zhou;Pieter Abbeel;Sergey Levine,haarnoja@berkeley.edu;azhou42@berkeley.edu;pabbeel@cs.berkeley.edu;svlevine@eecs.berkeley.edu,3;7;5,4;4;4,Invite to Workshop Track,0,4,0.0,yes,10/27/17,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5,18;18;18;18,
1117,1117,1117,1117,1117,1117,1117,1117,ICLR,2018,Challenges in Disentangling Independent Factors of Variation,Attila  Szabo;Qiyang  Hu;Tiziano  Portenier;Matthias  Zwicker;Paolo  Favaro,szabo@inf.unibe.ch;hu@inf.unibe.ch;portenier@inf.unibe.ch;zwicker@inf.unibe.ch;paolo.favaro@inf.unibe.ch,6;5;5,4;3;3,Invite to Workshop Track,0,3,0.0,yes,10/27/17,University of Bern;University of Bern;University of Bern;University of Bern;University of Bern,364;364;364;364;364,105;105;105;105;105,5;4;1
1118,1118,1118,1118,1118,1118,1118,1118,ICLR,2018,Ground-Truth Adversarial Examples,Nicholas Carlini;Guy Katz;Clark Barrett;David L. Dill,nicholas@carlini.com;katz911@gmail.com;barrett@cs.stanford.edu;dill@cs.stanford.edu,5;4;6,4;4;3,Reject,0,0,0.0,yes,10/27/17,University of California Berkeley;Hebrew University of Jerusalem;Stanford University;Stanford University,5;62;4;4,18;205;3;3,4
1119,1119,1119,1119,1119,1119,1119,1119,ICLR,2018,Learning Parsimonious Deep Feed-forward Networks,Zhourong Chen;Xiaopeng Li;Nevin L. Zhang,zchenbb@cse.ust.hk;xlibo@cse.ust.hk;lzhang@cse.ust.hk,5;4;5,5;2;2,Reject,0,5,0.0,yes,10/27/17,The Hong Kong University of Science and Technology;The Hong Kong University of Science and Technology;The Hong Kong University of Science and Technology,40;40;40,44;44;44,
1120,1120,1120,1120,1120,1120,1120,1120,ICLR,2018,Learning Deep Generative Models of Graphs,Yujia Li;Oriol Vinyals;Chris Dyer;Razvan Pascanu;Peter Battaglia,yujiali@google.com;vinyals@google.com;cdyer@google.com;razp@google.com;peterbattaglia@google.com,5;6;6,3;3;4,Invite to Workshop Track,2,7,0.0,yes,10/27/17,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,5;10
1121,1121,1121,1121,1121,1121,1121,1121,ICLR,2018,An information-theoretic analysis of deep latent-variable models,Alex Alemi;Ben Poole;Ian Fischer;Josh Dillon;Rif A. Saurus;Kevin Murphy,alemi@google.com;poole@cs.stanford.edu;iansf@google.com;jvdillon@google.com;rif@google.com;kpmurphy@google.com,5;7;5,4;5;5,Reject,1,5,0.0,yes,10/27/17,Google;Stanford University;Google;Google;Google;Google,-1;4;-1;-1;-1;-1,-1;3;-1;-1;-1;-1,5;1
1122,1122,1122,1122,1122,1122,1122,1122,ICLR,2018,Trust-PCL: An Off-Policy Trust Region Method for Continuous Control,Ofir Nachum;Mohammad Norouzi;Kelvin Xu;Dale Schuurmans,ofirnachum@google.com;mnorouzi@google.com;iamkelvinxu@gmail.com;schuurmans@google.com,6;5;5,4;4;1,Accept (Poster),0,5,0.0,yes,10/27/17,Google;Google;University of California Berkeley;Google,-1;-1;5;-1,-1;-1;18;-1,
1123,1123,1123,1123,1123,1123,1123,1123,ICLR,2018,Modular Continual Learning in a Unified Visual Environment,Kevin T. Feigelis;Blue Sheffer;Daniel L. K. Yamins,feigelis@stanford.edu;bsheffer@stanford.edu;yamins@stanford.edu,6;8;8,2;3;2,Accept (Poster),0,5,0.0,yes,10/27/17,Stanford University;Stanford University;Stanford University,4;4;4,3;3;3,
1124,1124,1124,1124,1124,1124,1124,1124,ICLR,2018,PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples,Yang Song;Taesup Kim;Sebastian Nowozin;Stefano Ermon;Nate Kushman,yangsong@cs.stanford.edu;taesup.kim@umontreal.ca;sebastian.nowozin@microsoft.com;ermon@cs.stanford.edu;nkushman@microsoft.com,7;7;7,4;4;4,Accept (Poster),0,3,0.0,yes,10/27/17,Stanford University;University of Montreal;Microsoft;Stanford University;Microsoft,4;124;-1;4;-1,3;108;-1;3;-1,4
1125,1125,1125,1125,1125,1125,1125,1125,ICLR,2018,BinaryFlex: On-the-Fly Kernel Generation in Binary Convolutional Networks,Vincent W.-S. Tseng;Sourav Bhattachary;Javier Fernández Marqués;Milad Alizadeh;Catherine Tong;Nicholas Donald Lane,wt262@cornell.edu;sourav.bhattacharya@nokia-bell-labs.com;javier.fernandezmarques@cs.ox.ac.uk;milad.alizadeh@cs.ox.ac.uk;eu.tong@cs.ox.ac.uk;nicholas.lane@cs.ox.uk,5;5;3,3;3;4,Reject,0,4,0.0,yes,10/27/17,Cornell University;Nokia Bell Labs;University of Oxford;University of Oxford;University of Oxford;,7;-1;51;51;51;-1,19;-1;1;1;1;-1,2
1126,1126,1126,1126,1126,1126,1126,1126,ICLR,2018,Neural Networks for irregularly observed continuous-time Stochastic Processes,Francois W. Belletti;Alexander Ku;Joseph E. Gonzalez,francois.belletti@berkeley.edu;alexku@berkeley.edu;jegonzal@berkeley.edu,5;5;2,5;4;3,Reject,0,4,0.0,yes,10/27/17,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,18;18;18,
1127,1127,1127,1127,1127,1127,1127,1127,ICLR,2018,Learning to diagnose from scratch by exploiting dependencies among labels,Li Yao;Eric Poblenz;Dmitry Dagunts;Ben Covington;Devon Bernard;Kevin Lyman,li@enlitic.com;eric@enlitic.com;dmitry@enlitic.com;ben@enlitic.com;devon@entlic.com;kevin@enlitic.com,6;6;6,3;4;3,Reject,0,3,0.0,yes,10/27/17,Enlitic;Enlitic;Enlitic;Enlitic;Entlic;Enlitic,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,
1128,1128,1128,1128,1128,1128,1128,1128,ICLR,2018,Towards Provable Control for Unknown Linear Dynamical Systems,Sanjeev Arora;Elad Hazan;Holden Lee;Karan Singh;Cyril Zhang;Yi Zhang,arora@cs.princeton.edu;ehazan@cs.princeton.edu;holdenl@princeton.edu;karans@cs.princeton.edu;cyril.zhang@cs.princeton.edu;y.zhang@cs.princeton.edu,4;7;5,3;3;4,Invite to Workshop Track,0,4,0.0,yes,10/27/17,Princeton University;Princeton University;Princeton University;Princeton University;Princeton University;Princeton University,31;31;31;31;31;31,7;7;7;7;7;7,
1129,1129,1129,1129,1129,1129,1129,1129,ICLR,2018,Depth separation and weight-width trade-offs for sigmoidal neural networks,Amit Deshpande;Navin Goyal;Sushrut Karmalkar,amitdesh@microsoft.com;navingo@microsoft.com;sushrutk@cs.utexas.edu,6;5;3,4;4;5,Reject,0,4,0.0,yes,10/27/17,"Microsoft;Microsoft;University of Texas, Austin",-1;-1;21,-1;-1;49,
1130,1130,1130,1130,1130,1130,1130,1130,ICLR,2018,Time Limits in Reinforcement Learning,Fabio Pardo;Arash Tavakoli;Vitaly Levdik;Petar Kormushev,f.pardo@imperial.ac.uk;a.tavakoli@imperial.ac.uk;v.levdik@imperial.ac.uk;p.kormushev@imperial.ac.uk,5;4;4,4;5;4,Reject,0,7,0.0,yes,10/27/17,Imperial College London;Imperial College London;Imperial College London;Imperial College London,74;74;74;74,8;8;8;8,10
1131,1131,1131,1131,1131,1131,1131,1131,ICLR,2018,Learning Independent Features with Adversarial Nets for Non-linear ICA,Philemon Brakel;Yoshua Bengio,pbpop3@gmail.com;yoshua.bengio@umontreal.ca,5;3;6,5;5;3,Reject,0,4,0.0,yes,10/27/17,Google;University of Montreal,-1;124,-1;108,4
1132,1132,1132,1132,1132,1132,1132,1132,ICLR,2018,Discovering the mechanics of hidden neurons,Simon Carbonnelle;Christophe De Vleeschouwer,simon.carbonnelle@uclouvain.be;christophe.devleeschouwer@uclouvain.be,7;4;5,4;4;4,Reject,1,7,0.0,yes,10/27/17,UCL;UCL,291;291,16;16,
1133,1133,1133,1133,1133,1133,1133,1133,ICLR,2018,DNN Model Compression Under Accuracy Constraints,Soroosh Khoram;Jing Li,khoram@wisc.edu;jli@ece.wisc.edu,4;3;3,3;3;5,Reject,0,0,0.0,yes,10/27/17,University of Southern California;University of Southern California,31;31,66;66,
1134,1134,1134,1134,1134,1134,1134,1134,ICLR,2018,Using Deep Reinforcement Learning to Generate Rationales for Molecules,Benson Chen;Connor Coley;Regina Barzilay;Tommi Jaakkola,bensonc@mit.edu;ccoley@mit.edu;regina@csail.mit.edu;tommi@csail.mit.edu,5;5;5,4;4;4,Reject,0,3,0.0,yes,10/27/17,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2,5;5;5;5,10
1135,1135,1135,1135,1135,1135,1135,1135,ICLR,2018,Fast Node Embeddings: Learning Ego-Centric Representations,Tiago Pimentel;Adriano Veloso;Nivio Ziviani,tpimentel@dcc.ufmg.br;adrianov@dcc.ufmg.br;nivio@dcc.ufmg.br,5;6;4,4;4;5,Invite to Workshop Track,0,4,0.0,yes,10/27/17,Universidade Federal de Minas Gerais;Universidade Federal de Minas Gerais;Universidade Federal de Minas Gerais,468;468;468,715;715;715,3;10
1136,1136,1136,1136,1136,1136,1136,1136,ICLR,2018,HybridNet: A Hybrid Neural Architecture to Speed-up Autoregressive  Models,Yanqi Zhou;Wei Ping;Sercan Arik;Kainan Peng;Greg Diamos,zhouyanqi@baidu.com;pingwei01@baidu.com;sercanarik@baidu.com;pengkainan@baidu.com;gregdiamos@baidu.com,6;4;4,5;5;5,Reject,0,3,0.0,yes,10/27/17,Baidu;Baidu;Baidu;Baidu;Baidu,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,
1137,1137,1137,1137,1137,1137,1137,1137,ICLR,2018,Sparse Regularized Deep Neural Networks For Efficient Embedded Learning,Jia Bi,jb4e14@soton.ac.uk,4;4;2,4;5;3,Reject,0,3,0.0,yes,10/27/17,University of Southampton,181,126,9
1138,1138,1138,1138,1138,1138,1138,1138,ICLR,2018,GANITE: Estimation of Individualized Treatment Effects using Generative Adversarial Nets,Jinsung Yoon;James Jordon;Mihaela van der Schaar,jsyoon0823@gmail.com;james.jordon@hertford.ox.ac.uk;mihaela.vanderschaar@oxford-man.ox.ac.uk,6;6;6,4;3;3,Accept (Poster),0,3,0.0,yes,10/27/17,"University of California, Los Angeles;University of Oxford;University of Oxford",20;51;51,15;1;1,5;4
1139,1139,1139,1139,1139,1139,1139,1139,ICLR,2018,Network Signatures from Image Representation of Adjacency Matrices: Deep/Transfer Learning for Subgraph Classification,Kshiteesh Hegde;Malik Magdon-Ismail;Ram Ramanathan;Bishal Thapa,hegdek2@rpi.edu;magdon@rpi.edu;ram@gotenna.com;bishal.thapa@raytheon.com,3;6;6,3;3;3,Reject,0,6,0.0,yes,10/27/17,Rensselaer Polytechnic Institute;Rensselaer Polytechnic Institute;Gotenna;Raytheon,153;153;-1;-1,304;304;-1;-1,6;10
1140,1140,1140,1140,1140,1140,1140,1140,ICLR,2018,SCAN: Learning Hierarchical Compositional Visual Concepts,Irina Higgins;Nicolas Sonnerat;Loic Matthey;Arka Pal;Christopher P Burgess;Matko Bošnjak;Murray Shanahan;Matthew Botvinick;Demis Hassabis;Alexander Lerchner,irinah@google.com;sonnerat@google.com;lmatthey@google.com;arkap@google.com;cpburgess@google.com;matko@google.com;mshanahan@google.com;botvinick@google.com;demishassabis@google.com;lerchner@google.com,5;6;7,4;4;4,Accept (Poster),0,6,0.0,yes,10/27/17,Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,5
1141,1141,1141,1141,1141,1141,1141,1141,ICLR,2018,Variance-based Gradient Compression for Efficient Distributed Deep Learning,Yusuke Tsuzuku;Hiroto Imachi;Takuya Akiba,tsuzuku@ms.k.u-tokyo.ac.jp;imachi@preferred.jp;akiba@preferred.jp,6;4;7,4;4;4,Invite to Workshop Track,0,7,1.0,yes,10/27/17,"The University of Tokyo;Preferred Networks, Inc.;Preferred Networks, Inc.",52;-1;-1,45;-1;-1,1
1142,1142,1142,1142,1142,1142,1142,1142,ICLR,2018,Learning to Encode Text as Human-Readable Summaries using Generative Adversarial Networks,Yau-Shian Wang;Hung-Yi Lee,king6101@gmail.com;tlkagkb93901106@gmail.com,5;4;6,4;4;4,Reject,0,5,0.0,yes,10/27/17,National Taiwan University;,85;-1,197;-1,
1143,1143,1143,1143,1143,1143,1143,1143,ICLR,2018,Subspace Network: Deep Multi-Task Censored Regression for Modeling Neurodegenerative Diseases,Mengying Sun;Inci M. Baytas;Zhangyang Wang;Jiayu Zhou,sunmeng2@msu.edu;baytasin@msu.edu;atlaswang@tamu.edu;jiayuz@msu.edu,4;5;5,5;3;4,Reject,0,7,0.0,yes,10/27/17,SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;Texas A&M;SUN YAT-SEN UNIVERSITY,468;468;42;468,352;352;160;352,5
1144,1144,1144,1144,1144,1144,1144,1144,ICLR,2018,UCB EXPLORATION VIA Q-ENSEMBLES,Richard Y. Chen;Szymon Sidor;Pieter Abbeel;John Schulman,richardchen@openai.com;szymon@openai.com;pabbeel@cs.berkeley.edu;joschu@openai.com,6;7;5,5;4;3,Reject,0,12,0.0,yes,10/27/17,OpenAI;OpenAI;University of California Berkeley;OpenAI,-1;-1;5;-1,-1;-1;18;-1,
1145,1145,1145,1145,1145,1145,1145,1145,ICLR,2018,Convergence rate of sign stochastic gradient descent for non-convex functions,Jeremy Bernstein;Kamyar Azizzadenesheli;Yu-Xiang Wang;Anima Anandkumar,bernstein@caltech.edu;kazizzad@uci.edu;yuxiangw@cs.cmu.edu;animakumar@gmail.com,4;4;5,4;4;5,Reject,0,15,2.0,yes,10/27/17,"California Institute of Technology;University of California, Irvine;Carnegie Mellon University;University of California-Irvine",139;36;1;36,3;99;24;99,1;9
1146,1146,1146,1146,1146,1146,1146,1146,ICLR,2018,Empirical Analysis of the Hessian of Over-Parametrized Neural Networks,Levent Sagun;Utku Evci;V. Ugur Guney;Yann Dauphin;Leon Bottou,leventsagun@gmail.com;ue225@nyu.edu;vug@fb.com;yann@dauphin.io;leonb@fb.com,5;4;5,2;4;4,Invite to Workshop Track,0,7,0.0,yes,10/27/17,CEA;New York University;Facebook;Facebook;Facebook,-1;26;-1;-1;-1,-1;27;-1;-1;-1,9
1147,1147,1147,1147,1147,1147,1147,1147,ICLR,2018,CNNs as Inverse Problem Solvers and Double Network Superresolution,Cem TARHAN;Gözde BOZDAĞI AKAR,cemtarhan@aselsan.com.tr;bozdagi@metu.edu.tr,6;3;4,2;5;4,Reject,0,3,0.0,yes,10/27/17,METU;METU,210;210,654;654,
1148,1148,1148,1148,1148,1148,1148,1148,ICLR,2018,IVE-GAN: Invariant Encoding Generative Adversarial Networks,Robin Winter;Djork-Arnè Clevert,robin.winter@bayer.com;djork-arne.clevert@bayer.com,5;4;5,4;5;4,Reject,2,0,0.0,yes,10/27/17,Bayer Ag;Bayer Ag,-1;-1,-1;-1,5;4
1149,1149,1149,1149,1149,1149,1149,1149,ICLR,2018,Composable Planning with Attributes,Amy Zhang;Adam Lerer;Sainbayar Sukhbaatar;Rob Fergus;Arthur Szlam,amyzhang@fb.com;alerer@fb.com;sainbar@cs.nyu.edu;fergus@cs.nyu.edu;aszlam@fb.com,5;4;7,4;5;3,Reject,0,5,0.0,yes,10/27/17,Facebook;Facebook;New York University;New York University;Facebook,-1;-1;26;26;-1,-1;-1;27;27;-1,10
1150,1150,1150,1150,1150,1150,1150,1150,ICLR,2018,How do deep convolutional neural networks learn from raw audio waveforms?,Yuan Gong;Christian Poellabauer,ygong1@nd.edu;cpoellab@nd.edu,3;3;2,4;5;5,Reject,0,7,0.0,yes,10/27/17,University of Notre Dame;University of Notre Dame,124;124,150;150,
1151,1151,1151,1151,1151,1151,1151,1151,ICLR,2018,Relevance of Unsupervised Metrics in Task-Oriented Dialogue for Evaluating Natural Language Generation,Shikhar Sharma;Layla El Asri;Hannes Schulz;Jeremie Zumer,shikhar.sharma@microsoft.com;layla.elasri@microsoft.com;hannes.schulz@microsoft.com;jeremie_zumer@hotmail.com,4;5;5,4;4;3,Reject,0,3,0.0,yes,10/27/17,Microsoft;Microsoft;Microsoft;,-1;-1;-1;-1,-1;-1;-1;-1,3
1152,1152,1152,1152,1152,1152,1152,1152,ICLR,2018,Evaluation of generative networks through their data augmentation capacity,Timothée Lesort;Florian Bordes;Jean-Francois Goudou;David Filliat,t.lesort@gmail.com;florian.bordes@umontreal.ca;jean-francois.goudou@thalesgroup.com;david.filliat@ensta-paristech.fr,3;3;5,5;5;3,Reject,0,3,0.0,yes,10/27/17,ENSTA ParisTech;University of Montreal;Thalesgroup;ENSTA ParisTech,468;124;-1;468,1103;108;-1;1103,5;4
1153,1153,1153,1153,1153,1153,1153,1153,ICLR,2018,FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling,Jie Chen;Tengfei Ma;Cao Xiao,chenjie@us.ibm.com;tengfei.ma1@ibm.com;cxiao@us.ibm.com,6;7;7;8,4;2;4;4,Accept (Poster),2,20,1.0,yes,10/27/17,International Business Machines;International Business Machines;International Business Machines,-1;-1;-1,-1;-1;-1,10
1154,1154,1154,1154,1154,1154,1154,1154,ICLR,2018,Interpretable and Pedagogical Examples,Smitha Milli;Pieter Abbeel;Igor Mordatch,smilli@berkeley.edu;pabbeel@cs.berkeley.edu;igor.mordatch@gmail.com,8;8;4,3;4;3,Reject,0,3,0.0,yes,10/27/17,University of California Berkeley;University of California Berkeley;University of Washington,5;5;6,18;18;25,
1155,1155,1155,1155,1155,1155,1155,1155,ICLR,2018,On the Generalization Effects of DenseNet Model Structures ,Yin Liu;Vincent Chen,liuyin14@mails.tsinghua.edu.cn;389091983@qq.com,2;3;3,5;4;4,Reject,0,0,0.0,yes,10/27/17,Tsinghua University;QQ.com,10;-1,30;-1,8
1156,1156,1156,1156,1156,1156,1156,1156,ICLR,2018,INTERPRETATION OF NEURAL NETWORK IS FRAGILE,Amirata Ghorbani;Abubakar Abid;James Zou,amiratag@stanford.edu;a12d@stanford.edu;jamesz@stanford.edu,6;4;5,2;4;5,Reject,9,8,0.0,yes,10/27/17,Stanford University;Stanford University;Stanford University,4;4;4,3;3;3,4
1157,1157,1157,1157,1157,1157,1157,1157,ICLR,2018,Faster Discovery of Neural Architectures by Searching for Paths in a Large Model,Hieu Pham;Melody Y. Guan;Barret Zoph;Quoc V. Le;Jeff Dean,hyhieu@cmu.edu;mguan@stanford.edu;barretzoph@google.com;qvl@google.com;jeff@google.com,6;5;5,2;3;2,Invite to Workshop Track,4,14,0.0,yes,10/27/17,Carnegie Mellon University;Stanford University;Google;Google;Google,1;4;-1;-1;-1,24;3;-1;-1;-1,
1158,1158,1158,1158,1158,1158,1158,1158,ICLR,2018,Deep Lipschitz networks and Dudley GANs,Ehsan Abbasnejad;Javen Shi;Anton van den Hengel,ehsan.abbasnejad@adelaide.edu.au;javen.shi@adelaide.edu.au;anton.vandenhengel@adelaide.edu.au,8;5;5,4;3;1,Reject,0,3,0.0,yes,10/27/17,The University of Adelaide;The University of Adelaide;The University of Adelaide,124;124;124,134;134;134,5;4;8
1159,1159,1159,1159,1159,1159,1159,1159,ICLR,2018,Acquiring Target Stacking Skills by Goal-Parameterized Deep Reinforcement Learning,Wenbin Li;Jeannette Bohg;Mario Fritz,wenbinli@mpi-inf.mpg.de;bohg@stanford.edu;mfritz@mpi-inf.mpg.de,5;4;5,4;4;3,Reject,0,0,0.0,yes,10/27/17,"Saarland Informatics Campus, Max-Planck Institute;Stanford University;Saarland Informatics Campus, Max-Planck Institute",-1;4;-1,-1;3;-1,8
1160,1160,1160,1160,1160,1160,1160,1160,ICLR,2018,XGAN: Unsupervised Image-to-Image Translation for many-to-many Mappings,Amelie Royer;Konstantinos Bousmalis;Stephan Gouws;Fred Bertsch;Inbar Mosseri;Forrester Cole;Kevin Murphy,aroyer@ist.ac.at;konstantinos@google.com;sgouws@google.com;fredbertsch@google.com;inbarm@google.com;fcole@google.com;kpmurphy@google.com,3;4;4,5;4;3,Reject,2,9,0.0,yes,10/27/17,Institute of Science and Technology Austria;Google;Google;Google;Google;Google;Google,99;-1;-1;-1;-1;-1;-1,1103;-1;-1;-1;-1;-1;-1,4
1161,1161,1161,1161,1161,1161,1161,1161,ICLR,2018,Meta-Learning Transferable Active Learning Policies by Deep Reinforcement Learning,Kunkun Pang;Mingzhi Dong;Timothy Hospedales,k.pang@ed.ac.uk;mingzhi.dong.13@ucl.ac.uk;t.hospedales@ed.ac.uk,6;7;6,3;4;4,Reject,0,6,0.0,yes,10/27/17,University of Edinburgh;University College London;University of Edinburgh,33;46;33,27;16;27,6
1162,1162,1162,1162,1162,1162,1162,1162,ICLR,2018,Feature Map Variational Auto-Encoders,Lars Maaløe;Ole Winther,larsma@dtu.dk;olwi@dtu.dk,5;3;6,4;3;4,Reject,14,5,0.0,yes,10/27/17,Technical University of Denmark;Technical University of Denmark,210;210,153;153,5
1163,1163,1163,1163,1163,1163,1163,1163,ICLR,2018,Structured Exploration via Hierarchical Variational Policy Networks,Stephan Zheng;Yisong Yue,stephan@caltech.edu;yyue@caltech.edu,4;7;5,5;3;3,Reject,0,8,0.0,yes,10/27/17,California Institute of Technology;California Institute of Technology,139;139,3;3,
1164,1164,1164,1164,1164,1164,1164,1164,ICLR,2018,Bayesian Time Series Forecasting with Change Point and Anomaly Detection,Anderson Y. Zhang;Miao Lu;Deguang Kong;Jimmy Yang,ye.zhang@yale.edu;mlu@oath.com;dkong@oath.com;jianyang@oath.com,5;6;4,5;3;5,Reject,0,3,0.0,yes,10/27/17,Yale University;Yahoo;Oath;Oath,62;-1;-1;-1,12;-1;-1;-1,11
1165,1165,1165,1165,1165,1165,1165,1165,ICLR,2018,An empirical study on evaluation metrics of generative adversarial networks,Gao Huang;Yang Yuan;Qiantong Xu;Chuan Guo;Yu Sun;Felix Wu;Kilian Weinberger,gh349@cornell.edu;yy528@cornell.edu;qx57@cornell.edu;cg563@cornell.edu;yusun@berkeley.edu;fw245@cornell.edu;kqw4@cornell.edu,8;7;5;5,3;4;5;3,Reject,2,11,0.0,yes,10/27/17,Cornell University;Cornell University;Cornell University;Cornell University;University of California Berkeley;Cornell University;Cornell University,7;7;7;7;5;7;7,19;19;19;19;18;19;19,5;4
1166,1166,1166,1166,1166,1166,1166,1166,ICLR,2018,Explaining the Mistakes of Neural Networks with Latent Sympathetic Examples,Riaan Zoetmulder;Efstratios Gavves;Peter O'Connor,riaan.zoetmulder@student.uva.nl;egavves@uva.nl;peter.ed.oconnor@gmail.com,4;4;6,5;3;4,Reject,0,4,0.0,yes,10/27/17,University of Amsterdam;University of Amsterdam;,181;181;-1,59;59;-1,5;4
1167,1167,1167,1167,1167,1167,1167,1167,ICLR,2018,FAST READING COMPREHENSION WITH CONVNETS,Felix Wu;Ni Lao;John Blitzer;Guandao Yang;Kilian Weinberger,fw245@cornell.edu;nlao@google.com;blitzer@google.com;gy46@cornell.edu;kqw4@cornell.edu,4;7;5,4;3;4,Reject,0,0,0.0,yes,10/27/17,Cornell University;Google;Google;Cornell University;Cornell University,7;-1;-1;7;7,19;-1;-1;19;19,
1168,1168,1168,1168,1168,1168,1168,1168,ICLR,2018,Block-Sparse Recurrent Neural Networks,Sharan Narang;Eric Undersander;Gregory Diamos,sharan@baidu.com;undersandereric@baidu.com;gdiamos@baidu.com,5;7;5,3;4;4,Reject,0,8,0.0,yes,10/27/17,Baidu;Baidu;Baidu,-1;-1;-1,-1;-1;-1,3
1169,1169,1169,1169,1169,1169,1169,1169,ICLR,2018,Bias-Variance Decomposition for Boltzmann Machines,Mahito Sugiyama;Koji Tsuda;Hiroyuki Nakahara,mahito@nii.ac.jp;tsuda@k.u-tokyo.ac.jp;hiro@brain.riken.jp,5;7;5,2;5;5,Reject,0,3,0.0,yes,10/27/17,Meiji University;The University of Tokyo;RIKEN,468;52;-1,334;45;-1,8
1170,1170,1170,1170,1170,1170,1170,1170,ICLR,2018,Unseen Class Discovery in Open-world Classification,Lei Shu;Hu Xu;Bing Liu,lshu3@uic.edu;hxu48@uic.edu;liub@uic.edu,5;5;4,4;5;4,Reject,0,0,0.0,yes,10/27/17,"University of Illinois, Chicago;University of Illinois, Chicago;University of Illinois, Chicago",57;57;57,255;255;255,
1171,1171,1171,1171,1171,1171,1171,1171,ICLR,2018,DLVM: A modern compiler infrastructure for deep learning systems,Richard Wei;Lane Schwartz;Vikram Adve,xwei12@illinois.edu;lanes@illinois.edu;vadve@illinois.edu,5;7;5,4;4;3,Invite to Workshop Track,0,4,0.0,yes,10/27/17,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",3;3;3,37;37;37,10
1172,1172,1172,1172,1172,1172,1172,1172,ICLR,2018,Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design,Daniel Neil;Marwin Segler;Laura Guasch;Mohamed Ahmed;Dean Plumbley;Matthew Sellwood;Nathan Brown,daniel.neil@benevolent.ai;marwin.segler@benevolent.ai;laura.guasch@benevolent.ai;mohamed.ahmed@benevolent.ai;dean.plumbley@benevolent.ai;matthew.sellwood@benevolent.ai;nathan.brown@benevolent.ai,4;7;6,2;4;3,Invite to Workshop Track,0,6,1.0,yes,10/27/17,BenevolentAI;BenevolentAI;BenevolentAI;BenevolentAI;BenevolentAI;BenevolentAI;BenevolentAI,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,
1173,1173,1173,1173,1173,1173,1173,1173,ICLR,2018,Transfer Learning to Learn with Multitask Neural Model Search,Catherine Wong;Andrea Gesmundo,catwong@cs.stanford.edu;agesmundo@google.com,5;7;4,2;3;4,Reject,0,7,0.0,yes,10/27/17,Stanford University;Google,4;-1,3;-1,6
1174,1174,1174,1174,1174,1174,1174,1174,ICLR,2018,Multiple Source Domain Adaptation with Adversarial Learning,Han Zhao;Shanghang Zhang;Guanhang Wu;Jo\~{a}o  P. Costeira;Jos\'{e} M. F.  Moura;Geoffrey J. Gordon,han.zhao@cs.cmu.edu;shanghaz@andrew.cmu.edu;guanhanw@andrew.cmu.edu;jpc@isr.ist.utl.pt;moura@andrew.cmu.edu;ggordon@cs.cmu.edu,6;6;6;6,3;5;5;4,Invite to Workshop Track,0,9,0.0,yes,10/27/17,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;University of Lisbon;Carnegie Mellon University;Carnegie Mellon University,1;1;1;468;1;1,24;24;24;509;24;24,4;1;8
1175,1175,1175,1175,1175,1175,1175,1175,ICLR,2018,Deep Generative Dual Memory Network for Continual Learning,Nitin Kamra;Umang Gupta;Yan Liu,nkamra@usc.edu;umanggup@usc.edu;yanliu.cs@usc.edu,5;6;7,4;4;2,Reject,0,7,0.0,yes,10/27/17,University of Southern California;University of Southern California;University of Southern California,31;31;31,66;66;66,5
1176,1176,1176,1176,1176,1176,1176,1176,ICLR,2018,Learning to Infer,Joseph Marino;Yisong Yue;Stephan Mandt,jmarino@caltech.edu;yyue@caltech.edu;stephan.mandt@disneyresearch.com,5;6;5,4;5;4,Invite to Workshop Track,0,9,0.0,yes,10/27/17,"California Institute of Technology;California Institute of Technology;Disney Research, Disney",139;139;-1,3;3;-1,11;5;1
1177,1177,1177,1177,1177,1177,1177,1177,ICLR,2018,Latent Space Oddity: on the Curvature of Deep Generative Models,Georgios Arvanitidis;Lars Kai Hansen;Søren Hauberg,gear@dtu.dk;lkai@dtu.dk;sohau@dtu.dk,3;7;7,4;4;3,Accept (Poster),0,5,0.0,yes,10/27/17,Technical University of Denmark;Technical University of Denmark;Technical University of Denmark,210;210;210,153;153;153,5
1178,1178,1178,1178,1178,1178,1178,1178,ICLR,2018,The Context-Aware Learner,Conor Durkan;Amos Storkey;Harrison Edwards,conor.durkan@ed.ac.uk;a.storkey@ed.ac.uk;h.l.edwards@sms.ed.ac.uk,6;4;4,5;3;4,Reject,0,0,0.0,yes,10/27/17,University of Edinburgh;University of Edinburgh;University of Edinburgh,33;33;33,27;27;27,6;8
1179,1179,1179,1179,1179,1179,1179,1179,ICLR,2018,When and where do feed-forward neural networks learn localist representations?,Ella M. Gale;Nicolas Martin;Jeffrey Bowers,eg16993@bristol.ac.uk;nm13850@bristol.ac.uk;j.bowers@bristol.ac.uk,3;3;5,3;5;4,Reject,0,5,0.0,yes,10/27/17,University of Bristol;University of Bristol;University of Bristol,124;124;124,76;76;76,
1180,1180,1180,1180,1180,1180,1180,1180,ICLR,2018,Generating Differentially Private Datasets Using GANs,Aleksei Triastcyn;Boi Faltings,aleksei.triastcyn@epfl.ch;boi.faltings@epfl.ch,6;5;4,4;4;4,Reject,2,4,0.0,yes,10/27/17,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne,468;468,38;38,5;4
1181,1181,1181,1181,1181,1181,1181,1181,ICLR,2018,Auxiliary Guided Autoregressive Variational Autoencoders,Thomas Lucas;Jakob Verbeek,thomas.lucas@inria.fr;jakob.verbeek@inria.fr,5;7;5,4;4;4,Reject,0,12,0.0,yes,10/27/17,INRIA;INRIA,-1;-1,-1;-1,5
1182,1182,1182,1182,1182,1182,1182,1182,ICLR,2018,Orthogonal Recurrent Neural Networks with Scaled Cayley Transform,Kyle Helfrich;Devin Willmott;Qiang Ye,kyle.helfrich@uky.edu;devin.willmott@uky.edu;qiang.ye@uky.edu,7;6;5,3;3;4,Reject,0,5,0.0,yes,10/27/17,University of Kentucky;University of Kentucky;University of Kentucky,210;210;210,346;346;346,
1183,1183,1183,1183,1183,1183,1183,1183,ICLR,2018,Can Deep Reinforcement Learning solve Erdos-Selfridge-Spencer Games?,Maithra Raghu;Alex Irpan;Jacob Andreas;Robert Kleinberg;Quoc Le;Jon Kleinberg,maithrar@gmail.com;alexirpan@google.com;j.d.andreas@gmail.com;rdk@cs.cornell.edu;qvl@google.com;kleinber@cs.cornell.edu,5;6;6,3;3;3,Invite to Workshop Track,0,13,0.0,yes,10/27/17,Cornell University;Google;University of California Berkeley;Cornell University;Google;Cornell University,7;-1;5;7;-1;7,19;-1;18;19;-1;19,8
1184,1184,1184,1184,1184,1184,1184,1184,ICLR,2018,DNN Representations as Codewords: Manipulating Statistical Properties via Penalty Regularization,Daeyoung Choi;Changho Shin;Hyunghun Cho;Wonjong Rhee,choid@snu.ac.kr;ch.shin@snu.ac.kr;webofthink@snu.ac.kr;wrhee@snu.ac.kr,5;5;5,5;3;4,Reject,0,3,0.0,yes,10/27/17,Seoul National University;Seoul National University;Seoul National University;Seoul National University,46;46;46;46,74;74;74;74,
1185,1185,1185,1185,1185,1185,1185,1185,ICLR,2018,A Tensor Analysis on Dense Connectivity via Convolutional Arithmetic Circuits,Emilio Rafael Balda;Arash Behboodi;Rudolf Mathar,emilio.balda@ti.rwth-aachen.de;arash.behboodi@ti.rwth-aachen.de;mathar@ti.rwth-aachen.de,4;4;5,3;3;3,Reject,0,3,0.0,yes,10/27/17,RWTH Aachen University;RWTH Aachen University;RWTH Aachen University,99;99;99,79;79;79,8
1186,1186,1186,1186,1186,1186,1186,1186,ICLR,2018,Classification and Disease Localization in Histopathology Using Only Global Labels: A Weakly-Supervised Approach,Pierre Courtiol;Eric W. Tramel;Marc Sanselme;Gilles Wainrib,pierre.courtiol@owkin.com;eric.tramel@owkin.com;marc.sanselme@owkin.com;gilles.wainrib@owkin.com,5;6;5,4;3;3,Reject,0,8,0.0,yes,10/27/17,;;;,-1;-1;-1;-1,-1;-1;-1;-1,2
1187,1187,1187,1187,1187,1187,1187,1187,ICLR,2018,Realtime query completion via deep language models,Po-Wei Wang;J. Zico Kolter;Vijai Mohan;Inderjit S. Dhillon,poweiw@cs.cmu.edu;zkolter@cs.cmu.edu;vijaim@amazon.com;isd@a9.com,4;6;5,5;3;3,Reject,3,2,0.0,yes,10/27/17,Carnegie Mellon University;Carnegie Mellon University;Amazon;A9,1;1;-1;-1,24;24;-1;-1,3
1188,1188,1188,1188,1188,1188,1188,1188,ICLR,2018,LEARNING SEMANTIC WORD RESPRESENTATIONS VIA TENSOR FACTORIZATION,Eric Bailey;Charles Meyer;Shuchin Aeron,popcorncolonel@gmail.com;cmey63@gmail.com;shuchin@ece.tufts.edu,5;5;5,3;5;5,Reject,0,6,0.0,yes,10/27/17,Tufts University;;Tufts University,153;-1;153,169;-1;169,3
1189,1189,1189,1189,1189,1189,1189,1189,ICLR,2018,Finding ReMO (Related Memory Object): A Simple neural architecture for Text based Reasoning,Jihyung Moon;Hyochang Yang;Sungzoon Cho,jhmoon@dm.snu.ac.kr;hyochang@dm.snu.ac.kr;zoon@snu.ac.kr,4;4;4,4;4;4,Reject,0,4,0.0,yes,10/27/17,Seoul National University;Seoul National University;Seoul National University,46;46;46,74;74;74,
1190,1190,1190,1190,1190,1190,1190,1190,ICLR,2018,Representing dynamically: An active process for describing sequential data,Juan Sebastian Olier;Emilia Barakova;Matthias Rauterberg;Carlo Regazzoni,j.s.olier.jauregui@tue.nl;e.i.barakova@tue.nl;g.w.m.rauterberg@tue.nl;carlo.regazzoni@unige.it,6;3;4;4,3;3;4;4,Reject,0,0,0.0,yes,10/27/17,Eindhoven University of Technology;Eindhoven University of Technology;Eindhoven University of Technology;Università degli Studi di Genova,181;181;181;-1,141;141;141;-1,5;11
1191,1191,1191,1191,1191,1191,1191,1191,ICLR,2018,Modeling Latent Attention Within Neural Networks,Christopher Grimm;Dilip Arumugam;Siddharth Karamcheti;David Abel;Lawson L.S. Wong;Michael L. Littman,crgrimm@umich.edu;dilip_arumugam@brown.edu;siddharth_karamcheti@brown.edu;david_abel@brown.edu;lsw@brown.edu;mlittman@cs.brown.edu,4;5;7,4;4;4,Reject,0,3,0.0,yes,10/27/17,University of Michigan;Brown University;Brown University;Brown University;Brown University;Brown University,8;62;62;62;62;62,21;50;50;50;50;50,3;2
1192,1192,1192,1192,1192,1192,1192,1192,ICLR,2018,Deep Active Learning for Named Entity Recognition,Yanyao Shen;Hyokun Yun;Zachary C. Lipton;Yakov Kronrod;Animashree Anandkumar,shenyanyao@utexas.edu;yunhyoku@amazon.com;zlipton@cmu.edu;kronrod@amazon.com;animakumar@gmail.com,6;6;7,3;4;4,Accept (Poster),0,5,0.0,yes,10/27/17,"University of Texas, Austin;Amazon;Carnegie Mellon University;Amazon;University of California-Irvine",21;-1;1;-1;36,49;-1;24;-1;99,3
1193,1193,1193,1193,1193,1193,1193,1193,ICLR,2018,On the limitations of first order approximation in GAN dynamics,Jerry Li;Aleksander Madry;John Peebles;Ludwig Schmidt,jerryzli@mit.edu;madry@mit.edu;jpeebles@mit.edu;ludwigs@mit.edu,4;5;7,4;3;3,Invite to Workshop Track,0,3,0.0,yes,10/27/17,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2,5;5;5;5,5;4
1194,1194,1194,1194,1194,1194,1194,1194,ICLR,2018,Learning Covariate-Specific Embeddings with Tensor Decompositions,Kevin Tian;Teng Zhang;James Zou,kjtian@stanford.edu;tengz@stanford.edu;jamesz@stanford.edu,5;5;5,3;5;4,Reject,0,0,0.0,yes,10/27/17,Stanford University;Stanford University;Stanford University,4;4;4,3;3;3,3;7
1195,1195,1195,1195,1195,1195,1195,1195,ICLR,2018,"LSD-Net: Look, Step and Detect for Joint Navigation and Multi-View Recognition with Deep Reinforcement Learning",N dinesh reddy,dnarapur@andrew.cmu.edu,4;6;3,4;4;4,Reject,0,0,0.0,yes,10/27/17,Carnegie Mellon University,1,24,
1196,1196,1196,1196,1196,1196,1196,1196,ICLR,2018,Cross-View Training for Semi-Supervised Learning,Kevin Clark;Thang Luong;Quoc V. Le,kevclark@cs.stanford.edu;qvl@google.com;thangluong@google.com,2;5;7,4;4;4,Invite to Workshop Track,0,6,0.0,yes,10/27/17,Stanford University;Google;Google,4;-1;-1,3;-1;-1,3;4
1197,1197,1197,1197,1197,1197,1197,1197,ICLR,2018,Learning Weighted Representations for Generalization Across Designs,Fredrik D. Johansson;Nathan Kallus;Uri Shalit;David Sontag,fredrikj@mit.edu;kallus@cornell.edu;urish22@gmail.com;dsontag@csail.mit.edu,5;8;7,3;3;4,Reject,0,4,0.0,yes,10/27/17,Massachusetts Institute of Technology;Cornell University;Technion;Massachusetts Institute of Technology,2;7;24;2,5;19;327;5,1;8
1198,1198,1198,1198,1198,1198,1198,1198,ICLR,2018,Exploring Asymmetric Encoder-Decoder Structure for Context-based Sentence Representation Learning,Shuai Tang;Hailin Jin;Chen Fang;Zhaowen Wang;Virginia R. de Sa,shuaitang93@ucsd.edu;hljin@adobe.com;cfang@adobe.com;zhawang@adobe.com;desa@ucsd.edu,7;6;3,4;4;5,Reject,4,5,0.0,yes,10/27/17,"University of California, San Diego;Adobe Systems;Adobe Systems;Adobe Systems;University of California, San Diego",11;-1;-1;-1;11,31;-1;-1;-1;31,
1199,1199,1199,1199,1199,1199,1199,1199,ICLR,2018,Data Augmentation Generative Adversarial Networks,Anthreas Antoniou;Amos Storkey;Harrison Edwards,a.antoniou@sms.ed.ac.uk;a.storkey@ed.ac.uk;h.l.edwards@sms.ed.ac.uk,4;9;6,4;5;3,Invite to Workshop Track,0,3,0.0,yes,10/27/17,University of Edinburgh;University of Edinburgh;University of Edinburgh,33;33;33,27;27;27,5;4;6
1200,1200,1200,1200,1200,1200,1200,1200,ICLR,2018,Long-term Forecasting using Tensor-Train RNNs,Rose Yu;Stephan Zheng;Anima Anandkumar;Yisong Yue,rose@caltech.edu;stephan@caltech.edu;anima@caltech.edu;yyue@caltech.edu,4;5;6,4;3;4,Reject,0,0,0.0,yes,10/27/17,California Institute of Technology;California Institute of Technology;California Institute of Technology;California Institute of Technology,139;139;139;139,3;3;3;3,
1201,1201,1201,1201,1201,1201,1201,1201,ICLR,2018,Autostacker: an Automatic Evolutionary Hierarchical  Machine Learning System,Boyuan Chen;Warren Mo;Ishanu Chattopadhyay;Hod Lipson,boyuan.chen@columbia.edu;warrenmo@uchicago.edu;ishanu@uchicago.edu;hod.lipson@columbia.edu,4;3;4,5;4;5,Reject,0,0,0.0,yes,10/27/17,Columbia University;University of Chicago;University of Chicago;Columbia University,15;46;46;15,14;9;9;14,
1202,1202,1202,1202,1202,1202,1202,1202,ICLR,2018,Inducing Grammars with and for Neural Machine Translation,Ke Tran;Yonatan Bisk,ketranmanh@gmail.com;ybisk@yonatanbisk.com,3;6;5,5;5;4,Reject,0,7,0.0,yes,10/27/17,University of Amsterdam;University of Washington,181;6,59;25,3
1203,1203,1203,1203,1203,1203,1203,1203,ICLR,2018,Reinforcement and Imitation Learning for Diverse Visuomotor Skills,Yuke Zhu;Ziyu Wang;Josh Merel;Andrei Rusu;Tom Erez;Serkan Cabi;Saran Tunyasuvunakool;János Kramár;Raia Hadsell;Nando de Freitas;Nicolas Heess,yukez@cs.stanford.edu;ziyu@google.com;jsmerel@google.com;andreirusu@google.com;etom@google.com;cabi@google.com;stunya@google.com;janosk@google.com;raia@google.com;nandodefreitas@google.com;heess@google.com,4;4;6,4;4;5,Reject,0,6,0.0,yes,10/27/17,Stanford University;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,4;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,3;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,6
1204,1204,1204,1204,1204,1204,1204,1204,ICLR,2018,Regret Minimization for Partially Observable Deep Reinforcement Learning,Peter H. Jin;Sergey Levine;Kurt Keutzer,phj@eecs.berkeley.edu;svlevine@eecs.berkeley.edu;keutzer@berkeley.edu,4;7;5,4;4;5,Invite to Workshop Track,0,5,0.0,yes,10/27/17,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,18;18;18,
1205,1205,1205,1205,1205,1205,1205,1205,ICLR,2018,Dense Recurrent Neural Network with Attention Gate,Yong-Ho Yoo;Kook Han;Sanghyun Cho;Kyoung-Chul Koh;Jong-Hwan Kim,yhyoo@rit.kaist.ac.kr;khan@rit.kaist.ac.kr;scho@rit.kaist.ac.kr;kckoh@rit.kaist.ac.kr;johkim@rit.kaist.ac.kr,2;4;4,4;4;4,Reject,0,2,0.0,yes,10/27/17,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,21;21;21;21;21,95;95;95;95;95,3
1206,1206,1206,1206,1206,1206,1206,1206,ICLR,2018,Stabilizing Gradients for Deep Neural Networks via Efficient SVD Parameterization,Jiong Zhang;Qi Lei;Inderjit S. Dhillon,zhangjiong724@utexas.edu;leiqi@ices.utexas.edu;inderjit@cs.utexas.edu,7;5;5,4;4;3,Reject,0,3,0.0,yes,10/27/17,"University of Texas, Austin;University of Texas, Austin;University of Texas, Austin",21;21;21,49;49;49,8
1207,1207,1207,1207,1207,1207,1207,1207,ICLR,2018,Semantic Interpolation in Implicit Models,Yannic Kilcher;Aurelien Lucchi;Thomas Hofmann,yannic.kilcher@inf.ethz.ch;aurelien.lucchi@inf.ethz.ch;thomas.hofmann@inf.ethz.ch,6;5;7,3;4;4,Accept (Poster),0,5,0.0,yes,10/27/17,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,9;9;9,10;10;10,
1208,1208,1208,1208,1208,1208,1208,1208,ICLR,2018,A Painless Attention Mechanism for Convolutional Neural Networks,Pau Rodríguez;Guillem Cucurull;Jordi Gonzàlez;Josep M. Gonfaus;Xavier Roca,pau.rodriguez@cvc.uab.es;gcucurull@cvc.uab.cat;poal@cvc.uab.cat;xavir@cvc.uab.es,5;5;6,4;4;4,Reject,0,10,1.0,yes,10/27/17,"Computer Vision Center, Universitat Autònoma de Barcelona;Universitat Autonoma de Barcelona;Universitat Autonoma de Barcelona;Computer Vision Center, Universitat Autònoma de Barcelona",468;468;468;468,146;146;146;146,7
1209,1209,1209,1209,1209,1209,1209,1209,ICLR,2018,Bayesian Hypernetworks,David Krueger;Chin-Wei Huang;Riashat Islam;Ryan Turner;Alexandre Lacoste;Aaron Courville,david.scott.krueger@gmail.com;chin-wei.huang@umontreal.ca;riashat.islam@mail.mcgill.ca;turnerry@iro.umontreal.ca;allac@elementai.com;aaron.courville@gmail.com,6;6;6,4;4;4,Reject,1,12,0.0,yes,10/27/17,University of Montreal;University of Montreal;McGill University;University of Montreal;Element AI;University of Montreal,124;124;81;124;-1;124,108;108;42;108;-1;108,11;4;1
1210,1210,1210,1210,1210,1210,1210,1210,ICLR,2018,Distributional Adversarial Networks,Chengtao Li;David Alvarez-Melis;Keyulu Xu;Stefanie Jegelka;Suvrit Sra,ctli@mit.edu;dalvmel@mit.edu;keyulu@mit.edu;stefje@csail.mit.edu;suvrit@mit.edu,6;6;6,3;4;3,Invite to Workshop Track,0,4,0.0,yes,10/27/17,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2;2,5;5;5;5;5,5;4
1211,1211,1211,1211,1211,1211,1211,1211,ICLR,2018,Structured Deep Factorization Machine: Towards General-Purpose Architectures,José P. González-Brenes;Ralph Edezhath,jgonzalez@chegg.com;redezhath@chegg.com,3;4;4,5;5;3,Reject,0,1,0.0,yes,10/27/17,Chegg;Chegg,-1;-1,-1;-1,3
1212,1212,1212,1212,1212,1212,1212,1212,ICLR,2018,Learning to search with MCTSnets,Arthur Guez;Theophane Weber;Ioannis Antonoglou;Karen Simonyan;Oriol Vinyals;Daan Wierstra;Remi Munos;David Silver,aguez@google.com;theophane@google.com;ioannisa@google.com;simonyan@google.com;vinyals@google.com;wierstra@google.com;munos@google.com;davidsilver@google.com,7;4;5,3;4;4,Reject,0,3,0.0,yes,10/27/17,Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1,
1213,1213,1213,1213,1213,1213,1213,1213,ICLR,2018,Open Loop Hyperparameter Optimization and Determinantal Point Processes,Jesse Dodge;Kevin Jamieson;Noah A. Smith,jessed@cs.cmu.edu;jamieson@cs.washington.edu;nasmith@cs.washington.edu,4;4;4,5;5;5,Reject,0,3,0.0,yes,9/27/18,Carnegie Mellon University;University of Washington;University of Washington,1;6;6,24;25;25,
1214,1214,1214,1214,1214,1214,1214,1214,ICLR,2018,Understanding and Exploiting the Low-Rank Structure of Deep Networks,Craig Bakker;Michael J. Henry;Nathan O. Hodas,craig.bakker@pnnl.gov;michael.j.henry@pnnl.gov;nathan.hodas@pnnl.gov,4;5;2,4;4;4,Reject,0,0,0.0,yes,10/27/17,Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory,-1;-1;-1,-1;-1;-1,
1215,1215,1215,1215,1215,1215,1215,1215,ICLR,2018,Adversarially Regularized Autoencoders,Junbo (Jake) Zhao;Yoon Kim;Kelly Zhang;Alexander M. Rush;Yann LeCun,jakezhao@cs.nyu.edu;yoonkim@seas.harvard.edu;kz918@nyu.edu;srush@seas.harvard.edu;yann@cs.nyu.edu,5;6;3;9,4;3;4;3,Invite to Workshop Track,0,7,0.0,yes,10/27/17,New York University;Harvard University;New York University;Harvard University;New York University,26;37;26;37;26,27;6;27;6;27,5;4
1216,1216,1216,1216,1216,1216,1216,1216,ICLR,2018,Expressive power of recurrent neural networks,Valentin Khrulkov;Alexander Novikov;Ivan Oseledets,khrulkov.v@gmail.com;sasha.v.novikov@gmail.com;i.oseledets@skoltech.ru,6;6;6,4;5;3,Accept (Poster),0,4,0.0,yes,10/27/17,Skolkovo Institute of Science and Technology;Higher School of Economics;Skolkovo Institute of Science and Technology,-1;468;-1,-1;377;-1,1
1217,1217,1217,1217,1217,1217,1217,1217,ICLR,2018,The (Un)reliability of saliency methods,Pieter-Jan Kindermans;Sara Hooker;Julius Adebayo;Kristof T. Schütt;Maximilian Alber;Sven Dähne;Dumitru Erhan;Been Kim,pikinder@google.com;shooker@google.com;juliusad@google.com;kristof.schuett@tu-berlin.de;maximilian.aber@tu-berlin.de;sven.daehne@tu-berlin.de;dumitru@google.com;beenkim@google.com,5;4;4,3;4;4,Reject,0,4,0.0,yes,10/27/17,Google;Google;Google;TU Berlin;TU Berlin;TU Berlin;Google;Google,-1;-1;-1;104;104;104;-1;-1,-1;-1;-1;92;92;92;-1;-1,
1218,1218,1218,1218,1218,1218,1218,1218,ICLR,2018,Three factors influencing minima in SGD,Stanisław Jastrzębski;Zac Kenton;Devansh Arpit;Nicolas Ballas;Asja Fischer;Amos Storkey;Yoshua Bengio,staszek.jastrzebski@gmail.com;zakenton@gmail.com;devansh.arpit@umontreal.ca;ballas.n@gmail.com;asja.fischer@gmail.com;a.storkey@ed.ac.uk;yoshua.umontreal@gmail.com,6;3;5,4;4;4,Reject,0,8,0.0,yes,10/27/17,University of Montreal;University of Montreal;University of Montreal;University of Montreal;University of Bonn;University of Edinburgh;University of Montreal,124;124;124;124;124;33;124,108;108;108;108;100;27;108,8
1219,1219,1219,1219,1219,1219,1219,1219,ICLR,2018,A Deep Learning Approach for Survival Clustering without End-of-life Signals,S Chandra Mouli;Bruno Ribeiro;Jennifer Neville,chandr@purdue.edu;ribeiro@cs.purdue.edu;neville@cs.purdue.edu,6;4;6,1;4;5,Reject,0,3,0.0,yes,10/27/17,Purdue University;Purdue University;Purdue University,28;28;28,60;60;60,
1220,1220,1220,1220,1220,1220,1220,1220,ICLR,2018,Predicting Multiple Actions for Stochastic Continuous Control,Sanjeev Kumar;Christian Rupprecht;Federico Tombari;Gregory D. Hager,sanjeev.kumar@in.tum.de;christian.rupprecht@in.tum.de;tombari@in.tum.de;hager@cs.tum.edu,3;7;4,4;3;4,Reject,0,5,0.0,yes,10/27/17,Technical University Munich;Technical University Munich;Technical University Munich;TU Munich,55;55;55;55,41;41;41;34,
1221,1221,1221,1221,1221,1221,1221,1221,ICLR,2018,Multi-Task Learning by Deep Collaboration and Application in Facial Landmark Detection,Ludovic Trottier;Philippe Giguère;Brahim Chaib-draa,ludovic.trottier.1@ulaval.ca;philippe.giguere@ift.ulaval.ca;brahim.chaib-draa@ift.ulaval.ca,5;6;6,5;4;4,Reject,0,6,0.0,yes,10/27/17,Laval university;Laval university;Laval university,468;468;468,265;265;265,8
1222,1222,1222,1222,1222,1222,1222,1222,ICLR,2018,State Space LSTM Models with Particle MCMC Inference,Xun Zheng;Manzil Zaheer;Amr Ahmed;Yuan Wang;Eric P. Xing;Alex Smola,xunzheng90@gmail.com;manzil@cmu.edu;amra@google.com;yuanwang@google.com;epxing@cs.cmu.edu;alex@smola.org,3;5;7,5;5;5,Reject,0,6,0.0,yes,10/27/17,Carnegie Mellon University;Carnegie Mellon University;Google;Google;Carnegie Mellon University;Carnegie-Mellon University,1;1;-1;-1;1;1,24;24;-1;-1;24;24,
1223,1223,1223,1223,1223,1223,1223,1223,ICLR,2018,NOVEL AND EFFECTIVE PARALLEL MIX-GENERATOR GENERATIVE ADVERSARIAL NETWORKS,Xia Xiao;Sanguthevar Rajasekaran,xia.xiao@uconn.edu;sanguthevar.rajasekaran@uconn.edu,3;6;5,5;4;4,Reject,1,1,0.0,yes,10/27/17,University of Connecticut;University of Connecticut,153;153,324;324,5;4
1224,1224,1224,1224,1224,1224,1224,1224,ICLR,2018,Topology Adaptive Graph Convolutional  Networks,Jian Du;Shanghang Zhang;Guanhang Wu;José M. F. Moura;Soummya Kar,jiand@andrew.cmu.edu;shanghaz@andrew.cmu.edu;guanhanw@andrew.cmu.edu;moura@andrew.cmu.edu;soummyak@andrew.cmu.edu,6;4;5,3;4;4,Reject,0,10,0.0,yes,10/27/17,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1;1,24;24;24;24;24,10
1225,1225,1225,1225,1225,1225,1225,1225,ICLR,2018,From Information Bottleneck To Activation Norm Penalty,Allen Nie;Mihir Mongia;James Zou,anie@stanford.edu;mihir.mongia@mssm.edu;jamesz@stanford.edu,7;4;4,3;3;4,Reject,0,2,0.0,yes,10/27/17,Stanford University;Arnhold Institute;Stanford University,4;-1;4,3;-1;3,3;1
1226,1226,1226,1226,1226,1226,1226,1226,ICLR,2018,TESLA: Task-wise Early Stopping and Loss Aggregation for Dynamic Neural Network Inference,Chun-Min Chang;Chia-Ching Lin;Hung-Yi Ou Yang;Chin-Laung Lei;Kuan-Ta Chen,cmchang@iis.sinica.edu.tw;d05921018@ntu.edu.tw;frank840925@gmail.com;cllei@ntu.edu.tw;swc@iis.sinica.edu.tw,4;5;4,4;2;2,Reject,0,3,0.0,yes,10/27/17,Academia Sinica;National Taiwan University;;National Taiwan University;Academia Sinica,-1;85;-1;85;-1,-1;197;-1;197;-1,
1227,1227,1227,1227,1227,1227,1227,1227,ICLR,2018,Stochastic Hyperparameter Optimization through Hypernetworks,Jonathan Lorraine;David Duvenaud,lorraine@cs.toronto.edu;duvenaud@cs.toronto.edu,6;6;6,4;3;1,Reject,0,4,0.0,yes,10/27/17,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",17;17,22;22,
1228,1228,1228,1228,1228,1228,1228,1228,ICLR,2018,Analyzing and Exploiting NARX Recurrent Neural Networks for Long-Term Dependencies,Robert DiPietro;Christian Rupprecht;Nassir Navab;Gregory D. Hager,rdipietro@gmail.com;christian.rupprecht@in.tum.de;nassir.navab@tum.de;hager@cs.jhu.edu,3;7;6,4;5;4,Invite to Workshop Track,0,4,0.0,yes,10/27/17,Johns Hopkins University;Technical University Munich;Technical University Munich;Johns Hopkins University,71;55;55;71,13;41;41;13,3
1229,1229,1229,1229,1229,1229,1229,1229,ICLR,2018,Deep Temporal Clustering: Fully unsupervised learning of time-domain features,Naveen Sai Madiraju;Seid M. Sadat;Dimitry Fisher;Homa Karimabadi,naveen@avlab.ai;behnam@avlab.ai;dimitry@avlab.ai;homa@avlab.ai,3;5;4,5;4;4,Reject,0,0,0.0,yes,10/27/17,Arizona State University;;;,90;-1;-1;-1,126;-1;-1;-1,
1230,1230,1230,1230,1230,1230,1230,1230,ICLR,2018,Hierarchical Adversarially Learned Inference,Mohamed Ishmael Belghazi;Sai Rajeswar;Olivier Mastropietro;Negar Rostamzadeh;Jovana Mitrovic;Aaron Courville,ishmael.belghazi@gmail.com;rajsai24@gmail.com;oli.mastro@gmail.com;negar.rostamzadeh@gmail.com;jovana.mitrovic@spc.ox.ac.uk;aaron.courville@gmail.com,5;5;7,5;5;3,Reject,0,13,0.0,yes,10/27/17,University of Montreal;University of Montreal;University of Montreal;Element AI;University of Oxford;University of Montreal,124;124;124;-1;51;124,108;108;108;-1;1;108,5;4
1231,1231,1231,1231,1231,1231,1231,1231,ICLR,2018,Intriguing Properties of Adversarial Examples,Ekin Dogus Cubuk;Barret Zoph;Samuel Stern Schoenholz;Quoc V. Le,cubuk@google.com;barretzoph@google.com;schsam@google.com;qvl@google.com,5;8;3,2;3;4,Invite to Workshop Track,2,5,0.0,yes,10/27/17,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,4
1232,1232,1232,1232,1232,1232,1232,1232,ICLR,2018,TRL: Discriminative Hints for Scalable Reverse Curriculum Learning,Chen Wang;Xiangyu Chen;Zelin Ye;Jialu Wang;Ziruo Cai;Shixiang Gu;Cewu Lu,jere.wang@sjtu.edu.cn;cxy_1997@sjtu.edu.cn;h_e_r_o@sjtu.edu.cn;faldict@sjtu.edu.cn;sjtu_caiziruo@sjtu.edu.cn;sg717@cam.ac.uk;lucewu@sjtu.edu.cn,4;4;5,4;4;4,Reject,0,4,0.0,yes,10/27/17,Shanghai Jiao Tong University;Shanghai Jiao Tong University;Shanghai Jiao Tong University;Shanghai Jiao Tong University;Shanghai Jiao Tong University;University of Cambridge;Shanghai Jiao Tong University,57;57;57;57;57;71;57,188;188;188;188;188;2;188,
1233,1233,1233,1233,1233,1233,1233,1233,ICLR,2018,Continuous-fidelity Bayesian Optimization with Knowledge Gradient,Jian Wu;Peter I. Frazier,jw926@cornell.edu;pf98@cornell.edu,5;4;6,4;5;5,Reject,0,4,0.0,yes,10/27/17,Cornell University;Cornell University,7;7,19;19,11
1234,1234,1234,1234,1234,1234,1234,1234,ICLR,2018,Neural Program Search: Solving Data Processing Tasks from Description and Examples,Illia Polosukhin;Alexander Skidanov,illia@near.ai;alex@near.ai,4;5;7,4;4;4,Invite to Workshop Track,0,2,0.0,yes,10/27/17,NEAR;NEAR,-1;-1,-1;-1,3
1235,1235,1235,1235,1235,1235,1235,1235,ICLR,2018,GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks,Zhao Chen;Vijay Badrinarayanan;Chen-Yu Lee;Andrew Rabinovich,zchen@magicleap.com;vbadrinarayanan@magicleap.com;clee@magicleap.com;arabinovich@magicleap.com,6;4;4,2;4;4,Reject,0,9,0.0,yes,10/27/17,Magic Leap;Magic Leap;Magic Leap;Magic Leap,-1;-1;-1;-1,-1;-1;-1;-1,
1236,1236,1236,1236,1236,1236,1236,1236,ICLR,2018,Lung Tumor Location and Identification with AlexNet and a Custom CNN,Allison M Rossetto;Wenjin Zhou,allison_rossetto@student.uml.edu;wenjin_zhou@uml.edu,2;3;3,5;4;4,Reject,0,4,0.0,yes,10/27/17,"University of Massachusetts, Lowell;University of Massachusetts, Lowell",-1;-1,-1;-1,
1237,1237,1237,1237,1237,1237,1237,1237,ICLR,2018,Image Transformer,Ashish Vaswani;Niki Parmar;Jakob Uszkoreit;Noam Shazeer;Lukasz Kaiser,avaswani@google.com;nikip@google.com;uszkoreit@google.com;noam@google.com;lukaszkaiser@google.com,6;3;5,4;3;4,Reject,0,3,0.0,yes,10/27/17,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,5
1238,1238,1238,1238,1238,1238,1238,1238,ICLR,2018,Not-So-CLEVR: Visual Relations Strain Feedforward Neural Networks,Junkyung Kim;Matthew Ricci;Thomas Serre,junkyung_kim@brown.edu;matthew_ricci_1@brown.edu;thomas_serre@brown.edu,6;6;6,3;3;4,Invite to Workshop Track,0,6,0.0,yes,10/27/17,Brown University;Brown University;Brown University,62;62;62,50;50;50,
1239,1239,1239,1239,1239,1239,1239,1239,ICLR,2018,No Spurious Local Minima in a Two Hidden Unit ReLU Network,Chenwei Wu;Jiajun Luo;Jason D. Lee,wucw14@mails.tsinghua.edu.cn;jiajunlu@usc.edu;jasonlee@marshall.usc.edu,4;6;6,4;3;2,Invite to Workshop Track,0,3,0.0,yes,10/27/17,Tsinghua University;University of Southern California;University of Southern California,10;31;31,30;66;66,
1240,1240,1240,1240,1240,1240,1240,1240,ICLR,2018,Semi-Supervised Learning via New Deep Network Inversion,Balestriero R.;Roger V.;Glotin H.;Baraniuk R.,randallbalestriero@gmail.com;roger.dyni@gmail.com;herve.glotin@univ-tln.fr;richb@rice.edu,5;4;7,4;5;2,Reject,0,5,0.0,yes,10/27/17,Rice University;;CNRS university Toulon;Rice University,85;-1;468;85,86;-1;1103;86,
1241,1241,1241,1241,1241,1241,1241,1241,ICLR,2018,Stable Distribution Alignment Using the Dual of the Adversarial Distance,Ben Usman;Kate Saenko;Brian Kulis,usmn@bu.edu;saenko@bu.edu;bkulis@bu.edu,5;6;6,4;4;3,Invite to Workshop Track,0,7,0.0,yes,10/27/17,Boston University;Boston University;Boston University,69;69;69,70;70;70,5;4
1242,1242,1242,1242,1242,1242,1242,1242,ICLR,2018,Discovery of Predictive Representations With a Network of General Value Functions,Matthew Schlegel;Andrew Patterson;Adam White;Martha White,mkschleg@ualberta.ca;andnpatt@indiana.edu;amw8@ualberta.ca;whitem@ualberta.ca,4;4;5,1;4;4,Reject,0,6,0.0,yes,10/27/17,University of Alberta;University of Arizona;University of Alberta;University of Alberta,99;181;99;99,119;161;119;119,
1243,1243,1243,1243,1243,1243,1243,1243,ICLR,2018,Generation and Consolidation of Recollections for Efficient Deep Lifelong Learning,Matt Riemer;Michele Franceschini;and Tim Klinger,mdriemer@us.ibm.com;franceschini@us.ibm.com;tklinger@us.ibm.com,5;5;5,2;3;3,Reject,0,4,0.0,yes,10/27/17,International Business Machines;International Business Machines;International Business Machines,-1;-1;-1,-1;-1;-1,5
1244,1244,1244,1244,1244,1244,1244,1244,ICLR,2018,CyCADA: Cycle-Consistent Adversarial Domain Adaptation,Judy Hoffman;Eric Tzeng;Taesung Park;Jun-Yan Zhu;Phillip Isola;Kate Saenko;Alyosha Efros;Trevor Darrell,jhoffman@eecs.berkeley.edu;etzeng@eecs.berkeley.edu;taesung_park@berkeley.edu;junyanz@berkeley.edu;isola@eecs.berkeley.edu;saenko@bu.edu;efros@eecs.berkeley.edu;trevor@eecs.berkeley.edu,5;5;9,5;5;5,Reject,2,5,0.0,yes,10/27/17,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;Boston University;University of California Berkeley;University of California Berkeley,5;5;5;5;5;69;5;5,18;18;18;18;18;70;18;18,5;4;2
1245,1245,1245,1245,1245,1245,1245,1245,ICLR,2018,The Manifold Assumption and Defenses Against Adversarial Perturbations,Xi Wu;Uyeong Jang;Lingjiao Chen;Somesh Jha,xiwu@cs.wisc.edu;wjang@cs.wisc.edu;lchen@cs.wisc.edu;jha@cs.wisc.edu,3;4;5,3;3;3,Reject,0,4,0.0,yes,10/27/17,University of Southern California;University of Southern California;University of Southern California;University of Southern California,31;31;31;31,66;66;66;66,4;8
1246,1246,1246,1246,1246,1246,1246,1246,ICLR,2018,Graph2Seq: Scalable Learning Dynamics for Graphs,Shaileshh Bojja Venkatakrishnan;Mohammad Alizadeh;Pramod Viswanath,bjjvnkt@csail.mit.edu;alizadeh@csail.mit.edu;pramodv@illinois.edu,4;4;4,4;4;3,Reject,0,6,0.0,yes,9/27/18,"Massachusetts Institute of Technology;Massachusetts Institute of Technology;University of Illinois, Urbana Champaign",2;2;3,5;5;37,10;8
1247,1247,1247,1247,1247,1247,1247,1247,ICLR,2018,Reinforcement Learning from Imperfect Demonstrations,Yang Gao;Huazhe(Harry) Xu;Ji Lin;Fisher Yu;Sergey Levine;Trevor Darrell,yg@eecs.berkeley.edu;huazhe_xu@eecs.berkeley.edu;lin-j14@mails.tsinghua.edu.cn;fy@eecs.berkeley.edu;svlevine@eecs.berkeley.edu;trevor@eecs.berkeley.edu,5;6;5,3;5;4,Invite to Workshop Track,1,4,0.0,yes,10/27/17,University of California Berkeley;University of California Berkeley;Tsinghua University;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;10;5;5;5,18;18;30;18;18;18,
1248,1248,1248,1248,1248,1248,1248,1248,ICLR,2018,FigureQA: An Annotated Figure Dataset for Visual Reasoning,Samira Ebrahimi Kahou;Adam Atkinson;Vincent Michalski;Ákos Kádár;Adam Trischler;Yoshua Bengio,samira.ebrahimi@microsoft.com;adatkins@microsoft.com;vincent.michalski@umontreal.ca;kadar.akos@gmail.com;adam.trischler@microsoft.com;yoshua.bengio@umontreal.ca,6;6;6,4;3;4,Invite to Workshop Track,0,10,0.0,yes,10/27/17,Microsoft;Microsoft;University of Montreal;;Microsoft;University of Montreal,-1;-1;124;-1;-1;124,-1;-1;108;-1;-1;108,10
1249,1249,1249,1249,1249,1249,1249,1249,ICLR,2018,Seq2SQL: Generating Structured Queries From Natural Language Using Reinforcement Learning ,Victor Zhong;Caiming Xiong;Richard Socher,victor@victorzhong.com;cxiong@salesforce.com;richard@socher.org,5;4;5,5;4;4,Reject,7,4,0.0,yes,10/27/17,SalesForce.com;SalesForce.com;SalesForce.com,-1;-1;-1,-1;-1;-1,3
1250,1250,1250,1250,1250,1250,1250,1250,ICLR,2018,Learning to select examples for program synthesis,Yewen Pu;Zachery Miranda;Armando Solar-Lezama;Leslie Pack Kaelbling,yewenpu@mit.edu;zmiranda@mit.edu;asolar@csail.mit.edu;lpk@csail.mit.edu,4;5;5,4;4;3,Reject,0,3,0.0,yes,10/27/17,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2,5;5;5;5,
1251,1251,1251,1251,1251,1251,1251,1251,ICLR,2018,Neuron as an Agent,Shohei Ohsawa;Kei Akuzawa;Tatsuya Matsushima;Gustavo Bezerra;Yusuke Iwasawa;Hiroshi Kajino;Seiya Takenaka;Yutaka Matsuo,ohsawa@weblab.t.u-tokyo.ac.jp;akuzawa-kei@weblab.t.u-tokyo.ac.jp;matsushima@weblab.t.u-tokyo.ac.jp;gustavo@weblab.t.u-tokyo.ac.jp;iwasawa@weblab.t.u-tokyo.ac.jp;kjn@jp.ibm.com;s.takenaka@aediworks.com;matsuo@weblab.t.u-tokyo.ac.jp,6;7;3,4;3;5,Invite to Workshop Track,6,5,1.0,yes,10/27/17,The University of Tokyo;The University of Tokyo;The University of Tokyo;The University of Tokyo;The University of Tokyo;International Business Machines;Aediworks;The University of Tokyo,52;52;52;52;52;-1;-1;52,45;45;45;45;45;-1;-1;45,
1252,1252,1252,1252,1252,1252,1252,1252,ICLR,2018,Toward predictive machine learning for active vision,Emmanuel Daucé,emmanuel.dauce@centrale-marseille.fr,5;3;3,2;4;5,Reject,0,5,0.0,yes,10/27/17,Centrale Marseille,-1,-1,2
1253,1253,1253,1253,1253,1253,1253,1253,ICLR,2018,Value Propagation Networks,Nantas Nardelli;Gabriel Synnaeve;Zeming Lin;Pushmeet Kohli;Nicolas Usunier,nantas@robots.ox.ac.uk;gab@fb.com;zlin@fb.com;pushmeet@google.com;usunier@fb.com,5;7;5,4;3;2,Invite to Workshop Track,0,3,0.0,yes,9/27/18,University of Oxford;Facebook;Facebook;Google;Facebook,51;-1;-1;-1;-1,1;-1;-1;-1;-1,
1254,1254,1254,1254,1254,1254,1254,1254,ICLR,2018,Correcting Nuisance Variation using Wasserstein Distance,Gil Tabak;Minjie Fan;Samuel J. Yang;Stephan Hoyer;Geoff Davis,tabak.gil@gmail.com;mjfan@google.com;samuely@google.com;shoyer@google.com;geoffd@google.com,5;4;7,3;5;3,Reject,0,6,0.0,yes,10/27/17,Stanford University;Google;Google;Google;Google,4;-1;-1;-1;-1,3;-1;-1;-1;-1,
1255,1255,1255,1255,1255,1255,1255,1255,ICLR,2018,Parametric Adversarial Divergences are Good Task Losses for Generative Modeling,Gabriel Huang;Hugo Berard;Ahmed Touati;Gauthier Gidel;Pascal Vincent;Simon Lacoste-Julien,gbxhuang@gmail.com;berard.hugo@gmail.com;ahmed.touati@umontreal.ca;gauthier.gidel@inria.fr;pascal.vincent@umontreal.ca;slacoste@iro.umontreal.ca,6;4;4,3;4;3,Invite to Workshop Track,0,12,1.0,yes,10/27/17,University of Montreal;University of Montreal;University of Montreal;INRIA;University of Montreal;University of Montreal,124;124;124;-1;124;124,108;108;108;-1;108;108,5;4
1256,1256,1256,1256,1256,1256,1256,1256,ICLR,2018,Adaptive Memory Networks,Daniel Li;Asim Kadav,li.daniel@berkeley.edu;asim@nec-labs.com,5;7;4,4;5;3,Invite to Workshop Track,2,7,0.0,yes,10/27/17,University of California Berkeley;NEC-Labs,5;-1,18;-1,
1257,1257,1257,1257,1257,1257,1257,1257,ICLR,2018,Discrete Autoencoders for Sequence Models,Lukasz Kaiser;Samy Bengio,lukaszkaiser@google.com;bengio@google.com,5;4;6,5;4;1,Reject,0,4,0.0,yes,10/27/17,Google;Google,-1;-1,-1;-1,3
1258,1258,1258,1258,1258,1258,1258,1258,ICLR,2018,“Style” Transfer for Musical Audio Using Multiple Time-Frequency Representations,Shaun Barry;Youngmoo Kim,smb484@drexel.edu;ykim@drexel.edu,6;4;7,4;4;3,Reject,0,2,0.0,yes,10/27/17,Drexel University;Drexel University,291;291,392;392,
1259,1259,1259,1259,1259,1259,1259,1259,ICLR,2018,Model-based imitation learning from state trajectories,Subhajit Chaudhury;Daiki Kimura;Tadanobu Inoue;Ryuki Tachibana,subhajit@jp.ibm.com;daiki@jp.ibm.com;inouet@jp.ibm.com;ryuki@jp.ibm.com,7;4;3,3;4;5,Reject,1,3,0.0,yes,10/27/17,International Business Machines;International Business Machines;International Business Machines;International Business Machines,-1;-1;-1;-1,-1;-1;-1;-1,
1260,1260,1260,1260,1260,1260,1260,1260,ICLR,2018,Neural Task Graph Execution,Sungryull Sohn;Junhyuk Oh;Honglak Lee,srsohn@umich.edu;junhyuk@umich.edu;honglak@eecs.umich.edu,6;6;4,4;3;4,Reject,0,4,0.0,yes,10/27/17,University of Michigan;University of Michigan;University of Michigan,8;8;8,21;21;21,10
1261,1261,1261,1261,1261,1261,1261,1261,ICLR,2018,Efficient Exploration through Bayesian   Deep Q-Networks,Kamyar Azizzadenesheli;Emma Brunskill;Animashree Anandkumar,kazizzad@uci.edu;ebrun@cs.stanford.edu;animakumar@gmail.com,6;5;5,4;4;4,Reject,2,14,0.0,yes,10/27/17,"University of California, Irvine;Stanford University;University of California-Irvine",36;4;36,99;3;99,11
1262,1262,1262,1262,1262,1262,1262,1262,ICLR,2018,UNSUPERVISED SENTENCE EMBEDDING USING DOCUMENT STRUCTURE-BASED CONTEXT,Taesung Lee;Youngja Park,taesung.lee@ibm.com;young_park@us.ibm.com,7;5;5,4;4;4,Reject,0,7,0.0,yes,10/27/17,International Business Machines;International Business Machines,-1;-1,-1;-1,
1263,1263,1263,1263,1263,1263,1263,1263,ICLR,2018,Time-Dependent Representation for Neural Event Sequence Prediction,Yang Li;Nan Du;Samy Bengio,liyang@google.com;dunan@google.com;bengio@google.com,4;4;5,4;5;3,Invite to Workshop Track,0,4,0.0,yes,10/27/17,Google;Google;Google,-1;-1;-1,-1;-1;-1,3
1264,1264,1264,1264,1264,1264,1264,1264,ICLR,2018,Directing Generative Networks with Weighted Maximum Mean Discrepancy,Maurice Diesendruck;Guy W. Cole;Sinead Williamson,momod@utexas.edu;guywcole@utexas.edu;sinead.williamson@mccombs.utexas.edu,4;4;4,4;4;5,Reject,0,3,0.0,yes,10/27/17,"University of Texas, Austin;University of Texas, Austin;University of Texas, Austin",21;21;21,49;49;49,5;7
1265,1265,1265,1265,1265,1265,1265,1265,ICLR,2018,Decoding Decoders: Finding Optimal Representation Spaces for Unsupervised Similarity Tasks,Vitalii Zhelezniak;Dan Busbridge;April Shen;Samuel L. Smith;Nils Y. Hammerla,vitali.zhelezniak@babylonhealth.com;dan.busbridge@babylonhealth.com;april.shen@babylonhealth.com;slsmith@google.com;nils.hammerla@babylonhealth.com,6;5;4,4;5;4,Invite to Workshop Track,0,6,0.0,yes,10/27/17,babylon health;babylon health;babylon health;Google;babylon health,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,3
1266,1266,1266,1266,1266,1266,1266,1266,ICLR,2018,Learning Parametric Closed-Loop Policies for Markov Potential Games,Sergio Valcarcel Macua;Javier Zazo;Santiago Zazo,sergio@prowler.io;javier.zazo.ruiz@upm.es;santiago@gaps.ssr.upm.es,7;6;6,2;3;1,Accept (Poster),0,4,0.0,yes,10/27/17,Prowler.io;Universidad Politécnica de Madrid;Universidad Politécnica de Madrid,-1;-1;-1,-1;-1;-1,
1267,1267,1267,1267,1267,1267,1267,1267,ICLR,2018,Soft Value Iteration Networks for Planetary Rover Path Planning,Max Pflueger;Ali Agha;Gaurav S. Sukhatme,mpflueger@gmail.com;aliahga@jpl.nasa.gov;gaurav@usc.edu,3;3;4,5;3;4,Reject,0,3,0.0,yes,10/27/17,University of Southern California;NASA;University of Southern California,31;-1;31,66;-1;66,
1268,1268,1268,1268,1268,1268,1268,1268,ICLR,2018,A Classification-Based Perspective on GAN Distributions,Shibani Santurkar;Ludwig Schmidt;Aleksander Madry,shibani@mit.edu;ludwigs@mit.edu;madry@mit.edu,5;6;3,5;4;4,Reject,0,3,0.0,yes,10/27/17,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2,5;5;5,5;4
1269,1269,1269,1269,1269,1269,1269,1269,ICLR,2018,Inference Suboptimality in Variational Autoencoders,Chris Cremer;Xuechen Li;David Duvenaud,ccremer@cs.toronto.edu;lxuechen@cs.toronto.edu;duvenaud@cs.toronto.edu,6;6;6,5;5;4,Invite to Workshop Track,0,4,0.0,yes,10/27/17,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",17;17;17,22;22;22,5;1
1270,1270,1270,1270,1270,1270,1270,1270,ICLR,2018,Achieving Strong Regularization for Deep Neural Networks,Dae Hoon Park;Chiu Man Ho;Yi Chang,pdhvip@gmail.com;chiuman100@gmail.com;yi.chang@huawei.com,6;4;5,2;5;5,Reject,0,5,0.0,yes,10/27/17,Huawei Technologies Ltd.;;Huawei Technologies Ltd.,-1;-1;-1,-1;-1;-1,8
1271,1271,1271,1271,1271,1271,1271,1271,ICLR,2018,A Spectral Approach to Generalization and Optimization in Neural Networks,Farzan Farnia;Jesse Zhang;David Tse,farnia@stanford.edu;jessez@stanford.edu;dntse@stanford.edu,6;6;4,3;3;4,Reject,1,11,0.0,yes,10/27/17,Stanford University;Stanford University;Stanford University,4;4;4,3;3;3,1;8
1272,1272,1272,1272,1272,1272,1272,1272,ICLR,2018,Small Coresets to Represent Large Training Data for Support Vector Machines,Cenk Baykal;Murad Tukan;Dan Feldman;Daniela Rus,baykal@mit.edu;muradtuk@gmail.com;dannyf.post@gmail.com;rus@csail.mit.edu,5;7;5,3;3;4,Reject,0,8,0.0,yes,10/27/17,Massachusetts Institute of Technology;University of Haifa;University of Haifa;Massachusetts Institute of Technology,2;153;153;2,5;608;608;5,
1273,1273,1273,1273,1273,1273,1273,1273,ICLR,2018,Investigating Human Priors for Playing Video Games,Rachit Dubey;Pulkit Agrawal;Deepak Pathak;Thomas L. Griffiths;Alexei A. Efros,rach0012@berkeley.edu;pulkitag@berkeley.edu;pathak@berkeley.edu;tom_griffiths@berkeley.edu;efros@eecs.berkeley.edu,4;5;7,3;4;4,Invite to Workshop Track,0,6,0.0,yes,10/27/17,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5;5,18;18;18;18;18,
1274,1274,1274,1274,1274,1274,1274,1274,ICLR,2018,Learning Deep Models: Critical Points and Local Openness,Maher Nouiehed;Meisam Razaviyayn,nouiehed@usc.edu;razaviya@usc.edu,6;5;6,4;4;4,Invite to Workshop Track,0,3,0.0,yes,10/27/17,University of Southern California;University of Southern California,31;31,66;66,
1275,1275,1275,1275,1275,1275,1275,1275,ICLR,2018,Contextual Explanation Networks,Maruan Al-Shedivat;Avinava Dubey;Eric P. Xing,alshedivat@cs.cmu.edu;akdubey@cs.cmu.edu;epxing@cs.cmu.edu,6;6;6,5;2;3,Reject,0,4,0.0,yes,10/27/17,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1,24;24;24,1;10
1276,1276,1276,1276,1276,1276,1276,1276,ICLR,2018,LSH Softmax: Sub-Linear Learning and Inference of the Softmax Layer in Deep Architectures,Daniel Levy;Danlu Chan;Stefano Ermon,danilevy@cs.stanford.edu;taineleau@gmail.com;ermon@cs.stanford.edu,5;5;5,4;3;4,Reject,7,3,0.0,yes,10/27/17,Stanford University;Fudan University;Stanford University,4;78;4,3;116;3,3
1277,1277,1277,1277,1277,1277,1277,1277,ICLR,2018,Recurrent Relational Networks for complex relational reasoning,Rasmus Berg Palm;Ulrich Paquet;Ole Winther,rasmusbergpalm@gmail.com;upaq@google.com;olwi@dtu.dk,3;5;5,5;3;3,Reject,0,4,0.0,yes,10/27/17,Technical University of Denmark;Google;Technical University of Denmark,210;-1;210,153;-1;153,
1278,1278,1278,1278,1278,1278,1278,1278,ICLR,2018,Influence-Directed Explanations for Deep Convolutional Networks,Anupam Datta;Matt Fredrikson;Klas Leino;Linyi Li;Shayak Sen,danupam@cmu.edu;mfredrik@cs.cmu.edu;kleino@cs.cmu.edu;ly-li14@mails.tsinghua.edu.cn;shayaks@cs.cmu.edu,5;4;4,3;5;3,Reject,0,0,0.0,yes,10/27/17,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Tsinghua University;Carnegie Mellon University,1;1;1;10;1,24;24;24;30;24,
1279,1279,1279,1279,1279,1279,1279,1279,ICLR,2018,Recurrent Neural Networks with Top-k Gains for Session-based Recommendations,Balázs Hidasi;Alexandros Karatzoglou,hidasib@gmail.com;alexk@tid.es,8;4;6,4;5;5,Reject,0,3,0.0,yes,10/27/17,Gravity R&D;Telefonica Research,-1;-1,-1;-1,
1280,1280,1280,1280,1280,1280,1280,1280,ICLR,2018, Explicit Induction Bias for Transfer Learning with Convolutional Networks,Xuhong LI;Yves GRANDVALET;Franck DAVOINE,xuhong.li@utc.fr;yves.grandvalet@utc.fr;franck.davoine@utc.fr,6;7;6,5;4;4,Reject,0,4,1.0,yes,10/27/17,Université de technologie de Compiègne;Université de technologie de Compiègne;Université de technologie de Compiègne,-1;-1;-1,-1;-1;-1,6
1281,1281,1281,1281,1281,1281,1281,1281,ICLR,2018,Simple Nearest Neighbor Policy Method for Continuous Control Tasks,Elman Mansimov;Kyunghyun Cho,mansimov@cs.nyu.edu;kyunghyun.cho@nyu.edu,4;4;3,5;4;5,Reject,0,6,0.0,yes,10/27/17,New York University;New York University,26;26,27;27,
1282,1282,1282,1282,1282,1282,1282,1282,ICLR,2018,Deep Mean Field Theory: Layerwise Variance and Width Variation as Methods to Control Gradient Explosion,Greg Yang;Sam S. Schoenholz,gregyang@microsoft.com;schsam@google.com,7;5;5,3;1;3,Invite to Workshop Track,0,6,0.0,yes,10/27/17,Microsoft;Google,-1;-1,-1;-1,
1283,1283,1283,1283,1283,1283,1283,1283,ICLR,2018,Optimizing the Latent Space of Generative Networks,Piotr Bojanowski;Armand Joulin;David Lopez-Paz;Arthur Szlam,bojanowski@fb.com;ajoulin@fb.com;dlp@fb.com;aszlam@fb.com,4;6;6,4;4;3,Reject,0,3,0.0,yes,10/27/17,Facebook;Facebook;Facebook;Facebook,-1;-1;-1;-1,-1;-1;-1;-1,5;4
1284,1284,1284,1284,1284,1284,1284,1284,ICLR,2018,Accelerating Neural Architecture Search using Performance Prediction,Bowen Baker*;Otkrist Gupta*;Ramesh Raskar;Nikhil Naik,bowen@mit.edu;otkrist@mit.edu;raskar@mit.edu;naik@mit.edu,6;6;4,4;5;3,Invite to Workshop Track,0,5,0.0,yes,10/27/17,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2,5;5;5;5,3;11
1285,1285,1285,1285,1285,1285,1285,1285,ICLR,2018,Data-efficient Deep Reinforcement Learning for Dexterous Manipulation,Ivo Popov;Nicolas Heess;Timothy P. Lillicrap;Roland Hafner;Gabriel Barth-Maron;Matej Vecerik;Thomas Lampe;Tom Erez;Yuval Tassa;Martin Riedmiller,ivaylo.popov@hotmail.com;heess@google.com;countzero@google.com;rhafner@google.com;gabrielbm@google.com;matejvecerik@google.com;thomaslampe@google.com;etom@google.com;tassa@google.com;riedmiller@google.com,4;2;3,4;5;4,Reject,1,0,0.0,yes,10/27/17,;Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,
1286,1286,1286,1286,1286,1286,1286,1286,ICLR,2018,Routing Networks: Adaptive Selection of Non-Linear Functions for Multi-Task Learning,Clemens Rosenbaum;Tim Klinger;Matthew Riemer,crosenbaum@umass.edu;tklinger@us.ibm.com;mdriemer@us.ibm.com,7;6;8,3;3;4,Accept (Poster),0,9,0.0,yes,10/27/17,"University of Massachusetts, Amherst;International Business Machines;International Business Machines",30;-1;-1,191;-1;-1,
1287,1287,1287,1287,1287,1287,1287,1287,ICLR,2018,Gradients explode - Deep Networks are shallow - ResNet explained,George Philipp;Dawn Song;Jaime G. Carbonell,george.philipp@email.de;dawnsong@gmail.com;jgc@cs.cmu.edu,3;5;8,2;4;1,Invite to Workshop Track,5,6,2.0,yes,10/27/17,Carnegie Mellon University;University of California Berkeley;Carnegie Mellon University,1;5;1,24;18;24,
1288,1288,1288,1288,1288,1288,1288,1288,ICLR,2018,GraphVAE: Towards Generation of Small Graphs Using Variational Autoencoders,Martin Simonovsky;Nikos Komodakis,simonovm@imagine.enpc.fr;nikos.komodakis@enpc.fr,5;7;7,3;2;4,Reject,0,13,0.0,yes,10/27/17,ENPC;ENPC,468;468,280;280,5;10
1289,1289,1289,1289,1289,1289,1289,1289,ICLR,2018,UPS: optimizing Undirected Positive Sparse graph for neural graph filtering,Mikhail Yurochkin;Dung Thai;Hung Hai Bui;XuanLong Nguyen,moonfolk@umich.edu;dthai@iesl.cs.umass.edu;bui.h.hung@gmail.com;xuanlong@umich.edu,6;3;4,3;3;3,Reject,0,4,0.0,yes,10/27/17,"University of Michigan;University of Massachusetts, Amherst;Google;University of Michigan",8;30;-1;8,21;191;-1;21,10
1290,1290,1290,1290,1290,1290,1290,1290,ICLR,2018,Learning Document Embeddings With CNNs,Shunan Zhao;Chundi Lui;Maksims Volkovs,shunan@layer6.ai;chundi@layer6.ai;maksims.volkovs@gmail.com,6;4;2,4;3;5,Reject,3,6,0.0,yes,10/27/17,Layer 6 AI;Layer 6 AI;,-1;-1;-1,-1;-1;-1,3
1291,1291,1291,1291,1291,1291,1291,1291,ICLR,2018,Predict Responsibly: Increasing Fairness by Learning to Defer,David Madras;Toniann Pitassi;Richard Zemel,david.madras@mail.utoronto.ca;zemel@cs.toronto.edu;toni@cs.toronto.edu,5;4;6,3;5;3,Invite to Workshop Track,0,5,0.0,yes,10/27/17,"Toronto University;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",17;17;17,22;22;22,7
1292,1292,1292,1292,1292,1292,1292,1292,ICLR,2018,Reward Estimation via State Prediction,Daiki Kimura;Subhajit Chaudhury;Ryuki Tachibana;Sakyasingha Dasgupta,daiki@jp.ibm.com;subhajit@jp.ibm.com;ryuki@jp.ibm.com;sakya@leapmind.io,4;5;3,4;3;4,Reject,0,3,0.0,yes,10/27/17,"International Business Machines;International Business Machines;International Business Machines;LeapMind, Inc.",-1;-1;-1;-1,-1;-1;-1;-1,5
1293,1293,1293,1293,1293,1293,1293,1293,ICLR,2018,Sparse Attentive Backtracking: Long-Range Credit Assignment in Recurrent Networks,Nan Rosemary Ke;Anirudh Goyal;Olexa Bilaniuk;Jonathan Binas;Laurent Charlin;Chris Pal;Yoshua Bengio,rosemary.nan.ke@gmail.com;anirudhgoyal9119@gmail.com;obilaniu@gmail.com;jbinas@gmail.com;lcharlin@gmail.com;chris.j.pal@gmail.com;yoshua.umontreal@gmail.com,5;5;8,3;4;4,Reject,0,7,0.0,yes,10/27/17,Polytechnique Montreal;University of Montreal;University of Montreal;University of Montreal;University of Montreal;Ecole Polytechnique de Montreal;University of Montreal,364;124;124;124;124;365;124,108;108;108;108;108;1103;108,
1294,1294,1294,1294,1294,1294,1294,1294,ICLR,2018,GraphGAN: Generating Graphs via Random Walks,Aleksandar Bojchevski;Oleksandr Shchur;Daniel Zügner;Stephan Günnemann,a.bojchevski@in.tum.de;shchur@in.tum.de;daniel.zuegner@gmail.com;guennemann@in.tum.de,6;7;4,4;4;5,Reject,4,12,0.0,yes,10/27/17,Technical University Munich;Technical University Munich;Technical University Munich;Technical University Munich,55;55;55;55,41;41;41;41,10;5;8
1295,1295,1295,1295,1295,1295,1295,1295,ICLR,2018,LEAP: Learning Embeddings for Adaptive Pace,Vithursan Thangarasa;Graham W. Taylor,vthangar@uoguelph.ca;gwtaylor@uoguelph.ca,6;3;4,3;4;4,Reject,0,2,0.0,yes,10/27/17,University of Guelph;University of Guelph,291;291,1103;1103,
1296,1296,1296,1296,1296,1296,1296,1296,ICLR,2018,Prototype Matching Networks for Large-Scale Multi-label  Genomic Sequence Classification,Jack Lanchantin;Arshdeep Sekhon;Ritambhara Singh;Yanjun Qi,jjl5sw@virginia.edu;as5cu@virginia.edu;rs3zz@virginia.edu;yq2h@virginia.edu,5;5;5,4;5;3,Reject,0,15,0.0,yes,10/27/17,University of Virginia;University of Virginia;University of Virginia;University of Virginia,62;62;62;62,113;113;113;113,6
1297,1297,1297,1297,1297,1297,1297,1297,ICLR,2018,Forced Apart: Discovering Disentangled Representations Without Exhaustive Labels,Alexey Romanov;Anna Rumshisky,jgc128@outlook.com;arum@cs.uml.edu,5;5;4,5;4;4,Reject,0,4,0.0,yes,10/27/17,"University of Massachusetts, Lowell;University of Massachusetts, Lowell",-1;-1,-1;-1,
1298,1298,1298,1298,1298,1298,1298,1298,ICLR,2018,Beyond Shared Hierarchies: Deep Multitask Learning through Soft Layer Ordering,Elliot Meyerson;Risto Miikkulainen,ekm@cs.utexas.edu;risto@cs.utexas.edu,7;6;7,4;3;4,Accept (Poster),0,5,0.0,yes,10/27/17,"University of Texas, Austin;University of Texas, Austin",21;21,49;49,
1299,1299,1299,1299,1299,1299,1299,1299,ICLR,2018,Attention-based Graph Neural Network for Semi-supervised Learning,Kiran K. Thekumparampil;Sewoong Oh;Chong Wang;Li-Jia Li,kirankoshy@gmail.com;sewoong79@gmail.com;chongw@google.com;lijiali@cs.stanford.edu,6;6;7,3;2;4,Reject,4,5,0.0,yes,10/27/17,"University of Illinois, Urbana Champaign;University of Illinois at Urbana-Champaign;Google;Stanford University",3;3;-1;4,37;37;-1;3,10
1300,1300,1300,1300,1300,1300,1300,1300,ICLR,2018,Attacking Binarized Neural Networks,Angus Galloway;Graham W. Taylor;Medhat Moussa,gallowaa@uoguelph.ca;gwtaylor@uoguelph.ca;mmoussa@uoguelph.ca,7;7;6,3;4;5,Accept (Poster),1,3,0.0,yes,10/27/17,University of Guelph;University of Guelph;University of Guelph,291;291;291,1103;1103;1103,4
1301,1301,1301,1301,1301,1301,1301,1301,ICLR,2018,A Semantic Loss Function for Deep Learning with Symbolic Knowledge,Jingyi Xu;Zilu Zhang;Tal Friedman;Yitao Liang;Guy Van den Broeck,jixu@g.ucla.edu;zhangzilu@pku.edu.cn;tal@cs.ucla.edu;yliang@cs.ucla.edu;guyvdb@cs.ucla.edu,7;5;4,3;3;4,Reject,0,3,0.0,yes,10/27/17,"University of California, Los Angeles;Peking University;University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles",20;24;20;20;20,15;27;15;15;15,
1302,1302,1302,1302,1302,1302,1302,1302,ICLR,2018,Representing Entropy : A short proof of the equivalence between soft Q-learning and policy gradients,Pierre H. Richemond;Brendan Maginnis,phr17@imperial.ac.uk;b.maginnis@imperial.ac.uk,5;2;5,5;5;4,Reject,0,3,0.0,yes,10/27/17,Imperial College London;Imperial College London,74;74,8;8,1
1303,1303,1303,1303,1303,1303,1303,1303,ICLR,2018,Generative Discovery of Relational Medical Entity Pairs,Chenwei Zhang;Yaliang Li;Nan Du;Wei Fan;Philip S. Yu,czhang99@uic.edu;yaliangli@baidu.com;nandu@baidu.com;davidwfan@tencent.com;psyu@uic.edu,4;4;2,3;4;5,Reject,0,3,1.0,yes,10/27/17,"University of Illinois, Chicago;Baidu;Baidu;Tencent AI Lab;University of Illinois, Chicago",57;-1;-1;-1;57,255;-1;-1;-1;255,5
1304,1304,1304,1304,1304,1304,1304,1304,ICLR,2018,Understanding GANs: the LQG Setting,Soheil Feizi;Changho Suh;Fei Xia;David Tse,sfeizi@stanford.edu;chsuh@kaist.ac.kr;feixia@stanford.edu;dntse@stanford.edu,4;4;5,4;5;4,Reject,8,2,0.0,yes,10/27/17,Stanford University;Korea Advanced Institute of Science and Technology;Stanford University;Stanford University,4;21;4;4,3;95;3;3,5;4
1305,1305,1305,1305,1305,1305,1305,1305,ICLR,2018,Synthesizing Robust Adversarial Examples,Anish Athalye;Logan Engstrom;Andrew Ilyas;Kevin Kwok,aathalye@mit.edu;engstrom@mit.edu;ailyas@mit.edu;kevink16@gmail.com,5;6;8,4;4;3,Reject,9,8,0.0,yes,10/27/17,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2,5;5;5;5,4
1306,1306,1306,1306,1306,1306,1306,1306,ICLR,2018,Extending the Framework of Equilibrium Propagation to General Dynamics,Benjamin Scellier;Anirudh Goyal;Jonathan Binas;Thomas Mesnard;Yoshua Bengio,benjamin.scellier@polytechnique.edu;anirudhgoyal9119@gmail.com;jbinas@gmail.com;thomas.mesnard@gmail.com;yoshua.umontreal@gmail.com,4;3;6,4;4;2,Invite to Workshop Track,0,3,0.0,yes,10/27/17,Ecole polytechnique;University of Montreal;University of Montreal;University of Montreal;University of Montreal,468;124;124;124;124,115;108;108;108;108,8
1307,1307,1307,1307,1307,1307,1307,1307,ICLR,2018,Lifelong Word Embedding via Meta-Learning,Hu Xu;Bing Liu;Lei Shu;Philip S. Yu,hxu48@uic.edu;liub@uic.edu;lshu3@uic.edu;psyu@uic.edu,4;5;3,4;4;4,Reject,0,0,0.0,yes,10/27/17,"University of Illinois, Chicago;University of Illinois, Chicago;University of Illinois, Chicago;University of Illinois, Chicago",57;57;57;57,255;255;255;255,3;6
1308,1308,1308,1308,1308,1308,1308,1308,ICLR,2018,A comparison of second-order methods for deep convolutional neural networks,Patrick H. Chen;Cho-jui Hsieh,phpchen@ucdavis.edu;chohsieh@ucdavis.edu,5;6;3,5;3;4,Reject,0,3,0.0,yes,10/27/17,"University of California, Davis;University of California, Davis",78;78,54;54,
1309,1309,1309,1309,1309,1309,1309,1309,ICLR,2018,Automatic Parameter Tying in Neural Networks,Yibo Yang;Nicholas Ruozzi;Vibhav Gogate,yibo.yang@utdallas.edu;nicholas.ruozzi@utdallas.edu;vgogate@hlt.utdallas.edu,6;6;6,5;4;4,Reject,0,4,0.0,yes,10/27/17,"University of Texas, Dallas;University of Texas, Dallas;University of Texas, Dallas",85;85;85,239;239;239,
1310,1310,1310,1310,1310,1310,1310,1310,ICLR,2018,Learning to Treat Sepsis with Multi-Output Gaussian Process Deep Recurrent Q-Networks,Joseph Futoma;Anthony Lin;Mark Sendak;Armando Bedoya;Meredith Clement;Cara O'Brien;Katherine Heller,jfutoma14@gmail.com;anthony.lin@duke.edu;mark.sendak@duke.edu;armando.bedoya@duke.edu;meredith.edwards@duke.edu;cara.obrien@duke.edu;kheller@gmail.com,6;3;4,3;4;4,Reject,0,8,0.0,yes,10/27/17,Duke University;Duke University;Duke University;Duke University;Duke University;Duke University;Duke University,46;46;46;46;46;46;46,17;17;17;17;17;17;17,
1311,1311,1311,1311,1311,1311,1311,1311,ICLR,2018,Latent forward model for Real-time Strategy game planning with incomplete information,Yuandong Tian;Qucheng Gong,yuandong@fb.com;qucheng@fb.com,5;4;4,4;5;3,Reject,0,1,0.0,yes,10/27/17,Facebook;Facebook,-1;-1,-1;-1,
1312,1312,1312,1312,1312,1312,1312,1312,ICLR,2018,WHAI: Weibull Hybrid Autoencoding Inference for Deep Topic Modeling,Hao Zhang;Bo Chen;Dandan Guo;Mingyuan Zhou,zhanghao_xidian@163.com;bchen@mail.xidian.edu.cn;gdd_xidian@126.com;mzhou@utexas.edu,6;6;5,4;2;4,Accept (Poster),0,3,1.0,yes,10/27/17,"Xidian University;Tsinghua University;Xidian University;University of Texas, Austin",468;10;468;21,917;30;917;49,5;1
1313,1313,1313,1313,1313,1313,1313,1313,ICLR,2018,Faster Distributed Synchronous SGD with Weak Synchronization,Cong Xie;Oluwasanmi O. Koyejo;Indranil Gupta,cx2@illinois.edu;sanmi@illinois.edu;indy@illinois.edu,4;3;4,5;4;5,Reject,0,0,0.0,yes,10/27/17,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",3;3;3,37;37;37,
1314,1314,1314,1314,1314,1314,1314,1314,ICLR,2018,Deep Learning for Physical Processes: Incorporating Prior Scientific Knowledge,Emmanuel de Bezenac;Arthur Pajot;Patrick Gallinari,emmanuel.de_bezenac@lip6.fr;arthur.pajot@lip6.fr;patrick.gallinari@lip6.fr,7;7;6,3;3;2,Accept (Poster),6,4,2.0,yes,10/27/17,LIP6;LIP6;LIP6,-1;-1;-1,-1;-1;-1,
1315,1315,1315,1315,1315,1315,1315,1315,ICLR,2018,DeepArchitect: Automatically Designing and Training Deep Architectures,Renato Negrinho;Geoff Gordon,negrinho@cs.cmu.edu;ggordon@cs.cmu.edu,4;5;4,5;3;5,Reject,0,6,0.0,yes,10/27/17,Carnegie Mellon University;Carnegie Mellon University,1;1,24;24,10
1316,1316,1316,1316,1316,1316,1316,1316,ICLR,2018,Improving generalization by regularizing in $L^2$ function space,Ari S Benjamin;Konrad Kording,aarrii@seas.upenn.edu,6;5;4,3;4;3,Reject,0,1,0.0,yes,10/27/17,University of Pennsylvania,19,10,8
1317,1317,1317,1317,1317,1317,1317,1317,ICLR,2018,"Fast and Accurate Text Classification: Skimming, Rereading and Early Stopping",Keyi Yu;Yang Liu;Alexander G. Schwing;Jian Peng,yu-ky14@mails.tsinghua.edu.cn;liu301@illinois.edu;aschwing@illinois.edu;jianpeng@illinois.edu,5;5;7,4;3;4,Invite to Workshop Track,3,6,0.0,yes,10/27/17,"Tsinghua University;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",10;3;3;3,30;37;37;37,3
1318,1318,1318,1318,1318,1318,1318,1318,ICLR,2018,Gaussian Process Neurons,Sebastian Urban;Patrick van der Smagt,surban@tum.de;smagt@brml.org,5;4;7,4;5;2,Reject,0,1,0.0,yes,10/27/17,Technical University Munich;TU Munich,55;55,41;34,11
1319,1319,1319,1319,1319,1319,1319,1319,ICLR,2018,Neural Tree Transducers for Tree to Tree Learning,João Sedoc;Dean Foster;Lyle Ungar,joao@cis.upenn.edu;dean@foster.net;ungar@cis.upenn.edu,3;7;2,4;4;5,Reject,0,0,0.0,yes,10/27/17,University of Pennsylvania;University of Pennsylvania;University of Pennsylvania,19;19;19,10;10;10,
1320,1320,1320,1320,1320,1320,1320,1320,ICLR,2018,Network of Graph Convolutional Networks Trained on Random Walks,Sami Abu-El-Haija;Amol Kapoor;Bryan Perozzi;Joonseok Lee,haija@google.com;ajk2227@columbia.edu;bperozzi@acm.org;joonseok@google.com,5;5;6,2;4;5,Reject,2,7,0.0,yes,10/27/17,Google;Columbia University;;Google,-1;15;-1;-1,-1;14;-1;-1,10;4;8
1321,1321,1321,1321,1321,1321,1321,1321,ICLR,2018,LSH-SAMPLING BREAKS THE COMPUTATIONAL CHICKEN-AND-EGG LOOP IN ADAPTIVE STOCHASTIC GRADIENT ESTIMATION,Beidi Chen;Yingchen Xu;Anshumali Shrivastava,beidi.chen@rice.edu;yingchen.xu@rice.edu;anshumali@rice.edu,8;4;4,4;5;5,Invite to Workshop Track,0,10,0.0,yes,10/27/17,Rice University;Rice University;Rice University,85;85;85,86;86;86,
1322,1322,1322,1322,1322,1322,1322,1322,ICLR,2018,Deep Boosting of Diverse Experts,Wei Zhang;Qiuyu Chen;Jun Yu;Jianping Fan,weizh@fudan.edu.cn;qchen12@uncc.edu;yujun@hdu.edu.cn;jfan@uncc.edu,2;6;5,5;3;4,Reject,0,2,0.0,yes,10/27/17,"Fudan University;University of North Carolina, Charlotte;Shandong University;University of North Carolina, Charlotte",78;74;4;74,116;1103;3;1103,
1323,1323,1323,1323,1323,1323,1323,1323,ICLR,2018,Bounding and Counting Linear Regions of Deep Neural Networks,Thiago Serra;Christian Tjandraatmadja;Srikumar Ramalingam,tserra@gmail.com;ctjandra@andrew.cmu.edu;srikumar.ramalingam@gmail.com,6;4;6,5;5;3,Reject,0,6,0.0,yes,10/27/17,Carnegie Mellon University;Carnegie Mellon University;University of Utah,1;1;52,24;24;200,
1324,1324,1324,1324,1324,1324,1324,1324,ICLR,2018,Jointly Learning to Construct and Control Agents using Deep Reinforcement Learning,Charles Schaff;David Yunis;Ayan Chakrabarti;Matthew R. Walter,cbschaff@ttic.edu;dyunis@uchicago.edu;ayan@wustl.edu;mwalter@ttic.edu,9;4;5,5;4;3,Invite to Workshop Track,0,3,0.0,yes,10/27/17,"Toyota Technological Institute at Chicago;University of Chicago;Washington University, St. Louis;Toyota Technological Institute at Chicago",-1;46;104;-1,-1;9;50;-1,
1325,1325,1325,1325,1325,1325,1325,1325,ICLR,2018,Tandem Blocks in Deep Convolutional Neural Networks,Chris Hettinger;Tanner Christensen;Jeff Humpherys;Tyler J Jarvis,chrishettinger@gmail.com;tkchristensen@byu.edu;jeffh@math.byu.edu;jarvis@math.byu.edu,5;7;4,4;4;4,Reject,0,12,0.0,yes,10/27/17,Brigham Young University;Brigham Young University;Brigham Young University;Brigham Young University,-1;-1;-1;-1,-1;-1;-1;-1,
1326,1326,1326,1326,1326,1326,1326,1326,ICLR,2018,On the difference between building and extracting patterns: a causal analysis of deep generative models.,Michel Besserve;Dominik Janzing;Bernhard Schoelkopf,michel.besserve@tuebingen.mpg.de;dominik.janzing@tuebingen.mpg.de;bs@tuebingen.mpg.de,2;7;7,4;3;2,Reject,0,0,0.0,yes,10/27/17,"Max Planck Institute for Intelligent Systems, Max-Planck Institute;Max Planck Institute for Intelligent Systems, Max-Planck Institute;Max Planck Institute for Intelligent Systems, Max-Planck Institute",-1;-1;-1,-1;-1;-1,5;4
1327,1327,1327,1327,1327,1327,1327,1327,ICLR,2018,Generalization of Learning using Reservoir Computing,Sanjukta Krishnagopal;Yiannis Aloimonos;Michelle Girvan,sanjukta@umd.edu;yiannis@cs.umd.edu;girvan@umd.edu,4;4;4,3;5;4,Reject,0,4,0.0,yes,10/27/17,"University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park",12;12;12,69;69;69,8
1328,1328,1328,1328,1328,1328,1328,1328,ICLR,2018,Adversarial Examples for Natural Language Classification Problems,Volodymyr Kuleshov;Shantanu Thakoor;Tingfung Lau;Stefano Ermon,vol.kuleshov@gmail.com;shanu.thakoor@gmail.com;ldf921@126.com;ermon@cs.stanford.edu,4;6;4,5;3;4,Reject,1,13,0.0,yes,10/27/17,Stanford University;Stanford University;126;Stanford University,4;4;-1;4,3;3;-1;3,3;4
1329,1329,1329,1329,1329,1329,1329,1329,ICLR,2018,"Model Specialization for Inference Via End-to-End Distillation, Pruning, and Cascades",Daniel Kang;Karey Shi;Thao Ngyuen;Stephanie Mallard;Peter Bailis;Matei Zaharia,ddkang@stanford.edu;kareyshi@stanford.edu;thao2605@stanford.edu;pbailis@cs.stanford.edu;matei@cs.stanford.edu,6;4;3,3;4;4,Reject,0,0,0.0,yes,10/27/17,Stanford University;Stanford University;Stanford University;Stanford University;Stanford University,4;4;4;4;4,3;3;3;3;3,
1330,1330,1330,1330,1330,1330,1330,1330,ICLR,2018,Learning Sparse Structured Ensembles with SG-MCMC and Network Pruning,Yichi Zhang;Zhijian Ou,zhangyic17@mails.tsinghua.edu.cn;ozj@tsinghua.edu.cn,4;6;6,4;3;5,Reject,1,5,0.0,yes,10/27/17,Tsinghua University;Tsinghua University,10;10,30;30,3
1331,1331,1331,1331,1331,1331,1331,1331,ICLR,2018,A dynamic game approach to training robust deep policies,Olalekan Ogunmolu,opo140030@utdallas.edu,5;3;5,4;3;2,Reject,0,1,0.0,yes,10/27/17,"University of Texas, Dallas",85,239,4
1332,1332,1332,1332,1332,1332,1332,1332,ICLR,2018,MACHINE VS MACHINE: MINIMAX-OPTIMAL DEFENSE AGAINST ADVERSARIAL EXAMPLES,Jihun Hamm,hammj@cse.ohio-state.edu,5;6;5,3;3;4,Reject,1,7,0.0,yes,10/27/17,Ohio State University,-1,-1,4
1333,1333,1333,1333,1333,1333,1333,1333,ICLR,2018,EXPLORING NEURAL ARCHITECTURE SEARCH FOR LANGUAGE TASKS,Minh-Thang Luong;David Dohan;Adams Wei Yu;Quoc V. Le;Barret Zoph;Vijay Vasudevan,luong.m.thang@gmail.com;ddohan@google.com;adamsyuwei@gmail.com;qvl@google.com;barretzoph@google.com;vrv@google.com,3;4;3,4;4;4,Reject,1,1,0.0,yes,10/27/17,Google;Google;;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,3
1334,1334,1334,1334,1334,1334,1334,1334,ICLR,2018,Multimodal Sentiment Analysis To Explore the Structure of Emotions,Anthony Hu;Seth Flaxman,anthony.hu@stats.ox.ac.uk;s.flaxman@imperial.ac.uk,6;4;5,5;5;5,Reject,0,4,0.0,yes,10/27/17,University of Oxford;Imperial College London,51;74,1;8,3
1335,1335,1335,1335,1335,1335,1335,1335,ICLR,2018,Building Generalizable Agents with a Realistic and Rich 3D Environment,Yi Wu;Yuxin Wu;Georgia Gkioxari;Yuandong Tian,jxwuyi@gmail.com;ppwwyyxxc@gmail.com;georgia.gkioxari@gmail.com;yuandong.tian@gmail.com,4;5;8,5;4;4,Invite to Workshop Track,4,6,0.0,yes,10/27/17,University of California Berkeley;Facebook;Facebook;Facebook,5;-1;-1;-1,18;-1;-1;-1,8
1336,1336,1336,1336,1336,1336,1336,1336,ICLR,2018,LEARNING TO SHARE: SIMULTANEOUS PARAMETER TYING AND SPARSIFICATION IN DEEP LEARNING,Dejiao Zhang;Haozhu Wang;Mario Figueiredo;Laura Balzano,dejiao@umich.edu;hzwang@umich.edu;mario.figueiredo@lx.it.pt;girasole@umich.edu,6;8;7,3;5;4,Accept (Poster),0,8,0.0,yes,10/27/17,"University of Michigan;University of Michigan;Instituto de Telecomunicações, Portugal;University of Michigan",8;8;468;8,21;21;1103;21,8
1337,1337,1337,1337,1337,1337,1337,1337,ICLR,2018,Self-Supervised Learning of Object Motion Through Adversarial Video Prediction,Alex X. Lee;Frederik Ebert;Richard Zhang;Chelsea Finn;Pieter Abbeel;Sergey Levine,rich.zhang@eecs.berkeley.edu;febert@berkeley.edu;cbfinn@eecs.berkeley.edu;pabbeel@cs.berkeley.edu;svlevine@eecs.berkeley.edu,7;3;3;3,5;4;5;5,Reject,2,0,0.0,yes,10/27/17,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5;5,18;18;18;18;18,
1338,1338,1338,1338,1338,1338,1338,1338,ICLR,2018,ElimiNet: A Model for Eliminating Options for Reading Comprehension with Multiple Choice Questions,Soham Parikh;Ananya Sai;Preksha Nema;Mitesh M Khapra,sohamp@cse.iitm.ac.in;ananyasb@cse.iitm.ac.in;preksha@cse.iitm.ac.in;miteshk@cse.iitm.ac.in,5;5;4,3;3;4,Reject,0,0,0.0,yes,10/27/17,Indian Institute of Technology Madras;Indian Institute of Technology Madras;Indian Institute of Technology Madras;Indian Institute of Technology Madras,153;153;153;153,625;625;625;625,
1339,1339,1339,1339,1339,1339,1339,1339,ICLR,2018,Forward Modeling for Partial Observation Strategy Games - A StarCraft Defogger,Gabriel Synnaeve;Zeming Lin;Jonas Gehring;Vasil Khalidov;Nicolas Carion;Nicolas Usunier,gab@fb.com;zlin@fb.com;jgehring@fb.com;vkhalidov@fb.com;alcinos@fb.com;usunier@fb.com,5;4;5,4;1;3,Reject,0,2,0.0,yes,10/27/17,Facebook;Facebook;Facebook;Facebook;Facebook;Facebook,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,
1340,1340,1340,1340,1340,1340,1340,1340,ICLR,2018,Real-valued (Medical) Time Series Generation with Recurrent Conditional GANs,Stephanie Hyland;Cristóbal Esteban;Gunnar Rätsch,stephanie.hyland@inf.ethz.ch;cr_est@ethz.ch;raetsch@inf.ethz.ch,4;6;5,4;4;4,Reject,0,3,0.0,yes,10/27/17,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,9;9;9,10;10;10,5;4
1341,1341,1341,1341,1341,1341,1341,1341,ICLR,2018,Regularization Neural Networks via Constrained Virtual  Movement Field,Zhendong Zhang;Cheolkon Jung,zhd.zhang.ai@gmail.com;zhengzk@xidian.edu.cn,5;5;6,4;4;4,Invite to Workshop Track,0,5,0.0,yes,10/27/17,Tsinghua University;Tsinghua University,10;10,30;30,4
1342,1342,1342,1342,1342,1342,1342,1342,ICLR,2018,Learning to Write by Learning the Objective,Ari Holtzman;Jan Buys;Maxwell Forbes;Antoine Bosselut;Yejin Choi,ahai@cs.washington.edu;jbuys@cs.washington.edu;mbforbes@cs.washington.edu;antoineb@cs.washington.edu;yejin@cs.washington.edu,6;5;4,5;4;5,Invite to Workshop Track,0,4,0.0,yes,10/27/17,University of Washington;University of Washington;University of Washington;University of Washington;University of Washington,6;6;6;6;6,25;25;25;25;25,3;5
1343,1343,1343,1343,1343,1343,1343,1343,ICLR,2018,Key Protected Classification for GAN Attack Resilient Collaborative Learning,Mert Bülent Sarıyıldız;Ramazan Gökberk Cinbiş;Erman Ayday,mbsariyildiz@gmail.com;gokberkcinbis@gmail.com;erman@cs.bilkent.edu.tr,4;5;3,4;2;4,Reject,0,12,0.0,yes,10/27/17,;METU;Bilkent University,-1;210;291,-1;654;426,5;4
1344,1344,1344,1344,1344,1344,1344,1344,ICLR,2018,Bit-Regularized Optimization of Neural Nets,Mohamed Amer;Aswin Raghavan;Graham W. Taylor;Sek Chai,mohamed.amer@sri.com;aswin.raghavan@sri.com;gwtaylor@uoguelph.ca;sek.chai@sri.com,4;3;4,5;4;4,Reject,2,6,0.0,yes,10/27/17,SRI International;SRI International;University of Guelph;SRI International,-1;-1;291;-1,-1;-1;1103;-1,
1345,1345,1345,1345,1345,1345,1345,1345,ICLR,2018,ShakeDrop regularization,Yoshihiro Yamada;Masakazu Iwamura;Koichi Kise,yamada@m.cs.osakafu-u.ac.jp;masa@cs.osakafu-u.ac.jp;kise@cs.osakafu-u.ac.jp,5;4;4,2;4;3,Reject,3,10,0.0,yes,10/27/17,Meiji University;Meiji University;Meiji University,468;468;468,334;334;334,
1346,1346,1346,1346,1346,1346,1346,1346,ICLR,2018,AUTOMATA GUIDED HIERARCHICAL REINFORCEMENT LEARNING FOR ZERO-SHOT SKILL COMPOSITION,Xiao Li;Yao Ma;Calin Belta,xli87@bu.edu;yaoma@bu.edu;cbelta@bu.edu,5;3;4,4;4;3,Reject,0,3,0.0,yes,10/27/17,Boston University;Boston University;Boston University,69;69;69,70;70;70,
1347,1347,1347,1347,1347,1347,1347,1347,ICLR,2018,Quadrature-based features for kernel approximation,Marina Munkhoeva;Yermek Kapushev;Evgeny Burnaev;Ivan Oseledets,marina.munkhoeva@skolkovotech.ru;kapushev@gmail.com;e.burnaev@skoltech.ru;i.oseledets@skoltech.ru,4;7;6,3;5;4,Reject,0,3,0.0,yes,10/27/17,Skolkovo Institute of Science and Technology;Skolkovo Institute of Science and Technology;Skolkovo Institute of Science and Technology;Skolkovo Institute of Science and Technology,-1;-1;-1;-1,-1;-1;-1;-1,
1348,1348,1348,1348,1348,1348,1348,1348,ICLR,2018,Parametrizing filters of a CNN with a GAN,Yannic Kilcher;Gary Becigneul;Thomas Hofmann,yannic.kilcher@inf.ethz.ch;gary.becigneul@inf.ethz.ch;thomas.hofmann@inf.ethz.ch,2;4;4,4;4;5,Reject,0,1,0.0,yes,10/27/17,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,9;9;9,10;10;10,5;4
1349,1349,1349,1349,1349,1349,1349,1349,ICLR,2018,Jiffy: A Convolutional Approach to Learning Time Series Similarity,Divya Shanmugam;Davis Blalock;John Guttag,divyas@mit.edu;dblalock@mit.edu;jguttag@mit.edu,6;4;8,4;4;3,Reject,0,10,0.0,yes,10/27/17,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2,5;5;5,
1350,1350,1350,1350,1350,1350,1350,1350,ICLR,2018,Transfer Learning on Manifolds via Learned Transport Operators,Marissa Connor;Christopher Rozell,marissa.connor@gatech.edu;crozell@gatech.edu,5;4;4,4;4;4,Reject,0,0,0.0,yes,10/27/17,Georgia Institute of Technology;Georgia Institute of Technology,13;13,33;33,5;6
1351,1351,1351,1351,1351,1351,1351,1351,ICLR,2018,Visualizing the Loss Landscape of Neural Nets,Hao Li;Zheng Xu;Gavin Taylor;Tom Goldstein,haoli@cs.umd.edu;xuzh@cs.umd.edu;taylor@usna.edu;tomg@cs.umd.edu,5;4;5,4;3;3,Invite to Workshop Track,0,5,0.0,yes,10/27/17,"University of Maryland, College Park;University of Maryland, College Park;University of Arizona;University of Maryland, College Park",12;12;181;12,69;69;161;69,8
1352,1352,1352,1352,1352,1352,1352,1352,ICLR,2018,Demystifying overcomplete nonlinear auto-encoders: fast SGD convergence towards sparse representation from random initialization,Cheng Tang;Claire Monteleoni,tangch@gwu.edu;cmontel@gwu.edu,2;3;2,4;3;4,Reject,0,4,0.0,yes,10/27/17,George Washington University;George Washington University,210;210,226;226,9
1353,1353,1353,1353,1353,1353,1353,1353,ICLR,2018,On Characterizing the Capacity of Neural Networks Using Algebraic Topology,William H. Guss;Ruslan Salakhutdinov,wguss@cs.cmu.edu;rsalakhu@cs.cmu.edu,3;4;4,5;5;5,Reject,0,4,0.0,yes,10/27/17,Carnegie Mellon University;Carnegie Mellon University,1;1,24;24,
1354,1354,1354,1354,1354,1354,1354,1354,ICLR,2018,Gated ConvNets for Letter-Based ASR,Vitaliy Liptchinsky;Gabriel Synnaeve;Ronan Collobert,vitaliy888@fb.com;gab@fb.com;locronan@fb.com,3;6;4,5;5;4,Reject,1,4,0.0,yes,10/27/17,Facebook;Facebook;Facebook,-1;-1;-1,-1;-1;-1,3
1355,1355,1355,1355,1355,1355,1355,1355,ICLR,2018,Variance Regularizing Adversarial Learning,Karan Grewal;R Devon Hjelm;Yoshua Bengio,karanraj.grewal@mail.utoronto.ca;erroneus@gmail.com;yoshua.umontreal@gmail.com,5;4;6,4;4;3,Reject,0,0,0.0,yes,10/27/17,Toronto University;University of Montreal;University of Montreal,17;124;124,22;108;108,5;4
1356,1356,1356,1356,1356,1356,1356,1356,ICLR,2018,The Information-Autoencoding Family: A Lagrangian Perspective on Latent Variable Generative Modeling,Shengjia Zhao;Jiaming Song;Stefano Ermon,sjzhao@stanford.edu;tsong@cs.stanford.edu;ermon@cs.stanford.edu,4;5;6,4;4;4,Reject,0,5,0.0,yes,10/27/17,Stanford University;Stanford University;Stanford University,4;4;4,3;3;3,5;4;8
1357,1357,1357,1357,1357,1357,1357,1357,ICLR,2018,Learning Priors for Adversarial Autoencoders,Hui-Po Wang;Wei-Jan Ko;Wen-Hsiao Peng,a88575847@gmail.com;ts771164@gmail.com;wpeng@cs.nctu.edu.tw,6;5;6,3;3;4,Reject,1,5,0.0,yes,10/27/17,National Chiao Tung University;;National Chiao Tung University,153;-1;153,452;-1;452,5;4
1358,1358,1358,1358,1358,1358,1358,1358,ICLR,2018,Faster Reinforcement Learning with Expert State Sequences,Xiaoxiao Guo;Shiyu Chang;Mo Yu;Miao Liu;Gerald Tesauro,xiaoxiao.guo@ibm.com;shiyu.chang@ibm.com;yum@us.ibm.com,6;5;6,3;5;4,Reject,2,4,0.0,yes,10/27/17,International Business Machines;International Business Machines;International Business Machines,-1;-1;-1,-1;-1;-1,
1359,1359,1359,1359,1359,1359,1359,1359,ICLR,2018,Training Neural Machines with Partial Traces,Matthew Mirman;Dimitar Dimitrov;Pavle Djordjevich;Timon Gehr;Martin Vechev,matthew.mirman@inf.ethz.ch;dpavle@student.ethz.ch;dimitar.dimitrov@inf.ethz.ch;timon.gehr@inf.ethz.ch;martin.vechev@inf.ethz.ch,4;5;4,5;4;3,Reject,0,5,0.0,yes,10/27/17,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,9;9;9;9;9,10;10;10;10;10,8
1360,1360,1360,1360,1360,1360,1360,1360,ICLR,2018,STRUCTURED ALIGNMENT NETWORKS,Yang Liu;Matt Gardner,yang.liu2@ed.ac.uk;mattg@allenai.org,6;5;5,4;4;4,Reject,0,1,0.0,yes,10/27/17,University of Edinburgh;Allen Institute for Artificial Intelligence,33;-1,27;-1,3
1361,1361,1361,1361,1361,1361,1361,1361,ICLR,2018,Revisiting The Master-Slave Architecture In Multi-Agent Deep Reinforcement Learning,Xiangyu Kong;Fangchen Liu;Bo Xin;Yizhou Wang,kxyzc1992@gmail.com;liufangchen@pku.edu.cn;boxin@microsoft.com;yizhou.wang@pku.edu.cn,5;5;4,5;4;3,Reject,0,6,0.0,yes,10/27/17,;Peking University;Microsoft;Peking University,-1;24;-1;24,-1;27;-1;27,
1362,1362,1362,1362,1362,1362,1362,1362,ICLR,2018,GATED FAST WEIGHTS FOR ASSOCIATIVE RETRIEVAL,Imanol Schlag;Jürgen Schmidhuber,imanol@idsia.ch;juergen@idsia.ch,3;5;4,5;4;4,Reject,0,0,0.0,yes,10/27/17,IDSIA;IDSIA,-1;-1,-1;-1,
1363,1363,1363,1363,1363,1363,1363,1363,ICLR,2018,Connectivity Learning in Multi-Branch Networks,Karim Ahmed;Lorenzo Torresani,karim.mmm@gmail.com;lt@dartmouth.edu,5;5;5,5;4;4,Reject,0,3,0.0,yes,10/27/17,Dartmouth College;Dartmouth College,153;153,89;89,
1364,1364,1364,1364,1364,1364,1364,1364,ICLR,2018,AMPNet: Asynchronous Model-Parallel Training for Dynamic Neural Networks,Alexander L. Gaunt;Matthew A. Johnson;Alan Lawrence;Maik Riechert;Daniel Tarlow;Ryota Tomioka;Dimitrios Vytiniotis;Sam Webster,algaunt@microsoft.com;matjoh@microsoft.com;allawr@microsoft.com;a-mariec@microsoft.com;dannytarlow@gmail.com;ryoto@microsoft.com;dimitris@microsoft.com;sweb@microsoft.com,6;6;4,5;4;5,Reject,0,6,0.0,yes,10/27/17,Microsoft;Microsoft;Microsoft;Microsoft;MSR Cambridge;Microsoft;Microsoft;Microsoft,-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1,
1365,1365,1365,1365,1365,1365,1365,1365,ICLR,2018,POLICY DRIVEN GENERATIVE ADVERSARIAL NETWORKS FOR ACCENTED SPEECH GENERATION,Prannay Khosla;Preethi Jyothi;Vinay P. Namboodiri;Mukundhan Srinivasan,prannayk@iitk.ac.in;pjyothi@cse.iitb.ac.in;vinaypn@cse.iitk.ac.in;msrinivasan@nvidia.com,5;3;4,4;4;4,Reject,0,6,0.0,yes,10/27/17,IIT Kanpur;Indian Institute of Technology Bombay;IIT Kanpur;NVIDIA,139;115;139;-1,578;367;578;-1,5;4
1366,1366,1366,1366,1366,1366,1366,1366,ICLR,2018,Autonomous Vehicle Fleet Coordination With Deep Reinforcement Learning,Cane Punma,cane.cane@live.com,3;3;4,5;3;4,Reject,0,0,0.0,yes,10/27/17,,,,6
1367,1367,1367,1367,1367,1367,1367,1367,ICLR,2018,BLOCK-NORMALIZED GRADIENT METHOD: AN EMPIRICAL STUDY FOR TRAINING DEEP NEURAL NETWORK,Adams Wei Yu;Lei Huang;Qihang Lin;Ruslan Salakhutdinov;Jaime Carbonell,weiyu@cs.cmu.edu;huanglei@nlsde.buaa.edu.cn;qihang-lin@uiowa.edu;rsalakhu@cs.cmu.edu;jgc@cs.cmu.edu,4;9;2,5;5;5,Reject,0,5,0.0,yes,10/27/17,Carnegie Mellon University;Beihang University;University of Iowa;Carnegie Mellon University;Carnegie Mellon University,1;124;153;1;1,24;658;223;24;24,8
1368,1368,1368,1368,1368,1368,1368,1368,ICLR,2018,Learning Representations for Faster Similarity Search,Ludwig Schmidt;Kunal Talwar,ludwigs@mit.edu;kunal@google.com,4;4;4,5;5;4,Reject,0,0,0.0,yes,10/27/17,Massachusetts Institute of Technology;Google,2;-1,5;-1,
1369,1369,1369,1369,1369,1369,1369,1369,ICLR,2018,Optimal transport maps for distribution preserving operations on latent spaces of Generative Models,Eirikur Agustsson;Alexander Sage;Radu Timofte;Luc Van Gool,aeirikur@vision.ee.ethz.ch;sagea@student.ethz.ch;radu.timofte@vision.ee.ethz.ch;vangool@vision.ee.ethz.ch,4;6;6,3;3;4,Reject,0,4,0.0,yes,10/27/17,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,9;9;9;9,10;10;10;10,5;4
1370,1370,1370,1370,1370,1370,1370,1370,ICLR,2018,Neural Compositional Denotational Semantics for Question Answering,Nitish Gupta;Mike Lewis,nitishg@cis.upenn.edu;mikelewis@facebook.com,4;5;7,4;4;4,Reject,0,5,0.0,yes,10/27/17,University of Pennsylvania;,19;-1,10;-1,10
1371,1371,1371,1371,1371,1371,1371,1371,ICLR,2018,Variational Bi-LSTMs,Samira Shabanian;Devansh Arpit;Adam Trischler;Yoshua Bengio,s.shabanian@gmail.com;devansharpit@gmail.com;adam.trischler@microsoft.com;yoshua.umontreal@gmail.com,4;7;6,4;3;4,Reject,6,5,0.0,yes,10/27/17,Microsoft;University of Montreal;Microsoft;University of Montreal,-1;124;-1;124,-1;108;-1;108,
1372,1372,1372,1372,1372,1372,1372,1372,ICLR,2018,TOWARDS ROBOT VISION MODULE DEVELOPMENT WITH EXPERIENTIAL ROBOT LEARNING,Ahmed A Aly;Joanne Bechta Dugan,aaa2cn@virginia.edu;jbd@virginia.edu,3;2;2,4;3;4,Reject,0,0,0.0,yes,10/27/17,University of Virginia;University of Virginia,62;62,113;113,5;4;2
1373,1373,1373,1373,1373,1373,1373,1373,ICLR,2018,Towards a Testable Notion of Generalization for Generative Adversarial Networks,Robert Cornish;Hongseok Yang;Frank Wood,rcornish@robots.ox.ac.uk;hongseok.yang@cs.ox.ac.uk;fwood@robots.ox.ac.uk,5;6;4,3;3;4,Reject,6,3,0.0,yes,10/27/17,University of Oxford;University of Oxford;University of Oxford,51;51;51,1;1;1,5;4
1374,1374,1374,1374,1374,1374,1374,1374,ICLR,2018,3C-GAN: AN CONDITION-CONTEXT-COMPOSITE GENERATIVE ADVERSARIAL NETWORKS FOR GENERATING IMAGES SEPARATELY,Yeu-Chern Harn;Vladimir Jojic,ycharn@cs.unc.edu;vjojic@gmail.com,5;4;4,5;4;5,Reject,0,0,0.0,yes,10/27/17,"University of North Carolina, Chapel Hill;University of North Carolina, Chapel Hill",74;74,56;56,5
1375,1375,1375,1375,1375,1375,1375,1375,ICLR,2018,Alpha-divergence bridges maximum likelihood and reinforcement learning in neural sequence generation,Sotetsu Koyamada;Yuta Kikuchi;Atsunori Kanemura;Shin-ichi Maeda;Shin Ishii,sotetsu.koyamada@gmail.com,4;4;4,5;1;3,Reject,0,0,0.0,yes,10/27/17,,,,3
1376,1376,1376,1376,1376,1376,1376,1376,ICLR,2018,Don't encrypt the data; just approximate the model \ Towards Secure Transaction and Fair Pricing of Training Data,Xinlei Xu,xxu@hmc.edu,2;4;3,4;5;5,Reject,0,0,0.0,yes,10/27/17,Harvey Mudd College,468,981,4
1377,1377,1377,1377,1377,1377,1377,1377,ICLR,2018,An inference-based policy gradient method for learning options,Matthew J. A. Smith;Herke van Hoof;Joelle Pineau,matthew.smith5@mail.mcgill.ca;herke.vanhoof@mail.mcgill.ca;jpineau@cs.mcgill.ca,3;3;4,4;5;4,Reject,0,5,0.0,yes,10/27/17,McGill University;McGill University;McGill University,81;81;81,42;42;42,
1378,1378,1378,1378,1378,1378,1378,1378,ICLR,2018,Sequential Coordination of Deep Models for Learning Visual Arithmetic,Eric Crawford;Guillaume Rabusseau;Joelle Pineau,eric.crawford@mail.mcgill.ca;guillaume.rabusseau@mail.mcgill.ca;jpineau@cs.mcgill.ca,4;3;2,4;4;4,Reject,0,4,0.0,yes,10/27/17,McGill University;McGill University;McGill University,81;81;81,42;42;42,
1379,1379,1379,1379,1379,1379,1379,1379,ICLR,2018,What is image captioning made of?,Pranava Madhyastha;Josiah Wang;Lucia Specia,p.madhyastha@sheffield.ac.uk;j.k.wang@sheffield.ac.uk;l.specia@sheffield.ac.uk,4;4;4,5;4;5,Reject,0,10,0.0,yes,10/27/17,University of Sheffield;University of Sheffield;University of Sheffield,210;210;210,104;104;104,
1380,1380,1380,1380,1380,1380,1380,1380,ICLR,2018,Fraternal Dropout,Konrad Zolna;Devansh Arpit;Dendi Suhubdy;Yoshua Bengio,konrad.zolna@gmail.com;devansh.arpit@umontreal.ca;dasuhubd@ncsu.edu;bengioy@iro.umontreal.ca,5;5;6,4;3;3,Accept (Poster),0,7,0.0,yes,10/27/17,Jagiellonian University;University of Montreal;North Carolina State University;University of Montreal,468;124;85;124,695;108;275;108,3
1381,1381,1381,1381,1381,1381,1381,1381,ICLR,2018,TD Learning with Constrained Gradients,Ishan Durugkar;Peter Stone,ishand@cs.utexas.edu;pstone@cs.utexas.edu,2;3;4,4;4;4,Reject,1,4,0.0,yes,10/27/17,"University of Texas, Austin;University of Texas, Austin",21;21,49;49,
1382,1382,1382,1382,1382,1382,1382,1382,ICLR,2018,WSNet: Learning Compact and Efficient Networks with Weight Sampling,Xiaojie Jin;Yingzhen Yang;Ning Xu;Jianchao Yang;Jiashi Feng;Shuicheng Yan,xiaojie.jin@u.nus.edu;superyyzg@gmail.com;ning.xu@snap.com;jiachao.yang@snap.com;elefjia@nus.edu.sg;yanshuicheng@360.com,6;6;5,4;3;5,Invite to Workshop Track,0,0,0.0,yes,10/27/17,National University of Singapore;;Snap Inc.;Snap Inc.;National University of Singapore;360,16;-1;-1;-1;16;-1,22;-1;-1;-1;22;-1,
1383,1383,1383,1383,1383,1383,1383,1383,ICLR,2018,Generative Entity Networks: Disentangling Entitites and Attributes in Visual Scenes using Partial Natural Language Descriptions,Charlie Nash;Sebastian Nowozin;Nate Kushman,charlie.nash@ed.ac.uk;sebastian.nowozin@microsoft.com;nate@kushman.org,4;5;5,4;4;5,Reject,0,3,0.0,yes,10/27/17,University of Edinburgh;Microsoft;Microsoft Research,33;-1;-1,27;-1;-1,3;5
1384,1384,1384,1384,1384,1384,1384,1384,ICLR,2018,Prediction Under Uncertainty with Error Encoding Networks,Mikael Henaff;Junbo Zhao;Yann Lecun,mbh305@nyu.edu;j.zhao@nyu.edu;yann@cs.nyu.edu,4;5;5,4;3;2,Reject,7,3,0.0,yes,10/27/17,New York University;New York University;New York University,26;26;26,27;27;27,4
1385,1385,1385,1385,1385,1385,1385,1385,ICLR,2018,Noise-Based Regularizers for Recurrent Neural Networks,Adji B. Dieng;Jaan Altosaar;Rajesh Ranganath;David M. Blei,abd2141@columbia.edu;altosaar@princeton.edu;rajeshr@cs.princeton.edu;david.blei@columbia.edu,2;5;3,5;4;3,Reject,2,2,0.0,yes,10/27/17,Columbia University;Princeton University;Princeton University;Columbia University,15;31;31;15,14;7;7;14,3
1386,1386,1386,1386,1386,1386,1386,1386,ICLR,2018,Learning To Generate Reviews and Discovering Sentiment,Alec Radford;Rafal Jozefowicz;Ilya Sutskever,alec@openai.com;rafal@openai.com;ilya@openai.com,4;2;4,3;5;5,Reject,1,0,0.0,yes,10/27/17,OpenAI;OpenAI;OpenAI,-1;-1;-1,-1;-1;-1,3;5
1387,1387,1387,1387,1387,1387,1387,1387,ICLR,2018,Deep learning mutation prediction enables early stage lung cancer detection in liquid biopsy,Steven T. Kothen-Hill;Asaf Zviran;Rafael C. Schulman;Sunil Deochand;Federico Gaiti;Dillon Maloney;Kevin Y. Huang;Will Liao;Nicolas Robine;Nathaniel D. Omans;Dan A. Landau,sth2022@med.cornell.edu;azviran@nygenome.org;rschulman@nygenome.org;sdd325@nyu.edu;fgaiti@nygenome.org;dmaloney@nygenome.org;khuang@nygenome.org;wliao@nygenome.org;nrobine@nygenome.org;nao2013@med.cornell.edu;dal3005@med.cornell.edu,8;4;5,4;3;4,Invite to Workshop Track,0,7,0.0,yes,10/27/17,Cornell University;;;New York University;;;Johns Hopkins University;;;Cornell University;Cornell University,7;-1;-1;26;-1;-1;71;-1;-1;7;7,19;-1;-1;27;-1;-1;13;-1;-1;19;19,
1388,1388,1388,1388,1388,1388,1388,1388,ICLR,2018,Covariant Compositional Networks For Learning Graphs,Risi Kondor;Truong Son Hy;Horace Pan;Brandon M. Anderson;Shubhendu Trivedi,risi@cs.uchicago.edu;hytruongson@uchicago.edu;hopan@cs.uchicago.edu;brandona@uchicago.edu;shubhendu@ttic.edu,5;5;6,3;2;3,Invite to Workshop Track,0,12,0.0,yes,10/27/17,University of Chicago;University of Chicago;University of Chicago;University of Chicago;Toyota Technological Institute at Chicago,46;46;46;46;-1,9;9;9;9;-1,10
1389,1389,1389,1389,1389,1389,1389,1389,ICLR,2018,A novel method to determine the number of latent dimensions with SVD,Asana Neishabouri;Michel Desmarais,asana.neishabouri@polymtl.ca;michel.desmarais@polymtl.ca,1;2;3,4;5;4,Reject,1,0,0.0,yes,10/27/17,Polytechnique Montreal;Polytechnique Montreal,364;364,108;108,
1390,1390,1390,1390,1390,1390,1390,1390,ICLR,2018,Automatic Goal Generation for Reinforcement Learning Agents,David Held;Xinyang Geng;Carlos Florensa;Pieter Abbeel,dheld@andrew.cmu.edu;young.geng@berkeley.edu;florensa@berkeley.edu;pabbeel@berkeley.edu,8;4;6,4;4;4,Reject,0,19,0.0,yes,10/27/17,Carnegie Mellon University;University of California Berkeley;University of California Berkeley;University of California Berkeley,1;5;5;5,24;18;18;18,5;4
1391,1391,1391,1391,1391,1391,1391,1391,ICLR,2018,A Goal-oriented Neural Conversation Model by Self-Play,Wei Wei;Quoc V. Le;Andrew M. Dai;Li-Jia Li,wewei@google.com;adai@google.com;qvl@google.com;lijiali@google.com,6;4;3,3;3;4,Reject,0,0,0.0,yes,10/27/17,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,3
1392,1392,1392,1392,1392,1392,1392,1392,ICLR,2018,Bayesian Uncertainty Estimation for Batch Normalized Deep Networks,Mattias Teye;Hossein Azizpour;Kevin Smith,teye@kth.se;azizpour@kth.se;ksmith@kth.se,5;5;6,3;4;4,Reject,0,5,1.0,yes,10/27/17,"KTH Royal Institute of Technology, Stockholm, Sweden;KTH Royal Institute of Technology, Stockholm, Sweden;KTH Royal Institute of Technology, Stockholm, Sweden",124;124;124,173;173;173,11
1393,1393,1393,1393,1393,1393,1393,1393,ICLR,2018,Learning Efficient Tensor Representations with Ring Structure Networks,Qibin Zhao;Masashi Sugiyama;Longhao Yuan;Andrzej Cichocki,qibin.zhao@riken.jp;sugi@k.u-tokyo.ac.jp;longhao.yuan@riken.jp;a.cichocki@riken.jp,5;5;6,4;4;3,Invite to Workshop Track,0,5,0.0,yes,10/27/17,RIKEN;The University of Tokyo;RIKEN;RIKEN,-1;52;-1;-1,-1;45;-1;-1,
1394,1394,1394,1394,1394,1394,1394,1394,ICLR,2018,Video Action Segmentation with Hybrid Temporal Networks,Li Ding;Chenliang Xu,liding@rochester.edu;chenliang.xu@rochester.edu,3;4;3,5;4;5,Reject,0,0,0.0,yes,10/27/17,University of Rochester;University of Rochester,104;104,153;153,2
1395,1395,1395,1395,1395,1395,1395,1395,ICLR,2018,The Variational Homoencoder: Learning to Infer High-Capacity Generative Models from Few Examples,Luke Hewitt;Andrea Gane;Tommi Jaakkola;Joshua B. Tenenbaum,lbh@mit.edu;agane@mit.edu;tommi@csail.mit.edu;jbt@mit.edu,7;5;6,4;5;3,Reject,0,5,0.0,yes,10/27/17,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2,5;5;5;5,5;11
1396,1396,1396,1396,1396,1396,1396,1396,ICLR,2018,Hyperedge2vec: Distributed Representations for Hyperedges,Ankit Sharma;Shafiq Joty;Himanshu Kharkwal;Jaideep Srivastava,sharm170@umn.edu;srjoty@ntu.edu.sg;himanshukharkwal765@gmail.com;srivasta@umn.edu,5;5;5,3;3;4,Reject,0,5,0.0,yes,10/27/17,"University of Minnesota, Minneapolis;National Taiwan University;;University of Minnesota, Minneapolis",55;85;-1;55,56;197;-1;56,10
1397,1397,1397,1397,1397,1397,1397,1397,ICLR,2018,Combining Model-based and Model-free RL via Multi-step Control Variates,Tong Che;Yuchen Lu;George Tucker;Surya Bhupatiraju;Shane Gu;Sergey Levine;Yoshua Bengio,gerryche@berkeley.edu;luyuchen.paul@gmail.com;gjt@google.com;sbhupatiraju@google.com;shanegu@google.com;svlevine@eecs.berkeley.edu;bengioy@iro.umontreal.ca,5;5;4,4;4;3,Reject,0,2,0.0,yes,10/27/17,University of California Berkeley;University of Montreal;Google;Google;Google;University of California Berkeley;University of Montreal,5;124;-1;-1;-1;5;124,18;108;-1;-1;-1;18;108,
1398,1398,1398,1398,1398,1398,1398,1398,ICLR,2018,Tree2Tree Learning with Memory Unit,Ning Miao;Hengliang Wang;Ran Le;Chongyang Tao;Mingyue Shang;Rui Yan;Dongyan Zhao,miaoning@pku.edu.cn;wanghl@pku.edu.cn;leran@buaa.edu.cn;chongyangtao@pku.edu.cn;shangmy@pku.edu.cn;ruiyan@pku.edu.cn;zhaody@pku.edu.cn,2;5;4,4;4;4,Reject,0,1,0.0,yes,10/27/17,Peking University;Peking University;Beihang University;Peking University;Peking University;Peking University;Peking University,24;24;124;24;24;24;24,27;27;658;27;27;27;27,3
1399,1399,1399,1399,1399,1399,1399,1399,ICLR,2018,Revisiting Knowledge Base Embedding as Tensor Decomposition,Jiezhong Qiu;Hao Ma;Yuxiao Dong;Kuansan Wang;Jie Tang,xptree@gmail.com;haoma@microsoft.com;yuxdong@microsoft.com;kuansanw@microsoft.com;jietang@tsinghua.edu.cn,3;5;3,4;4;4,Reject,5,0,0.0,yes,10/27/17,;Microsoft;Microsoft;Microsoft;Tsinghua University,-1;-1;-1;-1;10,-1;-1;-1;-1;30,
1400,1400,1400,1400,1400,1400,1400,1400,ICLR,2018,Assessing the scalability of biologically-motivated deep learning algorithms and architectures,Anonymous,ICLR.cc/2018/Conference/Paper607/Authors,8;5;6,5;3;4,Withdrawn,1,0,,yes,1/13/18,,,,
1401,1401,1401,1401,1401,1401,1401,1401,ICLR,2018,Neural Variational Sparse Topic Model,Anonymous,ICLR.cc/2018/Conference/Paper79/Authors,5;3;3,4;4;4,Withdrawn,0,0,,yes,1/21/18,,,,3;5
1402,1402,1402,1402,1402,1402,1402,1402,ICLR,2018,Melody Generation for Pop Music via Word Representation of Musical Properties,Andrew Shin;Leopold Crestel;Hiroharu Kato;Kuniaki Saito;Katsunori Ohnishi;Masataka Yamaguchi;Masahiro Nakawaki;Yoshitaka Ushiku;Tatsuya Harada,andrew@mi.t.u-tokyo.ac.jp;crestel@ircam.fr;kato@mi.t.u-tokyo.ac.jp;k-saito@mi.t.u-tokyo.ac.jp;ohnishi@mi.t.u-tokyo.ac.jp;yamaguchi@mi.t.u-tokyo.ac.jp;nakawaki.ici@gmail.com;ushiku@mi.t.u-tokyo.ac.jp;harada@mi.t.u-tokyo.ac.jp,4;4;5,5;4;4,Withdrawn,0,0,,yes,1/29/18,The University of Tokyo;;The University of Tokyo;The University of Tokyo;The University of Tokyo;The University of Tokyo;;The University of Tokyo;The University of Tokyo,52;-1;52;52;52;52;-1;52;52,45;-1;45;45;45;45;-1;45;45,
1403,1403,1403,1403,1403,1403,1403,1403,ICLR,2018,Spatial Variational Auto-Encoding via Matrix-Variate Normal Distributions,Zhengyang Wang;Hao Yuan;Shuiwang Ji,zwang6@eecs.wsu.edu;hao.yuan@wsu.edu;sji@eecs.wsu.edu,5;4;3,2;4;5,Withdrawn,0,0,,yes,12/3/17,SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY,468;468;468,352;352;352,5
1404,1404,1404,1404,1404,1404,1404,1404,ICLR,2018,Dense Transformer Networks,Jun Li;Yongjun Chen;Lei Cai;Ian Davidson;Shuiwang Ji,jun.li3@wsu.edu;yongjun.chen@wsu.edu;lei.cai@wsu.edu;davidson@cs.ucdavis.edu;sji@eecs.wsu.edu,3;4;4,5;4;4,Withdrawn,0,0,,yes,12/2/17,"SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;University of California, Davis;SUN YAT-SEN UNIVERSITY",468;468;468;78;468,352;352;352;54;352,2
1405,1405,1405,1405,1405,1405,1405,1405,ICLR,2018,Self-Organization adds application robustness to deep learners,Pitoyo Hartono;Thomas Trappenberg,hartono@ieee.org;tt@cs.dal.edu,4;2;2,4;5;5,Withdrawn,0,0,,yes,12/25/17,Meiji University;,468;-1,334;-1,
1406,1406,1406,1406,1406,1406,1406,1406,ICLR,2018,Information Theoretic Co-Training,David McAllester,mcallester@ttic.edu,4;5;4,4;3;4,Withdrawn,0,0,,yes,1/5/18,Toyota Technological Institute at Chicago,-1,-1,
1407,1407,1407,1407,1407,1407,1407,1407,ICLR,2018,Towards Quantum Inspired Convolution Networks,Davi Geiger;Zvi Kedem,dg1@nyu.edu;kedem@nyu.edu,3;4;5,5;3;3,Withdrawn,3,0,,yes,12/2/17,New York University;New York University,26;26,27;27,11
1408,1408,1408,1408,1408,1408,1408,1408,ICLR,2018,A cluster-to-cluster framework for neural machine translation,Anonymous,ICLR.cc/2018/Conference/Paper150/Authors,6;3;5,3;4;2,Withdrawn,0,0,,yes,12/13/17,,,,3
1409,1409,1409,1409,1409,1409,1409,1409,ICLR,2018,Deep Epitome for Unravelling Generalized Hamming Network: A Fuzzy Logic Interpretation of Deep Learning,Anonymous,ICLR.cc/2018/Conference/Paper167/Authors,3;7;4,3;2;4,Withdrawn,0,0,,yes,1/4/18,,,,1
1410,1410,1410,1410,1410,1410,1410,1410,ICLR,2018,Interactive Boosting of Neural Networks for Small-sample Image Classification,Xiaoxu Li;Dongliang Chang;Zheng-Hua Tan;Zhanyu Ma;Jun Guo;Jie Cao,xiaoxulilut@gmail.com;dlchanglut@hotmai.com;zt@es.aau.dk;mazhanyu@bupt.edu.cn;guojun@bupt.edu.cn;caoj@lut.cn,5;5;4,4;5;4,Withdrawn,1,4,,yes,1/2/18,;Hotmai;Aarhus University;Beijing University of Post and Telecommunication;Beijing University of Post and Telecommunication;,-1;-1;28;468;468;-1,-1;-1;60;1103;1103;-1,8
1411,1411,1411,1411,1411,1411,1411,1411,ICLR,2018,Tensor-Based Preposition Representation,Hongyu Gong;Suma Bhat;Pramod Viswanath,hgong6@illinois.edu;pramodv@illinois.edu;spbhat2@illinois.edu,6;4;5,4;4;4,Withdrawn,0,0,,yes,12/12/17,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",3;3;3,37;37;37,
1412,1412,1412,1412,1412,1412,1412,1412,ICLR,2018,Tactical Decision Making for Lane Changing with Deep Reinforcement Learning,Mustafa Mukadam;Akansel Cosgun;Alireza Nakhaei;Kikuo Fujimura,mmukadam3@gatech.edu;acosgun@hra.com;anakhaei@hra.com;kfujimura@hra.com,3;3;3,4;5;5,Withdrawn,0,0,,yes,12/13/17,Georgia Institute of Technology;Hra;Hra;Hra,13;-1;-1;-1,33;-1;-1;-1,
1413,1413,1413,1413,1413,1413,1413,1413,ICLR,2018, A Matrix Approximation View of NCE that Justifies Self-Normalization,Jacob Goldberger;Oren Melamud,jacob.goldberger@biu.ac.il;oren@melamuds.com,6;2;3,3;4;5,Withdrawn,0,0,,yes,12/14/17,Bar Ilan University;Melamuds,-1;-1,-1;-1,3;1
1414,1414,1414,1414,1414,1414,1414,1414,ICLR,2018,Empirical Investigation on Model Capacity and Generalization of Neural Networks for Text,Anonymous,ICLR.cc/2018/Conference/Paper265/Authors,4;3;4,5;4;5,Withdrawn,0,0,,yes,1/22/18,,,,3;8
1415,1415,1415,1415,1415,1415,1415,1415,ICLR,2018,Detecting Anomalies in Communication Packet Streams based on  Generative Adversarial Networks,Anonymous,ICLR.cc/2018/Conference/Paper280/Authors,6;4;5,4;5;3,Withdrawn,0,3,,yes,1/3/18,,,,4;6;10
1416,1416,1416,1416,1416,1416,1416,1416,ICLR,2018,Distributional Inclusion Vector Embedding for Unsupervised Hypernymy Detection,Haw-Shiuan Chang;ZiYun Wang;Luke Vilnis;Andrew McCallum,hschang@cs.umass.edu;wang-zy14@mails.tsinghua.edu.cn;luke@cs.umass.edu;mccallum@cs.umass.edu,4;5;5,5;5;5,Withdrawn,0,3,,yes,12/15/17,"University of Massachusetts, Amherst;Tsinghua University;University of Massachusetts, Amherst;University of Massachusetts, Amherst",30;10;30;30,191;30;191;191,3;8
1417,1417,1417,1417,1417,1417,1417,1417,ICLR,2018,Embedding Multimodal Relational Data,Pouya Pezeshkpour;Liyan Chen;Sameer Singh,pezeshkp@uci.edu;liyanc@uci.edu;sameer@uci.edu,6;4;5,4;4;5,Withdrawn,0,0,,yes,12/13/17,"University of California, Irvine;University of California, Irvine;University of California, Irvine",36;36;36,99;99;99,
1418,1418,1418,1418,1418,1418,1418,1418,ICLR,2018,pix2code: Generating Code from a Graphical User Interface Screenshot,Anonymous,ICLR.cc/2018/Conference/Paper334/Authors,2;5;5,5;4;4,Withdrawn,0,0,,yes,12/14/17,,,,10
1419,1419,1419,1419,1419,1419,1419,1419,ICLR,2018,Improved Learning in Convolutional Neural Networks with Shifted Exponential Linear Units (ShELUs),Anonymous,ICLR.cc/2018/Conference/Paper459/Authors,1;4;3,5;5;5,Withdrawn,0,5,,yes,1/17/18,,,,
1420,1420,1420,1420,1420,1420,1420,1420,ICLR,2018,Robust Task Clustering for Deep and Diverse Multi-Task and Few-Shot Learning,Mo Yu;Xiaoxiao Guo;Jinfeng Yi;Shiyu Chang;Saloni Potdar;Gerald Tesauro;Haoyu Wang;Bowen Zhou,yum@us.ibm.com;xiaoxiao.guo@ibm.com;jinfengyi.ustc@gmail.com,5;4;5,4;4;4,Withdrawn,0,0,,yes,12/14/17,International Business Machines;International Business Machines;JD AI Research,-1;-1;-1,-1;-1;-1,6
1421,1421,1421,1421,1421,1421,1421,1421,ICLR,2018,Per-Weight Class-Based Learning Rates via Analytical Continuation,Michael Rotman;Lior Wolf,migo007@gmail.com;wolf@fb.com,3;3;3,4;2;4,Withdrawn,0,0,,yes,12/2/17,Tel Aviv University;Facebook,37;-1,217;-1,
1422,1422,1422,1422,1422,1422,1422,1422,ICLR,2018,VSE++: Improving Visual-Semantic Embeddings with Hard Negatives,Fartash Faghri;David J. Fleet;Jamie Ryan Kiros;Sanja Fidler,faghri@cs.toronto.edu;fleet@cs.toronto.edu;rkiros@cs.toronto.edu;fidler@cs.toronto.edu,4,4,Withdrawn,0,0,,yes,11/16/17,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",17;17;17;17,22;22;22;22,
1423,1423,1423,1423,1423,1423,1423,1423,ICLR,2018,Dynamically Learning the Learning Rates:  Online Hyperparameter Optimization,Tuhin Sarkar;Anima Anandkumar;Leo Dirac,tsarkar@mit.edu;animakumar@gmail.com;leodirac@amazon.com,4;5;4,4;4;4,Withdrawn,0,0,,yes,1/3/18,Massachusetts Institute of Technology;University of California-Irvine;Amazon,2;36;-1,5;99;-1,11
1424,1424,1424,1424,1424,1424,1424,1424,ICLR,2018,MULTI-MODAL GEOLOCATION ESTIMATION USING DEEP NEURAL NETWORKS,Jesse Johns;Jeremiah Rounds;Michael Henry,jesse.johns@pnnl.gov;jeremiah.rounds@pnnl.gov;michael.j.henry@pnnl.gov,4;4;3,4;4;4,Withdrawn,0,0,,yes,1/3/18,Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory,-1;-1;-1,-1;-1;-1,
1425,1425,1425,1425,1425,1425,1425,1425,ICLR,2018,Accelerating Convolutional Neural Networks using Iterative Two-Pass Decomposition,Wei-Shiang Lin;Hao-Ning Wu;Chih-Tsun Huang,weishianglin1993@gmail.com;wuhoward2002@gmail.com;cthuang@cs.nthu.edu.tw,4;3;5,4;5;4,Withdrawn,0,0,,yes,1/5/18,;;National Tsing Hua University,-1;-1;181,-1;-1;323,
1426,1426,1426,1426,1426,1426,1426,1426,ICLR,2018,Adaptive Weight Sparsity for Training Deep Neural Networks,Michael James;Jack Lindsey;Ilya Sharapov,michael@cerebras.net;jacklindsey@stanford.edu;ilya@cerebras.net,5;3;4,3;4;2,Withdrawn,0,1,,yes,1/20/18,"Cerebras Systems, Inc;Stanford University;Cerebras Systems, Inc",-1;4;-1,-1;3;-1,
1427,1427,1427,1427,1427,1427,1427,1427,ICLR,2018,Deep Net Triage: Assessing The Criticality of Network Layers by Structural Compression,Theodore S. Nowak;Jason J. Corso,tsnowak@umich.edu;jjcorso@umich.edu,5;4;5,4;4;3,Withdrawn,0,0,,yes,1/6/18,University of Michigan;University of Michigan,8;8,21;21,
1428,1428,1428,1428,1428,1428,1428,1428,ICLR,2018,Deep Active Learning over the Long Tail,Anonymous,ICLR.cc/2018/Conference/Paper718/Authors,5;4;4,3;4;4,Withdrawn,0,1,,yes,1/5/18,,,,
1429,1429,1429,1429,1429,1429,1429,1429,ICLR,2018,THE LOCAL DIMENSION OF DEEP MANIFOLD,Mengxiao Zhang;Wangquan Wu;Yanren Zhang;Kun He;Tao Yu;Huan Long;John E. Hopcroft,zmx@hust.edu.cn;u201514497@hust.edu.cn;hhxjzyr@hust.edu.cn;brooklet60@hust.edu.cn;ydtydr@sjtu.edu.cn;longhuan@cs.sjtu.edu.cn;jeh@cs.cornell.edu,3;3;5,4;3;4,Withdrawn,0,0,,yes,1/3/18,Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Shanghai Jiao Tong University;Shanghai Jiao Tong University;Cornell University,40;40;40;40;57;57;7,44;44;44;44;188;188;19,
1430,1430,1430,1430,1430,1430,1430,1430,ICLR,2018,Learning to Imagine Manipulation Goals for Robot Task Planning,Chris Paxton;Kapil Katyal;Christian Rupprecht;Raman Arora;Gregory D Hager,cpaxton@jhu.edu;kkatyal2@jhu.edu;christian.rupprecht@in.tum.de;arora@cs.jhu.edu;hager@cs.jhu.edu,3;3;3,4;3;3,Withdrawn,1,0,,yes,12/22/17,Johns Hopkins University;Johns Hopkins University;Technical University Munich;Johns Hopkins University;Johns Hopkins University,71;71;55;71;71,13;13;41;13;13,
1431,1431,1431,1431,1431,1431,1431,1431,ICLR,2018,Human-like Clustering with Deep Convolutional Neural Networks,Ali Borji;Aysegul Dundar,aliborji@gmail.com;adundar@purdue.edu,4;3,5;5,Withdrawn,1,1,,yes,12/4/17,University of Central Florida;Purdue University,81;28,1103;60,2
1432,1432,1432,1432,1432,1432,1432,1432,ICLR,2018,Attribute-aware Collaborative Filtering: Survey and Classification,Wen-Hao Chen;Chin-Chi Hsu;Mi-Yen Yeh;Shou-De Lin,b02902023@ntu.edu.tw;chinchi@iis.sinica.edu.tw;miyen@iis.sinica.edu.tw;sdlin@csie.ntu.edu.tw,5;5;4,4;5;5,Withdrawn,0,0,,yes,12/11/17,National Taiwan University;Academia Sinica;Academia Sinica;National Taiwan University,85;-1;-1;85,197;-1;-1;197,
1433,1433,1433,1433,1433,1433,1433,1433,ICLR,2018,Continuous Propagation: Layer-Parallel Training,Michael James;Devansh Arpit;Herman Sahota;Ilya Sharapov,michae@cerebras.net;devansharpit@gmail.com;herman@cerebras.net;ilya@cerebras.net,5;4;3,4;3;4,Withdrawn,1,3,,yes,1/19/18,"Cerebras Systems, Inc;University of Montreal;Cerebras Systems, Inc;Cerebras Systems, Inc",-1;124;-1;-1,-1;108;-1;-1,9
1434,1434,1434,1434,1434,1434,1434,1434,ICLR,2018,Learning Topics using Semantic Locality,Ziyi Zhao;Krittaphat Pugdeethosapol;Sheng Lin;Zhe Li;Yanzhi Wang;Qinru Qiu,zzhao37@syr.edu;kpugdeet@syr.edu;shlin@syr.edu;zli89@syr.edu;ywang393@syr.edu;qiqiu@syr.edu,3;4;3,4;4;5,Withdrawn,0,0,,yes,1/24/18,Syracuse University;Syracuse University;Syracuse University;Syracuse University;Syracuse University;Syracuse University,244;244;244;244;244;244,275;275;275;275;275;275,
1435,1435,1435,1435,1435,1435,1435,1435,ICLR,2018,Anticipatory Asynchronous Advantage Actor-Critic (A4C): The power of Anticipation in Deep Reinforcement Learning,Xun Luan;Tharun Medini;Anshumali Shrivastava,xun.luan@rice.edu;trm3@rice.edu;anshumali@rice.edu,4;2;3,4;5;5,Withdrawn,0,2,,yes,1/13/18,Rice University;Rice University;Rice University,85;85;85,86;86;86,
1436,1436,1436,1436,1436,1436,1436,1436,ICLR,2018,Sparse Deep Scattering Croisé Network,Romain Cosentino;Randall Balestriero;Richard Baraniuk;Ankit Patel,rom.cosentino@gmail.com;randallbalestriero@gmail.com;ankitpatel715@gmail.com;baraniuk@gmail.com,6,4,Withdrawn,0,0,,yes,11/25/17,Rice University;Rice University;;,85;85;-1;-1,86;86;-1;-1,
1437,1437,1437,1437,1437,1437,1437,1437,ICLR,2018,Incremental Learning in Deep Convolutional Neural Networks Using Partial Network Sharing,Syed Shakib Sarwar;Aayush Ankit;Kaushik Roy,sarwar@purdue.edu;aankit@purdue.edu;kaushik@purdue.edu,4;2;4,4;5;5,Withdrawn,0,0,,yes,12/7/17,Purdue University;Purdue University;Purdue University,28;28;28,60;60;60,6
1438,1438,1438,1438,1438,1438,1438,1438,ICLR,2018,Bayesian Embeddings for Long-Tailed Datasets,Victor Fragoso;Deva Ramanan,victor.fragoso@mail.wvu.edu;deva@andrew.cmu.edu,5;5;5,4;4;4,Withdrawn,1,3,,yes,1/17/18,West Virginia University;Carnegie Mellon University,-1;1,-1;24,11
1439,1439,1439,1439,1439,1439,1439,1439,ICLR,2018,Deep Hyperspherical Defense against Adversarial Perturbations,Weiyang Liu;Zhen Liu;Zhehui Chen;Bo Dai;Tuo Zhao;Le Song,wyliu@gatech.edu;liuzhen1994@gatech.edu;zchen451@gatech.edu;bohr.dai@gmail.com;tourzhao@gatech.edu;lsong@cc.gatech.edu,4;5;5,5;4;3,Withdrawn,0,0,,yes,12/4/17,Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology,13;13;13;13;13;13,33;33;33;33;33;33,4
1440,1440,1440,1440,1440,1440,1440,1440,ICLR,2018,FastNorm: Improving Numerical Stability of Deep Network Training with Efficient Normalization,Sadhika Malladi;Ilya Sharapov,sadhika@mit.edu;ilya@cerebras.net,4;4;3,3;4;3,Withdrawn,0,2,,yes,1/20/18,"Massachusetts Institute of Technology;Cerebras Systems, Inc",2;-1,5;-1,
1441,1441,1441,1441,1441,1441,1441,1441,ICLR,2018,Cluster-based Warm-Start Nets,Anonymous,ICLR.cc/2018/Conference/Paper998/Authors,6;3;3,5;4;4,Withdrawn,0,4,,yes,1/5/18,,,,
1442,1442,1442,1442,1442,1442,1442,1442,ICLR,2018,Do Convolutional Neural Networks act  as Compositional Nearest Neighbors?,Anonymous,ICLR.cc/2018/Conference/Paper1109/Authors,4;3;3,5;5;3,Withdrawn,1,13,,yes,1/17/18,,,,5;2
1443,1443,1443,1443,1443,1443,1443,1443,ICLR,2018,DETECTING ADVERSARIAL PERTURBATIONS WITH SALIENCY,Chiliang Zhang;Zuochang Ye;Bo Zhang;Deli Zhao,zhangcl16@mails.tsinghua.edu.cn;zuochang@tsinhua.edu.cn;zhangbo@xiaomi.com;zhaodeli@gmail.com,3;4;4;4,5;4;4;4,Withdrawn,0,1,,yes,12/12/17,Tsinghua University;Tsinghua University;Xiaomi;Xiaomi,10;10;-1;-1,30;30;-1;-1,4;8
1444,1444,1444,1444,1444,1444,1444,1444,ICLR,2018,Withdraw,Liyuan Liu;Jingbo Shang;Xiaotao Gu;Xiang Ren;Jian Peng;Jiawei Han,ll2@illinois.edu;shang7@illinois.edu;xiaotao2@illinois.du;xiangren@usc.edu;jianpeng@illinois.edu;hanj@illinois.edu,5;4;3,3;5;5,Withdrawn,0,0,,yes,12/15/17,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;;University of Southern California;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",3;3;-1;31;3;3,37;37;-1;66;37;37,
1445,1445,1445,1445,1445,1445,1445,1445,ICLR,2018,Complex- and Real-Valued Neural Network Architectures,Nils Moenning;Suresh Manandhar,nm819@york.ac.uk;suresh.manandhar@york.ac.uk,2;4;3,5;4;4,Withdrawn,0,3,,yes,1/3/18,University of York;University of York,210;210,137;137,
1446,1446,1446,1446,1446,1446,1446,1446,ICLR,2018,Learning with Mental Imagery,Anonymous,ICLR.cc/2018/Conference/Paper839/Authors,4;3;3,4;4;4,Withdrawn,0,0,,yes,1/5/18,,,,5;4
1447,1447,1447,1447,1447,1447,1447,1447,ICLR,2018,Withdrawn,withdrawn.,withdrawn,5;7;5,5;4;4,Withdrawn,0,0,,yes,1/2/18,,,,
1448,1448,1448,1448,1448,1448,1448,1448,ICLR,2018,HyperNetworks with statistical filtering for defending adversarial examples,Anonymous,ICLR.cc/2018/Conference/Paper293/Authors,5;4;5,3;3;4,Withdrawn,0,0,,yes,12/18/17,,,,4
1449,1449,1449,1449,1449,1449,1449,1449,ICLR,2018,withdrawn,withdrawn,withdrawn,4;5;4,4;4;4,Withdrawn,0,0,,yes,9/27/18,,,,
1450,1450,1450,1450,1450,1450,1450,1450,ICLR,2019,RANDOM MASK: Towards Robust Convolutional Neural Networks,Tiange Luo;Tianle Cai;Mengxiao Zhang;Siyu Chen;Liwei Wang,luotg@pku.edu.cn;caitianle1998@pku.edu.cn;zhan147@usc.edu;siyuchen@pku.edu.cn;wanglw@cis.pku.edu.cn,4;6;7,3;3;3,Reject,10,18,1.0,yes,9/27/18,Peking University;Peking University;University of Southern California;Peking University;Peking University,24;24;30;24;24,27;27;66;27;27,4
1451,1451,1451,1451,1451,1451,1451,1451,ICLR,2019,"Unsupervised Discovery of Parts, Structure, and Dynamics",Zhenjia Xu*;Zhijian Liu*;Chen Sun;Kevin Murphy;William T. Freeman;Joshua B. Tenenbaum;Jiajun Wu,xuzhenjia1997@gmail.com;zhijian@mit.edu;chensun@google.com;kpmurphy@google.com;billf@mit.edu;jbt@mit.edu;jiajunwu@mit.edu,6;6;7;5,3;3;4;3,Accept (Poster),0,14,0.0,yes,9/27/18,Shanghai Jiao Tong University;Massachusetts Institute of Technology;Google;Google;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,52;2;-1;-1;2;2;2,188;5;-1;-1;5;5;5,
1452,1452,1452,1452,1452,1452,1452,1452,ICLR,2019,BabyAI: A Platform to Study the Sample Efficiency of Grounded Language Learning,Maxime Chevalier-Boisvert;Dzmitry Bahdanau;Salem Lahlou;Lucas Willems;Chitwan Saharia;Thien Huu Nguyen;Yoshua Bengio,maximechevalierb@gmail.com;dimabgv@gmail.com;salemlahlou9@gmail.com;lcswillems@gmail.com;chitwaniit@gmail.com;thien@cs.uoregon.edu;yoshua.bengio@umontreal.ca,7;6;6,5;4;4,Accept (Poster),0,10,0.0,yes,9/27/18,University of Montreal;University of Montreal;University of Montreal;Ecole Normale Superieure;Indian Institute of Technology Bombay;University of Oregon;University of Montreal,123;123;123;99;115;199;123,108;108;108;603;367;267;108,
1453,1453,1453,1453,1453,1453,1453,1453,ICLR,2019,Environment Probing Interaction Policies,Wenxuan Zhou;Lerrel Pinto;Abhinav Gupta,wenxuanz@andrew.cmu.edu;lerrelp@andrew.cmu.edu;abhinavg@cs.cmu.edu,6;6;6,3;2;4,Accept (Poster),0,5,0.0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1,24;24;24,8
1454,1454,1454,1454,1454,1454,1454,1454,ICLR,2019,Classifier-agnostic saliency map extraction,Konrad Zolna;Krzysztof J. Geras;Kyunghyun Cho,konrad.zolna@gmail.com;k.j.geras@nyu.edu;kyunghyun.cho@nyu.edu,4;5;4,3;4;4,Reject,0,5,1.0,yes,9/27/18,Jagiellonian University;New York University;New York University,478;26;26,695;27;27,
1455,1455,1455,1455,1455,1455,1455,1455,ICLR,2019,Reinforced Imitation Learning from Observations,Konrad Zolna;Negar Rostamzadeh;Yoshua Bengio;Sungjin Ahn;Pedro O. Pinheiro,konrad.zolna@gmail.com;negar.rostamzadeh@gmail.com;yoshua.umontreal@gmail.com;sjn.ahn@gmail.com;pedro@opinheiro.com,5;6;4,5;2;4,Reject,0,6,0.0,yes,9/27/18,Jagiellonian University;Element AI;University of Montreal;Rutgers University;Opinheiro,478;-1;123;34;-1,695;-1;108;172;-1,
1456,1456,1456,1456,1456,1456,1456,1456,ICLR,2019,Max-MIG: an Information Theoretic Approach for Joint Learning from Crowds,Peng Cao*;Yilun Xu*;Yuqing Kong;Yizhou  Wang,caopeng2016@pku.edu.cn;xuyilun@pku.edu.cn;yuqing.kong@pku.edu.cn;yizhou.wang@pku.edu.cn,6;7;6,4;4;4,Accept (Poster),0,8,0.0,yes,9/27/18,Peking University;Peking University;Peking University;Peking University,24;24;24;24,27;27;27;27,
1457,1457,1457,1457,1457,1457,1457,1457,ICLR,2019,Understanding Straight-Through Estimator in Training Activation Quantized Neural Nets,Penghang Yin;Jiancheng Lyu;Shuai Zhang;Stanley Osher;Yingyong Qi;Jack Xin,yph@ucla.edu;jianchel@uci.edu;shuazhan@qti.qualcomm.com;sjo@math.ucla.edu;yingyong@qti.qualcomm.com;jxin@math.uci.edu,6;7;7,3;4;4,Accept (Poster),1,10,0.0,yes,9/27/18,"University of California, Los Angeles;University of California, Irvine;Qualcomm Inc, QualComm;University of California, Los Angeles;Qualcomm Inc, QualComm;University of California, Irvine",20;35;-1;20;-1;35,15;99;-1;15;-1;99,1
1458,1458,1458,1458,1458,1458,1458,1458,ICLR,2019,Learning Multi-Level Hierarchies with Hindsight,Andrew Levy;George Konidaris;Robert Platt;Kate Saenko,andrew_levy2@brown.edu;gdk@cs.brown.edu;saenko@bu.edu;rplatt@ccs.neu.edu,6;7;5,4;4;4,Accept (Poster),0,7,0.0,yes,9/27/18,Brown University;Brown University;Boston University;Northeastern University,65;65;65;16,50;50;70;839,
1459,1459,1459,1459,1459,1459,1459,1459,ICLR,2019,NOODL: Provable Online Dictionary Learning and Sparse Coding,Sirisha Rambhatla;Xingguo Li;Jarvis Haupt,rambh002@umn.edu;lixx1661@umn.edu;jdhaupt@umn.edu,7;6;7,2;2;2,Accept (Poster),0,3,0.0,yes,9/27/18,"University of Minnesota, Minneapolis;University of Minnesota, Minneapolis;University of Minnesota, Minneapolis",57;57;57,56;56;56,
1460,1460,1460,1460,1460,1460,1460,1460,ICLR,2019,Learning to Infer and Execute 3D Shape Programs,Yonglong Tian;Andrew Luo;Xingyuan Sun;Kevin Ellis;William T. Freeman;Joshua B. Tenenbaum;Jiajun Wu,yonglong@mit.edu;aluo@mit.edu;xs5@princeton.edu;ellisk@mit.edu;billf@mit.edu;jbt@mit.edu;jiajunwu@mit.edu,6;7;7,4;5;4,Accept (Poster),3,13,0.0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Princeton University;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;30;2;2;2;2,5;5;7;5;5;5;5,
1461,1461,1461,1461,1461,1461,1461,1461,ICLR,2019,Meta-learning with differentiable closed-form solvers,Luca Bertinetto;Joao F. Henriques;Philip Torr;Andrea Vedaldi,luca@robots.ox.ac.uk;joao@robots.ox.ac.uk;philip.torr@eng.ox.ac.uk;vedaldi@robots.ox.ac.uk,5;2;7,3;5;4,Accept (Poster),0,21,3.0,yes,9/27/18,University of Oxford;University of Oxford;University of Oxford;University of Oxford,50;50;50;50,1;1;1;1,6
1462,1462,1462,1462,1462,1462,1462,1462,ICLR,2019,Adaptive Gradient Methods with Dynamic Bound of Learning Rate,Liangchen Luo;Yuanhao Xiong;Yan Liu;Xu Sun,luolc@pku.edu.cn;xiongyh@zju.edu.cn;yanliu.cs@usc.edu;xusun@pku.edu.cn,7;4;6,4;5;4,Accept (Poster),9,6,1.0,yes,9/27/18,Peking University;Zhejiang University;University of Southern California;Peking University,24;57;30;24,27;177;66;27,1;8
1463,1463,1463,1463,1463,1463,1463,1463,ICLR,2019,AD-VAT: An Asymmetric Dueling mechanism for learning Visual Active Tracking,Fangwei Zhong;Peng Sun;Wenhan Luo;Tingyun Yan;Yizhou Wang,zfw@pku.edu.cn;pengsun000@gmail.com;whluo.china@gmail.com;yanty18@pku.edu.cn;yizhou.wang@pku.edu.cn,5;4;6,4;3;4,Accept (Poster),0,5,0.0,yes,9/27/18,Peking University;Tecent Inc.;Tencent AI Lab;Peking University;Peking University,24;-1;-1;24;24,27;-1;-1;27;27,4;8
1464,1464,1464,1464,1464,1464,1464,1464,ICLR,2019,Variational Autoencoder with Arbitrary Conditioning,Oleg Ivanov;Michael Figurnov;Dmitry Vetrov,tigvarts@gmail.com;michael@figurnov.ru;vetrovd@yandex.ru,6;7;6,3;3;3,Accept (Poster),0,8,2.0,yes,9/27/18,Samsung;Google;Higher School of Economics,-1;-1;478,-1;-1;377,5
1465,1465,1465,1465,1465,1465,1465,1465,ICLR,2019,Optimistic mirror descent in saddle-point problems: Going the extra (gradient) mile,Panayotis Mertikopoulos;Bruno Lecouat;Houssam Zenati;Chuan-Sheng Foo;Vijay Chandrasekhar;Georgios Piliouras,panayotis.mertikopoulos@imag.fr;bruno_lecouat@i2r.a-star.edu.sg;houssam_zenati@i2r.a-star.edu.sg;foocs@i2r.a-star.edu.sg;vijay@i2r.a-star.edu.sg;georgios@sutd.edu.sg,7;6;5,3;5;5,Accept (Poster),0,5,2.0,yes,9/27/18,Imag Montpellier Université;A*STAR;A*STAR;A*STAR;A*STAR;Singapore University of Technology and Design,-1;-1;-1;-1;-1;478,-1;-1;-1;-1;-1;1103,5;4
1466,1466,1466,1466,1466,1466,1466,1466,ICLR,2019,A2BCD: Asynchronous Acceleration with Optimal Complexity,Robert Hannah;Fei Feng;Wotao Yin,roberthannah89@gmail.com;fei.feng@math.ucla.edu;wotaoyin@math.ucla.edu,7;7;9,4;5;5,Accept (Poster),0,6,0.0,yes,9/27/18,"University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles",20;20;20,15;15;15,1
1467,1467,1467,1467,1467,1467,1467,1467,ICLR,2019,Minimal Random Code Learning: Getting Bits Back from Compressed Model Parameters,Marton Havasi;Robert Peharz;José Miguel Hernández-Lobato,mh740@cam.ac.uk;rp587@cam.ac.uk;jmh233@cam.ac.uk,7;6;7,4;2;3,Accept (Poster),2,6,0.0,yes,9/27/18,University of Cambridge;University of Cambridge;University of Cambridge,71;71;71,2;2;2,
1468,1468,1468,1468,1468,1468,1468,1468,ICLR,2019,Model-Predictive Policy Learning with Uncertainty Regularization for Driving in Dense Traffic,Mikael Henaff;Alfredo Canziani;Yann LeCun,mbh305@nyu.edu;canziani@nyu.edu;yann@cs.nyu.edu,6;7;6,4;5;5,Accept (Poster),0,6,0.0,yes,9/27/18,New York University;New York University;New York University,26;26;26,27;27;27,
1469,1469,1469,1469,1469,1469,1469,1469,ICLR,2019,Feature-Wise Bias Amplification,Klas Leino;Emily Black;Matt Fredrikson;Shayak Sen;Anupam Datta,kleino@cs.cmu.edu;emilybla@cs.cmu.edu;mfredrik@cs.cmu.edu;shayaks@cs.cmu.edu;danupam@cmu.edu,6;6;7,5;4;4,Accept (Poster),0,13,0.0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1;1,24;24;24;24;24,
1470,1470,1470,1470,1470,1470,1470,1470,ICLR,2019,RelGAN: Relational Generative Adversarial Networks for Text Generation,Weili Nie;Nina Narodytska;Ankit Patel,wn8@rice.edu;nnarodytska@vmware.com;abp4@rice.edu,6;8;6,4;4;4,Accept (Poster),0,0,6.0,yes,9/27/18,Rice University;VMware Inc;Rice University,85;-1;85,86;-1;86,5;4
1471,1471,1471,1471,1471,1471,1471,1471,ICLR,2019,Neural Speed Reading with Structural-Jump-LSTM,Christian Hansen;Casper Hansen;Stephen Alstrup;Jakob Grue Simonsen;Christina Lioma,chrh@di.ku.dk;c.hansen@di.ku.dk;s.alstrup@di.ku.dk;simonsen@di.ku.dk;c.lioma@di.ku.dk,7;7;5,5;4;4,Accept (Poster),0,4,0.0,yes,9/27/18,University of Copenhagen;University of Copenhagen;University of Copenhagen;University of Copenhagen;University of Copenhagen,99;99;99;99;99,109;109;109;109;109,3
1472,1472,1472,1472,1472,1472,1472,1472,ICLR,2019,Time-Agnostic Prediction: Predicting Predictable Video Frames,Dinesh Jayaraman;Frederik Ebert;Alexei Efros;Sergey Levine,dineshjayaraman@berkeley.edu;febert@berkeley.edu;efros@eecs.berkeley.edu;svlevine@eecs.berkeley.edu,8;7;7,4;3;5,Accept (Poster),0,7,1.0,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5,18;18;18;18,
1473,1473,1473,1473,1473,1473,1473,1473,ICLR,2019,Generative Question Answering: Learning to Answer the Whole Question,Mike Lewis;Angela Fan,mikelewis@fb.com;angelafan@fb.com,7;8;6,4;4;4,Accept (Poster),1,6,0.0,yes,9/27/18,Facebook;Facebook,-1;-1,-1;-1,5;4
1474,1474,1474,1474,1474,1474,1474,1474,ICLR,2019,Episodic Curiosity through Reachability,Nikolay Savinov;Anton Raichuk;Damien Vincent;Raphael Marinier;Marc Pollefeys;Timothy Lillicrap;Sylvain Gelly,nikolay.savinov@inf.ethz.ch;raveman@google.com;damienv@google.com;raphaelm@google.com;marc.pollefeys@inf.ethz.ch;countzero@google.com;sylvaingelly@google.com,8;7;8;6,4;4;3;4,Accept (Poster),2,17,0.0,yes,9/27/18,Swiss Federal Institute of Technology;Google;Google;Google;Swiss Federal Institute of Technology;Google;Google,10;-1;-1;-1;10;-1;-1,10;-1;-1;-1;10;-1;-1,
1475,1475,1475,1475,1475,1475,1475,1475,ICLR,2019,Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks,Yikang Shen;Shawn Tan;Alessandro Sordoni;Aaron Courville,yikang.shn@gmail.com;shawn@wtf.sg;alsordon@microsoft.com;aaron.courville@gmail.com,7;9;8,3;4;4,Accept (Oral),0,6,0.0,yes,9/27/18,University of Montreal;University of Montreal;Microsoft;University of Montreal,123;123;-1;123,108;108;-1;108,3
1476,1476,1476,1476,1476,1476,1476,1476,ICLR,2019,Automatically Composing Representation Transformations as a Means for Generalization,Michael Chang;Abhishek Gupta;Sergey Levine;Thomas L. Griffiths,mbchang@berkeley.edu;abhigupta@berkeley.edu;svlevine@eecs.berkeley.edu;tom_griffiths@berkeley.edu,7;9;7,2;4;3,Accept (Poster),1,8,0.0,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5,18;18;18;18,10;8
1477,1477,1477,1477,1477,1477,1477,1477,ICLR,2019,Bayesian Policy Optimization for Model Uncertainty,Gilwoo Lee;Brian Hou;Aditya Mandalika;Jeongseok Lee;Sanjiban Choudhury;Siddhartha S. Srinivasa,gilwoo@cs.uw.edu;bhou@cs.uw.edu;adityavk@cs.uw.edu;jslee02@cs.uw.edu;sanjibac@cs.uw.edu;siddh@cs.uw.edu,5;7;6;7,4;4;3;3,Accept (Poster),0,9,1.0,yes,9/27/18,"University of Washington, Seattle;University of Washington, Seattle;University of Washington, Seattle;University of Washington, Seattle;University of Washington, Seattle;University of Washington, Seattle",6;6;6;6;6;6,25;25;25;25;25;25,11
1478,1478,1478,1478,1478,1478,1478,1478,ICLR,2019,Diversity-Sensitive Conditional Generative Adversarial Networks,Dingdong Yang;Seunghoon Hong;Yunseok Jang;Tianchen Zhao;Honglak Lee,didoyang@umich.edu;hongseu@umich.edu;yunseokj@umich.edu;ericolon@umich.edu;honglak@eecs.umich.edu,6;7;7,5;3;5,Accept (Poster),1,5,0.0,yes,9/27/18,University of Michigan;University of Michigan;University of Michigan;University of Michigan;University of Michigan,8;8;8;8;8,21;21;21;21;21,5;4
1479,1479,1479,1479,1479,1479,1479,1479,ICLR,2019,Poincare Glove: Hyperbolic Word Embeddings,Alexandru Tifrea*;Gary Becigneul*;Octavian-Eugen Ganea*,tifreaa@student.ethz.ch;gary.becigneul@inf.ethz.ch;octavian.ganea@inf.ethz.ch,6;7;6,4;3;4,Accept (Poster),0,5,0.0,yes,9/27/18,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10,10;10;10,3;1;10
1480,1480,1480,1480,1480,1480,1480,1480,ICLR,2019,Stochastic Optimization of Sorting Networks via Continuous Relaxations,Aditya Grover;Eric Wang;Aaron Zweig;Stefano Ermon,adityag@cs.stanford.edu;ejwang@cs.stanford.edu;azweig@cs.stanford.edu;ermon@cs.stanford.edu,7;8;6,3;4;3,Accept (Poster),0,5,0.0,yes,9/27/18,Stanford University;Stanford University;Stanford University;Stanford University,4;4;4;4,3;3;3;3,10
1481,1481,1481,1481,1481,1481,1481,1481,ICLR,2019,Hyperbolic Attention Networks,Caglar Gulcehre;Misha Denil;Mateusz Malinowski;Ali Razavi;Razvan Pascanu;Karl Moritz Hermann;Peter Battaglia;Victor Bapst;David Raposo;Adam Santoro;Nando de Freitas,ca9lar@gmail.com;mdenil@google.com;mateuszm@google.com;alirazavi@google.com;razp@google.com;kmh@google.com;vbapst@google.com;drapso@google.com;adamsantoro@google.com;nandodefreitas@google.com,6;7;7,5;5;4,Accept (Poster),2,5,0.0,yes,9/27/18,Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,3;10;8
1482,1482,1482,1482,1482,1482,1482,1482,ICLR,2019,Adv-BNN: Improved Adversarial Defense through Robust Bayesian Neural Network,Xuanqing Liu;Yao Li;Chongruo Wu;Cho-Jui Hsieh,xqliu@cs.ucla.edu;yaoli@ucdavis.edu;crwu@ucdavis.edu;chohsieh@cs.ucla.edu,7;6;7,3;4;3,Accept (Poster),0,13,0.0,yes,9/27/18,"University of California, Los Angeles;University of California, Davis;University of California, Davis;University of California, Los Angeles",20;81;81;20,15;54;54;15,4;11
1483,1483,1483,1483,1483,1483,1483,1483,ICLR,2019,How Important is a Neuron,Kedar Dhamdhere;Mukund Sundararajan;Qiqi Yan,kedar@google.com;mukunds@google.com;qiqiyan@google.com,7;7;7,4;2;5,Accept (Poster),2,7,0.0,yes,9/27/18,Google;Google;Google,-1;-1;-1,-1;-1;-1,
1484,1484,1484,1484,1484,1484,1484,1484,ICLR,2019,Small nonlinearities in activation functions create bad local minima in neural networks,Chulhee Yun;Suvrit Sra;Ali Jadbabaie,chulheey@mit.edu;suvrit@mit.edu;jadbabai@mit.edu,7;7;8,3;3;4,Accept (Poster),0,5,1.0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2,5;5;5,1
1485,1485,1485,1485,1485,1485,1485,1485,ICLR,2019,Efficiently testing local optimality and escaping saddles for ReLU networks,Chulhee Yun;Suvrit Sra;Ali Jadbabaie,chulheey@mit.edu;suvrit@mit.edu;jadbabai@mit.edu,3;6;6;8,4;2;3;3,Accept (Poster),0,10,0.0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2,5;5;5,1;9
1486,1486,1486,1486,1486,1486,1486,1486,ICLR,2019,Local SGD Converges Fast and Communicates Little,Sebastian U. Stich,sebastian.stich@epfl.ch,8;5;8,5;5;4,Accept (Poster),0,7,0.0,yes,9/27/18,Swiss Federal Institute of Technology Lausanne,478,38,1;9
1487,1487,1487,1487,1487,1487,1487,1487,ICLR,2019,Learning to Learn without Forgetting by Maximizing Transfer and Minimizing Interference,Matthew Riemer;Ignacio Cases;Robert Ajemian;Miao Liu;Irina Rish;Yuhai Tu;and Gerald Tesauro,mdriemer@us.ibm.com;cases@stanford.edu;ajemian@mit.edu;miao.liu1@ibm.com;rish@us.ibm.com;yuhai@us.ibm.com;gtesauro@us.ibm.com,6;8;7,5;4;5,Accept (Poster),2,6,0.0,yes,9/27/18,International Business Machines;Stanford University;Massachusetts Institute of Technology;International Business Machines;International Business Machines;International Business Machines;International Business Machines,-1;4;2;-1;-1;-1;-1,-1;3;5;-1;-1;-1;-1,6
1488,1488,1488,1488,1488,1488,1488,1488,ICLR,2019,Excessive Invariance Causes Adversarial Vulnerability,Joern-Henrik Jacobsen;Jens Behrmann;Richard Zemel;Matthias Bethge,j.jacobsen@vectorinstitute.ai;jensb@uni-bremen.de;zemel@cs.toronto.edu;matthias.bethge@uni-tuebingen.de,6;6;7,4;2;4,Accept (Poster),0,8,6.0,yes,9/27/18,"Vector Institute;Universität Bremen;Department of Computer Science, University of Toronto;University of Tuebingen",-1;153;18;153,-1;291;22;94,4
1489,1489,1489,1489,1489,1489,1489,1489,ICLR,2019,Decoupled Weight Decay Regularization,Ilya Loshchilov;Frank Hutter,ilya.loshchilov@gmail.com;fh@cs.uni-freiburg.de,6;7;5,4;4;4,Accept (Poster),0,6,0.0,yes,9/27/18,;Universität Freiburg,-1;123,-1;82,8
1490,1490,1490,1490,1490,1490,1490,1490,ICLR,2019,Learning Robust Representations by Projecting Superficial Statistics Out,Haohan Wang;Zexue He;Zachary C. Lipton;Eric P. Xing,haohanw@cs.cmu.edu;zexueh@mail.bnu.edu.cn;zlipton@cmu.edu;epxing@cs.cmu.edu,7;7;9,4;4;3,Accept (Oral),0,4,0.0,yes,9/27/18,Carnegie Mellon University;Australian National University;Carnegie Mellon University;Carnegie Mellon University,1;106;1;1,24;48;24;24,8
1491,1491,1491,1491,1491,1491,1491,1491,ICLR,2019,Spectral Inference Networks: Unifying Deep and Spectral Learning,David Pfau;Stig Petersen;Ashish Agarwal;David G. T. Barrett;Kimberly L. Stachenfeld,pfau@google.com;svp@google.com;agarwal@google.com;barrettdavid@google.com;stachenfeld@google.com,5;7;5,3;3;3,Accept (Poster),0,3,0.0,yes,9/27/18,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,5;10
1492,1492,1492,1492,1492,1492,1492,1492,ICLR,2019,A rotation-equivariant convolutional neural network model of primary visual cortex,Alexander S. Ecker;Fabian H. Sinz;Emmanouil Froudarakis;Paul G. Fahey;Santiago A. Cadena;Edgar Y. Walker;Erick Cobos;Jacob Reimer;Andreas S. Tolias;Matthias Bethge,alexander.ecker@uni-tuebingen.de;sinz@bcm.edu;froudara@bcm.edu;paul.fahey@bcm.edu;sa.cadena721@gmail.com;eywalker@bcm.edu;emcobost@gmail.com;reimer@bcm.edu;astolias@bcm.edu;matthias.bethge@bethgelab.org,5;7;8,4;4;3,Accept (Poster),0,21,1.0,yes,9/27/18,"University of Tuebingen;Baylor College of Medicine;Baylor College of Medicine;Baylor College of Medicine;University of Tuebingen;Baylor College of Medicine;;Baylor College of Medicine;Baylor College of Medicine;Centre for Integrative Neuroscience, AG Bethge",153;-1;-1;-1;153;-1;-1;-1;-1;153,94;-1;-1;-1;94;-1;-1;-1;-1;94,
1493,1493,1493,1493,1493,1493,1493,1493,ICLR,2019,Backpropamine: training self-modifying neural networks with differentiable neuromodulated plasticity,Thomas Miconi;Aditya Rawal;Jeff Clune;Kenneth O. Stanley,tmiconi@uber.com;aditya.rawal@uber.com;jeffclune@uber.com;kstanley@uber.com,5;4;9,4;4;4,Accept (Poster),0,9,0.0,yes,9/27/18,Uber;Uber;Uber;Uber,-1;-1;-1;-1,-1;-1;-1;-1,3
1494,1494,1494,1494,1494,1494,1494,1494,ICLR,2019,Enabling Factorized Piano Music Modeling and Generation with the MAESTRO Dataset,Curtis Hawthorne;Andriy Stasyuk;Adam Roberts;Ian Simon;Cheng-Zhi Anna Huang;Sander Dieleman;Erich Elsen;Jesse Engel;Douglas Eck,fjord@google.com;astas@google.com;adarob@google.com;iansimon@google.com;annahuang@google.com;sedielem@google.com;eriche@google.com;jesseengel@google.com;deck@google.com,8;8;8,5;2;4,Accept (Oral),0,4,0.0,yes,9/27/18,Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1,
1495,1495,1495,1495,1495,1495,1495,1495,ICLR,2019,Invariant and Equivariant Graph Networks,Haggai Maron;Heli Ben-Hamu;Nadav Shamir;Yaron Lipman,haggai.maron@weizmann.ac.il;heli.benhamu@weizmann.ac.il;nadav13@gmail.com;yaron.lipman@weizmann.ac.il,8;4;9,5;5;4,Accept (Poster),0,8,0.0,yes,9/27/18,Weizmann Institute;Weizmann Institute;;Weizmann Institute,106;106;-1;106,1103;1103;-1;1103,10;8
1496,1496,1496,1496,1496,1496,1496,1496,ICLR,2019,Solving the Rubik's Cube with Approximate Policy Iteration,Stephen McAleer;Forest Agostinelli;Alexander Shmakov;Pierre Baldi,smcaleer@uci.edu;fagostin@uci.edu;ashmakov@uci.edu;pfbaldi@ics.uci.edu,7;7;7,4;4;3,Accept (Poster),0,3,1.0,yes,9/27/18,"University of California, Irvine;University of California, Irvine;University of California, Irvine;University of California, Irvine",35;35;35;35,99;99;99;99,
1497,1497,1497,1497,1497,1497,1497,1497,ICLR,2019,Execution-Guided Neural Program Synthesis,Xinyun Chen;Chang Liu;Dawn Song,xinyun.chen@berkeley.edu;liuchang@eecs.berkeley.edu;dawnsong.travel@gmail.com,7;7;7,4;2;5,Accept (Poster),0,12,0.0,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,18;18;18,
1498,1498,1498,1498,1498,1498,1498,1498,ICLR,2019,Transferring Knowledge across Learning Processes,Sebastian Flennerhag;Pablo G. Moreno;Neil D. Lawrence;Andreas Damianou,sflennerhag@turing.ac.uk;morepabl@amazon.com;lawrennd@amazon.com;damianou@amazon.com,8;8;6,3;4;4,Accept (Oral),0,13,0.0,yes,9/27/18,Alan Turing Institute;Amazon;Amazon;Amazon,-1;-1;-1;-1,-1;-1;-1;-1,6;2
1499,1499,1499,1499,1499,1499,1499,1499,ICLR,2019,Theoretical Analysis of Auto Rate-Tuning by Batch Normalization,Sanjeev Arora;Zhiyuan Li;Kaifeng Lyu,arora@cs.princeton.edu;zhiyuanli@cs.princeton.edu;vfleaking@gmail.com,7;7;7,4;2;2,Accept (Poster),0,7,0.0,yes,9/27/18,Princeton University;Princeton University;Tsinghua University,30;30;8,7;7;30,1;9;8
1500,1500,1500,1500,1500,1500,1500,1500,ICLR,2019,Neural Logic Machines,Honghua Dong;Jiayuan Mao;Tian Lin;Chong Wang;Lihong Li;Denny Zhou,dhh19951@gmail.com;maojiayuan@gmail.com;tianlin@google.com;chongw@google.com;lihongli.cs@gmail.com;dennyzhou@gmail.com,6;7;5,3;2;5,Accept (Poster),0,0,9.0,yes,9/27/18,Tsinghua University;Tsinghua University;Google;Google;Google;Google,8;8;-1;-1;-1;-1,30;30;-1;-1;-1;-1,10;8
1501,1501,1501,1501,1501,1501,1501,1501,ICLR,2019,"The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision",Jiayuan Mao;Chuang Gan;Pushmeet Kohli;Joshua B. Tenenbaum;Jiajun Wu,maojiayuan@gmail.com;ganchuang1990@gmail.com;pushmeet@google.com;jbt@mit.edu;jiajunwu@mit.edu,6;7;9,4;4;5,Accept (Oral),0,9,1.0,yes,9/27/18,Tsinghua University;International Business Machines;Google;Massachusetts Institute of Technology;Massachusetts Institute of Technology,8;-1;-1;2;2,30;-1;-1;5;5,8
1502,1502,1502,1502,1502,1502,1502,1502,ICLR,2019,Disjoint Mapping Network for Cross-modal Matching of Voices and Faces,Yandong Wen;Mahmoud Al Ismail;Weiyang Liu;Bhiksha Raj;Rita Singh,yandongw@andrew.cmu.edu;mahmoudi@andrew.cmu.edu;wyliu@gatech.edu;bhiksha@cs.cmu.edu;rsingh@cs.cmu.edu,7;6;7,4;3;4,Accept (Poster),0,3,0.0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;Georgia Institute of Technology;Carnegie Mellon University;Carnegie Mellon University,1;1;13;1;1,24;24;33;24;24,
1503,1503,1503,1503,1503,1503,1503,1503,ICLR,2019,Learning Neural PDE Solvers with Convergence Guarantees,Jun-Ting Hsieh;Shengjia Zhao;Stephan Eismann;Lucia Mirabella;Stefano Ermon,junting@stanford.edu;sjzhao@stanford.edu;seismann@stanford.edu;lucia.mirabella@siemens.com;ermon@cs.stanford.edu,7;8;6,4;4;3,Accept (Poster),2,3,2.0,yes,9/27/18,Stanford University;Stanford University;Stanford University;Siemens Corporate Research;Stanford University,4;4;4;-1;4,3;3;3;-1;3,
1504,1504,1504,1504,1504,1504,1504,1504,ICLR,2019,Dimensionality Reduction for Representing the Knowledge of Probabilistic Models,Marc T Law;Jake Snell;Amir-massoud Farahmand;Raquel Urtasun;Richard S Zemel,law@cs.toronto.edu;jsnell@cs.toronto.edu;farahmand@vectorinstitute.ai;urtasun@cs.toronto.edu;zemel@cs.toronto.edu,7;6;9,4;1;3,Accept (Poster),0,4,0.0,yes,9/27/18,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Vector Institute;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",18;18;-1;18;18,22;22;-1;22;22,8;1;6
1505,1505,1505,1505,1505,1505,1505,1505,ICLR,2019,Improving the Generalization of Adversarial Training with Domain Adaptation,Chuanbiao Song;Kun He;Liwei Wang;John E. Hopcroft,cbsong@hust.edu.cn;brooklet60@hust.edu.cn;wanglw@pku.edu.cn;jeh@cs.cornell.edu,6;6;6,2;3;4,Accept (Poster),0,7,0.0,yes,9/27/18,Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Peking University;Cornell University,39;39;24;7,44;44;27;19,4;8
1506,1506,1506,1506,1506,1506,1506,1506,ICLR,2019,Towards Understanding Regularization in Batch Normalization,Ping Luo;Xinjiang Wang;Wenqi Shao;Zhanglin Peng,pluo@ie.cuhk.edu.hk;wangxinjiang@sensetime.com;shaowenqi@sensetime.com;zhanglinpeng@sensetime.com,5;6;6,3;5;2,Accept (Poster),0,4,0.0,yes,9/27/18,The Chinese University of Hong Kong;SenseTime Group Limited;SenseTime Group Limited;SenseTime Group Limited,57;-1;-1;-1,40;-1;-1;-1,8
1507,1507,1507,1507,1507,1507,1507,1507,ICLR,2019,Differentiable Learning-to-Normalize via Switchable Normalization,Ping Luo;Jiamin Ren;Zhanglin Peng;Ruimao Zhang;Jingyu Li,pluo@ie.cuhk.edu.hk;renjiamin@sensetime.com;pengzhanglin@sensetime.com;zhangruimao@sensetime.com;lijingyu@sensetime.com,7;7;7,5;3;4,Accept (Poster),0,2,1.0,yes,9/27/18,The Chinese University of Hong Kong;SenseTime Group Limited;SenseTime Group Limited;SenseTime Group Limited;SenseTime Group Limited,57;-1;-1;-1;-1,40;-1;-1;-1;-1,
1508,1508,1508,1508,1508,1508,1508,1508,ICLR,2019,Neural Graph Evolution: Towards Efficient Automatic Robot Design,Tingwu Wang;Yuhao Zhou;Sanja Fidler;Jimmy Ba,tingwuwang@cs.toronto.edu;henryzhou@cs.toronto.edu;fidler@cs.toronto.edu;jba@cs.toronto.edu,5;8;6,4;4;4,Accept (Poster),0,8,0.0,yes,9/27/18,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",18;18;18;18,22;22;22;22,10
1509,1509,1509,1509,1509,1509,1509,1509,ICLR,2019,Systematic Generalization: What Is Required and Can It Be Learned?,Dzmitry Bahdanau*;Shikhar Murty*;Michael Noukhovitch;Thien Huu Nguyen;Harm de Vries;Aaron Courville,dimabgv@gmail.com;shikhar.murty@gmail.com;michael.noukhovitch@umontreal.ca;thien@cs.uoregon.edu;mail@harmdevries.com;aaron.courville@gmail.com,6;6;4,3;5;4,Accept (Poster),0,9,0.0,yes,9/27/18,University of Montreal;University of Montreal;University of Montreal;University of Oregon;University of Montreal;University of Montreal,123;123;123;199;123;123,108;108;108;267;108;108,8
1510,1510,1510,1510,1510,1510,1510,1510,ICLR,2019,Preferences Implicit in the State of the World,Rohin Shah;Dmitrii Krasheninnikov;Jordan Alexander;Pieter Abbeel;Anca Dragan,rohinmshah@berkeley.edu;dmitrii.krasheninnikov@student.uva.nl;jfalex@stanford.edu;pabbeel@cs.berkeley.edu;anca@berkeley.edu,6;6;7;7,3;4;3;4,Accept (Poster),0,7,0.0,yes,9/27/18,University of California Berkeley;University of Amsterdam;Stanford University;University of California Berkeley;University of California Berkeley,5;169;4;5;5,18;59;3;18;18,1
1511,1511,1511,1511,1511,1511,1511,1511,ICLR,2019,"Learning Particle Dynamics for Manipulating Rigid Bodies, Deformable Objects, and Fluids",Yunzhu Li;Jiajun Wu;Russ Tedrake;Joshua B. Tenenbaum;Antonio Torralba,liyunzhu@mit.edu;jiajunwu@mit.edu;russt@mit.edu;jbt@mit.edu;torralba@mit.edu,6;8;7,4;3;3,Accept (Poster),0,10,0.0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2;2,5;5;5;5;5,10
1512,1512,1512,1512,1512,1512,1512,1512,ICLR,2019,On the Relation Between the Sharpest Directions of DNN Loss and the SGD Step Length,Stanisław Jastrzębski;Zachary Kenton;Nicolas Ballas;Asja Fischer;Yoshua Bengio;Amos Storkey,staszek.jastrzebski@gmail.com;zakenton@gmail.com;ballasn@fb.com;asja.fischer@gmail.com;yoshua.umontreal@gmail.com;a.storkey@ed.ac.uk,7;6;6,3;4;3,Accept (Poster),0,4,0.0,yes,9/27/18,New York University;;Facebook;Institute for Cognitive Neuroscience/ Inst. for Neuroinformatics;University of Montreal;University of Edinburgh,26;-1;-1;-1;123;33,27;-1;-1;-1;108;27,8
1513,1513,1513,1513,1513,1513,1513,1513,ICLR,2019,A Variational Inequality Perspective on Generative Adversarial Networks,Gauthier Gidel;Hugo Berard;Gaëtan Vignoud;Pascal Vincent;Simon Lacoste-Julien,gauthier.gidel@umontreal.ca;hugo.berard@gmail.com;gaetan.vignoud@gmail.com;vincentp@iro.umontreal.ca;slacoste@iro.umontreal.ca,8;8;7,3;3;4,Accept (Poster),0,5,1.0,yes,9/27/18,University of Montreal;;INRIA;University of Montreal;University of Montreal,123;-1;-1;123;123,108;-1;-1;108;108,5;4
1514,1514,1514,1514,1514,1514,1514,1514,ICLR,2019,Learning-Based Frequency Estimation Algorithms,Chen-Yu Hsu;Piotr Indyk;Dina Katabi;Ali Vakilian,cyhsu@mit.edu;indyk@mit.edu;dina@csail.mit.edu;vakilian@mit.edu,6;7;6,1;4;4,Accept (Poster),0,4,0.0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2,5;5;5;5,1
1515,1515,1515,1515,1515,1515,1515,1515,ICLR,2019,Learning to Design RNA,Frederic Runge;Danny Stoll;Stefan Falkner;Frank Hutter,runget@cs.uni-freiburg.de;d.stoll@tutanota.com;sfalkner@cs.uni-freiburg.de;fh@cs.uni-freiburg.de,8;6;6,4;4;1,Accept (Poster),0,16,0.0,yes,9/27/18,Universität Freiburg;Universität Freiburg;Universität Freiburg;Universität Freiburg,123;123;123;123,82;82;82;82,6
1516,1516,1516,1516,1516,1516,1516,1516,ICLR,2019,L-Shapley and C-Shapley: Efficient Model Interpretation for Structured Data,Jianbo Chen;Le Song;Martin J. Wainwright;Michael I. Jordan,jianbochen@berkeley.edu;lsong@cc.gatech.edu;wainwrig@berkeley.edu;jordan@cs.berkeley.edu,6;7;7,4;3;2,Accept (Poster),2,5,0.0,yes,9/27/18,University of California Berkeley;Georgia Institute of Technology;University of California Berkeley;University of California Berkeley,5;13;5;5,18;33;18;18,10
1517,1517,1517,1517,1517,1517,1517,1517,ICLR,2019,Stochastic Gradient/Mirror Descent: Minimax Optimality and Implicit Regularization,Navid Azizan;Babak Hassibi,azizan@caltech.edu;hassibi@caltech.edu,5;7;5,3;4;3,Accept (Poster),0,5,0.0,yes,9/27/18,California Institute of Technology;California Institute of Technology,140;140,3;3,
1518,1518,1518,1518,1518,1518,1518,1518,ICLR,2019,Boosting Robustness Certification of Neural Networks,Gagandeep Singh;Timon Gehr;Markus Püschel;Martin Vechev,gsingh@inf.ethz.ch;timon.gehr@inf.ethz.ch;pueschel@inf.ethz.ch;martin.vechev@inf.ethz.ch,4;5;6,3;4;3,Accept (Poster),0,4,0.0,yes,9/27/18,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10;10,10;10;10;10,4;1
1519,1519,1519,1519,1519,1519,1519,1519,ICLR,2019,Two-Timescale Networks for Nonlinear Value Function Approximation,Wesley Chung;Somjit Nath;Ajin Joseph;Martha White,wchung@ualberta.ca;somjit@ualberta.ca;ajoseph@ualberta.ca;whitem@ualberta.ca,6;7;6;6,4;4;4;4,Accept (Poster),0,7,0.0,yes,9/27/18,University of Alberta;University of Alberta;University of Alberta;University of Alberta,99;99;99;99,119;119;119;119,1
1520,1520,1520,1520,1520,1520,1520,1520,ICLR,2019,Deep Lagrangian Networks: Using Physics as Model Prior for Deep Learning,Michael Lutter;Christian Ritter;Jan Peters,michael@robot-learning.de;ritter@stud.tu-darmstadt.de;peters@ias.tu-darmstadt.de,7;4;7,4;5;3,Accept (Poster),0,9,0.0,yes,9/27/18,TU Darmstadt;TU Darmstadt;TU Darmstadt,65;65;65,244;244;244,8
1521,1521,1521,1521,1521,1521,1521,1521,ICLR,2019,Improving MMD-GAN Training with Repulsive Loss Function,Wei Wang;Yuan Sun;Saman Halgamuge,weiw8@student.unimelb.edu.au;yuan.sun@rmit.edu.au;saman@unimelb.edu.au,7;7;6,5;5;2,Accept (Poster),2,5,0.0,yes,9/27/18,The University of Melbourne;Massachusetts Institute of Technology;The University of Melbourne,123;2;123,32;5;32,5;4
1522,1522,1522,1522,1522,1522,1522,1522,ICLR,2019,Deep Anomaly Detection with Outlier Exposure,Dan Hendrycks;Mantas Mazeika;Thomas Dietterich,hendrycks@berkeley.edu;mantas@ttic.edu;tgd@oregonstate.edu,6;6;8,4;5;4,Accept (Poster),3,9,1.0,yes,9/27/18,University of California Berkeley;Toyota Technological Institute at Chicago;Oregon State University,5;123;76,18;1103;318,3;5
1523,1523,1523,1523,1523,1523,1523,1523,ICLR,2019,Temporal Difference Variational Auto-Encoder,Karol Gregor;George Papamakarios;Frederic Besse;Lars Buesing;Theophane Weber,karol.gregor@gmail.com;g.papamakarios@ed.ac.uk;fbesse@google.com;lbuesing@google.com;theophane@google.com,8;9;7,4;4;5,Accept (Oral),0,3,1.0,yes,9/27/18,Google;University of Edinburgh;Google;Google;Google,-1;33;-1;-1;-1,-1;27;-1;-1;-1,5
1524,1524,1524,1524,1524,1524,1524,1524,ICLR,2019,Meta-Learning with Latent Embedding Optimization,Andrei A. Rusu;Dushyant Rao;Jakub Sygnowski;Oriol Vinyals;Razvan Pascanu;Simon Osindero;Raia Hadsell,andreirusu@google.com;dushyantr@google.com;sygi@google.com;vinyals@google.com;razp@google.com;osindero@google.com;raia@google.com,6;5;8,5;3;5,Accept (Poster),2,11,0.0,yes,9/27/18,Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,5;6
1525,1525,1525,1525,1525,1525,1525,1525,ICLR,2019,Active Learning with Partial Feedback,Peiyun Hu;Zachary C. Lipton;Anima Anandkumar;Deva Ramanan,peiyunh@cs.cmu.edu;zlipton@cmu.edu;anima@caltech.edu;deva@cs.cmu.edu,7;6;7,4;3;4,Accept (Poster),0,4,0.0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;California Institute of Technology;Carnegie Mellon University,1;1;140;1,24;24;3;24,
1526,1526,1526,1526,1526,1526,1526,1526,ICLR,2019,A Universal Music Translation Network,Noam Mor;Lior Wolf;Adam Polyak;Yaniv Taigman,noam.mor@gmail.com;wolf@fb.com;adampolyak@fb.com;yaniv@fb.com,7;6;8,4;4;4,Accept (Poster),2,6,1.0,yes,9/27/18,Tel Aviv University;Facebook;Facebook;Facebook,37;-1;-1;-1,217;-1;-1;-1,
1527,1527,1527,1527,1527,1527,1527,1527,ICLR,2019,Query-Efficient Hard-label Black-box Attack: An Optimization-based Approach,Minhao Cheng;Thong Le;Pin-Yu Chen;Huan Zhang;JinFeng Yi;Cho-Jui Hsieh,mhcheng@ucla.edu;thmle@ucdavis.edu;pin-yu.chen@ibm.com;huan@huan-zhang.com;yijinfeng@jd.com;chohsieh@cs.ucla.edu,7;6;7,3;5;4,Accept (Poster),0,3,0.0,yes,9/27/18,"University of California, Los Angeles;University of California, Davis;International Business Machines;University of California, Los Angeles;JD AI Research;University of California, Los Angeles",20;81;-1;20;-1;20,15;54;-1;15;-1;15,4;1;9
1528,1528,1528,1528,1528,1528,1528,1528,ICLR,2019,Multi-step Retriever-Reader Interaction for Scalable Open-domain Question Answering,Rajarshi Das;Shehzaad Dhuliawala;Manzil Zaheer;Andrew McCallum,rajarshi@cs.umass.edu;sdhuliawala@cs.umass.edu;manzil@cmu.edu;mccallum@cs.umass.edu,7;6;6,4;5;4,Accept (Poster),0,11,6.0,yes,9/27/18,"University of Massachusetts, Amherst;University of Massachusetts, Amherst;Carnegie Mellon University;University of Massachusetts, Amherst",30;30;1;30,191;191;24;191,
1529,1529,1529,1529,1529,1529,1529,1529,ICLR,2019,Analysing Mathematical Reasoning Abilities of Neural Models,David Saxton;Edward Grefenstette;Felix Hill;Pushmeet Kohli,saxton@google.com;etg@google.com;felixhill@google.com;pushmeet@google.com,7;6;6,3;3;4,Accept (Poster),0,0,4.0,yes,9/27/18,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,8
1530,1530,1530,1530,1530,1530,1530,1530,ICLR,2019,Bias-Reduced Uncertainty Estimation for Deep Neural Classifiers,Yonatan Geifman;Guy Uziel;Ran El-Yaniv,yonatan.g@cs.technion.ac.il;uzielguy@gmail.com;rani@cs.technion.ac.il,7;7;7,3;4;2,Accept (Poster),0,9,2.0,yes,9/27/18,Technion;Technion;Technion,25;25;25,327;327;327,11
1531,1531,1531,1531,1531,1531,1531,1531,ICLR,2019,The Deep Weight Prior,Andrei Atanov;Arsenii Ashukha;Kirill Struminsky;Dmitriy Vetrov;Max Welling,andrewatanov@yandex.ru;ars.ashuha@gmail.com;k.struminsky@gmail.com;vetrovd@yandex.ru;m.welling@uva.nl,6;8;7,4;4;3,Accept (Poster),0,8,0.0,yes,9/27/18,;Samsung;Higher School of Economics;Higher School of Economics;University of Amsterdam,-1;-1;478;478;169,-1;-1;377;377;59,5;11
1532,1532,1532,1532,1532,1532,1532,1532,ICLR,2019,Complement Objective Training,Hao-Yun Chen;Pei-Hsin Wang;Chun-Hao Liu;Shih-Chieh Chang;Jia-Yu Pan;Yu-Ting Chen;Wei Wei;Da-Cheng Juan,haoyunchen@gapp.nthu.edu.tw;peihsin@gapp.nthu.edu.tw;newgod1992@gapp.nthu.edu.tw;scchang@cs.nthu.edu.tw;jypan@google.com;yutingchen@google.com;wewei@google.com;dacheng@google.com,8;5;7,4;4;4,Accept (Poster),0,19,0.0,yes,9/27/18,National Tsing Hua University;National Tsing Hua University;National Tsing Hua University;National Tsing Hua University;Google;Google;Google;Google,199;199;199;199;-1;-1;-1;-1,323;323;323;323;-1;-1;-1;-1,3;4;2
1533,1533,1533,1533,1533,1533,1533,1533,ICLR,2019,Eidetic 3D LSTM: A Model for Video Prediction and Beyond,Yunbo Wang;Lu Jiang;Ming-Hsuan Yang;Li-Jia Li;Mingsheng Long;Li Fei-Fei,yunbowang1989@gmail.com;lujiang@google.com;mhyang@ucmerced.edu;lijiali@google.com;mingsheng@tsinghua.edu.cn;feifeili@cs.stanford.edu,7;7;7,5;4;4,Accept (Poster),0,6,0.0,yes,9/27/18,Tsinghua University;Google;University of California at Merced;Google;Tsinghua University;Stanford University,8;-1;478;-1;8;4,30;-1;1103;-1;30;3,
1534,1534,1534,1534,1534,1534,1534,1534,ICLR,2019,Diagnosing and Enhancing VAE Models,Bin Dai;David Wipf,v-bindai@microsoft.com;davidwipf@gmail.com,6;7;9,3;4;4,Accept (Poster),0,18,4.0,yes,9/27/18,Microsoft;Microsoft,-1;-1,-1;-1,5
1535,1535,1535,1535,1535,1535,1535,1535,ICLR,2019,Posterior Attention Models for Sequence to Sequence Learning,Shiv Shankar;Sunita Sarawagi,sshankar@umass.edu;sunita@iitb.ac.in,9;8;7,4;5;4,Accept (Poster),0,11,0.0,yes,9/27/18,"University of Massachusetts, Amherst;Indian Institute of Technology Bombay",30;115,191;367,
1536,1536,1536,1536,1536,1536,1536,1536,ICLR,2019,Building Dynamic Knowledge Graphs from Text using Machine Reading Comprehension,Rajarshi Das;Tsendsuren Munkhdalai;Xingdi Yuan;Adam Trischler;Andrew McCallum,rajarshi@cs.umass.edu;tsmunkhd@microsoft.com;eric.yuan@microsoft.com;adam.trischler@microsoft.com;mccallum@cs.umass.edu,7;6;7,4;4;4,Accept (Poster),0,6,0.0,yes,9/27/18,"University of Massachusetts, Amherst;Microsoft;Microsoft;Microsoft;University of Massachusetts, Amherst",30;-1;-1;-1;30,191;-1;-1;-1;191,10
1537,1537,1537,1537,1537,1537,1537,1537,ICLR,2019,An analytic theory of generalization dynamics and transfer learning in deep linear networks,Andrew K. Lampinen;Surya Ganguli,lampinen@stanford.edu;sganguli@stanford.edu,8;7;6,4;4;3,Accept (Poster),5,3,0.0,yes,9/27/18,Stanford University;Stanford University,4;4,3;3,8;1;6
1538,1538,1538,1538,1538,1538,1538,1538,ICLR,2019,Deep Learning 3D Shapes Using Alt-az Anisotropic 2-Sphere Convolution,Min Liu;Fupin Yao;Chiho Choi;Ayan Sinha;Karthik Ramani,liu66@purdue.edu;yao153@purdue.edu;chihochoi@purdue.edu;asinha@magicleap.com;ramani@purdue.edu,6;8;7,3;5;5,Accept (Poster),0,10,0.0,yes,9/27/18,Purdue University;Purdue University;Purdue University;Magic Leap;Purdue University,26;26;26;-1;26,60;60;60;-1;60,
1539,1539,1539,1539,1539,1539,1539,1539,ICLR,2019,Exemplar Guided Unsupervised Image-to-Image Translation with Semantic Consistency,Liqian Ma;Xu Jia;Stamatios Georgoulis;Tinne Tuytelaars;Luc Van Gool,liqian.ma@esat.kuleuven.be;xu.jia@esat.kuleuven.be;georgous@ee.ethz.ch;tinne.tuytelaars@esat.kuleuven.be;luc.vangool@esat.kuleuven.be,6;5;8,4;5;4,Accept (Poster),0,5,0.0,yes,9/27/18,KU Leuven;KU Leuven;Swiss Federal Institute of Technology;KU Leuven;KU Leuven,115;115;10;115;115,47;47;10;47;47,
1540,1540,1540,1540,1540,1540,1540,1540,ICLR,2019,Fixup Initialization: Residual Learning Without Normalization,Hongyi Zhang;Yann N. Dauphin;Tengyu Ma,hongyiz@mit.edu;yann@dauphin.io;tengyuma@stanford.edu,5;7;7,4;3;3,Accept (Poster),0,19,3.0,yes,9/27/18,Massachusetts Institute of Technology;Facebook;Stanford University,2;-1;4,5;-1;3,3;8
1541,1541,1541,1541,1541,1541,1541,1541,ICLR,2019,Capsule Graph Neural Network,Zhang Xinyi;Lihui Chen,xinyi001@e.ntu.edu.sg;elhchen@ntu.edu.sg,6;6;6,4;4;4,Accept (Poster),0,3,3.0,yes,9/27/18,National Taiwan University;National Taiwan University,85;85,197;197,10
1542,1542,1542,1542,1542,1542,1542,1542,ICLR,2019,Self-Tuning Networks: Bilevel Optimization of Hyperparameters using Structured Best-Response Functions,Matthew Mackay;Paul Vicol;Jonathan Lorraine;David Duvenaud;Roger Grosse,mmackay@cs.toronto.edu;pvicol@cs.toronto.edu;lorraine@cs.toronto.edu;duvenaud@cs.toronto.edu;rgrosse@cs.toronto.edu,6;7;6,3;4;3,Accept (Poster),0,6,0.0,yes,9/27/18,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",18;18;18;18;18,22;22;22;22;22,
1543,1543,1543,1543,1543,1543,1543,1543,ICLR,2019,Learning to Make Analogies by Contrasting Abstract Relational Structure,Felix Hill;Adam Santoro;David Barrett;Ari Morcos;Timothy Lillicrap,felixhill@google.com;adamsantoro@google.com;barrettdavid@google.com;arimorcos@google.com;countzero@google.com,6;7;7,3;3;5,Accept (Poster),1,0,11.0,yes,9/27/18,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,
1544,1544,1544,1544,1544,1544,1544,1544,ICLR,2019,DPSNet: End-to-end Deep Plane Sweep Stereo,Sunghoon Im;Hae-Gon Jeon;Stephen Lin;In So Kweon,dlarl8927@kaist.ac.kr;haegonj@andrew.cmu.edu;stevelin@microsoft.com;iskweon77@kaist.ac.kr,6;7;6,5;4;4,Accept (Poster),2,7,0.0,yes,9/27/18,Korea Advanced Institute of Science and Technology;Carnegie Mellon University;Microsoft;Korea Advanced Institute of Science and Technology,20;1;-1;20,95;24;-1;95,
1545,1545,1545,1545,1545,1545,1545,1545,ICLR,2019,M^3RL: Mind-aware Multi-agent Management Reinforcement Learning,Tianmin Shu;Yuandong Tian,tianmin.shu@ucla.edu;yuandong@fb.com,6;7;6,3;4;1,Accept (Poster),0,6,1.0,yes,9/27/18,"University of California, Los Angeles;Facebook",20;-1,15;-1,8
1546,1546,1546,1546,1546,1546,1546,1546,ICLR,2019,Hierarchical Reinforcement Learning via Advantage-Weighted Information Maximization,Takayuki Osa;Voot Tangkaratt;Masashi Sugiyama,osa@mfg.t.u-tokyo.ac.jp;voot.tangkaratt@riken.jp;sugi@k.u-tokyo.ac.jp,6;7;5,4;4;4,Accept (Poster),0,8,0.0,yes,9/27/18,The University of Tokyo;RIKEN;The University of Tokyo,54;-1;54,45;-1;45,
1547,1547,1547,1547,1547,1547,1547,1547,ICLR,2019,ACCELERATING NONCONVEX LEARNING VIA REPLICA EXCHANGE LANGEVIN DIFFUSION,Yi Chen;Jinglin Chen;Jing Dong;Jian Peng;Zhaoran Wang,yichen2016@u.northwestern.edu;jinglinc@illinois.edu;jd2736@columbia.edu;jianpeng@illinois.edu;zhaoranwang@gmail.com,4;7;6,4;4;4,Accept (Poster),0,4,0.0,yes,9/27/18,"Northwestern University;University of Illinois, Urbana Champaign;Columbia University;University of Illinois, Urbana Champaign;Northwestern University",44;3;15;3;44,20;37;14;37;20,9
1548,1548,1548,1548,1548,1548,1548,1548,ICLR,2019,Improving Sequence-to-Sequence Learning via Optimal Transport,Liqun Chen;Yizhe Zhang;Ruiyi Zhang;Chenyang Tao;Zhe Gan;Haichao Zhang;Bai Li;Dinghan Shen;Changyou Chen;Lawrence Carin,liqun.chen@duke.edu;yizhe.zhang@microsoft.com;rz68@duke.edu;chenyang.tao@duke.edu;zhe.gan@microsoft.com;hczhang1@gmail.com;bai.li@duke.edu;dinghan.shen@duke.edu;cchangyou@gmail.com;lcarin@duke.edu,6;7;5,3;4;4,Accept (Poster),0,5,2.0,yes,9/27/18,"Duke University;Microsoft;Duke University;Duke University;Microsoft;Horizon Robotics;Duke University;Duke University;State University of New York, Buffalo;Duke University",44;-1;44;44;-1;-1;44;44;81;44,17;-1;17;17;-1;-1;17;17;270;17,3
1549,1549,1549,1549,1549,1549,1549,1549,ICLR,2019,Overcoming Catastrophic Forgetting for Continual Learning via Model Adaptation,Wenpeng Hu;Zhou Lin;Bing Liu;Chongyang Tao;Zhengwei Tao;Jinwen Ma;Dongyan Zhao;Rui Yan,wenpeng.hu@pku.edu.cn;scene@pku.edu.cn;liub@uic.edu;chongyangtao@pku.edu.cn;tttzw@pku.edu.cn;jwma@math.pku.edu.cn;zhaody@pku.edu.cn;ruiyan@pku.edu.cn,5;6;7,4;2;4,Accept (Poster),0,13,0.0,yes,9/27/18,"Peking University;Peking University;University of Illinois, Chicago;Peking University;Peking University;Peking University;Peking University;Peking University",24;24;57;24;24;24;24;24,27;27;255;27;27;27;27;27,
1550,1550,1550,1550,1550,1550,1550,1550,ICLR,2019,Adaptive Posterior Learning: few-shot learning with a surprise-based memory module,Tiago Ramalho;Marta Garnelo,tiago.mpramalho@gmail.com;garnelo@google.com,7;6;7,4;4;3,Accept (Poster),0,6,1.0,yes,9/27/18,Google;Google,-1;-1,-1;-1,6
1551,1551,1551,1551,1551,1551,1551,1551,ICLR,2019,A Mean Field Theory of Batch Normalization,Greg Yang;Jeffrey Pennington;Vinay Rao;Jascha Sohl-Dickstein;Samuel S. Schoenholz,gregyang@microsoft.com;jpennin@google.com;vinaysrao@google.com;jaschasd@google.com;schsam@google.com,7;6;7,3;1;3,Accept (Poster),2,6,0.0,yes,9/27/18,Microsoft;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,
1552,1552,1552,1552,1552,1552,1552,1552,ICLR,2019,Rethinking the Value of Network Pruning,Zhuang Liu;Mingjie Sun;Tinghui Zhou;Gao Huang;Trevor Darrell,zhuangl@berkeley.edu;sunmj15@gmail.com;tinghuiz@eecs.berkeley.edu;gaohuang.thu@gmail.com;trevor@eecs.berkeley.edu,6;7;6,5;5;4,Accept (Poster),20,40,1.0,yes,9/27/18,University of California Berkeley;Tsinghua University;University of California Berkeley;Cornell University;University of California Berkeley,5;8;5;7;5,18;30;18;19;18,
1553,1553,1553,1553,1553,1553,1553,1553,ICLR,2019,Benchmarking Neural Network Robustness to Common Corruptions and Perturbations,Dan Hendrycks;Thomas Dietterich,hendrycks@berkeley.edu;tgd@oregonstate.edu,7;9;9,3;5;4,Accept (Poster),0,16,2.0,yes,9/27/18,University of California Berkeley;Oregon State University,5;76,18;318,4
1554,1554,1554,1554,1554,1554,1554,1554,ICLR,2019,Universal Transformers,Mostafa Dehghani;Stephan Gouws;Oriol Vinyals;Jakob Uszkoreit;Lukasz Kaiser,dehghani@uva.nl;sgouws@google.com;vinyals@google.com;usz@google.com;lukaszkaiser@google.com,8;6;6,4;4;2,Accept (Poster),0,13,0.0,yes,9/27/18,University of Amsterdam;Google;Google;Google;Google,169;-1;-1;-1;-1,59;-1;-1;-1;-1,3;8
1555,1555,1555,1555,1555,1555,1555,1555,ICLR,2019,Characterizing Audio Adversarial Examples Using Temporal Dependency,Zhuolin Yang;Bo Li;Pin-Yu Chen;Dawn Song,lucas110550@sjtu.edu.cn;lxbosky@gmail.com;pin-yu.chen@ibm.com;dawnsong@gmail.com,7;6;6,3;3;3,Accept (Poster),4,4,0.0,yes,9/27/18,Shanghai Jiao Tong University;University of California Berkeley;International Business Machines;University of California Berkeley,52;5;-1;5,188;18;-1;18,4
1556,1556,1556,1556,1556,1556,1556,1556,ICLR,2019,Contingency-Aware Exploration in Reinforcement Learning,Jongwook Choi;Yijie Guo;Marcin Moczulski;Junhyuk Oh;Neal Wu;Mohammad Norouzi;Honglak Lee,jwook@umich.edu;guoyijie@umich.edu;marcin.lukasz.moczulski@gmail.com;junhyuk@umich.edu;neal@nealwu.com;mnorouzi@google.com;honglak@eecs.umich.edu,7;6;7,4;3;2,Accept (Poster),0,10,0.0,yes,9/27/18,University of Michigan;University of Michigan;University of Oxford;University of Michigan;Google;Google;University of Michigan,8;8;50;8;-1;-1;8,21;21;1;21;-1;-1;21,
1557,1557,1557,1557,1557,1557,1557,1557,ICLR,2019,Visual Semantic Navigation using Scene Priors,Wei Yang;Xiaolong Wang;Ali Farhadi;Abhinav Gupta;Roozbeh Mottaghi,wyang@ee.cuhk.edu.hk;xiaolonw@cs.cmu.edu;ali@cs.washington.edu;abhinavg@cs.cmu.edu;roozbehm@allenai.org,7;7;7,3;1;4,Accept (Poster),0,7,0.0,yes,9/27/18,The Chinese University of Hong Kong;Carnegie Mellon University;University of Washington;Carnegie Mellon University;Allen Institute for Artificial Intelligence,57;1;6;1;-1,40;24;25;24;-1,10;8
1558,1558,1558,1558,1558,1558,1558,1558,ICLR,2019,On Computation and Generalization of Generative Adversarial Networks under Spectrum Control,Haoming Jiang;Zhehui Chen;Minshuo Chen;Feng Liu;Dingding Wang;Tuo Zhao,jianghm@gatech.edu;zhchen@gatech.edu;mchen393@gatech.edu;fliu2016@fau.edu;wangd@fau.edu;tourzhao@gatech.edu,8;6;7,4;2;4,Accept (Poster),0,4,0.0,yes,9/27/18,Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Florida Atlantic University;Florida Atlantic University;Georgia Institute of Technology,13;13;13;314;314;13,33;33;33;712;712;33,5;4;8
1559,1559,1559,1559,1559,1559,1559,1559,ICLR,2019,The Comparative Power of ReLU Networks and Polynomial Kernels in the Presence of Sparse Latent Structure,Frederic Koehler;Andrej Risteski,fkoehler@mit.edu;risteski@mit.edu,7;7;7,3;3;3,Accept (Poster),0,1,0.0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2,5;5,4
1560,1560,1560,1560,1560,1560,1560,1560,ICLR,2019,"The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks",Jonathan Frankle;Michael Carbin,jfrankle@mit.edu;mcarbin@csail.mit.edu,5;9;9,4;4;4,Accept (Oral),0,14,2.0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2,5;5,
1561,1561,1561,1561,1561,1561,1561,1561,ICLR,2019,Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer,David Berthelot*;Colin Raffel*;Aurko Roy;Ian Goodfellow,dberth@google.com;craffel@gmail.com;aurkor@google.com;goodfellow@google.com,7;8;9,4;3;4,Accept (Poster),0,18,0.0,yes,9/27/18,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,
1562,1562,1562,1562,1562,1562,1562,1562,ICLR,2019,Learning Implicitly Recurrent CNNs Through Parameter Sharing,Pedro Savarese;Michael Maire,savarese@ttic.edu;mmaire@uchicago.edu,8;7;6,4;3;4,Accept (Poster),0,7,0.0,yes,9/27/18,Toyota Technological Institute at Chicago;University of Chicago,123;48,1103;9,
1563,1563,1563,1563,1563,1563,1563,1563,ICLR,2019,G-SGD: Optimizing ReLU Neural Networks in its Positively Scale-Invariant Space,Qi Meng;Shuxin Zheng;Huishuai Zhang;Wei Chen;Qiwei Ye;Zhi-Ming Ma;Nenghai Yu;Tie-Yan Liu,meq@microsoft.com;zhengsx@mail.ustc.edu.cn;huzhang@microsoft.com;wche@microsoft.com;qiwye@microsoft.com;mazm@amt.ac.cn;ynh@ustc.edu.cn;tyliu@microsoft.com,7;7;7,4;2;3,Accept (Poster),0,7,1.0,yes,9/27/18,Microsoft;University of Science and Technology of China;Microsoft;Microsoft;Microsoft;Chinese Academy of Sciences;University of Science and Technology of China;Microsoft,-1;478;-1;-1;-1;62;478;-1,-1;132;-1;-1;-1;1103;132;-1,1
1564,1564,1564,1564,1564,1564,1564,1564,ICLR,2019,SGD Converges to Global Minimum in Deep Learning via Star-convex Path,Yi Zhou;Junjie Yang;Huishuai Zhang;Yingbin Liang;Vahid Tarokh,yi.zhou610@duke.edu;baymax@mail.ustc.edu.cn;huishuai.zhang@microsoft.com;liang.889@osu.edu;vahid.tarokh@duke.edu,8;6;5,4;4;5,Accept (Poster),0,5,0.0,yes,9/27/18,Duke University;University of Science and Technology of China;Microsoft;Ohio State University;Duke University,44;478;-1;76;44,17;132;-1;318;17,9
1565,1565,1565,1565,1565,1565,1565,1565,ICLR,2019,Discovery of Natural Language Concepts in Individual Units of CNNs,Seil Na;Yo Joong Choe;Dong-Hyun Lee;Gunhee Kim,seil.na@vision.snu.ac.kr;yj.c@kakaocorp.com;benjamin.lee@kakaobrain.com;gunhee@snu.ac.kr,6;6;6,4;4;3,Accept (Poster),0,6,0.0,yes,9/27/18,Seoul National University;Kakao;Kakao Brain;Seoul National University,41;-1;-1;41,74;-1;-1;74,3
1566,1566,1566,1566,1566,1566,1566,1566,ICLR,2019,LanczosNet: Multi-Scale Deep Graph Convolutional Networks,Renjie Liao;Zhizhen Zhao;Raquel Urtasun;Richard Zemel,rjliao@cs.toronto.edu;zhizhenz@illinois.edu;urtasun@uber.com;zemel@cs.toronto.edu,7;7;8,3;5;4,Accept (Poster),0,4,0.0,yes,9/27/18,"Department of Computer Science, University of Toronto;University of Illinois, Urbana Champaign;Uber;Department of Computer Science, University of Toronto",18;3;-1;18,22;37;-1;22,10
1567,1567,1567,1567,1567,1567,1567,1567,ICLR,2019,Learning to Learn with Conditional Class Dependencies,Xiang Jiang;Mohammad Havaei;Farshid Varno;Gabriel Chartrand;Nicolas Chapados;Stan Matwin,xiang.jiang@dal.ca;mohammad@imagia.com;f.varno@dal.ca;gabriel@imagia.com;nic@imagia.com;stan@cs.dal.ca,6;8;4,3;3;5,Accept (Poster),0,4,0.0,yes,9/27/18,Dalhousie University;Imagia;Dalhousie University;Imagia;Imagia;Dalhousie University,314;-1;314;-1;-1;314,289;-1;289;-1;-1;289,6
1568,1568,1568,1568,1568,1568,1568,1568,ICLR,2019,Optimal Transport Maps For Distribution Preserving Operations on Latent Spaces of Generative Models,Eirikur Agustsson;Alexander Sage;Radu Timofte;Luc Van Gool,aeirikur@vision.ee.ethz.ch;alexander.sage@gmail.com;radu.timofte@vision.ee.ethz.ch;vangool@vision.ee.ethz.ch,5;7;5,3;5;3,Accept (Poster),0,4,0.0,yes,9/27/18,Swiss Federal Institute of Technology;;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;-1;10;10,10;-1;10;10,5;4
1569,1569,1569,1569,1569,1569,1569,1569,ICLR,2019,MARGINALIZED AVERAGE ATTENTIONAL NETWORK FOR WEAKLY-SUPERVISED LEARNING,Yuan Yuan;Yueming Lyu;Xi Shen;Ivor W. Tsang;Dit-Yan Yeung,yuanyuan910115@gmail.com;lv_yueming@outlook.com;shenxiluc@gmail.com;ivor.tsang@uts.edu.au;dyyeung@cse.ust.hk,5;6;3,3;4;3,Accept (Poster),0,4,0.0,yes,9/27/18,The Hong Kong University of Science and Technology;University of Technology Sydney;ENPC;University of Technology Sydney;The Hong Kong University of Science and Technology,39;106;478;106;39,44;216;280;216;44,1
1570,1570,1570,1570,1570,1570,1570,1570,ICLR,2019,Predict then Propagate: Graph Neural Networks meet Personalized PageRank,Johannes Klicpera;Aleksandar Bojchevski;Stephan Günnemann,klicpera@in.tum.de;a.bojchevski@in.tum.de;guennemann@in.tum.de,5;5;7,4;4;4,Accept (Poster),0,10,5.0,yes,9/27/18,Technical University Munich;Technical University Munich;Technical University Munich,54;54;54,41;41;41,10
1571,1571,1571,1571,1571,1571,1571,1571,ICLR,2019,Variance Reduction for Reinforcement Learning in Input-Driven Environments,Hongzi Mao;Shaileshh Bojja Venkatakrishnan;Malte Schwarzkopf;Mohammad Alizadeh,hongzi@csail.mit.edu;bjjvnkt@csail.mit.edu;malte@csail.mit.edu;alizadeh@csail.mit.edu,6;7;9,4;4;4,Accept (Poster),0,6,0.0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2,5;5;5;5,6
1572,1572,1572,1572,1572,1572,1572,1572,ICLR,2019,Algorithmic Framework for Model-based Deep Reinforcement Learning with Theoretical Guarantees,Yuping Luo;Huazhe Xu;Yuanzhi Li;Yuandong Tian;Trevor Darrell;Tengyu Ma,yupingl@cs.princeton.edu;huazhe_xu@eecs.berkeley.edu;yuanzhili92@gmail.com;yuandong@fb.com;trevor@eecs.berkeley.edu;tengyuma@stanford.edu,7;6;6,4;4;2,Accept (Poster),0,8,0.0,yes,9/27/18,Princeton University;University of California Berkeley;;Facebook;University of California Berkeley;Stanford University,30;5;-1;-1;5;4,7;18;-1;-1;18;3,1
1573,1573,1573,1573,1573,1573,1573,1573,ICLR,2019,Efficient Training on Very Large Corpora via Gramian Estimation,Walid Krichene;Nicolas Mayoraz;Steffen Rendle;Li Zhang;Xinyang Yi;Lichan Hong;Ed Chi;John Anderson,walidk@google.com;nmayoraz@google.com;srendle@google.com;liqzhang@google.com;xinyang@google.com;lichan@google.com;edchi@google.com;janders@google.com,8;7;7,4;4;2,Accept (Poster),8,4,0.0,yes,9/27/18,Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1,8
1574,1574,1574,1574,1574,1574,1574,1574,ICLR,2019,Representing Formal Languages: A Comparison Between Finite Automata and Recurrent Neural Networks ,Joshua J. Michalenko;Ameesh Shah;Abhinav Verma;Richard G. Baraniuk;Swarat Chaudhuri;Ankit B. Patel,jjm7@rice.edu;ameesh@rice.edu;averma@rice.edu;richb@rice.edu;swarat@rice.edu;abp4@rice.edu,7;5;5,3;3;3,Accept (Poster),1,10,0.0,yes,9/27/18,Rice University;Rice University;Rice University;Rice University;Rice University;Rice University,85;85;85;85;85;85,86;86;86;86;86;86,
1575,1575,1575,1575,1575,1575,1575,1575,ICLR,2019,From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following,Justin Fu;Anoop Korattikara;Sergey Levine;Sergio Guadarrama,justinjfu@eecs.berkeley.edu;kbanoop@google.com;svlevine@eecs.berkeley.edu;sguada@google.com,9;5;5,5;4;4,Accept (Poster),0,4,0.0,yes,9/27/18,University of California Berkeley;Google;University of California Berkeley;Google,5;-1;5;-1,18;-1;18;-1,3
1576,1576,1576,1576,1576,1576,1576,1576,ICLR,2019,Learning from Positive and Unlabeled Data with a Selection Bias,Masahiro Kato;Takeshi Teshima;Junya Honda,mkato@ms.k.u-tokyo.ac.jp;teshima@ms.k.u-tokyo.ac.jp;honda@edu.k.u-tokyo.ac.jp,7;6;5,2;4;4,Accept (Poster),0,3,0.0,yes,9/27/18,The University of Tokyo;The University of Tokyo;The University of Tokyo,54;54;54,45;45;45,
1577,1577,1577,1577,1577,1577,1577,1577,ICLR,2019,Dynamic Channel Pruning: Feature Boosting and Suppression,Xitong Gao;Yiren Zhao;Łukasz Dudziak;Robert Mullins;Cheng-zhong Xu,xt.gao@siat.ac.cn;yaz21@cam.ac.uk;lukaszd.mail@gmail.com;robert.mullins@cl.cam.ac.uk;czxu@um.edu.mo,6;7;6;7,4;5;3;4,Accept (Poster),0,10,2.0,yes,9/27/18,Chinese Academy of Sciences;University of Cambridge;Samsung;University of Cambridge;,62;71;-1;71;-1,1103;2;-1;2;-1,
1578,1578,1578,1578,1578,1578,1578,1578,ICLR,2019,Maximal Divergence Sequential Autoencoder for Binary Software Vulnerability Detection,Tue Le;Tuan Nguyen;Trung Le;Dinh Phung;Paul Montague;Olivier De Vel;Lizhen Qu,tue.le.ict@jvn.edu.vn;nguyenvutuan1995@gmail.com;trunglm@monash.edu;dinh.phung@monash.edu;paul.montague@dst.defence.gov.au;olivier.devel@dst.defence.gov.au;lizhen.qu@data61.csiro.au,6;7;6;6,4;2;3;2,Accept (Poster),0,8,0.0,yes,9/27/18,";;Monash University;Monash University;;;, CSIRO",-1;-1;123;123;-1;-1;-1,-1;-1;80;80;-1;-1;-1,5
1579,1579,1579,1579,1579,1579,1579,1579,ICLR,2019,Large Scale GAN Training for High Fidelity Natural Image Synthesis,Andrew Brock;Jeff Donahue;Karen Simonyan,ajb5@hw.ac.uk;jeffdonahue@google.com;simonyan@google.com,9;7;8,4;3;4,Accept (Oral),10,11,1.0,yes,9/27/18,Heriot-Watt University;Google;Google,261;-1;-1,363;-1;-1,5;4
1580,1580,1580,1580,1580,1580,1580,1580,ICLR,2019,Learning Exploration Policies for Navigation,Tao Chen;Saurabh Gupta;Abhinav Gupta,taoc1@andrew.cmu.edu;sgupta@eecs.berkeley.edu;abhinavg@cs.cmu.edu,7;3;7,4;5;5,Accept (Poster),0,18,0.0,yes,9/27/18,Carnegie Mellon University;University of California Berkeley;Carnegie Mellon University,1;5;1,24;18;24,
1581,1581,1581,1581,1581,1581,1581,1581,ICLR,2019,Variational Autoencoders with Jointly Optimized Latent Dependency Structure,Jiawei He;Yu Gong;Joseph Marino;Greg Mori;Andreas Lehrmann,jha203@sfu.ca;yu_gong@sfu.ca;jmarino@caltech.edu;mori@cs.sfu.ca;andreas.lehrmann@gmail.com,7;8;6,4;5;3,Accept (Poster),0,9,0.0,yes,9/27/18,Simon Fraser University;Simon Fraser University;California Institute of Technology;Simon Fraser University;Facebook,62;62;140;62;-1,253;253;3;253;-1,5;11;10
1582,1582,1582,1582,1582,1582,1582,1582,ICLR,2019,Efficient Multi-Objective Neural Architecture Search via Lamarckian Evolution,Thomas Elsken;Jan Hendrik Metzen;Frank Hutter,thomas.elsken@de.bosch.com;janhendrik.metzen@de.bosch.com;fh@cs.uni-freiburg.de,6;6;6,3;3;4,Accept (Poster),0,4,0.0,yes,9/27/18,Bosch;Bosch;Universität Freiburg,-1;-1;123,-1;-1;82,
1583,1583,1583,1583,1583,1583,1583,1583,ICLR,2019,Amortized Bayesian Meta-Learning,Sachin Ravi;Alex Beatson,sachinr@princeton.edu;abeatson@cs.princeton.edu,6;6;5,3;4;3,Accept (Poster),0,5,0.0,yes,9/27/18,Princeton University;Princeton University,30;30,7;7,4;6
1584,1584,1584,1584,1584,1584,1584,1584,ICLR,2019,How to train your MAML,Antreas Antoniou;Harrison Edwards;Amos Storkey,a.antoniou@sms.ed.ac.uk;h.l.edwards@sms.ac.uk;a.storkey@sms.ed.ac.uk,5;6;7,3;5;4,Accept (Poster),4,5,0.0,yes,9/27/18,University of Edinburgh;;University of Edinburgh,33;-1;33,27;-1;27,6;8
1585,1585,1585,1585,1585,1585,1585,1585,ICLR,2019,Dynamically Unfolding Recurrent Restorer: A Moving Endpoint Control Method for Image Restoration,Xiaoshuai Zhang;Yiping Lu;Jiaying Liu;Bin Dong,jet@pku.edu.cn;luyiping9712@pku.edu.cn;liujiaying@pku.edu.cn;dongbin@math.pku.edu.cn,6;6;7,3;5;4,Accept (Poster),0,9,1.0,yes,9/27/18,Peking University;Peking University;Peking University;Peking University,24;24;24;24,27;27;27;27,
1586,1586,1586,1586,1586,1586,1586,1586,ICLR,2019,Learning To Simulate,Nataniel Ruiz;Samuel Schulter;Manmohan Chandraker,nruiz9@bu.edu;samuel@nec-labs.com;manu@nec-labs.com,6;7;6,4;4;5,Accept (Poster),0,5,0.0,yes,9/27/18,Boston University;NEC-Labs;NEC-Labs,65;-1;-1,70;-1;-1,2
1587,1587,1587,1587,1587,1587,1587,1587,ICLR,2019,What do you learn from context? Probing for sentence structure in contextualized word representations,Ian Tenney;Patrick Xia;Berlin Chen;Alex Wang;Adam Poliak;R Thomas McCoy;Najoung Kim;Benjamin Van Durme;Samuel R. Bowman;Dipanjan Das;Ellie Pavlick,iftenney@google.com;paxia@cs.jhu.edu;bchen6@swarthmore.edu;alexwang@nyu.edu;azpoliak@cs.jhu.edu;tom.mccoy@jhu.edu;n.kim@jhu.edu;vandurme@cs.jhu.edu;bowman@nyu.edu;dipanjand@google.com;ellie_pavlick@brown.edu,7;7;7,4;4;4,Accept (Poster),0,5,0.0,yes,9/27/18,Google;Johns Hopkins University;Swarthmore College;New York University;Johns Hopkins University;Johns Hopkins University;Johns Hopkins University;Johns Hopkins University;New York University;Google;Brown University,-1;72;478;26;72;72;72;72;26;-1;65,-1;13;1103;27;13;13;13;13;27;-1;50,3
1588,1588,1588,1588,1588,1588,1588,1588,ICLR,2019,"Towards Robust, Locally Linear Deep Networks",Guang-He Lee;David Alvarez-Melis;Tommi S. Jaakkola,guanghe@csail.mit.edu;davidam@csail.mit.edu;tommi@csail.mit.edu,8;8;7,3;4;4,Accept (Poster),0,9,1.0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2,5;5;5,
1589,1589,1589,1589,1589,1589,1589,1589,ICLR,2019,Gradient descent aligns the layers of deep linear networks,Ziwei Ji;Matus Telgarsky,ziweiji2@illinois.edu;mjt@illinois.edu,9;6;7,4;5;4,Accept (Poster),0,6,0.0,yes,9/27/18,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",3;3,37;37,
1590,1590,1590,1590,1590,1590,1590,1590,ICLR,2019,Bayesian Deep Convolutional Networks with Many Channels are Gaussian Processes,Roman Novak;Lechao Xiao;Yasaman Bahri;Jaehoon Lee;Greg Yang;Jiri Hron;Daniel A. Abolafia;Jeffrey Pennington;Jascha Sohl-dickstein,romann@google.com;xlc@google.com;yasamanb@google.com;jaehlee@google.com;gregyang@microsoft.com;jh2084@cam.ac.uk;danabo@google.com;jpennin@google.com;jaschasd@google.com,7;7;7;6,3;2;5;4,Accept (Poster),0,18,1.0,yes,9/27/18,Google;Google;Google;Google;Microsoft;University of Cambridge;Google;Google;Google,-1;-1;-1;-1;-1;71;-1;-1;-1,-1;-1;-1;-1;-1;2;-1;-1;-1,11
1591,1591,1591,1591,1591,1591,1591,1591,ICLR,2019,MisGAN: Learning from Incomplete Data with Generative Adversarial Networks,Steven Cheng-Xian Li;Bo Jiang;Benjamin Marlin,cxl@cs.umass.edu;bjiang@sjtu.edu.cn;marlin@cs.umass.edu,7;6;7,4;5;4,Accept (Poster),0,3,0.0,yes,9/27/18,"University of Massachusetts, Amherst;Shanghai Jiao Tong University;University of Massachusetts, Amherst",30;52;30,191;188;191,5;4
1592,1592,1592,1592,1592,1592,1592,1592,ICLR,2019,Information Theoretic lower bounds on negative log likelihood,Luis A. Lastras-Montaño,lastrasl@us.ibm.com,6;7;6,3;3;4,Accept (Poster),0,8,0.0,yes,9/27/18,International Business Machines,-1,-1,1
1593,1593,1593,1593,1593,1593,1593,1593,ICLR,2019,A Data-Driven and Distributed Approach to Sparse Signal Representation and Recovery,Ali Mousavi;Gautam Dasarathy;Richard G. Baraniuk,ali.mousavi1988@gmail.com;gautamd@asu.edu;richb@rice.edu,7;8;6,3;4;3,Accept (Poster),0,6,0.0,yes,9/27/18,Google;Arizona State University;Rice University,-1;95;85,-1;126;86,
1594,1594,1594,1594,1594,1594,1594,1594,ICLR,2019,Efficient Augmentation via Data Subsampling,Michael Kuchnik;Virginia Smith,mkuchnik@andrew.cmu.edu;smithv@cmu.edu,6;7;6,4;4;3,Accept (Poster),0,3,0.0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University,1;1,24;24,
1595,1595,1595,1595,1595,1595,1595,1595,ICLR,2019,Explaining Image Classifiers by Counterfactual Generation,Chun-Hao Chang;Elliot Creager;Anna Goldenberg;David Duvenaud,kingsley@cs.toronto.edu;creager@cs.toronto.edu;anna.goldenberg@utoronto.ca;duvenaud@cs.toronto.edu,5;7;5,4;3;5,Accept (Poster),0,6,2.0,yes,9/27/18,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Toronto University;Department of Computer Science, University of Toronto",18;18;18;18,22;22;22;22,5
1596,1596,1596,1596,1596,1596,1596,1596,ICLR,2019,A Max-Affine Spline Perspective of Recurrent Neural Networks,Zichao Wang;Randall Balestriero;Richard Baraniuk,richb@rice.edu;zw16@rice.edu;randallbalestriero@gmail.com,6;6;6,3;3;3,Accept (Poster),0,3,0.0,yes,9/27/18,Rice University;Rice University;Rice University,85;85;85,86;86;86,1;8
1597,1597,1597,1597,1597,1597,1597,1597,ICLR,2019,Learning Actionable Representations with Goal Conditioned Policies,Dibya Ghosh;Abhishek Gupta;Sergey Levine,dibya.ghosh@berkeley.edu;abhigupta@berkeley.edu;svlevine@eecs.berkeley.edu,6;6;5,4;4;4,Accept (Poster),0,13,0.0,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,18;18;18,5
1598,1598,1598,1598,1598,1598,1598,1598,ICLR,2019,Supervised Community Detection with Line Graph Neural Networks,Zhengdao Chen;Lisha Li;Joan Bruna,zc1216@nyu.edu;lapis.lazuli.8@gmail.com;bruna@cims.nyu.edu,6;9;8,4;4;4,Accept (Poster),0,5,0.0,yes,9/27/18,New York University;;New York University,26;-1;26,27;-1;27,5;10
1599,1599,1599,1599,1599,1599,1599,1599,ICLR,2019,Unsupervised Learning via Meta-Learning,Kyle Hsu;Sergey Levine;Chelsea Finn,kyle.hsu@mail.utoronto.ca;svlevine@eecs.berkeley.edu;cbfinn@eecs.berkeley.edu,7;6;6;8,4;3;3;4,Accept (Poster),0,8,0.0,yes,9/27/18,Toronto University;University of California Berkeley;University of California Berkeley,18;5;5,22;18;18,6
1600,1600,1600,1600,1600,1600,1600,1600,ICLR,2019,Music Transformer: Generating Music with Long-Term Structure,Cheng-Zhi Anna Huang;Ashish Vaswani;Jakob Uszkoreit;Ian Simon;Curtis Hawthorne;Noam Shazeer;Andrew M. Dai;Matthew D. Hoffman;Monica Dinculescu;Douglas Eck,chengzhiannahuang@gmail.com;avaswani@google.com;uszkoreit@google.com;iansimon@google.com;fjord@google.com;noam@google.com;adai@google.com;mhoffman@google.com;noms@google.com;deck@google.com,7;6;4;5,3;4;4;3,Accept (Poster),0,12,0.0,yes,9/27/18,Harvard University;Google;Google;Google;Google;Google;Google;Google;Google;Google,39;-1;-1;-1;-1;-1;-1;-1;-1;-1,6;-1;-1;-1;-1;-1;-1;-1;-1;-1,
1601,1601,1601,1601,1601,1601,1601,1601,ICLR,2019,"Deep, Skinny Neural Networks are not Universal Approximators",Jesse Johnson,jejo.math@gmail.com,6;8;7,4;4;4,Accept (Poster),2,2,1.0,yes,9/27/18,,,,
1602,1602,1602,1602,1602,1602,1602,1602,ICLR,2019,Multilingual Neural Machine Translation with Knowledge Distillation,Xu Tan;Yi Ren;Di He;Tao Qin;Zhou Zhao;Tie-Yan Liu,xuta@microsoft.com;rayeren613@gmail.com;dihe@microsoft.com;taoqin@microsoft.com;zhaozhou@zju.edu.cn;tyliu@microsoft.com,7;7;7,4;3;4,Accept (Poster),0,16,6.0,yes,9/27/18,Microsoft;Zhejiang University;Microsoft;Microsoft;Zhejiang University;Microsoft,-1;57;-1;-1;57;-1,-1;177;-1;-1;177;-1,3
1603,1603,1603,1603,1603,1603,1603,1603,ICLR,2019,textTOvec: DEEP CONTEXTUALIZED NEURAL AUTOREGRESSIVE TOPIC MODELS OF LANGUAGE WITH DISTRIBUTED COMPOSITIONAL PRIOR,Pankaj Gupta;Yatin Chaudhary;Florian Buettner;Hinrich Schuetze,pankaj_gupta96@yahoo.com;yatinchaudhary91@gmail.com;fbuettner.phys@gmail.com;hinrich@hotmail.com,8;7;6,4;4;4,Accept (Poster),6,18,1.0,yes,9/27/18,Siemens Corporate Research;Technical University Munich;Siemens Corporate Research;Institut für Informatik,-1;54;-1;-1,-1;41;-1;-1,3;5;8
1604,1604,1604,1604,1604,1604,1604,1604,ICLR,2019,Learning Embeddings into Entropic Wasserstein Spaces,Charlie Frogner;Farzaneh Mirzazadeh;Justin Solomon,frogner@mit.edu;farzaneh@ibm.com;jsolomon@mit.edu,7;7;3,3;4;4,Accept (Poster),0,12,3.0,yes,9/27/18,Massachusetts Institute of Technology;International Business Machines;Massachusetts Institute of Technology,2;-1;2,5;-1;5,
1605,1605,1605,1605,1605,1605,1605,1605,ICLR,2019,Deep Layers as Stochastic Solvers,Adel Bibi;Bernard Ghanem;Vladlen Koltun;Rene Ranftl,adel.bibi@kaust.edu.sa;bernard.ghanem@kaust.edu.sa;vkoltun@gmail.com;ranftlr@gmail.com,7;7;8,5;4;1,Accept (Poster),2,8,0.0,yes,9/27/18,KAUST;KAUST;Intel;Intel,123;123;-1;-1,1103;1103;-1;-1,9
1606,1606,1606,1606,1606,1606,1606,1606,ICLR,2019,Learning to Describe Scenes with Programs,Yunchao Liu;Zheng Wu;Daniel Ritchie;William T. Freeman;Joshua B. Tenenbaum;Jiajun Wu,georgeycliu@gmail.com;14wuzheng@sjtu.edu.cn;daniel_ritchie@brown.edu;billf@mit.edu;jbt@mit.edu;jiajunwu@mit.edu,6;4;6,3;3;4,Accept (Poster),0,13,1.0,yes,9/27/18,Tsinghua University;Shanghai Jiao Tong University;Brown University;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,8;52;65;2;2;2,30;188;50;5;5;5,
1607,1607,1607,1607,1607,1607,1607,1607,ICLR,2019,BA-Net: Dense Bundle Adjustment Networks,Chengzhou Tang;Ping Tan,cta73@sfu.ca;pingtan@sfu.ca,8;7;9,4;4;4,Accept (Oral),0,6,0.0,yes,9/27/18,Simon Fraser University;Simon Fraser University,62;62,253;253,1
1608,1608,1608,1608,1608,1608,1608,1608,ICLR,2019,Stochastic Prediction of Multi-Agent Interactions from Partial Observations,Chen Sun;Per Karlsson;Jiajun Wu;Joshua B Tenenbaum;Kevin Murphy,chensun@google.com;perk@google.com;jiajunwu@mit.edu;jbt@mit.edu;kpmurphy@google.com,6;6;6,4;4;4,Accept (Poster),0,6,0.0,yes,9/27/18,Google;Google;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Google,-1;-1;2;2;-1,-1;-1;5;5;-1,10
1609,1609,1609,1609,1609,1609,1609,1609,ICLR,2019,"Plan Online, Learn Offline: Efficient Learning and Exploration via Model-Based Control",Kendall Lowrey;Aravind Rajeswaran;Sham Kakade;Emanuel Todorov;Igor Mordatch,kendall.lowrey@gmail.com;rajeswaran.aravind@gmail.com;sham@cs.washington.edu;etodorov@gmail.com;mordatch@openai.com,5;6;4,5;4;3,Accept (Poster),0,7,0.0,yes,9/27/18,"University of Washington, Seattle;University of Washington;University of Washington;Roboti LLC;OpenAI",6;6;6;-1;-1,25;25;25;-1;-1,
1610,1610,1610,1610,1610,1610,1610,1610,ICLR,2019,Composing Complex Skills by Learning Transition Policies,Youngwoon Lee*;Shao-Hua Sun*;Sriram Somasundaram;Edward S. Hu;Joseph J. Lim,lee504@usc.edu;shaohuas@usc.edu;sriramso@usc.edu;hues@usc.edu;limjj@usc.edu,7;9;7,4;4;4,Accept (Poster),0,5,0.0,yes,9/27/18,University of Southern California;University of Southern California;University of Southern California;University of Southern California;University of Southern California,30;30;30;30;30,66;66;66;66;66,
1611,1611,1611,1611,1611,1611,1611,1611,ICLR,2019,The role of over-parametrization in generalization of neural networks,Behnam Neyshabur;Zhiyuan Li;Srinadh Bhojanapalli;Yann LeCun;Nathan Srebro,bneyshabur@gmail.com;zhiyuanli@cs.princeton.edu;srinadh@ttic.edu;yann@cs.nyu.edu;nati@ttic.edu,7;7;7,3;5;3,Accept (Poster),0,5,0.0,yes,9/27/18,New York University;Princeton University;Toyota Technological Institute at Chicago;New York University;Toyota Technological Institute at Chicago,26;30;123;26;123,27;7;1103;27;1103,1;8
1612,1612,1612,1612,1612,1612,1612,1612,ICLR,2019,Learning Procedural Abstractions and Evaluating Discrete Latent Temporal Structure,Karan Goel;Emma Brunskill,kgoel93@gmail.com;ebrun@cs.stanford.edu,5;6;7,2;3;3,Accept (Poster),0,6,1.0,yes,9/27/18,Stanford University;Stanford University,4;4,3;3,11
1613,1613,1613,1613,1613,1613,1613,1613,ICLR,2019,Learning Mixed-Curvature Representations in Product Spaces,Albert Gu;Frederic Sala;Beliz Gunel;Christopher Ré,albertgu@stanford.edu;fredsala@stanford.edu;bgunel@stanford.edu;chrismre@cs.stanford.edu,7;7;7,5;2;3,Accept (Poster),0,8,0.0,yes,9/27/18,Stanford University;Stanford University;Stanford University;Stanford University,4;4;4;4,3;3;3;3,3;10
1614,1614,1614,1614,1614,1614,1614,1614,ICLR,2019,Synthetic Datasets for Neural Program Synthesis,Richard Shin;Neel Kant;Kavi Gupta;Chris Bender;Brandon Trabucco;Rishabh Singh;Dawn Song,ricshin@berkeley.edu;kantneel@berkeley.edu;kavi@berkeley.edu;chrisbender@berkeley.edu;btrabucco@berkeley.edu;rising@google.com;dawnsong@cs.berkeley.edu,6;6;7,4;2;3,Accept (Poster),0,6,0.0,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;Google;University of California Berkeley,5;5;5;5;5;-1;5,18;18;18;18;18;-1;18,8
1615,1615,1615,1615,1615,1615,1615,1615,ICLR,2019,Energy-Constrained Compression for Deep Neural Networks via Weighted Sparse Projection and Layer Input Masking,Haichuan Yang;Yuhao Zhu;Ji Liu,h.yang@rochester.edu;yzhu@rochester.edu;ji.liu.uwisc@gmail.com,7;7;7,4;4;3,Accept (Poster),0,9,0.0,yes,9/27/18,University of Rochester;University of Rochester;University of Rochester,106;106;106,153;153;153,1
1616,1616,1616,1616,1616,1616,1616,1616,ICLR,2019,The Unusual Effectiveness of Averaging in GAN Training,Yasin Yaz{\i}c{\i};Chuan-Sheng Foo;Stefan Winkler;Kim-Hui Yap;Georgios Piliouras;Vijay Chandrasekhar,yasin001@e.ntu.edu.sg;foocs@i2r.a-star.edu.sg;stefan.winkler@adsc-create.edu.sg;ekhyap@ntu.edu.sg;georgios@sutd.edu.sg;vijay@i2r.a-star.edu.sg,6;5;6,4;2;4,Accept (Poster),3,6,0.0,yes,9/27/18,National Taiwan University;A*STAR;ADSC;National Taiwan University;Singapore University of Technology and Design;A*STAR,85;-1;-1;85;478;-1,197;-1;-1;197;1103;-1,5
1617,1617,1617,1617,1617,1617,1617,1617,ICLR,2019,Selfless Sequential Learning,Rahaf Aljundi;Marcus Rohrbach;Tinne Tuytelaars,rahaf.aljundi@gmail.com;mrf@fb.com;tinne.tuytelaars@esat.kuleuven.be,6;6;7,5;4;4,Accept (Poster),0,7,0.0,yes,9/27/18,KU Leuven;Facebook;KU Leuven,115;-1;115,47;-1;47,
1618,1618,1618,1618,1618,1618,1618,1618,ICLR,2019,Learning Protein Structure with a Differentiable Simulator,John Ingraham;Adam Riesselman;Chris Sander;Debora Marks,john.ingraham@gmail.com;adam.riesselman@gmail.com;cccsander@gmail.com;deboramarks@gmail.com,6;7;6;7,3;5;5;3,Accept (Oral),4,7,0.0,yes,9/27/18,Massachusetts Institute of Technology;Harvard University;Harvard University;Harvard University,2;39;39;39,5;6;6;6,
1619,1619,1619,1619,1619,1619,1619,1619,ICLR,2019,DOM-Q-NET:  Grounded RL on Structured Language,Sheng Jia;Jamie Ryan Kiros;Jimmy Ba,sheng.jia@mail.utoronto.ca;kirosjamie@gmail.com;jba@cs.utoronto.ca,7;6;7,3;3;1,Accept (Poster),0,10,0.0,yes,9/27/18,Toronto University;Google;Toronto University,18;-1;18,22;-1;22,10
1620,1620,1620,1620,1620,1620,1620,1620,ICLR,2019,Predicting the Generalization Gap in Deep Networks with Margin Distributions,Yiding Jiang;Dilip Krishnan;Hossein Mobahi;Samy Bengio,ydjiang@google.com;dilipkay@google.com;hmobahi@google.com;bengio@google.com,6;5;9,4;4;4,Accept (Poster),2,5,0.0,yes,9/27/18,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,8
1621,1621,1621,1621,1621,1621,1621,1621,ICLR,2019,Neural TTS Stylization with Adversarial and Collaborative Games,Shuang Ma;Daniel Mcduff;Yale Song,shuangma@buffalo.edu;damcduff@microsoft.com;yalesong@csail.mit.edu,6;6;7;6,5;5;5;3,Accept (Poster),0,14,1.0,yes,9/27/18,"State University of New York, Buffalo;Microsoft;Massachusetts Institute of Technology",81;-1;2,270;-1;5,5;4
1622,1622,1622,1622,1622,1622,1622,1622,ICLR,2019,Learning a Meta-Solver for Syntax-Guided Program Synthesis,Xujie Si;Yuan Yang;Hanjun Dai;Mayur Naik;Le Song,xsi@cis.upenn.edu;yyang754@gatech.edu;hanjundai@gatech.edu;mhnaik@cis.upenn.edu;lsong@cc.gatech.edu,7;7;7,5;4;2,Accept (Poster),0,14,0.0,yes,9/27/18,University of Pennsylvania;Georgia Institute of Technology;Georgia Institute of Technology;University of Pennsylvania;Georgia Institute of Technology,19;13;13;19;13,10;33;33;10;33,6;10
1623,1623,1623,1623,1623,1623,1623,1623,ICLR,2019,DyRep: Learning Representations over Dynamic Graphs,Rakshit Trivedi;Mehrdad Farajtabar;Prasenjeet Biswal;Hongyuan Zha,rstrivedi@gatech.edu;farajtabar@google.com;bprasenjeet1108@gmail.com;zha@cc.gatech.edu,6;7;8,4;5;4,Accept (Poster),4,9,4.0,yes,9/27/18,Georgia Institute of Technology;Google;Georgia Institute of Technology;Georgia Institute of Technology,13;-1;13;13,33;-1;33;33,10
1624,1624,1624,1624,1624,1624,1624,1624,ICLR,2019,Modeling the Long Term Future in Model-Based Reinforcement Learning,Nan Rosemary Ke;Amanpreet Singh;Ahmed Touati;Anirudh Goyal;Yoshua Bengio;Devi Parikh;Dhruv Batra,rosemary.nan.ke@gmail.com;asg@fb.com;ahmed.touati@umontreal.ca;anirudhgoyal9119@gmail.com;yoshua.umontreal@gmail.com;parikh@gatech.edu;dbatra@gatech.edu,7;6;6,4;4;4,Accept (Poster),0,23,0.0,yes,9/27/18,Polytechnique Montreal;Facebook;University of Montreal;University of Montreal;University of Montreal;Georgia Institute of Technology;Georgia Institute of Technology,386;-1;123;123;123;13;13,108;-1;108;108;108;33;33,
1625,1625,1625,1625,1625,1625,1625,1625,ICLR,2019,ProbGAN: Towards Probabilistic GAN with Theoretical Guarantees,Hao He;Hao Wang;Guang-He Lee;Yonglong Tian,haohe@mit.edu;hwang87@mit.edu;guanghe@mit.edu;yonglong@mit.edu,5;6;9,4;3;4,Accept (Poster),0,9,0.0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2,5;5;5;5,5;4;11
1626,1626,1626,1626,1626,1626,1626,1626,ICLR,2019,Defensive Quantization: When Efficiency Meets Robustness,Ji Lin;Chuang Gan;Song Han,jilin@mit.edu;ganchuang1990@gmail.com;songhan@mit.edu,7;6;7,4;3;2,Accept (Poster),13,4,0.0,yes,9/27/18,Massachusetts Institute of Technology;International Business Machines;Massachusetts Institute of Technology,2;-1;2,5;-1;5,4
1627,1627,1627,1627,1627,1627,1627,1627,ICLR,2019,Identifying and Controlling Important Neurons in Neural Machine Translation,Anthony Bau;Yonatan Belinkov;Hassan Sajjad;Nadir Durrani;Fahim Dalvi;James Glass,abau@mit.edu;belinkov@mit.edu;hsajjad@hbku.edu.qa;ndurrani@qf.org.qa;faimaduddin@qf.org.qa;glass@mit.edu,7;10;6,3;3;4,Accept (Poster),0,5,1.0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Peking University;QCRI;QCRI;Massachusetts Institute of Technology,2;2;24;199;199;2,5;5;27;1103;1103;5,3
1628,1628,1628,1628,1628,1628,1628,1628,ICLR,2019,Knowledge Flow: Improve Upon Your Teachers,Iou-Jen Liu;Jian Peng;Alexander Schwing,iliu3@illinois.edu;jianpeng@illinois.edu;aschwing@illinois.edu,6;8;7,3;5;4,Accept (Poster),0,7,1.0,yes,9/27/18,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",3;3;3,37;37;37,
1629,1629,1629,1629,1629,1629,1629,1629,ICLR,2019,FUNCTIONAL VARIATIONAL BAYESIAN NEURAL NETWORKS,Shengyang Sun;Guodong Zhang;Jiaxin Shi;Roger Grosse,ssy@cs.toronto.edu;gdzhang.cs@gmail.com;shijx15@mails.tsinghua.edu.cn;rgrosse@cs.toronto.edu,7;6;6,4;4;3,Accept (Poster),0,7,0.0,yes,9/27/18,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Tsinghua University;Department of Computer Science, University of Toronto",18;18;8;18,22;22;30;22,11;1
1630,1630,1630,1630,1630,1630,1630,1630,ICLR,2019,K for the Price of 1: Parameter-efficient Multi-task and Transfer Learning,Pramod Kaushik Mudrakarta;Mark Sandler;Andrey Zhmoginov;Andrew Howard,pramodkm@uchicago.edu;mark.sandler@gmail.com;azhmogin@google.com;howarda@google.com,6;7;8,3;5;4,Accept (Poster),0,5,0.0,yes,9/27/18,University of Chicago;Google;Google;Google,48;-1;-1;-1,9;-1;-1;-1,6
1631,1631,1631,1631,1631,1631,1631,1631,ICLR,2019,Neural Program Repair by Jointly Learning to Localize and Repair,Marko Vasic;Aditya Kanade;Petros Maniatis;David Bieber;Rishabh Singh,vasic@utexas.edu;akanade@google.com;maniatis@google.com;dbieber@google.com;rising@google.com,7;6;5,4;5;5,Accept (Poster),0,11,0.0,yes,9/27/18,"University of Texas, Austin;Google;Google;Google;Google",22;-1;-1;-1;-1,49;-1;-1;-1;-1,
1632,1632,1632,1632,1632,1632,1632,1632,ICLR,2019,On the Minimal Supervision for Training Any Binary Classifier from Only Unlabeled Data,Nan Lu;Gang Niu;Aditya Krishna Menon;Masashi Sugiyama,lu@ms.k.u-tokyo.ac.jp;gang.niu@riken.jp;adityakmenon@google.com;sugi@k.u-tokyo.ac.jp,7;8;8;7,4;3;3;4,Accept (Poster),0,15,0.0,yes,9/27/18,The University of Tokyo;RIKEN;Google;The University of Tokyo,54;-1;-1;54,45;-1;-1;45,1
1633,1633,1633,1633,1633,1633,1633,1633,ICLR,2019,Regularized Learning for  Domain Adaptation under Label Shifts,Kamyar Azizzadenesheli;Anqi Liu;Fanny Yang;Animashree Anandkumar,kazizzad@uci.edu;anqiliu@caltech.edu;fan.yang@stat.math.ethz.ch;anima@caltech.edu,7;6;6,3;4;4,Accept (Poster),0,6,0.0,yes,9/27/18,"University of California, Irvine;California Institute of Technology;Swiss Federal Institute of Technology;California Institute of Technology",35;140;10;140,99;3;10;3,1;8
1634,1634,1634,1634,1634,1634,1634,1634,ICLR,2019,Toward Understanding the Impact of Staleness in Distributed Machine Learning,Wei Dai;Yi Zhou;Nanqing Dong;Hao Zhang;Eric Xing,daviddai@apple.com;zhou.1172@osu.edu;nanqing.dong@petuum.com;hao.zhang@petuum.com;eric.xing@petuum.com,7;4;9,5;5;4,Accept (Poster),0,5,0.0,yes,9/27/18,Apple;Ohio State University;Petuum Inc.;Petuum Inc.;Petuum Inc.,-1;76;-1;-1;-1,-1;318;-1;-1;-1,9
1635,1635,1635,1635,1635,1635,1635,1635,ICLR,2019,Generalizable Adversarial Training via Spectral Normalization,Farzan Farnia;Jesse Zhang;David Tse,farnia@stanford.edu;jessez@stanford.edu;dntse@stanford.edu,6;6;5,4;5;3,Accept (Poster),2,6,0.0,yes,9/27/18,Stanford University;Stanford University;Stanford University,4;4;4,3;3;3,4;1;8
1636,1636,1636,1636,1636,1636,1636,1636,ICLR,2019,Don't let your Discriminator  be fooled,Brady Zhou;Philipp Krähenbühl,brady.zhou@utexas.edu;philkr@cs.utexas.edu,6;7;7,4;3;3,Accept (Poster),0,4,0.0,yes,9/27/18,"University of Texas, Austin;University of Texas, Austin",22;22,49;49,5;4
1637,1637,1637,1637,1637,1637,1637,1637,ICLR,2019,Diversity and Depth in Per-Example Routing Models,Prajit Ramachandran;Quoc V. Le,prajitram@gmail.com;qvl@google.com,7;6;6,5;4;5,Accept (Poster),0,5,0.0,yes,9/27/18,"University of Illinois, Urbana Champaign;Google",3;-1,37;-1,
1638,1638,1638,1638,1638,1638,1638,1638,ICLR,2019,Latent Convolutional Models,ShahRukh Athar;Evgeny Burnaev;Victor Lempitsky,sathar@cs.stonybrook.edu;e.burnaev@skoltech.ru;lempitsky@skoltech.ru,7;6;7,4;3;2,Accept (Poster),0,4,0.0,yes,9/27/18,"State University of New York, Stony Brook;Skolkovo Institute of Science and Technology;Skolkovo Institute of Science and Technology",41;-1;-1,258;-1;-1,5;4
1639,1639,1639,1639,1639,1639,1639,1639,ICLR,2019,Learning Grid Cells as Vector Representation of Self-Position Coupled with Matrix Representation of Self-Motion,Ruiqi Gao;Jianwen Xie;Song-Chun Zhu;Ying Nian Wu,ruiqigao@ucla.edu;jianwen@ucla.edu;sczhu@stat.ucla.edu;ywu@stat.ucla.edu,7;7;8,5;4;4,Accept (Poster),0,10,0.0,yes,9/27/18,"University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles",20;20;20;20,15;15;15;15,
1640,1640,1640,1640,1640,1640,1640,1640,ICLR,2019,Meta-Learning Update Rules for Unsupervised Representation Learning,Luke Metz;Niru Maheswaranathan;Brian Cheung;Jascha Sohl-Dickstein,lmetz@google.com;nirum@google.com;bcheung@berkeley.edu;jaschasd@google.com,8;8;8,3;4;3,Accept (Oral),0,3,1.0,yes,9/27/18,Google;Google;University of California Berkeley;Google,-1;-1;5;-1,-1;-1;18;-1,5;6
1641,1641,1641,1641,1641,1641,1641,1641,ICLR,2019,Discriminator Rejection Sampling,Samaneh Azadi;Catherine Olsson;Trevor Darrell;Ian Goodfellow;Augustus Odena,sazadi@berkeley.edu;catherio@google.com;trevor@eecs.berkeley.edu;goodfellow@google.com;augustusodena@google.com,7;6;6,4;3;4,Accept (Poster),0,9,1.0,yes,9/27/18,University of California Berkeley;Google;University of California Berkeley;Google;Google,5;-1;5;-1;-1,18;-1;18;-1;-1,5
1642,1642,1642,1642,1642,1642,1642,1642,ICLR,2019,TimbreTron: A WaveNet(CycleGAN(CQT(Audio))) Pipeline for Musical Timbre Transfer,Sicong Huang;Qiyang Li;Cem Anil;Xuchan Bao;Sageev Oore;Roger B. Grosse,huang@cs.toronto.edu;colinli@cs.toronto.edu;anilcem@cs.toronto.edu;jennybao@cs.toronto.edu;sageev@dal.ca;rgrosse@cs.toronto.edu,4;7;8,5;4;4,Accept (Poster),3,7,5.0,yes,9/27/18,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Dalhousie University;Department of Computer Science, University of Toronto",18;18;18;18;314;18,22;22;22;22;289;22,
1643,1643,1643,1643,1643,1643,1643,1643,ICLR,2019,Smoothing the Geometry of Probabilistic Box Embeddings,Xiang Li;Luke Vilnis;Dongxu Zhang;Michael Boratko;Andrew McCallum,xiangl@cs.umass.edu;luke@cs.umass.edu;dongxuzhang@cs.umass.edu;mboratko@math.umass.edu;mccallum@cs.umass.edu,7;8;8,3;3;4,Accept (Oral),0,6,0.0,yes,9/27/18,"University of Massachusetts, Amherst;University of Massachusetts, Amherst;University of Massachusetts, Amherst;University of Massachusetts, Amherst;University of Massachusetts, Amherst",30;30;30;30;30,191;191;191;191;191,10
1644,1644,1644,1644,1644,1644,1644,1644,ICLR,2019,Bounce and Learn: Modeling Scene Dynamics with Real-World Bounces,Senthil Purushwalkam;Abhinav Gupta;Danny Kaufman;Bryan Russell,spurushw@andrew.cmu.edu;abhinavg@cs.cmu.edu;dkaufman@adobe.com;brussell@adobe.com,8;6;7,4;3;4,Accept (Poster),0,8,0.0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;Adobe Systems;Adobe Systems,1;1;-1;-1,24;24;-1;-1,
1645,1645,1645,1645,1645,1645,1645,1645,ICLR,2019,Critical Learning Periods in Deep Networks,Alessandro Achille;Matteo Rovere;Stefano Soatto,achille@cs.ucla.edu;matrovere@gmail.com;soatto@cs.ucla.edu,9;8;6,4;4;5,Accept (Poster),0,6,0.0,yes,9/27/18,"University of California, Los Angeles;;University of California, Los Angeles",20;-1;20,15;-1;15,
1646,1646,1646,1646,1646,1646,1646,1646,ICLR,2019,GANSynth: Adversarial Neural Audio Synthesis,Jesse Engel;Kumar Krishna Agrawal;Shuo Chen;Ishaan Gulrajani;Chris Donahue;Adam Roberts,jesseengel@google.com;kumarkagrawal@gmail.com;chenshuo@google.com;igul222@gmail.com;christopherdonahue@gmail.com;adarob@google.com,6;7;8,3;4;3,Accept (Poster),0,4,0.0,yes,9/27/18,"Google;Google;Google;Google;University of California, San Diego;Google",-1;-1;-1;-1;11;-1,-1;-1;-1;-1;31;-1,5;4
1647,1647,1647,1647,1647,1647,1647,1647,ICLR,2019,A Closer Look at Few-shot Classification,Wei-Yu Chen;Yen-Cheng Liu;Zsolt Kira;Yu-Chiang Frank Wang;Jia-Bin Huang,weiyuc@andrew.cmu.edu;ycliu@gatech.edu;zkira@gatech.edu;ycwang@ntu.edu.tw;jbhuang@vt.edu,6;6;6,2;5;4,Accept (Poster),0,7,12.0,yes,9/27/18,Carnegie Mellon University;Georgia Institute of Technology;Georgia Institute of Technology;National Taiwan University;Virginia Tech,1;13;13;85;81,24;33;33;197;300,6;8
1648,1648,1648,1648,1648,1648,1648,1648,ICLR,2019,Robustness May Be at Odds with Accuracy,Dimitris Tsipras;Shibani Santurkar;Logan Engstrom;Alexander Turner;Aleksander Madry,tsipras@mit.edu;shibani@mit.edu;engstrom@mit.edu;turneram@mit.edu;madry@mit.edu,8;7;8,3;4;2,Accept (Poster),0,5,0.0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2;2,5;5;5;5;5,4;8
1649,1649,1649,1649,1649,1649,1649,1649,ICLR,2019,Visceral Machines: Risk-Aversion in  Reinforcement Learning with Intrinsic Physiological Rewards,Daniel McDuff;Ashish Kapoor,damcduff@microsoft.com;akapoor@microsoft.com,6;6;7,4;4;5,Accept (Poster),0,4,0.0,yes,9/27/18,Microsoft;Microsoft,-1;-1,-1;-1,
1650,1650,1650,1650,1650,1650,1650,1650,ICLR,2019,Diffusion Scattering Transforms on Graphs,Fernando Gama;Alejandro Ribeiro;Joan Bruna,fgama@seas.upenn.edu;aribeiro@seas.upenn.edu;bruna@cims.nyu.edu,7;6;9,3;4;5,Accept (Poster),0,7,0.0,yes,9/27/18,University of Pennsylvania;University of Pennsylvania;New York University,19;19;26,10;10;27,2;10
1651,1651,1651,1651,1651,1651,1651,1651,ICLR,2019,Learning Self-Imitating Diverse Policies,Tanmay Gangwani;Qiang Liu;Jian Peng,gangwan2@uiuc.edu;lqiang@cs.utexas.edu;jianpeng@illinois.edu,6;8;8,2;3;4,Accept (Poster),0,11,2.0,yes,9/27/18,"University of Illinois, Urbana-Champaign;University of Texas, Austin;University of Illinois, Urbana Champaign",3;22;3,37;49;37,
1652,1652,1652,1652,1652,1652,1652,1652,ICLR,2019,Adaptive Input Representations for Neural Language Modeling,Alexei Baevski;Michael Auli,alexei.b@gmail.com;michael.auli@gmail.com,7;8;8,4;4;4,Accept (Poster),0,6,0.0,yes,9/27/18,Facebook;Facebook,-1;-1,-1;-1,3
1653,1653,1653,1653,1653,1653,1653,1653,ICLR,2019,Top-Down Neural Model For Formulae,Karel Chvalovský,karel@chvalovsky.cz,6;6;6,2;3;4,Accept (Poster),0,5,0.0,yes,9/27/18,Czech Technical University in Prague,314,740,
1654,1654,1654,1654,1654,1654,1654,1654,ICLR,2019,Deep learning generalizes because the parameter-function map is biased towards simple functions,Guillermo Valle-Perez;Chico Q. Camargo;Ard A. Louis,guillermo.valle@dtc.ox.ac.uk;chico.camargo@gmail.com;ard.louis@physics.ox.ac.uk,7;5;4,4;3;4,Accept (Poster),0,8,0.0,yes,9/27/18,University of Oxford;;University of Oxford,50;-1;50,1;-1;1,1;8
1655,1655,1655,1655,1655,1655,1655,1655,ICLR,2019,GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding,Alex Wang;Amanpreet Singh;Julian Michael;Felix Hill;Omer Levy;Samuel R. Bowman,alexwang@nyu.edu;amanpreet@nyu.edu;julianjm@cs.washington.edu;felixhill@google.com;omerlevy@cs.washington.edu;bowman@nyu.edu,8;7;5,4;1;2,Accept (Poster),2,3,0.0,yes,9/27/18,New York University;New York University;University of Washington;Google;University of Washington;New York University,26;26;6;-1;6;26,27;27;25;-1;25;27,3;8
1656,1656,1656,1656,1656,1656,1656,1656,ICLR,2019,Learning to Navigate the Web,Izzeddin Gur;Ulrich Rueckert;Aleksandra Faust;Dilek Hakkani-Tur,izzeddingur@gmail.com;rueckert@google.com;sandrafaust@google.com;dilek@ieee.org,8;7;7,3;3;3,Accept (Poster),0,4,0.0,yes,9/27/18,UC Santa Barbara;Google;Google;Amazon Alexa AI,37;-1;-1;-1,53;-1;-1;-1,3;6
1657,1657,1657,1657,1657,1657,1657,1657,ICLR,2019,Learnable Embedding Space for Efficient Neural Architecture Compression,Shengcao Cao;Xiaofang Wang;Kris M. Kitani,caoshengcao@pku.edu.cn;xiaofan2@cs.cmu.edu;kkitani@cs.cmu.edu,6;7;5,3;4;3,Accept (Poster),1,15,11.0,yes,9/27/18,Peking University;Carnegie Mellon University;Carnegie Mellon University,24;1;1,27;24;24,11
1658,1658,1658,1658,1658,1658,1658,1658,ICLR,2019,Universal  Stagewise Learning for Non-Convex Problems with  Convergence on  Averaged Solutions,Zaiyi Chen;Zhuoning Yuan;Jinfeng Yi;Bowen Zhou;Enhong Chen;Tianbao Yang,czy6516@hotmail.com;zhuoning-yuan@uiowa.edu;jinfengyi.ustc@gmail.com;bwen@jd.com;cheneh@ustc.edu.cn;tianbao-yang@uiowa.edu,6;8;6,4;4;4,Accept (Poster),0,8,0.0,yes,9/27/18,;University of Iowa;JD AI Research;JD AI Research;University of Science and Technology of China;University of Iowa,-1;153;-1;-1;478;153,-1;223;-1;-1;132;223,9;8
1659,1659,1659,1659,1659,1659,1659,1659,ICLR,2019,Discriminator-Actor-Critic: Addressing Sample Inefficiency and Reward Bias in Adversarial Imitation Learning,Ilya Kostrikov;Kumar Krishna Agrawal;Debidatta Dwibedi;Sergey Levine;Jonathan Tompson,kostrikov@cs.nyu.edu;kumarkagrawal@gmail.com;debidatta@google.com;slevine@google.com;tompson@google.com,6;8;7,4;2;3,Accept (Poster),6,15,3.0,yes,9/27/18,New York University;Google;Google;Google;Google,26;-1;-1;-1;-1,27;-1;-1;-1;-1,4
1660,1660,1660,1660,1660,1660,1660,1660,ICLR,2019,Learning concise representations for regression by evolving networks of trees,William La Cava;Tilak Raj Singh;James Taggart;Srinivas Suri;Jason H. Moore,lacava@upenn.edu;tilakraj@seas.upenn.edu;surisr@seas.upenn.edu;jhmoore@upenn.edu,6;7;8,3;1;4,Accept (Poster),0,5,0.0,yes,9/27/18,University of Pennsylvania;University of Pennsylvania;University of Pennsylvania;University of Pennsylvania,19;19;19;19,10;10;10;10,8
1661,1661,1661,1661,1661,1661,1661,1661,ICLR,2019,Interpolation-Prediction Networks for Irregularly Sampled Time Series,Satya Narayan Shukla;Benjamin Marlin,snshukla@cs.umass.edu;marlin@cs.umass.edu,6;6;6,4;4;4,Accept (Poster),0,10,0.0,yes,9/27/18,"University of Massachusetts, Amherst;University of Massachusetts, Amherst",30;30,191;191,
1662,1662,1662,1662,1662,1662,1662,1662,ICLR,2019,GAN Dissection: Visualizing and Understanding Generative Adversarial Networks,David Bau;Jun-Yan Zhu;Hendrik Strobelt;Bolei Zhou;Joshua B. Tenenbaum;William T. Freeman;Antonio Torralba,davidbau@csail.mit.edu;junyanz@csail.mit.edu;hendrik.strobelt@ibm.com;bzhou@csail.mit.edu;jbt@csail.mit.edu;billf@csail.mit.edu;torralba@csail.mit.edu,7;7;8,3;4;4,Accept (Poster),0,4,0.0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology;International Business Machines;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;-1;2;2;2;2,5;5;-1;5;5;5;5,5;4;2
1663,1663,1663,1663,1663,1663,1663,1663,ICLR,2019,Deterministic PAC-Bayesian generalization bounds for deep networks via generalizing noise-resilience,Vaishnavh Nagarajan;Zico Kolter,vaishnavh@cs.cmu.edu;zkolter@cs.cmu.edu,5;7;8;7,4;3;5;2,Accept (Poster),0,30,1.0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University,1;1,24;24,11;1;8
1664,1664,1664,1664,1664,1664,1664,1664,ICLR,2019,Deep reinforcement learning with relational inductive biases,Vinicius Zambaldi;David Raposo;Adam Santoro;Victor Bapst;Yujia Li;Igor Babuschkin;Karl Tuyls;David Reichert;Timothy Lillicrap;Edward Lockhart;Murray Shanahan;Victoria Langston;Razvan Pascanu;Matthew Botvinick;Oriol Vinyals;Peter Battaglia,vzambaldi@google.com;draposo@google.com;adamsantoro@google.com;vbapst@google.com;yujiali@google.com;ibab@google.com;karltuyls@google.com;reichert@google.com;countzero@google.com;locked@google.com;mshanahan@google.com;vlangston@google.com;razp@google.com;botvinick@google.com;vinyals@google.com;peterbattaglia@google.com,6;7;7,4;3;4,Accept (Poster),0,7,0.0,yes,9/27/18,Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,8
1665,1665,1665,1665,1665,1665,1665,1665,ICLR,2019,Tree-Structured Recurrent Switching Linear Dynamical Systems for Multi-Scale Modeling,Josue Nassar;Scott Linderman;Monica Bugallo;Il Memming Park,josue.nassar@stonybrook.edu;scott.linderman@columbia.edu;monica.bugallo@stonybrook.edu;memming.park@stonybrook.edu,7;7;6,2;2;4,Accept (Poster),0,7,0.0,yes,9/27/18,"State University of New York, Stony Brook;Columbia University;State University of New York, Stony Brook;State University of New York, Stony Brook",41;15;41;41,258;14;258;258,11
1666,1666,1666,1666,1666,1666,1666,1666,ICLR,2019,Biologically-Plausible Learning Algorithms Can Scale to Large Datasets,Will Xiao;Honglin Chen;Qianli Liao;Tomaso Poggio,xiaow@fas.harvard.edu;chenhonglin@g.ucla.edu;lql@mit.edu;tp@csail.mit.edu,9;9;4,5;4;4,Accept (Poster),0,14,0.0,yes,9/27/18,"Harvard University;University of California, Los Angeles;Massachusetts Institute of Technology;Massachusetts Institute of Technology",39;20;2;2,6;15;5;5,
1667,1667,1667,1667,1667,1667,1667,1667,ICLR,2019,Generative predecessor models for sample-efficient imitation learning,Yannick Schroecker;Mel Vecerik;Jon Scholz,yannickschroecker@gatech.edu;vec@google.com;jscholz@google.com,7;5;6,4;5;3,Accept (Poster),0,12,0.0,yes,9/27/18,Georgia Institute of Technology;Google;Google,13;-1;-1,33;-1;-1,5
1668,1668,1668,1668,1668,1668,1668,1668,ICLR,2019,Kernel RNN Learning (KeRNL),Christopher Roth;Ingmar Kanitscheider;Ila Fiete,christopher_roth@utexas.edu;ingmar@openai.com;fiete@mit.edu,7;5;6,4;1;4,Accept (Poster),0,3,0.0,yes,9/27/18,"University of Texas, Austin;OpenAI;Massachusetts Institute of Technology",22;-1;2,49;-1;5,
1669,1669,1669,1669,1669,1669,1669,1669,ICLR,2019,Integer Networks for Data Compression with Latent-Variable Models,Johannes Ballé;Nick Johnston;David Minnen,jballe@google.com;nickj@google.com;dminnen@google.com,7;8;6,3;3;3,Accept (Poster),0,9,1.0,yes,9/27/18,Google;Google;Google,-1;-1;-1,-1;-1;-1,
1670,1670,1670,1670,1670,1670,1670,1670,ICLR,2019,Sample Efficient Imitation Learning for Continuous Control,Fumihiro Sasaki;Tetsuya Yohira;Atsuo Kawaguchi,fumihiro.fs.sasaki@jp.ricoh.com,7;5;5;5,5;4;5;5,Accept (Poster),0,2,0.0,yes,9/27/18,"Ricoh software limited, Beijing",-1,-1,5;4
1671,1671,1671,1671,1671,1671,1671,1671,ICLR,2019,Janossy Pooling: Learning Deep Permutation-Invariant Functions for Variable-Size Inputs,Ryan L. Murphy;Balasubramaniam Srinivasan;Vinayak Rao;Bruno Ribeiro,murph213@purdue.edu;bsriniv@purdue.edu;varao@purdue.edu;ribeiro@cs.purdue.edu,5;7;8,4;4;4,Accept (Poster),0,8,1.0,yes,9/27/18,Purdue University;Purdue University;Purdue University;Purdue University,26;26;26;26,60;60;60;60,
1672,1672,1672,1672,1672,1672,1672,1672,ICLR,2019,Modeling Uncertainty with Hedged Instance Embeddings,Seong Joon Oh;Kevin P. Murphy;Jiyan Pan;Joseph Roth;Florian Schroff;Andrew C. Gallagher,coallaoh@linecorp.com;agallagher@google.com;kpmurphy@google.com;fschroff@google.com;jiyanpan@google.com;josephroth@google.com,7;7;7,5;3;3,Accept (Poster),0,7,0.0,yes,9/27/18,LINE;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,
1673,1673,1673,1673,1673,1673,1673,1673,ICLR,2019,Deep Decoder: Concise Image Representations from Untrained Non-convolutional Networks,Reinhard Heckel;Paul Hand,rh43@rice.edu;p.hand@northeastern.edu,7;8;8,3;4;4,Accept (Poster),0,6,0.0,yes,9/27/18,Rice University;Northeastern University,85;16,86;839,
1674,1674,1674,1674,1674,1674,1674,1674,ICLR,2019,Probabilistic Planning with Sequential Monte Carlo methods,Alexandre Piche;Valentin Thomas;Cyril Ibrahim;Yoshua Bengio;Chris Pal,alexandrelpiche@gmail.com;vltn.thomas@gmail.com;cyril.ibrahim@elementai.com;yoshua.umontreal@gmail.com;christopher.pal@polymtl.ca,5;6;8,4;4;4,Accept (Poster),0,14,5.0,yes,9/27/18,University of Montreal;University of Montreal;Element AI;University of Montreal;Polytechnique Montreal,123;123;-1;123;386,108;108;-1;108;108,11
1675,1675,1675,1675,1675,1675,1675,1675,ICLR,2019,Random mesh projectors for inverse problems,Konik Kothari*;Sidharth Gupta*;Maarten v. de Hoop;Ivan Dokmanic,kkothar3@illinois.edu;gupta67@illinois.edu;mdehoop@rice.edu;dokmanic@illinois.edu,6;7;4,4;4;3,Accept (Poster),0,19,0.0,yes,9/27/18,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;Rice University;University of Illinois, Urbana Champaign",3;3;85;3,37;37;86;37,
1676,1676,1676,1676,1676,1676,1676,1676,ICLR,2019,A Direct Approach to Robust Deep Learning Using Adversarial Networks,Huaxia Wang;Chun-Nam Yu,hwang38@stevens.edu;cnyu@cs.cornell.edu,5;7;6,4;3;3,Accept (Poster),3,6,0.0,yes,9/27/18,Stevens Institute of Technology;Cornell University,153;7,512;19,5;4
1677,1677,1677,1677,1677,1677,1677,1677,ICLR,2019,"Learning to Adapt in Dynamic, Real-World Environments through Meta-Reinforcement Learning",Anusha Nagabandi;Ignasi Clavera;Simin Liu;Ronald S. Fearing;Pieter Abbeel;Sergey Levine;Chelsea Finn,nagaban2@berkeley.edu;iclavera@berkeley.edu;simin.liu@berkeley.edu;ronf@berkeley.edu;pabbeel@berkeley.edu;svlevine@eecs.berkeley.edu;cbfinn@eecs.berkeley.edu,7;7;2,3;5;5,Accept (Poster),2,10,0.0,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5;5;5;5,18;18;18;18;18;18;18,6;2
1678,1678,1678,1678,1678,1678,1678,1678,ICLR,2019,Robust Conditional Generative Adversarial Networks,Grigorios G. Chrysos;Jean Kossaifi;Stefanos Zafeiriou,greggchrysos@gmail.com;jean.kossaifi@gmail.com;s.zafeiriou@imperial.ac.uk,6;6;6,4;4;4,Accept (Poster),0,10,1.0,yes,9/27/18,Imperial College London;Imperial College London;Imperial College London,72;72;72,8;8;8,5;4;1;2
1679,1679,1679,1679,1679,1679,1679,1679,ICLR,2019,signSGD with Majority Vote is Communication Efficient and Fault Tolerant,Jeremy Bernstein;Jiawei Zhao;Kamyar Azizzadenesheli;Anima Anandkumar,bernstein@caltech.edu;jiaweizhao.zjw@qq.com;kazizzad@uci.edu;anima@caltech.edu,6;6;7,5;5;4,Accept (Poster),1,9,1.0,yes,9/27/18,"California Institute of Technology;;University of California, Irvine;California Institute of Technology",140;-1;35;140,3;-1;99;3,4;1
1680,1680,1680,1680,1680,1680,1680,1680,ICLR,2019,The Singular Values of Convolutional Layers,Hanie Sedghi;Vineet Gupta;Philip M. Long,hsedghi@google.com;vineet@google.com;plong@google.com,8;4;7,4;5;3,Accept (Poster),0,24,2.0,yes,9/27/18,Google;Google;Google,-1;-1;-1,-1;-1;-1,
1681,1681,1681,1681,1681,1681,1681,1681,ICLR,2019,Deep Online Learning Via Meta-Learning: Continual Adaptation for Model-Based RL,Anusha Nagabandi;Chelsea Finn;Sergey Levine,nagaban2@berkeley.edu;cbfinn@eecs.berkeley.edu;svlevine@eecs.berkeley.edu,7;7;7,3;3;3,Accept (Poster),0,3,0.0,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,18;18;18,6
1682,1682,1682,1682,1682,1682,1682,1682,ICLR,2019,InfoBot: Transfer and Exploration via the Information Bottleneck,Anirudh Goyal;Riashat Islam;DJ Strouse;Zafarali Ahmed;Hugo Larochelle;Matthew Botvinick;Yoshua Bengio;Sergey Levine,anirudhgoyal9119@gmail.com;riashat.islam@mail.mcgill.ca;danieljstrouse@gmail.com;zafarali.ahmed@mail.mcgill.ca;hugolarochelle@google.com;botvinick@google.com;svlevine@eecs.berkeley.edu;yoshua.bengio@mila.quebec,7;7;3,3;3;3,Accept (Poster),0,28,1.0,yes,9/27/18,University of Montreal;McGill University;Princeton University;McGill University;Google;Google;University of California Berkeley;University of Montreal,123;85;30;85;-1;-1;5;123,108;42;7;42;-1;-1;18;108,
1683,1683,1683,1683,1683,1683,1683,1683,ICLR,2019,Optimal Control Via Neural Networks: A Convex Approach,Yize Chen;Yuanyuan Shi;Baosen Zhang,yizechen@uw.edu;yyshi@uw.edu;zhangbao@uw.edu,6;8;7,3;4;4,Accept (Poster),1,10,1.0,yes,9/27/18,"University of Washington, Seattle;University of Washington, Seattle;University of Washington, Seattle",6;6;6,25;25;25,
1684,1684,1684,1684,1684,1684,1684,1684,ICLR,2019,Preventing Posterior Collapse with delta-VAEs,Ali Razavi;Aaron van den Oord;Ben Poole;Oriol Vinyals,alirazavi@google.com;avdnoord@google.com;pooleb@google.com;vinyals@google.com,6;7;6,3;4;3,Accept (Poster),2,4,1.0,yes,9/27/18,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,5
1685,1685,1685,1685,1685,1685,1685,1685,ICLR,2019,Combinatorial Attacks on Binarized Neural Networks,Elias B Khalil;Amrita Gupta;Bistra Dilkina,lyes@gatech.edu;agupta375@gatech.edu;dilkina@usc.edu,5;6;7,4;4;4,Accept (Poster),0,10,0.0,yes,9/27/18,Georgia Institute of Technology;Georgia Institute of Technology;University of Southern California,13;13;30,33;33;66,4
1686,1686,1686,1686,1686,1686,1686,1686,ICLR,2019,How Powerful are Graph Neural Networks?,Keyulu Xu*;Weihua Hu*;Jure Leskovec;Stefanie Jegelka,keyulu@mit.edu;weihuahu@stanford.edu;jure@cs.stanford.edu;stefje@mit.edu,7;7;8,5;5;5,Accept (Oral),22,30,4.0,yes,9/27/18,Massachusetts Institute of Technology;Stanford University;Stanford University;Massachusetts Institute of Technology,2;4;4;2,5;3;3;5,10
1687,1687,1687,1687,1687,1687,1687,1687,ICLR,2019,On the Convergence of A Class of Adam-Type Algorithms  for Non-Convex Optimization,Xiangyi Chen;Sijia Liu;Ruoyu Sun;Mingyi Hong,chen5719@umn.edu;sijia.liu@ibm.com;ruoyus@illinois.edu;mhong@umn.edu,7;7;6,3;2;3,Accept (Poster),0,8,1.0,yes,9/27/18,"University of Minnesota, Minneapolis;International Business Machines;University of Illinois, Urbana Champaign;University of Minnesota, Minneapolis",57;-1;3;57,56;-1;37;56,9
1688,1688,1688,1688,1688,1688,1688,1688,ICLR,2019,Learning when to Communicate at Scale in Multiagent Cooperative and Competitive Tasks,Amanpreet Singh;Tushar Jain;Sainbayar Sukhbaatar,amanpreet@nyu.edu;tushar@nyu.edu;sainbar@cs.nyu.edu,7;6;6,3;3;3,Accept (Poster),0,12,0.0,yes,9/27/18,New York University;New York University;New York University,26;26;26,27;27;27,9
1689,1689,1689,1689,1689,1689,1689,1689,ICLR,2019,signSGD via Zeroth-Order Oracle,Sijia Liu;Pin-Yu Chen;Xiangyi Chen;Mingyi Hong,sijia.liu@ibm.com;pin-yu.chen@ibm.com;chen5719@umn.edu;mhong@umn.edu,7;8;6,5;3;2,Accept (Poster),0,11,0.0,yes,9/27/18,"International Business Machines;International Business Machines;University of Minnesota, Minneapolis;University of Minnesota, Minneapolis",-1;-1;57;57,-1;-1;56;56,4;9
1690,1690,1690,1690,1690,1690,1690,1690,ICLR,2019,Information asymmetry in KL-regularized RL,Alexandre Galashov;Siddhant M. Jayakumar;Leonard Hasenclever;Dhruva Tirumala;Jonathan Schwarz;Guillaume Desjardins;Wojciech M. Czarnecki;Yee Whye Teh;Razvan Pascanu;Nicolas Heess,agalashov@google.com;sidmj@google.com;leonardh@google.com;dhruvat@google.com;schwarzjn@google.com;gdesjardins@google.com;lejlot@google.com;ywteh@google.com;razp@google.com;heess@google.com,7;5;7,3;5;4,Accept (Poster),0,4,0.0,yes,9/27/18,Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,
1691,1691,1691,1691,1691,1691,1691,1691,ICLR,2019,Cost-Sensitive Robustness against Adversarial Examples,Xiao Zhang;David Evans,xz7bc@virginia.edu;evans@virginia.edu,5;8;5,4;3;3,Accept (Poster),0,12,0.0,yes,9/27/18,University of Virginia;University of Virginia,65;65,113;113,4
1692,1692,1692,1692,1692,1692,1692,1692,ICLR,2019,Learning deep representations by mutual information estimation and maximization,R Devon Hjelm;Alex Fedorov;Samuel Lavoie-Marchildon;Karan Grewal;Phil Bachman;Adam Trischler;Yoshua Bengio,devon.hjelm@microsoft.com;eidos92@gmail.com;samuel.lavoie-marchildon@umontreal.ca;karang@cs.toronto.edu;phil.bachman@gmail.com;adam.trischler@microsoft.com;yoshua.umontreal@gmail.com,7;9;7,5;3;4,Accept (Oral),2,7,0.0,yes,9/27/18,"Microsoft;;University of Montreal;Department of Computer Science, University of Toronto;Microsoft;Microsoft;University of Montreal",-1;-1;123;18;-1;-1;123,-1;-1;108;22;-1;-1;108,4
1693,1693,1693,1693,1693,1693,1693,1693,ICLR,2019,A Generative Model For Electron Paths,John Bradshaw;Matt J. Kusner;Brooks Paige;Marwin H. S. Segler;José Miguel Hernández-Lobato,jab255@cam.ac.uk;mkusner@turing.ac.uk;bpaige@turing.ac.uk;marwin.segler@benevolent.ai;jmh233@cam.ac.uk,8;4;8,4;4;4,Accept (Poster),0,8,0.0,yes,9/27/18,University of Cambridge;Alan Turing Institute;Alan Turing Institute;BenevolentAI;University of Cambridge,71;-1;-1;-1;71,2;-1;-1;-1;2,5
1694,1694,1694,1694,1694,1694,1694,1694,ICLR,2019,Whitening and Coloring Batch Transform for GANs,Aliaksandr Siarohin;Enver Sangineto;Nicu Sebe,aliaksandr.siarohin@unitn.it;enver.sangineto@unitn.it;niculae.sebe@unitn.it,7;7;7,4;2;4,Accept (Poster),0,13,0.0,yes,9/27/18,University of Trento;University of Trento;University of Trento,18;18;18,258;258;258,5;4
1695,1695,1695,1695,1695,1695,1695,1695,ICLR,2019,Learning protein sequence embeddings using information from structure,Tristan Bepler;Bonnie Berger,tbepler@mit.edu;bab@mit.edu,8;7;7,4;3;4,Accept (Poster),0,8,0.0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2,5;5,
1696,1696,1696,1696,1696,1696,1696,1696,ICLR,2019,DARTS: Differentiable Architecture Search,Hanxiao Liu;Karen Simonyan;Yiming Yang,hanxiaol@cs.cmu.edu;simonyan@google.com;yiming@cs.cmu.edu,6;7;8,2;5;3,Accept (Poster),24,10,6.0,yes,9/27/18,Carnegie Mellon University;Google;Carnegie Mellon University,1;-1;1,24;-1;24,3
1697,1697,1697,1697,1697,1697,1697,1697,ICLR,2019,Representation Degeneration Problem in Training Natural Language Generation Models,Jun Gao;Di He;Xu Tan;Tao Qin;Liwei Wang;Tieyan Liu,jungao@cs.toronto.edu;dihe@microsoft.com;xu.tan@microsoft.com;taoqin@microsoft.com;wanglw@cis.pku.edu.cn;tyliu@microsoft.com,7;7;7,4;3;3,Accept (Poster),0,5,0.0,yes,9/27/18,"Department of Computer Science, University of Toronto;Microsoft;Microsoft;Microsoft;Peking University;Microsoft",18;-1;-1;-1;24;-1,22;-1;-1;-1;27;-1,3
1698,1698,1698,1698,1698,1698,1698,1698,ICLR,2019,Structured Adversarial Attack:  Towards General Implementation and Better Interpretability,Kaidi Xu;Sijia Liu;Pu Zhao;Pin-Yu Chen;Huan Zhang;Quanfu Fan;Deniz Erdogmus;Yanzhi Wang;Xue Lin,xu.kaid@husky.neu.edu;sijia.liu@ibm.com;zhao.pu@husky.neu.edu;pin-yu.chen@ibm.com;ecezhang@ucdavis.edu;qfan@us.ibm.com;erdogmus@ece.neu.edu;yanz.wang@northeastern.edu;xue.lin@northeastern.edu,6;7;7,2;2;3,Accept (Poster),0,8,0.0,yes,9/27/18,"Northeastern University;International Business Machines;Northeastern University;International Business Machines;University of California, Davis;International Business Machines;Northeastern University;Northeastern University;Northeastern University",16;-1;16;-1;81;-1;16;16;16,839;-1;839;-1;54;-1;839;839;839,4
1699,1699,1699,1699,1699,1699,1699,1699,ICLR,2019,Learning sparse relational transition models,Victoria Xia;Zi Wang;Kelsey Allen;Tom Silver;Leslie Pack Kaelbling,victoria.f.xia281@gmail.com;ziw@mit.edu;krallen@mit.edu;tslvr@mit.edu;lpk@csail.mit.edu,6;7;8,4;2;3,Accept (Poster),0,0,1.0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2;2,5;5;5;5;5,
1700,1700,1700,1700,1700,1700,1700,1700,ICLR,2019,ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware,Han Cai;Ligeng Zhu;Song Han,hancai@mit.edu;ligeng@mit.edu;songhan@mit.edu,7;6;6,2;4;2,Accept (Poster),1,20,7.0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2,5;5;5,
1701,1701,1701,1701,1701,1701,1701,1701,ICLR,2019,No Training Required: Exploring Random Encoders for Sentence Classification,John Wieting;Douwe Kiela,jwieting@cs.cmu.edu;dkiela@fb.com,7;7;8,4;4;4,Accept (Poster),0,7,0.0,yes,9/27/18,Carnegie Mellon University;Facebook,1;-1,24;-1,3
1702,1702,1702,1702,1702,1702,1702,1702,ICLR,2019,Equi-normalization of Neural Networks,Pierre Stock;Benjamin Graham;Rémi Gribonval;Hervé Jégou,pstock@fb.com;benjamingraham@fb.com;remi.gribonval@inria.fr;rvj@fb.com,7;7;5,4;3;3,Accept (Poster),2,6,1.0,yes,9/27/18,Facebook;Facebook;INRIA;Facebook,-1;-1;-1;-1,-1;-1;-1;-1,
1703,1703,1703,1703,1703,1703,1703,1703,ICLR,2019,Deep Frank-Wolfe For Neural Network Optimization,Leonard Berrada;Andrew Zisserman;M. Pawan Kumar,lberrada@robots.ox.ac.uk;az@robots.ox.ac.uk;pawan@robots.ox.ac.uk,7;7;8,4;5;4,Accept (Poster),0,8,0.0,yes,9/27/18,University of Oxford;University of Oxford;University of Oxford,50;50;50,1;1;1,9;8
1704,1704,1704,1704,1704,1704,1704,1704,ICLR,2019,Learning what and where to attend,Drew Linsley;Dan Shiebler;Sven Eberhardt;Thomas Serre,drewlinsley@gmail.com;danshiebler@gmail.com;sven2sven2sven2@gmail.com;thomas_serre@brown.edu,6;6;8,4;3;3,Accept (Poster),0,9,0.0,yes,9/27/18,Brown University;Twitter;Amazon;Brown University,65;-1;-1;65,50;-1;-1;50,1
1705,1705,1705,1705,1705,1705,1705,1705,ICLR,2019,Minimum Divergence vs. Maximum Margin: an Empirical Comparison on Seq2Seq Models,Huan Zhang;Hai Zhao,zhanghuan0468@gmail.com;zhaohai@cs.sjtu.edu.cn,7;7;5,4;4;4,Accept (Poster),0,4,0.0,yes,9/27/18,Shanghai Jiao Tong University;Shanghai Jiao Tong University,52;52,188;188,3
1706,1706,1706,1706,1706,1706,1706,1706,ICLR,2019,Adaptivity of deep ReLU network for learning in Besov and mixed smooth Besov spaces: optimal rate and curse of dimensionality,Taiji Suzuki,taiji@mist.i.u-tokyo.ac.jp,8;6;6,2;2;2,Accept (Poster),0,7,0.0,yes,9/27/18,The University of Tokyo,54,45,3;9
1707,1707,1707,1707,1707,1707,1707,1707,ICLR,2019,Von Mises-Fisher Loss for Training Sequence to Sequence Models with Continuous Outputs,Sachin Kumar;Yulia Tsvetkov,sachink@cs.cmu.edu;ytsvetko@cs.cmu.edu,6;6;7,5;4;4,Accept (Poster),6,5,5.0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University,1;1,24;24,3
1708,1708,1708,1708,1708,1708,1708,1708,ICLR,2019,Feed-forward Propagation in Probabilistic Neural Networks with Categorical and Max Layers,Alexander Shekhovtsov;Boris Flach,shekhovtsov@gmail.com;bflach@inf.tu-dresden.de,6;6;6,3;5;4,Accept (Poster),0,7,0.0,yes,9/27/18,Czech Technical University in Prague;TU Dresden,314;169,740;155,
1709,1709,1709,1709,1709,1709,1709,1709,ICLR,2019,Generative Code Modeling with Graphs,Marc Brockschmidt;Miltiadis Allamanis;Alexander L. Gaunt;Oleksandr Polozov,mabrocks@microsoft.com;miallama@microsoft.com;algaunt@microsoft.com;polozov@microsoft.com,7;7;7,5;4;4,Accept (Poster),0,14,0.0,yes,9/27/18,Microsoft;Microsoft;Microsoft;Microsoft,-1;-1;-1;-1,-1;-1;-1;-1,5;10
1710,1710,1710,1710,1710,1710,1710,1710,ICLR,2019,Do Deep Generative Models Know What They Don't Know? ,Eric Nalisnick;Akihiro Matsukawa;Yee Whye Teh;Dilan Gorur;Balaji Lakshminarayanan,e.nalisnick@eng.cam.ac.uk;amatsukawa@google.com;ywteh@google.com;dilang@google.com;balajiln@google.com,7;6;7,4;4;3,Accept (Poster),2,18,1.0,yes,9/27/18,University of Cambridge;Google;Google;Google;Google,71;-1;-1;-1;-1,2;-1;-1;-1;-1,5
1711,1711,1711,1711,1711,1711,1711,1711,ICLR,2019,Harmonizing Maximum Likelihood with GANs for Multimodal Conditional Generation,Soochan Lee;Junsoo Ha;Gunhee Kim,soochan.lee@vision.snu.ac.kr;kuc2477@gmail.com;gunhee@snu.ac.kr,4;8;7,5;4;3,Accept (Poster),0,5,3.0,yes,9/27/18,Seoul National University;Hanyang University;Seoul National University,41;228;41,74;377;74,5;1
1712,1712,1712,1712,1712,1712,1712,1712,ICLR,2019,Visual Explanation by Interpretation: Improving Visual Feedback Capabilities of Deep Neural Networks,Jose Oramas;Kaili Wang;Tinne Tuytelaars,jose.oramas@esat.kuleuven.be;kaili.wang@esat.kuleuven.be;tinne.tuytelaars@esat.kuleuven.be,8;5;4,4;3;5,Accept (Poster),0,6,0.0,yes,9/27/18,KU Leuven;KU Leuven;KU Leuven,115;115;115,47;47;47,
1713,1713,1713,1713,1713,1713,1713,1713,ICLR,2019,Learning Finite State Representations of Recurrent Policy Networks,Anurag Koul;Alan Fern;Sam Greydanus,koula@oregonstate.edu;alan.fern@oregonstate.edu;sgrey@google.com,7;7;6,3;5;3,Accept (Poster),0,9,0.0,yes,9/27/18,Oregon State University;Oregon State University;Google,76;76;-1,318;318;-1,
1714,1714,1714,1714,1714,1714,1714,1714,ICLR,2019,Human-level Protein Localization with Convolutional Neural Networks,Elisabeth Rumetshofer;Markus Hofmarcher;Clemens Röhrl;Sepp Hochreiter;Günter Klambauer,rumetshofer@ml.jku.at;hofmarcher@ml.jku.at;clemens.roehrl@meduniwien.ac.at;hochreit@ml.jku.at;klambauer@ml.jku.at,4;5;8,4;3;4,Accept (Poster),0,4,0.0,yes,9/27/18,Johannes Kepler University Linz;Johannes Kepler University Linz;;Johannes Kepler University Linz;Johannes Kepler University Linz,228;228;-1;228;228,538;538;-1;538;538,
1715,1715,1715,1715,1715,1715,1715,1715,ICLR,2019,Value Propagation Networks,Nantas Nardelli;Gabriel Synnaeve;Zeming Lin;Pushmeet Kohli;Philip H. S. Torr;Nicolas Usunier,nantas@robots.ox.ac.uk;gab@fb.com;zlin@fb.com;pushmeet@google.com;philip.torr@eng.ox.ac.uk;usunier@fb.com,6;7;7,3;3;3,Accept (Poster),0,3,0.0,yes,9/27/18,University of Oxford;Facebook;Facebook;Google;University of Oxford;Facebook,50;-1;-1;-1;50;-1,1;-1;-1;-1;1;-1,
1716,1716,1716,1716,1716,1716,1716,1716,ICLR,2019,Adversarial Attacks on Graph Neural Networks via Meta Learning,Daniel Zügner;Stephan Günnemann,zuegnerd@in.tum.de;guennemann@in.tum.de,7;7;6,4;4;4,Accept (Poster),0,6,0.0,yes,9/27/18,Technical University Munich;Technical University Munich,54;54,41;41,4;10
1717,1717,1717,1717,1717,1717,1717,1717,ICLR,2019,Learning Latent Superstructures in Variational Autoencoders for Deep Multidimensional Clustering,Xiaopeng Li;Zhourong Chen;Leonard K. M. Poon;Nevin L. Zhang,xlibo@cse.ust.hk;zchenbb@cse.ust.hk;kmpoon@eduhk.hk;lzhang@cse.ust.hk,7;7;8,3;4;4,Accept (Poster),0,8,0.0,yes,9/27/18,The Hong Kong University of Science and Technology;The Hong Kong University of Science and Technology;The Education University of Hong Kong;The Hong Kong University of Science and Technology,39;39;89;39,44;44;40;44,5
1718,1718,1718,1718,1718,1718,1718,1718,ICLR,2019,Off-Policy Evaluation and Learning from Logged Bandit Feedback: Error Reduction via Surrogate Policy,Yuan Xie;Boyi Liu;Qiang Liu;Zhaoran Wang;Yuan Zhou;Jian Peng,xieyuan@umail.iu.edu;boyiliu2018@u.northwestern.edu;lqiang@cs.utexas.edu;zhaoranwang@gmail.com;yzhoucs@iu.edu;jianpeng@illinois.edu,6;8;6,4;4;3,Accept (Poster),0,4,0.0,yes,9/27/18,"Indiana University, Bloomington;Northwestern University;University of Texas, Austin;Northwestern University;Indiana University, Bloomington;University of Illinois, Urbana Champaign",72;44;22;44;72;3,117;20;49;20;117;37,1
1719,1719,1719,1719,1719,1719,1719,1719,ICLR,2019,SNIP: SINGLE-SHOT NETWORK PRUNING BASED ON CONNECTION SENSITIVITY,Namhoon Lee;Thalaiyasingam Ajanthan;Philip Torr,namhoon@robots.ox.ac.uk;ajanthan@robots.ox.ac.uk;phst@robots.ox.ac.uk,8;7;9,5;4;4,Accept (Poster),8,15,4.0,yes,9/27/18,University of Oxford;University of Oxford;University of Oxford,50;50;50,1;1;1,
1720,1720,1720,1720,1720,1720,1720,1720,ICLR,2019,Generating Multi-Agent Trajectories using Programmatic Weak Supervision,Eric Zhan;Stephan Zheng;Yisong Yue;Long Sha;Patrick Lucey,ezhan@caltech.edu;stzheng@caltech.edu;yyue@caltech.edu;lsha@stats.com;plucey@stats.com,7;6;6,3;3;3,Accept (Poster),0,4,0.0,yes,9/27/18,California Institute of Technology;California Institute of Technology;California Institute of Technology;STATS LLC;STATS LLC,140;140;140;-1;-1,3;3;3;-1;-1,5
1721,1721,1721,1721,1721,1721,1721,1721,ICLR,2019,Learning to Represent Edits,Pengcheng Yin;Graham Neubig;Miltiadis Allamanis;Marc Brockschmidt;Alexander L. Gaunt,pcyin@cs.cmu.edu;gneubig@cs.cmu.edu;miallama@microsoft.com;mabrocks@microsoft.com;algaunt@microsoft.com,6;7;6,3;3;4,Accept (Poster),0,13,2.0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;Microsoft;Microsoft;Microsoft,1;1;-1;-1;-1,24;24;-1;-1;-1,3
1722,1722,1722,1722,1722,1722,1722,1722,ICLR,2019,Transfer Learning for Sequences via Learning to Collocate,Wanyun Cui;Guangyu Zheng;Zhiqiang Shen;Sihang Jiang;Wei Wang,cui.wanyun@sufe.edu.cn;simonzgy@outlook.com;shen54@illinois.edu;tedjiangfdu@gmail.com;weiwang1@fudan.edu.cn,5;6;6,4;4;3,Accept (Poster),0,7,5.0,yes,9/27/18,"Shanghai University of Finance and Economics;Fudan University;University of Illinois, Urbana Champaign;;Fudan University",261;78;3;-1;78,1103;116;37;-1;116,3;6
1723,1723,1723,1723,1723,1723,1723,1723,ICLR,2019,Multi-Agent Dual Learning,Yiren Wang;Yingce Xia;Tianyu He;Fei Tian;Tao Qin;ChengXiang Zhai;Tie-Yan Liu,yiren@illinois.edu;yingce.xia@gmail.com;hetianyu@mail.ustc.edu.cn;fetia@microsoft.com;taoqin@microsoft.com;czhai@illinois.edu;tie-yan.liu@microsoft.com,6;6;6,2;3;4,Accept (Poster),4,8,0.0,yes,9/27/18,"University of Illinois, Urbana Champaign;Microsoft;University of Science and Technology of China;Microsoft;Microsoft;University of Illinois, Urbana Champaign;Microsoft",3;-1;478;-1;-1;3;-1,37;-1;132;-1;-1;37;-1,3;2
1724,1724,1724,1724,1724,1724,1724,1724,ICLR,2019,On the Sensitivity of Adversarial Robustness to Input Data Distributions,Gavin Weiguang Ding;Kry Yik Chau Lui;Xiaomeng Jin;Luyu Wang;Ruitong Huang,gavin.ding@borealisai.com;yikchau.y.lui@borealisai.com;tracy.jin@mail.utoronto.ca;luyu.wang@borealisai.com;ruitong.huang@borealisai.com,7;5;7,3;4;2,Accept (Poster),0,3,0.0,yes,9/27/18,Borealis AI;Borealis AI;Toronto University;Borealis AI;Borealis AI,-1;-1;18;-1;-1,-1;-1;22;-1;-1,4
1725,1725,1725,1725,1725,1725,1725,1725,ICLR,2019,GENERATING HIGH FIDELITY IMAGES WITH SUBSCALE PIXEL NETWORKS AND MULTIDIMENSIONAL UPSCALING,Jacob Menick;Nal Kalchbrenner,jmenick@google.com;nalk@google.com,9;10;7,3;5;3,Accept (Oral),2,9,1.0,yes,9/27/18,Google;Google,-1;-1,-1;-1,
1726,1726,1726,1726,1726,1726,1726,1726,ICLR,2019,Scalable Unbalanced Optimal Transport using Generative Adversarial Networks,Karren D. Yang;Caroline Uhler,karren@mit.edu;cuhler@mit.edu,7;6;6,4;4;4,Accept (Poster),0,8,0.0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2,5;5,5;4
1727,1727,1727,1727,1727,1727,1727,1727,ICLR,2019,ANYTIME MINIBATCH: EXPLOITING STRAGGLERS IN ONLINE DISTRIBUTED OPTIMIZATION,Nuwan Ferdinand;Haider Al-Lawati;Stark Draper;Matthew Nokleby,nuwan.ferdinand@utoronto.ca;haider.al.lawati@mail.utoronto.ca;stark.draper@utoronto.ca;matthew.nokleby@target.com,4;7;7,4;4;4,Accept (Poster),0,5,0.0,yes,9/27/18,Toronto University;Toronto University;Toronto University;Target,18;18;18;-1,22;22;22;-1,
1728,1728,1728,1728,1728,1728,1728,1728,ICLR,2019,Aggregated Momentum: Stability Through Passive Damping,James Lucas;Shengyang Sun;Richard Zemel;Roger  Grosse,jlucas@cs.toronto.edu;ssy@cs.toronto.edu;zemel@cs.toronto.edu;rgrosse@cs.toronto.edu,7;6;5,3;3;4,Accept (Poster),0,7,0.0,yes,9/27/18,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",18;18;18;18,22;22;22;22,
1729,1729,1729,1729,1729,1729,1729,1729,ICLR,2019,ClariNet: Parallel Wave Generation in End-to-End Text-to-Speech,Wei Ping;Kainan Peng;Jitong Chen,weiping.thu@gmail.com;pengkainan@baidu.com;jitongc@gmail.com,6;9;7,3;4;4,Accept (Poster),0,7,0.0,yes,9/27/18,Baidu;Baidu;,-1;-1;-1,-1;-1;-1,
1730,1730,1730,1730,1730,1730,1730,1730,ICLR,2019,Adaptive Estimators Show Information Compression in Deep Neural Networks,Ivan Chelombiev;Conor Houghton;Cian O'Donnell,ic14436@bristol.ac.uk;conor.houghton@bristol.ac.uk;cian.odonnell@bristol.ac.uk,7;6;7,4;4;3,Accept (Poster),0,6,0.0,yes,9/27/18,University of Bristol;University of Bristol;University of Bristol,123;123;123,76;76;76,8
1731,1731,1731,1731,1731,1731,1731,1731,ICLR,2019,A Unified Theory of Early Visual Representations from Retina to Cortex through Anatomically Constrained Deep CNNs,Jack Lindsey;Samuel A. Ocko;Surya Ganguli;Stephane Deny,lindsey6@stanford.edu;socko@stanford.edu;sganguli@stanford.edu;sdeny@stanford.edu,8;8;8,5;5;3,Accept (Oral),0,6,0.0,yes,9/27/18,Stanford University;Stanford University;Stanford University;Stanford University,4;4;4;4,3;3;3;3,5
1732,1732,1732,1732,1732,1732,1732,1732,ICLR,2019,RNNs implicitly implement tensor-product representations,R. Thomas McCoy;Tal Linzen;Ewan Dunbar;Paul Smolensky,tom.mccoy@jhu.edu;tal.linzen@jhu.edu;ewan.dunbar@univ-paris-diderot.fr;smolensky@jhu.edu,7;6;6,4;4;4,Accept (Poster),0,7,0.0,yes,9/27/18,Johns Hopkins University;Johns Hopkins University;Université Paris Diderot;Johns Hopkins University,72;72;478;72,13;13;234;13,
1733,1733,1733,1733,1733,1733,1733,1733,ICLR,2019,An Empirical study of Binary Neural Networks' Optimisation,Milad Alizadeh;Javier Fernández-Marqués;Nicholas D. Lane;Yarin Gal,milad.alizadeh@cs.ox.ac.uk;javier.fernandezmarques@cs.ox.ac.uk;nicholas.lane@cs.ox.ac.uk;yarin.gal@cs.ox.ac.uk,8;4;6,4;4;3,Accept (Poster),2,3,0.0,yes,9/27/18,University of Oxford;University of Oxford;University of Oxford;University of Oxford,50;50;50;50,1;1;1;1,
1734,1734,1734,1734,1734,1734,1734,1734,ICLR,2019,A Statistical Approach to Assessing Neural Network Robustness,Stefan Webb;Tom Rainforth;Yee Whye Teh;M. Pawan Kumar,info@stefanwebb.me;twgr@robots.ox.ac.uk;y.w.teh@stats.ox.ac.uk;pawan@robots.ox.ac.uk,6;7;8,5;4;3,Accept (Poster),0,10,0.0,yes,9/27/18,;University of Oxford;University of Oxford;University of Oxford,-1;50;50;50,-1;1;1;1,
1735,1735,1735,1735,1735,1735,1735,1735,ICLR,2019,Meta-Learning For Stochastic Gradient MCMC,Wenbo Gong;Yingzhen Li;José Miguel Hernández-Lobato,wg242@cam.ac.uk;yl494@cam.ac.uk;jmh233@cam.ac.uk,7;7;6,4;4;3,Accept (Poster),0,6,0.0,yes,9/27/18,University of Cambridge;University of Cambridge;University of Cambridge,71;71;71,2;2;2,11;6
1736,1736,1736,1736,1736,1736,1736,1736,ICLR,2019,Wizard of Wikipedia: Knowledge-Powered Conversational Agents,Emily Dinan;Stephen Roller;Kurt Shuster;Angela Fan;Michael Auli;Jason Weston,edinan@fb.com;roller@fb.com;kshuster@fb.com;angelafan@fb.com;michaelauli@fb.com;jase@fb.com,7;6;8,4;5;4,Accept (Poster),0,0,7.0,yes,9/27/18,Facebook;Facebook;Facebook;Facebook;Facebook;Facebook,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,
1737,1737,1737,1737,1737,1737,1737,1737,ICLR,2019,Information-Directed Exploration for Deep Reinforcement Learning,Nikolay Nikolov;Johannes Kirschner;Felix Berkenkamp;Andreas Krause,nikolay.nikolov14@imperial.ac.uk;jkirschner@inf.ethz.ch;befelix@inf.ethz.ch;krausea@ethz.ch,7;7;7,4;3;4,Accept (Poster),2,3,0.0,yes,9/27/18,Imperial College London;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,72;10;10;10,8;10;10;10,1
1738,1738,1738,1738,1738,1738,1738,1738,ICLR,2019,FlowQA: Grasping Flow in History for Conversational Machine Comprehension,Hsin-Yuan Huang;Eunsol Choi;Wen-tau Yih,hsinyuan@caltech.edu;eunsol@cs.washington.edu;scottyih@allenai.org,7;6;7,5;4;4,Accept (Poster),0,0,8.0,yes,9/27/18,California Institute of Technology;University of Washington;Allen Institute for Artificial Intelligence,140;6;-1,3;25;-1,
1739,1739,1739,1739,1739,1739,1739,1739,ICLR,2019,FFJORD: Free-Form Continuous Dynamics for Scalable Reversible Generative Models,Will Grathwohl;Ricky T. Q. Chen;Jesse Bettencourt;Ilya Sutskever;David Duvenaud,wgrathwohl@cs.toronto.edu;rtqichen@cs.toronto.edu;jessebett@cs.toronto.edu;ilyasu@openai.com;duvenaud@cs.toronto.edu,7;7;7,4;3;4,Accept (Oral),1,8,2.0,yes,9/27/18,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;OpenAI;Department of Computer Science, University of Toronto",18;18;18;-1;18,22;22;22;-1;22,5
1740,1740,1740,1740,1740,1740,1740,1740,ICLR,2019,Towards GAN Benchmarks Which Require Generalization,Ishaan Gulrajani;Colin Raffel;Luke Metz,igul222@gmail.com;craffel@gmail.com;lmetz@google.com,7;6;3,4;4;4,Accept (Poster),0,4,0.0,yes,9/27/18,Google;Google;Google,-1;-1;-1,-1;-1;-1,5;8
1741,1741,1741,1741,1741,1741,1741,1741,ICLR,2019,Multi-Domain Adversarial Learning,Alice Schoenauer-Sebag;Louise Heinrich;Marc Schoenauer;Michele Sebag;Lani F. Wu;Steve J. Altschuler,alice.schoenauer@polytechnique.org;louise.heinrich@ucsf.edu;marc.schoenauer@inria.fr;sebag@lri.fr;lani.wu@ucsf.edu;steven.altschuler@ucsf.edu,5;8;6,4;5;5,Accept (Poster),0,3,0.0,yes,9/27/18,"University of California, San Francisco;University of California, San Francisco;INRIA;CNRS, Université Paris-Saclay;University of California, San Francisco;University of California, San Francisco",-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,4;1
1742,1742,1742,1742,1742,1742,1742,1742,ICLR,2019,Mode Normalization,Lucas Deecke;Iain Murray;Hakan Bilen,l.deecke@ed.ac.uk;i.murray@ed.ac.uk;hbilen@ed.ac.uk,6;5;6,4;4;4,Accept (Poster),6,5,0.0,yes,9/27/18,University of Edinburgh;University of Edinburgh;University of Edinburgh,33;33;33,27;27;27,
1743,1743,1743,1743,1743,1743,1743,1743,ICLR,2019,Universal Successor Features Approximators,Diana Borsa;Andre Barreto;John Quan;Daniel J. Mankowitz;Hado van Hasselt;Remi Munos;David Silver;Tom Schaul,borsa@google.com;andrebarreto@google.com;johnquan@google.com;dmankowitz@google.com;hado@google.com;munos@google.com;davidsilver@google.com;schaul@google.com,7;5;6,3;2;4,Accept (Poster),0,0,0.0,yes,9/27/18,Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1,
1744,1744,1744,1744,1744,1744,1744,1744,ICLR,2019,Adversarial Imitation via Variational Inverse Reinforcement Learning,Ahmed H. Qureshi;Byron Boots;Michael C. Yip,a1quresh@eng.ucsd.edu;bboots@cc.gatech.edu;yip@ucsd.edu,6;6;6,4;3;4,Accept (Poster),0,11,0.0,yes,9/27/18,"University of California, San Diego;Georgia Institute of Technology;University of California, San Diego",11;13;11,31;33;31,5;4;6
1745,1745,1745,1745,1745,1745,1745,1745,ICLR,2019,On Self Modulation for Generative Adversarial Networks,Ting Chen;Mario Lucic;Neil Houlsby;Sylvain Gelly,iamtingchen@gmail.com;lucic@google.com;neilhoulsby@google.com;sylvaingelly@google.com,5;7;7,5;4;4,Accept (Poster),0,4,0.0,yes,9/27/18,"University of California, Los Angeles;Google;Google;Google",20;-1;-1;-1,15;-1;-1;-1,5;4
1746,1746,1746,1746,1746,1746,1746,1746,ICLR,2019,Attentive Neural Processes,Hyunjik Kim;Andriy Mnih;Jonathan Schwarz;Marta Garnelo;Ali Eslami;Dan Rosenbaum;Oriol Vinyals;Yee Whye Teh,hyunjikk@google.com;amnih@google.com;schwarzjn@google.com;garnelo@google.com;aeslami@google.com;danro@google.com;vinyals@google.com;ywteh@google.com,6;6;7,4;4;4,Accept (Poster),0,5,0.0,yes,9/27/18,Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1,
1747,1747,1747,1747,1747,1747,1747,1747,ICLR,2019,Relaxed Quantization for Discretized Neural Networks,Christos Louizos;Matthias Reisser;Tijmen Blankevoort;Efstratios Gavves;Max Welling,c.louizos@uva.nl;m.reisser@uva.nl;tijmen@qti.qualcomm.com;egavves@uva.nl;m.welling@uva.nl,7;7;7,4;3;4,Accept (Poster),0,12,0.0,yes,9/27/18,"University of Amsterdam;University of Amsterdam;Qualcomm Inc, QualComm;University of Amsterdam;University of Amsterdam",169;169;-1;169;169,59;59;-1;59;59,
1748,1748,1748,1748,1748,1748,1748,1748,ICLR,2019,Learning Factorized Multimodal Representations,Yao-Hung Hubert Tsai;Paul Pu Liang;Amir Zadeh;Louis-Philippe Morency;Ruslan Salakhutdinov,yaohungt@cs.cmu.edu;pliang@cs.cmu.edu;abagherz@cs.cmu.edu;morency@cs.cmu.edu;rsalakhu@cs.cmu.edu,7;7;6,3;2;3,Accept (Poster),0,5,0.0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1;1,24;24;24;24;24,5
1749,1749,1749,1749,1749,1749,1749,1749,ICLR,2019,code2seq: Generating Sequences from Structured Representations of Code,Uri Alon;Shaked Brody;Omer Levy;Eran Yahav,urialon1@gmail.com;shakedbr@cs.technion.ac.il;omerlevy@gmail.com;yahave@cs.technion.ac.il,6;7;5,4;4;4,Accept (Poster),0,15,0.0,yes,9/27/18,Technion;Technion;Facebook;Technion,25;25;-1;25,327;327;-1;327,3
1750,1750,1750,1750,1750,1750,1750,1750,ICLR,2019,Spreading vectors for similarity search,Alexandre Sablayrolles;Matthijs Douze;Cordelia Schmid;Hervé Jégou,asablayrolles@fb.com;matthijs@fb.com;cordelia.schmid@inria.fr;rvj@fb.com,6;6;7,4;3;4,Accept (Poster),0,8,0.0,yes,9/27/18,Facebook;Facebook;INRIA;Facebook,-1;-1;-1;-1,-1;-1;-1;-1,
1751,1751,1751,1751,1751,1751,1751,1751,ICLR,2019,DeepOBS: A Deep Learning Optimizer Benchmark Suite,Frank Schneider;Lukas Balles;Philipp Hennig,frank.schneider@tuebingen.mpg.de;lukas.balles@tuebingen.mpg.de;philipp.hennig@uni-tuebingen.de,6;7;6,4;4;4,Accept (Poster),0,9,0.0,yes,9/27/18,"Max Planck Institute for Intelligent Systems, Max-Planck Institute;Max Planck Institute for Intelligent Systems, Max-Planck Institute;University of Tuebingen",-1;-1;153,-1;-1;94,8
1752,1752,1752,1752,1752,1752,1752,1752,ICLR,2019,Conditional Network Embeddings,Bo Kang;Jefrey Lijffijt;Tijl De Bie,bo.kang@ugent.be;jefrey.lijffijt@ugent.be;tijl.debie@ugent.be,6;4;5,3;4;4,Accept (Poster),0,6,0.0,yes,9/27/18,Ghent University;Ghent University;Ghent University,478;478;478,107;107;107,11
1753,1753,1753,1753,1753,1753,1753,1753,ICLR,2019,Neural Persistence: A Complexity Measure for Deep Neural Networks Using Algebraic Topology,Bastian Rieck;Matteo Togninalli;Christian Bock;Michael Moor;Max Horn;Thomas Gumbsch;Karsten Borgwardt,bastian.rieck@bsse.ethz.ch;matteo.togninalli@bsse.ethz.ch;christian.bock@bsse.ethz.ch;michael.moor@bsse.ethz.ch;max.horn@bsse.ethz.ch;thomas.gumbsch@bsse.ethz.ch;karsten.borgwardt@bsse.ethz.ch,7;6;4,4;5;4,Accept (Poster),2,6,0.0,yes,9/27/18,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10;10;10;10;10,10;10;10;10;10;10;10,10
1754,1754,1754,1754,1754,1754,1754,1754,ICLR,2019,Large-Scale Answerer in Questioner's Mind for Visual Dialog Question Generation,Sang-Woo Lee;Tong Gao;Sohee Yang;Jaejun Yoo;Jung-Woo Ha,sang.woo.lee@navercorp.com;tong.gao@navercorp.com;sh.yang@navercorp.com;jaejun.yoo@navercorp.com;jungwoo.ha@navercorp.com,6;7;6,4;5;2,Accept (Poster),0,13,1.0,yes,9/27/18,NAVER;NAVER;NAVER;NAVER;NAVER,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,
1755,1755,1755,1755,1755,1755,1755,1755,ICLR,2019,Learning to Screen for Fast Softmax  Inference on Large Vocabulary Neural Networks,Patrick Chen;Si Si;Sanjiv Kumar;Yang Li;Cho-Jui Hsieh,patrickchen@g.ucla.edu;sisidaisy@google.com;sanjivk@google.com;liyang@google.com;chohsieh@cs.ucla.edu,7;6;8,4;3;4,Accept (Poster),0,6,0.0,yes,9/27/18,"University of California, Los Angeles;Google;Google;Google;University of California, Los Angeles",20;-1;-1;-1;20,15;-1;-1;-1;15,3
1756,1756,1756,1756,1756,1756,1756,1756,ICLR,2019,Learning to Schedule Communication in Multi-agent Reinforcement Learning,Daewoo Kim;Sangwoo Moon;David Hostallero;Wan Ju Kang;Taeyoung Lee;Kyunghwan Son;Yung Yi,kdw2139@gmail.com;swmoon00@gmail.com;ddhostallero@kaist.ac.kr;soarhigh0714@gmail.com;tylee0325@gmail.com;khson@lanada.kaist.ac.kr;yiyung@kaist.edu,7;7;8,3;2;5,Accept (Poster),0,6,0.0,yes,9/27/18,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;KAIST,20;20;20;20;20;20;20,95;95;95;95;95;95;95,
1757,1757,1757,1757,1757,1757,1757,1757,ICLR,2019,"On Random Deep Weight-Tied Autoencoders: Exact Asymptotic Analysis, Phase Transitions, and Implications to Training",Ping Li;Phan-Minh Nguyen,pingli98@gmail.com;npminh@stanford.edu,8;9;8,4;4;4,Accept (Oral),0,7,0.0,yes,9/27/18,Rutgers University New Brunswick;Stanford University,34;4,172;3,
1758,1758,1758,1758,1758,1758,1758,1758,ICLR,2019,DISTRIBUTIONAL CONCAVITY REGULARIZATION FOR GANS,Shoichiro Yamaguchi;Masanori Koyama,guguchi@preferred.jp;masomatics@preferred.jp,6;7;8;7,4;4;1;1,Accept (Poster),0,6,0.0,yes,9/27/18,"Preferred Networks, Inc.;Preferred Networks, Inc.",-1;-1,-1;-1,5;4
1759,1759,1759,1759,1759,1759,1759,1759,ICLR,2019,ROBUST ESTIMATION VIA GENERATIVE ADVERSARIAL NETWORKS,Chao GAO;jiyi LIU;Yuan YAO;Weizhi ZHU,chaogao@galton.uchicago.edu;jiyi.liu@yale.edu;yuany@ust.hk;wzhuai@connect.ust.hk,7;5;7,4;5;5,Accept (Poster),0,10,0.0,yes,9/27/18,University of Chicago;Yale University;The Hong Kong University of Science and Technology;The Hong Kong University of Science and Technology,48;62;39;39,9;12;44;44,5
1760,1760,1760,1760,1760,1760,1760,1760,ICLR,2019,Pay Less Attention with Lightweight and Dynamic Convolutions,Felix Wu;Angela Fan;Alexei Baevski;Yann Dauphin;Michael Auli,fw245@cornell.edu;angelfan@fb.com;alexei.b@gmail.com;yann@dauphin.io;michael.auli@gmail.com,8;8;8,4;4;4,Accept (Oral),0,13,6.0,yes,9/27/18,Cornell University;Facebook;Facebook;Facebook;Facebook,7;-1;-1;-1;-1,19;-1;-1;-1;-1,3;5
1761,1761,1761,1761,1761,1761,1761,1761,ICLR,2019,Analysis of Quantized Models,Lu Hou;Ruiliang Zhang;James T. Kwok,lhouab@cse.ust.hk;ruiliang.zhang@tusimple.ai;jamesk@cse.ust.hk,7;6;7,4;4;4,Accept (Poster),0,9,0.0,yes,9/27/18,The Hong Kong University of Science and Technology;TuSimple;The Hong Kong University of Science and Technology,39;-1;39,44;-1;44,
1762,1762,1762,1762,1762,1762,1762,1762,ICLR,2019,Prior Convictions: Black-box Adversarial Attacks with Bandits and Priors,Andrew Ilyas;Logan Engstrom;Aleksander Madry,ailyas@mit.edu;engstrom@mit.edu;madry@mit.edu,7;7;8,5;3;2,Accept (Poster),1,12,0.0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2,5;5;5,4
1763,1763,1763,1763,1763,1763,1763,1763,ICLR,2019,Approximability of Discriminators Implies Diversity in GANs,Yu Bai;Tengyu Ma;Andrej Risteski,yub@stanford.edu;tengyuma@stanford.edu;risteski@mit.edu,8;7;7,2;3;3,Accept (Poster),2,4,0.0,yes,9/27/18,Stanford University;Stanford University;Massachusetts Institute of Technology,4;4;2,3;3;5,5;4
1764,1764,1764,1764,1764,1764,1764,1764,ICLR,2019,Self-Monitoring Navigation Agent via Auxiliary Progress Estimation,Chih-Yao Ma;Jiasen Lu;Zuxuan Wu;Ghassan AlRegib;Zsolt Kira;Richard Socher;Caiming Xiong,cyma@gatech.edu;jiasenlu@gatech.edu;zxwu@cs.umd.edu;alregib@gatech.edu;zkira@gatech.edu;rsocher@salesforce.com;cxiong@salesforce.com,6;7;8,4;4;5,Accept (Poster),11,17,0.0,yes,9/27/18,"Georgia Institute of Technology;Georgia Institute of Technology;University of Maryland, College Park;Georgia Institute of Technology;Georgia Institute of Technology;SalesForce.com;SalesForce.com",13;13;12;13;13;-1;-1,33;33;69;33;33;-1;-1,
1765,1765,1765,1765,1765,1765,1765,1765,ICLR,2019,Residual Non-local Attention Networks for Image Restoration,Yulun Zhang;Kunpeng Li;Kai Li;Bineng Zhong;Yun Fu,yulun100@gmail.com;kunpengli@ece.neu.edu;li.kai.gml@gmail.com;bnzhong@hqu.edu.cn;yunfu@ece.neu.edu,7;7;6,3;5;3,Accept (Poster),0,6,0.0,yes,9/27/18,Northeastern University;Northeastern University;Northeastern University;Tsinghua University;Northeastern University,16;16;16;8;16,839;839;839;30;839,
1766,1766,1766,1766,1766,1766,1766,1766,ICLR,2019,Emergent Coordination Through Competition,Siqi Liu;Guy Lever;Josh Merel;Saran Tunyasuvunakool;Nicolas Heess;Thore Graepel,liusiqi@google.com;guylever@google.com;jsmerel@google.com;stunya@google.com;heess@google.com;thore@google.com,6;7;7,3;3;3,Accept (Poster),0,3,0.0,yes,9/27/18,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,
1767,1767,1767,1767,1767,1767,1767,1767,ICLR,2019,Differentiable Perturb-and-Parse: Semi-Supervised Parsing with a Structured Variational Autoencoder,Caio Corro;Ivan Titov,c.f.corro@uva.nl;i.a.titov@uva.nl,8;7;5,4;3;3,Accept (Poster),0,5,0.0,yes,9/27/18,University of Amsterdam;University of Amsterdam,169;169,59;59,3;5
1768,1768,1768,1768,1768,1768,1768,1768,ICLR,2019,Meta-Learning Probabilistic Inference for Prediction,Jonathan Gordon;John Bronskill;Matthias Bauer;Sebastian Nowozin;Richard Turner,jg801@cam.ac.uk;jfb54@cam.ac.uk;bauer@tue.mpg.de;nowozin@google.com;ret26@cam.ac.uk,7;6;8,4;2;4,Accept (Poster),0,10,3.0,yes,9/27/18,"University of Cambridge;University of Cambridge;Max Planck Institute for Intelligent Systems, Max-Planck Institute;Google;University of Cambridge",71;71;-1;-1;71,2;2;-1;-1;2,6
1769,1769,1769,1769,1769,1769,1769,1769,ICLR,2019,Data-Dependent Coresets for Compressing Neural Networks with Applications to Generalization Bounds,Cenk Baykal;Lucas Liebenwein;Igor Gilitschenski;Dan Feldman;Daniela Rus,baykal@mit.edu;lucasl@mit.edu;igilitschenski@mit.edu;dannyf@gmail.com;rus@csail.mit.edu,6;7;6,4;4;3,Accept (Poster),0,4,0.0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;;Massachusetts Institute of Technology,2;2;2;-1;2,5;5;5;-1;5,8
1770,1770,1770,1770,1770,1770,1770,1770,ICLR,2019,Beyond Greedy Ranking: Slate Optimization via List-CVAE,Ray Jiang;Sven Gowal;Yuqiu Qian;Timothy Mann;Danilo J. Rezende,rayjiang@google.com;sgowal@google.com;yqqian@cs.hku.hk;timothymann@google.com;danilor@google.com,6;6;7,3;4;4,Accept (Poster),0,7,0.0,yes,9/27/18,Google;Google;The University of Hong Kong;Google;Google,-1;-1;89;-1;-1,-1;-1;40;-1;-1,5
1771,1771,1771,1771,1771,1771,1771,1771,ICLR,2019,Generating Liquid Simulations with Deformation-aware Neural Networks,Lukas Prantl;Boris Bonev;Nils Thuerey,lukas.prantl@tum.de;boris.bonev@tum.de;nils.thuerey@tum.de,7;5;7,4;4;3,Accept (Poster),0,4,0.0,yes,9/27/18,Technical University Munich;Technical University Munich;Technical University Munich,54;54;54,41;41;41,
1772,1772,1772,1772,1772,1772,1772,1772,ICLR,2019,Soft Q-Learning with Mutual-Information Regularization,Jordi Grau-Moya;Felix Leibfried;Peter Vrancx,jordi@prowler.io;felix@prowler.io;peter@prowler.io,7;6;6,4;3;4,Accept (Poster),4,12,0.0,yes,9/27/18,Prowler.io;Prowler.io;Prowler.io,-1;-1;-1,-1;-1;-1,
1773,1773,1773,1773,1773,1773,1773,1773,ICLR,2019,Hindsight policy gradients,Paulo Rauber;Avinash Ummadisingu;Filipe Mutz;Jürgen Schmidhuber,paulo@idsia.ch;avinash.ummadisingu@usi.ch;filipe.mutz@ifes.edu.br;juergen@idsia.ch,7;7;7,4;4;4,Accept (Poster),2,7,0.0,yes,9/27/18,IDSIA;Università della Svizzera Italiana;Instituto Federal do Espirito Santo;IDSIA,-1;153;-1;-1,-1;1103;-1;-1,
1774,1774,1774,1774,1774,1774,1774,1774,ICLR,2019,Structured Neural Summarization,Patrick Fernandes;Miltiadis Allamanis;Marc Brockschmidt,t-pafern@microsoft.com;miallama@microsoft.com;mabrocks@microsoft.com,7;6;6,3;4;4,Accept (Poster),0,11,0.0,yes,9/27/18,Microsoft;Microsoft;Microsoft,-1;-1;-1,-1;-1;-1,3;10
1775,1775,1775,1775,1775,1775,1775,1775,ICLR,2019,ALISTA: Analytic Weights Are As Good As Learned Weights in LISTA,Jialin Liu;Xiaohan Chen;Zhangyang Wang;Wotao Yin,liujl11@math.ucla.edu;chernxh@tamu.edu;atlaswang@tamu.edu;wotaoyin@math.ucla.edu,7;9;10,4;5;5,Accept (Poster),0,7,0.0,yes,9/27/18,"University of California, Los Angeles;Texas A&M;Texas A&M;University of California, Los Angeles",20;44;44;20,15;160;160;15,
1776,1776,1776,1776,1776,1776,1776,1776,ICLR,2019,Subgradient Descent Learns Orthogonal Dictionaries,Yu Bai;Qijia Jiang;Ju Sun,yub@stanford.edu;qjiang2@stanford.edu;sunju@stanford.edu,6;7;7;7;7,1;2;3;4;3,Accept (Poster),0,8,0.0,yes,9/27/18,Stanford University;Stanford University;Stanford University,4;4;4,3;3;3,
1777,1777,1777,1777,1777,1777,1777,1777,ICLR,2019,A new dog learns old tricks:  RL finds classic optimization algorithms,Weiwei Kong;Christopher Liaw;Aranyak Mehta;D. Sivakumar,wkong37@gatech.edu;cvliaw@cs.ubc.ca;aranyak@google.com;siva@google.com,6;6;7,3;3;5,Accept (Poster),0,11,0.0,yes,9/27/18,Georgia Institute of Technology;University of British Columbia;Google;Google,13;36;-1;-1,33;34;-1;-1,4
1778,1778,1778,1778,1778,1778,1778,1778,ICLR,2019,Rigorous Agent Evaluation: An Adversarial Approach to Uncover Catastrophic Failures,Jonathan Uesato*;Ananya Kumar*;Csaba Szepesvari*;Tom Erez;Avraham Ruderman;Keith Anderson;Krishnamurthy (Dj) Dvijotham;Nicolas Heess;Pushmeet Kohli,juesato@gmail.com;ananya@cs.stanford.edu;szepi@google.com;etom@google.com;aruderman@google.com;keithanderson@google.com;dvij@google.com;heess@google.com;pushmeet@google.com,6;6;6,3;3;3,Accept (Poster),0,9,0.0,yes,9/27/18,;Stanford University;Google;Google;Google;Google;Google;Google;Google,-1;4;-1;-1;-1;-1;-1;-1;-1,-1;3;-1;-1;-1;-1;-1;-1;-1,4
1779,1779,1779,1779,1779,1779,1779,1779,ICLR,2019,AntisymmetricRNN: A Dynamical System View on Recurrent Neural Networks,Bo Chang;Minmin Chen;Eldad Haber;Ed H. Chi,bchang@stat.ubc.ca;minminc@google.com;haber@math.ubc.ca;edchi@google.com,7;7;6,5;5;5,Accept (Poster),0,3,0.0,yes,9/27/18,University of British Columbia;Google;University of British Columbia;Google,36;-1;36;-1,34;-1;34;-1,
1780,1780,1780,1780,1780,1780,1780,1780,ICLR,2019,Recall Traces: Backtracking Models for Efficient Reinforcement Learning,Anirudh Goyal;Philemon Brakel;William Fedus;Soumye Singhal;Timothy Lillicrap;Sergey Levine;Hugo Larochelle;Yoshua Bengio,anirudhgoyal9119@gmail.com;philemon@google.com;liam.fedus@gmail.com;singhalsoumye@gmail.com;countzero@google.com;svlevine@eecs.berkeley.edu;hugolarochelle@google.com;yoshua.bengio@mila.quebec,7;7;6,2;3;3,Accept (Poster),0,10,0.0,yes,9/27/18,University of Montreal;Google;University of Montreal;IIT Kanpur;Google;University of California Berkeley;Google;University of Montreal,123;-1;123;123;-1;5;-1;123,108;-1;108;578;-1;18;-1;108,
1781,1781,1781,1781,1781,1781,1781,1781,ICLR,2019,Auxiliary Variational MCMC,Raza Habib;David Barber,raza.habib@cs.ucl.ac.uk;david.barber@ucl.ac.uk,7;7;7,5;4;4,Accept (Poster),0,11,0.0,yes,9/27/18,University College London;University College London,50;50,16;16,11
1782,1782,1782,1782,1782,1782,1782,1782,ICLR,2019,Training for Faster Adversarial Robustness Verification via Inducing ReLU Stability,Kai Y. Xiao;Vincent Tjeng;Nur Muhammad (Mahi) Shafiullah;Aleksander Madry,kaix@mit.edu;vtjeng@mit.edu;nshafiul@mit.edu;madry@mit.edu,5;8;7,3;2;3,Accept (Poster),0,14,0.0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2,5;5;5;5,4
1783,1783,1783,1783,1783,1783,1783,1783,ICLR,2019,RotDCF: Decomposition of Convolutional Filters for Rotation-Equivariant Deep Networks,Xiuyuan Cheng;Qiang Qiu;Robert Calderbank;Guillermo Sapiro,xiuyuan.cheng@duke.edu;qiang.qiu@duke.edu;robert.calderbank@duke.edu;guillermo.sapiro@duke.edu,7;7;7,3;2;4,Accept (Poster),0,4,1.0,yes,9/27/18,Duke University;Duke University;Duke University;Duke University,44;44;44;44,17;17;17;17,
1784,1784,1784,1784,1784,1784,1784,1784,ICLR,2019,Three Mechanisms of Weight Decay Regularization,Guodong Zhang;Chaoqi Wang;Bowen Xu;Roger Grosse,gdzhang.cs@gmail.com;cqwang@cs.toronto.edu;bowenxu@cs.toronto.com;rgrosse@cs.toronto.edu,6;7;7,4;5;4,Accept (Poster),0,9,0.0,yes,9/27/18,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",18;18;18;18,22;22;22;22,8
1785,1785,1785,1785,1785,1785,1785,1785,ICLR,2019,Imposing Category Trees Onto Word-Embeddings Using A Geometric Construction,Tiansi Dong;Chrisitan Bauckhage;Hailong Jin;Juanzi Li;Olaf Cremers;Daniel Speicher;Armin B. Cremers;Joerg Zimmermann,tian1shi2@gmail.com;christian.bauckhage@iais.fraunhofer.de;jinhl15@mails.tsinghua.edu.cn;lijuanzi2008@gmail.com;cremerso@iai.uni-bonn.de;dsp@bit.uni-bonn.de;abc@iai.uni-bonn.de;jz@bit.uni-bonn.de,3;4;4,4;5;4,Accept (Poster),0,2,32.0,yes,9/27/18,University of Bonn;Fraunhofer IIS;Tsinghua University;;University of Bonn;University of Bonn;University of Bonn;University of Bonn,123;-1;8;-1;123;123;123;123,100;-1;30;-1;100;100;100;100,3
1786,1786,1786,1786,1786,1786,1786,1786,ICLR,2019,PeerNets: Exploiting Peer Wisdom Against Adversarial Attacks,Jan Svoboda;Jonathan Masci;Federico Monti;Michael Bronstein;Leonidas Guibas,jan.svoboda@usi.ch;jonathan@nnaisense.com;federico.monti@usi.ch;michael.bronstein@usi.ch;guibas@cs.stanford.edu,6;7,5;4,Accept (Poster),2,8,0.0,yes,9/27/18,Università della Svizzera Italiana;NNAISENSE;Università della Svizzera Italiana;Università della Svizzera Italiana;Stanford University,153;-1;153;153;4,1103;-1;1103;1103;3,4;10
1787,1787,1787,1787,1787,1787,1787,1787,ICLR,2019,Measuring Compositionality in Representation Learning,Jacob Andreas,jda@cs.berkeley.edu,7;6;6,4;4;4,Accept (Poster),0,4,0.0,yes,9/27/18,University of California Berkeley,5,18,8
1788,1788,1788,1788,1788,1788,1788,1788,ICLR,2019,ProxQuant: Quantized Neural Networks via Proximal Operators,Yu Bai;Yu-Xiang Wang;Edo Liberty,yub@stanford.edu;yuxiangw@cs.ucsb.edu;libertye@amazon.com,7;5;8,4;4;4,Accept (Poster),2,9,0.0,yes,9/27/18,Stanford University;UC Santa Barbara;Amazon,4;37;-1,3;53;-1,9
1789,1789,1789,1789,1789,1789,1789,1789,ICLR,2019,Directed-Info GAIL: Learning Hierarchical Policies from Unsegmented Demonstrations using Directed Information,Mohit Sharma;Arjun Sharma;Nicholas Rhinehart;Kris M. Kitani,mohits1@andrew.cmu.edu;arjuns2@andrew.cmu.edu;nrhineha@cs.cmu.edu;kkitani@cs.cmu.edu,6;6;8,4;4;4,Accept (Poster),0,6,0.0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1,24;24;24;24,5;4;10
1790,1790,1790,1790,1790,1790,1790,1790,ICLR,2019,Hierarchical RL Using an Ensemble of Proprioceptive Periodic Policies,Kenneth Marino;Abhinav Gupta;Rob Fergus;Arthur Szlam,kdmarino@cs.cmu.edu;abhinavg@cs.cmu.edu;fergus@cs.nyu.edu;aszlam@fb.com,7;6;7,5;4;3,Accept (Poster),0,7,0.0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;New York University;Facebook,1;1;26;-1,24;24;27;-1,
1791,1791,1791,1791,1791,1791,1791,1791,ICLR,2019,ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness,Robert Geirhos;Patricia Rubisch;Claudio Michaelis;Matthias Bethge;Felix A. Wichmann;Wieland Brendel,robert@geirhos.de;patricia@rubisch.net;claudio.michaelis@bethgelab.org;matthias.bethge@uni-tuebingen.de;felix.wichmann@uni-tuebingen.de;wieland.brendel@bethgelab.org,8;7;8,4;4;4,Accept (Oral),2,9,3.0,yes,9/27/18,"University of Tuebingen;;Centre for Integrative Neuroscience, AG Bethge;University of Tuebingen;University of Tuebingen;Centre for Integrative Neuroscience, AG Bethge",153;-1;153;153;153;153,94;-1;94;94;94;94,2
1792,1792,1792,1792,1792,1792,1792,1792,ICLR,2019,Verification of Non-Linear Specifications for Neural Networks,Chongli Qin;Krishnamurthy (Dj) Dvijotham;Brendan O'Donoghue;Rudy Bunel;Robert Stanforth;Sven Gowal;Jonathan Uesato;Grzegorz Swirszcz;Pushmeet Kohli,chongliqin@google.com;dvij@google.com;bodonoghue@google.com;rbunel@google.com;stanforth@google.com;sgowal@google.com;juesato@google.com;swirszcz@google.com;pushmeet@google.com,7;7;5,3;5;3,Accept (Poster),0,12,0.0,yes,9/27/18,Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1,4
1793,1793,1793,1793,1793,1793,1793,1793,ICLR,2019,LEARNING FACTORIZED REPRESENTATIONS FOR OPEN-SET DOMAIN ADAPTATION,Mahsa Baktashmotlagh;Masoud Faraki;Tom Drummond;Mathieu Salzmann,m.baktashmotlagh@qut.edu.au;masoud.faraki@monash.edu;tom.drummond@monash.edu;mathieu.salzmann@epfl.ch,7;6;6,3;4;5,Accept (Poster),2,10,0.0,yes,9/27/18,South China University of Technology;Monash University;Monash University;Swiss Federal Institute of Technology Lausanne,478;123;123;478,576;80;80;38,
1794,1794,1794,1794,1794,1794,1794,1794,ICLR,2019,Variance Networks: When Expectation Does Not Meet Your Expectations,Kirill Neklyudov;Dmitry Molchanov;Arsenii Ashukha;Dmitry Vetrov,k.necludov@gmail.com;dmolch111@gmail.com;ars.ashuha@gmail.com;vetrovd@yandex.ru,6;6;6,3;4;4,Accept (Poster),0,3,0.0,yes,9/27/18,Higher School of Economics;Samsung;Samsung;Higher School of Economics,478;-1;-1;478,377;-1;-1;377,4;11
1795,1795,1795,1795,1795,1795,1795,1795,ICLR,2019,Function Space Particle Optimization for Bayesian Neural Networks,Ziyu Wang;Tongzheng Ren;Jun Zhu;Bo Zhang,wzy196@gmail.com;rtz19970824@gmail.com;dcszj@mail.tsinghua.edu.cn;dcszb@mail.tsinghua.edu.cn,7;7;7,3;4;3,Accept (Poster),0,10,2.0,yes,9/27/18,Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University,8;8;8;8,30;30;30;30,4;11
1796,1796,1796,1796,1796,1796,1796,1796,ICLR,2019,CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model,Florian Mai;Lukas Galke;Ansgar Scherp,florian.ren.mai@googlemail.com;lga@informatik.uni-kiel.de;mail@ansgarscherp.net,6;5;6,3;4;4,Accept (Poster),0,6,0.0,yes,9/27/18,Idiap Research Institute;Kiel University;University of Stirling,-1;62;478,-1;12;309,3
1797,1797,1797,1797,1797,1797,1797,1797,ICLR,2019,Evaluating Robustness of Neural Networks with Mixed Integer Programming,Vincent Tjeng;Kai Y. Xiao;Russ Tedrake,vtjeng@mit.edu;kaix@mit.edu;russt@mit.edu,7;8;7,5;5;1,Accept (Poster),0,14,0.0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2,5;5;5,4
1798,1798,1798,1798,1798,1798,1798,1798,ICLR,2019,Post Selection Inference with Incomplete Maximum Mean Discrepancy Estimator,Makoto Yamada;Denny Wu;Yao-Hung Hubert Tsai;Hirofumi Ohta;Ruslan Salakhutdinov;Ichiro Takeuchi;Kenji Fukumizu,makoto.yamada@riken.jp;yiwu1@andrew.cmu.edu;yaohungt@cs.cmu.edu;hirofumi-ohta@g.ecc.u-tokyo.ac.jp;rsalakhu@cs.cmu.edu;takeuchi.ichiro@nitech.ac.jp;fukumizu@ism.ac.jp,6;5;8,4;4;4,Accept (Poster),0,6,0.0,yes,9/27/18,"RIKEN;Carnegie Mellon University;Carnegie Mellon University;The University of Tokyo;Carnegie Mellon University;Meiji University;The Institute of Statistical Mathematics, Japan",-1;1;1;54;1;478;-1,-1;24;24;45;24;334;-1,5;4
1799,1799,1799,1799,1799,1799,1799,1799,ICLR,2019,Opportunistic Learning: Budgeted Cost-Sensitive Learning from Data Streams,Mohammad Kachuee;Orpaz Goldstein;Kimmo Kärkkäinen;Sajad Darabi;Majid Sarrafzadeh,mkachuee@cs.ucla.edu;orpgol@cs.ucla.edu;kimmo@cs.ucla.edu;sajad.darabi@cs.ucla.edu;majid@cs.ucla.edu,7;6;6,4;4;4,Accept (Poster),0,14,0.0,yes,9/27/18,"University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles",20;20;20;20;20,15;15;15;15;15,
1800,1800,1800,1800,1800,1800,1800,1800,ICLR,2019,Riemannian Adaptive Optimization Methods,Gary Becigneul;Octavian-Eugen Ganea,gary.becigneul@inf.ethz.ch;octavian.ganea@inf.ethz.ch,7;7;7,3;5;4,Accept (Poster),5,6,3.0,yes,9/27/18,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10,10;10,1;8
1801,1801,1801,1801,1801,1801,1801,1801,ICLR,2019,StrokeNet: A Neural Painting Environment,Ningyuan Zheng;Yifan Jiang;Dingjiang Huang,zhengningyuan@qq.com;winhehe@163.com;djhuang@dase.ecnu.edu.cn,7;6;8,4;4;5,Accept (Poster),0,8,2.0,yes,9/27/18,Australian National University;Australian National University;Australian National University,106;106;106,48;48;48,
1802,1802,1802,1802,1802,1802,1802,1802,ICLR,2019,Non-vacuous Generalization Bounds at the ImageNet Scale: a PAC-Bayesian Compression Approach,Wenda Zhou;Victor Veitch;Morgane Austern;Ryan P. Adams;Peter Orbanz,wz2335@columbia.edu;victorveitch@gmail.com;ma3293@columbia.edu;rpa@princeton.edu;porbanz@stat.columbia.edu,6;6;8,4;5;4,Accept (Poster),0,5,0.0,yes,9/27/18,Columbia University;Columbia University;Columbia University;Princeton University;Columbia University,15;15;15;30;15,14;14;14;7;14,1;8
1803,1803,1803,1803,1803,1803,1803,1803,ICLR,2019,RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space,Zhiqing Sun;Zhi-Hong Deng;Jian-Yun Nie;Jian Tang,1500012783@pku.edu.cn;zhdeng@pku.edu.cn;nie@iro.umontreal.ca;jian.tang@hec.ca,7;7;7,4;3;4,Accept (Poster),1,21,13.0,yes,9/27/18,Peking University;Peking University;University of Montreal;HEC Montreal,24;24;123;-1,27;27;108;-1,4;10
1804,1804,1804,1804,1804,1804,1804,1804,ICLR,2019,STCN: Stochastic Temporal Convolutional Networks,Emre Aksan;Otmar Hilliges,eaksan@inf.ethz.ch;otmar.hilliges@inf.ethz.ch,6;6;6,4;3;5,Accept (Poster),0,8,0.0,yes,9/27/18,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10,10;10,
1805,1805,1805,1805,1805,1805,1805,1805,ICLR,2019,From Hard to Soft: Understanding Deep Network Nonlinearities via Vector Quantization and Statistical Inference,Randall Balestriero;Richard Baraniuk,randallbalestriero@gmail.com;richb@rice.edu,6;6;7,3;4;5,Accept (Poster),0,4,0.0,yes,9/27/18,Rice University;Rice University,85;85,86;86,2
1806,1806,1806,1806,1806,1806,1806,1806,ICLR,2019,Competitive experience replay,Hao Liu;Alexander Trott;Richard Socher;Caiming Xiong,lhao499@gmail.com;atrott@salesforce.com;rsocher@salesforce.com;cxiong@salesforce.com,5;7;6;7,4;4;4;5,Accept (Poster),0,6,0.0,yes,9/27/18,University of California Berkeley;SalesForce.com;SalesForce.com;SalesForce.com,5;-1;-1;-1,18;-1;-1;-1,4
1807,1807,1807,1807,1807,1807,1807,1807,ICLR,2019,Deterministic Variational Inference for Robust Bayesian Neural Networks,Anqi Wu;Sebastian Nowozin;Edward Meeds;Richard E. Turner;José Miguel Hernández-Lobato;Alexander L. Gaunt,anqiw@princeton.edu;sebastian.nowozin@microsoft.com;ted.meeds@microsoft.com;ret26@cam.ac.uk;jmh233@cam.ac.uk;algaunt@microsoft.com,7;7;7,3;3;5,Accept (Oral),0,3,1.0,yes,9/27/18,Princeton University;Microsoft;Microsoft;University of Cambridge;University of Cambridge;Microsoft,30;-1;-1;71;71;-1,7;-1;-1;2;2;-1,11
1808,1808,1808,1808,1808,1808,1808,1808,ICLR,2019,Marginal Policy Gradients: A Unified Family of Estimators for Bounded Action Spaces with Applications,Carson Eisenach;Haichuan Yang;Ji Liu;Han Liu,eisenach@princeton.edu;h.yang@rochester.edu;ji.liu.uwisc@gmail.com;hanliu.cmu@gmail.com,7;7;6,4;3;3,Accept (Poster),0,4,0.0,yes,9/27/18,Princeton University;University of Rochester;University of Rochester;,30;106;106;-1,7;153;153;-1,
1809,1809,1809,1809,1809,1809,1809,1809,ICLR,2019,Harmonic Unpaired Image-to-image Translation,Rui Zhang;Tomas Pfister;Jia Li,zhangrui@ict.ac.cn;tpfister@google.com;lijiali@google.com,6;5;4,5;5;5,Accept (Poster),3,7,0.0,yes,9/27/18,"Institute of Computing Technology, Chinese Academy of Sciences;Google;Google",62;-1;-1,1103;-1;-1,10
1810,1810,1810,1810,1810,1810,1810,1810,ICLR,2019,Trellis Networks for Sequence Modeling,Shaojie Bai;J. Zico Kolter;Vladlen Koltun,shaojieb@cs.cmu.edu;zkolter@cs.cmu.edu;vkoltun@gmail.com,7;6;7,3;3;3,Accept (Poster),0,6,0.0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;Intel,1;1;-1,24;24;-1,3
1811,1811,1811,1811,1811,1811,1811,1811,ICLR,2019,Adversarial Reprogramming of Neural Networks,Gamaleldin F. Elsayed;Ian Goodfellow;Jascha Sohl-Dickstein,gamaleldin.elsayed@gmail.com;goodfellow@google.com;jaschasd@google.com,8;6;4,4;5;3,Accept (Poster),2,6,2.0,yes,9/27/18,Google;Google;Google,-1;-1;-1,-1;-1;-1,4;2
1812,1812,1812,1812,1812,1812,1812,1812,ICLR,2019,Variational Bayesian Phylogenetic Inference,Cheng Zhang;Frederick A. Matsen IV,zc.rabbit@gmail.com;matsen@fredhutch.org,6;5;7,3;3;1,Accept (Poster),0,6,0.0,yes,9/27/18,Fred Hutchinson Cancer Research Center;Fred Hutchinson Cancer Research Center,-1;-1,-1;-1,11;10
1813,1813,1813,1813,1813,1813,1813,1813,ICLR,2019,Learning to Understand Goal Specifications by Modelling Reward,Dzmitry Bahdanau;Felix Hill;Jan Leike;Edward Hughes;Arian Hosseini;Pushmeet Kohli;Edward Grefenstette,dimabgv@gmail.com;felixhill@google.com;leike@google.com;edwardhughes@google.com;seyedarian.hosseini@umontreal.ca;pushmeet@google.com;etg@google.com,7;7;6,4;5;4,Accept (Poster),0,0,21.0,yes,9/27/18,University of Montreal;Google;Google;Google;University of Montreal;Google;Google,123;-1;-1;-1;123;-1;-1,108;-1;-1;-1;108;-1;-1,
1814,1814,1814,1814,1814,1814,1814,1814,ICLR,2019,Overcoming the Disentanglement vs Reconstruction Trade-off via Jacobian Supervision,José Lezama,jlezama@fing.edu.uy,5;7;7,3;4;4,Accept (Poster),2,5,0.0,yes,9/27/18,Facultad de Ingeniería,478,631,4
1815,1815,1815,1815,1815,1815,1815,1815,ICLR,2019,Relational Forward Models for Multi-Agent Learning,Andrea Tacchetti;H. Francis Song;Pedro A. M. Mediano;Vinicius Zambaldi;János Kramár;Neil C. Rabinowitz;Thore Graepel;Matthew Botvinick;Peter W. Battaglia,atacchet@google.com;songf@google.com;pmediano@imperial.ac.uk;vzambaldi@google.com;janosk@google.com;ncr@google.com;thore@google.com;botvinick@google.com;peterbattaglia@google.com,7;6;7;6,3;4;3;3,Accept (Poster),0,21,1.0,yes,9/27/18,Google;Google;Imperial College London;Google;Google;Google;Google;Google;Google,-1;-1;72;-1;-1;-1;-1;-1;-1,-1;-1;8;-1;-1;-1;-1;-1;-1,
1816,1816,1816,1816,1816,1816,1816,1816,ICLR,2019,NADPEx: An on-policy temporally consistent exploration method for deep reinforcement learning,Sirui Xie;Junning Huang;Lanxin Lei;Chunxiao Liu;Zheng Ma;Wei Zhang;Liang Lin,xiesirui@sensetime.com;huangjunning@sensetime.com;leilanxin@sensetime.com;liuchunxiao@sensetime.com;mazheng@sensetime.com;wayne.zhang@sensetime.com;linliang@ieee.org,6;8;7,3;3;3,Accept (Poster),2,9,0.0,yes,9/27/18,SenseTime Group Limited;SenseTime Group Limited;SenseTime Group Limited;SenseTime Group Limited;SenseTime Group Limited;SenseTime Group Limited;SUN YAT-SEN UNIVERSITY,-1;-1;-1;-1;-1;-1;478,-1;-1;-1;-1;-1;-1;352,
1817,1817,1817,1817,1817,1817,1817,1817,ICLR,2019,"Woulda, Coulda, Shoulda: Counterfactually-Guided Policy Search",Lars Buesing;Theophane Weber;Yori Zwols;Nicolas Heess;Sebastien Racaniere;Arthur Guez;Jean-Baptiste Lespiau,lbuesing@google.com;theophane@google.com;yori@google.com;heess@google.com;sracaniere@google.com;aguez@google.com;jblespiau@google.com,7;7;7,2;3;3,Accept (Poster),0,5,1.0,yes,9/27/18,Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,
1818,1818,1818,1818,1818,1818,1818,1818,ICLR,2019,Big-Little Net: An Efficient Multi-Scale Feature Representation for Visual and Speech Recognition,Chun-Fu (Richard) Chen;Quanfu Fan;Neil Mallinar;Tom Sercu;Rogerio Feris,chenrich@us.ibm.com;qfan@us.ibm.com;neil.r.mallinar@ibm.com;tom.sercu1@ibm.com;rsferis@us.ibm.com,7;6;7,4;5;4,Accept (Poster),0,4,0.0,yes,9/27/18,International Business Machines;International Business Machines;International Business Machines;International Business Machines;International Business Machines,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,8
1819,1819,1819,1819,1819,1819,1819,1819,ICLR,2019,There Are Many Consistent Explanations of Unlabeled Data: Why You Should Average,Ben Athiwaratkun;Marc Finzi;Pavel Izmailov;Andrew Gordon Wilson,pa338@cornell.edu;maf388@cornell.edu;izmailovpavel@gmail.com;andrew@cornell.edu,6;8;6,4;4;1,Accept (Poster),2,6,5.0,yes,9/27/18,Cornell University;Cornell University;Cornell University;Cornell University,7;7;7;7,19;19;19;19,8
1820,1820,1820,1820,1820,1820,1820,1820,ICLR,2019,Wasserstein Barycenter Model Ensembling,Pierre Dognin*;Igor Melnyk*;Youssef Mroueh*;Jarret Ross*;Cicero Dos Santos*;Tom Sercu*,pdognin@us.ibm.com;igor.melnyk@ibm.com;mroueh@us.ibm.com;rossja@us.ibm.com;cicerons@us.ibm.com;tom.sercu1@ibm.com,6;6;6,4;3;4,Accept (Poster),0,5,0.0,yes,9/27/18,International Business Machines;International Business Machines;International Business Machines;International Business Machines;International Business Machines;International Business Machines,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,3
1821,1821,1821,1821,1821,1821,1821,1821,ICLR,2019,Learning what you can do before doing anything,Oleh Rybkin;Karl Pertsch;Konstantinos G. Derpanis;Kostas Daniilidis;Andrew Jaegle,oleh@seas.upenn.edu;pertsch@usc.edu;kosta@ryerson.ca;kostas@seas.upenn.edu;ajaegle@upenn.edu,6;7;6,4;4;3,Accept (Poster),0,6,0.0,yes,9/27/18,University of Pennsylvania;University of Southern California;Ryerson University;University of Pennsylvania;University of Pennsylvania,19;30;314;19;19,10;66;1103;10;10,
1822,1822,1822,1822,1822,1822,1822,1822,ICLR,2019,Learning Localized Generative Models for 3D Point Clouds via Graph Convolution,Diego Valsesia;Giulia Fracastoro;Enrico Magli,diego.valsesia@polito.it;giulia.fracastoro@polito.it;enrico.magli@polito.it,6;9;7,4;3;3,Accept (Poster),0,7,0.0,yes,9/27/18,Politecnico di Torino;Politecnico di Torino;Politecnico di Torino,478;478;478,407;407;407,10;5;2;8
1823,1823,1823,1823,1823,1823,1823,1823,ICLR,2019,Policy Transfer with Strategy Optimization,Wenhao Yu;C. Karen Liu;Greg Turk,wyu68@gatech.edu;karenliu@cc.gatech.edu;turk@cc.gatech.edu,7;6;7,4;4;4,Accept (Poster),0,7,0.0,yes,9/27/18,Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology,13;13;13,33;33;33,6
1824,1824,1824,1824,1824,1824,1824,1824,ICLR,2019,Measuring and regularizing networks in function space,Ari Benjamin;David Rolnick;Konrad Kording,aarrii@seas.upenn.edu;drolnick@mit.edu;koerding@gmail.com,6;6;6,4;3;4,Accept (Poster),0,5,1.0,yes,9/27/18,University of Pennsylvania;Massachusetts Institute of Technology;University of Pennsylvania,19;2;19,10;5;10,
1825,1825,1825,1825,1825,1825,1825,1825,ICLR,2019,Learning Two-layer Neural Networks with Symmetric Inputs,Rong Ge;Rohith Kuditipudi;Zhize Li;Xiang Wang,rongge@cs.duke.edu;rohith.kuditipudi@duke.edu;zz-li14@mails.tsinghua.edu.cn;xwang@cs.duke.edu,7;6;7,4;4;5,Accept (Poster),0,3,0.0,yes,9/27/18,Duke University;Duke University;Tsinghua University;Duke University,44;44;8;44,17;17;30;17,9
1826,1826,1826,1826,1826,1826,1826,1826,ICLR,2019,CEM-RL: Combining evolutionary and gradient-based methods for policy search,Pourchot;Sigaud,alois.pourchot@telecom-paristech.fr;olivier.sigaud@upmc.fr,6;7;7,3;5;4,Accept (Poster),0,7,0.0,yes,9/27/18,"Télécom ParisTech;Computer Science Lab  - Pierre and Marie Curie University, Paris, France",478;478,188;123,
1827,1827,1827,1827,1827,1827,1827,1827,ICLR,2019,Multiple-Attribute Text Rewriting,Guillaume Lample;Sandeep Subramanian;Eric Smith;Ludovic Denoyer;Marc'Aurelio Ranzato;Y-Lan Boureau,glample@fb.com;sandeep.subramanian.1@umontreal.ca;ems@fb.com;ludovic.denoyer@lip6.fr;ranzato@fb.com;ylan@fb.com,7;6;6,3;4;3,Accept (Poster),0,11,0.0,yes,9/27/18,Facebook;University of Montreal;Facebook;LIP6;Facebook;Facebook,-1;123;-1;-1;-1;-1,-1;108;-1;-1;-1;-1,4;7
1828,1828,1828,1828,1828,1828,1828,1828,ICLR,2019,Improving Generalization and Stability of Generative Adversarial Networks,Hoang Thanh-Tung;Truyen Tran;Svetha Venkatesh,hoangtha@deakin.edu.au;truyen.tran@deakin.edu.au;svetha.venkatesh@deakin.edu.au,6;7;7,4;3;3,Accept (Poster),0,14,0.0,yes,9/27/18,Deakin University;Deakin University;Deakin University,478;478;478,334;334;334,5;4;8
1829,1829,1829,1829,1829,1829,1829,1829,ICLR,2019,Sliced Wasserstein Auto-Encoders,Soheil Kolouri;Phillip E. Pope;Charles E. Martin;Gustavo K. Rohde,skolouri@hrl.com;pepope@hrl.com;cemartin@hrl.com;gustavo@virginia.edu,6;4;6,4;4;4,Accept (Poster),0,8,0.0,yes,9/27/18,HRL Labs;HRL Labs;HRL Labs;University of Virginia,-1;-1;-1;65,-1;-1;-1;113,5;4
1830,1830,1830,1830,1830,1830,1830,1830,ICLR,2019,The Laplacian in RL: Learning Representations with Efficient Approximations,Yifan Wu;George Tucker;Ofir Nachum,yw4@andrew.cmu.edu;gjt@google.com;ofirnachum@google.com,7;7;7,3;3;4,Accept (Poster),0,3,0.0,yes,9/27/18,Carnegie Mellon University;Google;Google,1;-1;-1,24;-1;-1,10
1831,1831,1831,1831,1831,1831,1831,1831,ICLR,2019,Sparse Dictionary Learning by Dynamical Neural Networks,Tsung-Han Lin;Ping Tak Peter Tang,tsung-han.lin@intel.com;peter.tang@intel.com,6;9;8,4;4;4,Accept (Poster),0,3,0.0,yes,9/27/18,Intel;Intel,-1;-1,-1;-1,
1832,1832,1832,1832,1832,1832,1832,1832,ICLR,2019,Learning Programmatically Structured Representations with Perceptor Gradients,Svetlin Penkov;Subramanian Ramamoorthy,sv.penkov@gmail.com;s.ramamoorthy@ed.ac.uk,7;5;6,5;3;1,Accept (Poster),0,8,0.0,yes,9/27/18,University of Edinburgh;University of Edinburgh,33;33,27;27,
1833,1833,1833,1833,1833,1833,1833,1833,ICLR,2019,Adversarial Audio Synthesis,Chris Donahue;Julian McAuley;Miller Puckette,cdonahue@ucsd.edu;jmcauley@eng.ucsd.edu;msp@ucsd.edu,6;5;6,3;4;4,Accept (Poster),0,7,0.0,yes,9/27/18,"University of California, San Diego;University of California, San Diego;University of California, San Diego",11;11;11,31;31;31,5;4
1834,1834,1834,1834,1834,1834,1834,1834,ICLR,2019,PATE-GAN: Generating Synthetic Data with Differential Privacy Guarantees,James Jordon;Jinsung Yoon;Mihaela van der Schaar,james.jordon@wolfson.ox.ac.uk;jsyoon0823@gmail.com;mihaela.vanderschaar@eng.ox.ac.uk,6;7;7,4;4;3,Accept (Poster),0,11,1.0,yes,9/27/18,"University of Oxford;University of California, Los Angeles;University of Oxford",50;20;50,1;15;1,5;4;1
1835,1835,1835,1835,1835,1835,1835,1835,ICLR,2019,Caveats for information bottleneck in deterministic scenarios,Artemy Kolchinsky;Brendan D. Tracey;Steven Van Kuyk,artemyk@gmail.com;tracey.brendan@gmail.com;steven.jvk@gmail.com,2;8;6,4;4;4,Accept (Poster),0,18,0.0,yes,9/27/18,Santa Fe Institute;;,-1;-1;-1,-1;-1;-1,
1836,1836,1836,1836,1836,1836,1836,1836,ICLR,2019,Unsupervised Adversarial Image Reconstruction,Arthur Pajot;Emmanuel de Bezenac;Patrick Gallinari,arthur.pajot@lip6.fr;emmanuel.de-bezenac@lip6.fr;patrick.gallinari@lip6.fr,6;8;4,3;4;3,Accept (Poster),0,7,0.0,yes,9/27/18,LIP6;LIP6;LIP6,-1;-1;-1,-1;-1;-1,5;4
1837,1837,1837,1837,1837,1837,1837,1837,ICLR,2019,LEARNING TO PROPAGATE LABELS: TRANSDUCTIVE PROPAGATION NETWORK FOR FEW-SHOT LEARNING,Yanbin Liu;Juho Lee;Minseop Park;Saehoon Kim;Eunho Yang;Sung Ju Hwang;Yi Yang,csyanbin@gmail.com;juho.lee@stats.ox.ac.uk;mike_seop@aitrics.com;shkim@aitrics.com;eunhoy@kaist.ac.kr;sjhwang82@kaist.ac.kr;yi.yang@uts.edu.au,5;6;7,3;3;4,Accept (Poster),7,12,0.0,yes,9/27/18,University of Technology Sydney;University of Oxford;AITRICS;AITRICS;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;University of Technology Sydney,106;50;-1;-1;20;20;106,216;1;-1;-1;95;95;216,6;10
1838,1838,1838,1838,1838,1838,1838,1838,ICLR,2019,Approximating CNNs with Bag-of-local-Features models works surprisingly well on ImageNet,Wieland Brendel;Matthias Bethge,wieland.brendel@bethgelab.org;matthias.bethge@uni-tuebingen.de,6;7;7,4;4;4,Accept (Poster),2,19,2.0,yes,9/27/18,"Centre for Integrative Neuroscience, AG Bethge;University of Tuebingen",153;153,94;94,
1839,1839,1839,1839,1839,1839,1839,1839,ICLR,2019,"Attention, Learn to Solve Routing Problems!",Wouter Kool;Herke van Hoof;Max Welling,w.w.m.kool@uva.nl;h.c.vanhoof@uva.nl;m.welling@uva.nl,7;6;7,5;5;5,Accept (Poster),0,5,0.0,yes,9/27/18,University of Amsterdam;University of Amsterdam;University of Amsterdam,169;169;169,59;59;59,
1840,1840,1840,1840,1840,1840,1840,1840,ICLR,2019,Minimal Images in Deep Neural Networks: Fragile Object Recognition in Natural Images,Sanjana Srivastava;Guy Ben-Yosef;Xavier Boix,sanjanas@mit.edu;gby@csail.mit.edu;xboix@mit.edu,7;7;6,4;4;4,Accept (Poster),0,5,0.0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2,5;5;5,4
1841,1841,1841,1841,1841,1841,1841,1841,ICLR,2019,Detecting Egregious Responses in Neural Sequence-to-sequence Models,Tianxing He;James Glass,tianxing@mit.edu;glass@mit.edu,7;7;8,4;3;2,Accept (Poster),0,8,0.0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2,5;5,4
1842,1842,1842,1842,1842,1842,1842,1842,ICLR,2019,Analyzing Inverse Problems with Invertible Neural Networks,Lynton Ardizzone;Jakob Kruse;Carsten Rother;Ullrich Köthe,lynton.ardizzone@iwr.uni-heidelberg.de;jakob.kruse@iwr.uni-heidelberg.de;carsten.rother@iwr.uni-heidelberg.de;ullrich.koethe@iwr.uni-heidelberg.de,7;7;6,5;2;3,Accept (Poster),2,16,0.0,yes,9/27/18,Heidelberg University;Heidelberg University;Heidelberg University;Heidelberg University,199;199;199;199,45;45;45;45,1
1843,1843,1843,1843,1843,1843,1843,1843,ICLR,2019,Initialized Equilibrium Propagation for Backprop-Free Training,Peter O'Connor;Efstratios Gavves;Max Welling,peter.ed.oconnor@gmail.com;egavves@uva.nl;m.welling@uva.nl,7;5;8,5;4;5,Accept (Poster),0,10,1.0,yes,9/27/18,;University of Amsterdam;University of Amsterdam,-1;169;169,-1;59;59,
1844,1844,1844,1844,1844,1844,1844,1844,ICLR,2019,L2-Nonexpansive Neural Networks,Haifeng Qian;Mark N. Wegman,qianhaifeng@us.ibm.com;wegman@us.ibm.com,8;5;6,4;3;4,Accept (Poster),13,49,0.0,yes,9/27/18,International Business Machines;International Business Machines,-1;-1,-1;-1,4;8
1845,1845,1845,1845,1845,1845,1845,1845,ICLR,2019,KnockoffGAN: Generating Knockoffs for Feature Selection using Generative Adversarial Networks,James Jordon;Jinsung Yoon;Mihaela van der Schaar,james.jordon@wolfson.ox.ac.uk;jsyoon0823@gmail.com;mihaela.vanderschaar@eng.ox.ac.uk,6;10;7,4;4;4,Accept (Oral),0,4,1.0,yes,9/27/18,"University of Oxford;University of California, Los Angeles;University of Oxford",50;20;50,1;15;1,5;4
1846,1846,1846,1846,1846,1846,1846,1846,ICLR,2019,INVASE: Instance-wise Variable Selection using Neural Networks,Jinsung Yoon;James Jordon;Mihaela van der Schaar,jsyoon0823@gmail.com;james.jordon@wolfson.ox.ac.uk;mihaela.vanderschaar@eng.ox.ac.uk,6;6;6,3;4;3,Accept (Poster),0,6,0.0,yes,9/27/18,"University of California, Los Angeles;University of Oxford;University of Oxford",20;50;50,15;1;1,
1847,1847,1847,1847,1847,1847,1847,1847,ICLR,2019,Gradient Descent Provably Optimizes Over-parameterized Neural Networks,Simon S. Du;Xiyu Zhai;Barnabas Poczos;Aarti Singh,ssdu@cs.cmu.edu;xiyuzhai@mit.edu;bapoczos@cs.cmu.edu;aartisingh@cmu.edu,8;8;3;7,4;4;5;4,Accept (Poster),18,18,1.0,yes,9/27/18,Carnegie Mellon University;Massachusetts Institute of Technology;Carnegie Mellon University;Carnegie Mellon University,1;2;1;1,24;5;24;24,1;9
1848,1848,1848,1848,1848,1848,1848,1848,ICLR,2019,Phase-Aware Speech Enhancement with Deep Complex U-Net,Hyeong-Seok Choi;Jang-Hyun Kim;Jaesung Huh;Adrian Kim;Jung-Woo Ha;Kyogu Lee,kekepa15@snu.ac.kr;blue378@snu.ac.kr;jaesung.huh@navercorp.com;adrian.kim@navercorp.com;jungwoo.ha@navercorp.com;kglee@snu.ac.kr,6;7;7,4;4;4,Accept (Poster),0,5,2.0,yes,9/27/18,Seoul National University;Seoul National University;NAVER;NAVER;NAVER;Seoul National University,41;41;-1;-1;-1;41,74;74;-1;-1;-1;74,
1849,1849,1849,1849,1849,1849,1849,1849,ICLR,2019,Context-adaptive Entropy Model for End-to-end Optimized Image Compression,Jooyoung Lee;Seunghyun Cho;Seung-Kwon Beack,leejy1003@etri.re.kr;shcho@etri.re.kr;skbeack@etri.re.kr,7;7;6,5;4;3,Accept (Poster),3,9,0.0,yes,9/27/18,Electronics and Telecommunications Research Institute;Electronics and Telecommunications Research Institute;Electronics and Telecommunications Research Institute,-1;-1;-1,-1;-1;-1,
1850,1850,1850,1850,1850,1850,1850,1850,ICLR,2019,Beyond Pixel Norm-Balls: Parametric Adversaries using an Analytically Differentiable Renderer,Hsueh-Ti Derek Liu;Michael Tao;Chun-Liang Li;Derek Nowrouzezahrai;Alec Jacobson,hsuehtil@cs.toronto.edu;mtao@dgp.toronto.edu;chunlial@cs.cmu.edu;derek@cim.mcgill.ca;jacobson@cs.toronto.edu,7;7;6,4;4;3,Accept (Poster),0,8,0.0,yes,9/27/18,"Department of Computer Science, University of Toronto;University of Toronto;Carnegie Mellon University;McGill University;Department of Computer Science, University of Toronto",18;18;1;85;18,22;22;24;42;22,4
1851,1851,1851,1851,1851,1851,1851,1851,ICLR,2019,Multilingual Neural Machine Translation With Soft Decoupled Encoding,Xinyi Wang;Hieu Pham;Philip Arthur;Graham Neubig,xinyiw1@cs.cmu.edu;hyhieu@cmu.edu;philip.arthur@monash.edu;gneubig@cs.cmu.edu,6;7;6,5;4;4,Accept (Poster),0,8,0.0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;Monash University;Carnegie Mellon University,1;1;123;1,24;24;80;24,3
1852,1852,1852,1852,1852,1852,1852,1852,ICLR,2019,Unsupervised Speech Recognition via Segmental Empirical Output Distribution Matching,Chih-Kuan Yeh;Jianshu Chen;Chengzhu Yu;Dong Yu,cjyeh@cs.cmu.edu;chenjianshu@gmail.com;czyu@tencent.com;dyu@tencent.com,7;7;7,4;4;4,Accept (Poster),0,5,0.0,yes,9/27/18,Carnegie Mellon University;Tencent AI Lab;Tencent AI Lab;Tencent AI Lab,1;-1;-1;-1,24;-1;-1;-1,3;2
1853,1853,1853,1853,1853,1853,1853,1853,ICLR,2019,Understanding Composition of Word Embeddings via Tensor Decomposition,Abraham Frandsen;Rong Ge,abef@cs.duke.edu;rongge@cs.duke.edu,7;6;6,2;4;3,Accept (Poster),0,4,0.0,yes,9/27/18,Duke University;Duke University,44;44,17;17,3;5;1
1854,1854,1854,1854,1854,1854,1854,1854,ICLR,2019,Neural Message Passing for Multi-Label Classification,Jack Lanchantin;Arshdeep Sekhon;Yanjun Qi,jjl5sw@virginia.edu;as5cu@virginia.edu;yq2h@virginia.edu,4;6;5,4;4;2,Reject,0,17,0.0,yes,9/27/18,University of Virginia;University of Virginia;University of Virginia,65;65;65,113;113;113,
1855,1855,1855,1855,1855,1855,1855,1855,ICLR,2019,ARM: Augment-REINFORCE-Merge Gradient for Stochastic Binary Networks,Mingzhang Yin;Mingyuan Zhou,mzyin@utexas.edu;mingyuan.zhou@mccombs.utexas.edu,8;6;7,4;4;3,Accept (Poster),0,10,1.0,yes,9/27/18,"University of Texas, Austin;University of Texas, Austin",22;22,49;49,
1856,1856,1856,1856,1856,1856,1856,1856,ICLR,2019,An Empirical Study of Example Forgetting during Deep Neural Network Learning,Mariya Toneva*;Alessandro Sordoni*;Remi Tachet des Combes*;Adam Trischler;Yoshua Bengio;Geoffrey J. Gordon,mariya.k.toneva@gmail.com;alsordon@microsoft.com;retachet@microsoft.com;adtrisch@microsoft.com;yoshua.bengio@mila.quebec;geoff.gordon@microsoft.com,7;8;9,4;4;5,Accept (Poster),0,9,1.0,yes,9/27/18,Carnegie Mellon University;Microsoft;Microsoft;Microsoft;University of Montreal;Microsoft,1;-1;-1;-1;123;-1,24;-1;-1;-1;108;-1,8
1857,1857,1857,1857,1857,1857,1857,1857,ICLR,2019,Distribution-Interpolation Trade off in Generative Models,Damian Leśniak;Igor Sieradzki;Igor Podolak,damian.lesniak@doctoral.uj.edu.pl;igor.sieradzki@doctoral.uj.edu.pl;igor.podolak@uj.edu.pl,6;5;7,4;4;3,Accept (Poster),0,9,0.0,yes,9/27/18,Jagiellonian University;Jagiellonian University;Jagiellonian University,478;478;478,695;695;695,5;1
1858,1858,1858,1858,1858,1858,1858,1858,ICLR,2019,Practical lossless compression with latent variables using bits back coding,James Townsend;Thomas Bird;David Barber,james.townsend@cs.ucl.ac.uk;thomas.bird@cs.ucl.ac.uk;david.barber@ucl.ac.uk,6;6;8,4;3;5,Accept (Poster),0,7,1.0,yes,9/27/18,University College London;University College London;University College London,50;50;50,16;16;16,5
1859,1859,1859,1859,1859,1859,1859,1859,ICLR,2019,Double Viterbi: Weight Encoding for High Compression Ratio and Fast On-Chip Reconstruction for Deep Neural Network,Daehyun Ahn;Dongsoo Lee;Taesu Kim;Jae-Joon Kim,daehyun.ahn@postech.ac.kr;dslee3@gmail.com;taesukim@postech.ac.kr;jaejoon@postech.ac.kr,6;7;6,3;2;4,Accept (Poster),0,13,0.0,yes,9/27/18,POSTECH;Samsung;POSTECH;POSTECH,123;-1;123;123,137;-1;137;137,
1860,1860,1860,1860,1860,1860,1860,1860,ICLR,2019,Zero-training Sentence Embedding via Orthogonal Basis,Ziyi Yang;Chenguang Zhu;Weizhu Chen,ziyi.yang@stanford.edu;chezhu@microsoft.com;wzchen@microsoft.com,5;4;5,4;4;4,Reject,6,6,0.0,yes,9/27/18,Stanford University;Microsoft;Microsoft,4;-1;-1,3;-1;-1,3
1861,1861,1861,1861,1861,1861,1861,1861,ICLR,2019,Guiding Policies with Language via Meta-Learning,John D. Co-Reyes;Abhishek Gupta;Suvansh Sanjeev;Nick Altieri;Jacob Andreas;John DeNero;Pieter Abbeel;Sergey Levine,jcoreyes@eecs.berkeley.edu;abhigupta@berkeley.edu;suvansh@berkeley.edu;naltieri@berkeley.edu;j.d.andreas@gmail.com;denero@berkeley.edu;pabbeel@cs.berkeley.edu;svlevine@eecs.berkeley.edu,6;6;6,4;4;3,Accept (Poster),0,7,0.0,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;Microsoft;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5;-1;5;5;5,18;18;18;18;-1;18;18;18,3;6
1862,1862,1862,1862,1862,1862,1862,1862,ICLR,2019,Towards the first adversarially robust neural network model on MNIST,Lukas Schott;Jonas Rauber;Matthias Bethge;Wieland Brendel,lukas.schott@bethgelab.org;jonas.rauber@bethgelab.org;matthias.bethge@bethgelab.org;wieland.brendel@bethgelab.org,7;7;6,4;3;3,Accept (Poster),7,18,1.0,yes,9/27/18,"Centre for Integrative Neuroscience, AG Bethge;Centre for Integrative Neuroscience, AG Bethge;Centre for Integrative Neuroscience, AG Bethge;Centre for Integrative Neuroscience, AG Bethge",153;153;153;153,94;94;94;94,4;2
1863,1863,1863,1863,1863,1863,1863,1863,ICLR,2019,Neural network gradient-based learning of black-box function interfaces,Alon Jacovi;Guy Hadash;Einat Kermany;Boaz Carmeli;Ofer Lavi;George Kour;Jonathan Berant,alon.jacovi@il.ibm.com;guyh@il.ibm.com;einatke@il.ibm.com;boazc@il.ibm.com;oferl@il.ibm.com;gkour@ibm.com;joberant@cs.tau.ac.il,7;7;7,3;3;3,Accept (Poster),0,5,0.0,yes,9/27/18,International Business Machines;International Business Machines;International Business Machines;International Business Machines;International Business Machines;International Business Machines;Tel Aviv University,-1;-1;-1;-1;-1;-1;37,-1;-1;-1;-1;-1;-1;217,
1864,1864,1864,1864,1864,1864,1864,1864,ICLR,2019,Learning Multimodal Graph-to-Graph Translation for Molecule Optimization,Wengong Jin;Kevin Yang;Regina Barzilay;Tommi Jaakkola,wengong@csail.mit.edu;yangk@mit.edu;regina@csail.mit.edu;tommi@csail.mit.edu,7;7;6,5;4;4,Accept (Poster),0,9,0.0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2,5;5;5;5,4;10
1865,1865,1865,1865,1865,1865,1865,1865,ICLR,2019,Lagging Inference Networks and Posterior Collapse in Variational Autoencoders,Junxian He;Daniel Spokoyny;Graham Neubig;Taylor Berg-Kirkpatrick,junxianh@cs.cmu.edu;dspokoyn@cs.cmu.edu;gneubig@cs.cmu.edu;tberg@cs.cmu.edu,8;7;8,4;4;4,Accept (Poster),3,12,2.0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1,24;24;24;24,5;1
1866,1866,1866,1866,1866,1866,1866,1866,ICLR,2019,Stable Opponent Shaping in Differentiable Games,Alistair Letcher;Jakob Foerster;David Balduzzi;Tim Rocktäschel;Shimon Whiteson,ahp.letcher@gmail.com;jakobfoerster@gmail.com;dbalduzzi@google.com;tim.rocktaeschel@gmail.com;shimon.whiteson@cs.ox.ac.uk,8;6;6,4;2;1,Accept (Poster),0,7,0.0,yes,9/27/18,University of Oxford;University of Oxford;Google;Facebook AI Research;University of Oxford,50;50;-1;-1;50,1;1;-1;-1;1,5;1
1867,1867,1867,1867,1867,1867,1867,1867,ICLR,2019,DELTA: DEEP LEARNING TRANSFER USING FEATURE MAP WITH ATTENTION FOR CONVOLUTIONAL NETWORKS,Xingjian Li;Haoyi Xiong;Hanchao Wang;Yuxuan Rao;Liping Liu;Jun Huan,1762778193@qq.com;xhyccc@gmail.com;wanghanchao01@baidu.com;yrao4@illinois.edu;liuliping@baidu.com;huanjun@baidu.com,6;7;6,4;3;4,Accept (Poster),0,9,0.0,yes,9/27/18,";Baidu;Baidu;University of Illinois, Urbana Champaign;Baidu;Baidu",-1;-1;-1;3;-1;-1,-1;-1;-1;37;-1;-1,6
1868,1868,1868,1868,1868,1868,1868,1868,ICLR,2019,CAMOU: Learning Physical Vehicle Camouflages to Adversarially Attack Detectors in the Wild,Yang Zhang;Hassan Foroosh;Philip David;Boqing Gong,yangzhang4065@gmail.com;foroosh@cs.ucf.edu;philip.j.david4.civ@mail.mil;boqinggo@outlook.com,4;8;7,3;4;3,Accept (Poster),0,4,4.0,yes,9/27/18,University of Central Florida;University of Central Florida;Army Reserach laboratory;International Computer Science Institute,78;78;-1;-1,1103;1103;-1;-1,4
1869,1869,1869,1869,1869,1869,1869,1869,ICLR,2019,Visual Reasoning by Progressive Module Networks,Seung Wook Kim;Makarand Tapaswi;Sanja Fidler,seung@cs.toronto.edu;makarand@cs.toronto.edu;fidler@cs.toronto.edu,6;6;7,4;4;5,Accept (Poster),0,12,0.0,yes,9/27/18,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",18;18;18,22;22;22,
1870,1870,1870,1870,1870,1870,1870,1870,ICLR,2019,Recurrent Experience Replay in Distributed Reinforcement Learning,Steven Kapturowski;Georg Ostrovski;John Quan;Remi Munos;Will Dabney,skapturowski@google.com;ostrovski@google.com;johnquan@google.com;munos@google.com;wdabney@google.com,7;8;7,2;4;3,Accept (Poster),7,11,0.0,yes,9/27/18,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,
1871,1871,1871,1871,1871,1871,1871,1871,ICLR,2019,Dynamic Sparse Graph for Efficient Deep Learning,Liu Liu;Lei Deng;Xing Hu;Maohua Zhu;Guoqi Li;Yufei Ding;Yuan Xie,liu_liu@ucsb.edu;leideng@ucsb.edu;huxing@ece.ucsb.edu;maohuazhu@ucsb.edu;liguoqi@mail.tsinghua.edu.cn;yufeiding@cs.ucsb.edu;yuanxie@ucsb.edu,7;8;7,2;3;4,Accept (Poster),0,4,0.0,yes,9/27/18,UC Santa Barbara;UC Santa Barbara;UC Santa Barbara;UC Santa Barbara;Tsinghua University;UC Santa Barbara;UC Santa Barbara,37;37;37;37;8;37;37,53;53;53;53;30;53;53,10
1872,1872,1872,1872,1872,1872,1872,1872,ICLR,2019,Learning Recurrent Binary/Ternary Weights,Arash Ardakani;Zhengyun Ji;Sean C. Smithson;Brett H. Meyer;Warren J. Gross,arash.ardakani@mail.mcgill.ca;zhengyun.ji@mail.mcgill.ca;sean.smithson@mail.mcgill.ca;brett.meyer@mcgill.ca;warren.gross@mcgill.ca,7;6;8,4;3;3,Accept (Poster),0,11,0.0,yes,9/27/18,McGill University;McGill University;McGill University;McGill University;McGill University,85;85;85;85;85,42;42;42;42;42,3
1873,1873,1873,1873,1873,1873,1873,1873,ICLR,2019,Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning,Ying Wen;Yaodong Yang;Rui Luo;Jun Wang;Wei Pan,ying.wen@cs.ucl.ac.uk;yaodong.yang@cs.ucl.ac.uk;rui.luo@cs.ucl.ac.uk;jun.wang@cs.ucl.ac.uk;wei.pan@tudelft.nl,7;8;7,4;3;4,Accept (Poster),0,10,0.0,yes,9/27/18,University College London;University College London;University College London;University College London;Delft University of Technology,50;50;50;50;89,16;16;16;16;63,
1874,1874,1874,1874,1874,1874,1874,1874,ICLR,2019,"Slalom: Fast, Verifiable and Private Execution of Neural Networks in Trusted Hardware",Florian Tramer;Dan Boneh,tramer@cs.stanford.edu;dabo@cs.stanford.edu,7;7;9,3;2;4,Accept (Oral),0,4,10.0,yes,9/27/18,Stanford University;Stanford University,4;4,3;3,
1875,1875,1875,1875,1875,1875,1875,1875,ICLR,2019,"A comprehensive, application-oriented study of catastrophic forgetting in DNNs",B. Pfülb;A. Gepperth,benedikt.pfuelb@cs.hs-fulda.de;alexander.gepperth@cs.hs-fulda.de,6;7;5,5;3;4,Accept (Poster),0,1,0.0,yes,9/27/18,HS Fulda;HS Fulda,-1;-1,-1;-1,
1876,1876,1876,1876,1876,1876,1876,1876,ICLR,2019,Stable Recurrent Models,John Miller;Moritz Hardt,miller_john@berkeley.edu;hardt@berkeley.edu,6;7;6,4;2;4,Accept (Poster),0,10,2.0,yes,9/27/18,University of California Berkeley;University of California Berkeley,5;5,18;18,1
1877,1877,1877,1877,1877,1877,1877,1877,ICLR,2019,Large Scale Graph Learning From Smooth Signals,Vassilis Kalofolias;Nathanaël Perraudin,v.kalofolias@gmail.com;nathanael.perraudin@sdsc.ethz.ch,7;5;7,5;3;4,Accept (Poster),0,7,0.0,yes,9/27/18,;Swiss Federal Institute of Technology,-1;10,-1;10,10
1878,1878,1878,1878,1878,1878,1878,1878,ICLR,2019,Accumulation Bit-Width Scaling For Ultra-Low Precision Training Of Deep Networks,Charbel Sakr;Naigang Wang;Chia-Yu Chen;Jungwook Choi;Ankur Agrawal;Naresh Shanbhag;Kailash Gopalakrishnan,sakr2@illinois.edu;nwang@us.ibm.com;cchen@us.ibm.com;choij@us.ibm.com;ankuragr@us.ibm.com;shanbhag@illinois.edu;kailash@us.ibm.com,7;6;6,4;4;3,Accept (Poster),0,7,0.0,yes,9/27/18,"University of Illinois, Urbana Champaign;International Business Machines;International Business Machines;International Business Machines;International Business Machines;University of Illinois, Urbana Champaign;International Business Machines",3;-1;-1;-1;-1;3;-1,37;-1;-1;-1;-1;37;-1,
1879,1879,1879,1879,1879,1879,1879,1879,ICLR,2019,Learning to Remember More with Less Memorization,Hung Le;Truyen Tran;Svetha Venkatesh,lethai@deakin.edu.au;truyen.tran@deakin.edu.au;svetha.venkatesh@deakin.edu.au,7;7;8,3;4;4,Accept (Oral),0,5,3.0,yes,9/27/18,Deakin University;Deakin University;Deakin University,478;478;478,334;334;334,1
1880,1880,1880,1880,1880,1880,1880,1880,ICLR,2019,Bayesian Prediction of Future Street Scenes using Synthetic Likelihoods,Apratim Bhattacharyya;Mario Fritz;Bernt Schiele,abhattac@mpi-inf.mpg.de;mfritz@mpi-inf.mpg.de;schiele@mpi-inf.mpg.de,6;8;6,4;4;2,Accept (Poster),0,8,1.0,yes,9/27/18,"Saarland Informatics Campus, Max-Planck Institute;Saarland Informatics Campus, Max-Planck Institute;Saarland Informatics Campus, Max-Planck Institute",-1;-1;-1,-1;-1;-1,11
1881,1881,1881,1881,1881,1881,1881,1881,ICLR,2019,Label super-resolution networks,Kolya Malkin;Caleb Robinson;Le Hou;Rachel Soobitsky;Jacob Czawlytko;Dimitris Samaras;Joel Saltz;Lucas Joppa;Nebojsa Jojic,kolya_malkin@hotmail.com;dcrobins@gatech.edu;le.hou@stonybrook.edu;rsoobitsky@chesapeakeconservancy.org;jczawlytko@chesapeakeconservancy.org;samaras@cs.stonybrook.edu;joel.saltz@stonybrookmedicine.edu;lujoppa@microsoft.com;jojic@microsoft.com,7;6;9,4;4;4,Accept (Poster),0,5,0.0,yes,9/27/18,"Yale University;Georgia Institute of Technology;State University of New York, Stony Brook;;;State University of New York, Stony Brook;Renaissance School of Medicine at Stony Brook University;Microsoft;Microsoft",62;13;41;-1;-1;41;41;-1;-1,12;33;258;-1;-1;258;258;-1;-1,2
1882,1882,1882,1882,1882,1882,1882,1882,ICLR,2019,Variational Smoothing in Recurrent Neural Network Language Models,Lingpeng Kong;Gabor Melis;Wang Ling;Lei Yu;Dani Yogatama,lingpenk@cs.cmu.edu;melisgl@google.com;lingwang@google.com;leiyu@google.com;dyogatama@google.com,7;6;2,4;4;5,Accept (Poster),0,3,0.0,yes,9/27/18,Carnegie Mellon University;Google;Google;Google;Google,1;-1;-1;-1;-1,24;-1;-1;-1;-1,3;11
1883,1883,1883,1883,1883,1883,1883,1883,ICLR,2019,Hierarchical interpretations for neural network predictions,Chandan Singh;W. James Murdoch;Bin Yu,chandan_singh@berkeley.edu;jmurdoch@berkeley.edu;binyu@berkeley.edu,6;6;7,4;4;3,Accept (Poster),0,10,0.0,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,18;18;18,4
1884,1884,1884,1884,1884,1884,1884,1884,ICLR,2019,h-detach: Modifying the LSTM Gradient Towards Better Optimization,Bhargav Kanuparthi;Devansh Arpit;Giancarlo Kerg;Nan Rosemary Ke;Ioannis Mitliagkas;Yoshua Bengio,bhargavkanuparthi25@gmail.com;devansharpit@gmail.com;giancarlo.kerg@gmail.com;rosemary.nan.ke@gmail.com;ioannis@iro.umontreal.ca;yoshua.umontreal@gmail.com,7;5;6,5;4;3,Accept (Poster),0,4,5.0,yes,9/27/18,University of Montreal;University of Montreal;University of Montreal;Polytechnique Montreal;University of Montreal;University of Montreal,123;123;123;386;123;123,108;108;108;108;108;108,10;8
1885,1885,1885,1885,1885,1885,1885,1885,ICLR,2019,Neural Probabilistic Motor Primitives for Humanoid Control,Josh Merel;Leonard Hasenclever;Alexandre Galashov;Arun Ahuja;Vu Pham;Greg Wayne;Yee Whye Teh;Nicolas Heess,jsmerel@google.com;leonardh@google.com;agalashov@google.com;arahuja@google.com;vuph@google.com;gregwayne@google.com;ywteh@google.com;heess@google.com,3;6;4,4;4;4,Accept (Poster),0,8,0.0,yes,9/27/18,Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1,
1886,1886,1886,1886,1886,1886,1886,1886,ICLR,2019,Hierarchical Visuomotor Control of Humanoids,Josh Merel;Arun Ahuja;Vu Pham;Saran Tunyasuvunakool;Siqi Liu;Dhruva Tirumala;Nicolas Heess;Greg Wayne,jsmerel@google.com;arahuja@google.com;vuph@google.com;stunya@google.com;liusiqi@google.com;dhruvat@google.com;heess@google.com;gregwayne@google.com,5;8;6,3;3;4,Accept (Poster),2,9,0.0,yes,9/27/18,Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1,
1887,1887,1887,1887,1887,1887,1887,1887,ICLR,2019,Adversarial Domain Adaptation for Stable Brain-Machine Interfaces,Ali Farshchian;Juan A. Gallego;Joseph P. Cohen;Yoshua Bengio;Lee E. Miller;Sara A. Solla,a-farshchiansadegh@northwestern.edu;juan.gallego@northwestern.edu;joseph@josephpcohen.com;yoshua.bengio@umontreal.ca;lm@northwestern.edu;solla@northwestern.edu,9;5;7,4;3;5,Accept (Poster),2,8,0.0,yes,9/27/18,Northwestern University;Northwestern University;University of Montreal;University of Montreal;Northwestern University;Northwestern University,44;44;123;123;44;44,20;20;108;108;20;20,4
1888,1888,1888,1888,1888,1888,1888,1888,ICLR,2019,Emerging Disentanglement in Auto-Encoder Based Unsupervised Image Content Transfer,Ori Press;Tomer Galanti;Sagie Benaim;Lior Wolf,theoripress@gmail.com;tomer22g@gmail.com;sagiebenaim@gmail.com;wolf@fb.com,6;6;6,2;3;1,Accept (Poster),0,6,0.0,yes,9/27/18,Tel Aviv University;Tel Aviv University;Tel Aviv University;Facebook,37;37;37;-1,217;217;217;-1,
1889,1889,1889,1889,1889,1889,1889,1889,ICLR,2019,Global-to-local Memory Pointer Networks for Task-Oriented Dialogue,Chien-Sheng Wu;Richard Socher;Caiming Xiong,jason.wu@connect.ust.hk;rsocher@salesforce.com;cxiong@salesforce.com,8;8;5,2;2;3,Accept (Poster),0,0,10.0,yes,9/27/18,The Hong Kong University of Science and Technology;SalesForce.com;SalesForce.com,39;-1;-1,44;-1;-1,
1890,1890,1890,1890,1890,1890,1890,1890,ICLR,2019,"Improving Differentiable Neural Computers Through Memory Masking, De-allocation, and Link Distribution Sharpness Control",Robert Csordas;Juergen Schmidhuber,robert@idsia.ch;juergen@idsia.ch,7;8;7,5;5;5,Accept (Poster),0,5,0.0,yes,9/27/18,IDSIA;IDSIA,-1;-1,-1;-1,
1891,1891,1891,1891,1891,1891,1891,1891,ICLR,2019,Learning Representations of Sets through Optimized Permutations,Yan Zhang;Jonathon Hare;Adam Prügel-Bennett,yz5n12@ecs.soton.ac.uk;jsh2@ecs.soton.ac.uk;apb@ecs.soton.ac.uk,3;6;6,2;4;4,Accept (Poster),0,25,0.0,yes,9/27/18,University of Southampton;University of Southampton;University of Southampton,169;169;169,126;126;126,
1892,1892,1892,1892,1892,1892,1892,1892,ICLR,2019,Kernel Change-point Detection with Auxiliary Deep Generative Models,Wei-Cheng Chang;Chun-Liang Li;Yiming Yang;Barnabás Póczos,wchang2@cs.cmu.edu;chunlial@cs.cmu.edu;yiming@cs.cmu.edu;bapoczos@cs.cmu.edu,7;8;8,4;3;4,Accept (Poster),0,5,0.0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1,24;24;24;24,5;1
1893,1893,1893,1893,1893,1893,1893,1893,ICLR,2019,Unsupervised Domain Adaptation for Distance Metric Learning,Kihyuk Sohn;Wenling Shang;Xiang Yu;Manmohan Chandraker,kihyuk.sohn@gmail.com;wendyshang1208@gmail.com;xiangyu@nec-labs.com;manu@nec-labs.com,8;5;8,5;4;4,Accept (Poster),0,5,0.0,yes,9/27/18,NEC-Labs;;NEC-Labs;NEC-Labs,-1;-1;-1;-1,-1;-1;-1;-1,7;2
1894,1894,1894,1894,1894,1894,1894,1894,ICLR,2019,Feature Intertwiner for Object Detection,Hongyang Li;Bo Dai;Shaoshuai Shi;Wanli Ouyang;Xiaogang Wang,yangli@ee.cuhk.edu.hk;db014@ie.cuhk.edu.hk;shaoss@link.cuhk.edu.hk;wanli.ouyang@gmail.com;xgwang@ee.cuhk.edu.hk,7;5;9,3;4;4,Accept (Poster),0,3,0.0,yes,9/27/18,The Chinese University of Hong Kong;The Chinese University of Hong Kong;The Chinese University of Hong Kong;University of Sydney;The Chinese University of Hong Kong,57;57;57;-1;57,40;40;40;-1;40,2
1895,1895,1895,1895,1895,1895,1895,1895,ICLR,2019,On the Universal Approximability and Complexity Bounds of Quantized ReLU Neural Networks,Yukun Ding;Jinglan Liu;Jinjun Xiong;Yiyu Shi,yding5@nd.edu;jliu16@nd.edu;jinjun@us.ibm.com;yshi4@nd.edu,7;6;8,3;3;3,Accept (Poster),0,4,0.0,yes,9/27/18,University of Notre Dame;University of Notre Dame;International Business Machines;University of Notre Dame,115;115;-1;115,150;150;-1;150,1
1896,1896,1896,1896,1896,1896,1896,1896,ICLR,2019,SNAS: stochastic neural architecture search,Sirui Xie;Hehui Zheng;Chunxiao Liu;Liang Lin,xiesirui@sensetime.com;zhenghehui@sensetime.com;liuchunxiao@sensetime.com;linliang@ieee.org,7;6;7,4;4;4,Accept (Poster),2,19,1.0,yes,9/27/18,SenseTime Group Limited;SenseTime Group Limited;SenseTime Group Limited;SUN YAT-SEN UNIVERSITY,-1;-1;-1;478,-1;-1;-1;352,1
1897,1897,1897,1897,1897,1897,1897,1897,ICLR,2019,LayoutGAN: Generating Graphic Layouts with Wireframe Discriminators,Jianan Li;Jimei Yang;Aaron Hertzmann;Jianming Zhang;Tingfa Xu,lijianan15@gmail.com;jimyang@adobe.com;hertzman@adobe.com;jianmzha@adobe.com;ciom_xtf1@bit.edu.cn,7;7;6,3;4;4,Accept (Poster),0,6,0.0,yes,9/27/18,;Adobe Systems;Adobe Systems;Adobe Systems;BIT,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,5;4;10
1898,1898,1898,1898,1898,1898,1898,1898,ICLR,2019,"Don't Settle for Average, Go for the Max: Fuzzy Sets and Max-Pooled Word Vectors",Vitalii Zhelezniak;Aleksandar Savkov;April Shen;Francesco Moramarco;Jack Flann;Nils Y. Hammerla,vitali.zhelezniak@babylonhealth.com;sasho.savkov@babylonhealth.com;april.shen@babylonhealth.com;francesco.moramarco@babylonhealth.com;jack.flann@babylonhealth.com;nils.hammerla@babylonhealth.com,8;8;5,3;4;3,Accept (Poster),3,12,1.0,yes,9/27/18,babylon health;babylon health;babylon health;babylon health;babylon health;babylon health,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,3
1899,1899,1899,1899,1899,1899,1899,1899,ICLR,2019,ADef: an Iterative Algorithm to Construct Adversarial Deformations,Rima Alaifari;Giovanni S. Alberti;Tandri Gauksson,rima.alaifari@sam.math.ethz.ch;alberti@dima.unige.it;tandrig@sam.math.ethz.ch,6;7;7,3;4;3,Accept (Poster),0,8,0.0,yes,9/27/18,Swiss Federal Institute of Technology;Università degli Studi di Genova;Swiss Federal Institute of Technology,10;-1;10,10;-1;10,4
1900,1900,1900,1900,1900,1900,1900,1900,ICLR,2019,On the Turing Completeness of Modern Neural Network Architectures,Jorge Pérez;Javier Marinković;Pablo Barceló,jperez@dcc.uchile.cl;javier.marinkovic95@gmail.com;pbarcelo@dcc.uchile.cl,6;7;7,2;2;2,Accept (Poster),2,14,0.0,yes,9/27/18,Universidad de Chile;;Universidad de Chile,314;-1;314,660;-1;660,
1901,1901,1901,1901,1901,1901,1901,1901,ICLR,2019,Spherical CNNs on Unstructured Grids,Chiyu Max Jiang;Jingwei Huang;Karthik Kashinath;Prabhat;Philip Marcus;Matthias Niessner,chiyu.jiang@berkeley.edu;jingweih@stanford.edu;kkashinath@lbl.gov;prabhat@lbl.gov;pmarcus@me.berkeley.edu;niessner@tum.de,7;6;7,3;3;5,Accept (Poster),2,3,0.0,yes,9/27/18,University of California Berkeley;Stanford University;Lawrence Berkeley National Lab;Lawrence Berkeley National Lab;University of California Berkeley;Technical University Munich,5;4;-1;-1;5;54,18;3;-1;-1;18;41,2
1902,1902,1902,1902,1902,1902,1902,1902,ICLR,2019,Convolutional Neural Networks on Non-uniform Geometrical Signals Using Euclidean Spectral Transformation,Chiyu Max Jiang;Dequan Wang;Jingwei Huang;Philip Marcus;Matthias Niessner,chiyu.jiang@berkeley.edu;dqw@berkeley.edu;jingweih@stanford.edu;pmarcus@me.berkeley.edu;niessner@tum.de,5;7;4,3;4;3,Accept (Poster),0,3,1.0,yes,9/27/18,University of California Berkeley;University of California Berkeley;Stanford University;University of California Berkeley;Technical University Munich,5;5;4;5;54,18;18;3;18;41,10
1903,1903,1903,1903,1903,1903,1903,1903,ICLR,2019,Graph Wavelet Neural Network,Bingbing Xu;Huawei Shen;Qi Cao;Yunqi Qiu;Xueqi Cheng,xubingbing@ict.ac.cn;shenhuawei@ict.ac.cn;caoqi@ict.ac.cn;qiuyunqi@ict.ac.cn;cxq@ict.ac.cn,7;7;4,5;4;4,Accept (Poster),8,17,4.0,yes,9/27/18,"Institute of Computing Technology, Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences",62;62;62;62;62,1103;1103;1103;1103;1103,10
1904,1904,1904,1904,1904,1904,1904,1904,ICLR,2019,"A Closer Look at Deep Learning Heuristics: Learning rate restarts, Warmup and Distillation",Akhilesh Gotmare;Nitish Shirish Keskar;Caiming Xiong;Richard Socher,akhilesh.gotmare@epfl.ch;nkeskar@salesforce.com;cxiong@salesforce.com;rsocher@salesforce.com,4;7;6,4;5;4,Accept (Poster),0,5,0.0,yes,9/27/18,Swiss Federal Institute of Technology Lausanne;SalesForce.com;SalesForce.com;SalesForce.com,478;-1;-1;-1,38;-1;-1;-1,9
1905,1905,1905,1905,1905,1905,1905,1905,ICLR,2019,Revealing interpretable object representations from human behavior,Charles Y. Zheng;Francisco Pereira;Chris I. Baker;Martin N. Hebart,charles.zheng@nih.gov;francisco.pereira@nih.gov;bakerchris@mail.nih.gov;martin.hebart@nih.gov,5;7;7,4;4;4,Accept (Poster),0,6,0.0,yes,9/27/18,National Institutes of Health;National Institutes of Health;National Institutes of Health;National Institutes of Health,-1;-1;-1;-1,-1;-1;-1;-1,
1906,1906,1906,1906,1906,1906,1906,1906,ICLR,2019,Near-Optimal Representation Learning for Hierarchical Reinforcement Learning,Ofir Nachum;Shixiang Gu;Honglak Lee;Sergey Levine,ofirnachum@google.com;shanegu@google.com;honglak@google.com;svlevine@eecs.berkeley.edu,8;7;9,3;5;5,Accept (Poster),1,11,0.0,yes,9/27/18,Google;Google;Google;University of California Berkeley,-1;-1;-1;5,-1;-1;-1;18,1
1907,1907,1907,1907,1907,1907,1907,1907,ICLR,2019,Quaternion Recurrent Neural Networks,Titouan Parcollet;Mirco Ravanelli;Mohamed Morchid;Georges Linarès;Chiheb Trabelsi;Renato De Mori;Yoshua Bengio,titouan.parcollet@alumni.univ-avignon.fr;mirco.ravanelli@gmail.com;mohamed.morchid@univ-avignon.fr;georges.linares@univ-avignon.fr;chiheb.trabelsi@polymtl.ca;rdemori@cs.mcgill.ca;yoshua.bengio@mila.quebec,7;7;8,5;5;4,Accept (Poster),0,8,0.0,yes,9/27/18,University of Avignon;University of Montreal;University of Avignon;University of Avignon;Polytechnique Montreal;McGill University;University of Montreal,199;123;199;199;386;85;123,267;108;267;267;108;42;108,
1908,1908,1908,1908,1908,1908,1908,1908,ICLR,2019,Efficient Lifelong Learning with A-GEM,Arslan Chaudhry;Marc’Aurelio Ranzato;Marcus Rohrbach;Mohamed Elhoseiny,arslan.chaudhry@eng.ox.ac.uk;ranzato@fb.com;mrf@fb.com;elhoseiny@fb.com,7;7;6,4;4;4,Accept (Poster),0,0,7.0,yes,9/27/18,University of Oxford;Facebook;Facebook;Facebook,50;-1;-1;-1,1;-1;-1;-1,
1909,1909,1909,1909,1909,1909,1909,1909,ICLR,2019,Quasi-hyperbolic momentum and Adam for deep learning,Jerry Ma;Denis Yarats,maj@fb.com;denisy@fb.com,6;7;8,4;4;3,Accept (Poster),0,18,1.0,yes,9/27/18,Facebook;Facebook,-1;-1,-1;-1,
1910,1910,1910,1910,1910,1910,1910,1910,ICLR,2019,AdaShift: Decorrelation and Convergence of Adaptive Learning Rate Methods,Zhiming Zhou*;Qingru Zhang*;Guansong Lu;Hongwei Wang;Weinan Zhang;Yong Yu,heyohai@apex.sjtu.edu.cn;neverquit@sjtu.edu.cn;gslu@apex.sjtu.edu.cn;wanghongwei55@gmail.com;wnzhang@sjtu.edu.cn;yyu@apex.sjtu.edu.cn,6;6;9,4;4;4,Accept (Poster),3,5,5.0,yes,9/27/18,Shanghai Jiao Tong University;Shanghai Jiao Tong University;Shanghai Jiao Tong University;Shanghai Jiao Tong University;Shanghai Jiao Tong University;Shanghai Jiao Tong University,52;52;52;52;52;52,188;188;188;188;188;188,1;8
1911,1911,1911,1911,1911,1911,1911,1911,ICLR,2019,Diversity is All You Need: Learning Skills without a Reward Function,Benjamin Eysenbach;Abhishek Gupta;Julian Ibarz;Sergey Levine,beysenba@cs.cmu.edu;abhigupta@berkeley.edu;julianibarz@google.com;svlevine@eecs.berkeley.edu,8;7;7,4;3;4,Accept (Poster),0,3,0.0,yes,9/27/18,Carnegie Mellon University;University of California Berkeley;Google;University of California Berkeley,1;5;-1;5,24;18;-1;18,
1912,1912,1912,1912,1912,1912,1912,1912,ICLR,2019,MAE: Mutual Posterior-Divergence Regularization for Variational AutoEncoders,Xuezhe Ma;Chunting Zhou;Eduard Hovy,xuezhem@cs.cmu.edu;ctzhou@cs.cmu.edu;ehovy@cs.cmu.edu,7;6;6,5;4;4,Accept (Poster),2,7,1.0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1,24;24;24,5
1913,1913,1913,1913,1913,1913,1913,1913,ICLR,2019,Learning a SAT Solver from Single-Bit Supervision,"Daniel Selsam;Matthew Lamm;Benedikt B\{u}nz;Percy Liang;Leonardo de Moura;David L. Dill""",dselsam@cs.stanford.edu;mlamm@cs.stanford.edu;buenz@cs.stanford.edu;pliang@cs.stanford.edu;leonardo@microsoft.com;dill@cs.stanford.edu,7;7;7,3;4;3,Accept (Poster),1,14,0.0,yes,9/27/18,Stanford University;Stanford University;Stanford University;Stanford University;Microsoft;Stanford University,4;4;4;4;-1;4,3;3;3;3;-1;3,10
1914,1914,1914,1914,1914,1914,1914,1914,ICLR,2019, Reasoning About Physical Interactions with Object-Oriented Prediction and Planning,Michael Janner;Sergey Levine;William T. Freeman;Joshua B. Tenenbaum;Chelsea Finn;Jiajun Wu,janner@berkeley.edu;svlevine@eecs.berkeley.edu;billf@mit.edu;jbt@mit.edu;cbfinn@eecs.berkeley.edu;jiajunwu@mit.edu,7;9;5,4;4;5,Accept (Poster),4,5,0.0,yes,9/27/18,University of California Berkeley;University of California Berkeley;Massachusetts Institute of Technology;Massachusetts Institute of Technology;University of California Berkeley;Massachusetts Institute of Technology,5;5;2;2;5;2,18;18;5;5;18;5,
1915,1915,1915,1915,1915,1915,1915,1915,ICLR,2019, The relativistic discriminator: a key element missing from standard GAN,Alexia Jolicoeur-Martineau,alexia.jolicoeur-martineau@mail.mcgill.ca,6;6;7,2;4;3,Accept (Poster),0,5,3.0,yes,9/27/18,McGill University,85,42,5;4
1916,1916,1916,1916,1916,1916,1916,1916,ICLR,2019,SOM-VAE: Interpretable Discrete Representation Learning on Time Series,Vincent Fortuin;Matthias Hüser;Francesco Locatello;Heiko Strathmann;Gunnar Rätsch,fortuin@inf.ethz.ch;mhueser@inf.ethz.ch;locatelf@inf.ethz.ch;heiko.strathmann@gmail.com;raetsch@inf.ethz.ch,6;9;6,2;4;4,Accept (Poster),0,1,2.0,yes,9/27/18,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10;10;10,10;10;10;10;10,5
1917,1917,1917,1917,1917,1917,1917,1917,ICLR,2019,The Limitations of Adversarial Training and the Blind-Spot Attack,Huan Zhang*;Hongge Chen*;Zhao Song;Duane Boning;Inderjit S. Dhillon;Cho-Jui Hsieh,huan@huan-zhang.com;chenhg@mit.edu;zhaos@utexas.edu;boning@mtl.mit.edu;inderjit@cs.utexas.edu;chohsieh@cs.ucla.edu,6;7;7,4;3;2,Accept (Poster),0,10,0.0,yes,9/27/18,"University of California, Los Angeles;Massachusetts Institute of Technology;University of Texas, Austin;Massachusetts Institute of Technology;University of Texas, Austin;University of California, Los Angeles",20;2;22;2;22;20,15;5;49;5;49;15,4
1918,1918,1918,1918,1918,1918,1918,1918,ICLR,2019,Generating Multiple Objects at Spatially Distinct Locations,Tobias Hinz;Stefan Heinrich;Stefan Wermter,hinz@informatik.uni-hamburg.de;heinrich@informatik.uni-hamburg.de;wermter@informatik.uni-hamburg.de,6;7;8,4;4;4,Accept (Poster),0,8,1.0,yes,9/27/18,University of Hamburg;University of Hamburg;University of Hamburg,228;228;228,207;207;207,3;4;5
1919,1919,1919,1919,1919,1919,1919,1919,ICLR,2019,Multi-class classification without multi-class labels,Yen-Chang Hsu;Zhaoyang Lv;Joel Schlosser;Phillip Odom;Zsolt Kira,yenchang.hsu@gatech.edu;zhaoyang.lv@gatech.edu;joel.schlosser@gtri.gatech.edu;phillip.odom@gtri.gatech.edu;zkira@gatech.edu,6;7;5,3;4;4,Accept (Poster),0,5,0.0,yes,9/27/18,Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology,13;13;13;13;13,33;33;33;33;33,10
1920,1920,1920,1920,1920,1920,1920,1920,ICLR,2019,Optimal Completion Distillation for Sequence Learning,Sara Sabour;William Chan;Mohammad Norouzi,sasabour@google.com;williamchan@google.com;mnorouzi@google.com,7;7;6,4;4;3,Accept (Poster),1,8,1.0,yes,9/27/18,Google;Google;Google,-1;-1;-1,-1;-1;-1,
1921,1921,1921,1921,1921,1921,1921,1921,ICLR,2019,Graph HyperNetworks for Neural Architecture Search,Chris Zhang;Mengye Ren;Raquel Urtasun,cjzhang@edu.uwaterloo.ca;mren@cs.toronto.edu;urtasun@cs.toronto.edu,7;6;7,4;4;4,Accept (Poster),5,7,0.0,yes,9/27/18,"University of Waterloo;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",26;18;18,207;22;22,10
1922,1922,1922,1922,1922,1922,1922,1922,ICLR,2019,GO Gradient for Expectation-Based Objectives,Yulai Cong;Miaoyun Zhao;Ke Bai;Lawrence Carin,yulaicong@gmail.com;miaoyun9zhao@gmail.com;ke.bai@duke.edu;lcarin@duke.edu,7;7;6,4;4;4,Accept (Poster),2,5,3.0,yes,9/27/18,Duke University;Duke University;Duke University;Duke University,44;44;44;44,17;17;17;17,
1923,1923,1923,1923,1923,1923,1923,1923,ICLR,2019,InstaGAN: Instance-aware Image-to-Image Translation,Sangwoo Mo;Minsu Cho;Jinwoo Shin,swmo@kaist.ac.kr;mscho@postech.ac.kr;jinwoos@kaist.ac.kr,7;8;7,4;5;5,Accept (Poster),4,6,0.0,yes,9/27/18,Korea Advanced Institute of Science and Technology;POSTECH;Korea Advanced Institute of Science and Technology,20;123;20,95;137;95,5;4;2
1924,1924,1924,1924,1924,1924,1924,1924,ICLR,2019,DialogWAE: Multimodal Response Generation with Conditional Wasserstein Auto-Encoder,Xiaodong Gu;Kyunghyun Cho;Jung-Woo Ha;Sunghun Kim,guxiaodong1987@126.com;kyunghyun.cho@nyu.edu;jungwoo.ha@navercorp.com;hunkim@cse.ust.hk,7;7;5,4;3;3,Accept (Poster),0,3,3.0,yes,9/27/18,126;New York University;NAVER;The Hong Kong University of Science and Technology,-1;26;-1;39,-1;27;-1;44,5
1925,1925,1925,1925,1925,1925,1925,1925,ICLR,2019,Supervised Policy Update for Deep Reinforcement Learning,Quan Vuong;Yiming Zhang;Keith W. Ross,quan.hovuong@gmail.com;yiming.zhang@nyu.edu;keithwross@nyu.edu,6;9;6,4;2;3,Accept (Poster),7,21,0.0,yes,9/27/18,"University of California, San Diego;New York University;New York University",11;26;26,31;27;27,
1926,1926,1926,1926,1926,1926,1926,1926,ICLR,2019,Unsupervised Learning of the Set of Local Maxima,Lior Wolf;Sagie Benaim;Tomer Galanti,wolf@fb.com;sagiebenaim@gmail.com;tomer22g@gmail.com,8;8;8,3;4;3,Accept (Poster),0,10,0.0,yes,9/27/18,Facebook;Tel Aviv University;Tel Aviv University,-1;37;37,-1;217;217,8
1927,1927,1927,1927,1927,1927,1927,1927,ICLR,2019,Per-Tensor Fixed-Point Quantization of the Back-Propagation Algorithm,Charbel Sakr;Naresh Shanbhag,sakr2@illinois.edu;shanbhag@illinois.edu,7;3;8,3;2;4,Accept (Poster),0,9,0.0,yes,9/27/18,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",3;3,37;37,
1928,1928,1928,1928,1928,1928,1928,1928,ICLR,2019,Generalized Tensor Models for Recurrent Neural Networks,Valentin Khrulkov;Oleksii Hrinchuk;Ivan Oseledets,khrulkov.v@gmail.com;oleksii.hrinchuk@skoltech.ru;i.oseledets@skoltech.ru,6;7;7,4;3;4,Accept (Poster),0,5,0.0,yes,9/27/18,Skolkovo Institute of Science and Technology;Skolkovo Institute of Science and Technology;Skolkovo Institute of Science and Technology,-1;-1;-1,-1;-1;-1,
1929,1929,1929,1929,1929,1929,1929,1929,ICLR,2019,Towards Metamerism via Foveated Style Transfer,Arturo Deza;Aditya Jonnalagadda;Miguel P. Eckstein,deza@dyns.ucsb.edu;aditya_jonnalagadda@ece.ucsb.edu;eckstein@psych.ucsb.edu,7;8;7,4;4;5,Accept (Poster),0,7,0.0,yes,9/27/18,UC Santa Barbara;UC Santa Barbara;UC Santa Barbara,37;37;37,53;53;53,5
1930,1930,1930,1930,1930,1930,1930,1930,ICLR,2019,Learning To Solve Circuit-SAT: An Unsupervised Differentiable Approach,Saeed Amizadeh;Sergiy Matusevych;Markus Weimer,saeed.amizadeh@gmail.com;sergiym@microsoft.com;markus.weimer@microsoft.com,6;8;7,5;4;3,Accept (Poster),0,7,0.0,yes,9/27/18,Microsoft;Microsoft;Microsoft,-1;-1;-1,-1;-1;-1,8
1931,1931,1931,1931,1931,1931,1931,1931,ICLR,2019,Deep Convolutional Networks as shallow Gaussian Processes,Adrià Garriga-Alonso;Carl Edward Rasmussen;Laurence Aitchison,ag919@cam.ac.uk;cer54@cam.ac.uk;laurence.aitchison@gmail.com,5;8;5,5;3;4,Accept (Poster),0,3,0.0,yes,9/27/18,University of Cambridge;University of Cambridge;HHMI Janelia Research Campus,71;71;-1,2;2;-1,
1932,1932,1932,1932,1932,1932,1932,1932,ICLR,2019,"Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow",Xue Bin Peng;Angjoo Kanazawa;Sam Toyer;Pieter Abbeel;Sergey Levine,jasonpeng142@hotmail.com;kanazawa@eecs.berkeley.edu;sdt@berkeley.edu;pabbeel@cs.berkeley.edu;svlevine@eecs.berkeley.edu,6;10;8,3;4;3,Accept (Poster),0,6,0.0,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5;5,18;18;18;18;18,5;4
1933,1933,1933,1933,1933,1933,1933,1933,ICLR,2019,Hierarchical Generative Modeling for Controllable Speech Synthesis,Wei-Ning Hsu;Yu Zhang;Ron J. Weiss;Heiga Zen;Yonghui Wu;Yuxuan Wang;Yuan Cao;Ye Jia;Zhifeng Chen;Jonathan Shen;Patrick Nguyen;Ruoming Pang,wnhsu@mit.edu;ngyuzh@google.com;ronw@google.com;heigazen@google.com;yonghui@google.com;logpie@gmail.com;yuancao@google.com;jiaye@google.com;zhifengc@google.com;jonathanasdf@google.com;drpng@google.com;rpang@google.com,8;6;5,4;5;4,Accept (Poster),0,25,0.0,yes,9/27/18,Massachusetts Institute of Technology;Google;Google;Google;Google;Bytedance;Google;Google;Google;Google;Google;Google,2;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,5;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,5
1934,1934,1934,1934,1934,1934,1934,1934,ICLR,2019,Doubly Reparameterized Gradient Estimators for Monte Carlo Objectives,George Tucker;Dieterich Lawson;Shixiang Gu;Chris J. Maddison,gjt@google.com;jdl404@nyu.edu;shanegu@google.com;cmaddis@google.com,7;7;6,3;5;4,Accept (Poster),0,4,2.0,yes,9/27/18,Google;New York University;Google;Google,-1;26;-1;-1,-1;27;-1;-1,1
1935,1935,1935,1935,1935,1935,1935,1935,ICLR,2019,Reward Constrained Policy Optimization,Chen Tessler;Daniel J. Mankowitz;Shie Mannor,chen.tessler@gmail.com;daniel.mankowitz@gmail.com;shiemannor@gmail.com,6;6;7,2;4;2,Accept (Poster),0,5,0.0,yes,9/27/18,Technion;Google;Technion,25;-1;25,327;-1;327,1
1936,1936,1936,1936,1936,1936,1936,1936,ICLR,2019,Coarse-grain Fine-grain Coattention Network for Multi-evidence Question Answering,Victor Zhong;Caiming Xiong;Nitish Shirish Keskar;Richard Socher,victor@victorzhong.com;cxiong@salesforce.com;nkeskar@salesforce.com;richard@socher.org,7;7;4,4;5;3,Accept (Poster),0,10,0.0,yes,9/27/18,University of Washington;SalesForce.com;SalesForce.com;SalesForce.com,6;-1;-1;-1,25;-1;-1;-1,
1937,1937,1937,1937,1937,1937,1937,1937,ICLR,2019,Preconditioner on Matrix Lie Group for SGD,Xi-Lin Li,lixilinx@gmail.com,5;8;7,5;5;3,Accept (Poster),0,5,1.0,yes,9/27/18,,,,
1938,1938,1938,1938,1938,1938,1938,1938,ICLR,2019,LeMoNADe: Learned Motif and Neuronal Assembly Detection in calcium imaging videos,Elke Kirschbaum;Manuel Haußmann;Steffen Wolf;Hannah Sonntag;Justus Schneider;Shehabeldin Elzoheiry;Oliver Kann;Daniel Durstewitz;Fred A Hamprecht,elke.kirschbaum@iwr.uni-heidelberg.de;manuel.haussmann@iwr.uni-heidelberg.de;steffen.wolf@iwr.uni-heidelberg.de;hannah.sonntag@mpimf-heidelberg.mpg.de;justus.schneider@physiologie.uni-heidelberg.de;shehab.elzoheiry@physiologie.uni-heidelberg.de;oliver.kann@physiologie.uni-heidelberg.de;daniel.durstewitz@zi-mannheim.de;fred.hamprecht@iwr.uni-heidelberg.de,5;8;8,4;4;5,Accept (Poster),0,7,0.0,yes,9/27/18,Heidelberg University;Heidelberg University;Heidelberg University;Max-Planck Institute;Heidelberg University;Heidelberg University;Heidelberg University;ZI Mannheim;Heidelberg University,199;199;199;-1;199;199;199;-1;199,45;45;45;-1;45;45;45;-1;45,5
1939,1939,1939,1939,1939,1939,1939,1939,ICLR,2019,On the loss landscape of a class of deep neural networks with no bad local valleys,Quynh Nguyen;Mahesh Chandra Mukkamala;Matthias Hein,quynh@cs.uni-saarland.de;mmahesh.chandra873@gmail.com;matthias.hein@uni-tuebingen.de,7;6;8,4;5;4,Accept (Poster),0,10,0.0,yes,9/27/18,Saarland University;Saarland University;University of Tuebingen,89;89;153,1103;1103;94,
1940,1940,1940,1940,1940,1940,1940,1940,ICLR,2019,DHER: Hindsight Experience Replay for Dynamic Goals,Meng Fang;Cheng Zhou;Bei Shi;Boqing Gong;Jia Xu;Tong Zhang,moefang@gmail.com;chengzhmike@gmail.com;shibei00@gmail.com;boqinggo@outlook.com;jiaxu@cs.wisc.edu;tongzhang@tongzhang-ml.org,6;6;7,3;4;4,Accept (Poster),0,17,2.0,yes,9/27/18,Tencent AI Lab;Hong Kong University of Science and Technology;Tencent AI Lab;International Computer Science Institute;University of Southern California;,-1;39;-1;-1;30;-1,-1;44;-1;-1;66;-1,
1941,1941,1941,1941,1941,1941,1941,1941,ICLR,2019,SPIGAN: Privileged Adversarial Learning from Simulation,Kuan-Hui Lee;German Ros;Jie Li;Adrien Gaidon,kuan.lee@tri.global;germanros1987@gmail.com;jie.li@tri.global;adrien.gaidon@tri.global,7;6;7,5;5;4,Accept (Poster),0,5,0.0,yes,9/27/18,Toyota Research Institute;Intel;Toyota Research Institute;Toyota Research Institute,-1;-1;-1;-1,-1;-1;-1;-1,5;4;2
1942,1942,1942,1942,1942,1942,1942,1942,ICLR,2019,Augmented Cyclic Adversarial Learning for Low Resource Domain Adaptation,Ehsan Hosseini-Asl;Yingbo Zhou;Caiming Xiong;Richard Socher,ehosseiniasl@salesforce.com;yingbo.zhou@salesforce.com;cxiong@salesforce.com;rsocher@salesforce.com,6;5;8,4;4;2,Accept (Poster),0,13,0.0,yes,9/27/18,SalesForce.com;SalesForce.com;SalesForce.com;SalesForce.com,-1;-1;-1;-1,-1;-1;-1;-1,4
1943,1943,1943,1943,1943,1943,1943,1943,ICLR,2019,GamePad: A Learning Environment for Theorem Proving,Daniel Huang;Prafulla Dhariwal;Dawn Song;Ilya Sutskever,dehuang@berkeley.edu;prafulla@openai.com;dawnsong@cs.berkeley.edu;ilyasu@openai.com,4;7;7,4;3;2,Accept (Poster),0,5,1.0,yes,9/27/18,University of California Berkeley;OpenAI;University of California Berkeley;OpenAI,5;-1;5;-1,18;-1;18;-1,1
1944,1944,1944,1944,1944,1944,1944,1944,ICLR,2019,Deep Graph Infomax,Petar Veličković;William Fedus;William L. Hamilton;Pietro Liò;Yoshua Bengio;R Devon Hjelm,petar.velickovic@cst.cam.ac.uk;liam.fedus@gmail.com;wleif@stanford.edu;pietro.lio@cst.cam.ac.uk;yoshua.umontreal@gmail.com;devon.hjelm@microsoft.com,7;9;5,3;4;4,Accept (Poster),2,13,0.0,yes,9/27/18,University of Cambridge;University of Montreal;Stanford University;University of Cambridge;University of Montreal;Microsoft,71;123;4;71;123;-1,2;108;3;2;108;-1,10
1945,1945,1945,1945,1945,1945,1945,1945,ICLR,2019,Fluctuation-dissipation relations for stochastic gradient descent,Sho Yaida,shoyaida@fb.com,8;5;6,5;4;3,Accept (Poster),0,3,1.0,yes,9/27/18,Facebook,-1,-1,1
1946,1946,1946,1946,1946,1946,1946,1946,ICLR,2019,A Kernel Random Matrix-Based Approach for Sparse PCA,Mohamed El Amine Seddik;Mohamed Tamaazousti;Romain Couillet,melaseddik@gmail.com;mohamed.tamaazousti@cea.fr;romain.couillet@gmail.com,6;5;7,4;5;2,Accept (Poster),0,4,0.0,yes,9/27/18,CEA;CEA;,228;228;-1,811;811;-1,
1947,1947,1947,1947,1947,1947,1947,1947,ICLR,2019,Sample Efficient Adaptive Text-to-Speech,Yutian Chen;Yannis Assael;Brendan Shillingford;David Budden;Scott Reed;Heiga Zen;Quan Wang;Luis C. Cobo;Andrew Trask;Ben Laurie;Caglar Gulcehre;Aäron van den Oord;Oriol Vinyals;Nando de Freitas,yutianc@google.com;yannisassael@google.com;shillingford@google.com;budden@google.com;reedscot@google.com;heigazen@google.com;quanw@google.com;luisca@google.com;atrask@google.com;benl@google.com;caglarg@google.com;avdnoord@google.com;vinyals@google.com;nandodefreitas@google.com,7;7;6,4;4;5,Accept (Poster),0,3,0.0,yes,9/27/18,Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,6
1948,1948,1948,1948,1948,1948,1948,1948,ICLR,2019,ProMP: Proximal Meta-Policy Search,Jonas Rothfuss;Dennis Lee;Ignasi Clavera;Tamim Asfour;Pieter Abbeel,jonas.rothfuss@gmail.com;dennisl88@berkeley.edu;iclavera@berkeley.edu;asfour@kit.edu;pabbeel@cs.berkeley.edu,6;7;9,3;3;3,Accept (Poster),11,10,2.0,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley;Karlsruhe Institute of Technology;University of California Berkeley,5;5;5;153;5,18;18;18;133;18,6
1949,1949,1949,1949,1949,1949,1949,1949,ICLR,2019,A Convergence Analysis of Gradient Descent for Deep Linear Neural Networks,Sanjeev Arora;Nadav Cohen;Noah Golowich;Wei Hu,arora@cs.princeton.edu;cohennadav@ias.edu;ngolowich@college.harvard.edu;huwei@cs.princeton.edu,7;7;7,4;4;5,Accept (Poster),0,10,0.0,yes,9/27/18,"Princeton University;Institue for Advanced Study, Princeton;Harvard University;Princeton University",30;-1;39;30,7;-1;6;7,
1950,1950,1950,1950,1950,1950,1950,1950,ICLR,2019,Slimmable Neural Networks,Jiahui Yu;Linjie Yang;Ning Xu;Jianchao Yang;Thomas Huang,jyu79@illinois.edu;linjie.yang@snap.com;ning.xu@snap.com;jianchao.yang@bytedance.com;huang@ifp.uiuc.edu,8;9;7,4;5;4,Accept (Poster),1,15,0.0,yes,9/27/18,"University of Illinois, Urbana Champaign;Snap Inc.;Snap Inc.;Bytedance;University of Illinois, Urbana-Champaign",3;-1;-1;-1;3,37;-1;-1;-1;37,2
1951,1951,1951,1951,1951,1951,1951,1951,ICLR,2019,Discrete flow posteriors for variational inference in discrete dynamical systems,Laurence Aitchison;Vincent Adam;Srinivas C. Turaga,laurence.aitchison@gmail.com;vincent.adam@prowler.io;turagas@janelia.hhmi.org,4;4;7,3;4;4,Reject,0,0,0.0,yes,9/27/18,HHMI Janelia Research Campus;Prowler.io;HHMI Janelia Research Campus,-1;-1;-1,-1;-1;-1,5
1952,1952,1952,1952,1952,1952,1952,1952,ICLR,2019,Variational Sparse Coding,Francesco Tonolini;Bjorn Sand Jensen;Roderick Murray-Smith,2402432t@student.gla.ac.uk;bjorn.jensen@glasgow.ac.uk;roderick.murray-smith@glasgow.ac.uk,4;5;5,4;5;4,Reject,3,6,2.0,yes,9/27/18,University of Glasgow;University of Glasgow;University of Glasgow,169;169;169,80;80;80,5;1
1953,1953,1953,1953,1953,1953,1953,1953,ICLR,2019,Detecting Memorization in ReLU Networks,Edo Collins;Siavash Arjomand Bigdeli;Sabine Süsstrunk,edo.collins@epfl.ch;siavash.bigdeli@epfl.ch;sabine.susstrunk@epfl.ch,6;5;9,4;4;5,Reject,0,11,0.0,yes,9/27/18,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne,478;478;478,38;38;38,8
1954,1954,1954,1954,1954,1954,1954,1954,ICLR,2019,Reinforcement Learning with Perturbed Rewards,Jingkang Wang;Yang Liu;Bo Li,wangjksjtu_01@sjtu.edu.cn;yangliu@ucsc.edu;lxbosky@gmail.com,6;6;6,4;3;3,Reject,0,9,0.0,yes,9/27/18,Shanghai Jiao Tong University;University of Southern California;University of California Berkeley,52;30;5,188;66;18,1
1955,1955,1955,1955,1955,1955,1955,1955,ICLR,2019,Local Critic Training of Deep Neural Networks,Hojung Lee;Jong-Seok Lee,hjlee92@yonsei.ac.kr;jong-seok.lee@yonsei.ac.kr,6;7;6,3;5;4,Reject,0,7,0.0,yes,9/27/18,Yonsei University;Yonsei University,478;478,231;231,
1956,1956,1956,1956,1956,1956,1956,1956,ICLR,2019,Learning from Noisy Demonstration Sets via Meta-Learned Suitability Assessor,Te-Lin Wu;Jaedong Hwang;Jingyun Yang;Shaofan Lai;Carl Vondrick;Joseph J. Lim,telinwu@usc.edu;jd730@snu.ac.kr;jingyuny@usc.edu;shaofanl@usc.edu;vondrick@cs.columbia.edu;limjj@usc.edu,4;4;4,4;4;3,Reject,0,0,0.0,yes,9/27/18,University of Southern California;Seoul National University;University of Southern California;University of Southern California;Columbia University;University of Southern California,30;41;30;30;15;30,66;74;66;66;14;66,6
1957,1957,1957,1957,1957,1957,1957,1957,ICLR,2019,Detecting Topological Defects in 2D Active Nematics Using Convolutional Neural Networks,Ruoshi Liu;Michael M. Norton;Seth Fraden;Pengyu Hong,ruoshiliu@brandeis.edu;mmnorton@brandeis.edu;fraden@brandeis.edu;hongpeng@brandeis.edu,4;4;2,4;4;5,Reject,0,0,0.0,yes,9/27/18,Brandeis University;Brandeis University;Brandeis University;Brandeis University,314;314;314;314,223;223;223;223,
1958,1958,1958,1958,1958,1958,1958,1958,ICLR,2019,Rating Continuous Actions in Spatial Multi-Agent Problems,Uwe Dick;Maryam Tavakol;Ulf Brefeld,uwe.dick@leuphana.de;tavakol@leuphana.de;brefeld@leuphana.de,5;4;4,4;3;4,Reject,0,0,0.0,yes,9/27/18,Inst. of Information Systems / Machine Learning;Inst. of Information Systems / Machine Learning;Inst. of Information Systems / Machine Learning,-1;-1;-1,-1;-1;-1,4
1959,1959,1959,1959,1959,1959,1959,1959,ICLR,2019,SSoC: Learning Spontaneous and Self-Organizing Communication for Multi-Agent Collaboration,Xiangyu Kong;Jing Li;Bo Xin;Yizhou Wang,kong@pku.edu.cn;lijingg@pku.edu.cn;jimxinbo@gmail.com;yizhou.wang@pku.edu.cn,4;5;5,3;4;3,Reject,0,1,0.0,yes,9/27/18,Peking University;Peking University;Microsoft;Peking University,24;24;-1;24,27;27;-1;27,
1960,1960,1960,1960,1960,1960,1960,1960,ICLR,2019,Generating Realistic Stock Market Order Streams,Junyi Li;Xintong Wang;Yaoyang Lin;Arunesh Sinha;Michael P. Wellman,junyili@umich.edu;xintongw@umich.edu;yaoyang@umich.edu;arunesh@umich.edu;wellman@umich.edu,5;5;4,4;4;5,Reject,0,4,0.0,yes,9/27/18,University of Michigan;University of Michigan;University of Michigan;University of Michigan;University of Michigan,8;8;8;8;8,21;21;21;21;21,5;4
1961,1961,1961,1961,1961,1961,1961,1961,ICLR,2019,Causal Reasoning from Meta-reinforcement learning,Ishita Dasgupta;Jane Wang;Silvia Chiappa;Jovana Mitrovic;Pedro Ortega;David Raposo;Edward Hughes;Peter Battaglia;Matthew Botvinick;Zeb Kurth-Nelson,ishitadasgupta@g.harvard.edu;wangjane@google.com;csilvia@google.com;mitrovic@google.com;pedroortega@google.com;draposo@google.com;edwardhughes@google.com;peterbattaglia@google.com;botvinick@google.com;zebk@google.com,5;4;4;7,4;3;4;4,Reject,0,10,0.0,yes,9/27/18,Harvard University;Google;Google;Google;Google;Google;Google;Google;Google;Google,39;-1;-1;-1;-1;-1;-1;-1;-1;-1,6;-1;-1;-1;-1;-1;-1;-1;-1;-1,6
1962,1962,1962,1962,1962,1962,1962,1962,ICLR,2019,MahiNet: A Neural Network for Many-Class Few-Shot Learning with Class Hierarchy,Lu Liu;Tianyi Zhou;Guodong Long;Jing Jiang;Chengqi Zhang,lu.liu.cs.uts@gmail.com;tianyizh@uw.edu;guodong.long@uts.edu.au;jing.jiang@uts.edu.au;chengqi.zhang@uts.edu.au,5;6;5,3;3;3,Reject,0,7,0.0,yes,9/27/18,"University of Technology Sydney;University of Washington, Seattle;University of Technology Sydney;University of Technology Sydney;University of Technology Sydney",106;6;106;106;106,216;25;216;216;216,6
1963,1963,1963,1963,1963,1963,1963,1963,ICLR,2019,CoDraw: Collaborative Drawing as a Testbed for Grounded Goal-driven Communication,Nikita Kitaev;Jin-Hwa Kim;Xinlei Chen;Marcus Rohrbach;Yuandong Tian;Dhruv Batra;Devi Parikh,kitaev@cs.berkeley.edu;jnhwkim@gmail.com;xinleic@fb.com;maroffm@gmail.com;yuandong@fb.com;dbatra@gatech.edu;parikh@gatech.edu,4;6;7,4;4;4,Reject,0,3,0.0,yes,9/27/18,University of California Berkeley;SK T-Brain;Facebook;Facebook;Facebook;Georgia Institute of Technology;Georgia Institute of Technology,5;-1;-1;-1;-1;13;13,18;-1;-1;-1;-1;33;33,3
1964,1964,1964,1964,1964,1964,1964,1964,ICLR,2019,I Know the Feeling: Learning to Converse with Empathy,Hannah Rashkin;Eric Michael Smith;Margaret Li;Y-Lan Boureau,hrashkin@cs.washington.edu;ems@fb.com;hadasah@gmail.com;ylan@fb.com,4;7;5,4;4;3,Reject,0,11,0.0,yes,9/27/18,University of Washington;Facebook;Facebook;Facebook,6;-1;-1;-1,25;-1;-1;-1,
1965,1965,1965,1965,1965,1965,1965,1965,ICLR,2019,Neural MMO: A massively multiplayer game environment for intelligent agents,Joseph Suarez;Yilun Du;Phillip Isola;Igor Mordatch,joseph15@stanford.edu;yilundu@gmail.com;phillipi@mit.edu;mordatch@openai.com,6;5;7,4;2;5,Reject,0,5,0.0,yes,9/27/18,Stanford University;Massachusetts Institute of Technology;Massachusetts Institute of Technology;OpenAI,4;2;2;-1,3;5;5;-1,
1966,1966,1966,1966,1966,1966,1966,1966,ICLR,2019,Temporal Gaussian Mixture Layer for Videos,AJ Piergiovanni;Michael S. Ryoo,ajpiergi@indiana.edu;mryoo@indiana.edu,6;6;7,5;3;5,Reject,0,6,1.0,yes,9/27/18,University of Arizona;University of Arizona,169;169,161;161,
1967,1967,1967,1967,1967,1967,1967,1967,ICLR,2019,LEARNING GENERATIVE MODELS FOR DEMIXING OF STRUCTURED SIGNALS FROM THEIR SUPERPOSITION USING GANS,Mohammadreza Soltani;Swayambhoo Jain;Abhinav V. Sambasivan,msoltani@iastate.edu;swayambhoo.jain@technicolor.com;samba014@umn.edu,7;4;5,4;5;4,Reject,0,3,0.0,yes,9/27/18,"Iowa State University;Technicolor;University of Minnesota, Minneapolis",169;-1;57,341;-1;56,5;4
1968,1968,1968,1968,1968,1968,1968,1968,ICLR,2019,Invariant-equivariant representation learning for multi-class data,Ilya Feige,ilya@asidatascience.com,7;5;4,2;5;3,Reject,0,5,0.0,yes,9/27/18,University College London,50,16,5
1969,1969,1969,1969,1969,1969,1969,1969,ICLR,2019,Graph Matching Networks for Learning the Similarity of Graph Structured Objects,Yujia Li;Chenjie Gu;Thomas Dullien;Oriol Vinyals;Pushmeet Kohli,yujiali@google.com;gcj@google.com;thomasdullien@google.com;vinyals@google.com;pushmeet@google.com,6;5;6,4;4;4,Reject,0,3,0.0,yes,9/27/18,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,10
1970,1970,1970,1970,1970,1970,1970,1970,ICLR,2019,Probabilistic Semantic Embedding,Yue Jiao;Jonathon Hare;Adam Prügel-Bennett,yj5y15@ecs.soton.ac.uk;jsh2@ecs.soton.ac.uk;apb@ecs.soton.ac.uk,7;4;4,3;4;4,Reject,0,7,0.0,yes,9/27/18,University of Southampton;University of Southampton;University of Southampton,169;169;169,126;126;126,
1971,1971,1971,1971,1971,1971,1971,1971,ICLR,2019,SENSE: SEMANTICALLY ENHANCED NODE SEQUENCE EMBEDDING,Swati Rallapalli;Liang Ma;Mudhakar Srivatsa;Ananthram Swami;Heesung Kwon;Graham Bent;Christopher Simpkin,srallapalli@us.ibm.com;maliang@us.ibm.com;msrivats@us.ibm.com;ananthram.swami.civ@mail.mil;heesung.kwon.civ@mail.mil;gbent@uk.ibm.com;simpkin.chris@gmail.com,4;4;5,4;5;3,Reject,0,3,0.0,yes,9/27/18,International Business Machines;International Business Machines;International Business Machines;Army Reserach laboratory;Army Reserach laboratory;International Business Machines;,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,1;10
1972,1972,1972,1972,1972,1972,1972,1972,ICLR,2019,Pix2Scene: Learning Implicit 3D Representations from Images,Sai Rajeswar;Fahim Mannan;Florian Golemo;David Vazquez;Derek Nowrouzezahrai;Aaron Courville,rajsai24@gmail.com;fmannan@gmail.com;florian.golemo@inria.fr;dvazquez@cvc.uab.es;dereknow@gmail.com;aaron.courville@gmail.com,6;5;6,1;4;4,Reject,0,15,2.0,yes,9/27/18,"University of Montreal;;INRIA;Computer Vision Center, Universitat Autònoma de Barcelona;;University of Montreal",123;-1;-1;478;-1;123,108;-1;-1;146;-1;108,5;4;2
1973,1973,1973,1973,1973,1973,1973,1973,ICLR,2019,COCO-GAN: Conditional Coordinate Generative Adversarial Network,Chieh Hubert Lin;Chia-Che Chang;Yu-Sheng Chen;Da-Cheng Juan;Wei Wei;Hwann-Tzong Chen,hubert052702@gmail.com;chang810249@gmail.com;nothinglo@cmlab.csie.ntu.edu.tw;dacheng@google.com;wewei@google.com;htchen@cs.nthu.edu.tw,6;4;6,4;5;4,Reject,0,27,0.0,yes,9/27/18,National Tsing Hua University;National Tsing Hua University;National Taiwan University;Google;Google;National Tsing Hua University,199;199;85;-1;-1;199,323;323;197;-1;-1;323,5;4
1974,1974,1974,1974,1974,1974,1974,1974,ICLR,2019,Implicit Autoencoders,Alireza Makhzani,a.makhzani@gmail.com,3;6;6,3;4;3,Reject,0,7,1.0,yes,9/27/18,,,,5;4
1975,1975,1975,1975,1975,1975,1975,1975,ICLR,2019,Identifying Bias in AI using Simulation,Daniel McDuff;Roger Cheng;Ashish Kapoor,damcduff@microsoft.com;rocheng@microsoft.com;akapoor@microsoft.com,7;5;6,5;4;2,Reject,0,9,0.0,yes,9/27/18,Microsoft;Microsoft;Microsoft,-1;-1;-1,-1;-1;-1,11;7
1976,1976,1976,1976,1976,1976,1976,1976,ICLR,2019,Backplay: 'Man muss immer umkehren',Cinjon Resnick;Roberta Raileanu;Sanyam Kapoor;Alexander Peysakhovich;Kyunghyun Cho;Joan Bruna,cinjon.resnick@gmail.com;raileanu.roberta@gmail.com;sanyam@nyu.edu;alexpeys@fb.com;kyunghyun.cho@nyu.edu;bruna@cims.nyu.edu,5;5;5,4;4;3,Reject,0,8,0.0,yes,9/27/18,New York University;New York University;New York University;Facebook;New York University;New York University,26;26;26;-1;26;26,27;27;27;-1;27;27,
1977,1977,1977,1977,1977,1977,1977,1977,ICLR,2019,Intrinsic Social Motivation via Causal Influence in Multi-Agent RL,Natasha Jaques;Angeliki Lazaridou;Edward Hughes;Caglar Gulcehre;Pedro A. Ortega;DJ Strouse;Joel Z. Leibo;Nando de Freitas,jaquesn@mit.edu;angeliki@google.com;edwardhughes@google.com;caglarg@google.com;pedroortega@google.com;danieljstrouse@gmail.com;jzl@google.com;nandodefreitas@google.com,5;4;6,3;5;3,Reject,0,9,1.0,yes,9/27/18,Massachusetts Institute of Technology;Google;Google;Google;Google;Princeton University;Google;Google,2;-1;-1;-1;-1;30;-1;-1,5;-1;-1;-1;-1;7;-1;-1,
1978,1978,1978,1978,1978,1978,1978,1978,ICLR,2019,Dynamic Planning Networks,Norman L. Tasfi;Miriam Capretz,ntasfi@gmail.com;mcapretz@uwo.ca,6;4;6,5;5;2,Reject,1,0,8.0,yes,9/27/18,University of Western Ontario;University of Western Ontario,478;478,1103;1103,8
1979,1979,1979,1979,1979,1979,1979,1979,ICLR,2019,Learning to Coordinate Multiple Reinforcement Learning Agents for Diverse Query Reformulation,Rodrigo Nogueira;Jannis Bulian;Massimiliano Ciaramita,rodrigonogueira@nyu.edu;jbulian@google.com;massi@google.com,4;7;5,3;4;4,Reject,2,0,4.0,yes,9/27/18,New York University;Google;Google,26;-1;-1,27;-1;-1,8
1980,1980,1980,1980,1980,1980,1980,1980,ICLR,2019,Recovering the Lowest Layer of Deep Networks with High Threshold Activations,Surbhi Goel;Rina Panigrahy,surbhi@cs.utexas.edu;rinap@google.com,4;5;4,4;3;4,Reject,0,3,0.0,yes,9/27/18,"University of Texas, Austin;Google",22;-1,49;-1,
1981,1981,1981,1981,1981,1981,1981,1981,ICLR,2019,Improved Learning of One-hidden-layer Convolutional Neural Networks with Overlaps,Simon S. Du;Surbhi Goel,ssdu@cs.cmu.edu;surbhi@cs.utexas.edu,6;5;6,3;1;4,Reject,0,3,0.0,yes,9/27/18,"Carnegie Mellon University;University of Texas, Austin",1;22,24;49,2
1982,1982,1982,1982,1982,1982,1982,1982,ICLR,2019,Using Ontologies To Improve Performance In Massively Multi-label Prediction,Ethan Steinberg;Peter J. Liu,ethan.steinberg@gmail.com;peterjliu@google.com,6;5;4,3;3;4,Reject,0,4,0.0,yes,9/27/18,Stanford University;Google,4;-1,3;-1,11
1983,1983,1983,1983,1983,1983,1983,1983,ICLR,2019,Overfitting Detection of Deep Neural Networks without a Hold Out Set,Konrad Groh,konrad.groh@de.bosch.com,4;5;3,4;3;4,Reject,0,1,0.0,yes,9/27/18,Bosch,-1,-1,
1984,1984,1984,1984,1984,1984,1984,1984,ICLR,2019,TarMAC: Targeted Multi-Agent Communication,Abhishek Das;Theophile Gervet;Joshua Romoff;Dhruv Batra;Devi Parikh;Mike Rabbat;Joelle Pineau,abhshkdz@gatech.edu;tgervet@andrew.cmu.edu;joshua.romoff@mail.mcgill.ca;dbatra@gatech.edu;parikh@gatech.edu;mikerabbat@fb.com;jpineau@cs.mcgill.ca,6;6;6,5;4;5,Reject,0,11,0.0,yes,9/27/18,Georgia Institute of Technology;Carnegie Mellon University;McGill University;Georgia Institute of Technology;Georgia Institute of Technology;Facebook;McGill University,13;1;85;13;13;-1;85,33;24;42;33;33;-1;42,
1985,1985,1985,1985,1985,1985,1985,1985,ICLR,2019,DATNet: Dual Adversarial Transfer for Low-resource Named Entity Recognition,Joey Tianyi Zhou;Hao Zhang;Di Jin;Hongyuan Zhu;Rick Siow Mong Goh;Kenneth Kwok,joey.tianyi.zhou@gmail.com;isaac.changhau@gmail.com;jindi15@mit.edu;hongyuanzhu.cn@gmail.com;gohsm@ihpc.a-star.edu.sg;kenkwok@ihpc.a-star.edu.sg,6;6;6,4;5;4,Reject,0,3,0.0,yes,9/27/18,";A*STAR;Massachusetts Institute of Technology;Institute for Infocomm Research;Institute of High Performance Computing, Singapore, A*STAR;Institute of High Performance Computing, Singapore, A*STAR",-1;-1;2;-1;-1;-1,-1;-1;5;-1;-1;-1,4;8
1986,1986,1986,1986,1986,1986,1986,1986,ICLR,2019,MILE: A Multi-Level Framework for Scalable Graph Embedding,Jiongqian Liang;Saket Gurukar;Srinivasan Parthasarathy,liang.albert@outlook.com;gurukar.1@osu.edu;srini@cse.ohio-state.edu,7;4;6,3;4;5,Reject,0,5,0.0,yes,9/27/18,Ohio State University;Ohio State University;Ohio State University,76;76;76,318;318;318,10
1987,1987,1987,1987,1987,1987,1987,1987,ICLR,2019,Adversarial Attacks on Node Embeddings,Aleksandar Bojchevski;Stephan Günnemann,a.bojchevski@in.tum.de;guennemann@in.tum.de,6;5;6,4;5;3,Reject,0,3,0.0,yes,9/27/18,Technical University Munich;Technical University Munich,54;54,41;41,4;10
1988,1988,1988,1988,1988,1988,1988,1988,ICLR,2019,DADAM: A consensus-based distributed adaptive gradient method for online optimization,Parvin Nazari;Davoud Ataee Tarzanagh;George Michailidis,p_nazari@aut.ac.ir;tarzanagh@ufl.edu;gmichail@ufl.edu,8;4;6,3;4;4,Reject,0,10,3.0,yes,9/27/18,University of Tehran;University of Florida;University of Florida,478;123;123,683;143;143,1;9
1989,1989,1989,1989,1989,1989,1989,1989,ICLR,2019,Training Hard-Threshold Networks with Combinatorial Search in a Discrete Target Propagation Setting,Lukas Nabergall;Justin Toth;Leah Cousins,lnaberga@uwaterloo.ca;wjtoth@uwaterloo.ca;lm2cousi@uwaterloo.ca,3;4;5,5;4;5,Reject,0,4,0.0,yes,9/27/18,University of Waterloo;University of Waterloo;University of Waterloo,26;26;26,207;207;207,
1990,1990,1990,1990,1990,1990,1990,1990,ICLR,2019,Projective Subspace Networks For Few-Shot Learning,Christian Simon;Piotr Koniusz;Mehrtash Harandi,christian.simon@anu.edu.au;piotr.koniusz@data61.csiro.au;mehrtash.harandi@monash.edu,6;6;6,3;4;4,Reject,0,9,0.0,yes,9/27/18,"Australian National University;, CSIRO;Monash University",106;-1;123,48;-1;80,6;8
1991,1991,1991,1991,1991,1991,1991,1991,ICLR,2019,Alignment Based Mathching Networks for One-Shot Classification and Open-Set Recognition,Paresh Malalur;Tommi Jaakkola,pareshmg@csail.mit.edu;tommi@csail.mit.edu,7;6;7;4,3;2;4;4,Reject,0,3,0.0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2,5;5,
1992,1992,1992,1992,1992,1992,1992,1992,ICLR,2019,Cramer-Wold AutoEncoder,Jacek Tabor;Szymon Knop;Przemysław Spurek;Igor Podolak;Marcin Mazur;Stanisław Jastrzębski,jacek.tabor@uj.edu.pl;szymon.knop@doctoral.uj.edu.pl;przemyslaw.spurek@uj.edu.pl;igor.podolak@uj.edu.pl;marcin.mazur@uj.edu.pl;staszek.jastrzebski@gmail.com,6;7;5,4;4;4,Reject,0,9,0.0,yes,9/27/18,Jagiellonian University;Jagiellonian University;Jagiellonian University;Jagiellonian University;Jagiellonian University;New York University,478;478;478;478;478;26,695;695;695;695;695;27,5
1993,1993,1993,1993,1993,1993,1993,1993,ICLR,2019,Directional Analysis of Stochastic Gradient Descent via von Mises-Fisher Distributions in Deep Learning,Cheolhyoung Lee;Kyunghyun Cho;Wanmo Kang,bloodwass@kaist.ac.kr;kyunghyun.cho@nyu.edu;wanmo.kang@kaist.edu,6;5;4,3;3;3,Reject,0,7,0.0,yes,9/27/18,Korea Advanced Institute of Science and Technology;New York University;KAIST,20;26;20,95;27;95,
1994,1994,1994,1994,1994,1994,1994,1994,ICLR,2019,Unsupervised Control Through Non-Parametric Discriminative Rewards,David Warde-Farley;Tom Van de Wiele;Tejas Kulkarni;Catalin Ionescu;Steven Hansen;Volodymyr Mnih,d.warde.farley@gmail.com;tomvandewiele@google.com;tejasdkulkarni@gmail.com;cdi@google.com;stevenhansen@google.com;vmnih@google.com,7;8;5,3;5;5,Accept (Poster),3,15,4.0,yes,9/27/18,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,
1995,1995,1995,1995,1995,1995,1995,1995,ICLR,2019,Clean-Label Backdoor Attacks,Alexander Turner;Dimitris Tsipras;Aleksander Madry,turneram@mit.edu;tsipras@mit.edu;madry@mit.edu,6;7;4,2;2;4,Reject,0,11,0.0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2,5;5;5,5;4
1996,1996,1996,1996,1996,1996,1996,1996,ICLR,2019,Universal Successor Features for Transfer Reinforcement Learning,Chen Ma;Dylan R. Ashley;Junfeng Wen;Yoshua Bengio,chenchloem@gmail.com;dashley@ualberta.ca;junfengwen@gmail.com;yoshua.umontreal@gmail.com,6;4;7,5;5;5,Reject,0,15,3.0,yes,9/27/18,;University of Alberta;University of Alberta;University of Montreal,-1;99;99;123,-1;119;119;108,8
1997,1997,1997,1997,1997,1997,1997,1997,ICLR,2019,Improved Language Modeling by Decoding the Past,Siddhartha Brahma,sidbrahma@gmail.com,3;6;7,5;3;5,Reject,8,6,0.0,yes,9/27/18,International Business Machines,-1,-1,3
1998,1998,1998,1998,1998,1998,1998,1998,ICLR,2019,ON BREIMAN’S DILEMMA IN NEURAL NETWORKS: SUCCESS AND FAILURE OF NORMALIZED MARGINS,Yifei HUANG;Yuan YAO;Weizhi ZHU,yhuangcc@ust.hk;yuany@ust.hk;wzhuai@connect.ust.hk,4;5;5,4;4;3,Reject,0,3,0.0,yes,9/27/18,The Hong Kong University of Science and Technology;The Hong Kong University of Science and Technology;The Hong Kong University of Science and Technology,39;39;39,44;44;44,1;8
1999,1999,1999,1999,1999,1999,1999,1999,ICLR,2019,Decoupling Gating from Linearity,Yonathan Fiat;Eran Malach;Shai Shalev-Shwartz,jonathan.fiat@gmail.com;eran.malach@mail.huji.ac.il;shais@cs.huji.ac.il,3;2;3,4;5;5,Reject,0,0,0.0,yes,9/27/18,Hebrew University of Jerusalem;Hebrew University of Jerusalem;Hebrew University of Jerusalem,65;65;65,205;205;205,
2000,2000,2000,2000,2000,2000,2000,2000,ICLR,2019,Learning Information Propagation in the Dynamical Systems via Information Bottleneck Hierarchy,Gaurav Gupta;Mohamed Ridha Znaidi;Paul Bogdan,ggaurav@usc.edu;znaidi@usc.edu;pbogdan@usc.edu,5;4;5,3;2;4,Reject,0,6,0.0,yes,9/27/18,University of Southern California;University of Southern California;University of Southern California,30;30;30,66;66;66,
2001,2001,2001,2001,2001,2001,2001,2001,ICLR,2019,Efficient Dictionary Learning with Gradient Descent,Dar Gilboa;Sam Buchanan;John Wright,dg2893@columbia.edu;sdb2157@columbia.edu;jw2966@columbia.edu,5;4;5,4;3;2,Reject,0,0,0.0,yes,9/27/18,Columbia University;Columbia University;Columbia University,15;15;15,14;14;14,9
2002,2002,2002,2002,2002,2002,2002,2002,ICLR,2019,Learning Backpropagation-Free Deep Architectures with Kernels,Shiyu Duan;Shujian Yu;Yunmei Chen;Jose Principe,michaelshiyu3@gmail.com;yusjlcy9011@ufl.edu,6;6;5,4;4;3,Reject,0,25,2.0,yes,9/27/18,University of Florida;University of Florida,123;123,143;143,1
2003,2003,2003,2003,2003,2003,2003,2003,ICLR,2019,Entropic GANs meet VAEs: A Statistical Approach to Compute Sample Likelihoods in GANs,Yogesh Balaji;Hamed Hasani;Rama Chellappa;Soheil Feizi,yogesh@cs.umd.edu;hassani@seas.upenn.edu;rama@umiacs.umd.edu;sfeizi@cs.umd.edu,6;5;5,5;3;4,Reject,0,4,0.0,yes,9/27/18,"University of Maryland, College Park;University of Pennsylvania;University of Maryland, College Park;University of Maryland, College Park",12;19;12;12,69;10;69;69,5;4;1
2004,2004,2004,2004,2004,2004,2004,2004,ICLR,2019,Incremental training of multi-generative adversarial networks,Qi Tan;Pingzhong Tang;Ke Xu;Weiran Shen;Song Zuo,thunderingtan@gmail.com;kenshinping@gmail.com;xuke@tsinghua.edu.cn;emersonswr@gmail.com;songzuo.z@gmail.com,5;6;6,3;4;3,Reject,0,3,0.0,yes,9/27/18,Tsinghua University;Tsinghua University;Tsinghua University;;Google,8;8;8;-1;-1,30;30;30;-1;-1,5;4
2005,2005,2005,2005,2005,2005,2005,2005,ICLR,2019,Transformer-XL: Language Modeling with Longer-Term Dependency,Zihang Dai*;Zhilin Yang*;Yiming Yang;William W. Cohen;Jaime Carbonell;Quoc V. Le;Ruslan Salakhutdinov,zander.dai@gmail.com;zhiliny@cs.cmu.edu;yiming@cs.cmu.edu;wcohen@google.com;jgc@cs.cmu.edu;qvl@google.com;rsalakhu@cs.cmu.edu,6;6;4,4;4;4,Reject,0,9,6.0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Google;Carnegie Mellon University;Google;Carnegie Mellon University,1;1;1;-1;1;-1;1,24;24;24;-1;24;-1;24,3
2006,2006,2006,2006,2006,2006,2006,2006,ICLR,2019,Formal Limitations on the Measurement of Mutual Information,David McAllester;Karl Stratos,mcallester@ttic.edu;stratos@ttic.edu,6;8;4,3;5;4,Reject,0,12,0.0,yes,9/27/18,Toyota Technological Institute at Chicago;Toyota Technological Institute at Chicago,123;123,1103;1103,1
2007,2007,2007,2007,2007,2007,2007,2007,ICLR,2019,Area Attention,Yang Li;Lukasz Kaiser;Samy Bengio;Si Si,liyang@google.com;lukaszkaiser@google.com;bengio@google.com;sisidaisy@google.com,6;5;5,4;5;4,Reject,12,10,0.0,yes,9/27/18,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,3
2008,2008,2008,2008,2008,2008,2008,2008,ICLR,2019,The Conditional Entropy Bottleneck,Ian Fischer,iansf@google.com,6;6;2,3;3;4,Reject,0,20,0.0,yes,9/27/18,Google,-1,-1,4;8
2009,2009,2009,2009,2009,2009,2009,2009,ICLR,2019,State-Denoised Recurrent Neural Networks,Michael C. Mozer;Denis Kazakov;Robert V. Lindsey,mozer@colorado.edu;denis.kazakov@colorado.edu;rob@imagen.ai,6;5;5,4;3;4,Reject,0,3,0.0,yes,9/27/18,"University of Colorado, Boulder;University of Colorado, Boulder;",44;44;-1,100;100;-1,8
2010,2010,2010,2010,2010,2010,2010,2010,ICLR,2019,NETWORK COMPRESSION USING CORRELATION ANALYSIS OF LAYER RESPONSES,Xavier Suau;Luca Zappella;Nicholas Apostoloff,xsuaucuadros@apple.com;lzappella@apple.com;napostoloff@apple.com,6;5;5,3;4;5,Reject,0,7,1.0,yes,9/27/18,Apple;Apple;Apple,-1;-1;-1,-1;-1;-1,1
2011,2011,2011,2011,2011,2011,2011,2011,ICLR,2019,LIT: Block-wise Intermediate Representation Training for Model Compression,Animesh Koratana*;Daniel Kang*;Peter Bailis;Matei Zaharia,koratana@stanford.edu;ddkang@stanford.edu;pbailis@cs.stanford.edu;matei@cs.stanford.edu,5;6;6,4;4;3,Reject,0,7,0.0,yes,9/27/18,Stanford University;Stanford University;Stanford University;Stanford University,4;4;4;4,3;3;3;3,5
2012,2012,2012,2012,2012,2012,2012,2012,ICLR,2019,NLProlog: Reasoning with Weak Unification for Natural Language Question Answering,Leon Weber;Pasquale Minervini;Ulf Leser;Tim Rocktäschel,leonweber@posteo.de;p.minervini@gmail.com;leser@informatik.hu-berlin.de;tim.rocktaeschel@gmail.com,5;7;7,3;3;4,Reject,0,5,0.0,yes,9/27/18,Humboldt Universität Berlin;University College London;Humboldt Universität Berlin;Facebook AI Research,261;50;261;-1,62;16;62;-1,3
2013,2013,2013,2013,2013,2013,2013,2013,ICLR,2019,Revisiting Reweighted Wake-Sleep,Tuan Anh Le;Adam R. Kosiorek;N. Siddharth;Yee Whye Teh;Frank Wood,tuananh@robots.ox.ac.uk;adamk@robots.ox.ac.uk;nsid@robots.ox.ac.uk;y.w.teh@stats.ox.ac.uk;fwood@cs.ubc.ca,6;6;5,4;3;3,Reject,0,10,0.0,yes,9/27/18,University of Oxford;University of Oxford;University of Oxford;University of Oxford;University of British Columbia,50;50;50;50;36,1;1;1;1;34,5
2014,2014,2014,2014,2014,2014,2014,2014,ICLR,2019,Looking for ELMo's friends: Sentence-Level Pretraining Beyond Language Modeling,Samuel R. Bowman;Ellie Pavlick;Edouard Grave;Benjamin Van Durme;Alex Wang;Jan Hula;Patrick Xia;Raghavendra Pappagari;R. Thomas McCoy;Roma Patel;Najoung Kim;Ian Tenney;Yinghui Huang;Katherin Yu;Shuning Jin;Berlin Chen,bowman@nyu.edu;ellie_pavlick@brown.edu;egrave@fb.com;vandurme@cs.jhu.edu;alexwang@nyu.edu;jan.hula21@gmail.com;paxia@jhu.edu;raghu1991.p@gmail.com;tom.mccoy@jhu.edu;romapatel@brown.edu;n.kim@jhu.edu;iftenney@google.com;huangyi@us.ibm.com;yukatherin@fb.com;jinxx596@d.umn.edu;bchen6@swarthmore.edu,5;7;8,3;4;4,Reject,0,3,2.0,yes,9/27/18,"New York University;Brown University;Facebook;Johns Hopkins University;New York University;;Johns Hopkins University;Johns Hopkins University;Johns Hopkins University;Brown University;Johns Hopkins University;Google;International Business Machines;Facebook;University of Minnesota, Minneapolis;Swarthmore College",26;65;-1;72;26;-1;72;72;72;65;72;-1;-1;-1;57;478,27;50;-1;13;27;-1;13;13;13;50;13;-1;-1;-1;56;1103,3
2015,2015,2015,2015,2015,2015,2015,2015,ICLR,2019,Estimating Information Flow in DNNs,Ziv Goldfeld;Ewout van den Berg;Kristjan Greenewald;Brian Kingsbury;Igor Melnyk;Nam Nguyen;Yury Polyanskiy,zivg@mit.edu;evandenberg@us.ibm.com;kristjan.h.greenewald@ibm.com;bedk@us.ibm.com;igor.melnyk@ibm.com;nnguyen@us.ibm.com;yp@mit.edu,7;7;4,4;4;5,Reject,0,9,0.0,yes,9/27/18,Massachusetts Institute of Technology;International Business Machines;International Business Machines;International Business Machines;International Business Machines;International Business Machines;Massachusetts Institute of Technology,2;-1;-1;-1;-1;-1;2,5;-1;-1;-1;-1;-1;5,
2016,2016,2016,2016,2016,2016,2016,2016,ICLR,2019,Approximation and non-parametric estimation of ResNet-type convolutional neural networks via block-sparse fully-connected neural networks,Kenta Oono;Taiji Suzuki,k.oono.delta@gmail.com;taiji@mist.i.u-tokyo.ac.jp,4;6;4,4;3;3,Reject,0,6,0.0,yes,9/27/18,The University of Tokyo;The University of Tokyo,54;54,45;45,1
2017,2017,2017,2017,2017,2017,2017,2017,ICLR,2019,Learning to Reinforcement Learn by Imitation,Rosen Kralev;Russell Mendonca;Alvin Zhang;Tianhe Yu;Abhishek Gupta;Pieter Abbeel;Sergey Levine;Chelsea Finn,rdkralev@gmail.com;russellm@berkeley.edu;alvinz@berkeley.edu;tianheyu927@gmail.com;abhigupta@berkeley.edu;pabbeel@cs.berkeley.edu;svlevine@eecs.berkeley.edu;cbfinn@eecs.berkeley.edu,4;3;2;5,3;2;5;2,Reject,0,0,0.0,yes,9/27/18,;University of California Berkeley;University of California Berkeley;Stanford University;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,-1;5;5;4;5;5;5;5,-1;18;18;3;18;18;18;18,6
2018,2018,2018,2018,2018,2018,2018,2018,ICLR,2019,Non-Synergistic Variational Autoencoders,Gonzalo Barrientos;Sten Sootla,gonzalo.ayquipa.16@ucl.ac.uk;sten.sootla.17@ucl.ac.uk,3;4;3,4;3;5,Reject,0,0,0.0,yes,9/27/18,University College London;University College London,50;50,16;16,5;1
2019,2019,2019,2019,2019,2019,2019,2019,ICLR,2019,On the Trajectory of Stochastic Gradient Descent in the Information Plane,Emilio Rafael Balda;Arash Behboodi;Rudolf Mathar,emilio.balda@ti.rwth-aachen.de;arash.behboodi@ti.rwth-aachen.de;mathar@ti.rwth-aachen.de,4;6;2,3;4;4,Reject,0,5,0.0,yes,9/27/18,RWTH Aachen University;RWTH Aachen University;RWTH Aachen University,99;99;99,79;79;79,
2020,2020,2020,2020,2020,2020,2020,2020,ICLR,2019,Adversarial Vulnerability of Neural Networks Increases with Input Dimension,Carl-Johann Simon-Gabriel;Yann Ollivier;Léon Bottou;Bernhard Schölkopf;David Lopez-Paz,cjsimon@tuebingen.mpg.de;yol@fb.com;leon@bottou.org;bs@tuebingen.mpg.de;dlp@fb.com,6;4;9;5,4;5;4;5,Reject,0,20,2.0,yes,9/27/18,"Max Planck Institute for Intelligent Systems, Max-Planck Institute;Facebook;Facebook;Max Planck Institute for Intelligent Systems, Max-Planck Institute;Facebook",-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,4;1
2021,2021,2021,2021,2021,2021,2021,2021,ICLR,2019,CoT: Cooperative Training for Generative Modeling of Discrete Data,Sidi Lu;Lantao Yu;Siyuan Feng;Yaoming Zhu;Weinan Zhang;Yong Yu,steve_lu@apex.sjtu.edu.cn;yulantao@apex.sjtu.edu.cn;siyuanfeng@apex.sjtu.edu;ymzhu@apex.sjtu.edu.cn;wnzhang@apex.sjtu.edu.cn;yyu@apex.sjtu.edu.cn,6;7;7,4;2;2,Reject,9,21,0.0,yes,9/27/18,Shanghai Jiao Tong University;Shanghai Jiao Tong University;Shanghai Jiao Tong University;Shanghai Jiao Tong University;Shanghai Jiao Tong University;Shanghai Jiao Tong University,52;52;52;52;52;52,188;188;188;188;188;188,5;8
2022,2022,2022,2022,2022,2022,2022,2022,ICLR,2019,Direct Optimization through $\arg \max$  for  Discrete Variational Auto-Encoder,Guy Lorberbom;Tamir Hazan,guy_lorber@campus.technion.ac.il;tamir.hazan@technion.ac.il,7;5;7,4;4;4,Reject,0,5,1.0,yes,9/27/18,Technion;Technion,25;25,327;327,
2023,2023,2023,2023,2023,2023,2023,2023,ICLR,2019,Sinkhorn AutoEncoders,Giorgio Patrini;Marcello Carioni;Patrick Forré;Samarth Bhargav;Max Welling;Rianne van den Berg;Tim Genewein;Frank Nielsen,patrinig@hotmail.com;marcello.carioni@uni-graz.at;patrickforre@gmail.com;samarth.bhargav@student.uva.nl;welling.max@gmail.com;riannevdberg@gmail.com;tim.genewein@de.bosch.com;nielsen@lix.polytechnique.fr,7;5;6;7,3;4;3;3,Reject,0,7,0.0,yes,9/27/18,"University of Amsterdam;Karl-Franzens University Graz;University of Amsterdam;University of Amsterdam;University of California - Irvine;University of Amsterdam;Bosch;Ecole Polytechnique, France",169;478;169;169;35;169;-1;478,59;1103;59;59;99;59;-1;115,5;1
2024,2024,2024,2024,2024,2024,2024,2024,ICLR,2019,Structured Prediction using cGANs with Fusion Discriminator,Faisal Mahmood;Wenhao Xu;Nicholas J. Durr;Jeremiah W. Johnson;Alan Yuille,faisalm@jhu.edu;wxu47@jhu.edu;ndurr@jhu.edu;jeremiah.johnson@unh.edu;alan.l.yuille@gmail.com,5;3;3,3;4;4,Reject,0,3,0.0,yes,9/27/18,Johns Hopkins University;Johns Hopkins University;Johns Hopkins University;University of New Hampshire;Johns Hopkins University,72;72;72;261;72,13;13;13;1103;13,5;4;2
2025,2025,2025,2025,2025,2025,2025,2025,ICLR,2019,Evaluating GANs via Duality,Paulina Grnarova;Kfir Y Levy;Aurelien Lucchi;Nathanael Perraudin;Thomas Hofmann;Andreas Krause,paulina.grnarova@inf.ethz.ch;yehuda.levy@inf.ethz.ch;aurelien.lucchi@inf.ethz.ch;nathanael.perraudin@sdsc.ethz.ch;thomas.hofmann@inf.ethz.ch;krausea@ethz.ch,4;5;3,3;3;4,Reject,0,12,0.0,yes,9/27/18,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10;10;10;10,10;10;10;10;10;10,5;4
2026,2026,2026,2026,2026,2026,2026,2026,ICLR,2019,Hyper-Regularization: An Adaptive Choice for the Learning Rate in Gradient Descent,Guangzeng Xie;Hao Jin;Dachao Lin;Zhihua Zhang,smsxgz@pku.edu.cn;jin.hao@pku.edu.cn;lindachao@pku.edu.cn;zhzhang@math.pku.edu.cn,4;4;4,5;4;4,Reject,0,5,0.0,yes,9/27/18,Peking University;Peking University;Peking University;Peking University,24;24;24;24,27;27;27;27,9
2027,2027,2027,2027,2027,2027,2027,2027,ICLR,2019,Wasserstein proximal of GANs,Alex Tong Lin;Wuchen Li;Stanley Osher;Guido Montufar,atlin@math.ucla.edu;wcli@math.ucla.edu;sjo@math.ucla.edu;montufar@math.ucla.edu,3;6;4,5;3;3,Reject,0,5,0.0,yes,9/27/18,"University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles",20;20;20;20,15;15;15;15,5
2028,2028,2028,2028,2028,2028,2028,2028,ICLR,2019,On the Convergence and Robustness of Batch Normalization,Yongqiang Cai;Qianxiao Li;Zuowei Shen,matcyon@nus.edu.sg;liqix@ihpc.a-star.edu.sg;matzuows@nus.edu.sg,4;6;4,5;3;3,Reject,0,9,0.0,yes,9/27/18,"National University of Singapore;Institute of High Performance Computing, Singapore, A*STAR;National University of Singapore",16;-1;16,22;-1;22,4
2029,2029,2029,2029,2029,2029,2029,2029,ICLR,2019,Aligning Artificial Neural Networks to the Brain yields Shallow Recurrent Architectures,Jonas Kubilius;Martin Schrimpf;Ha Hong;Najib J. Majaj;Rishi Rajalingham;Elias B. Issa;Kohitij Kar;Pouya Bashivan;Jonathan Prescott-Roy;Kailyn Schmidt;Aran Nayebi;Daniel Bear;Daniel L. K. Yamins;James J. DiCarlo,qbilius@mit.edu;msch@mit.edu;dicarlo@mit.edu,7;7;5,4;4;4,Reject,0,11,0.0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2,5;5;5,
2030,2030,2030,2030,2030,2030,2030,2030,ICLR,2019,Finding Mixed Nash Equilibria of Generative Adversarial Networks,Ya-Ping Hsieh;Chen Liu;Volkan Cevher,ya-ping.hsieh@epfl.ch;chen.liu@epfl.ch;volkan.cevher@epfl.ch,6;4;5,4;4;5,Reject,0,10,0.0,yes,9/27/18,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne,478;478;478,38;38;38,5;4;1;9
2031,2031,2031,2031,2031,2031,2031,2031,ICLR,2019,Local Stability and Performance of Simple Gradient Penalty $\mu$-Wasserstein GAN,Cheolhyeong Kim;Seungtae Park;Hyung Ju Hwang,tyty4@postech.ac.kr;swash21@postech.ac.kr;hjhwang@postech.ac.kr,6;5;4,4;4;3,Reject,0,6,0.0,yes,9/27/18,POSTECH;POSTECH;POSTECH,123;123;123,137;137;137,5;1
2032,2032,2032,2032,2032,2032,2032,2032,ICLR,2019,Learning Discriminators as Energy Networks in Adversarial Learning,Pingbo Pan;Yan Yan;Tianbao Yang;Yi Yang,pingbo.pan@student.uts.edu.au;yan.yan-3@student.uts.edu.au;tianbao-yang@uiowa.edu;yi.yang@uts.edu.au,5;5;5,4;5;4,Reject,5,5,0.0,yes,9/27/18,University of Technology Sydney;University of Technology Sydney;University of Iowa;University of Technology Sydney,106;106;153;106,216;216;223;216,4;2
2033,2033,2033,2033,2033,2033,2033,2033,ICLR,2019,Optimistic Acceleration for Optimization,Jun-Kun Wang;Xiaoyun Li;Ping Li,jimwang@gatech.edu;xl374@scarletmail.rutgers.edu;pingli98@gmail.com,5;6;5;4,4;2;4;4,Reject,0,7,0.0,yes,9/27/18,Georgia Institute of Technology;Rutgers University;Rutgers University New Brunswick,13;34;34,33;172;172,
2034,2034,2034,2034,2034,2034,2034,2034,ICLR,2019,Rethinking learning rate schedules for stochastic optimization,Rong Ge;Sham M. Kakade;Rahul Kidambi;Praneeth Netrapalli,rongge@cs.duke.edu;sham@cs.washington.edu;rkidambi@uw.edu;praneeth@microsoft.com,6;4;6,4;4;4,Reject,0,7,0.0,yes,9/27/18,"Duke University;University of Washington;University of Washington, Seattle;Microsoft",44;6;6;-1,17;25;25;-1,9
2035,2035,2035,2035,2035,2035,2035,2035,ICLR,2019,On the Ineffectiveness of Variance Reduced Optimization for Deep Learning,Aaron Defazio,aaron.defazio@gmail.com,5;6;5,5;3;4,Reject,0,9,0.0,yes,9/27/18,Facebook,-1,-1,9
2036,2036,2036,2036,2036,2036,2036,2036,ICLR,2019,EDDI: Efficient Dynamic Discovery of High-Value Information with Partial VAE,Chao Ma;Sebastian Tschiatschek;Konstantina Palla;Jose Miguel Hernandez Lobato;Sebastian Nowozin;Cheng Zhang,cm905@cam.ac.uk;sebastian.tschiatschek@microsoft.com;konstantina.palla@microsoft.com;jmh233@cam.ac.uk;sebastian.nowozin@microsoft.com;cheng.zhang@microsoft.com,6;5;6,2;4;4,Reject,0,5,0.0,yes,9/27/18,University of Cambridge;Microsoft;Microsoft;University of Cambridge;Microsoft;Microsoft,71;-1;-1;71;-1;-1,2;-1;-1;2;-1;-1,5;11
2037,2037,2037,2037,2037,2037,2037,2037,ICLR,2019,"Search-Guided, Lightly-supervised Training of  Structured Prediction Energy Networks",Amirmohammad Rooshenas;Dongxu Zhang;Gopal Sharma;Andrew McCallum,pedram@cs.umass.edu;dongxuzhang@cs.umass.edu;gopalsharma@cs.umass.edu;mccallum@cs.umass.edu,5;7;4,4;4;4,Reject,0,3,0.0,yes,9/27/18,"University of Massachusetts, Amherst;University of Massachusetts, Amherst;University of Massachusetts, Amherst;University of Massachusetts, Amherst",30;30;30;30,191;191;191;191,
2038,2038,2038,2038,2038,2038,2038,2038,ICLR,2019,Distinguishability of Adversarial Examples,Yi Qin;Ryan Hunt;Chuan Yue,yiqin@mines.edu;ryhunt@mines.edu;chuanyue@mines.edu,4;4;4,4;5;4,Reject,1,2,0.0,yes,9/27/18,Colorado School of Mines;Colorado School of Mines;Colorado School of Mines,169;169;169,268;268;268,4;2
2039,2039,2039,2039,2039,2039,2039,2039,ICLR,2019,On the Geometry of Adversarial Examples,Marc Khoury;Dylan Hadfield-Menell,khoury@eecs.berkeley.edu;dhm@berkeley.edu,5;3;6,3;4;4,Reject,0,13,0.0,yes,9/27/18,University of California Berkeley;University of California Berkeley,5;5,18;18,4;1
2040,2040,2040,2040,2040,2040,2040,2040,ICLR,2019,Partially Mutual Exclusive Softmax for Positive and Unlabeled data,Ugo Tanielian;Flavian vasile;Mike Gartrell,u.tanielian@criteo.com;f.vasile@criteo.com;m.gartrell@criteo.com,5;4;5,4;4;4,Reject,0,2,0.0,yes,9/27/18,Criteo;Criteo;Criteo,-1;-1;-1,-1;-1;-1,3;4
2041,2041,2041,2041,2041,2041,2041,2041,ICLR,2019,Learning Global Additive Explanations for Neural Nets Using Model Distillation,Sarah Tan;Rich Caruana;Giles Hooker;Paul Koch;Albert Gordo,ht395@cornell.edu;rcaruana@microsoft.com;gjh27@cornell.edu;paulkoch@microsoft.com;albert.gordo.s@gmail.com,6;4;6,5;5;4,Reject,0,7,0.0,yes,9/27/18,Cornell University;Microsoft;Cornell University;Microsoft;Facebook,7;-1;7;-1;-1,19;-1;19;-1;-1,
2042,2042,2042,2042,2042,2042,2042,2042,ICLR,2019,Predicted Variables in Programming,Victor Carbune;Thierry Coppey;Alexander Daryin;Thomas Deselaers;Nikhil Sarda;Jay Yagnik,victor.carbune@gmail.com;thierryc@google.com;shurick@google.com;deselaers@google.com;nikhilsarda@google.com;jyagnik@google.com,5;5;7,3;3;3,Reject,0,3,0.0,yes,9/27/18,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,
2043,2043,2043,2043,2043,2043,2043,2043,ICLR,2019,Learning Abstract Models for Long-Horizon Exploration,Evan Zheran Liu;Ramtin Keramati;Sudarshan Seshadri;Kelvin Guu;Panupong Pasupat;Emma Brunskill;Percy Liang,evanliu@cs.stanford.edu;keramati@stanford.edu;ssesha@stanford.edu;kguu@stanford.edu;ppasupat@cs.stanford.edu;ebrun@cs.stanford.edu;pliang@cs.stanford.edu,6;5;4,2;4;4,Reject,0,10,0.0,yes,9/27/18,Stanford University;Stanford University;Stanford University;Stanford University;Stanford University;Stanford University;Stanford University,4;4;4;4;4;4;4,3;3;3;3;3;3;3,
2044,2044,2044,2044,2044,2044,2044,2044,ICLR,2019,Domain Adaptation for Structured Output via Disentangled Patch Representations,Yi-Hsuan Tsai;Kihyuk Sohn;Samuel Schulter;Manmohan Chandraker,wasidennis@gmail.com;kihyuk.sohn@gmail.com;samuel@nec-labs.com;manu@nec-labs.com,5;7;5,5;5;4,Reject,2,7,1.0,yes,9/27/18,NEC-Labs;NEC-Labs;NEC-Labs;NEC-Labs,-1;-1;-1;-1,-1;-1;-1;-1,4;2
2045,2045,2045,2045,2045,2045,2045,2045,ICLR,2019,Meta-Learning Neural Bloom Filters,Jack W Rae;Sergey Bartunov;Timothy P Lillicrap,jwrae@google.com;bartunov@google.com;countzero@google.com,7;6;3,3;4;1,Reject,0,10,0.0,yes,9/27/18,Google;Google;Google,-1;-1;-1,-1;-1;-1,6
2046,2046,2046,2046,2046,2046,2046,2046,ICLR,2019,The Universal Approximation Power of Finite-Width Deep ReLU Networks,Dmytro Perekrestenko;Philipp Grohs;Dennis Elbrächter;Helmut Bölcskei,pdmytro@nari.ee.ethz.ch;philipp.grohs@univie.ac.at;dennis.elbraechter@univie.ac.at;boelcskei@nari.ee.ethz.ch,5;5;6,3;4;3,Reject,0,5,0.0,yes,9/27/18,Swiss Federal Institute of Technology;University of Vienna;University of Vienna;Swiss Federal Institute of Technology,10;199;199;10,10;164;164;10,1
2047,2047,2047,2047,2047,2047,2047,2047,ICLR,2019,Transfer Learning for Related Reinforcement Learning Tasks via Image-to-Image Translation,Shani Gamrian;Yoav Goldberg,gamrianshani@gmail.com;yoav.goldberg@gmail.com,4;7;7,4;3;3,Reject,0,5,0.0,yes,9/27/18,Bar Ilan University;Bar-Ilan University,95;95,456;456,5;4;6
2048,2048,2048,2048,2048,2048,2048,2048,ICLR,2019,Guiding Physical Intuition with Neural Stethoscopes,Fabian Fuchs;Oliver Groth;Adam Kosiorek;Alex Bewley;Markus Wulfmeier;Andrea Vedaldi;Ingmar Posner,fabian@robots.ox.ac.uk;ogroth@robots.ox.ac.uk;adamk@robots.ox.ac.uk;alex.bewley@gmail.com;m.wulfmeier@gmail.com;vedaldi@robots.ox.ac.uk;ingmar@robots.ox.ac.uk,6;4;7,4;3;3,Reject,0,3,0.0,yes,9/27/18,University of Oxford;University of Oxford;University of Oxford;Wayve Technologies;Google;University of Oxford;University of Oxford,50;50;50;-1;-1;50;50,1;1;1;-1;-1;1;1,4
2049,2049,2049,2049,2049,2049,2049,2049,ICLR,2019,An Adversarial Learning Framework for a Persona-based Multi-turn Dialogue Model,Oluwatobi O. Olabiyi;Anish Khazane;Alan Salimov;Erik T.Mueller,oluwatobi.olabiyi@capitalone.com;anish.khazan@capitalone.com;alan.salimov@capitalone.com;erik.mueller@capitalone.com,5;4;6,4;4;3,Reject,0,6,0.0,yes,9/27/18,Capital One Bank;Capital One Bank;Capital One Bank;Capital One Bank,-1;-1;-1;-1,-1;-1;-1;-1,4
2050,2050,2050,2050,2050,2050,2050,2050,ICLR,2019,Adaptive Sample-space & Adaptive Probability coding: a neural-network based approach for compression,Ken Nakanishi;Shin-ichi Maeda;Takeru Miyato;Masanori Koyama,ikyhn1.ken.n@gmail.com;ichi@preferred.jp;miyato@preferred.jp;masomatics@preferred.jp,5;7;5,4;3;4,Reject,0,5,0.0,yes,9/27/18,"The University of Tokyo;Preferred Networks, Inc.;Preferred Networks, Inc.;Preferred Networks, Inc.",54;-1;-1;-1,45;-1;-1;-1,
2051,2051,2051,2051,2051,2051,2051,2051,ICLR,2019,Seq2Slate: Re-ranking and Slate Optimization with RNNs,Irwan Bello;Sayali Kulkarni;Sagar Jain;Craig Boutilier;Ed Chi;Elad Eban;Xiyang Luo;Alan Mackey;Ofer Meshi,ibello@google.com;sayali@google.com;sagarj@google.com;cboutilier@google.com;edchi@google.com;elade@google.com;xyluo@google.com;mackeya@google.com;meshi@google.com,7;6;6,5;4;4,Reject,0,5,0.0,yes,9/27/18,Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1,
2052,2052,2052,2052,2052,2052,2052,2052,ICLR,2019,An Efficient and Margin-Approaching Zero-Confidence Adversarial Attack,Yang Zhang;Shiyu Chang;Mo Yu;Kaizhi Qian,yang.zhang2@ibm.com;shiyu.chang@ibm.com;yum@us.ibm.com;kqian3@illinois.edu,6;5;5,5;3;4,Reject,8,15,0.0,yes,9/27/18,"International Business Machines;International Business Machines;International Business Machines;University of Illinois, Urbana Champaign",-1;-1;-1;3,-1;-1;-1;37,4
2053,2053,2053,2053,2053,2053,2053,2053,ICLR,2019,Neural Causal Discovery with Learnable Input Noise,Tailin Wu;Thomas Breuel;Jan Kautz,tailin@mit.edu;tbreuel@nvidia.com;jkautz@nvidia.com,4;4;8,5;4;4,Reject,0,3,0.0,yes,9/27/18,Massachusetts Institute of Technology;NVIDIA;NVIDIA,2;-1;-1,5;-1;-1,
2054,2054,2054,2054,2054,2054,2054,2054,ICLR,2019,Dense Morphological Network: An Universal Function Approximator,Ranjan Mondal;Sanchayan Santra;Bhabatosh Chanda,ranjan.rev@gmail.com;sanchayan_r@isical.ac.in;chanda@isical.ac.in,5;5;5,3;4;5,Reject,2,9,0.0,yes,9/27/18,"Indian Statistical Institute, Kolkata;Indian Statistical Institute, Kolkata;Indian Statistical Institute, Kolkata",478;478;478,1103;1103;1103,
2055,2055,2055,2055,2055,2055,2055,2055,ICLR,2019,Filter Training and Maximum Response: Classification via Discerning,Lei Gu,gul2@uci.edu,2;3;6,1;4;3,Reject,0,0,0.0,yes,9/27/18,"University of California, Irvine",35,99,
2056,2056,2056,2056,2056,2056,2056,2056,ICLR,2019,BEHAVIOR MODULE IN NEURAL NETWORKS,Andrey Sakryukin;Yongkang Wong;Mohan S. Kankanhalli,asakryukin@u.nus.edu;yongkang.wong@nus.edu.sg;mohan@comp.nus.edu.sg,3;3;4,4;5;5,Reject,0,0,0.0,yes,9/27/18,National University of Singapore;National University of Singapore;National University of Singapore,16;16;16,22;22;22,
2057,2057,2057,2057,2057,2057,2057,2057,ICLR,2019,Surprising Negative Results for Generative  Adversarial Tree Search ,Kamyar Azizzadenesheli;Brandon Yang;Weitang Liu;Emma Brunskill;Zachary Lipton;Animashree Anandkumar,kazizzad@uci.edu;bcyang@stanford.edu;wetliu@ucdavis.edu;ebrun@cs.stanford.edu;zlipton@cmu.edu;anima@caltech.edu,5;5;6,3;4;2,Reject,0,4,0.0,yes,9/27/18,"University of California, Irvine;Stanford University;University of California, Davis;Stanford University;Carnegie Mellon University;California Institute of Technology",35;4;81;4;1;140,99;3;54;3;24;3,5;4
2058,2058,2058,2058,2058,2058,2058,2058,ICLR,2019,Evaluation Methodology for Attacks Against Confidence Thresholding Models,Ian Goodfellow;Yao Qin;David Berthelot,goodfellow@google.com;yaoqin@google.com;dberth@google.com,2;3;4,4;3;4,Reject,0,1,0.0,yes,9/27/18,Google;Google;Google,-1;-1;-1,-1;-1;-1,4
2059,2059,2059,2059,2059,2059,2059,2059,ICLR,2019,Featurized Bidirectional GAN: Adversarial Defense via Adversarially Learned Semantic Inference,Ruying Bao;Sihang Liang;Qingcan Wang,rbao@princeton.edu;sihangl@princeton.edu;qingcanw@princeton.edu,4;3;3,5;4;4,Reject,0,4,0.0,yes,9/27/18,Princeton University;Princeton University;Princeton University,30;30;30,7;7;7,5;4
2060,2060,2060,2060,2060,2060,2060,2060,ICLR,2019,ATTACK GRAPH CONVOLUTIONAL NETWORKS BY ADDING FAKE NODES,Xiaoyun Wang;Joe Eaton;Cho-Jui Hsieh;Felix Wu,xiywang@ucdavis.edu;featon@nvidia.com;chohsieh@ucdavis.edu;sfwu@ucdavis.edu,4;3;3,3;4;2,Reject,0,0,0.0,yes,9/27/18,"University of California, Davis;NVIDIA;University of California, Davis;University of California, Davis",81;-1;81;81,54;-1;54;54,4;10
2061,2061,2061,2061,2061,2061,2061,2061,ICLR,2019,Stochastic Quantized Activation: To prevent Overfitting in Fast Adversarial Training,Wonjun Yoon;Jisuk Park;Daeshik Kim,wonjun.yoon@kaist.ac.kr;ssuk30@kaist.ac.kr;daeshik@kaist.ac.kr,4;5;4,3;5;4,Reject,0,0,0.0,yes,9/27/18,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,20;20;20,95;95;95,4
2062,2062,2062,2062,2062,2062,2062,2062,ICLR,2019,BlackMarks: Black-box Multi-bit Watermarking for Deep Neural Networks,Huili Chen;Bita Darvish Rouhani;Farinaz Koushanfar,huc044@ucsd.edu;bita@ucsd.edu;farinaz@ucsd.edu,5;4;4,3;4;4,Reject,0,0,0.0,yes,9/27/18,"University of California, San Diego;University of California, San Diego;University of California, San Diego",11;11;11,31;31;31,4
2063,2063,2063,2063,2063,2063,2063,2063,ICLR,2019,Purchase as Reward : Session-based  Recommendation by Imagination Reconstruction,Qibing Li;Xiaolin Zheng,qblee@zju.edu.cn;xlzheng@zju.edu.cn,5;6;5,3;2;5,Reject,0,3,0.0,yes,9/27/18,Zhejiang University;Zhejiang University,57;57,177;177,
2064,2064,2064,2064,2064,2064,2064,2064,ICLR,2019,Optimal Attacks against Multiple Classifiers,Juan C. Perdomo;Yaron Singer,jcperdomo@berkeley.edu;yaron@seas.harvard.edu,5;6;4;6,4;3;4;4,Reject,0,5,0.0,yes,9/27/18,University of California Berkeley;Harvard University,5;39,18;6,4
2065,2065,2065,2065,2065,2065,2065,2065,ICLR,2019,Label Propagation Networks,Kojin Oshiba;Nir Rosenfeld;Amir Globerson,kojinoshiba@college.harvard.edu;nirr@g.harvard.edu;amir.globerson@gmail.com,5;5;6,4;2;4,Reject,5,4,0.0,yes,9/27/18,Harvard University;Harvard University;Tel Aviv University,39;39;37,6;6;217,10
2066,2066,2066,2066,2066,2066,2066,2066,ICLR,2019,Prob2Vec: Mathematical Semantic Embedding for Problem Retrieval in Adaptive Tutoring,Du Su;Ali Yekkehkhany;Yi Lu;Wenmiao Lu,dusu3@illinois.edu;yekkehk2@illinois.edu;yilu4@illinois.edu;wenmiao.lu@gmail.com,3;5;4,3;3;4,Reject,2,5,0.0,yes,9/27/18,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;",3;3;3;-1,37;37;37;-1,
2067,2067,2067,2067,2067,2067,2067,2067,ICLR,2019,Visualizing and Understanding the Semantics of Embedding Spaces via Algebraic Formulae,Piero Molino;Yang Wang;Jiawei Zhang,piero@uber.com;gnavvy@uber.com;rivulet.zhang@gmail.com,3;3;4,4;3;3,Reject,0,6,0.0,yes,9/27/18,Uber;Uber;Purdue University,-1;-1;26,-1;-1;60,3
2068,2068,2068,2068,2068,2068,2068,2068,ICLR,2019,Cross-Entropy Loss Leads To Poor Margins,Kamil Nar;Orhan Ocal;S. Shankar Sastry;Kannan Ramchandran,nar@berkeley.edu;ocal@eecs.berkeley.edu;sastry@eecs.berkeley.edu;kannanr@eecs.berkeley.edu,3;4;5;8;5,4;5;4;3;4,Reject,1,8,0.0,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5,18;18;18;18,4
2069,2069,2069,2069,2069,2069,2069,2069,ICLR,2019,A Case for Object Compositionality in Deep Generative Models of Images,Sjoerd van Steenkiste;Karol Kurach;Sylvain Gelly,sjoerd@idsia.ch;kkurach@gmail.com;sylvain.gelly@gmail.com,5;4;6,5;5;4,Reject,0,6,0.0,yes,9/27/18,IDSIA;Google;Google,-1;-1;-1,-1;-1;-1,5
2070,2070,2070,2070,2070,2070,2070,2070,ICLR,2019,Object-Oriented Model Learning through Multi-Level Abstraction,Guangxiang Zhu;Jianhao Wang;ZhiZhou Ren;Chongjie Zhang,guangxiangzhu@outlook.com;jh-wang15@mails.tsinghua.edu.cn;rzz16@mails.tsinghua.edu.cn;chongjie@tsinghua.edu.cn,4;4;6,4;3;3,Reject,0,9,0.0,yes,9/27/18,Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University,8;8;8;8,30;30;30;30,8
2071,2071,2071,2071,2071,2071,2071,2071,ICLR,2019,IB-GAN: Disentangled Representation Learning with Information Bottleneck GAN,Insu Jeon;Wonkwang Lee;Gunhee Kim,isjeon@vision.snu.ac.kr;wonkwang.lee.94@gmail.com;gunhee@snu.ac.kr,7;7;4,3;4;4,Reject,0,7,1.0,yes,9/27/18,Seoul National University;Hanyang University;Seoul National University,41;228;41,74;377;74,5
2072,2072,2072,2072,2072,2072,2072,2072,ICLR,2019,ACE: Artificial Checkerboard Enhancer to Induce and Evade Adversarial Attacks,Jisung Hwang;Younghoon Kim;Sanghyuk Chun;Jaejun Yoo;Ji-Hoon Kim;Dongyoon Han;Jung-Woo Ha,jeshwang92@uchicago.edu;snu13dlx@snu.ac.kr;sanghyuk.c@navercorp.com;jaejun.yoo@navercorp.com;genesis.kim@navercorp.com;dongyoon.han@navercorp.com;jungwoo.ha@navercorp.com,4;4;6,2;3;1,Reject,0,6,0.0,yes,9/27/18,University of Chicago;Seoul National University;NAVER;NAVER;NAVER;NAVER;NAVER,48;41;-1;-1;-1;-1;-1,9;74;-1;-1;-1;-1;-1,4;2
2073,2073,2073,2073,2073,2073,2073,2073,ICLR,2019,Second-Order Adversarial Attack and Certifiable Robustness,Bai Li;Changyou Chen;Wenlin Wang;Lawrence Carin,bai.li@duke.edu;cchangyou@gmail.com;wenlin.wang@duke.edu;lcarin@duke.edu,4;5;3,5;3;5,Reject,0,6,0.0,yes,9/27/18,"Duke University;State University of New York, Buffalo;Duke University;Duke University",44;81;44;44,17;270;17;17,4;1
2074,2074,2074,2074,2074,2074,2074,2074,ICLR,2019,Learning Heuristics for Automated Reasoning through Reinforcement Learning,Gil Lederman;Markus N. Rabe;Edward A. Lee;Sanjit A. Seshia,gilled@berkeley.edu;markus.norman.rabe@gmail.com;eal@berkeley.edu;sseshia@eecs.berkeley.edu,5;6;7,3;4;4,Reject,0,6,0.0,yes,9/27/18,University of California Berkeley;Google;University of California Berkeley;University of California Berkeley,5;-1;5;5,18;-1;18;18,
2075,2075,2075,2075,2075,2075,2075,2075,ICLR,2019,Strength in Numbers: Trading-off Robustness and Computation via Adversarially-Trained Ensembles,Edward Grefenstette;Robert Stanforth;Brendan O'Donoghue;Jonathan Uesato;Grzegorz Swirszcz;Pushmeet Kohli,etg@google.com;stanforth@google.com;bodonoghue@google.com;juesato@google.com;swirszcz@google.com;pushmeet@google.com,5;6;4,4;3;3,Reject,0,5,0.0,yes,9/27/18,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,4
2076,2076,2076,2076,2076,2076,2076,2076,ICLR,2019,Explaining Adversarial Examples with Knowledge Representation,Xingyu Zhou;Tengyu Ma;Huahong Zhang,xingyu.zhou@vanderbilt.edu;tengyu.ma@vanderbilt.edu;huahong.zhang@vanderbilt.edu,3;3;2,4;2;5,Reject,0,0,0.0,yes,9/27/18,Vanderbilt University;Vanderbilt University;Vanderbilt University,228;228;228,105;105;105,4
2077,2077,2077,2077,2077,2077,2077,2077,ICLR,2019,Cutting Down Training Memory by Re-fowarding,Jianwei Feng;Dong Huang,jfeng1@andrew.cmu.edu;donghuang@cmu.edu,6;4;4;6,3;3;2;3,Reject,0,6,0.0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University,1;1,24;24,10
2078,2078,2078,2078,2078,2078,2078,2078,ICLR,2019,Neural Networks with Structural Resistance to Adversarial Attacks,Luca de Alfaro,luca@ucsc.edu,7;5;5,4;3;3,Reject,2,1,0.0,yes,9/27/18,University of Southern California,30,66,4
2079,2079,2079,2079,2079,2079,2079,2079,ICLR,2019,Controlling Over-generalization and its Effect on Adversarial Examples Detection and Generation,Mahdieh Abbasi;Arezoo Rajabi;Azadeh Sadat Mozafari;Rakesh B. Bobba;Christian Gagné,mahdieh.abbasi.1@ulaval.ca;rajabia@oregonstate.edu;azadeh-sadat.mozafari.1@ulaval.ca;rakesh.bobba@oregonstate.edu;christian.gagne@gel.ulaval.ca,4;4;3,4;5;3,Reject,8,6,0.0,yes,9/27/18,Laval university;Oregon State University;Laval university;Oregon State University;Laval university,478;76;478;76;478,265;318;265;318;265,4;2;8
2080,2080,2080,2080,2080,2080,2080,2080,ICLR,2019,Robust Determinantal Generative Classifier for Noisy Labels and Adversarial Attacks,Kimin Lee;Sukmin Yun;Kibok Lee;Honglak Lee;Bo Li;Jinwoo Shin,kiminlee@kaist.ac.kr;sm3199@kaist.ac.kr;kibok@umich.edu;honglak@eecs.umich.edu;lxbosky@gmail.com;jinwoos@kaist.ac.kr,7;3;4,5;4;4,Reject,0,8,0.0,yes,9/27/18,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;University of Michigan;University of Michigan;University of California Berkeley;Korea Advanced Institute of Science and Technology,20;20;8;8;5;20,95;95;21;21;18;95,5;4
2081,2081,2081,2081,2081,2081,2081,2081,ICLR,2019,Single Shot Neural Architecture Search Via Direct Sparse Optimization,Xinbang Zhang;Zehao Huang;Naiyan Wang,xinbang.zhang@nlpr.ia.ac.cn;zehaohuang18@gmail.com;winsty@gmail.com,7;6;6,3;4;3,Reject,0,18,1.0,yes,9/27/18,"Institute of automation, Chinese academy of science, Chinese Academy of Sciences;;",62;-1;-1,1103;-1;-1,
2082,2082,2082,2082,2082,2082,2082,2082,ICLR,2019,How Training Data Affect the Accuracy and Robustness of Neural Networks for Image Classification,Suhua Lei;Huan Zhang;Ke Wang;Zhendong Su,sulei@ucdavis.edu;huan@huan-zhang.com;kewang@visa.com;zhendong.su@inf.ethz.ch,5;4;5,4;4;3,Reject,0,5,0.0,yes,9/27/18,"University of California, Davis;University of California, Los Angeles;VISA;Swiss Federal Institute of Technology",81;20;-1;10,54;15;-1;10,4
2083,2083,2083,2083,2083,2083,2083,2083,ICLR,2019,EFFICIENT TWO-STEP ADVERSARIAL DEFENSE FOR DEEP NEURAL NETWORKS,Ting-Jui Chang;Yukun He;Peng Li,tingjui.chang@tamu.edu;dominiche@tamu.edu;pli@tamu.edu,5;6;7,4;3;3,Reject,2,1,0.0,yes,9/27/18,Texas A&M;Texas A&M;Texas A&M,44;44;44,160;160;160,4
2084,2084,2084,2084,2084,2084,2084,2084,ICLR,2019,Adaptive Neural Trees,Ryutaro Tanno;Kai Arulkumaran;Daniel C. Alexander;Antonio Criminisi;Aditya Nori,ryutaro.tanno.15@ucl.ac.uk;kailash.arulkumaran13@imperial.ac.uk;d.alexander@ucl.ac.uk;antcrim@microsoft.com;adityan@microsoft.com,6;6;4,4;3;4,Reject,0,7,2.0,yes,9/27/18,University College London;Imperial College London;University College London;Microsoft;Microsoft,50;72;50;-1;-1,16;8;16;-1;-1,
2085,2085,2085,2085,2085,2085,2085,2085,ICLR,2019,Graph Convolutional Network with Sequential Attention For Goal-Oriented Dialogue Systems,Suman Banerjee;Mitesh M. Khapra,suman@cse.iitm.ac.in;miteshk@cse.iitm.ac.in,5;6;7,3;4;2,Reject,0,0,5.0,yes,9/27/18,Indian Institute of Technology Madras;Indian Institute of Technology Madras,153;153,625;625,3;10
2086,2086,2086,2086,2086,2086,2086,2086,ICLR,2019,Probabilistic Neural-Symbolic Models for Interpretable Visual Question Answering,Ramakrishna Vedantam;Stefan Lee;Marcus Rohrbach;Dhruv Batra;Devi Parikh,vrama@gatech.edu;steflee@gatech.edu;maroffm@gmail.com;dbatra@gatech.edu;parikh@gatech.edu,6;8;7,3;5;3,Reject,0,17,1.0,yes,9/27/18,Georgia Institute of Technology;Georgia Institute of Technology;Facebook;Georgia Institute of Technology;Georgia Institute of Technology,13;13;-1;13;13,33;33;-1;33;33,
2087,2087,2087,2087,2087,2087,2087,2087,ICLR,2019,Neural separation of observed and unobserved distributions,Tavi Halperin;Ariel Ephrat;Yedid Hoshen,tavihalperin@gmail.com;ariel.ephrat@gmail.com;yedidh@fb.com,5;6;6,4;2;4,Reject,0,4,1.0,yes,9/27/18,Hebrew University of Jerusalem;Google;Facebook,65;-1;-1,205;-1;-1,
2088,2088,2088,2088,2088,2088,2088,2088,ICLR,2019,BNN+: Improved Binary Network Training,Sajad Darabi;Mouloud Belbahri;Matthieu Courbariaux;Vahid Partovi Nia,sajad.darabi@cs.ucla.edu;belbahrim@dms.umontreal.ca;matthieu.courbariaux@gmail.com;vahid.partovinia@huawei.com,4;8;6,4;4;3,Reject,13,15,1.0,yes,9/27/18,"University of California, Los Angeles;University of Montreal;;Huawei Technologies Ltd.",20;123;-1;-1,15;108;-1;-1,
2089,2089,2089,2089,2089,2089,2089,2089,ICLR,2019,Dynamic Early Terminating of Multiply Accumulate Operations for Saving Computation Cost in Convolutional Neural Networks,Yu-Yi Su;Yung-Chih Chen;Xiang-Xiu Wu;Shih-Chieh Chang,wwball34@gmail.com;ycchen.phi@gmail.com;jaubau999@gmail.com;scchang@cs.nthu.edu.tw,5;6;6,3;5;3,Reject,0,8,0.0,yes,9/27/18,"National Tsing Hua University;Department of Computer Science and Engineering, Yuan Ze University;;National Tsing Hua University",199;478;-1;199,323;822;-1;323,
2090,2090,2090,2090,2090,2090,2090,2090,ICLR,2019,SEGEN: SAMPLE-ENSEMBLE GENETIC EVOLUTIONARY NETWORK MODEL,Jiawei Zhang;Limeng Cui;Fisher B. Gouza,jiawei@ifmlab.org;lmcui932@163.com;fisherbgouza@gmail.com,4;5;5,4;2;5,Reject,0,5,0.0,yes,9/27/18,SUN YAT-SEN UNIVERSITY;163;,478;-1;-1,352;-1;-1,
2091,2091,2091,2091,2091,2091,2091,2091,ICLR,2019,Local Binary Pattern Networks for Character Recognition,Jeng-Hau Lin;Yunfan Yang;Rajesh K. Gupta;Zhuowen Tu,jel252@ucsd.edu;yuy130@ucsd.edu;rgupta@ucsd.edu;ztu@ucsd.edu,5;6;5,5;4;4,Reject,0,3,0.0,yes,9/27/18,"University of California, San Diego;University of California, San Diego;University of California, San Diego;University of California, San Diego",11;11;11;11,31;31;31;31,
2092,2092,2092,2092,2092,2092,2092,2092,ICLR,2019,A  Model Cortical Network for Spatiotemporal Sequence Learning and Prediction,Jielin Qiu;Ge Huang;Tai Sing Lee,ternence1996@gmail.com;hgesummer@gmail.com;taislee@andrew.cmu.edu,7;7;3,3;3;3,Reject,0,13,0.0,yes,9/27/18,Shanghai Jiao Tong University;Carnegie Mellon University;Carnegie Mellon University,52;1;1,188;24;24,
2093,2093,2093,2093,2093,2093,2093,2093,ICLR,2019,Graph Transformer ,Yuan Li;Xiaodan Liang;Zhiting Hu;Yinbo Chen;Eric P. Xing,liyuanchristy@gmail.com;xiaodan1@cs.cmu.edu;zhitingh@cs.cmu.edu;cyb15@mails.tsinghua.edu.cn;epxing@cs.cmu.edu,6;6;6,5;5;3,Reject,3,8,0.0,yes,9/27/18,Duke University;Carnegie Mellon University;Carnegie Mellon University;Tsinghua University;Carnegie Mellon University,44;1;1;8;1,17;24;24;30;24,6;10
2094,2094,2094,2094,2094,2094,2094,2094,ICLR,2019,Automata Guided Skill Composition,Xiao Li;Yao Ma;Calin Belta,xli87@bu.edu;yaoma@bu.edu;cbelta@bu.edu,5;7;6;5,2;3;4;2,Reject,0,5,1.0,yes,9/27/18,Boston University;Boston University;Boston University,65;65;65,70;70;70,
2095,2095,2095,2095,2095,2095,2095,2095,ICLR,2019,Manifold Mixup: Learning Better Representations by Interpolating Hidden States,Vikas Verma;Alex Lamb;Christopher Beckham;Amir Najafi;Aaron Courville;Ioannis Mitliagkas;Yoshua Bengio,vikasverma.iitm@gmail.com;lambalex@iro.umontreal.ca;christopher.j.beckham@gmail.com;najafy@ce.sharif.edu;aaron.courville@gmail.com;imitliagkas@gmail.com;yoshua.umontreal@gmail.com,8;6;4,2;4;4,Reject,0,33,0.0,yes,9/27/18,;University of Montreal;Polytechnique Montreal;Sharif University of Technology;University of Montreal;University of Montreal;University of Montreal,-1;123;386;314;123;123;123,-1;108;108;603;108;108;108,4;1;8
2096,2096,2096,2096,2096,2096,2096,2096,ICLR,2019,Convolutional Neural Networks combined with Runge-Kutta Methods,Mai Zhu;Bo Chang;Chong Fu,zhumai@stumail.neu.edu.cn;bchang@stat.ubc.ca;fuchong@mail.neu.edu.cn,4;5;6,3;4;3,Reject,0,3,0.0,yes,9/27/18,Northeastern University;University of British Columbia;Northeastern University,16;36;16,839;34;839,
2097,2097,2097,2097,2097,2097,2097,2097,ICLR,2019,Exploring and Enhancing the Transferability of Adversarial Examples,Lei Wu;Zhanxing Zhu;Cheng Tai,leiwu@pku.edu.cn;zhanxing.zhu@pku.edu.cn;chengtai@pku.edu.cn,4;6;6,2;3;3,Reject,0,5,0.0,yes,9/27/18,Peking University;Peking University;Peking University,24;24;24,27;27;27,4
2098,2098,2098,2098,2098,2098,2098,2098,ICLR,2019,Invariance and Inverse Stability under ReLU,Jens Behrmann;Sören Dittmer;Pascal Fernsel;Peter Maass,jensb@uni-bremen.de;sdittmer@math.uni-bremen.de;pfernsel@math.uni-bremen.de;pmaass@uni-bremen.de,6;6;7,3;4;3,Reject,0,8,0.0,yes,9/27/18,Universität Bremen;Universität Bremen;Universität Bremen;Universität Bremen,153;153;153;153,291;291;291;291,4
2099,2099,2099,2099,2099,2099,2099,2099,ICLR,2019,Improving Sentence Representations with Multi-view Frameworks,Shuai Tang;Virginia R. de Sa,shuaitang93@ucsd.edu;desa@ucsd.edu,6;5;7,5;4;4,Reject,0,8,0.0,yes,9/27/18,"University of California, San Diego;University of California, San Diego",11;11,31;31,5
2100,2100,2100,2100,2100,2100,2100,2100,ICLR,2019,Principled Deep Neural Network Training through Linear Programming,Daniel Bienstock;Gonzalo Muñoz;Sebastian Pokutta,dano@columbia.edu;gonzalo.munoz@polymtl.ca;sebastian.pokutta@isye.gatech.edu,6;6;8,4;3;3,Reject,0,9,0.0,yes,9/27/18,Columbia University;Polytechnique Montreal;Georgia Institute of Technology,15;386;13,14;108;33,
2101,2101,2101,2101,2101,2101,2101,2101,ICLR,2019,The Importance of Norm Regularization in Linear Graph Embedding: Theoretical Analysis and Empirical Demonstration,Yihan Gao;Chao Zhang;Jian Peng;Aditya Parameswaran,gaoyihan@gmail.com;czhang82@illinois.edu;jianpeng@illinois.edu;adityagp@illinois.edu,7;4;4,3;3;4,Reject,0,5,0.0,yes,9/27/18,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",3;3;3;3,37;37;37;37,10;1;8
2102,2102,2102,2102,2102,2102,2102,2102,ICLR,2019,Learning to Decompose Compound Questions with Reinforcement Learning,Haihong Yang;Han Wang;Shuang Guo;Wei Zhang;Huajun Chen,capriceyhh@zju.edu.cn;wanghanwh@zju.edu.cn;guoshuang@zju.edu.cn;lantau.zw@alibaba-inc.com;huajunsir@zju.edu.cn,6;5;5,5;3;4,Reject,0,4,0.0,yes,9/27/18,Zhejiang University;Zhejiang University;Zhejiang University;Tel Aviv University;Zhejiang University,57;57;57;37;57,177;177;177;217;177,10
2103,2103,2103,2103,2103,2103,2103,2103,ICLR,2019,Dimension-Free Bounds for Low-Precision Training,Zheng Li;Christopher De Sa,lzlz19971997@gmail.com;cdesa@cs.cornell.edu,6;6;6,3;4;3,Reject,0,4,0.0,yes,9/27/18,Tsinghua University;Cornell University,8;7,30;19,1;9
2104,2104,2104,2104,2104,2104,2104,2104,ICLR,2019,Hint-based Training for Non-Autoregressive Translation,Zhuohan Li;Di He;Fei Tian;Tao Qin;Liwei Wang;Tie-Yan Liu,lizhuohan@pku.edu.cn;dihe@microsoft.com;fetia@microsoft.com;taoqin@microsoft.com;wanglw@cis.pku.edu.cn;tyliu@microsoft.com,6;6;4,4;3;4,Reject,0,8,0.0,yes,9/27/18,Peking University;Microsoft;Microsoft;Microsoft;Peking University;Microsoft,24;-1;-1;-1;24;-1,27;-1;-1;-1;27;-1,3
2105,2105,2105,2105,2105,2105,2105,2105,ICLR,2019,Convergence Guarantees for RMSProp and ADAM in Non-Convex Optimization and an Empirical Comparison to Nesterov Acceleration,Soham De;Anirbit Mukherjee;Enayat Ullah,sohamde@cs.umd.edu;amukhe14@jhu.edu;enayat@jhu.edu,5;4;5,3;5;4,Reject,0,7,0.0,yes,9/27/18,"University of Maryland, College Park;Johns Hopkins University;Johns Hopkins University",12;72;72,69;13;13,1;8
2106,2106,2106,2106,2106,2106,2106,2106,ICLR,2019,Globally Soft Filter Pruning For Efficient Convolutional Neural Networks,Ke Xu;Xiaoyun Wang;Qun Jia;Jianjing An;Dong Wang,17112071@bjtu.edu.cn;16120304@bjtu.edu.cn;16120347@bjtu.edu.cn;16112065@bjtu.edu.cn;wangdong@bjtu.edu.cn,6;5;4,4;4;3,Reject,0,5,0.0,yes,9/27/18,Beijing jiaotong univercity;Beijing jiaotong univercity;Beijing jiaotong univercity;Beijing jiaotong univercity;Beijing jiaotong univercity,478;478;478;478;478,854;854;854;854;854,
2107,2107,2107,2107,2107,2107,2107,2107,ICLR,2019,Quantization for Rapid Deployment of Deep Neural Networks,Jun Haeng Lee;Sangwon Ha;Saerom Choi;Won-Jo Lee;Seungwon Lee,junhaeng2.lee@samsung.com;sw815.ha@samsung.com;sincere.choi@samsung.com;w-j.lee@samsung.com;seungw.lee@samsung.com,5;5;5,3;4;4,Reject,0,5,0.0,yes,9/27/18,Samsung;Samsung;Samsung;Samsung;Samsung,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,1;2
2108,2108,2108,2108,2108,2108,2108,2108,ICLR,2019,Discovering Low-Precision Networks Close to Full-Precision Networks for Efficient Embedded Inference,Jeffrey L. McKinstry;Steven K. Esser;Rathinakumar Appuswamy;Deepika Bablani;John V. Arthur;Izzet B. Yildiz;Dharmendra S. Modha,jlmckins@us.ibm.com;sesser@us.ibm.com;rappusw@us.ibm.com;deepika.bablani@ibm.com;arthurjo@us.ibm.com;izzet.burak.yildiz@gmail.com;dmodha@us.ibm.com,4;6;5,5;3;4,Reject,0,6,1.0,yes,9/27/18,International Business Machines;International Business Machines;International Business Machines;International Business Machines;International Business Machines;;International Business Machines,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,
2109,2109,2109,2109,2109,2109,2109,2109,ICLR,2019,Can I trust you more? Model-Agnostic Hierarchical Explanations,Michael Tsang;Youbang Sun;Dongxu Ren;Beibei Xin;Yan Liu,tsangm@usc.edu;syb98@mail.ustc.edu.cn;rdx15@mails.tsinghua.edu.cn;bxin@usc.edu;yanliu.cs@usc.edu,5;6;6,4;4;4,Reject,0,9,0.0,yes,9/27/18,University of Southern California;University of Science and Technology of China;Tsinghua University;University of Southern California;University of Southern California,30;478;8;30;30,66;132;30;66;66,
2110,2110,2110,2110,2110,2110,2110,2110,ICLR,2019,Towards a better understanding of Vector Quantized Autoencoders,Aurko Roy;Ashish Vaswani;Niki Parmar;Arvind Neelakantan,aurkor@google.com;avaswani@google.com;nikip@google.com;aneelakantan@google.com,6;7;5;3,3;4;4;4,Reject,0,13,0.0,yes,9/27/18,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,3;5
2111,2111,2111,2111,2111,2111,2111,2111,ICLR,2019,Human-Guided Column Networks: Augmenting Deep Learning with Advice,Mayukh Das;Yang Yu;Devendra Singh Dhami;Gautam Kunapuli;Sriraam Natarajan,mayukh.das1@utdallas.edu;yangyu@hlt.utdallas.edu;devendra.dhami@utdallas.edu;gautam.kunapuli@utdallas.edu;sriraam.natarajan@utdallas.edu,6;4;5,3;4;5,Reject,0,3,0.0,yes,9/27/18,"University of Texas, Dallas;University of Texas, Dallas;University of Texas, Dallas;University of Texas, Dallas;University of Texas, Dallas",85;85;85;85;85,239;239;239;239;239,
2112,2112,2112,2112,2112,2112,2112,2112,ICLR,2019,Weakly-supervised Knowledge Graph Alignment with Adversarial Learning,Meng Qu;Jian Tang;Yoshua Bengio,qumn123@gmail.com;tangjianpku@gmail.com;yoshua.bengio@mila.quebec,5;5;5,3;4;3,Reject,2,0,0.0,yes,9/25/19,University of Montreal;HEC Montreal;University of Montreal,123;-1;123,108;-1;108,4;1;10
2113,2113,2113,2113,2113,2113,2113,2113,ICLR,2019,RelWalk -- A Latent Variable Model Approach to Knowledge Graph Embedding,Danushka Bollegala;Huda Hakami;Yuichi Yoshida;Ken-ichi Kawarabayashi,danushka@liverpool.ac.uk;h.a.hakami@liverpool.ac.uk;yyoshida@nii.ac.jp;k_keniti@nii.ac.jp,6;5;4,4;5;4,Reject,10,5,0.0,yes,9/27/18,University of Liverpool;University of Liverpool;Meiji University;Meiji University,123;123;478;478,177;177;334;334,3;5;10
2114,2114,2114,2114,2114,2114,2114,2114,ICLR,2019,Riemannian TransE: Multi-relational Graph Embedding in Non-Euclidean Space,Atsushi Suzuki;Yosuke Enokida;Kenji Yamanishi,atsushi-suzuki@g.ecc.u-tokyo.ac.jp;xenolay@g.ecc.u-tokyo.ac.jp;yamanishi@mist.i.u-tokyo.ac.jp,5;5;5,2;5;3,Reject,4,3,0.0,yes,9/27/18,The University of Tokyo;The University of Tokyo;The University of Tokyo,54;54;54,45;45;45,10
2115,2115,2115,2115,2115,2115,2115,2115,ICLR,2019,Why Do Neural Response Generation Models Prefer Universal Replies?,Bowen Wu;Nan Jiang;Zhifeng Gao;Zongsheng Wang;Suke Li;Wenge Rong;Baoxun Wang,jasonwbw@yahoo.com;nanjiang@buaa.edu.cn;gao_zhifeng@pku.edu.cn;jasonwang0512@gmail.com;lisuke@ss.pku.edu.cn;w.rong@buaa.edu.cn;baoxun.wang@gmail.com,3;7;1,4;3;5,Reject,2,10,1.0,yes,9/27/18,Tencent AI Lab;Beihang University;Peking University;;Peking University;Beihang University;,-1;115;24;-1;24;115;-1,-1;658;27;-1;27;658;-1,5
2116,2116,2116,2116,2116,2116,2116,2116,ICLR,2019,CGNF: Conditional Graph Neural Fields,Tengfei Ma;Cao Xiao;Junyuan Shang;Jimeng Sun,tengfei.ma1@ibm.com;cxiao@us.ibm.com;sjy1203@pku.edu.cn;jsun@cc.gatech.edu,4;5;5,5;4;5,Reject,0,4,0.0,yes,9/27/18,International Business Machines;International Business Machines;Peking University;Georgia Institute of Technology,-1;-1;24;13,-1;-1;27;33,10
2117,2117,2117,2117,2117,2117,2117,2117,ICLR,2019,Few-shot Classification on Graphs with Structural Regularized GCNs,Shengzhong Zhang;Ziang Zhou;Zengfeng Huang;Zhongyu Wei,17210980007@fudan.edu.cn;15300180085@fudan.edu.cn;huangzf@fudan.edu.cn;zywei@fudan.edu.cn,4;6;5,4;3;4,Reject,0,5,0.0,yes,9/27/18,Fudan University;Fudan University;Fudan University;Fudan University,78;78;78;78,116;116;116;116,10;6;8
2118,2118,2118,2118,2118,2118,2118,2118,ICLR,2019,Optimization on Multiple Manifolds,Mingyang Yi;Huishuai Zhang;Wei Chen;Zhi-ming Ma;Tie-yan Liu,yimingyang17@mails.ucas.edu.cn;huishuai.zhang@microsoft.com;wche@microsoft.com;mazm@amt.ac.cn;tie-yan.liu@mircosoft.com,7;1;3,3;5;4,Reject,2,0,0.0,yes,9/27/18,University of Chinese Academy of Sciences;Microsoft;Microsoft;Chinese Academy of Sciences;Mircosoft,62;-1;-1;62;-1,1103;-1;-1;1103;-1,1
2119,2119,2119,2119,2119,2119,2119,2119,ICLR,2019,On the Selection of Initialization and Activation Function for Deep Neural Networks,Soufiane Hayou;Arnaud Doucet;Judith Rousseau,soufiane.hayou@stats.ox.ac.uk;doucet@stats.ox.ac.uk;judith.rousseau@stats.ox.ac.uk,3;4;5,5;4;4,Reject,0,13,0.0,yes,9/27/18,University of Oxford;University of Oxford;University of Oxford,50;50;50,1;1;1,
2120,2120,2120,2120,2120,2120,2120,2120,ICLR,2019,Online Learning for Supervised Dimension Reduction,Ning Zhang;Qiang Wu,ningzhang0123@gmail.com;qwu@mtsu.edu,2;5;6,5;4;5,Reject,0,0,0.0,yes,9/27/18,;SUN YAT-SEN UNIVERSITY,-1;478,-1;352,
2121,2121,2121,2121,2121,2121,2121,2121,ICLR,2019,Accelerated Sparse Recovery Under Structured Measurements,Ke Li;Jitendra Malik,ke.li@eecs.berkeley.edu;malik@eecs.berkeley.edu,4;5;5,5;3;4,Reject,0,0,0.0,yes,9/27/18,University of California Berkeley;University of California Berkeley,5;5,18;18,
2122,2122,2122,2122,2122,2122,2122,2122,ICLR,2019,The Cakewalk Method,Uri Patish;Shimon Ullman,uri.patish@gmail.com;shimon.ullman@gmail.com,5;4;4,3;4;4,Reject,0,6,0.0,yes,9/27/18,Weizmann Institute;,106;-1,1103;-1,10
2123,2123,2123,2123,2123,2123,2123,2123,ICLR,2019,A fast quasi-Newton-type method for large-scale stochastic optimisation,Adrian Wills;Thomas B. Schön;Carl Jidling,adrian.wills@newcastle.edu.au;thomas.schon@it.uu.se;carl.jidling@it.uu.se,4;5;5,4;5;5,Reject,0,6,0.0,yes,9/27/18,"University of Newcastle, Australia;Uppsala University;Uppsala University",386;153;153,291;86;86,
2124,2124,2124,2124,2124,2124,2124,2124,ICLR,2019,Deep Denoising: Rate-Optimal Recovery of Structured Signals with a Deep Prior,Reinhard Heckel;Wen Huang;Paul Hand;Vladislav Voroninski,rh43@rice.edu;wen.huang@xmu.edu.cn;p.hand@northeastern.edu;vlad@helm.ai,5;6;6,3;3;4,Reject,0,3,0.0,yes,9/27/18,Rice University;Xiamen University;Northeastern University;,85;62;16;-1,86;12;839;-1,5
2125,2125,2125,2125,2125,2125,2125,2125,ICLR,2019,Perception-Aware Point-Based Value Iteration for Partially Observable Markov Decision Processes,Mahsa Ghasemi;Ufuk Topcu,mahsa.ghasemi@utexas.edu;utopcu@utexas.edu,6;4;7,4;4;2,Reject,0,3,0.0,yes,9/27/18,"University of Texas, Austin;University of Texas, Austin",22;22,49;49,
2126,2126,2126,2126,2126,2126,2126,2126,ICLR,2019,Towards Consistent Performance on Atari using Expert Demonstrations,Tobias Pohlen;Bilal Piot;Todd Hester;Mohammad Gheshlaghi Azar;Dan Horgan;David Budden;Gabriel Barth-Maron;Hado van Hasselt;John Quan;Mel Večerík;Matteo Hessel;Rémi Munos;Olivier Pietquin,pohlen@google.com;piot@google.com;toddhester@google.com;mazar@google.com;horgan@google.com;budden@google.com;gabrielbm@google.com;hado@google.com;johnquan@google.com;vec@google.com;mtthss@google.com;munos@google.com;pietquin@google.com,6;5;7;7,4;4;1;4,Reject,0,3,0.0,yes,9/27/18,Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,
2127,2127,2127,2127,2127,2127,2127,2127,ICLR,2019,Manifold Alignment via Feature Correspondence,Jay S. Stanley III;Guy Wolf;Smita Krishnaswamy,jay.stanley@yale.edu;guy.wolf@yale.edu;smita.krishnaswamy@yale.edu,5;5;4,4;4;3,Reject,0,0,0.0,yes,9/27/18,Yale University;Yale University;Yale University,62;62;62,12;12;12,10
2128,2128,2128,2128,2128,2128,2128,2128,ICLR,2019,Consistent Jumpy Predictions for Videos and Scenes,Ananya Kumar;S. M. Ali Eslami;Danilo Rezende;Marta Garnelo;Fabio Viola;Edward Lockhart;Murray Shanahan,skywalker94@gmail.com;aeslami@google.com;danilor@google.com;garnelo@google.com;fviola@google.com;locked@google.com;mshanahan@google.com,7;4;5,2;4;4,Reject,0,7,0.0,yes,9/27/18,Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,
2129,2129,2129,2129,2129,2129,2129,2129,ICLR,2019,LEARNING ADVERSARIAL EXAMPLES WITH RIEMANNIAN GEOMETRY,Shufei Zhang;Kaizhu Huang;Rui Zhang;Amir Hussain,shufei.zhang@xjtlu.edu.cn;kaizhu.huang@xjtlu.edu.cn;rui.zhang02@xjtlu.edu.cn;ahu@cs.stir.ac.uk,6;4;3,2;5;5,Reject,0,15,0.0,yes,9/27/18,Tsinghua University;Tsinghua University;Tsinghua University;University of Stirling,8;8;8;478,30;30;30;309,4
2130,2130,2130,2130,2130,2130,2130,2130,ICLR,2019,"On Tighter Generalization Bounds for Deep Neural Networks: CNNs, ResNets, and Beyond",Xingguo Li;Junwei Lu;Zhaoran Wang;Jarvis Haupt;Tuo Zhao,xingguol@princeton.edu;junweilu@hsph.harvard.edu;zhaoranwang@gmail.com;jdhaupt@umn.edu;tourzhao@gatech.edu,7;7;5,4;4;3,Reject,0,22,0.0,yes,9/27/18,"Princeton University;Harvard University;Northwestern University;University of Minnesota, Minneapolis;Georgia Institute of Technology",30;39;44;57;13,7;6;20;56;33,1;8
2131,2131,2131,2131,2131,2131,2131,2131,ICLR,2019,DEEP GEOMETRICAL GRAPH CLASSIFICATION,Mostafa Rahmani;Ping Li,rahmani.sut@gmail.com;pingli98@gmail.com,6;4;3,5;4;4,Reject,0,6,0.0,yes,9/27/18,Baidu;Rutgers University New Brunswick,-1;34,-1;172,10
2132,2132,2132,2132,2132,2132,2132,2132,ICLR,2019,Hierarchically Clustered Representation Learning,Su-Jin Shin;Kyungwoo Song;Il-Chul Moon,sujin.shin@kaist.ac.kr;gtshs2@kaist.ac.kr;icmoon@kaist.ac.kr,5;5;6,4;4;3,Reject,0,4,0.0,yes,9/27/18,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,20;20;20,95;95;95,5;11
2133,2133,2133,2133,2133,2133,2133,2133,ICLR,2019,A Deep Learning Approach for Dynamic Survival Analysis with Competing Risks,Changhee Lee;Mihaela van der Schaar,chl8856@gmail.com;mihaela@ee.ucla.edu,4;4;8,3;4;4,Reject,0,5,0.0,yes,9/27/18,"University of California, Los Angeles;University of California, Los Angeles",20;20,15;15,
2134,2134,2134,2134,2134,2134,2134,2134,ICLR,2019,Locally Linear Unsupervised Feature Selection,Guillaume DOQUET;Michèle SEBAG,doquet@lri.fr;sebag@lri.fr,4;6;3,5;2;5,Reject,2,3,0.0,yes,9/27/18,"CNRS, Université Paris-Saclay;CNRS, Université Paris-Saclay",-1;-1,-1;-1,
2135,2135,2135,2135,2135,2135,2135,2135,ICLR,2019,Scalable Neural Theorem Proving on Knowledge Bases and Natural Language,Pasquale Minervini;Matko Bosnjak;Tim Rocktäschel;Edward Grefenstette;Sebastian Riedel,p.minervini@gmail.com;matko.bosnjak@gmail.com;tim.rocktaeschel@gmail.com;etg@google.com;etg@google.com,5;4;5,3;3;3,Reject,0,13,0.0,yes,9/27/18,University College London;University College London;Facebook AI Research;Google;Google,50;50;-1;-1;-1,16;16;-1;-1;-1,1
2136,2136,2136,2136,2136,2136,2136,2136,ICLR,2019,A Frank-Wolfe Framework for Efficient and Effective Adversarial Attacks,Jinghui Chen;Jinfeng Yi;Quanquan Gu,jc4zg@virginia.edu;yijinfeng@jd.com;qgu@cs.ucla.edu,5;7;5,4;4;4,Reject,2,12,0.0,yes,9/27/18,"University of Virginia;JD AI Research;University of California, Los Angeles",65;-1;20,113;-1;15,4;9
2137,2137,2137,2137,2137,2137,2137,2137,ICLR,2019,ACTRCE: Augmenting Experience via Teacher’s Advice,Yuhuai Wu;Harris Chan;Jamie Kiros;Sanja Fidler;Jimmy Ba,ywu@cs.toronto.edu;hchan@cs.toronto.edu;kirosjamie@gmail.com;fidler@cs.toronto.edu;jba@cs.toronto.edu,5;5;7,4;4;5,Reject,0,14,0.0,yes,9/27/18,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Google;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",18;18;-1;18;18,22;22;-1;22;22,3
2138,2138,2138,2138,2138,2138,2138,2138,ICLR,2019,Learning data-derived privacy preserving representations from information metrics,Martin Bertran;Natalia Martinez;Afroditi Papadaki;Qiang Qiu;Miguel Rodrigues;Guillermo Sapiro,martin.bertran@duke.edu;natalia.martinez@duke.edu;a.papadaki.17@ucl.ac.uk;qiuqiang@gmail.com;m.rodrigues@ucl.ac.uk;guillermo.sapiro@duke.edu,6;6;5,3;4;4,Reject,0,6,0.0,yes,9/27/18,Duke University;Duke University;University College London;;University College London;Duke University,44;44;50;-1;50;44,17;17;16;-1;16;17,7;1;2
2139,2139,2139,2139,2139,2139,2139,2139,ICLR,2019,Probabilistic Federated Neural Matching,Mikhail Yurochkin;Mayank Agarwal;Soumya Ghosh;Kristjan Greenewald;Nghia Hoang;Yasaman Khazaeni,mikhail.yurochkin@ibm.com;mayank.agarwal@ibm.com;ghoshso@us.ibm.com;kristjan.h.greenewald@ibm.com;nghiaht@ibm.com;yasaman.khazaeni@us.ibm.com,4;6;6,3;4;4,Reject,0,4,0.0,yes,9/27/18,International Business Machines;International Business Machines;International Business Machines;International Business Machines;International Business Machines;International Business Machines,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,11
2140,2140,2140,2140,2140,2140,2140,2140,ICLR,2019,Learning to Refer to 3D Objects with Natural Language,Panos Achlioptas;Judy E. Fan;Robert X.D. Hawkins;Noah D. Goodman;Leo Guibas,optas@cs.stanford.edu;jefan@stanford.edu;rxdh@stanford.edu;ngoodman@stanford.edu;guibas@cs.stanford.edu,6;4;6,4;4;3,Reject,0,0,0.0,yes,9/27/18,Stanford University;Stanford University;Stanford University;Stanford University;Stanford University,4;4;4;4;4,3;3;3;3;3,8
2141,2141,2141,2141,2141,2141,2141,2141,ICLR,2019,Monge-Amp\`ere Flow for Generative Modeling,Linfeng Zhang;Weinan E;Lei Wang,linfengz@princeton.edu;weinan@math.princeton.edu;wanglei@iphy.ac.cn,6;6;7,4;3;3,Reject,0,10,2.0,yes,9/27/18,Princeton University;Princeton University;Chinese Academy of Sciences,30;30;62,7;7;1103,5
2142,2142,2142,2142,2142,2142,2142,2142,ICLR,2019,Negotiating Team Formation Using Deep Reinforcement Learning,Yoram Bachrach;Richard Everett;Edward Hughes;Angeliki Lazaridou;Joel Leibo;Marc Lanctot;Mike Johanson;Wojtek Czarnecki;Thore Graepel,yorambac@google.com;reverett@google.com;edwardhughes@google.com;jzl@google.com;angeliki@google.com;lanctot@google.com;mjohanson@google.com;lejlot@google.com;thore@google.com,5;6;5,3;2;3,Reject,0,8,0.0,yes,9/27/18,Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1,
2143,2143,2143,2143,2143,2143,2143,2143,ICLR,2019,Why do deep convolutional networks generalize so poorly to small image transformations?,Aharon Azulay;Yair Weiss,aharon.azulay@mail.huji.ac.il;yweiss@cs.huji.ac.il,7;7;5,5;4;4,Reject,0,8,1.0,yes,9/27/18,Hebrew University of Jerusalem;Hebrew University of Jerusalem,65;65,205;205,1;8
2144,2144,2144,2144,2144,2144,2144,2144,ICLR,2019,Psychophysical vs. learnt texture representations in novelty detection,Michael Grunwald;Matthias Hermann;Fabian Freiberg;Matthias O. Franz,m.grunwald@htwg-konstanz.de;matthias.hermann@htwg-konstanz.de;f.freiberg@htwg-konstanz.de;mfanz@htwg-konstanz.de,3;3;3;1,3;3;4;3,Reject,0,0,0.0,yes,9/27/18,University of Tuebingen;;;,153;-1;-1;-1,94;-1;-1;-1,
2145,2145,2145,2145,2145,2145,2145,2145,ICLR,2019,A fully automated periodicity detection in time series,Tom Puech;Matthieu Boussard,tom.puech@craft.ai;matthieu.boussard@craft.ai,3;5;3,3;2;2,Reject,0,0,0.0,yes,9/27/18,;,-1;-1,-1;-1,
2146,2146,2146,2146,2146,2146,2146,2146,ICLR,2019,Downsampling leads to Image Memorization in Convolutional Autoencoders,Adityanarayanan Radhakrishnan;Caroline Uhler;Mikhail Belkin,aradha@mit.edu;cuhler@mit.edu;mbelkin@cse.ohio-state.edu,3;5;5,3;2;2,Reject,0,4,0.0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Ohio State University,2;2;76,5;5;318,
2147,2147,2147,2147,2147,2147,2147,2147,ICLR,2019,CDeepEx: Contrastive Deep Explanations,Amir Feghahati;Christian R. Shelton;Michael J. Pazzani;Kevin Tang,sfegh001@ucr.edu;cshelton@cs.ucr.edu;pazzani@ucr.edu;ktang012@ucr.edu,5;5;6,4;4;5,Reject,0,13,0.0,yes,9/27/18,"University of California, Riverside;University of California, Riverside;University of California, Riverside;University of California, Riverside",57;57;57;57,197;197;197;197,2
2148,2148,2148,2148,2148,2148,2148,2148,ICLR,2019,Pooling Is Neither Necessary nor Sufficient for Appropriate Deformation Stability in CNNs,Avraham Ruderman;Neil C. Rabinowitz;Ari S. Morcos;Daniel Zoran,aruderman@google.com;ncr@google.com;arimorcos@gmail.com;danielzoran@google.com,5;5;4;5,5;2;4;2,Reject,0,4,0.0,yes,9/27/18,Google;Google;Facebook;Google,-1;-1;-1;-1,-1;-1;-1;-1,
2149,2149,2149,2149,2149,2149,2149,2149,ICLR,2019,Meta-Learning to Guide Segmentation,Kate Rakelly*;Evan Shelhamer*;Trevor Darrell;Alexei A. Efros;Sergey Levine,rakelly@eecs.berkeley.edu;shelhamer@cs.berkeley.edu;trevor@eecs.berkeley.edu;efros@eecs.berkeley.edu;svlevine@eecs.berkeley.edu,7;3;3,4;4;5,Reject,0,5,0.0,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5;5,18;18;18;18;18,6;2;8
2150,2150,2150,2150,2150,2150,2150,2150,ICLR,2019,A theoretical framework for deep and locally connected ReLU network,Yuandong Tian,yuandong@fb.com,5;3;7,3;4;4,Reject,2,5,0.0,yes,9/27/18,Facebook,-1,-1,10
2151,2151,2151,2151,2151,2151,2151,2151,ICLR,2019,FAST OBJECT LOCALIZATION VIA SENSITIVITY ANALYSIS,Mohammad K. Ebrahimpour;David C. Noelle,mebrahimpour@ucmerced.edu;dnoelle@ucmerced.edu,4;3;6,3;4;5,Reject,0,1,0.0,yes,9/27/18,University of California at Merced;University of California at Merced,478;478,1103;1103,
2152,2152,2152,2152,2152,2152,2152,2152,ICLR,2019,Generative Feature Matching Networks,Cicero Nogueira dos Santos;Inkit Padhi;Pierre Dognin;Youssef Mroueh,cicerons@us.ibm.com;inkit.padhi@ibm.com;pdognin@us.ibm.com;mroueh@us.ibm.com,6;6;6;6,3;4;3;3,Reject,0,23,0.0,yes,9/27/18,International Business Machines;International Business Machines;International Business Machines;International Business Machines,-1;-1;-1;-1,-1;-1;-1;-1,5;4
2153,2153,2153,2153,2153,2153,2153,2153,ICLR,2019,Generative Adversarial Models for Learning Private and Fair Representations,Chong Huang;Xiao Chen;Peter Kairouz;Lalitha Sankar;Ram Rajagopal,chuang83@asu.edu;markcx@stanford.edu;kairouzp@stanford.edu;lsankar@asu.edu;ramr@stanford.edu,4;4;7,3;3;3,Reject,0,8,0.0,yes,9/27/18,Arizona State University;Stanford University;Stanford University;Arizona State University;Stanford University,95;4;4;95;4,126;3;3;126;3,5;4;7
2154,2154,2154,2154,2154,2154,2154,2154,ICLR,2019,Understanding & Generalizing AlphaGo Zero,Ravichandra Addanki;Mohammad Alizadeh;Shaileshh Bojja Venkatakrishnan;Devavrat Shah;Qiaomin Xie;Zhi Xu,addanki@mit.edu;alizadeh@csail.mit.edu;bjjvnkt@csail.mit.edu;devavrat@mit.edu;qxie@mit.edu;zhixu@mit.edu,5;5;7,3;5;4,Reject,0,3,0.0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2;2;2,5;5;5;5;5;5,1
2155,2155,2155,2155,2155,2155,2155,2155,ICLR,2019,Link Prediction in Hypergraphs using Graph Convolutional Networks,Naganand Yadati;Vikram Nitin;Madhav Nimishakavi;Prateek Yadav;Anand Louis;Partha Talukdar,y.naganand@gmail.com;vikramnitin9@gmail.com;madhav@iisc.ac.in;prateekyadav@iisc.ac.in;anandl@iisc.ac.in;ppt@iisc.ac.in,6;5;4,2;4;5,Reject,0,9,0.0,yes,9/27/18,"Indian Institute of Science;BITS Pilani, BITS Pilani;Indian Institute of Science;Indian Institute of Science;Indian Institute of Science;Indian Institute of Science",478;-1;478;478;478;478,273;-1;273;273;273;273,10
2156,2156,2156,2156,2156,2156,2156,2156,ICLR,2019,RedSync : Reducing Synchronization Traffic for Distributed Deep Learning,Jiarui Fang;Cho-Jui Hsieh,fang_jiarui@163.com;rainfarmer@gmail.com,5;5;5,3;4;4,Reject,2,5,0.0,yes,9/27/18,Tsinghua;,8;-1,30;-1,3
2157,2157,2157,2157,2157,2157,2157,2157,ICLR,2019,PRUNING IN TRAINING: LEARNING AND RANKING SPARSE CONNECTIONS IN DEEP CONVOLUTIONAL NETWORKS,Yanwei Fu;Shun Zhang;Donghao Li;Xinwei Sun;Xiangyang Xue;Yuan Yao,yanweifu@fudan.edu.cn;15300180012@fudan.edu.cn;15307100013@fudan.edu.cn;sxwxiaoxiaohehe@pku.edu.cn;xyxue@fudan.edu.cn;yuany@ust.hk,5;5;4,4;4;5,Reject,0,0,0.0,yes,9/27/18,Fudan University;Fudan University;Fudan University;Peking University;Fudan University;The Hong Kong University of Science and Technology,78;78;78;24;78;39,116;116;116;27;116;44,
2158,2158,2158,2158,2158,2158,2158,2158,ICLR,2019,Integral Pruning on Activations and Weights for Efficient Neural Networks,Qing Yang;Wei Wen;Zuoguan Wang;Yiran Chen;Hai Li,qing.yang21@duke.edu;wei.wen@duke.edu;zuoguan.wang@blacksesame.com;yiran.chen@duke.edu;hai.li@duke.edu,4;5;5,3;4;4,Reject,0,4,0.0,yes,9/27/18,Duke University;Duke University;Blacksesame;Duke University;Duke University,44;44;-1;44;44,17;17;-1;17;17,
2159,2159,2159,2159,2159,2159,2159,2159,ICLR,2019,Penetrating the Fog: the Path to Efficient CNN Models,Kun Wan;Boyuan Feng;Shu Yang;Yufei Ding,kun@cs.ucsb.edu;boyuan@cs.ucsb.edu;shuyang1995@ucsb.edu;yufeiding@cs.ucsb.edu,5;5;4,3;3;3,Reject,0,0,0.0,yes,9/27/18,UC Santa Barbara;UC Santa Barbara;UC Santa Barbara;UC Santa Barbara,37;37;37;37,53;53;53;53,
2160,2160,2160,2160,2160,2160,2160,2160,ICLR,2019,ACIQ: Analytical Clipping for Integer Quantization of neural networks,Ron Banner;Yury Nahshan;Elad Hoffer;Daniel Soudry,ron.banner@intel.com;yury.nahshan@intel.com;daniel.soudry@gmail.com;elad.hoffer@gmail.com,4;4;5,5;4;4,Reject,3,5,0.0,yes,9/27/18,Intel;Intel;Technion;Technion,-1;-1;25;25,-1;-1;327;327,
2161,2161,2161,2161,2161,2161,2161,2161,ICLR,2019,PRUNING WITH HINTS: AN EFFICIENT FRAMEWORK FOR MODEL ACCELERATION,Wei Gao;Yi Wei;Quanquan Li;Hongwei Qin;Wanli Ouyang;Junjie Yan,weigao1996@outlook.com;wei-y15@mails.tsinghua.edu.cn;liquanquan@sensetime.com;qinghongwei@sensetime.com;wanli.ouyang@sydney.edu.cn;yanjunjie@outlook.com,4;5;4,3;4;4,Reject,0,0,0.0,yes,9/27/18,Beihang University;Tsinghua University;SenseTime Group Limited;SenseTime Group Limited;Tsinghua University;SenseTime Group Limited,115;8;-1;-1;8;-1,658;30;-1;-1;30;-1,2;8
2162,2162,2162,2162,2162,2162,2162,2162,ICLR,2019,Progressive Weight Pruning Of Deep Neural Networks Using ADMM,Shaokai Ye;Tianyun Zhang;Kaiqi Zhang;Jiayu Li;Kaidi Xu;Yunfei Yang;Fuxun Yu;Jian Tang;Makan Fardad;Sijia Liu;Xiang Chen;Xue Lin;Yanzhi Wang,sye106@syr.edu;tzhan120@syr.edu;kzhang17@syr.edu;jli221@syr.edu;xu.kaid@husky.neu.edu;yunfei.yang717@gmail.com;fyu@gmu.edu;jtang02@syr.edu;makan@syr.edu;sijia.liu@ibm.com;xchen26@gmu.edu;xue.lin@northeastern.edu;yanz.wang@northeastern.edu,5;5;4,4;3;4,Reject,0,3,0.0,yes,9/27/18,Syracuse University;Syracuse University;Syracuse University;Syracuse University;Northeastern University;;George Mason University;Syracuse University;Syracuse University;International Business Machines;George Mason University;Northeastern University;Northeastern University,261;261;261;261;16;-1;99;261;261;-1;99;16;16,275;275;275;275;839;-1;336;275;275;-1;336;839;839,9
2163,2163,2163,2163,2163,2163,2163,2163,ICLR,2019,Understanding Opportunities for Efficiency in Single-image Super Resolution Networks,Royson Lee;Nic Lane;Marko Stankovic;Sourav Bhattacharya,rs@roysonlee.com;nicholas.d.lane@gmail.com;marko.stankovic996@gmail.com;bsourav@gmail.com,4;5;3,5;4;5,Reject,0,3,0.0,yes,9/27/18,Samsung;University of Oxford;;,-1;50;-1;-1,-1;1;-1;-1,
2164,2164,2164,2164,2164,2164,2164,2164,ICLR,2019,Actor-Attention-Critic for Multi-Agent Reinforcement Learning,Shariq Iqbal;Fei Sha,shariqiqbal2810@gmail.com;feisha.work@gmail.com,7;6;4,3;3;4,Reject,2,3,0.0,yes,9/27/18,University of Southern California;,30;-1,66;-1,4
2165,2165,2165,2165,2165,2165,2165,2165,ICLR,2019,On the Margin Theory of Feedforward Neural Networks,Colin Wei;Jason Lee;Qiang Liu;Tengyu Ma,colinwei@stanford.edu;jasonlee@marshall.usc.edu;lqiang@cs.texas.edu;tengyuma@cs.stanford.edu,6;7;5;5,4;3;4;4,Reject,2,16,0.0,yes,9/27/18,Stanford University;University of Southern California;;Stanford University,4;30;-1;4,3;66;-1;3,8
2166,2166,2166,2166,2166,2166,2166,2166,ICLR,2019,Are adversarial examples inevitable?,Ali Shafahi;W. Ronny Huang;Christoph Studer;Soheil Feizi;Tom Goldstein,ashafahi@gmail.com;w.ronny.huang@gmail.com;studer@cornell.edu;feizi.soheil@gmail.com;tomg@cs.umd.edu,6;7;8,4;4;4,Accept (Poster),3,12,2.0,yes,9/27/18,"University of Maryland, College Park;;Cornell University;;University of Maryland, College Park",12;-1;7;-1;12,69;-1;19;-1;69,4
2167,2167,2167,2167,2167,2167,2167,2167,ICLR,2019,Open Loop Hyperparameter Optimization and Determinantal Point Processes,Jesse Dodge;Kevin Jamieson;Noah Smith,jessed@cs.cmu.edu;jamieson@cs.washington.edu;nasmith@cs.washington.edu,5;5;6,3;5;4,Reject,0,3,0.0,yes,9/27/18,Carnegie Mellon University;University of Washington;University of Washington,1;6;6,24;25;25,
2168,2168,2168,2168,2168,2168,2168,2168,ICLR,2019,Cross-Task Knowledge Transfer for Visually-Grounded Navigation,Devendra Singh Chaplot;Lisa Lee;Ruslan Salakhutdinov;Devi Parikh;Dhruv Batra,chaplot@cs.cmu.edu;lslee@cs.cmu.edu;rsalakhu@cs.cmu.edu;parikh@gatech.edu;dbatra@gatech.edu,7;5;5,4;3;5,Reject,0,5,0.0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Georgia Institute of Technology;Georgia Institute of Technology,1;1;1;13;13,24;24;24;33;33,6
2169,2169,2169,2169,2169,2169,2169,2169,ICLR,2019,Expanding the Reach of Federated Learning by Reducing Client Resource Requirements,Sebastian Caldas;Jakub Konečný;Brendan McMahan;Ameet Talwalkar,scaldas@cmu.edu;konkey@google.com;mcmahan@google.com;talwalkar@cmu.edu,4;5;5,5;3;4,Reject,0,6,0.0,yes,9/27/18,Carnegie Mellon University;Google;Google;Carnegie Mellon University,1;-1;-1;1,24;-1;-1;24,
2170,2170,2170,2170,2170,2170,2170,2170,ICLR,2019,Generative Adversarial Self-Imitation Learning,Junhyuk Oh;Yijie Guo;Satinder Singh;Honglak Lee,junhyuk@umich.edu;guoyijie@umich.edu;baveja@umich.edu;honglak@google.com,5;5;6,4;5;5,Reject,0,7,0.0,yes,9/27/18,University of Michigan;University of Michigan;University of Michigan;Google,8;8;8;-1,21;21;21;-1,5;4
2171,2171,2171,2171,2171,2171,2171,2171,ICLR,2019,Diverse Machine Translation with a Single Multinomial Latent Variable,Tianxiao Shen;Myle Ott;Michael Auli;Marc’Aurelio Ranzato,tianxiao@mit.edu;myleott@fb.com;michaelauli@fb.com;ranzato@fb.com,3;6;5;7,4;4;4;4,Reject,0,6,0.0,yes,9/27/18,Massachusetts Institute of Technology;Facebook;Facebook;Facebook,2;-1;-1;-1,5;-1;-1;-1,
2172,2172,2172,2172,2172,2172,2172,2172,ICLR,2019,Learning and Planning with a Semantic Model,Yi Wu;Yuxin Wu;Aviv Tamar;Stuart Russell;Georgia Gkioxari;Yuandong Tian,jxwuyi@gmail.com;yuxinwu@fb.com;avivt@berkeley.edu;russell@cs.berkeley.edu;gkioxari@fb.com;yuandong@fb.com,5;4;7,4;4;3,Reject,0,6,0.0,yes,9/27/18,University of California Berkeley;Facebook;University of California Berkeley;University of California Berkeley;Facebook;Facebook,5;-1;5;5;-1;-1,18;-1;18;18;-1;-1,11;8
2173,2173,2173,2173,2173,2173,2173,2173,ICLR,2019,Visual Imitation with a Minimal Adversary,Scott Reed;Yusuf Aytar;Ziyu Wang;Tom Paine;Aäron van den Oord;Tobias Pfaff;Sergio Gomez;Alexander Novikov;David Budden;Oriol Vinyals,reedscot@google.com;yusufaytar@google.com;ziyu@google.com;tpaine@google.com;avdnoord@google.com;tpfaff@google.com;sergomez@google.com;anovikov@google.com;budden@google.com;vinyals@google.com,5;3;6,4;3;4,Reject,0,5,0.0,yes,9/27/18,Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,5;4
2174,2174,2174,2174,2174,2174,2174,2174,ICLR,2019,One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL,Tom Le Paine;Sergio Gomez;Ziyu Wang;Scott Reed;Yusuf Aytar;Tobias Pfaff;Matt Hoffman;Gabriel Barth-Maron;Serkan Cabi;David Budden;Nando de Freitas,tpaine@google.com;sergomez@google.com;ziyu@google.com;reedscot@google.com;yusufaytar@google.com;tpfaff@google.com;mwhoffman@google.com;gabrielbm@google.com;cabi@google.com;budden@google.com;nandodefreitas@google.com,4;4;5;5,4;4;3;3,Reject,0,10,0.0,yes,9/27/18,Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,
2175,2175,2175,2175,2175,2175,2175,2175,ICLR,2019,Mixed Precision Quantization of ConvNets via Differentiable Neural Architecture Search,Bichen Wu;Yanghan Wang;Peizhao Zhang;Yuandong Tian;Peter Vajda;Kurt Keutzer,bichen@berkeley.edu;yanghan@instagram.com;stzpz@fb.com;yuandong@fb.com;vajdap@fb.com,5;7;6;6,5;3;3;3,Reject,0,21,0.0,yes,9/27/18,University of California Berkeley;Instagram;Facebook;Facebook;Facebook,5;-1;-1;-1;-1,18;-1;-1;-1;-1,
2176,2176,2176,2176,2176,2176,2176,2176,ICLR,2019,Language Modeling with Graph Temporal Convolutional Networks,Hongyin Luo;Yichen Li;Jie Fu;James Glass,hyluo@mit.edu;yl3506@nyu.edu;jie.fu@polymtl.ca;glass@mit.edu,4;4;4,5;3;5,Reject,0,3,0.0,yes,9/27/18,Massachusetts Institute of Technology;New York University;Polytechnique Montreal;Massachusetts Institute of Technology,2;26;386;2,5;27;108;5,3;10
2177,2177,2177,2177,2177,2177,2177,2177,ICLR,2019,Lipschitz regularized Deep Neural Networks generalize,Adam M. Oberman;Jeff Calder,adam.oberman@mcgill.ca;jcalder@umn.edu,4;6;7,3;2;4,Reject,0,12,1.0,yes,9/27/18,"McGill University;University of Minnesota, Minneapolis",85;57,42;56,1;9;8
2178,2178,2178,2178,2178,2178,2178,2178,ICLR,2019,Count-Based Exploration with the Successor Representation,Marlos C. Machado;Marc G. Bellemare;Michael Bowling,machado@ualberta.ca;bellemare@google.com;mbowling@ualberta.ca,5;5;4,4;2;3,Reject,0,7,0.0,yes,9/27/18,University of Alberta;Google;University of Alberta,99;-1;99,119;-1;119,8
2179,2179,2179,2179,2179,2179,2179,2179,ICLR,2019,Precision Highway for Ultra Low-precision Quantization,Eunhyeok Park;Dongyoung Kim;Sungjoo Yoo;Peter Vajda,canusglow@gmail.com;dongyoungkim42@gmail.com;sungjoo.yoo@gmail.com;vajdap@fb.com,6;7;5,5;4;3,Reject,0,9,0.0,yes,9/27/18,Seoul National University;;Seoul National University;Facebook,41;-1;41;-1,74;-1;74;-1,3
2180,2180,2180,2180,2180,2180,2180,2180,ICLR,2019,Graph Neural Networks with Generated Parameters for Relation Extraction,Hao Zhu;Yankai Lin;Zhiyuan Liu;Jie Fu;Tat-seng Chua;Maosong Sun,prokilchu@gmail.com;linyk14@mails.tsinghua.edu.cn;liuzy@tsinghua.edu.cn;full.jeffrey@gmail.com;chuats@comp.nus.edu.sg;sms@tsinghua.edu.cn,4;6;6,4;3;4,Reject,0,4,0.0,yes,9/27/18,Tsinghua University;Tsinghua University;Tsinghua University;Polytechnique Montreal;National University of Singapore;Tsinghua University,8;8;8;386;16;8,30;30;30;108;22;30,3;10
2181,2181,2181,2181,2181,2181,2181,2181,ICLR,2019,Cohen Welling bases & SO(2)-Equivariant classifiers using Tensor nonlinearity.,Muthuvel Murugan;K Venkata Subrahmanyam,muthu@cmi.ac.in;kv@cmi.ac.in,6;3;7,2;2;4,Reject,0,9,0.0,yes,9/27/18,Chennai Mathematical Institute;Chennai Mathematical Institute,478;478,1103;1103,
2182,2182,2182,2182,2182,2182,2182,2182,ICLR,2019,Out-of-Sample Extrapolation with Neuron Editing,Matthew Amodio;David van Dijk;Ruth Montgomery;Guy Wolf;Smita Krishnaswamy,matthew.amodio@yale.edu;davidvandijk@gmail.com;ruth.montgomery@yale.edu;guy.wolf@yale.edu;smita.krishnaswamy@yale.edu,5;5;6,3;3;4,Reject,0,4,0.0,yes,9/27/18,Yale University;;Yale University;Yale University;Yale University,62;-1;62;62;62,12;-1;12;12;12,5;4
2183,2183,2183,2183,2183,2183,2183,2183,ICLR,2019,Security Analysis of Deep Neural Networks Operating in the Presence of Cache Side-Channel Attacks,Sanghyun Hong;Michael Davinroy;Yigitcan Kaya;Stuart Nevans Locke;Ian Rackow;Kevin Kulda;Dana Dachman-Soled;Tudor Dumitraș,shhong@cs.umd.edu;mdavinr1@swarthmore.edu;yigitcan@cs.umd.edu;stnevans@mail.rit.edu;ian.rackow@gmail.com;kevin_kulda1@baylor.edu;danadach@ece.umd.edu;tdumitra@umiacs.umd.edu,4;6;4,4;4;2,Reject,0,6,0.0,yes,9/27/18,"University of Maryland, College Park;Swarthmore College;University of Maryland, College Park;Rochester Institute of Technology;University of Maryland, College Park;Baylor University;University of Maryland, College Park;University of Maryland, College Park",12;478;12;123;12;478;12;12,69;1103;69;666;69;642;69;69,4;6
2184,2184,2184,2184,2184,2184,2184,2184,ICLR,2019,Analyzing Federated Learning through an Adversarial Lens,Arjun Nitin Bhagoji;Supriyo Chakraborty;Seraphin Calo;Prateek Mittal,abhagoji@princeton.edu;supriyo@us.ibm.com;scalo@us.ibm.com;pmittal@princeton.edu,5;4;6,4;5;4,Reject,0,4,0.0,yes,9/27/18,Princeton University;International Business Machines;International Business Machines;Princeton University,30;-1;-1;30,7;-1;-1;7,4
2185,2185,2185,2185,2185,2185,2185,2185,ICLR,2019,Knows When it Doesn’t Know: Deep Abstaining Classifiers,Sunil Thulasidasan;Tanmoy Bhattacharya;Jeffrey Bilmes;Gopinath Chennupati;Jamal Mohd-Yusof,sunil@lanl.gov;tanmoy@lanl.gov;bilmes@uw.edu;gchennupati@lanl.gov;jamal@lanl.gov,6;5;5,4;3;5,Reject,0,6,0.0,yes,9/27/18,"Los Alamos National Laboratory;Los Alamos National Laboratory;University of Washington, Seattle;Los Alamos National Laboratory;Los Alamos National Laboratory",-1;-1;6;-1;-1,-1;-1;25;-1;-1,
2186,2186,2186,2186,2186,2186,2186,2186,ICLR,2019,Simple Black-box Adversarial Attacks,Chuan Guo;Jacob R. Gardner;Yurong You;Andrew G. Wilson;Kilian Q. Weinberger,cg563@cornell.edu;jrg365@cornell.edu;yy785@cornell.edu;andrew@cornell.edu;kqw4@cornell.edu,4;6;6,5;3;3,Reject,0,6,0.0,yes,9/27/18,Cornell University;Cornell University;Cornell University;Cornell University;Cornell University,7;7;7;7;7,19;19;19;19;19,4
2187,2187,2187,2187,2187,2187,2187,2187,ICLR,2019,Modulating transfer between tasks in gradient-based meta-learning,Erin Grant;Ghassen Jerfel;Katherine Heller;Thomas L. Griffiths,eringrant@berkeley.edu;gj47@duke.edu;kheller@stat.duke.edu;tomg@princeton.edu,5;4;4,2;3;4,Reject,0,21,0.0,yes,9/27/18,University of California Berkeley;Duke University;Duke University;Princeton University,5;44;44;30,18;17;17;7,11;6;8
2188,2188,2188,2188,2188,2188,2188,2188,ICLR,2019,Approximation capability of neural networks on sets of probability measures and tree-structured data,Tomáš Pevný;Vojtěch Kovařík,pevnak@gmail.com;vojta.kovarik@gmail.com,6;5;4,3;5;5,Reject,0,4,0.0,yes,9/27/18,Czech Technical University in Prague;Czech Technical University in Prague,314;314,740;740,1
2189,2189,2189,2189,2189,2189,2189,2189,ICLR,2019,Uncertainty in Multitask Transfer Learning,Alexandre Lacoste;Boris Oreshkin;Wonchang Chung;Thomas Boquet;Negar Rostamzadeh;David Krueger,alex.lacoste.shmu@gmail.com;boris@elementai.com;wonchang@elementai.com;thomas@elementai.com;negar.rostamzadeh@gmail.com;david.scott.krueger@gmail.com,4;3;2,4;5;5,Reject,0,4,0.0,yes,9/27/18,Element AI;Element AI;Element AI;Element AI;Element AI;University of Montreal,-1;-1;-1;-1;-1;123,-1;-1;-1;-1;-1;108,6
2190,2190,2190,2190,2190,2190,2190,2190,ICLR,2019,Compound Density Networks,Agustinus Kristiadi;Asja Fischer,kristiadi@protonmail.com;asja.fischer@gmail.com,4;5;5,4;4;4,Reject,0,10,0.0,yes,9/27/18,University of Bonn;Institute for Cognitive Neuroscience/ Inst. for Neuroinformatics,123;-1,100;-1,4;11
2191,2191,2191,2191,2191,2191,2191,2191,ICLR,2019,Adversarial Examples Are a Natural Consequence of Test Error in Noise,Nicolas Ford;Justin Gilmer;Ekin D. Cubuk,nicf@google.com;gilmer@google.com;cubuk@google.com,4;5;4,4;3;3,Reject,6,5,3.0,yes,9/27/18,Google;Google;Google,-1;-1;-1,-1;-1;-1,4;1
2192,2192,2192,2192,2192,2192,2192,2192,ICLR,2019,Stochastic Adversarial Video Prediction,Alex X. Lee;Richard Zhang;Frederik Ebert;Pieter Abbeel;Chelsea Finn;Sergey Levine,alexlee_gk@cs.berkeley.edu;rich.zhang@eecs.berkeley.edu;febert@berkeley.edu;pabbeel@cs.berkeley.edu;cbfinn@eecs.berkeley.edu;svlevine@eecs.berkeley.edu,6;6;5,3;4;5,Reject,0,9,0.0,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5;5;5,18;18;18;18;18;18,4
2193,2193,2193,2193,2193,2193,2193,2193,ICLR,2019,On Generalization Bounds of a Family of Recurrent Neural Networks,Minshuo Chen;Xingguo Li;Tuo Zhao,mchen393@gatech.edu;lixx1661@umn.edu;tourzhao@gatech.edu,3;4;6,4;4;4,Reject,4,7,0.0,yes,9/27/18,"Georgia Institute of Technology;University of Minnesota, Minneapolis;Georgia Institute of Technology",13;57;13,33;56;33,1;8
2194,2194,2194,2194,2194,2194,2194,2194,ICLR,2019,N-Ary Quantization for CNN Model Compression and Inference Acceleration,Günther Schindler;Wolfgang Roth;Franz Pernkopf;Holger Fröning,guenther.schindler@ziti.uni-heidelberg.de;roth@tugraz.at;pernkopf@tugraz.at;holger.froening@ziti.uni-heidelberg.de,4;4;7,4;4;5,Reject,1,5,0.0,yes,9/27/18,Heidelberg University;Graz University of Technology;Graz University of Technology;Heidelberg University,199;106;106;199,45;443;443;45,
2195,2195,2195,2195,2195,2195,2195,2195,ICLR,2019,AntMan: Sparse Low-Rank Compression To Accelerate RNN Inference,Samyam Rajbhandari;Harsh Shrivastava;Yuxiong He,samyamr@microsoft.com;hshrivastava3@gatech.edu;yuxhe@microsoft.com,6;5;5,5;2;4,Reject,0,7,0.0,yes,9/27/18,Microsoft;Georgia Institute of Technology;Microsoft,-1;13;-1,-1;33;-1,
2196,2196,2196,2196,2196,2196,2196,2196,ICLR,2019,Convolutional CRFs for Semantic Segmentation,Marvin Teichmann;Roberto Cipolla,mttt2@cam.ac.uk;cipolla@eng.cam.ac.uk,7;4;6,4;4;4,Reject,0,8,1.0,yes,9/27/18,University of Cambridge;University of Cambridge,71;71,2;2,2
2197,2197,2197,2197,2197,2197,2197,2197,ICLR,2019,Holographic and other Point Set Distances for Machine Learning,Lukas Balles;Thomas Fischbacher,lukas.balles@tuebingen.mpg.de;tfish@google.com,3;7;4,4;3;3,Reject,0,0,0.0,yes,9/27/18,"Max Planck Institute for Intelligent Systems, Max-Planck Institute;Google",-1;-1,-1;-1,1;2
2198,2198,2198,2198,2198,2198,2198,2198,ICLR,2019,Pixel Redrawn For A Robust Adversarial Defense,Jiacang Ho;Dae-Ki Kang,ho_jiacang@hotmail.com;dkkang@dongseo.ac.kr,4;6;3,3;3;4,Reject,22,3,0.0,yes,9/27/18,;Dongseo University,-1;478,-1;1103,4
2199,2199,2199,2199,2199,2199,2199,2199,ICLR,2019,HIGHLY EFFICIENT 8-BIT LOW PRECISION INFERENCE OF CONVOLUTIONAL NEURAL NETWORKS,Haihao Shen;Jiong Gong;Xiaoli Liu;Guoming Zhang;Ge Jin;and Eric Lin,haihao.shen@intel.com;jiong.gong@intel.com;xiaoli.liu@intel.com;guoming.zhang@intel.com;ge.jin@intel.com;eric.lin@intel.com,6;4;4,4;4;4,Reject,0,0,0.0,yes,9/27/18,Intel;Intel;Intel;Intel;Intel;Intel,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,2
2200,2200,2200,2200,2200,2200,2200,2200,ICLR,2019,Modulated Variational Auto-Encoders for Many-to-Many Musical Timbre Transfer,Adrien Bitton;Philippe Esling;Axel Chemla-Romeu-Santos,bitton@ircam.fr;philippe.esling@ircam.fr;axel.chemla-romeu-santos@ircam.fr,5;5;3,3;3;4,Reject,5,7,0.0,yes,9/27/18,Sorbonne Université;;,-1;-1;-1,-1;-1;-1,5;4
2201,2201,2201,2201,2201,2201,2201,2201,ICLR,2019,Geometry aware convolutional filters for omnidirectional images representation,Renata Khasanova;Pascal Frossard,renata.khasanova@epfl.ch;pascal.frossard@epfl.ch,4;6;4,4;4;5,Reject,2,4,0.0,yes,9/27/18,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne,478;478,38;38,2
2202,2202,2202,2202,2202,2202,2202,2202,ICLR,2019,A CASE STUDY ON OPTIMAL DEEP LEARNING MODEL FOR UAVS,Chandan Kumar;Subrahmanyam Vaddi;Aishwarya Sarkar,chandan@iastate.edu;svaddi@iastate.edu;asarkar1@iastate.edu,3;3;2,3;2;2,Reject,0,0,0.0,yes,9/27/18,Iowa State University;Iowa State University;Iowa State University,169;169;169,341;341;341,
2203,2203,2203,2203,2203,2203,2203,2203,ICLR,2019,Deep Perm-Set Net: Learn to predict sets with unknown permutation and cardinality using deep neural networks,S. Hamid Rezatofighi;Roman Kaskman;Farbod T. Motlagh;Qinfeng Shi;Daniel Cremers;Laura Leal-Taixé;Ian Reid,hamid.rezatofighi@adelaide.edu.au;roman.kaskman@tum.de;farbod.motlagh@student.adelaide.edu.au;javen.shi@adelaide.edu.au;cremers@tum.de;leal.taixe@tum.de;ian.reid@adelaide.edu.au,7;3;3,3;3;4,Reject,0,7,0.0,yes,9/27/18,The University of Adelaide;Technical University Munich;The University of Adelaide;The University of Adelaide;Technical University Munich;Technical University Munich;The University of Adelaide,123;54;123;123;54;54;123,134;41;134;134;41;41;134,2
2204,2204,2204,2204,2204,2204,2204,2204,ICLR,2019,DEEP-TRIM: REVISITING L1 REGULARIZATION FOR CONNECTION PRUNING OF DEEP NETWORK,Chih-Kuan Yeh;Ian E.H. Yen;Hong-You Chen;Chun-Pei Yang;Shou-De Lin;Pradeep Ravikumar,cjyeh@cs.cmu.edu;eyan2@snapchat.com;applebasket70179@gmail.com;skylyyang@gmail.com;sdlin@csie.ntu.edu.tw;pradeep.ravikumar@gmail.com,4;6;4,4;3;3,Reject,0,3,0.0,yes,9/27/18,Carnegie Mellon University;Snap Inc.;Ohio State University;;National Taiwan University;Carnegie Mellon University,1;-1;76;-1;85;1,24;-1;318;-1;197;24,9
2205,2205,2205,2205,2205,2205,2205,2205,ICLR,2019,INTERPRETABLE CONVOLUTIONAL FILTER PRUNING,Zhuwei Qin;Fuxun Yu;Chenchen Liu;Xiang Chen,zqin@gmu.edu;fyu2@gmu.edu;chliu@clarkson.edu;xchen26@gmu.edu,4;4;3,4;3;4,Reject,0,16,0.0,yes,9/27/18,George Mason University;George Mason University;Clarkson University;George Mason University,99;99;478;99,336;336;1103;336,
2206,2206,2206,2206,2206,2206,2206,2206,ICLR,2019,Learning Internal Dense But External Sparse Structures of Deep Neural Network,Yiqun Duan,duanyiquncc@gmail.com,5;5;6,3;3;2,Reject,0,5,0.0,yes,9/27/18,University of British Columbia,36,34,
2207,2207,2207,2207,2207,2207,2207,2207,ICLR,2019,Understand the dynamics of GANs via Primal-Dual Optimization,Songtao Lu;Rahul Singh;Xiangyi Chen;Yongxin Chen;Mingyi Hong,lus@umn.edu;rasingh@gatech.edu;chen5719@umn.edu;yongchen@gatech.edu;mhong@umn.edu,4;5;6,4;3;3,Reject,3,4,0.0,yes,9/27/18,"University of Minnesota, Minneapolis;Georgia Institute of Technology;University of Minnesota, Minneapolis;Georgia Institute of Technology;University of Minnesota, Minneapolis",57;13;57;13;57,56;33;56;33;56,5;4
2208,2208,2208,2208,2208,2208,2208,2208,ICLR,2019,Do Language Models Have Common Sense?,Trieu H. Trinh;Quoc V. Le,thtrieu@google.com;qvl@google.com,5;4;4,4;4;4,Reject,6,1,0.0,yes,9/27/18,Google;Google,-1;-1,-1;-1,3
2209,2209,2209,2209,2209,2209,2209,2209,ICLR,2019,"Prototypical Examples in Deep Learning: Metrics, Characteristics, and Utility",Nicholas Carlini;Ulfar Erlingsson;Nicolas Papernot,nicholas@carlini.com;ulfar@google.com;papernot@google.com,5;3;5,4;3;4,Reject,0,14,0.0,yes,9/27/18,University of California Berkeley;Google;Google,5;-1;-1,18;-1;-1,4
2210,2210,2210,2210,2210,2210,2210,2210,ICLR,2019,Learning From the Experience of Others: Approximate Empirical Bayes in Neural Networks,Han Zhao;Yao-Hung Hubert Tsai;Ruslan Salakhutdinov;Geoff Gordon,han.zhao@cs.cmu.edu;yaohungt@cs.cmu.edu;rsalakhu@cs.cmu.edu;ggordon@cs.cmu.edu,6;3;7,4;5;4,Reject,0,7,0.0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1,24;24;24;24,
2211,2211,2211,2211,2211,2211,2211,2211,ICLR,2019,Pearl: Prototype lEArning via Rule Lists,Tianfan Fu*;Tian Gao*;Cao Xiao*;Tengfei Ma*;Jimeng Sun,tfu42@gatech.edu;tgao@us.ibm.com;cxiao@us.ibm.com;tengfei.ma1@ibm.com;jsun@cc.gatech.edu,5;3;4,4;3;4,Reject,0,4,0.0,yes,9/27/18,Georgia Institute of Technology;International Business Machines;International Business Machines;International Business Machines;Georgia Institute of Technology,13;-1;-1;-1;13,33;-1;-1;-1;33,
2212,2212,2212,2212,2212,2212,2212,2212,ICLR,2019,Optimal margin Distribution Network,Shen-Huan Lv;Lu Wang;Zhi-Hua Zhou,lvsh@lamda.nju.edu.cn;wangl@lamda.nju.edu.cn;zhouzh@lamda.nju.edu.cn,5;5;6,4;5;3,Reject,3,4,0.0,yes,9/27/18,Zhejiang University;Zhejiang University;Zhejiang University,57;57;57,177;177;177,11;8
2213,2213,2213,2213,2213,2213,2213,2213,ICLR,2019,An Information-Theoretic Metric of Transferability for Task Transfer Learning,Yajie Bao;Yang Li;Shao-Lun Huang;Lin Zhang;Amir R. Zamir;Leonidas J. Guibas,byjem123@163.com;tori2011@gmail.com;shaolun.huang@sz.tsinghua.edu.cn;linzhang@tsinghua.edu.cn;zamir@cs.stanford.edu;guibas@cs.stanford.edu,5;6;6,3;3;4,Reject,0,5,0.0,yes,9/27/18,163;Tsinghua University;Tsinghua University;Tsinghua University;Stanford University;Stanford University,-1;8;8;8;4;4,-1;30;30;30;3;3,6
2214,2214,2214,2214,2214,2214,2214,2214,ICLR,2019,Selective Convolutional Units: Improving CNNs via Channel Selectivity,Jongheon Jeong;Jinwoo Shin,jongheonj@kaist.ac.kr;jinwoos@kaist.ac.kr,6;5;5,2;3;3,Reject,0,5,0.0,yes,9/27/18,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,20;20,95;95,
2215,2215,2215,2215,2215,2215,2215,2215,ICLR,2019,Sufficient Conditions for Robustness to Adversarial Examples: a Theoretical and Empirical Study with Bayesian Neural Networks,Yarin Gal;Lewis Smith,yarin@cs.ox.ac.uk;lsgs@robots.ox.ac.uk,5;5;4,4;3;4,Reject,0,16,1.0,yes,9/27/18,University of Oxford;University of Oxford,50;50,1;1,11;4;1
2216,2216,2216,2216,2216,2216,2216,2216,ICLR,2019,Empirical Bounds on Linear Regions of Deep Rectifier Networks,Thiago Serra;Srikumar Ramalingam,tserra@gmail.com;srikumar.ramalingam@gmail.com,6;7;6,4;4;4,Reject,0,5,0.0,yes,9/27/18,Carnegie Mellon University;University of Utah,1;52,24;200,1
2217,2217,2217,2217,2217,2217,2217,2217,ICLR,2019,Provable Defenses against Spatially Transformed Adversarial Inputs: Impossibility and Possibility Results,Xinyang Zhang;Yifan Huang;Chanh Nguyen;Shouling Ji;Ting Wang,xizc15@lehigh.edu;yih319@lehigh.edu;cpn217@lehigh.edu;sji@zju.edu.cn;inbox.ting@gmail.com,5;3;5,3;4;3,Reject,0,0,0.0,yes,9/27/18,Lehigh University;Lehigh University;Lehigh University;Zhejiang University;Lehigh University,261;261;261;57;261,533;533;533;177;533,4
2218,2218,2218,2218,2218,2218,2218,2218,ICLR,2019,On Meaning-Preserving Adversarial Perturbations for Sequence-to-Sequence Models,Paul Michel;Graham Neubig;Xian Li;Juan Miguel Pino,pmichel1@cs.cmu.edu;gneubig@cs.cmu.edu;xianl@fb.com;juancarabina@fb.com,4;6;4;3,4;3;4;4,Reject,0,11,0.0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;Facebook;Facebook,1;1;-1;-1,24;24;-1;-1,3;4
2219,2219,2219,2219,2219,2219,2219,2219,ICLR,2019,Collapse of deep and narrow neural nets,Lu Lu;Yanhui Su;George Em Karniadakis,lu_lu_1@brown.edu;suyh@fzu.edu.cn;george_karniadakis@brown.edu,7;6;4,5;4;4,Reject,0,7,0.0,yes,9/27/18,Brown University;Tsinghua University;Brown University,65;8;65,50;30;50,
2220,2220,2220,2220,2220,2220,2220,2220,ICLR,2019,Diminishing Batch Normalization,Yintai Ma;Diego Klabjan,yintaima2020@u.northwestern.edu;d-klabjan@northwestern.edu,4;3;4,4;3;5,Reject,0,5,0.0,yes,9/27/18,Northwestern University;Northwestern University,44;44,20;20,9;8
2221,2221,2221,2221,2221,2221,2221,2221,ICLR,2019,The Nonlinearity Coefficient - Predicting Generalization in Deep Neural Networks,George Philipp;Jaime G. Carbonell,george.philipp@email.de;jgc@cs.cmu.edu,4;5;7,3;4;4,Reject,1,11,0.0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University,1;1,24;24,8
2222,2222,2222,2222,2222,2222,2222,2222,ICLR,2019,Multi-way Encoding for Robustness to Adversarial Attacks,Donghyun Kim;Sarah Adel Bargal;Jianming Zhang;Stan Sclaroff,donhk@bu.edu;sbargal@bu.edu;jianmzha@adobe.com;sclaroff@bu.edu,4;6;6;6,4;3;2;2,Reject,3,14,1.0,yes,9/27/18,Boston University;Boston University;Adobe Systems;Boston University,65;65;-1;65,70;70;-1;70,4;2
2223,2223,2223,2223,2223,2223,2223,2223,ICLR,2019,Improved robustness to adversarial examples using Lipschitz regularization of the loss,Chris Finlay;Adam M. Oberman;Bilal Abbasi,christopher.finlay@mail.mcgill.ca;adam.oberman@mcgill.ca;bilal.abbasi@mail.mcgill.ca,4;6;6,3;1;3,Reject,1,10,0.0,yes,9/27/18,McGill University;McGill University;McGill University,85;85;85,42;42;42,4
2224,2224,2224,2224,2224,2224,2224,2224,ICLR,2019,CONTROLLING COVARIATE SHIFT USING EQUILIBRIUM NORMALIZATION OF WEIGHTS,Aaron Defazio,aaron.defazio@gmail.com,4;6;7,4;1;4,Reject,0,2,0.0,yes,9/27/18,Facebook,-1,-1,
2225,2225,2225,2225,2225,2225,2225,2225,ICLR,2019,Implicit Maximum Likelihood Estimation,Ke Li;Jitendra Malik,ke.li@eecs.berkeley.edu;malik@eecs.berkeley.edu,3;4;5,4;4;4,Reject,2,2,0.0,yes,9/27/18,University of California Berkeley;University of California Berkeley,5;5,18;18,
2226,2226,2226,2226,2226,2226,2226,2226,ICLR,2019,Mental Fatigue Monitoring using Brain Dynamics Preferences,Yuangang Pan;Avinash K Singh;Ivor W. Tsang;Chin-teng Lin,yuangang.pan@student.uts.edu.au;avinashsingh@outlook.com;ivor.tsang@uts.edu.au;chin-teng.lin@uts.edu.au,7;4;2,3;3;5,Reject,0,6,0.0,yes,9/27/18,University of Technology Sydney;University of Technology Sydney;University of Technology Sydney;University of Technology Sydney,106;106;106;106,216;216;216;216,8
2227,2227,2227,2227,2227,2227,2227,2227,ICLR,2019,Gradient-based learning for F-measure and other performance metrics,Yu Gai;Zheng Zhang;Kyunghyun Cho,yg1246@nyu.edu;zz@nyu.edu;kyunghyun.cho@nyu.edu,3;5;5,4;3;5,Reject,0,6,0.0,yes,9/27/18,New York University;New York University;New York University,26;26;26,27;27;27,
2228,2228,2228,2228,2228,2228,2228,2228,ICLR,2019,MANIFOLDNET: A DEEP NEURAL NETWORK FOR MANIFOLD-VALUED DATA,Rudrasis Chakraborty;Jose Bouza;Jonathan Manton;Baba C. Vemuri,rudrasischa@gmail.com;josebouza@ufl.edu;jonathan.manton@ieee.org;baba.vemuri@gmail.com,5;4;4,3;4;4,Reject,0,12,0.0,yes,9/27/18,University of California Berkeley;University of Florida;;University of Florida,5;123;-1;123,18;143;-1;143,1;8
2229,2229,2229,2229,2229,2229,2229,2229,ICLR,2019,REPRESENTATION COMPRESSION AND GENERALIZATION IN DEEP NEURAL NETWORKS,Ravid Shwartz-Ziv;Amichai Painsky;Naftali Tishby,ravid.ziv@mail.huji.ac.il;amichai.painsky@mail.huji.ac.il;tishby@cs.huji.ac.il,3;6;4,3;3;3,Reject,0,3,0.0,yes,9/27/18,Hebrew University of Jerusalem;Hebrew University of Jerusalem;Hebrew University of Jerusalem,65;65;65,205;205;205,1;8
2230,2230,2230,2230,2230,2230,2230,2230,ICLR,2019,Distributionally Robust Optimization Leads to Better Generalization: on SGD and Beyond,Jikai Hou;Kaixuan Huang;Zhihua Zhang,houjikai@pku.edu.cn;hackyhuang@pku.edu.cn;zhzhang@math.pku.edu.cn,3;4;5,3;4;4,Reject,9,5,0.0,yes,9/27/18,Peking University;Peking University;Peking University,24;24;24,27;27;27,1;9;8
2231,2231,2231,2231,2231,2231,2231,2231,ICLR,2019,A Priori Estimates  of the Generalization Error for Two-layer Neural Networks,Lei Wu;Chao Ma;Weinan E,leiwu@pku.edu.cn;chaom@princeton.edu;weinan@math.princeton.edu,4;4;4;5,3;3;4;3,Reject,0,0,0.0,yes,9/27/18,Peking University;Princeton University;Princeton University,24;30;30,27;7;7,8
2232,2232,2232,2232,2232,2232,2232,2232,ICLR,2019,Online Hyperparameter Adaptation via Amortized Proximal Optimization,Paul Vicol;Jeffery Z. HaoChen;Roger Grosse,pvicol@cs.toronto.edu;zhc15@mails.tsinghua.edu.cn;rgrosse@cs.toronto.edu,7;6;5,3;4;4,Reject,0,11,0.0,yes,9/27/18,"Department of Computer Science, University of Toronto;Tsinghua University;Department of Computer Science, University of Toronto",18;8;18,22;30;22,9
2233,2233,2233,2233,2233,2233,2233,2233,ICLR,2019,Adversarial Exploration Strategy for Self-Supervised Imitation Learning,Zhang-Wei Hong;Tsu-Jui Fu;Tzu-Yun Shann;Yi-Hsiang Chang;Chun-Yi Lee,williamd4112@gapp.nthu.edu.tw;yesray0216@gmail.com;ariel@shann.net;shawn420@gapp.nthu.edu.tw;cylee@cs.nthu.edu.tw,7;5;5,3;3;3,Reject,0,12,0.0,yes,9/27/18,National Tsing Hua University;;University of British Columbia;National Tsing Hua University;National Tsing Hua University,199;-1;36;199;199,323;-1;34;323;323,4
2234,2234,2234,2234,2234,2234,2234,2234,ICLR,2019,On Learning Heteroscedastic Noise Models within Differentiable Bayes Filters,Alina Kloss;Jeannette Bohg,alina.kloss@tuebingen.mpg.de;bohg@stanford.edu,6;6;4,4;5;4,Reject,0,6,0.0,yes,9/27/18,"Max Planck Institute for Intelligent Systems, Max-Planck Institute;Stanford University",-1;4,-1;3,11
2235,2235,2235,2235,2235,2235,2235,2235,ICLR,2019,Manifold regularization with GANs for semi-supervised learning,Bruno Lecouat;Chuan-Sheng Foo;Houssam Zenati;Vijay Chandrasekhar,bruno_lecouat@i2r.a-star.edu.sg;foo_chuan_sheng@i2r.a-star.edu.sg;houssam.zenati@student.ecp.fr;vijay@i2r.a-star.edu.sg,7;5;5,4;4;4,Reject,0,4,0.0,yes,9/27/18,A*STAR;A*STAR;Ecole Centrale Paris;A*STAR,-1;-1;478;-1,-1;-1;452;-1,5;4
2236,2236,2236,2236,2236,2236,2236,2236,ICLR,2019,SOSELETO: A Unified Approach to Transfer Learning and Training with Noisy Labels,Or Litany;Daniel Freedman,orlitany@gmail.com;danielfreedman@google.com,7;5;5,4;4;4,Reject,0,4,0.0,yes,9/27/18,Facebook;Google,-1;-1,-1;-1,6
2237,2237,2237,2237,2237,2237,2237,2237,ICLR,2019,SOLAR: Deep Structured Representations for Model-Based Reinforcement Learning,Marvin Zhang*;Sharad Vikram*;Laura Smith;Pieter Abbeel;Matthew Johnson;Sergey Levine,marvin@cs.berkeley.edu;svikram@cs.ucsd.edu;smithlaura@berkeley.edu;pabbeel@cs.berkeley.edu;mattjj@google.com;svlevine@cs.berkeley.edu,5;5;5,4;4;3,Reject,0,10,0.0,yes,9/27/18,"University of California Berkeley;University of California, San Diego;University of California Berkeley;University of California Berkeley;Google;University of California Berkeley",5;11;5;5;-1;5,18;31;18;18;-1;18,
2238,2238,2238,2238,2238,2238,2238,2238,ICLR,2019,Selective Self-Training for semi-supervised Learning,Jisoo Jeong;Seungeui Lee;Nojun Kwak,soo3553@snu.ac.kr;dehlix@snu.ac.kr;nojunk@snu.ac.kr,5;4;4,4;5;4,Reject,0,5,0.0,yes,9/27/18,Seoul National University;Seoul National University;Seoul National University,41;41;41,74;74;74,
2239,2239,2239,2239,2239,2239,2239,2239,ICLR,2019,Dissecting an Adversarial framework for Information Retrieval,Ameet Deshpande;Mitesh M.Khapra,cs15b001@cse.iitm.ac.in;miteshk@cse.iitm.ac.in,6;5;4,4;3;4,Reject,0,3,0.0,yes,9/27/18,Indian Institute of Technology Madras;Indian Institute of Technology Madras,153;153,625;625,5;4
2240,2240,2240,2240,2240,2240,2240,2240,ICLR,2019,Learning Latent Semantic Representation from Pre-defined Generative Model,Jin-Young Kim;Sung-Bae Cho,seago0828@yonsei.ac.kr;sbcho@yonsei.ac.kr,5;3;4,2;5;3,Reject,0,0,0.0,yes,9/27/18,Yonsei University;Yonsei University,478;478,231;231,5;1
2241,2241,2241,2241,2241,2241,2241,2241,ICLR,2019,Adversarial Information Factorization,Antonia Creswell;Yumnah Mohamied;Biswa Sengupta;Anil Bharath,ac2211@ic.ac.uk;ym1008@ic.ac.uk;biswasengupta@gmail.com;aab01@ic.ac.uk,6;6;6,4;4;4,Reject,2,27,0.0,yes,9/27/18,Imperial College London;Imperial College London;;Imperial College London,72;72;-1;72,8;8;-1;8,5
2242,2242,2242,2242,2242,2242,2242,2242,ICLR,2019,TequilaGAN: How To Easily Identify GAN Samples,Rafael Valle;Wilson Cai;Anish P. Doshi,rafaelvalle@berkeley.edu;wcai@berkeley.edu;apdoshi@berkeley.edu,5;4;6,4;4;5,Reject,0,5,0.0,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,18;18;18,5;4
2243,2243,2243,2243,2243,2243,2243,2243,ICLR,2019,Guided Evolutionary Strategies: Escaping the curse of dimensionality in random search,Niru Maheswaranathan;Luke Metz;George Tucker;Dami Choi;Jascha Sohl-Dickstein,nirum@google.com;lmetz@google.com;gjt@google.com;damichoi@google.com;jaschasd@google.com,5;4;6,3;3;4,Reject,0,5,0.0,yes,9/27/18,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,6
2244,2244,2244,2244,2244,2244,2244,2244,ICLR,2019,DL2: Training and Querying Neural Networks with Logic,Marc Fischer;Mislav Balunovic;Dana Drachsler-Cohen;Timon Gehr;Ce Zhang;Martin Vechev,marcfisc@student.ethz.ch;bmislav@student.ethz.ch;dana.drachsler@inf.ethz.ch;timon.gehr@inf.ethz.ch;ce.zhang@inf.ethz.ch;martin.vechev@inf.ethz.ch,6;7;5,2;4;4,Reject,0,8,0.0,yes,9/27/18,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10;10;10;10,10;10;10;10;10;10,
2245,2245,2245,2245,2245,2245,2245,2245,ICLR,2019,Déjà Vu: An Empirical Evaluation of the Memorization Properties of Convnets,Alexandre Sablayrolles;Matthijs Douze;Cordelia Schmid;Hervé Jégou,asablayrolles@fb.com;matthijs@fb.com;cordelia.schmid@inria.fr;rvj@fb.com,6;4;5,2;2;4,Reject,0,4,0.0,yes,9/27/18,Facebook;Facebook;INRIA;Facebook,-1;-1;-1;-1,-1;-1;-1;-1,
2246,2246,2246,2246,2246,2246,2246,2246,ICLR,2019,iRDA Method for Sparse Convolutional Neural Networks,Xiaodong Jia;Liang Zhao;Lian Zhang;Juncai He;Jinchao Xu,jiaxiaodong1994@gmail.com;zhaoliang14@lsec.cc.ac.cn;lzhangay@ust.hk;juncaihe@pku.edu.cn;xu@math.psu.edu,3;3;3,5;4;5,Reject,0,0,0.0,yes,9/27/18,Peking University;Chinese Academy of Sciences;The Hong Kong University of Science and Technology;Peking University;Pennsylvania State University,24;62;39;24;41,27;1103;44;27;77,
2247,2247,2247,2247,2247,2247,2247,2247,ICLR,2019,Variation Network: Learning High-level Attributes for Controlled Input Manipulation,Gaëtan Hadjeres,hadjeres.g@gmail.com,3;6;4,4;2;3,Reject,0,3,0.0,yes,9/27/18,,,,5
2248,2248,2248,2248,2248,2248,2248,2248,ICLR,2019,Consistency-based anomaly detection with adaptive multiple-hypotheses predictions,Duc Tam Nguyen;Zhongyu Lou;Michael Klar;Thomas Brox,nguyen@cs.uni-freiburg.de;zhongyu.lou@de.bosch.com;michael.klar2@de.bosch.com;brox@cs.uni-freiburg.de,4;5;5,4;3;4,Reject,0,6,0.0,yes,9/27/18,Universität Freiburg;Bosch;Bosch;Universität Freiburg,123;-1;-1;123,82;-1;-1;82,5;4
2249,2249,2249,2249,2249,2249,2249,2249,ICLR,2019,"VARIATIONAL SGD: DROPOUT , GENERALIZATION AND CRITICAL POINT AT THE END OF CONVEXITY",Michael Tetelman,michael.tetelman@gmail.com,4;2;2,3;4;5,Reject,0,1,2.0,yes,9/27/18,"ARRAIY, INC",-1,-1,11
2250,2250,2250,2250,2250,2250,2250,2250,ICLR,2019,Learning to remember: Dynamic Generative Memory for Continual Learning,Oleksiy Ostapenko;Mihai Puscas;Tassilo Klein;Moin Nabi,oleksiy.ostapenko@sap.com;mihai.puscas@sap.com;mihaimarian.puscas@unitn.it;tassilo.klein@sap.com;m.nabi@sap.com,4;3;8,5;5;5,Reject,0,7,0.0,yes,9/27/18,SAP;SAP;University of Trento;SAP;SAP,314;314;18;314;314,300;300;258;300;300,5;4
2251,2251,2251,2251,2251,2251,2251,2251,ICLR,2019,Perfect Match: A Simple Method for Learning Representations For Counterfactual Inference With Neural Networks,Patrick Schwab;Lorenz Linhardt;Walter Karlen,patrick.schwab@hest.ethz.ch;llorenz@student.ethz.ch;walter.karlen@hest.ethz.ch,5;5;6,3;4;4,Reject,0,11,0.0,yes,9/27/18,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10,10;10;10,
2252,2252,2252,2252,2252,2252,2252,2252,ICLR,2019,Pyramid Recurrent Neural Networks for Multi-Scale Change-Point Detection,Zahra Ebrahimzadeh;Min Zheng;Selcuk Karakas;Samantha Kleinberg,shina.ebiz@gmail.com;mzheng3@stevens.edu;fkarakas@stevens.edu;samantha.kleinberg@stevens.edu,7;4;6,4;5;3,Reject,0,3,0.0,yes,9/27/18,Stevens Institute of Technology;Stevens Institute of Technology;Stevens Institute of Technology;Stevens Institute of Technology,153;153;153;153,512;512;512;512,
2253,2253,2253,2253,2253,2253,2253,2253,ICLR,2019,BIGSAGE: unsupervised inductive representation learning of graph via bi-attended sampling and global-biased aggregating,Xin Luo;Hankz Hankui Zhuo,luox35@mail2.sysu.edu.cn;zhuohank@mail.sysu.edu.cn,2;4;4,4;3;4,Reject,0,0,0.0,yes,9/27/18,SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY,478;478,352;352,6;10
2254,2254,2254,2254,2254,2254,2254,2254,ICLR,2019,Stochastic Learning of Additive Second-Order Penalties with  Applications to Fairness,Heinrich Jiang;Yifan Wu;Ofir Nachum,heinrichj@google.com;yw4@andrew.cmu.edu;ofirnachum@google.com,5;5;4,3;4;3,Reject,0,3,0.0,yes,9/27/18,Google;Carnegie Mellon University;Google,-1;1;-1,-1;24;-1,7
2255,2255,2255,2255,2255,2255,2255,2255,ICLR,2019,Learning to Augment Influential Data,Donghoon Lee;Chang D. Yoo,iamdh@kaist.ac.kr;cd_yoo@kaist.ac.kr,6;6;5,4;4;4,Reject,0,3,1.0,yes,9/27/18,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,20;20,95;95,8
2256,2256,2256,2256,2256,2256,2256,2256,ICLR,2019,Generative Adversarial Network Training is a Continual Learning Problem,Kevin J Liang;Chunyuan Li;Guoyin Wang;Lawrence Carin,kevin.liang@duke.edu;chunyuan.li@duke.edu;guoyin.wang@duke.edu;lcarin@duke.edu,7;5;3,4;4;5,Reject,6,13,0.0,yes,9/27/18,Duke University;Duke University;Duke University;Duke University,44;44;44;44,17;17;17;17,5;4
2257,2257,2257,2257,2257,2257,2257,2257,ICLR,2019,Multi-objective training of Generative Adversarial Networks with multiple discriminators,Isabela Albuquerque;João Monteiro;Thang Doan;Breandan Considine;Tiago Falk;Ioannis Mitliagkas,isabelamcalbuquerque@gmail.com;joaomonteirof@gmail.com;thang.doan@mail.mcgill.ca;breandan.considine@gmail.com;falk@emt.inrs.ca;ioannis@iro.umontreal.ca,6;6;5,4;3;3,Reject,1,13,0.0,yes,9/27/18,Institut national de la recherche scientifique;Institut national de la recherche scientifique;McGill University;University of Montreal;Institut national de la recherche scientifique;University of Montreal,-1;-1;85;123;-1;123,-1;-1;42;108;-1;108,5;4
2258,2258,2258,2258,2258,2258,2258,2258,ICLR,2019,Few-Shot Intent Inference via Meta-Inverse Reinforcement Learning,Kelvin Xu;Ellis Ratner;Anca Dragan;Sergey Levine;Chelsea Finn,kelvinxu@eecs.berkeley.edu;eratner@berkeley.edu;anca@berkeley.edu;svlevine@eecs.berkeley.edu;cbfinn@eecs.berkeley.edu,3;4;4,5;3;4,Reject,0,8,0.0,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5;5,18;18;18;18;18,6
2259,2259,2259,2259,2259,2259,2259,2259,ICLR,2019,Latent Domain Transfer: Crossing modalities with Bridging Autoencoders,Yingtao Tian;Jesse Engel,yittian@cs.stonybrook.edu;jesseengel@google.com,4;4;4,4;4;4,Reject,1,3,0.0,yes,9/27/18,"State University of New York, Stony Brook;Google",41;-1,258;-1,5;4
2260,2260,2260,2260,2260,2260,2260,2260,ICLR,2019,TENSOR RING NETS ADAPTED DEEP MULTI-TASK LEARNING,Xinqi Chen;Ming Hou;Guoxu Zhou;Qibin Zhao,xinqicham@gmail.com;ming.hou@riken.jp;gx.zhou@gdut.edu.cn;qibin.zhao@riken.jp,5;6;4,4;4;4,Reject,0,7,0.0,yes,9/27/18,South China University of Technology;RIKEN;South China University of Technology;RIKEN,478;-1;478;-1,576;-1;576;-1,
2261,2261,2261,2261,2261,2261,2261,2261,ICLR,2019,Overcoming Multi-model Forgetting,Yassine Benyahia*;Kaicheng Yu*;Kamil Bennani-Smires;Martin Jaggi;Anthony Davison;Mathieu Salzmann;Claudiu Musat,yassine.benyahia1@gmail.com;kaicheng.yu@epfl.ch;kamil.bennani-smires@swisscom.com;martin.jaggi@epfl.ch;anthony.davison@epfl.ch;mathieu.salzmann@epfl.ch;claudiu.musat@swisscom.com,6;5;6,2;5;4,Reject,2,4,0.0,yes,9/27/18,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swisscom;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swisscom,478;478;-1;478;478;478;-1,38;38;-1;38;38;38;-1,3;2
2262,2262,2262,2262,2262,2262,2262,2262,ICLR,2019,Learning to Separate Domains in Generalized Zero-Shot and Open Set Learning: a probabilistic perspective,Hanze Dong;Yanwei Fu;Leonid Sigal;SungJu Hwang;Yu-Gang Jiang;Xiangyang Xue,hzdong15@fudan.edu.cn;yanweifu@fudan.edu.cn;lsigal@cs.ubc.ca;sjhwang82@kaist.ac.kr;ygj@fudan.edu.cn;xyxue@fudan.edu.cn,5;5;6,4;3;3,Reject,1,10,0.0,yes,9/27/18,Fudan University;Fudan University;University of British Columbia;Korea Advanced Institute of Science and Technology;Fudan University;Fudan University,78;78;36;20;78;78,116;116;34;95;116;116,6
2263,2263,2263,2263,2263,2263,2263,2263,ICLR,2019,Uncovering Surprising Behaviors in Reinforcement Learning via Worst-case Analysis,Avraham Ruderman;Richard Everett;Bristy Sikder;Hubert Soyer;Jonathan Uesato;Ananya Kumar;Charlie Beattie;Pushmeet Kohli,aruderman@google.com;reverett@google.com;bristy@google.com;soyer@google.com;juesato@google.com;skywalker94@gmail.com;cbeattie@google.com;pushmeet@google.com,7;5;6,4;3;2,Reject,0,5,0.0,yes,9/27/18,Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1,8
2264,2264,2264,2264,2264,2264,2264,2264,ICLR,2019,Efficient Codebook and Factorization for Second Order Representation Learning,Pierre jacob;David Picard;Aymeric Histace;Edouard Klein,pierre.jacob@ensea.fr;picard@ensea.fr;aymeric.histace@ensea.fr;edouard.klein@gendarmerie.interieur.gouv.fr,4;5;6,5;2;4,Reject,0,8,0.0,yes,9/27/18,ETIS;ETIS;ETIS;,-1;-1;-1;-1,-1;-1;-1;-1,
2265,2265,2265,2265,2265,2265,2265,2265,ICLR,2019,Metric-Optimized Example Weights,Sen Zhao;Mahdi Milani Fard;Maya Gupta,senzhao@google.com;mmilanifard@google.com;mayagupta@google.com,4;4;7,4;4;3,Reject,0,0,0.0,yes,9/27/18,Google;Google;Google,-1;-1;-1,-1;-1;-1,7
2266,2266,2266,2266,2266,2266,2266,2266,ICLR,2019,Unsupervised classification into unknown number of classes,Sungyeob Han;Daeyoung Kim;Jungwoo Lee,syhan@cml.snu.ac.kr;kimdy7@snu.ac.kr;junglee@snu.ac.kr,4;4;5,2;4;4,Reject,0,5,0.0,yes,9/27/18,Seoul National University;Seoul National University;Seoul National University,41;41;41,74;74;74,5;4;10
2267,2267,2267,2267,2267,2267,2267,2267,ICLR,2019,Fortified Networks: Improving the Robustness of Deep Networks by Modeling the Manifold of Hidden Representations,Alex Lamb;Jonathan Binas;Anirudh Goyal;Dmitriy Serdyuk;Sandeep Subramanian;Ioannis Mitliagkas;Yoshua Bengio,lambalex@iro.umontreal.ca;jonathan.binas@umontreal.ca;anirudhgoyal9119@gmail.com;serdyuk.dmitriy@gmail.com;sandeep.subramanian@gmail.com;ioannis@iro.umontreal.ca;yoshua.bengio@mila.quebec,4;5;9;6,5;3;4;3,Reject,0,27,0.0,yes,9/27/18,University of Montreal;University of Montreal;University of Montreal;Element AI;University of Montreal;University of Montreal;University of Montreal,123;123;123;-1;123;123;123,108;108;108;-1;108;108;108,4
2268,2268,2268,2268,2268,2268,2268,2268,ICLR,2019,Better Accuracy with Quantified Privacy: Representations Learned via Reconstructive Adversarial Network,Sicong Liu;Anshumali Shrivastava;Junzhao Du;Lin Zhong,scliu007@gmail.com;anshumali@rice.edu;dujz@xidian.edu.cn;lzhong@rice.edu,4;5;3,4;4;4,Reject,0,3,0.0,yes,9/27/18,Rice University;Rice University;Tsinghua University;Rice University,85;85;8;85,86;86;30;86,4
2269,2269,2269,2269,2269,2269,2269,2269,ICLR,2019,Sample-efficient policy learning in multi-agent Reinforcement Learning via meta-learning,Jialian Li;Hang Su;Jun Zhu,lijialian7@163.com;suhangss@mail.tsinghua.edu.cn;dcszj@mail.tsinghua.edu.cn,4;4;4,3;4;4,Reject,0,0,0.0,yes,9/27/18,Tsinghua University;Tsinghua University;Tsinghua University,8;8;8,30;30;30,6
2270,2270,2270,2270,2270,2270,2270,2270,ICLR,2019,UaiNets: From Unsupervised to Active Deep Anomaly Detection,Tiago Pimentel;Marianne Monteiro;Juliano Viana;Adriano Veloso;Nivio Ziviani,tiago.pimentel@kunumi.com;marianne@kunumi.com;juliano@kunumi.com;adrianov@dcc.ufmg.br;nivio@dcc.ufmg.br,4;5;3,4;2;4,Reject,3,4,0.0,yes,9/27/18,Universidade Federal de Minas Gerais;Kunumi;Kunumi;Universidade Federal de Minas Gerais;Universidade Federal de Minas Gerais,478;-1;-1;478;478,715;-1;-1;715;715,
2271,2271,2271,2271,2271,2271,2271,2271,ICLR,2019,Differentially Private Federated Learning: A Client Level Perspective,Robin C. Geyer;Tassilo J. Klein;Moin Nabi,geyerr@ethz.ch;tassilo.klein@sap.com;moin.nabi@sap.com,4;4;4,3;4;4,Reject,0,6,0.0,yes,9/27/18,Swiss Federal Institute of Technology;SAP;SAP,10;314;314,10;300;300,4
2272,2272,2272,2272,2272,2272,2272,2272,ICLR,2019,LSH Microbatches for Stochastic Gradients:  Value in Rearrangement,Eliav Buchnik;Edith Cohen;Avinatan Hassidim;Yossi Matias,edith@cohenwang.com;eliavbuh@gmail.com,4;4;4;3,4;3;4;2,Reject,0,5,0.0,yes,9/27/18,Tel Aviv University;Tel Aviv University,37;37,217;217,
2273,2273,2273,2273,2273,2273,2273,2273,ICLR,2019,Learning to Control Visual Abstractions for Structured Exploration in Deep Reinforcement Learning,catalin ionescu;tejas kulkarni;aaron van de oord;andriy mnih;vlad mnih,cdi@google.com;tkulkarni@google.com;avdnoord@google.com;amnih@google.com;vmnih@google.com,4;4;5,3;3;3,Reject,0,6,0.0,yes,9/27/18,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,
2274,2274,2274,2274,2274,2274,2274,2274,ICLR,2019,Large-Scale Study of Curiosity-Driven Learning,Yuri Burda;Harri Edwards;Deepak Pathak;Amos Storkey;Trevor Darrell;Alexei A. Efros,yburda@openai.com;harri@openai.com;pathak@berkeley.edu;a.storkey@ed.ac.uk;trevor@eecs.berkeley.edu;efros@eecs.berkeley.edu,6;9;7,4;5;3,Accept (Poster),0,4,0.0,yes,9/27/18,OpenAI;OpenAI;University of California Berkeley;University of Edinburgh;University of California Berkeley;University of California Berkeley,-1;-1;5;33;5;5,-1;-1;18;27;18;18,
2275,2275,2275,2275,2275,2275,2275,2275,ICLR,2019,Generative Models from the perspective of Continual Learning,Timothée Lesort;Hugo Caselles-Dupré;Michael Garcia-Ortiz;Jean-François Goudou;David Filliat,timothee.lesort@thalesgroup.com;caselles@ensta.fr;mgarciaortiz@softbankrobotics.com;jean-francois.goudou@thalesgroup.com;david.filliat@ensta.fr,4;4;5,4;4;3,Reject,0,4,0.0,yes,9/27/18,ENSTA ParisTech;ENSTA ParisTech;SoftBank Robotics Europe;Thalesgroup;ENSTA ParisTech,478;478;-1;-1;478,1103;1103;-1;-1;1103,5
2276,2276,2276,2276,2276,2276,2276,2276,ICLR,2019,Graph Classification with Geometric Scattering,Feng Gao;Guy Wolf;Matthew Hirn,gaofeng2@msu.edu;guy.wolf@yale.edu;mhirn@msu.edu,5;6;5,4;3;4,Reject,0,6,0.0,yes,9/27/18,SUN YAT-SEN UNIVERSITY;Yale University;SUN YAT-SEN UNIVERSITY,478;62;478,352;12;352,10
2277,2277,2277,2277,2277,2277,2277,2277,ICLR,2019,An investigation of model-free planning,Arthur Guez;Mehdi Mirza;Karol Gregor;Rishabh Kabra;Sébastien Racanière;Théophane Weber;David Raposo;Adam Santoro;Laurent Orseau;Tom Eccles;Greg Wayne;David Silver;Timothy Lillicrap,aguez@google.com;mmirza@google.com;karolg@google.com;rkabra@google.com;sracaniere@google.com;theophane@google.com;draposo@google.com;adamsantoro@google.com;lorseau@google.com;eccles@google.com;gregwayne@google.com;davidsilver@google.com;countzero@google.com,5;5;4,4;3;5,Reject,0,4,0.0,yes,9/27/18,Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,
2278,2278,2278,2278,2278,2278,2278,2278,ICLR,2019,Learning What to Remember: Long-term Episodic Memory Networks for Learning from Streaming Data,Hyunwoo Jung;Moonsu Han;Minki Kang;Sungju Hwang,hyunwooj@kaist.ac.kr;mshan92@kaist.ac.kr;zzxc1133@kaist.ac.kr;sjhwang82@kaist.ac.kr,5;4;4,5;4;4,Reject,0,0,3.0,yes,9/27/18,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,20;20;20;20,95;95;95;95,
2279,2279,2279,2279,2279,2279,2279,2279,ICLR,2019,Multi-Task Learning for Semantic Parsing with Cross-Domain Sketch,Huan Wang;Yuxiang Hu;Li Dong;Feijun Jiang;Zaiqing Nie,odile.wh@alibaba-inc.com;yuxiang.hyx@alibaba-inc.com;li.dong@ed.ac.uk;feijun.jiangfj@alibaba-inc.com;zaiqing.nzq@alibaba-inc.com,3;4;5,4;4;4,Reject,0,0,4.0,yes,9/27/18,Alibaba Group;Alibaba Group;University of Edinburgh;Alibaba Group;Alibaba Group,-1;-1;33;-1;-1,-1;-1;27;-1;-1,3
2280,2280,2280,2280,2280,2280,2280,2280,ICLR,2019,GEOMETRIC AUGMENTATION FOR ROBUST NEURAL NETWORK CLASSIFIERS,Robert M. Taylor;Yusong Tan,rtaylor@mitre.org;ytan@mitre.org,4;4;3,3;4;5,Reject,0,0,0.0,yes,9/27/18,Virginia Tech;,81;-1,300;-1,4;11
2281,2281,2281,2281,2281,2281,2281,2281,ICLR,2019,Overlapping Community Detection with Graph Neural Networks,Oleksandr Shchur;Stephan Günnemann,shchur@in.tum.de;guennemann@in.tum.de,5;3;4,4;5;5,Reject,0,0,0.0,yes,9/27/18,Technical University Munich;Technical University Munich,54;54,41;41,10
2282,2282,2282,2282,2282,2282,2282,2282,ICLR,2019,A Guider Network for Multi-Dual Learning,Wenpeng Hu;Zhengwei Tao;Zhanxing Zhu;Bing Liu;Zhou Lin;Jinwen Ma;Dongyan Zhao;Rui Yan,wenpeng.hu@pku.edu.cn;tttzw@pku.edu.cn;zhanxing.zhu@pku.edu.cn;liub@uic.edu;jokerlin@pku.edu.cn;jwma@math.pku.edu.cn;zhaody@pku.edu.cn;ruiyan@pku.edu.cn,4;5;4,3;2;5,Reject,0,0,0.0,yes,9/27/18,"Peking University;Peking University;Peking University;University of Illinois, Chicago;Peking University;Peking University;Peking University;Peking University",24;24;24;57;24;24;24;24,27;27;27;255;27;27;27;27,3
2283,2283,2283,2283,2283,2283,2283,2283,ICLR,2019,Dual Learning: Theoretical Study and Algorithmic Extensions,Zhibing Zhao;Yingce Xia;Tao Qin;Tie-Yan Liu,zhaoz6@rpi.edu;yingce.xia@gmail.com;taoqin@microsoft.com;tyliu@microsoft.com,6;2;5,3;4;3,Reject,0,1,0.0,yes,9/27/18,Rensselaer Polytechnic Institute;Microsoft;Microsoft;Microsoft,169;-1;-1;-1,304;-1;-1;-1,3;1
2284,2284,2284,2284,2284,2284,2284,2284,ICLR,2019,Real-time Neural-based Input Method,Jiali Yao;Raphael Shu;Xinjian Li;Katsutoshi Ohtsuki;Hideki Nakayama,jiayao@microsoft.com;shu@nlab.ci.i.u-tokyo.ac.jp;xinjianl@andrew.cmu.edu;katsutoshi.ohtsuki@microsoft.com;nakayama@ci.i.u-tokyo.ac.jp,3;3;3,3;4;3,Reject,0,3,0.0,yes,9/27/18,Microsoft;The University of Tokyo;Carnegie Mellon University;Microsoft;The University of Tokyo,-1;54;1;-1;54,-1;45;24;-1;45,3
2285,2285,2285,2285,2285,2285,2285,2285,ICLR,2019,CHEMICAL NAMES STANDARDIZATION USING NEURAL SEQUENCE TO SEQUENCE MODEL,Junlang Zhan;Hai Zhao,longmr.zhan@sjtu.edu.cn;zhaohai@cs.sjtu.edu.cn,4;3;7,4;5;3,Reject,0,9,0.0,yes,9/27/18,Shanghai Jiao Tong University;Shanghai Jiao Tong University,52;52,188;188,
2286,2286,2286,2286,2286,2286,2286,2286,ICLR,2019,TabNN: A Universal Neural Network Solution for Tabular Data,Guolin Ke;Jia Zhang;Zhenhui Xu;Jiang Bian;Tie-Yan Liu,guolin.ke@microsoft.com;jia.zhang@microsoft.com;zhenhui.xu@pku.edu.cn;jiang.bian@microsoft.com;tyliu@microsoft.com,5;4;5,4;5;2,Reject,0,6,0.0,yes,9/27/18,Microsoft;Microsoft;Peking University;Microsoft;Microsoft,-1;-1;24;-1;-1,-1;-1;27;-1;-1,
2287,2287,2287,2287,2287,2287,2287,2287,ICLR,2019,Learning Representations of Categorical Feature Combinations via Self-Attention,Chen Xu;Chengzhen Fu;Peng Jiang;Wenwu Ou,chaos.xc@alibaba-inc.com;fuchengzhen@pku.edu.cn;jiangpeng.jp@alibaba-inc.com;wenwu.ou@alibaba-inc.com,5;5;5,4;3;4,Reject,0,0,0.0,yes,9/27/18,Alibaba Group;Peking University;Alibaba Group;Alibaba Group,-1;24;-1;-1,-1;27;-1;-1,
2288,2288,2288,2288,2288,2288,2288,2288,ICLR,2019,Spatial-Winograd Pruning Enabling Sparse Winograd Convolution,Jiecao Yu;Jongsoo Park;Maxim Naumov,jiecaoyu@umich.edu;jongsoo@fb.com;mnaumov@fb.com,5;4;6,3;3;3,Reject,0,8,0.0,yes,9/27/18,University of Michigan;Facebook;Facebook,8;-1;-1,21;-1;-1,
2289,2289,2289,2289,2289,2289,2289,2289,ICLR,2019,Phrase-Based Attentions,Phi Xuan Nguyen;Shafiq Joty,xuanphi001@e.ntu.edu.sg;srjoty@ntu.edu.sg,5;5;5,4;5;5,Reject,0,13,0.0,yes,9/27/18,National Taiwan University;National Taiwan University,85;85,197;197,3
2290,2290,2290,2290,2290,2290,2290,2290,ICLR,2019,Adaptive Pruning of Neural Language Models for Mobile Devices,Raphael Tang;Jimmy Lin,r33tang@uwaterloo.ca;jimmylin@uwaterloo.ca,6;5;6,4;3;4,Reject,0,5,0.0,yes,9/27/18,University of Waterloo;University of Waterloo,26;26,207;207,3
2291,2291,2291,2291,2291,2291,2291,2291,ICLR,2019,Total Style Transfer with a Single Feed-Forward Network,Minseong Kim;Hyun-Chul Choi,tyui592@ynu.ac.kr;pogary@ynu.ac.kr,4;5;4,5;5;3,Reject,2,0,0.0,yes,9/27/18,Yeungnam University.;Yeungnam University.,478;478,749;749,
2292,2292,2292,2292,2292,2292,2292,2292,ICLR,2019,Mixture of Pre-processing Experts Model for Noise Robust Deep Learning on Resource Constrained Platforms,Taesik Na;Minah Lee;Burhan A. Mudassar;Priyabrata Saha;Jong Hwan Ko;Saibal Mukhopadhyay,taesik.na@gatech.edu;minah.lee@gatech.edu;burhan.mudassar@gatech.edu;priyabratasaha@gatech.edu;jonghwan.ko@gatech.edu;smukhopadhyay6@gatech.edu,3;4;4,4;5;4,Reject,0,3,0.0,yes,9/27/18,Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology,13;13;13;13;13;13,33;33;33;33;33;33,4;2
2293,2293,2293,2293,2293,2293,2293,2293,ICLR,2019,Multi-Scale Stacked Hourglass Network for Human Pose Estimation,Chunsheng Guo;Wenlong Du;Na Ying,guo.chsh@gmail.com;dwl1993@hdu.edu.cn;yingna@hdu.edu.cn,3;4;3,5;4;5,Reject,3,0,0.0,yes,9/27/18,Shandong University;Shandong University;Shandong University,4;4;4,3;3;3,2
2294,2294,2294,2294,2294,2294,2294,2294,ICLR,2019,A quantifiable testing of global translational invariance in Convolutional and Capsule Networks,Weikai Qi,wikaiqi@gmail.com,3;4;3,5;4;5,Reject,0,0,0.0,yes,9/27/18,university of guelph,261,1103,
2295,2295,2295,2295,2295,2295,2295,2295,ICLR,2019,Multiple Encoder-Decoders Net for Lane Detection,Yuetong Du;Xiaodong Gu;Junqin Liu;Liwen He,1239832590@qq.com;gu3xuan@qq.com;65581134@qq.com;helw@njupt.edu.cn,2;2;4,4;5;4,Reject,0,0,0.0,yes,9/27/18,Tsinghua University;;;Tsinghua University,8;-1;-1;8,30;-1;-1;30,2
2296,2296,2296,2296,2296,2296,2296,2296,ICLR,2019,Unsupervised Video-to-Video Translation,Dina Bashkirova;Ben Usman;Kate Saenko,dbash@bu.edu;usmn@bu.edu;saenko@bu.edu,3;4;4,4;5;5,Reject,0,0,0.0,yes,9/27/18,Boston University;Boston University;Boston University,65;65;65,70;70;70,2
2297,2297,2297,2297,2297,2297,2297,2297,ICLR,2019,Generative model based on minimizing exact empirical Wasserstein distance,Akihiro Iohara;Takahito Ogawa;Toshiyuki Tanaka,iohara@sys.i.kyoto-u.ac.jp;takahito.ogawa@datagrid.co.jp;tt@i.kyoto-u.ac.jp,5;2;3,2;5;4,Reject,0,0,1.0,yes,9/27/18,Meiji University;DataGrid Inc.;Meiji University,478;-1;478,334;-1;334,5;4
2298,2298,2298,2298,2298,2298,2298,2298,ICLR,2019,Deli-Fisher GAN: Stable and Efficient Image Generation With Structured Latent Generative Space,Boli Fang;Chuck Jia;Miao Jiang;Dhawal Chaturvedi,bfang@iu.edu;jiac@iu.edu;miajiang@iu.edu;dhchat@iu.edu,2;2;3,4;5;5,Reject,0,1,0.0,yes,9/27/18,"Indiana University, Bloomington;Indiana University, Bloomington;Indiana University, Bloomington;Indiana University, Bloomington",72;72;72;72,117;117;117;117,5;4
2299,2299,2299,2299,2299,2299,2299,2299,ICLR,2019,Stacking for Transfer Learning,Peng Yuankai,pyk3350266@163.com,3;4;2,5;5;5,Reject,0,0,0.0,yes,9/27/18,Xi'an Jiaotong University,478,565,6;8
2300,2300,2300,2300,2300,2300,2300,2300,ICLR,2019,Graph Spectral Regularization For Neural Network Interpretability,Alexander Tong;David van Dijk;Jay Stanley;Guy Wolf;Smita Krishnaswamy,alexander.tong@yale.edu;david.vandijk@yale.edu;jay.stanley@yale.edu;guy.wolf@yale.edu;smita.krishnaswamy@yale.edu,4;3;4,4;5;3,Reject,0,0,0.0,yes,9/27/18,Yale University;Yale University;Yale University;Yale University;Yale University,62;62;62;62;62,12;12;12;12;12,10
2301,2301,2301,2301,2301,2301,2301,2301,ICLR,2019,RETHINKING SELF-DRIVING : MULTI -TASK KNOWLEDGE FOR BETTER GENERALIZATION AND ACCIDENT EXPLANATION ABILITY,Zhihao LI;Toshiyuki MOTOYOSHI;Kazuma SASAKI;Tetsuya OGATA;Shigeki SUGANO,mr.zhihao.li@gmail.com;motoyoshi@idr.ias.sci.waseda.ac.jp;ssk.sasaki@suou.waseda.jp;ogata@waseda.jp;sugano@waseda.jp,4;4;3,4;5;4,Reject,2,0,0.0,yes,9/27/18,Waseda University;Meiji University;Waseda University;Waseda University;Waseda University,261;478;261;261;261,642;334;642;642;642,2;8
2302,2302,2302,2302,2302,2302,2302,2302,ICLR,2019,Normalization Gradients are Least-squares Residuals,Yi Liu,liu.yi.pei@gmail.com,4;4;3,5;4;4,Reject,0,3,0.0,yes,9/27/18,,,,
2303,2303,2303,2303,2303,2303,2303,2303,ICLR,2019,Task-GAN for Improved GAN based Image Restoration,Jiahong Ouyang;Guanhua Wang;Enhao Gong;Kevin Chen;John Pauly;Greg Zaharchuk,jiahongo@stanford.edu;guanhua@stanford.edu;enhaog@stanford.edu;ktchen@stanford.edu;pauly@stanford.edu;gregz@stanford.edu,4;5;4,5;4;5,Reject,0,0,0.0,yes,9/27/18,Stanford University;Stanford University;Stanford University;Stanford University;Stanford University;Stanford University,4;4;4;4;4;4,3;3;3;3;3;3,5;4;2
2304,2304,2304,2304,2304,2304,2304,2304,ICLR,2019,Morpho-MNIST: Quantitative Assessment and Diagnostics for Representation Learning,Daniel C. Castro;Jeremy Tan;Bernhard Kainz;Ender Konukoglu;Ben Glocker,d.coelho-de-castro15@imperial.ac.uk;j.tan17@imperial.ac.uk;b.kainz@imperial.ac.uk;kender@vision.ee.ethz.ch;b.glocker@imperial.ac.uk,3;5;4,4;3;3,Reject,0,5,0.0,yes,9/27/18,Imperial College London;Imperial College London;Imperial College London;Swiss Federal Institute of Technology;Imperial College London,72;72;72;10;72,8;8;8;10;8,5;4
2305,2305,2305,2305,2305,2305,2305,2305,ICLR,2019,Small steps and giant leaps: Minimal Newton solvers for Deep Learning,Joao Henriques;Sebastien Ehrhardt;Samuel Albanie;Andrea Vedaldi,joao@robots.ox.ac.uk;hyenal@robots.ox.ac.uk;albanie@robots.ox.ac.uk;vedali@robots.ox.ac.uk,7;3;7,5;5;4,Reject,0,11,1.0,yes,9/27/18,University of Oxford;University of Oxford;University of Oxford;University of Oxford,50;50;50;50,1;1;1;1,
2306,2306,2306,2306,2306,2306,2306,2306,ICLR,2019,Characterizing the Accuracy/Complexity Landscape of Explanations of Deep Networks through Knowledge Extraction,Simon Odense;Artur d'Avila Garcez,simon.odense@city.ac.uk;a.garcez@city.ac.uk,4;4;4;5,2;5;4;3,Reject,0,4,0.0,yes,9/27/18,"City, University of London;City, University of London",314;314,373;373,
2307,2307,2307,2307,2307,2307,2307,2307,ICLR,2019,When Will Gradient Methods Converge to Max-margin Classifier under ReLU Models?,Tengyu Xu;Yi Zhou;Kaiyi Ji;Yingbin Liang,xu.3260@osu.edu;zhou.1172@osu.edu;ji.367@osu.edu;liang.889@osu.edu,5;4;5,5;3;4,Reject,0,0,0.0,yes,9/27/18,Ohio State University;Ohio State University;Ohio State University;Ohio State University,76;76;76;76,318;318;318;318,
2308,2308,2308,2308,2308,2308,2308,2308,ICLR,2019,A NON-LINEAR  THEORY FOR SENTENCE EMBEDDING,Hichem Mezaoui;Isar Nejadgholi,hichem@imrsv.ai;isar@imrsv.ai,3;3;3,3;3;4,Reject,0,0,1.0,yes,9/27/18,;,-1;-1,-1;-1,
2309,2309,2309,2309,2309,2309,2309,2309,ICLR,2019,The loss landscape of overparameterized neural networks,Y. Cooper,yaim@math.ias.edu,5;7;5,4;3;4,Reject,0,16,0.0,yes,9/27/18,"Institue for Advanced Study, Princeton",-1,-1,1
2310,2310,2310,2310,2310,2310,2310,2310,ICLR,2019,Stop memorizing: A data-dependent regularization framework for intrinsic pattern learning,Wei Zhu;Qiang Qiu;Bao Wang;Jianfeng Lu;Guillermo Sapiro;Ingrid Daubechies,zhu@math.duke.edu;qiang.qiu@duke.edu;wangbao@math.ucla.edu;jianfeng@math.duke.edu;guillermo.sapiro@duke.edu;ingrid@math.duke.edu,7;4;4,4;3;4,Reject,0,4,0.0,yes,9/27/18,"Duke University;Duke University;University of California, Los Angeles;Duke University;Duke University;Duke University",44;44;20;44;44;44,17;17;15;17;17;17,
2311,2311,2311,2311,2311,2311,2311,2311,ICLR,2019,Neural Variational Inference For Embedding Knowledge Graphs,Alexander I. Cowen-Rivers;Pasquale Minervini,mc_rivers@icloud.com;p.minervini@ucl.ac.uk,5;5;4,3;5;3,Reject,0,7,0.0,yes,9/27/18,University College London;University College London,50;50,16;16,5;10
2312,2312,2312,2312,2312,2312,2312,2312,ICLR,2019,Learning Implicit Generative Models by Teaching Explicit Ones,Chao Du;Kun Xu;Chongxuan Li;Jun Zhu;Bo Zhang,duchao0726@gmail.com;kunxu.thu@gmail.com;chongxuanli1991@gmail.com;dcszj@tsinghua.edu.cn;dcszb@tsinghua.edu.cn,6;7;5,4;3;4,Reject,0,6,0.0,yes,9/27/18,Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University,8;8;8;8;8,30;30;30;30;30,5;4
2313,2313,2313,2313,2313,2313,2313,2313,ICLR,2019,"The GAN Landscape: Losses, Architectures, Regularization, and Normalization",Karol Kurach;Mario Lucic;Xiaohua Zhai;Marcin Michalski;Sylvain Gelly,kkurach@gmail.com;lucic@google.com;xzhai@google.com;michalski@google.com;sylvain.gelly@gmail.com,4;4;7,3;2;4,Reject,0,3,0.0,yes,9/27/18,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,5;4
2314,2314,2314,2314,2314,2314,2314,2314,ICLR,2019,PAIRWISE AUGMENTED GANS WITH ADVERSARIAL RECONSTRUCTION LOSS,Aibek Alanov;Max Kochurov;Daniil Yashkov;Dmitry Vetrov,alanov.aibek@gmail.com;maxim.v.kochurov@gmail.com;daniil.yashkov@phystech.edu;vetrodim@gmail.com,6;4;5,4;3;4,Reject,0,5,0.0,yes,9/27/18,Higher School of Economics;Skolkovo Institute of Science and Technology;Moscow Institute of Physics and Technology;Higher School of Economics,478;-1;478;478,377;-1;254;377,5;4
2315,2315,2315,2315,2315,2315,2315,2315,ICLR,2019,Improving Sample-based Evaluation for Generative Adversarial Networks,Shaohui Liu*;Yi Wei*;Jiwen Lu;Jie Zhou,b1ueber2y@gmail.com;wei-y15@mails.tsinghua.edu.cn;lujiwen@tsinghua.edu.cn;jzhou@tsinghua.edu.cn,5;5;3,3;4;5,Reject,0,1,0.0,yes,9/27/18,Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University,8;8;8;8,30;30;30;30,5;4
2316,2316,2316,2316,2316,2316,2316,2316,ICLR,2019,Tinkering with black boxes: counterfactuals uncover modularity in generative models,Michel Besserve;Remy Sun;Bernhard Schoelkopf,michel.besserve@tuebingen.mpg.de;remy.sun@ens-rennes.fr;bs@tuebingen.mpg.de,4;6;4,5;3;4,Reject,0,3,0.0,yes,9/27/18,"Max Planck Institute for Intelligent Systems, Max-Planck Institute;Ecole Normale Superieure de Rennes;Max Planck Institute for Intelligent Systems, Max-Planck Institute",-1;478;-1,-1;1103;-1,5;4
2317,2317,2317,2317,2317,2317,2317,2317,ICLR,2019,Deep Generative Models for learning Coherent Latent Representations from Multi-Modal Data,Timo Korthals;Marc Hesse;Jürgen Leitner,korthals.timo@gmail.com;mhesse@cit-ec.uni-bielefeld.de;juxi.leitner@gmail.com,4;4;4,2;3;4,Reject,0,6,0.0,yes,9/27/18,"Bielefeld University, CITEC;Bielefeld University;South China University of Technology",314;314;478,288;288;576,5
2318,2318,2318,2318,2318,2318,2318,2318,ICLR,2019,Mol-CycleGAN - a generative model for molecular optimization,Łukasz Maziarka;Agnieszka Pocha;Jan Kaczmarczyk;Michał Warchoł,l.maziarka@gmail.com;lamiane.chan@gmail.com;jan.kaczmarczyk@ardigen.com;michal.warchol@ardigen.com,4;4;4,5;4;3,Reject,0,0,0.0,yes,9/27/18,Ardigen;;Ardigen;Ardigen,-1;-1;-1;-1,-1;-1;-1;-1,5
2319,2319,2319,2319,2319,2319,2319,2319,ICLR,2019,Effective and Efficient Batch Normalization Using Few Uncorrelated Data for Statistics' Estimation,Zhaodong Chen;Lei Deng;Guoqi Li;Jiawei Sun;Xing Hu;Ling Liang;YufeiDing;Yuan Xie,chenzd15@mails.tsinghua.edu.cn;leideng@ucsb.edu;liguoqi@mail.tsinghua.edu.cn;sunjw15@mails.tsinghua.edu.cn;xinghu@ucsb.edu;lingliang@ucsb.edu;yufeiding@cs.ucsb.edu;yuanxie@ucsb.edu,5;4;5,5;3;3,Reject,0,5,0.0,yes,9/27/18,Tsinghua University;UC Santa Barbara;Tsinghua University;Tsinghua University;UC Santa Barbara;UC Santa Barbara;UC Santa Barbara;UC Santa Barbara,8;37;8;8;37;37;37;37,30;53;30;30;53;53;53;53,9
2320,2320,2320,2320,2320,2320,2320,2320,ICLR,2019,Contextual Recurrent Convolutional Model for Robust Visual Learning,Siming Yan*;Bowen Xiao*;Yimeng Zhang;Tai Sing Lee,simingyan@pku.edu.cn;mike.xiao@pku.edu.cn;zym1010@gmail.com;taislee@andrew.cmu.edu,4;3;4,4;5;5,Reject,0,5,0.0,yes,9/27/18,Peking University;Peking University;;Carnegie Mellon University,24;24;-1;1,27;27;-1;24,2
2321,2321,2321,2321,2321,2321,2321,2321,ICLR,2019,A unified theory of adaptive stochastic gradient descent as Bayesian filtering,Laurence Aitchison,laurence.aitchison@gmail.com,5;5;7,3;4;4,Reject,0,29,0.0,yes,9/27/18,HHMI Janelia Research Campus,-1,-1,11
2322,2322,2322,2322,2322,2322,2322,2322,ICLR,2019,MEAN-FIELD ANALYSIS OF BATCH NORMALIZATION,Mingwei Wei;James Stokes;David J Schwab,m.wei@u.northwestern.edu;james@tunnel.tech;dschwab@gc.cuny.edu,7;6;5,3;3;3,Reject,0,6,0.0,yes,9/27/18,Northwestern University;;The City College of New York,44;-1;199,20;-1;1103,
2323,2323,2323,2323,2323,2323,2323,2323,ICLR,2019,Reconciling Feature-Reuse and Overfitting in DenseNet with Specialized Dropout,Kun Wan;Boyuan Feng;Lingwei Xie;Yufei Ding,kun@cs.ucsb.edu;boyuan@cs.ucsb.edu;xielingwei@stu.xmu.edu.cn;yufeiding@cs.ucsb.edu,5;3;4,4;3;4,Reject,0,0,0.0,yes,9/27/18,UC Santa Barbara;UC Santa Barbara;Xiamen University;UC Santa Barbara,37;37;62;37,53;53;12;53,
2324,2324,2324,2324,2324,2324,2324,2324,ICLR,2019,PCNN: Environment Adaptive Model Without Finetuning,Boyuan Feng;Kun Wan;Shu Yang;Yufei Ding,boyuan@cs.ucsb.edu;kun@cs.ucsb.edu;shuyang1995@ucsb.edu;yufeiding@cs.ucsb.edu,4;3;4,4;4;4,Reject,1,3,0.0,yes,9/27/18,UC Santa Barbara;UC Santa Barbara;UC Santa Barbara;UC Santa Barbara,37;37;37;37,53;53;53;53,6;2
2325,2325,2325,2325,2325,2325,2325,2325,ICLR,2019,ATTENTION INCORPORATE NETWORK: A NETWORK CAN ADAPT VARIOUS DATA SIZE,Liangbo He;Hao Sun,heliangbo@tsinghua.edu.cn;sh759811581@tsinghua.edu.cn,3;4;2,5;4;4,Reject,0,0,0.0,yes,9/27/18,Tsinghua University;Tsinghua University,8;8,30;30,
2326,2326,2326,2326,2326,2326,2326,2326,ICLR,2019,What a difference a pixel makes: An empirical examination of features used by CNNs for categorisation,Gaurav Malhotra;Jeffrey Bowers,gaurav.malhotra@bristol.ac.uk;j.bowers@bristol.ac.uk,4;4;7,4;4;5,Reject,1,12,0.0,yes,9/27/18,University of Bristol;University of Bristol,123;123,76;76,
2327,2327,2327,2327,2327,2327,2327,2327,ICLR,2019,Trace-back along capsules and its application on semantic segmentation  		,Tao Sun;Zhewei Wang;C. D. Smith;Jundong Liu,zw340113@ohio.edu;ts202115@ohio.edu;cdsmith.uk@gmail.com;liuj1@ohio.edu,6;6;5,4;3;4,Reject,0,12,0.0,yes,9/27/18,Ohio University;Ohio University;;Ohio University,386;386;-1;386,627;627;-1;627,2
2328,2328,2328,2328,2328,2328,2328,2328,ICLR,2019,"Neural Network Regression with Beta, Dirichlet, and Dirichlet-Multinomial Outputs",Peter Sadowski;Pierre Baldi,peter.sadowski@hawaii.edu;pfbaldi@ics.uci.edu,3;3;4,4;5;4,Reject,1,1,0.0,yes,9/27/18,"University of Hawaii, Manoa;University of California, Irvine",386;35,204;99,
2329,2329,2329,2329,2329,2329,2329,2329,ICLR,2019,Conditional Inference in Pre-trained Variational Autoencoders via Cross-coding,Ga Wu;Justin Domke;Scott Sanner,wuga@mie.utoronto.ca;domke@cs.umass.edu;ssanner@mie.utoronto.ca,4;4;4,4;4;5,Reject,0,2,0.0,yes,9/27/18,"Toronto University;University of Massachusetts, Amherst;Toronto University",18;30;18,22;191;22,5
2330,2330,2330,2330,2330,2330,2330,2330,ICLR,2019,Neural Rendering Model: Joint Generation and Prediction for Semi-Supervised Learning,Nhat Ho;Tan Nguyen;Ankit B. Patel;Anima Anandkumar;Michael I. Jordan;Richard G. Baraniuk,minhnhat@berkeley.edu;mn15@rice.edu;ankit.patel@bcm.edu;anima@caltech.edu;jordan@cs.berkeley.edu;richb@rice.edu,5;5;3,3;3;4,Reject,0,8,0.0,yes,9/27/18,University of California Berkeley;Rice University;Baylor College of Medicine;California Institute of Technology;University of California Berkeley;Rice University,5;85;-1;140;5;85,18;86;-1;3;18;86,5;8
2331,2331,2331,2331,2331,2331,2331,2331,ICLR,2019,Modular Deep Probabilistic Programming,Zhenwen Dai;Eric Meissner;Neil D. Lawrence,zhenwend@amazon.com;erimeiss@amazon.com;lawrennd@amazon.com,5;3;4,3;3;4,Reject,1,4,0.0,yes,9/27/18,Amazon;Amazon;Amazon,-1;-1;-1,-1;-1;-1,11
2332,2332,2332,2332,2332,2332,2332,2332,ICLR,2019,Asynchronous SGD without gradient delay for efficient distributed training,Roman Talyansky;Pavel Kisilev;Zach Melamed;Natan Peterfreund;Uri Verner,roma.talyansky@gmail.com;pavel.kisilev@huawei.com;zach.melamed@huawei.com;natan.peterfreund@gmail.com;uri.verner@gmail.com,5;4;4,4;4;5,Reject,0,0,0.0,yes,9/27/18,Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.;Amazon;,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,9
2333,2333,2333,2333,2333,2333,2333,2333,ICLR,2019,EnGAN: Latent Space MCMC and Maximum Entropy Generators for Energy-based Models,Rithesh Kumar;Anirudh Goyal;Aaron Courville;Yoshua Bengio,ritheshkumar.95@gmail.com;anirudhgoyal9119@gmail.com;aaron.courville@gmail.com;yoshua.umontreal@gmail.com,6;5;5,4;5;4,Reject,4,13,0.0,yes,9/27/18,University of Montreal;University of Montreal;University of Montreal;University of Montreal,123;123;123;123,108;108;108;108,5;4
2334,2334,2334,2334,2334,2334,2334,2334,ICLR,2019,Variational recurrent models for representation learning,Qingming Tang;Mingda Chen;Weiran Wang;Karen Livescu,qmtang@ttic.edu;mchen@ttic.edu;weiranw@amazon.com;klivescu@ttic.edu,5;5;3,3;3;5,Reject,0,3,2.0,yes,9/27/18,Toyota Technological Institute at Chicago;Toyota Technological Institute at Chicago;Amazon;Toyota Technological Institute at Chicago,123;123;-1;123,1103;1103;-1;1103,5
2335,2335,2335,2335,2335,2335,2335,2335,ICLR,2019,HR-TD: A Regularized TD Method to Avoid Over-Generalization,Ishan Durugkar;Bo Liu;Peter Stone,ishand@cs.utexas.edu;liubo19831214@gmail.com;pstone@cs.utexas.edu,4;3;2,4;4;5,Reject,0,0,0.0,yes,9/27/18,"University of Texas, Austin;University of California, San Diego;University of Texas, Austin",22;11;22,49;31;49,8
2336,2336,2336,2336,2336,2336,2336,2336,ICLR,2019,Discovering General-Purpose Active Learning Strategies,Ksenia Konyushkova;Raphael Sznitman;Pascal Fua,ksenia.konyushkova@epfl.ch;raphael.sznitman@artorg.unibe.ch;pascal.fua@epfl.ch,5;4;4;4,4;5;5;4,Reject,0,4,0.0,yes,9/27/18,Swiss Federal Institute of Technology Lausanne;University of Bern;Swiss Federal Institute of Technology Lausanne,478;386;478,38;105;38,
2337,2337,2337,2337,2337,2337,2337,2337,ICLR,2019,Multi-Objective Value Iteration with Parameterized Threshold-Based Safety Constraints,Hussein Sibai;Sayan Mitra,sibai2@illinois.edu;mitras@illinois.edu,5;5;3,4;2;4,Reject,0,5,0.0,yes,9/27/18,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",3;3,37;37,
2338,2338,2338,2338,2338,2338,2338,2338,ICLR,2019,Optimizing for Generalization in Machine Learning with Cross-Validation Gradients,Barratt;Shane;Sharma;Rishi,sbarratt@stanford.edu;rsh@stanford.edu,5;2;4,5;4;4,Reject,1,0,0.0,yes,9/27/18,Stanford University;Stanford University,4;4,3;3,8
2339,2339,2339,2339,2339,2339,2339,2339,ICLR,2019,FAVAE: SEQUENCE DISENTANGLEMENT USING IN- FORMATION BOTTLENECK PRINCIPLE,Masanori Yamada;Kim Heecheol;Kosuke Miyoshi;Hiroshi Yamakawa,yamada0224@gmail.com;h-kim@isi.imi.i.u-tokyo.ac.jp;miyoshi@narr.jp;hiroshi_yamakawa@dwango.co.jp,5;6;4,4;5;4,Reject,0,7,0.0,yes,9/27/18,NTT;The University of Tokyo;;,-1;54;-1;-1,-1;45;-1;-1,5
2340,2340,2340,2340,2340,2340,2340,2340,ICLR,2019,Learning Partially Observed PDE Dynamics with Neural Networks,Ibrahim Ayed;Emmanuel De Bézenac;Arthur Pajot;Patrick Gallinari,ayedibrahim@gmail.com;emmanuel.de-bezenac@lip6.fr;arthur.pajot@lip6.fr;patrick.gallinari@lip6.fr,6;5;5,3;5;3,Reject,0,6,0.0,yes,9/27/18,LIP6;LIP6;LIP6;LIP6,-1;-1;-1;-1,-1;-1;-1;-1,
2341,2341,2341,2341,2341,2341,2341,2341,ICLR,2019,Weak contraction mapping and optimization,Siwei Luo,siuluosiwei@gmail.com,3;1;4,2;5;5,Reject,0,4,0.0,yes,9/27/18,"University of Illinois, Chicago",57,255,9
2342,2342,2342,2342,2342,2342,2342,2342,ICLR,2019,Riemannian Stochastic Gradient Descent for Tensor-Train Recurrent Neural Networks,Jun Qi;Chin-Hui Lee;Javier Tejedor,jqi41@gatech.edu;qij13@uw.edu;javiertejedornoguerales@gmail.com,4;4;3,4;4;3,Reject,0,0,0.0,yes,9/27/18,"Georgia Institute of Technology;University of Washington, Seattle;",13;6;-1,33;25;-1,3
2343,2343,2343,2343,2343,2343,2343,2343,ICLR,2019,Unseen Action Recognition with Unpaired Adversarial Multimodal Learning,AJ Piergiovanni;Michael S. Ryoo,ajpiergi@indiana.edu;mryoo@indiana.edu,7;5;4,4;4;5,Reject,0,3,0.0,yes,9/27/18,University of Arizona;University of Arizona,169;169,161;161,4;6
2344,2344,2344,2344,2344,2344,2344,2344,ICLR,2019,Unlabeled Disentangling of GANs with Guided Siamese Networks,Gökhan Yildirim;Nikolay Jetchev;Urs Bergmann,gokhan.yildirim@zalando.de;nikolay.jetchev@zalando.de;urs.bergmann@zalando.de,5;6;6;5,4;4;3;4,Reject,0,11,1.0,yes,9/27/18,Zalando Research;Zalando Research;Zalando Research,-1;-1;-1,-1;-1;-1,5
2345,2345,2345,2345,2345,2345,2345,2345,ICLR,2019,Local Image-to-Image Translation via Pixel-wise Highway Adaptive Instance Normalization,Wonwoong Cho;Seunghwan Choi;Junwoo Park;David Keetae Park;Tao Qin;Jaegul Choo,tyflehd21@korea.ac.kr;shadow2496@korea.ac.kr;skp.1003874@sk.com;heykeetae@gmail.com;taotsin@msn.com;jchoo@korea.ac.kr,5;6;5,4;4;5,Reject,0,6,2.0,yes,9/27/18,Korea University;Korea University;Sk;;;Korea University,314;314;314;-1;-1;314,244;244;38;-1;-1;244,2
2346,2346,2346,2346,2346,2346,2346,2346,ICLR,2019,Empirically Characterizing Overparameterization Impact on Convergence,Newsha Ardalani;Joel Hestness;Gregory Diamos,newsha@baidu.com;joel@baidu.com;gregdiamos@baidu.com,5;4;3,3;5;4,Reject,0,0,0.0,yes,9/27/18,Baidu;Baidu;Baidu,-1;-1;-1,-1;-1;-1,
2347,2347,2347,2347,2347,2347,2347,2347,ICLR,2019,A Biologically Inspired Visual Working Memory for Deep Networks,Ethan Harris;Mahesan Niranjan;Jonathon Hare,ewah1g13@ecs.soton.ac.uk;mn@ecs.soton.ac.uk;jsh2@ecs.soton.ac.uk,9;4;5,5;4;4,Reject,0,5,0.0,yes,9/27/18,University of Southampton;University of Southampton;University of Southampton,169;169;169,126;126;126,
2348,2348,2348,2348,2348,2348,2348,2348,ICLR,2019,Using GANs for Generation of Realistic City-Scale Ride Sharing/Hailing Data Sets,Abhinav Jauhri;Brad Stocks;Jian Hui Li;Koichi Yamada;John Paul Shen,ajauhri@cmu.edu;brad.stocks@sv.cmu.edu;jian.hui.li@intel.com;koichi.yamada@intel.com;jpshen@cmu.edu,4;5;5,4;4;4,Reject,0,4,0.0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;Intel;Intel;Carnegie Mellon University,1;1;-1;-1;1,24;24;-1;-1;24,5;4
2349,2349,2349,2349,2349,2349,2349,2349,ICLR,2019,Pushing the bounds of dropout,Gábor Melis;Charles Blundell;Tomáš Kočiský;Karl Moritz Hermann;Chris Dyer;Phil Blunsom,melisgl@google.com;cblundell@google.com;tkocisky@google.com;kmh@google.com;cdyer@google.com;pblunsom@google.com,5;5;4,3;2;3,Reject,0,5,0.0,yes,9/25/19,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,3;1
2350,2350,2350,2350,2350,2350,2350,2350,ICLR,2019,Traditional and Heavy Tailed Self Regularization in Neural Network Models,Charles H. Martin;Michael W. Mahoney,charles@calculationconsulting.com;mmahoney@stat.berkeley.edu,4;4;6,4;1;5,Reject,0,11,0.0,yes,9/27/18,Calculationconsulting;University of California Berkeley,-1;5,-1;18,8
2351,2351,2351,2351,2351,2351,2351,2351,ICLR,2019,ISA-VAE: Independent Subspace Analysis with Variational Autoencoders,Jan Stühmer;Richard Turner;Sebastian Nowozin,t-jastuh@microsoft.com;ret26@cam.ac.uk;senowozi@microsoft.com,4;7;4,3;4;5,Reject,0,3,0.0,yes,9/27/18,Microsoft;University of Cambridge;Microsoft,-1;71;-1,-1;2;-1,5
2352,2352,2352,2352,2352,2352,2352,2352,ICLR,2019,Guaranteed Recovery of One-Hidden-Layer Neural Networks via Cross Entropy,Haoyu Fu;Yuejie Chi;Yingbin Liang,fu.436@osu.edu;yuejiechi@cmu.edu;liang.889@osu.edu,3;4;5,4;4;4,Reject,0,0,0.0,yes,9/27/18,Ohio State University;Carnegie Mellon University;Ohio State University,76;1;76,318;24;318,1;9
2353,2353,2353,2353,2353,2353,2353,2353,ICLR,2019,Metropolis-Hastings view on variational inference and adversarial training,Kirill Neklyudov;Dmitry Vetrov,k.necludov@gmail.com;vetrodim@gmail.com,6;9;5,4;4;3,Reject,0,10,0.0,yes,9/27/18,Higher School of Economics;Higher School of Economics,478;478,377;377,11;5;4;1
2354,2354,2354,2354,2354,2354,2354,2354,ICLR,2019,Open-Ended Content-Style Recombination Via Leakage Filtering,Karl Ridgeway;Michael C. Mozer,karl.ridgeway@colorado.edu;mozer@colorado.edu,5;7;5,4;4;3,Reject,0,3,0.0,yes,9/27/18,"University of Colorado, Boulder;University of Colorado, Boulder",44;44,100;100,5;6
2355,2355,2355,2355,2355,2355,2355,2355,ICLR,2019,VHEGAN: Variational Hetero-Encoder Randomized GAN for Zero-Shot Learning,Hao Zhang;Bo Chen;Long Tian;Zhengjue Wang;Mingyuan Zhou,zhanghao_xidian@163.com;bchen@mail.xidian.edu.cn;zhengjuewang@163.com;tianlong_xidian@163.com;mingyuan.zhou@mccombs.utexas.edu,5;5;5,4;5;5,Reject,1,4,0.0,yes,9/27/18,"Xidian University;Tsinghua University;Xidian University;163;University of Texas, Austin",478;8;478;-1;22,917;30;917;-1;49,5;4;6
2356,2356,2356,2356,2356,2356,2356,2356,ICLR,2019,$A^*$ sampling with probability matching,Yichi Zhou;Jun Zhu,vofhqn@gmail.com;dcszj@mail.tsinghua.edu.cn,5;6;3,5;2;5,Reject,0,0,0.0,yes,9/27/18,Tsinghua University;Tsinghua University,8;8,30;30,1
2357,2357,2357,2357,2357,2357,2357,2357,ICLR,2019,Maximum a Posteriori on a Submanifold: a General Image Restoration Method with GAN,Fangzhou Luo;Xiaolin Wu,fluo1993@gmail.com;xwu510@gmail.com,4;4;6,5;5;4,Reject,0,4,0.0,yes,9/27/18,;,-1;-1,-1;-1,5;4
2358,2358,2358,2358,2358,2358,2358,2358,ICLR,2019,Ada-Boundary: Accelerating the DNN Training via Adaptive Boundary Batch Selection,Hwanjun Song;Sundong Kim;Minseok Kim;Jae-Gil Lee,songhwanjun@kaist.ac.kr;sundong.kim@kaist.ac.kr;minseokkim@kaist.ac.kr;jaegil@kaist.ac.kr,5;5;5,4;3;4,Reject,0,8,0.0,yes,9/27/18,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,20;20;20;20,95;95;95;95,
2359,2359,2359,2359,2359,2359,2359,2359,ICLR,2019,A Resizable Mini-batch Gradient Descent based on a Multi-Armed Bandit,Seong Jin Cho;Sunghun Kang;Chang D. Yoo,ipcng00@kaist.ac.kr;sunghun.kang@kaist.ac.kr;cd_yoo@kaist.ac.kr,6;7;4,3;4;4,Reject,0,4,1.0,yes,9/27/18,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,20;20;20,95;95;95,
2360,2360,2360,2360,2360,2360,2360,2360,ICLR,2019,Neural Network Cost Landscapes as Quantum States,Abdulah Fawaz;Sebastien Piat;Paul Klein;Peter Mountney;Simone Severini,abdulah.fawaz.14@ucl.ac.uk;sebastien.piat@siemens-healthineers.com;klein.paul@siemens-healthineers.com;peter.mountney@siemens-healthineers.com;s.severini@ucl.ac.uk,5;3;4,5;4;4,Reject,0,4,0.0,yes,9/27/18,University College London;Siemens Healthineers;Siemens Healthineers;Siemens Healthineers;University College London,50;-1;-1;-1;50,16;-1;-1;-1;16,
2361,2361,2361,2361,2361,2361,2361,2361,ICLR,2019,Over-parameterization Improves Generalization in the XOR Detection Problem,Alon Brutzkus;Amir Globerson,brutzkus@gmail.com;amir.globerson@gmail.com,5;4;5,4;4;4,Reject,0,5,0.0,yes,9/27/18,Tel Aviv University;Tel Aviv University,37;37,217;217,1;8
2362,2362,2362,2362,2362,2362,2362,2362,ICLR,2019,On the effect of the activation function on the distribution of hidden nodes in a deep network,Philip M. Long;Hanie Sedghi,plong@google.com;hsedghi@google.com,4;4;5,3;3;3,Reject,0,3,0.0,yes,9/27/18,Google;Google,-1;-1,-1;-1,1
2363,2363,2363,2363,2363,2363,2363,2363,ICLR,2019,INFORMATION MAXIMIZATION AUTO-ENCODING,Dejiao Zhang;Tianchen Zhao;Laura Balzano,dejiao@umich.edu;ericolon@umich.edu;girasole@umich.edu,5;6;4,5;4;4,Reject,0,17,0.0,yes,9/27/18,University of Michigan;University of Michigan;University of Michigan,8;8;8,21;21;21,5
2364,2364,2364,2364,2364,2364,2364,2364,ICLR,2019,Infinitely Deep Infinite-Width Networks,Jovana Mitrovic;Peter Wirnsberger;Charles Blundell;Dino Sejdinovic;Yee Whye Teh,jovana.mitrovic@spc.ox.ac.uk;pewi@google.com;cblundell@google.com;dino.sejdinovic@stats.ox.ac.uk;ywteh@google.com,6;5;6,4;4;2,Reject,0,6,0.0,yes,9/27/18,University of Oxford;Google;Google;University of Oxford;Google,50;-1;-1;50;-1,1;-1;-1;1;-1,
2365,2365,2365,2365,2365,2365,2365,2365,ICLR,2019,Training Variational Auto Encoders with Discrete Latent Representations using Importance Sampling,Alexander Bartler;Felix Wiewel;Bin Yang;Lukas Mauch,alexander.bartler@iss.uni-stuttgart.de;felix.wiewel@iss.uni-stuttgart.de;bin.yang@iss.uni-stuttgart.de;lukas.mauch@iss.uni-stuttgart.de,3;1;3,5;5;5,Reject,0,0,0.0,yes,9/27/18,University of Stuttgart;University of Stuttgart;University of Stuttgart;University of Stuttgart,95;95;95;95,219;219;219;219,5;1
2366,2366,2366,2366,2366,2366,2366,2366,ICLR,2019,An Exhaustive Analysis of Lazy vs. Eager Learning Methods for Real-Estate Property Investment,Setareh Rafatirad;Maryam Heidari,srafatir@gmu.edu;mheidari@gmu.edu,3;4;2,5;4;4,Reject,0,0,0.0,yes,9/27/18,George Mason University;George Mason University,99;99,336;336,
2367,2367,2367,2367,2367,2367,2367,2367,ICLR,2019,Graph2Seq: Graph to Sequence Learning with Attention-Based Neural Networks,Kun Xu;Lingfei Wu;Zhiguo Wang;Yansong Feng;Michael Witbrock;Vadim Sheinin,xukun@pku.edu.cn;lwu@email.wm.edu;zhigwang@us.ibm.com;fengyansong@pku.edu.cn;witbrock@us.ibm.com;vadims@us.ibm.com,4;6;6,5;4;4,Reject,0,17,0.0,yes,9/27/18,Peking University;College of William and Mary;International Business Machines;Peking University;International Business Machines;International Business Machines,24;169;-1;24;-1;-1,27;261;-1;27;-1;-1,3;10
2368,2368,2368,2368,2368,2368,2368,2368,ICLR,2019,Scaling shared model governance via model splitting,Miljan Martic;Jan Leike;Andrew Trask;Matteo Hessel;Shane Legg;Pushmeet Kohli,miljanm@google.com;leike@google.com;atrask@google.com;mtthss@google.com;legg@google.com;pushmeet@google.com,5;4;9,4;3;4,Reject,0,10,0.0,yes,9/27/18,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,
2369,2369,2369,2369,2369,2369,2369,2369,ICLR,2019,An Alarm System for Segmentation Algorithm Based on Shape Model,Fengze Liu;Yingda Xia;Dong Yang;Alan Yuille;Daguang Xu,liufz13@gmail.com;yxia25@jhu.edu;don.yang.mech@gmail.com;alan.l.yuille@gmail.com;cathy.xudg@gmail.com,3;6;7;5,5;4;3;4,Reject,0,4,0.0,yes,9/27/18,Johns Hopkins University;Johns Hopkins University;NVIDIA;Johns Hopkins University;,72;72;-1;72;-1,13;13;-1;13;-1,5;2
2370,2370,2370,2370,2370,2370,2370,2370,ICLR,2019,Interpretable Continual Learning,Tameem Adel;Cuong V. Nguyen;Richard E. Turner;Zoubin Ghahramani;Adrian Weller,tah47@cam.ac.uk;nvcuong92@gmail.com;ret26@cam.ac.uk;zoubin@eng.cam.ac.uk;aw665@cam.ac.uk,4;5;6,4;3;3,Reject,0,2,1.0,yes,9/27/18,University of Cambridge;Amazon;University of Cambridge;University of Cambridge;University of Cambridge,71;-1;71;71;71,2;-1;2;2;2,
2371,2371,2371,2371,2371,2371,2371,2371,ICLR,2019,On the Spectral Bias of Neural Networks,Nasim Rahaman;Aristide Baratin;Devansh Arpit;Felix Draxler;Min Lin;Fred Hamprecht;Yoshua Bengio;Aaron Courville,nasim.rahaman@iwr.uni-heidelberg.de;aristidebaratin@hotmail.com;devansharpit@gmail.com;felix.draxler@iwr.uni-heidelberg.de;mavenlin@gmail.com;yoshua.umontreal@gmail.com;aaron.courville@umontreal.ca,6;4;6;5,3;4;3;3,Reject,0,12,1.0,yes,9/27/18,Heidelberg University;University of Montreal;University of Montreal;Heidelberg University;University of Montreal;University of Montreal;University of Montreal,199;123;123;199;123;123;123,45;108;108;45;108;108;108,
2372,2372,2372,2372,2372,2372,2372,2372,ICLR,2019,REVISTING NEGATIVE TRANSFER USING ADVERSARIAL LEARNING,Saneem Ahmed Chemmengath;Samarth Bharadwaj;Suranjana Samanta;Karthik Sankaranarayanan,saneem.cg@in.ibm.com;samarth.b@in.ibm.com;suransam@in.ibm.com;kartsank@in.ibm.com,4;2;6,4;4;4,Reject,0,1,0.0,yes,9/27/18,International Business Machines;International Business Machines;International Business Machines;International Business Machines,-1;-1;-1;-1,-1;-1;-1;-1,4
2373,2373,2373,2373,2373,2373,2373,2373,ICLR,2019,PIE: Pseudo-Invertible Encoder,Jan Jetze Beitler;Ivan Sosnovik;Arnold Smeulders,j.j.beitler@uva.nl;i.sosnovik@uva.nl;a.w.m.smeulders@uva.nl,3;5;5,4;5;4,Reject,2,3,12.0,yes,9/27/18,University of Amsterdam;University of Amsterdam;University of Amsterdam,169;169;169,59;59;59,5
2374,2374,2374,2374,2374,2374,2374,2374,ICLR,2019,Variational Domain Adaptation,Hirono Okamoto;Shohei Ohsawa;Itto Higuchi;Haruka Murakami;Mizuki Sango;Zhenghang Cui;Masahiro Suzuki;Hiroshi Kajino;Yutaka Matsuo,ohsawa@weblab.t.u-tokyo.ac.jp,4;4;5,3;5;3,Reject,0,6,1.0,yes,9/27/18,The University of Tokyo,54,45,5;11
2375,2375,2375,2375,2375,2375,2375,2375,ICLR,2019,Siamese Capsule Networks,James O' Neill,james.o-neill@liverpool.ac.uk,5;6;3,4;4;4,Reject,0,2,0.0,yes,9/27/18,University of Liverpool,123,177,6;2
2376,2376,2376,2376,2376,2376,2376,2376,ICLR,2019,Unsupervised Multi-Target Domain Adaptation: An Information Theoretic Approach,Behnam Gholami;Pritish Sahu;Ognjen (Oggi) Rudovic;Konstantinos Bousmalis;Vladimir Pavlovic,bb510@cs.rutgers.edu;ps851@cs.rutgers.edu;orudovic@mit.edu;konstantinos@google.com;vladimir@cs.rutgers.edu,4;6;5,4;5;4,Reject,0,4,0.0,yes,9/27/18,Rutgers University;Rutgers University;Massachusetts Institute of Technology;Google;Rutgers University,34;34;2;-1;34,172;172;5;-1;172,
2377,2377,2377,2377,2377,2377,2377,2377,ICLR,2019,Flow++: Improving Flow-Based Generative Models  with  Variational Dequantization and Architecture Design  ,Jonathan Ho;Xi Chen;Aravind Srinivas;Yan Duan;Pieter Abbeel,jonathanho@berkeley.edu;peter@covariant.ai;aravind_srinivas@berkeley.edu;dementrock@gmail.com;pabbeel@cs.berkeley.edu,6;6;5,4;3;5,Reject,0,4,0.0,yes,9/27/18,University of California Berkeley;covariant.ai;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;-1;5;5;5,18;-1;18;18;18,5
2378,2378,2378,2378,2378,2378,2378,2378,ICLR,2019,End-to-end learning of pharmacological assays from high-resolution microscopy images,Markus Hofmarcher;Elisabeth Rumetshofer;Sepp Hochreiter;Günter Klambauer,hofmarcher@ml.jku.at;rumetshofer@ml.jku.at;hochreit@ml.jku.at;klambauer@ml.jku.at,3;6;5,3;5;4,Reject,0,4,0.0,yes,9/27/18,Johannes Kepler University Linz;Johannes Kepler University Linz;Johannes Kepler University Linz;Johannes Kepler University Linz,228;228;228;228,538;538;538;538,2
2379,2379,2379,2379,2379,2379,2379,2379,ICLR,2019,Meta-Learning with Domain Adaptation for Few-Shot Learning under Domain Shift,Doyen Sahoo;Hung Le;Chenghao Liu;Steven C. H. Hoi,doyens@smu.edu.sg;hungle.2018@phdis.smu.edu.sg;chliu@smu.edu.sg;chhoi@smu.edu.sg,5;6;6,3;3;3,Reject,2,5,0.0,yes,9/27/18,Singapore Management University;Singapore Management University;Singapore Management University;Singapore Management University,89;89;89;89,1103;1103;1103;1103,4;6
2380,2380,2380,2380,2380,2380,2380,2380,ICLR,2019,On-Policy Trust Region Policy Optimisation with Replay Buffers,Dmitry Kangin;Nicolas Pugeault,d.kangin@exeter.ac.uk;n.pugeault@exeter.ac.uk,7;6;5,5;3;4,Reject,0,4,0.0,yes,9/27/18,University of Exeter;University of Exeter,386;386,130;130,
2381,2381,2381,2381,2381,2381,2381,2381,ICLR,2019,Stochastic Gradient Descent Learns State Equations with Nonlinear Activations,Samet Oymak,sametoymak@gmail.com,7;7;5,3;3;5,Reject,0,3,0.0,yes,9/27/18,"University of California, Riverside",57,197,1
2382,2382,2382,2382,2382,2382,2382,2382,ICLR,2019,Identifying Generalization Properties in Neural Networks,Huan Wang;Nitish Shirish Keskar;Caiming Xiong;Richard Socher,huan.wang@salesforce.com;nkeskar@salesforce.com;cxiong@salesforce.com;rsocher@salesforce.com,5;6;6,4;4;4,Reject,0,5,0.0,yes,9/27/18,SalesForce.com;SalesForce.com;SalesForce.com;SalesForce.com,-1;-1;-1;-1,-1;-1;-1;-1,1;8
2383,2383,2383,2383,2383,2383,2383,2383,ICLR,2019,Stackelberg GAN: Towards Provable Minimax Equilibrium via Multi-Generator Architectures,Hongyang Zhang;Susu Xu;Jiantao Jiao;Pengtao Xie;Ruslan Salakhutdinov;Eric P. Xing,hongyanz@cs.cmu.edu;susux@andrew.cmu.edu;jiantao@eecs.berkeley.edu;pengtao.xie@petuum.com;rsalakhu@cs.cmu.edu;epxing@cs.cmu.edu,4;5;7,4;3;3,Reject,0,10,0.0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;University of California Berkeley;Petuum Inc.;Carnegie Mellon University;Carnegie Mellon University,1;1;5;-1;1;1,24;24;18;-1;24;24,5
2384,2384,2384,2384,2384,2384,2384,2384,ICLR,2019,Stability of Stochastic Gradient Method with Momentum for Strongly Convex Loss Functions,Ali Ramezani-Kebrya;Ashish Khisti;and Ben Liang,aramezani@ece.utoronto.ca;akhisti@ece.utoronto.ca;liang@ece.utoronto.ca,4;6;4,5;4;4,Reject,0,16,1.0,yes,9/27/18,Toronto University;Toronto University;Toronto University,18;18;18,22;22;22,1;8
2385,2385,2385,2385,2385,2385,2385,2385,ICLR,2019,A Variational Dirichlet Framework for Out-of-Distribution Detection,Wenhu Chen;Yilin Shen;William Wang;Hongxia Jin,wenhuchen@ucsb.edu;yilin.shen@samsung.com;william@cs.ucsb.edu;hongxia.jin@samsung.com,6;5;6,4;5;3,Reject,4,7,0.0,yes,9/27/18,UC Santa Barbara;Samsung;UC Santa Barbara;Samsung,37;-1;37;-1,53;-1;53;-1,
2386,2386,2386,2386,2386,2386,2386,2386,ICLR,2019,Optimized Gated Deep Learning Architectures for Sensor Fusion,Myung Seok Shim;Peng Li,mrshim1101@tamu.edu;pli@tamu.edu,4;4;3,5;4;4,Reject,0,0,0.0,yes,9/27/18,Texas A&M;Texas A&M,44;44,160;160,
2387,2387,2387,2387,2387,2387,2387,2387,ICLR,2019,RESIDUAL NETWORKS CLASSIFY INPUTS BASED ON THEIR NEURAL TRANSIENT DYNAMICS,Fereshteh Lagzi,lagzi@informatik.uni-freiburg.de,4;2;5,4;4;5,Reject,0,4,0.0,yes,9/27/18,Universität Freiburg,123,82,
2388,2388,2388,2388,2388,2388,2388,2388,ICLR,2019,Better Generalization with On-the-fly Dataset Denoising,Jiaming Song;Tengyu Ma;Michael Auli;Yann Dauphin,jiaming.tsong@gmail.com;tengyuma@cs.stanford.edu;michael.auli@gmail.com;yann@dauphin.io,5;6;6,4;3;5,Reject,0,9,0.0,yes,9/27/18,Stanford University;Stanford University;Facebook;Facebook,4;4;-1;-1,3;3;-1;-1,8
2389,2389,2389,2389,2389,2389,2389,2389,ICLR,2019,Generative Adversarial Networks for Extreme Learned Image Compression,Eirikur Agustsson;Michael Tschannen;Fabian Mentzer;Radu Timofte;Luc van Gool,aeirikur@vision.ee.ethz.ch;michaelt@nari.ee.ethz.ch;mentzerf@vision.ee.ethz.ch;radu.timofte@vision.ee.ethz.ch;vangool@vision.ee.ethz.ch,6;6;4,3;3;4,Reject,0,5,0.0,yes,9/27/18,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10;10;10,10;10;10;10;10,5;4
2390,2390,2390,2390,2390,2390,2390,2390,ICLR,2019,A Synaptic Neural Network and Synapse Learning,Chang Li,changli@neatware.com,2;3;2;2,4;3;3;3,Reject,0,17,0.0,yes,9/27/18,Neatware,-1,-1,1;10
2391,2391,2391,2391,2391,2391,2391,2391,ICLR,2019,Reducing Overconfident Errors outside the Known Distribution,Zhizhong Li;Derek Hoiem,zli115@illinois.edu;dhoiem@illinois.edu,6;4;6,4;4;3,Reject,0,8,0.0,yes,9/27/18,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",3;3,37;37,7
2392,2392,2392,2392,2392,2392,2392,2392,ICLR,2019,Adversarial Sampling for Active Learning,Christoph Mayer;Radu Timofte,chmayer@vision.ee.ethz.ch;radu.timofte@vision.ee.ethz.ch,6;5;5,2;4;5,Reject,0,4,0.0,yes,9/27/18,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10,10;10,4
2393,2393,2393,2393,2393,2393,2393,2393,ICLR,2019,"Classification from Positive, Unlabeled and Biased Negative Data",Yu-Guan Hsieh;Gang Niu;Masashi Sugiyama,yu-guan.hsieh@ens.fr;gang.niu@riken.jp;sugi@k.u-tokyo.ac.jp,5;6;5,5;3;3,Reject,0,8,0.0,yes,9/27/18,Ecole Normale Superieure;RIKEN;The University of Tokyo,99;-1;54,603;-1;45,1
2394,2394,2394,2394,2394,2394,2394,2394,ICLR,2019,Learning Diverse Generations using Determinantal Point Processes,Mohamed Elfeki;Camille Couprie;Mohamed Elhoseiny,m.elfeki11@gmail.com;coupriec@fb.com;elhoseiny@fb.com,5;5;5,4;5;4,Reject,0,7,1.0,yes,9/27/18,University of Central Florida;Facebook;Facebook,78;-1;-1,1103;-1;-1,5;4
2395,2395,2395,2395,2395,2395,2395,2395,ICLR,2019,Probabilistic Binary Neural Networks,Jorn W.T. Peters;Tim Genewein;Max Welling,jornpeters@gmail.com;tim.genewein@gmail.com;welling.max@gmail.com,6;5;3,4;2;3,Reject,0,6,0.0,yes,9/27/18,University of Amsterdam;Google;University of California - Irvine,169;-1;35,59;-1;99,
2396,2396,2396,2396,2396,2396,2396,2396,ICLR,2019,Geomstats: a Python Package for Riemannian Geometry in Machine Learning,Nina Miolane;Johan Mathe;Claire Donnat;Mikael Jorda;Xavier Pennec,nmiolane@stanford.edu;johan@froglabs.ai;cdonnat@stanford.edu;mjorda@stanford.edu;xavier.pennec@inria.fr,4;4;3;8,4;5;5;2,Reject,1,1,0.0,yes,9/27/18,Stanford University;;Stanford University;Stanford University;INRIA,4;-1;4;4;-1,3;-1;3;3;-1,
2397,2397,2397,2397,2397,2397,2397,2397,ICLR,2019,Outlier Detection from Image Data,Lei Cao;Yizhou Yan;Samuel Madden;Elke Rundensteiner,lcao@csail.mit.edu;yyan2@wpi.edu;madden@csail.mit.edu;rundenst@cs.wpi.edu,4;5;5,4;3;4,Reject,1,11,0.0,yes,9/27/18,Massachusetts Institute of Technology;Worcester Polytechnic Institute;Massachusetts Institute of Technology;Worcester Polytechnic Institute,2;169;2;169,5;1103;5;1103,8
2398,2398,2398,2398,2398,2398,2398,2398,ICLR,2019,Investigating CNNs' Learning Representation under label noise,Ryuichiro Hataya;Hideki Nakayama,hataya@nlab.ci.i.u-tokyo.ac.jp;nakayama@nlab.ci.i.u-tokyo.aco.jp,5;4;5,4;5;5,Reject,0,3,0.0,yes,9/27/18,The University of Tokyo;,54;-1,45;-1,
2399,2399,2399,2399,2399,2399,2399,2399,ICLR,2019,Discriminative out-of-distribution detection for semantic segmentation,Petra Bevandić;Siniša Šegvić;Ivan Krešo;Marin Oršić,petra.bevandic@fer.hr;sinisa.segvic@fer.hr;ivan.kreso@fer.hr;marin.orsic@fer.hr,4;7;3,4;5;3,Reject,0,5,0.0,yes,9/27/18,"Faculty of Electrical Engineering and Computing, University of Zagreb;Faculty of Electrical Engineering and Computing, University of Zagreb;Faculty of Electrical Engineering and Computing, University of Zagreb;Faculty of Electrical Engineering and Computing, University of Zagreb",478;478;478;478,894;894;894;894,1;2
2400,2400,2400,2400,2400,2400,2400,2400,ICLR,2019,DppNet: Approximating Determinantal Point Processes with Deep Networks,Zelda Mariet;Jasper Snoek;Yaniv Ovadia,zelda@csail.mit.edu;jsnoek@google.com;yovadia@google.com,3;5;5,5;4;3,Reject,0,5,0.0,yes,9/27/18,Massachusetts Institute of Technology;Google;Google,2;-1;-1,5;-1;-1,5
2401,2401,2401,2401,2401,2401,2401,2401,ICLR,2019,Difference-Seeking Generative Adversarial Network,Yi-Lin Sung;Sung-Hsien Hsieh;Soo-Chang Pei;Chun-Shien Lu,r06942076@ntu.edu.tw;parvaty316@hotmail.com;peisc@ntu.edu.tw;lcs@iis.sinica.edu.tw,5;4;3,4;3;4,Reject,0,4,0.0,yes,9/27/18,National Taiwan University;;National Taiwan University;Academia Sinica,85;-1;85;-1,197;-1;197;-1,5;4
2402,2402,2402,2402,2402,2402,2402,2402,ICLR,2019,Learning to Search Efficient DenseNet with Layer-wise Pruning,Xuanyang Zhang;Hao liu;Zhanxing Zhu;Zenglin Xu,xuanyang91.zhang@gmail.com;uestcliuhao@gmail.com;zhanxing.zhu@pku.edu.cn;zenglin@gmail.com,4;4;4,5;4;4,Reject,0,0,0.0,yes,9/27/18,University of Electronic Science and Technology of China;University of California Berkeley;Peking University;University of Electronic Science and Technology of China,169;5;24;169,843;18;27;843,
2403,2403,2403,2403,2403,2403,2403,2403,ICLR,2019,Unsupervised  one-to-many image translation,Samuel Lavoie-Marchildon;Sebastien Lachapelle;Mikołaj Bińkowski;Aaron Courville;Yoshua Bengio;R Devon Hjelm,samuel.lavoie-marchildon@umontreal.ca;sebastien.lachapelle@umontreal.ca;mikbinkowski@gmail.com;aaron.courville@gmail.com;yoshua.umontreal@gmail.com;devon.hjelm@microsoft.com,3;4;4,4;4;4,Reject,0,5,0.0,yes,9/27/18,University of Montreal;University of Montreal;Imperial College London;University of Montreal;University of Montreal;Microsoft,123;123;72;123;123;-1,108;108;8;108;108;-1,
2404,2404,2404,2404,2404,2404,2404,2404,ICLR,2019,Calibration of neural network logit vectors to combat adversarial attacks,Oliver Goldstein,og14775@my.bristol.ac.uk,3;2;4,4;4;5,Reject,2,3,0.0,yes,9/27/18,University of Bristol,123,76,4
2405,2405,2405,2405,2405,2405,2405,2405,ICLR,2019,The Expressive Power of Gated Recurrent Units as a Continuous Dynamical System,Ian D. Jordan;Piotr Aleksander Sokol;Il Memming Park,ian.jordan@stonybrook.edu;piotr.sokol@stonybrook.edu;memming.park@stonybrook.edu,6;5;5,4;4;4,Reject,0,6,0.0,yes,9/27/18,"State University of New York, Stony Brook;State University of New York, Stony Brook;State University of New York, Stony Brook",41;41;41,258;258;258,
2406,2406,2406,2406,2406,2406,2406,2406,ICLR,2019,A Self-Supervised Method for Mapping Human Instructions to Robot Policies,Hsin-Wei Yu;Po-Yu Wu;Chih-An Tsao;You-An Shen;Shih-Hsuan Lin;Zhang-Wei Hong;Yi-Hsiang Chang;Chun-Yi Lee,hsinweiyo@gmail.com;bwoyu85928@gmai.com;hl6540@gmail.com;jerrylin1121@gmail.com;williamd4112@gapp.nthu.edu.tw;shawn420@gapp.nthu.edu.tw;cylee@gapp.nthu.edu.tw,4;3;2,5;5;4,Reject,0,0,0.0,yes,9/27/18,National Tsing Hua University;Gmai;National Tsing Hua University;National Tsing Hua University;National Tsing Hua University;National Tsing Hua University;National Tsing Hua University,199;-1;199;199;199;199;199,323;-1;323;323;323;323;323,
2407,2407,2407,2407,2407,2407,2407,2407,ICLR,2019,Detecting Out-Of-Distribution Samples Using Low-Order Deep Features Statistics,Igor M. Quintanilha;Roberto de M. E. Filho;José Lezama;Mauricio Delbracio;Leonardo O. Nunes,igormq@poli.ufrj.br;robertomest@poli.ufrj.br;jlezama@fing.edu.uy;mdelbra@fing.edu.uy;lnunes@microsoft.com,7;5;5,4;4;4,Reject,3,16,1.0,yes,9/27/18,Federal University of Rio de Janeiro - UFRJ;Federal University of Rio de Janeiro - UFRJ;Facultad de Ingeniería;Facultad de Ingeniería;Microsoft,386;386;478;478;-1,715;715;631;631;-1,
2408,2408,2408,2408,2408,2408,2408,2408,ICLR,2019,Context Dependent Modulation of Activation Function,Long Sha;Jonathan Schwarcz;Pengyu Hong,longsha@brandeis.edu;johnschwarcz@brandeis.edu;hongpeng@brandeis.edu,4;4;4;4;6,5;3;4;5;4,Reject,0,5,0.0,yes,9/27/18,Brandeis University;Brandeis University;Brandeis University,314;314;314,223;223;223,
2409,2409,2409,2409,2409,2409,2409,2409,ICLR,2019,Reliable Uncertainty Estimates in Deep Neural Networks using Noise Contrastive Priors,Danijar Hafner;Dustin Tran;Timothy Lillicrap;Alex Irpan;James Davidson,mail@danijar.com;trandustin@google.com;countzero@google.com;alexirpan@google.com;james@electric-thought.com,7;4;6,3;4;4,Reject,1,4,0.0,yes,9/27/18,"Department of Computer Science, University of Toronto;Google;Google;Google;Electric-thought",18;-1;-1;-1;-1,22;-1;-1;-1;-1,11
2410,2410,2410,2410,2410,2410,2410,2410,ICLR,2019,DON’T JUDGE A BOOK BY ITS COVER - ON THE DYNAMICS OF RECURRENT NEURAL NETWORKS,Doron Haviv;Alexander Rivkind;Omri Barak,doron.haviv12@gmail.com;sashkarivkind@gmail.com;omri.barak@gmail.com,6;5;7,3;4;4,Reject,0,6,0.0,yes,9/27/18,Technion;Technion;Technion,25;25;25,327;327;327,
2411,2411,2411,2411,2411,2411,2411,2411,ICLR,2019,Volumetric Convolution: Automatic Representation Learning in Unit Ball,Sameera Ramasinghe;Salman Khan;Nick Barnes,sameera.ramasinghe@anu.edu.au;salman.khan@anu.edu.au;nick.barnes@data61.csiro.au,6;5;5,2;5;3,Reject,0,7,0.0,yes,9/27/18,"Australian National University;Australian National University;, CSIRO",106;106;-1,48;48;-1,
2412,2412,2412,2412,2412,2412,2412,2412,ICLR,2019,Classification in the dark using tactile exploration,Mayur Mudigonda;Blake Tickell;Pulkit Agrawal,mudigonda@berkeley.edu;btickell@berkeley.edu;pulkitag@berkeley.edu,4;3;2,3;5;5,Reject,0,0,0.0,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,18;18;18,
2413,2413,2413,2413,2413,2413,2413,2413,ICLR,2019,"Deep Imitative Models for Flexible Inference, Planning, and Control",Nicholas Rhinehart;Rowan McAllister;Sergey Levine,nrhineha@cs.cmu.edu;rmcallister@berkeley.edu;svlevine@eecs.berkeley.edu,6;5;6,3;5;1,Reject,0,5,0.0,yes,9/25/19,Carnegie Mellon University;University of California Berkeley;University of California Berkeley,1;5;5,24;18;18,
2414,2414,2414,2414,2414,2414,2414,2414,ICLR,2019,SHE2: Stochastic Hamiltonian Exploration and Exploitation for Derivative-Free Optimization,Haoyi Xiong;Wenqing Hu;Zhanxing Zhu;Xinjian Li;Yunchao Zhang;Jun Huan,xhyccc@gmail.com;huwenqing.pku@gmail.com;zhanxing.zhu@pku.edu.cn;lixingjian@baidu.com;yzgv7@mst.edu;huanjun@baidu.com,4;3;3,4;3;5,Reject,0,0,0.0,yes,9/27/18,Baidu;Missouri University of Science and Technology;Peking University;Baidu;Missouri University of Science and Technology;Baidu,-1;477;24;-1;477;-1,-1;538;27;-1;538;-1,
2415,2415,2415,2415,2415,2415,2415,2415,ICLR,2019,Universal discriminative quantum neural networks,Hongxiang Chen;Leonard Wossnig;Hartmut Neven;Simone Severini;Masoud Mohseni,we.taper@gmail.com;leonard.wossnig.17@ucl.ac.uk;neven@google.com;s.severini@ucl.ac.uk;mohseni@google.com,5;5;2,3;2;2,Reject,0,0,0.0,yes,9/27/18,;University College London;Google;University College London;Google,-1;50;-1;50;-1,-1;16;-1;16;-1,8
2416,2416,2416,2416,2416,2416,2416,2416,ICLR,2019,Bias Also Matters: Bias Attribution for Deep Neural Network Explanation,Shengjie Wang;Tianyi Zhou;Jeff Bilmes,wangsj@cs.washington.edu;tianyi.david.zhou@gmail.com;bilmes@uw.edu,5;5;5,5;5;4,Reject,0,0,0.0,yes,9/27/18,"University of Washington;University of Washington;University of Washington, Seattle",6;6;6,25;25;25,
2417,2417,2417,2417,2417,2417,2417,2417,ICLR,2019,Dirichlet Variational Autoencoder,Weonyoung Joo;Wonsung Lee;Sungrae Park;and Il-Chul Moon,weonyoungjoo@gmail.com;aporia@kaist.ac.kr;sungraepark@kaist.ac.kr;icmoon@kaist.ac.kr,6;5;7,4;5;3,Reject,0,10,0.0,yes,9/27/18,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,20;20;20;20,95;95;95;95,5
2418,2418,2418,2418,2418,2418,2418,2418,ICLR,2019,DANA: Scalable Out-of-the-box Distributed ASGD Without Retuning,Ido Hakimi;Saar Barkai;Moshe Gabel;Assaf Schuster,idohakimi@gmail.com;saarbarkai@gmail.com;mgabel@cs.toronto.edu;assaf@cs.technion.ac.il,5;7;5,3;4;4,Reject,0,8,0.0,yes,9/27/18,"Technion;Technion;Department of Computer Science, University of Toronto;Technion",25;25;18;25,327;327;22;327,
2419,2419,2419,2419,2419,2419,2419,2419,ICLR,2019,Bayesian Deep Learning via Stochastic Gradient MCMC with a Stochastic Approximation Adaptation,Wei Deng;Xiao Zhang;Faming Liang;Guang Lin,deng106@purdue.edu;zhang923@purdue.edu;fmliang@purdue.edu;guanglin@purdue.edu,5;4;6,4;2;5,Reject,0,0,1.0,yes,9/27/18,Purdue University;Purdue University;Purdue University;Purdue University,26;26;26;26,60;60;60;60,4;11
2420,2420,2420,2420,2420,2420,2420,2420,ICLR,2019,Physiological Signal Embeddings (PHASE) via Interpretable Stacked Models,Hugh Chen;Scott Lundberg;Gabe Erion;Su-In Lee,hughchen@cs.washington.edu;slund1@cs.washington.edu;erion@cs.washington.edu;suinlee@cs.washington.edu,6;5;4,5;4;4,Reject,0,10,0.0,yes,9/27/18,University of Washington;University of Washington;University of Washington;University of Washington,6;6;6;6,25;25;25;25,3;1;2
2421,2421,2421,2421,2421,2421,2421,2421,ICLR,2019,ReNeg and Backseat Driver: Learning from demonstration with continuous human feedback,Zoe Papakipos;Jacob Beck;Michael Littman,zoe_papakipos@alumni.brown.edu;jacob_beck@alumni.brown.edu;mlittman@cs.brown.edu,3;4;2,4;4;5,Reject,0,12,0.0,yes,9/27/18,Brown University;Brown University;Brown University,65;65;65,50;50;50,10
2422,2422,2422,2422,2422,2422,2422,2422,ICLR,2019,Learning  agents with prioritization and parameter noise in continuous state and action space,Rajesh Devaraddi;G. Srinivasaraghavan,rajesh.dm@iiitb.ac.in;gsr@iiitb.ac.in,3;4;4,4;3;4,Reject,0,0,0.0,yes,9/27/18,Indian Institute of Technology Bombay;Indian Institute of Technology Bombay,115;115,367;367,
2423,2423,2423,2423,2423,2423,2423,2423,ICLR,2019,Gradient Descent Happens in a Tiny Subspace,Guy Gur-Ari;Daniel A. Roberts;Ethan Dyer,guyg@ias.edu;danr@fb.com;edyer@google.com,4;6;4,3;4;4,Reject,0,4,1.0,yes,9/27/18,"Institue for Advanced Study, Princeton;Facebook;Google",-1;-1;-1,-1;-1;-1,
2424,2424,2424,2424,2424,2424,2424,2424,ICLR,2019,Nested Dithered Quantization for Communication Reduction in Distributed Training,Afshin Abdi;Faramarz Fekri,abdi@ece.gatech.edu;fekri@ece.gatech.edu,5;5;7,4;3;4,Reject,0,5,0.0,yes,9/27/18,Georgia Institute of Technology;Georgia Institute of Technology,13;13,33;33,1
2425,2425,2425,2425,2425,2425,2425,2425,ICLR,2019,Learned optimizers that outperform on wall-clock and validation loss,Luke Metz;Niru Maheswaranathan;Jeremy Nixon;Daniel Freeman;Jascha Sohl-dickstein,lmetz@google.com;nirum@google.com;jeremynixon@google.com;cdfreeman@google.com;jaschasd@google.com,5;4;5,3;5;4,Reject,0,10,1.0,yes,9/27/18,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,
2426,2426,2426,2426,2426,2426,2426,2426,ICLR,2019,A Walk with SGD: How SGD Explores Regions of Deep Network Loss?,Chen Xing;Devansh Arpit;Christos Tsirigotis;Yoshua Bengio,xingchen1113@gmail.com;devansharpit@gmail.com;tsirif@gmail.com;yoshua.umontreal@gmail.com,4;4;3,5;3;4,Reject,1,0,0.0,yes,9/27/18,Nankai University;University of Montreal;;University of Montreal,478;123;-1;123,1049;108;-1;108,
2427,2427,2427,2427,2427,2427,2427,2427,ICLR,2019,Combining adaptive algorithms and hypergradient method: a performance and robustness study,Akram Erraqabi;Nicolas Le Roux,akram.er-raqabi@umontreal.ca;nicolas@le-roux.name,3;3;4,4;2;4,Reject,0,4,0.0,yes,9/27/18,University of Montreal;Google,123;-1,108;-1,
2428,2428,2428,2428,2428,2428,2428,2428,ICLR,2019,The Expressive Power of Deep Neural Networks with Circulant Matrices,Alexandre Araujo;Benjamin Negrevergne;Yann Chevaleyre;Jamal Atif,alexandre.araujo@dauphine.eu;benjamin.negrevergne@dauphine.fr;yann.chevaleyre@lamsade.dauphine.fr;jamal.atif@lamsade.dauphine.fr,6;4;7,5;4;4,Reject,0,3,0.0,yes,9/27/18,Univeristé Paris-Dauphine;Univeristé Paris-Dauphine;Univeristé Paris-Dauphine;Univeristé Paris-Dauphine,478;478;478;478,1103;1103;1103;1103,
2429,2429,2429,2429,2429,2429,2429,2429,ICLR,2019,The Anisotropic Noise in Stochastic Gradient Descent: Its Behavior of Escaping from Minima and Regularization Effects,Zhanxing Zhu;Jingfeng Wu;Bing Yu;Lei Wu;Jinwen Ma,zhanxing.zhu@pku.edu.cn;pkuwjf@pku.edu.cn;byu@pku.edu.cn;leiwu@pku.edu.cn;jwma@math.pku.edu.cn,5;4;6,4;5;3,Reject,0,0,0.0,yes,9/27/18,Peking University;Peking University;Peking University;Peking University;Peking University,24;24;24;24;24,27;27;27;27;27,
2430,2430,2430,2430,2430,2430,2430,2430,ICLR,2019,Stochastic Gradient Push for Distributed Deep Learning,Mahmoud Assran;Nicolas Loizou;Nicolas Ballas;Mike Rabbat,massran@fb.com;n.loizou@sms.ed.ac.uk;ballasn@fb.com;mikerabbat@fb.com,6;6;6,3;4;3,Reject,0,9,2.0,yes,9/27/18,Facebook;University of Edinburgh;Facebook;Facebook,-1;33;-1;-1,-1;27;-1;-1,1;9
2431,2431,2431,2431,2431,2431,2431,2431,ICLR,2019,Towards Language Agnostic Universal Representations,Armen Aghajanyan;Xia Song;Saurabh Tiwary,araghaja@microsoft.com;xiaso@microsoft.com;satiwary@microsoft.com,5;4;6,4;4;3,Reject,0,7,0.0,yes,9/27/18,Microsoft;Microsoft;Microsoft,-1;-1;-1,-1;-1;-1,6
2432,2432,2432,2432,2432,2432,2432,2432,ICLR,2019,Mapping the hyponymy relation of wordnet onto vector Spaces,Jean-Philippe Bernardy;Aleksandre Maskharashvili,jean-philippe.bernardy@gu.se;aleksandre.maskharashvili@gu.se,3;3;3,4;3;5,Reject,0,1,0.0,yes,9/27/18,Gothenburg University;Gothenburg University,478;478,197;197,
2433,2433,2433,2433,2433,2433,2433,2433,ICLR,2019,Model Compression with Generative Adversarial Networks,Ruishan Liu;Nicolo Fusi;Lester Mackey,ruishan@stanford.edu;fusi@microsoft.com;lmackey@microsoft.com,6;5;5,4;4;4,Reject,0,7,0.0,yes,9/27/18,Stanford University;Microsoft;Microsoft,4;-1;-1,3;-1;-1,5;4
2434,2434,2434,2434,2434,2434,2434,2434,ICLR,2019,Efficient Convolutional Neural Network Training with Direct Feedback Alignment,Donghyeon Han;Hoi-jun Yoo,hdh4797@kaist.ac.kr;hjyoo@kaist.ac.kr,4;4;5,4;4;3,Reject,0,4,0.0,yes,9/27/18,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,20;20,95;95,
2435,2435,2435,2435,2435,2435,2435,2435,ICLR,2019, Large-Scale Visual Speech Recognition,Brendan Shillingford;Yannis Assael;Matthew W. Hoffman;Thomas Paine;Cían Hughes;Utsav Prabhu;Hank Liao;Hasim Sak;Kanishka Rao;Lorrayne Bennett;Marie Mulville;Ben Coppin;Ben Laurie;Andrew Senior;Nando de Freitas,shillingford@google.com;assael@google.com;mwhoffman@google.com;tpaine@google.com;cianh@google.com;utsavprabhu@google.com;hankliao@google.com;hasim@google.com;kanishkarao@google.com;lorrayne@google.com;mariecharlotte@google.com;coppin@google.com;benl@google.com;andrewsenior@google.com;nandodefreitas@google.com,9;4;3,4;4;5,Reject,0,17,0.0,yes,9/27/18,Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,
2436,2436,2436,2436,2436,2436,2436,2436,ICLR,2019,The effectiveness of layer-by-layer training using the information bottleneck principle,Adar Elad;Doron Haviv;Yochai Blau;Tomer Michaeli,adarelad@campus.technion.ac.il;doron.haviv12@gmail.com;yochai@campus.technion.ac.il;tomer.m@ee.technion.ac.il,5;5;2,5;4;4,Reject,0,5,0.0,yes,9/27/18,Technion;Technion;Technion;Technion,25;25;25;25,327;327;327;327,8
2437,2437,2437,2437,2437,2437,2437,2437,ICLR,2019,The Case for Full-Matrix Adaptive Regularization,Naman Agarwal;Brian Bullins;Xinyi Chen;Elad Hazan;Karan Singh;Cyril Zhang;Yi Zhang,namanagarwal@google.com;bbullins@cs.princeton.edu;xinyic@google.com;ehazan@cs.princeton.edu;karans@cs.princeton.edu;cyril.zhang@cs.princeton.edu;y.zhang@cs.princeton.edu,5;6;5,3;3;3,Reject,0,3,0.0,yes,9/27/18,Google;Princeton University;Google;Princeton University;Princeton University;Princeton University;Princeton University,-1;30;-1;30;30;30;30,-1;7;-1;7;7;7;7,9
2438,2438,2438,2438,2438,2438,2438,2438,ICLR,2019,What Information Does a ResNet Compress?,Luke Nicholas Darlow;Amos Storkey,l.n.darlow@sms.ed.ac.uk;a.storkey@ed.ac.uk,4;4;6,4;3;5,Reject,0,11,0.0,yes,9/27/18,University of Edinburgh;University of Edinburgh,33;33,27;27,
2439,2439,2439,2439,2439,2439,2439,2439,ICLR,2019,Information Regularized Neural Networks,Tianchen Zhao;Dejiao Zhang;Zeyu Sun;Honglak Lee,ericolon@umich.edu;dejiao@umich.edu;zeyusun@umich.edu;honglak@eecs.umich.edu,6;5;6,4;3;3,Reject,0,7,0.0,yes,9/27/18,University of Michigan;University of Michigan;University of Michigan;University of Michigan,8;8;8;8,21;21;21;21,1
2440,2440,2440,2440,2440,2440,2440,2440,ICLR,2019,NICE: noise injection and clamping estimation for neural network quantization,Chaim Baskin;Natan Liss;Yoav Chai;Evgenii Zheltonozhskii;Eli Schwartz;Raja Girayes;Avi Mendelson;Alexander M.Bronstein,chaimbaskin@cs.technion.ac.il;lissnatan@campus.technion.ac.il;yoavchai1@mail.tau.ac.il;evgeniizh@campus.technion.ac.il;eli.shw@gmail.com;raja@tauex.tau.ac.il;avi.mendelson@tce.technion.ac.il;bron@cs.technion.ac.il,4;5;4,4;3;3,Reject,1,0,0.0,yes,9/27/18,Technion;Technion;Tel Aviv University;Technion;Tel Aviv University;Tel Aviv University;Technion;Technion,25;25;37;25;37;37;25;25,327;327;217;327;217;217;327;327,3;2
2441,2441,2441,2441,2441,2441,2441,2441,ICLR,2019,Mean Replacement Pruning  ,Utku Evci;Nicolas Le Roux;Pablo Castro;Leon Bottou,evcu@google.com;nicolas@le-roux.name;psc@google.com;leon@bottou.org,5;5;4,3;3;4,Reject,0,9,0.0,yes,9/27/18,Google;Google;Google;Facebook,-1;-1;-1;-1,-1;-1;-1;-1,
2442,2442,2442,2442,2442,2442,2442,2442,ICLR,2019,The Effectiveness of Pre-Trained Code Embeddings,Ben Trevett;Donald Reay;N. K. Taylor,bbt1@hw.ac.uk;n.k.taylor@hw.ac.uk;d.s.reay@hw.ac.uk,6;4;5,3;4;4,Reject,0,5,0.0,yes,9/27/18,Heriot-Watt University;Heriot-Watt University;Heriot-Watt University,261;261;261,363;363;363,3
2443,2443,2443,2443,2443,2443,2443,2443,ICLR,2019,Laplacian Networks: Bounding Indicator Function Smoothness for Neural Networks Robustness,Carlos Eduardo Rosar Kos Lassance;Vincent Gripon;Antonio Ortega,carlos.rosarkoslassance@imt-atlantique.fr;vincent.gripon@imt-atlantique.fr;antonio.ortega@ee.usc.edu,9;5;5,5;3;4,Reject,0,11,0.0,yes,9/27/18,IMT Atlantique;IMT Atlantique;University of Southern California,-1;-1;30,-1;-1;66,4;10
2444,2444,2444,2444,2444,2444,2444,2444,ICLR,2019,Jumpout: Improved Dropout for Deep Neural Networks with Rectified Linear Units,Shengjie Wang;Tianyi Zhou;Jeff Bilmes,tianyi.david.zhou@gmail.com,5;4;4,4;3;5,Reject,0,5,0.0,yes,9/27/18,University of Washington,6,25,8
2445,2445,2445,2445,2445,2445,2445,2445,ICLR,2019,Causal importance of orientation selectivity for generalization in image recognition,Jumpei Ukita,i.love.ny517@gmail.com,5;4;7,4;4;2,Reject,0,5,1.0,yes,9/27/18,The University of Tokyo,54,45,8
2446,2446,2446,2446,2446,2446,2446,2446,ICLR,2019,Backprop with Approximate Activations for Memory-efficient Network Training,Ayan Chakrabarti;Benjamin Moseley,ayan@wustl.edu;moseleyb@andrew.cmu.edu,5;5;7,5;3;4,Reject,0,9,0.0,yes,9/27/18,"Washington University, St. Louis;Carnegie Mellon University",99;1,50;24,
2447,2447,2447,2447,2447,2447,2447,2447,ICLR,2019,Experience replay for continual learning,David Rolnick;Arun Ahuja;Jonathan Schwarz;Timothy P. Lillicrap;Greg Wayne,drolnick@mit.edu;arahuja@google.com;schwarzjn@google.com;countzero@google.com;gregwayne@google.com,5;5;5,5;4;5,Reject,0,4,0.0,yes,9/27/18,Massachusetts Institute of Technology;Google;Google;Google;Google,2;-1;-1;-1;-1,5;-1;-1;-1;-1,
2448,2448,2448,2448,2448,2448,2448,2448,ICLR,2019,Doubly Sparse: Sparse Mixture of Sparse Experts for Efficient Softmax Inference,Shun Liao;Ting Chen;Tian Lin;Chong Wang;Dengyong Zhou,sliao3@cs.toronto.edu;tingchen@cs.ucla.edu;tianlin@google.com;dennyzhou@google.com;chongw@google.com,6;4;7,3;3;3,Reject,0,4,0.0,yes,9/27/18,"Department of Computer Science, University of Toronto;University of California, Los Angeles;Google;Google;Google",18;20;-1;-1;-1,22;15;-1;-1;-1,
2449,2449,2449,2449,2449,2449,2449,2449,ICLR,2019,LARGE BATCH SIZE TRAINING OF NEURAL NETWORKS WITH ADVERSARIAL TRAINING AND SECOND-ORDER INFORMATION,Zhewei Yao;Amir Gholami;Kurt Keutzer;Michael Mahoney,zheweiy@berkeley.edu;amirgh@berkeley.edu;keutzer@berkeley.edu;mmahoney@stat.berkeley.edu,7;4;4,4;5;4,Reject,0,7,1.0,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5,18;18;18;18,4;8
2450,2450,2450,2450,2450,2450,2450,2450,ICLR,2019,Double Neural Counterfactual Regret Minimization,Hui Li;Kailiang Hu;Zhibang Ge;Tao Jiang;Yuan Qi;Le Song,ken.lh@antfin.com;hkl163251@antfin.com;zhibang.zg@antfin.com;lvshan.jt@antfin.com;yuan.qi@antfin.com;lsong@cc.gatech.edu,5;6;4,4;2;5,Reject,5,8,0.0,yes,9/25/19,Alibaba Group and Ant Financial Services Group;Antfin;Antfin;Antfin;Antfin;Georgia Institute of Technology,-1;-1;-1;-1;-1;13,-1;-1;-1;-1;-1;33,
2451,2451,2451,2451,2451,2451,2451,2451,ICLR,2019,Escaping Flat Areas via Function-Preserving Structural Network Modifications,Yannic Kilcher;Gary Bécigneul;Thomas Hofmann,yannic.kilcher@inf.ethz.ch;garybecigneul06@gmail.com;thomas.hofmann@inf.ethz.ch,6;6;4,4;3;4,Reject,0,2,0.0,yes,9/27/18,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10,10;10;10,
2452,2452,2452,2452,2452,2452,2452,2452,ICLR,2019,DEFactor: Differentiable Edge Factorization-based Probabilistic Graph Generation,Rim Assouel;Mohamed Ahmed;Marwin Segler;Amir Saffari;Yoshua Bengio,rim.assouel@hotmail.fr;mohamed.ahmed@benevolent.ai;marwin.segler@benevolent.ai;amir.saffari@benevolent.ai;yoshua.bengio@mila.quebec,4;3;5,4;5;3,Reject,4,4,0.0,yes,9/27/18,University of Montreal;BenevolentAI;BenevolentAI;BenevolentAI;University of Montreal,123;-1;-1;-1;123,108;-1;-1;-1;108,5;10
2453,2453,2453,2453,2453,2453,2453,2453,ICLR,2019,Neural Random Projections for Language Modelling,Davide Nunes;Luis Antunes,nunesd@campus.ul.pt;xarax@ciencias.ulisboa.pt,3;4;3,4;3;4,Reject,0,7,0.0,yes,9/27/18,"Faculdade de Ciências, Universidade de Lisboa, Portugal;University of Lisbon",99;99,509;509,3
2454,2454,2454,2454,2454,2454,2454,2454,ICLR,2019,NATTACK: A STRONG AND UNIVERSAL GAUSSIAN BLACK-BOX ADVERSARIAL ATTACK,Yandong Li;Lijun Li;Liqiang Wang;Tong Zhang;Boqing Gong,lyndon.leeseu@outlook.com;lilijun1990@buaa.edu.cn;lwang@cs.ucf.edu;bradymzhang@tencent.com;boqinggo@outlook.com,7;4;4,3;3;5,Reject,4,25,0.0,yes,9/27/18,University of Central Florida;Beihang University;University of Central Florida;Tencent AI Lab;International Computer Science Institute,78;115;78;-1;-1,1103;658;1103;-1;-1,4
2455,2455,2455,2455,2455,2455,2455,2455,ICLR,2019,Improved resistance of neural networks to adversarial images through generative pre-training,Joachim Wabnig,joachim.wabnig@nokia-bell-labs.com,4;4;6,4;3;4,Reject,0,6,0.0,yes,9/27/18,Nokia Bell Labs,-1,-1,5;4
2456,2456,2456,2456,2456,2456,2456,2456,ICLR,2019,Select Via Proxy: Efficient Data Selection For Training Deep Networks,Cody Coleman;Stephen Mussmann;Baharan Mirzasoleiman;Peter Bailis;Percy Liang;Jure Leskovec;Matei Zaharia,cody@cs.stanford.edu;mussmann@stanford.edu;baharanm@stanford.edu;pbailis@stanford.edu;pliang@cs.stanford.edu;jure@cs.stanford.edu;mzaharia@stanford.edu,4;4;5,4;2;4,Reject,2,5,0.0,yes,9/27/18,Stanford University;Stanford University;Stanford University;Stanford University;Stanford University;Stanford University;Stanford University,4;4;4;4;4;4;4,3;3;3;3;3;3;3,3
2457,2457,2457,2457,2457,2457,2457,2457,ICLR,2019,Generalized Capsule Networks with Trainable Routing Procedure,Zhenhua Chen;Chuhua Wang;Tiancong Zhao;David Crandall,chen478@iu.edu;cw234@iu.edu;tz11@iu.edu;djcran@iu.edu,4;5;3,5;3;5,Reject,0,6,0.0,yes,9/27/18,"Indiana University, Bloomington;Indiana University, Bloomington;Indiana University, Bloomington;Indiana University, Bloomington",72;72;72;72,117;117;117;117,5;4;8
2458,2458,2458,2458,2458,2458,2458,2458,ICLR,2019,Padam: Closing the Generalization Gap of Adaptive Gradient Methods in Training Deep Neural Networks,Jinghui Chen;Quanquan Gu,jc4zg@virginia.edu;qgu@cs.ucla.edu,6;6;9,4;4;3,Reject,0,5,0.0,yes,9/27/18,"University of Virginia;University of California, Los Angeles",65;20,113;15,9;8
2459,2459,2459,2459,2459,2459,2459,2459,ICLR,2019,Text Embeddings for Retrieval from a Large Knowledge Base,Tolgahan Cakaloglu;Christian Szegedy;Xiaowei Xu,txcakaloglu@ualr.edu;szegedy@google.com;xwxu@ualr.edu,3;5;3,4;4;5,Reject,0,0,0.0,yes,9/27/18,"University of Arkansas, Little Rock;Google;University of Arkansas, Little Rock",314;-1;314,585;-1;585,3
2460,2460,2460,2460,2460,2460,2460,2460,ICLR,2019,Combining Learned Representations for Combinatorial Optimization,Saavan Patel;Sayeef Salahuddin,saavan@berkeley.edu;sayeef@berkeley.edu,4;4;5,3;5;3,Reject,0,4,0.0,yes,9/27/18,University of California Berkeley;University of California Berkeley,5;5,18;18,
2461,2461,2461,2461,2461,2461,2461,2461,ICLR,2019,Intriguing Properties of Learned Representations,Amartya Sanyal;Varun Kanade;Philip H. Torr,amartya.sanyal@cs.ox.ac.uk;varunk@cs.ox.ac.uk;philip.torr@eng.ox.ac.uk,3;6;5,4;2;2,Reject,0,5,0.0,yes,9/27/18,University of Oxford;University of Oxford;University of Oxford,50;50;50,1;1;1,4;6
2462,2462,2462,2462,2462,2462,2462,2462,ICLR,2019,Predictive Local Smoothness for Stochastic Gradient Methods,Jun Li;Hongfu Liu;Bineng Zhong;Yue Wu;Yun Fu,junl.mldl@gmail.com;hongfuliu@brandeis.edu;bnzhong@gmail.com;wuyuebupt@gmail.com;yunfu@ece.neu.edu,2;3;2;4,4;3;5;5,Reject,0,0,0.0,yes,9/27/18,Massachusetts Institute of Technology;Brandeis University;Tsinghua University;;Northeastern University,2;314;8;-1;16,5;223;30;-1;839,1;9
2463,2463,2463,2463,2463,2463,2463,2463,ICLR,2019,Accelerating first order optimization algorithms,Ange tato;Roger nkambou,nyamen_tato.ange_adrienne@courrier.uqam.ca;nkambou.roger@uqam.ca,3;4;4,3;3;5,Reject,0,0,0.0,yes,9/27/18,université du Québec à Montreal;université du Québec à Montreal,123;123,108;108,
2464,2464,2464,2464,2464,2464,2464,2464,ICLR,2019,An Analysis of Composite Neural Network Performance from Function Composition Perspective,Ming-Chuan Yang;Meng Chang Chen,mingchuan@iis.sinica.edu.tw;mcc@iis.sinica.edu.tw,3;3;3,2;3;4,Reject,0,3,0.0,yes,9/27/18,Academia Sinica;Academia Sinica,-1;-1,-1;-1,1;10
2465,2465,2465,2465,2465,2465,2465,2465,ICLR,2019,Complexity of Training ReLU Neural Networks,Digvijay Boob;Santanu S. Dey;Guanghui Lan,digvijaybb40@gatech.edu;santanu.dey@isye.gatech.edu;george.lan@isye.gatech.edu,3;5;4,5;5;3,Reject,0,0,0.0,yes,9/27/18,Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology,13;13;13,33;33;33,
2466,2466,2466,2466,2466,2466,2466,2466,ICLR,2019,Backdrop: Stochastic Backpropagation,Siavash Golkar;Kyle Cranmer,siavash.golkar@gmail.com;kyle.cranmer@nyu.edu,5;5;3,3;3;3,Reject,0,0,0.0,yes,9/27/18,New York University;New York University,26;26,27;27,8
2467,2467,2467,2467,2467,2467,2467,2467,ICLR,2019,A preconditioned accelerated stochastic gradient descent algorithm,Alexandru Onose;Seyed Iman Mossavat;Henk-Jan H. Smilde,alexandru.onose@asml.com;iman.mossavat@asml.com;henk-jan.smilde@asml.com,4;4;5,3;5;3,Reject,0,0,0.0,yes,9/27/18,Asml;National University of Singapore;Asml,-1;16;-1,-1;22;-1,1
2468,2468,2468,2468,2468,2468,2468,2468,ICLR,2019,Attentive Task-Agnostic Meta-Learning for Few-Shot Text Classification,Xiang Jiang;Mohammad Havaei;Gabriel Chartrand;Hassan Chouaib;Thomas Vincent;Andrew Jesson;Nicolas Chapados;Stan Matwin,xiang.jiang@dal.ca;mohammad@imagia.com;gabriel@imagia.com;hassan.chouaib@imagia.com;thomas.vincent@imagia.com;andrew.jesson@imagia.com;nic@imagia.com;stan@cs.dal.ca,5;5;7,4;3;3,Reject,0,0,0.0,yes,9/27/18,Dalhousie University;Imagia;Imagia;Imagia;Imagia;Imagia;Imagia;Dalhousie University,314;-1;-1;-1;-1;-1;-1;314,289;-1;-1;-1;-1;-1;-1;289,3;6;8
2469,2469,2469,2469,2469,2469,2469,2469,ICLR,2019,Open Vocabulary Learning on Source Code with a Graph-Structured Cache,Milan Cvitkovic;Badal Singh;Anima Anandkumar,mcvitkov@caltech.edu;sbadal@amazon.com;anima@caltech.edu,4;4;6,4;4;5,Reject,0,13,0.0,yes,9/27/18,California Institute of Technology;Amazon;California Institute of Technology,140;-1;140,3;-1;3,3;10
2470,2470,2470,2470,2470,2470,2470,2470,ICLR,2019,The Natural Language Decathlon: Multitask Learning as Question Answering,Bryan McCann;Nitish Shirish Keskar;Caiming Xiong;Richard Socher,bmccann@salesforce.com;nkeskar@salesforce.com;cxiong@salesforce.com;rsocher@salesforce.com,5;5;3,3;4;4,Reject,2,18,0.0,yes,9/27/18,SalesForce.com;SalesForce.com;SalesForce.com;SalesForce.com,-1;-1;-1;-1,-1;-1;-1;-1,3;6
2471,2471,2471,2471,2471,2471,2471,2471,ICLR,2019,NEURAL MALWARE CONTROL WITH DEEP REINFORCEMENT LEARNING,Yu Wang;Jack W. Stokes;Mady Marinescu,yu.wang@yale.edu;jstokes@microsoft.com;mady@microsoft.com,5;4;5,2;3;2,Reject,0,3,0.0,yes,9/27/18,Yale University;Microsoft;Microsoft,62;-1;-1,12;-1;-1,4
2472,2472,2472,2472,2472,2472,2472,2472,ICLR,2019,Beyond Games: Bringing Exploration to Robots in Real-world,Deepak Pathak;Dhiraj Gandhi;Abhinav Gupta,pathak@berkeley.edu;dgandhi@andrew.cmu.edu;abhinavg@cs.cmu.edu,5;3;3,3;5;4,Reject,0,18,0.0,yes,9/27/18,University of California Berkeley;Carnegie Mellon University;Carnegie Mellon University,5;1;1,18;24;24,
2473,2473,2473,2473,2473,2473,2473,2473,ICLR,2019,Overcoming catastrophic forgetting through weight consolidation and long-term memory,Shixian Wen;Laurent Itti,shixianwen1993@gmail.com;itti@usc.edu,4;4;4,4;4;5,Reject,2,0,0.0,yes,9/27/18,University of Southern California;University of Southern California,30;30,66;66,4
2474,2474,2474,2474,2474,2474,2474,2474,ICLR,2019,Localized random projections challenge benchmarks for bio-plausible deep learning,Bernd Illing;Wulfram Gerstner;Johanni Brea,bernd.illing@epfl.ch;wulfram.gerstner@epfl.ch;johanni.brea@epfl.ch,5;3;3,3;4;5,Reject,0,4,0.0,yes,9/27/18,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne,478;478;478,38;38;38,
2475,2475,2475,2475,2475,2475,2475,2475,ICLR,2019,Learning Goal-Conditioned Value Functions with one-step Path rewards rather than Goal-Rewards,Vikas Dhiman;Shurjo Banerjee;Jeffrey M Siskind;Jason J Corso,dhiman@umich.edu;shurjo@umich.edu;qobi@purdue.edu;jjcorso@umich.edu,4;1;3,3;4;4,Reject,2,6,1.0,yes,9/27/18,University of Michigan;University of Michigan;Purdue University;University of Michigan,8;8;26;8,21;21;60;21,
2476,2476,2476,2476,2476,2476,2476,2476,ICLR,2019,Unsupervised Hyper-alignment for Multilingual Word Embeddings,Jean Alaux;Edouard Grave;Marco Cuturi;Armand Joulin,jean.alaux--lorain@ens.fr;egrave@fb.com;marco.cuturi.cameto@gmail.com;ajoulin@fb.com,5;6;7,3;4;3,Accept (Poster),0,1,0.0,yes,9/27/18,Ecole Normale Superieure;Facebook;Google;Facebook,99;-1;-1;-1,603;-1;-1;-1,
2477,2477,2477,2477,2477,2477,2477,2477,ICLR,2019,Synthnet: Learning synthesizers end-to-end,Florin Schimbinschi;Christian Walder;Sarah Erfani;James Bailey,florinsch@student.unimelb.edu.au;christian.walder@data61.csiro.au;sarah.erfani@unimelb.edu.au;baileyj@unimelb.edu.au,4;4;3,3;4;5,Reject,0,21,0.0,yes,9/27/18,"The University of Melbourne;, CSIRO;The University of Melbourne;The University of Melbourne",123;-1;123;123,32;-1;32;32,5
2478,2478,2478,2478,2478,2478,2478,2478,ICLR,2019,Relational Graph Attention Networks,Dan Busbridge;Dane Sherburn;Pietro Cavallo;Nils Y. Hammerla,dan.busbridge@gmail.com;danesherbs@gmail.com;p.cavallo85@gmail.com;nils.hammerla@babylonhealth.com,4;4;4,5;4;5,Reject,6,4,0.0,yes,9/27/18,babylon health;babylon health;;babylon health,-1;-1;-1;-1,-1;-1;-1;-1,10
2479,2479,2479,2479,2479,2479,2479,2479,ICLR,2019,IEA: Inner Ensemble Average within a convolutional neural network,Abduallah Mohamed;Xinrui Hua;Xianda Zhou;Christian Claudel,abduallah.mohamed@utexas.edu;xinruihua@utexas.edu;xianda@utexas.edu;christian.claudel@utexas.edu,4;2;4,3;4;5,Reject,0,6,0.0,yes,9/27/18,"University of Texas, Austin;University of Texas, Austin;University of Texas, Austin;University of Texas, Austin",22;22;22;22,49;49;49;49,
2480,2480,2480,2480,2480,2480,2480,2480,ICLR,2019,ChainGAN: A sequential approach to GANs,Safwan Hossain;Kiarash Jamali;Yuchen Li;Frank Rudzicz,safwan.hossain@mail.utoronto.ca;kiarash.jamali@mail.utoronto.ca;ychnlgy.li@utoronto.ca;frank@spoclab.com,4;4;4,4;4;4,Reject,0,0,0.0,yes,9/27/18,Toronto University;Toronto University;Toronto University;University of Toronto,18;18;18;18,22;22;22;22,5;4
2481,2481,2481,2481,2481,2481,2481,2481,ICLR,2019,Adaptive Convolutional Neural Networks,Julio Cesar Zamora;Jesus Adan Cruz Vargas;Omesh Tickoo,julio.c.zamora.esquivel@intel.com;jesus.a.cruz.vargas@intel.com;omesh.tickoo@intel.com,5;4;4,3;3;4,Reject,0,3,0.0,yes,9/27/18,Intel;Intel;Intel,-1;-1;-1,-1;-1;-1,
2482,2482,2482,2482,2482,2482,2482,2482,ICLR,2019,Explicit Recall for Efficient Exploration,Honghua Dong;Jiayuan Mao;Xinyue Cui;Lihong Li,dhh19951@gmail.com;maojiayuan@gmail.com;rogar2233cxy@gmail.com;lihongli.cs@gmail.com,7;4;3,3;4;4,Reject,0,6,0.0,yes,9/27/18,Tsinghua University;Tsinghua University;;Google,8;8;-1;-1,30;30;-1;-1,
2483,2483,2483,2483,2483,2483,2483,2483,ICLR,2019,A Multi-modal one-class generative adversarial network for anomaly detection in manufacturing,Shuhui Qu;Janghwan Lee;Wei Xiong;Wonhyouk Jang;Jie Wang,shuhuiq@stanford.edu;jake.ee@samsung.com;w.xiong@samsung.com;damian.jang@samsung.com;jiewang@stanford.edu,3;4;5,4;5;4,Reject,0,0,0.0,yes,9/27/18,Stanford University;Samsung;Samsung;Samsung;Stanford University,4;-1;-1;-1;4,3;-1;-1;-1;3,5;4
2484,2484,2484,2484,2484,2484,2484,2484,ICLR,2019,Learning to encode spatial relations from natural language,Tiago Ramalho;Tomas Kocisky‎;Frederic Besse;S. M. Ali Eslami;Gabor Melis;Fabio Viola;Phil Blunsom;Karl Moritz Hermann,tiago.mpramalho@gmail.com;tkocisky@google.com;fbesse@google.com;aeslami@google.com;melisgl@google.com;fviola@google.com;pblunsom@google.com;kmh@google.com,6;5;5,5;4;4,Reject,0,3,0.0,yes,9/27/18,Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1,3
2485,2485,2485,2485,2485,2485,2485,2485,ICLR,2019,D-GAN: Divergent generative adversarial network for positive unlabeled learning and counter-examples generation,Florent CHIARONI. Mohamed-Cherif RAHAL. Nicolas HUEBER. Frédéric DUFAUX.,florent.chiaroni@vedecom.fr;mohamed.rahal@vedecom.fr;nicolas.hueber@isl.eu;frederic.dufaux@l2s.centralesupelec.fr,3;5;3,4;1;5,Reject,1,11,0.0,yes,9/27/18,CentraleSupelec;;;CentraleSupelec,478;-1;-1;478,452;-1;-1;452,5;4
2486,2486,2486,2486,2486,2486,2486,2486,ICLR,2019,Assessing Generalization in Deep Reinforcement Learning,Charles Packer*;Katelyn Gao*;Jernej Kos;Philipp Krahenbuhl;Vladlen Koltun;Dawn Song,cpacker@berkeley.edu;katelyn.gao@intel.com;jernej@kos.mx;philkr@cs.utexas.edu;vladlen.koltun@intel.com;dawnsong@berkeley.edu,5;3;5,2;5;3,Reject,0,5,0.0,yes,9/27/18,"University of California Berkeley;Intel;National University of Singapore;University of Texas, Austin;Intel;University of California Berkeley",5;-1;16;22;-1;5,18;-1;22;49;-1;18,8
2487,2487,2487,2487,2487,2487,2487,2487,ICLR,2019,Where and when to look? Spatial-temporal attention for action recognition in videos,Lili Meng;Bo Zhao;Bo Chang;Gao Huang;Frederick Tung;Leonid Sigal,lilimeng1103@gmail.com;bzhao03@cs.ubc.ca;bchang@stat.ubc.ca;gh349@cornell.edu;ftung@sfu.ca;lsigal@cs.ubc.ca,6;6;3,4;4;5,Reject,0,8,0.0,yes,9/27/18,University of British Columbia;University of British Columbia;University of British Columbia;Cornell University;Simon Fraser University;University of British Columbia,36;36;36;7;62;36,34;34;34;19;253;34,
2488,2488,2488,2488,2488,2488,2488,2488,ICLR,2019,Integrated Steganography and Steganalysis with Generative Adversarial Networks,Chong Yu,dxxzdxxz@126.com,5;6;5,5;4;2,Reject,0,2,0.0,yes,9/27/18,NVIDIA,-1,-1,5;4;2
2489,2489,2489,2489,2489,2489,2489,2489,ICLR,2019,Heated-Up Softmax Embedding,Xu Zhang;Felix Xinnan Yu;Svebor Karaman;Wei Zhang;Shih-Fu Chang,xu.zhang@columbia.edu;felixyu@google.com;svebor.karaman@gmail.com;wz2363@columbia.edu;sc250@columbia.edu,8;3;5,4;5;4,Reject,0,3,0.0,yes,9/27/18,Columbia University;Google;Columbia University;Columbia University;Columbia University,15;-1;15;15;15,14;-1;14;14;14,8
2490,2490,2490,2490,2490,2490,2490,2490,ICLR,2019,"S3TA: A Soft, Spatial, Sequential, Top-Down Attention Model",Alex Mott;Daniel Zoran;Mike Chrzanowski;Daan Wierstra;Danilo J. Rezende,alexmott@google.com;danielzoran@google.com;chrzanowskim@google.com;wierstra@google.com;danilor@google.com,5;5;5,4;4;4,Reject,0,4,0.0,yes,9/27/18,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,
2491,2491,2491,2491,2491,2491,2491,2491,ICLR,2019,ON THE USE OF CONVOLUTIONAL AUTO-ENCODER FOR INCREMENTAL CLASSIFIER LEARNING IN CONTEXT AWARE ADVERTISEMENT,Tin Lay Nwe;Shudong Xie;Balaji Nataraj;Yiqun Li;Joo-Hwee Lim,tlnma@i2r.a-star.edu.sg;xie_shudong@i2r.a-star.edu.sg;e0267605@u.nus.edu;yqli@i2r.a-star.edu.sg;joohwee@i2r.a-star.edu.sg,5;4;3,5;4;4,Reject,0,0,0.0,yes,9/27/18,A*STAR;A*STAR;National University of Singapore;A*STAR;A*STAR,-1;-1;16;-1;-1,-1;-1;22;-1;-1,
2492,2492,2492,2492,2492,2492,2492,2492,ICLR,2019,"Look Ma, No GANs! Image Transformation with ModifAE",Chad Atalla;Bartholomew Tam;Amanda Song;Gary Cottrell,chada@ucsd.edu;b4tam@ucsd.edu,3;4;5,4;4;3,Reject,0,0,0.0,yes,9/27/18,"University of California, San Diego;University of California, San Diego",11;11,31;31,5
2493,2493,2493,2493,2493,2493,2493,2493,ICLR,2019,Object detection deep learning networks for Optical Character Recognition,Christopher Bourez;Aurelien Coquard,christopher.bourez@gmail.com;acq@ivalua.com,2;1;2;1,5;5;5;5,Reject,0,0,0.0,yes,9/27/18,;Ivalua Inc,-1;-1,-1;-1,2
2494,2494,2494,2494,2494,2494,2494,2494,ICLR,2019,Learning to Drive by Observing the Best and Synthesizing the Worst,Mayank Bansal;Alex Krizhevsky;Abhijit Ogale,mayban@waymo.com;akrizhevsky@gmail.com;ogale@waymo.com,3;6;5,4;4;4,Reject,0,8,0.0,yes,9/27/18,Waymo;;Waymo,-1;-1;-1,-1;-1;-1,
2495,2495,2495,2495,2495,2495,2495,2495,ICLR,2019,An adaptive homeostatic algorithm for the unsupervised learning of visual features,Victor Boutin;Angelo Franciosini;Laurent Perrinet,victor.boutin@univ-amu.fr;angelo.franciosini@univ-amu.fr;laurent.perrinet@univ-amu.fr,5;4;9,5;4;4,Reject,0,5,0.0,yes,9/27/18,Aix Marseille Univ;Aix Marseille Univ;Aix Marseille Univ,478;478;478,297;297;297,
2496,2496,2496,2496,2496,2496,2496,2496,ICLR,2019,Efficient Exploration through Bayesian Deep Q-Networks,Kamyar Azizzadenesheli;Animashree Anandkumar,kazizzad@uci.edu;anima@caltech.edu,4;6;2;4,2;2;5;4,Reject,2,16,3.0,yes,9/27/18,"University of California, Irvine;California Institute of Technology",35;140,99;3,11;1
2497,2497,2497,2497,2497,2497,2497,2497,ICLR,2019,Radial Basis Feature Transformation to Arm CNNs Against Adversarial Attacks,Saeid Asgari Taghanaki;Shekoofeh Azizi;Ghassan Hamarneh,sasgarit@sfu.ca;shazizi@ece.ubc.ca;hamarneh@sfu.ca,4;4;3,4;3;4,Reject,0,0,0.0,yes,9/27/18,Simon Fraser University;University of British Columbia;Simon Fraser University,62;36;62,253;34;253,4;2
2498,2498,2498,2498,2498,2498,2498,2498,ICLR,2019,Machine Translation With Weakly Paired Bilingual Documents,Lijun Wu;Jinhua Zhu;Di He;Fei Gao;Xu Tan;Tao Qin;Tie-Yan Liu,wulijun3@mail2.sysu.edu.cn;teslazhu@mail.ustc.edu.cn;di_he@pku.edu.cn;feiga@microsoft.com;xuta@microsoft.com;taoqin@microsoft.com;tyliu@microsoft.com,7;6;5,5;3;5,Reject,1,14,0.0,yes,9/27/18,SUN YAT-SEN UNIVERSITY;University of Science and Technology of China;Peking University;Microsoft;Microsoft;Microsoft;Microsoft,478;478;24;-1;-1;-1;-1,352;132;27;-1;-1;-1;-1,3
2499,2499,2499,2499,2499,2499,2499,2499,ICLR,2019,DVOLVER: Efficient Pareto-Optimal Neural Network Architecture Search,Guillaume Michel;Mohammed Amine Alaoui;Alice Lebois;Amal Feriani;Mehdi Felhi,guillaume.michel@netatmo.com;mohammed-amine.alaoui@netatmo.com;alice.lebois@netatmo.com;amal.feriani@netatmo.com;mehdi.felhi@netatmo.com,4;5;4,4;3;4,Reject,0,3,0.0,yes,9/27/18,Netatmo;Netatmo;Netatmo;Georgia Institute of Technology;Netatmo,-1;-1;-1;13;-1,-1;-1;-1;33;-1,
2500,2500,2500,2500,2500,2500,2500,2500,ICLR,2019,Complementary-label learning for arbitrary losses and models,Takashi Ishida;Gang Niu;Aditya Krishna Menon;Masashi Sugiyama,ishida@ms.k.u-tokyo.ac.jp;gang.niu@riken.jp;aditya.menon@anu.edu.au;sugi@k.u-tokyo.ac.jp,5;5;6,3;4;4,Reject,0,3,0.0,yes,9/27/18,The University of Tokyo;RIKEN;Australian National University;The University of Tokyo,54;-1;106;54,45;-1;48;45,
2501,2501,2501,2501,2501,2501,2501,2501,ICLR,2019,Feature Transformers: A Unified Representation Learning Framework for Lifelong Learning,Hariharan Ravishankar;Rahul Venkataramani;Saihareesh Anamandra;Prasad Sudhakar,hariharan.ravishankar@ge.com;rahul.venkataramani@ge.com;saihareesh.anamandra@ge.com;prasad.sudhakar@ge.com,4;3;4,3;4;5,Reject,0,4,0.0,yes,9/27/18,General Electric;General Electric;General Electric;General Electric,-1;-1;-1;-1,-1;-1;-1;-1,
2502,2502,2502,2502,2502,2502,2502,2502,ICLR,2019,Functional Bayesian Neural Networks for Model Uncertainty Quantification,Nanyang Ye;Zhanxing Zhu,yn272@cam.ac.uk;zhanxing.zhu@pku.edu.cn,3;4;5,3;4;2,Reject,0,0,0.0,yes,9/27/18,University of Cambridge;Peking University,71;24,2;27,11
2503,2503,2503,2503,2503,2503,2503,2503,ICLR,2019,Learning shared manifold representation of images and attributes for generalized zero-shot learning,Masahiro Suzuki;Yusuke Iwasawa;Yutaka Matsuo,masa@weblab.t.u-tokyo.ac.jp;iwasawa@weblab.t.u-tokyo.ac.jp;matsuo@weblab.t.u-tokyo.ac.jp,5;5;4,4;4;5,Reject,0,6,0.0,yes,9/27/18,The University of Tokyo;The University of Tokyo;The University of Tokyo,54;54;54,45;45;45,5;6
2504,2504,2504,2504,2504,2504,2504,2504,ICLR,2019,Continual Learning via Explicit Structure Learning,Xilai Li;Yingbo Zhou;Tianfu Wu;Richard Socher;Caiming Xiong,xli47@ncsu.edu;yingbo.zhou@salesforce.com;tianfu_wu@ncsu.edu;rsocher@salesforce.com;cxiong@salesforce.com,4;4;4,4;4;5,Reject,0,8,0.0,yes,9/27/18,North Carolina State University;SalesForce.com;North Carolina State University;SalesForce.com;SalesForce.com,89;-1;89;-1;-1,275;-1;275;-1;-1,
2505,2505,2505,2505,2505,2505,2505,2505,ICLR,2019,In Your Pace: Learning the Right Example at the Right Time,Guy Hacohen;Daphna Weinshall,guy.hacohen@mail.huji.ac.il;daphna@cs.huji.ac.il,5;4;4,4;4;4,Reject,3,1,0.0,yes,9/27/18,Hebrew University of Jerusalem;Hebrew University of Jerusalem,65;65,205;205,6
2506,2506,2506,2506,2506,2506,2506,2506,ICLR,2019,Polar Prototype Networks,Pascal Mettes;Elise van der Pol;Cees G. M. Snoek,p.s.m.mettes@uva.nl;e.e.vanderpol@uva.nl;cgmsnoek@uva.nl,5;3;4,3;4;5,Reject,0,4,0.0,yes,9/27/18,University of Amsterdam;University of Amsterdam;University of Amsterdam,169;169;169,59;59;59,
2507,2507,2507,2507,2507,2507,2507,2507,ICLR,2019,Meta-Learning for Contextual Bandit Exploration,Amr Sharaf;Hal Daumé III,amr@cs.umd.edu;hal@umiacs.umd.edu,7;6;3,4;4;4,Reject,0,3,0.0,yes,9/27/18,"University of Maryland, College Park;University of Maryland, College Park",12;12,69;69,6
2508,2508,2508,2508,2508,2508,2508,2508,ICLR,2019,FEED: Feature-level Ensemble Effect for knowledge Distillation,SeongUk Park;Nojun Kwak,swpark0703@snu.ac.kr;nojunk@snu.ac.kr,5;4;4,3;3;4,Reject,0,5,0.0,yes,9/27/18,Seoul National University;Seoul National University,41;41,74;74,
2509,2509,2509,2509,2509,2509,2509,2509,ICLR,2019,Like What You Like: Knowledge Distill via Neuron Selectivity Transfer,Zehao Huang;Naiyan Wang,zehaohuang18@gmail.com;winsty@gmail.com,4;4;6,4;4;5,Reject,2,0,0.0,yes,9/27/18,;,-1;-1,-1;-1,2
2510,2510,2510,2510,2510,2510,2510,2510,ICLR,2019,Activity Regularization for Continual Learning,Quang H. Pham;Steven C. H. Hoi,hqpham.2017@smu.edu.sg;chhoi@smu.edu.sg,4;4;4,5;5;4,Reject,0,0,0.0,yes,9/27/18,Singapore Management University;Singapore Management University,89;89,1103;1103,
2511,2511,2511,2511,2511,2511,2511,2511,ICLR,2019,Empirical Study of Easy and Hard Examples in CNN Training,Ikki Kishida;Hideki Nakayama,kishida@nlab.ci.i.u-tokyo.ac.jp;nakayama@nlab.ci.i.u-tokyo.ac.jp,3;4;3,4;5;4,Reject,0,0,0.0,yes,9/27/18,The University of Tokyo;The University of Tokyo,54;54,45;45,8
2512,2512,2512,2512,2512,2512,2512,2512,ICLR,2019,Improving machine classification using human uncertainty measurements,Ruairidh M. Battleday;Joshua C. Peterson;Thomas L. Griffiths,ruairidh.battleday@gmail.com;peterson.c.joshua@gmail.com;tomg@princeton.edu,6;3;3,4;5;2,Reject,3,0,0.0,yes,9/27/18,Princeton University;University of California Berkeley;Princeton University,30;5;30,7;18;7,4;8
2513,2513,2513,2513,2513,2513,2513,2513,ICLR,2019,Policy Optimization via Stochastic Recursive Gradient Algorithm,Huizhuo Yuan;Chris Junchi Li;Yuhao Tang;Yuren Zhou,yuanhz@pku.edu.cn;junchi.li.duke@gmail.com;yuhaotang97@gmail.com;yuren.zhou@duke.edu,5;6;5,3;2;3,Reject,0,1,0.0,yes,9/27/18,Peking University;Tencent AI Lab;University of Nottingham;Duke University,24;-1;228;44,27;-1;146;17,
2514,2514,2514,2514,2514,2514,2514,2514,ICLR,2019,Discriminative Active Learning,Daniel Gissin;Shai Shalev-Shwartz,daniel.gissin@mail.huji.ac.il;shais@cs.huji.ac.il,6;8;4,4;4;4,Reject,0,15,0.0,yes,9/27/18,Hebrew University of Jerusalem;Hebrew University of Jerusalem,65;65,205;205,
2515,2515,2515,2515,2515,2515,2515,2515,ICLR,2019,Robustness and Equivariance of Neural Networks,Amit Deshpande;Sandesh Kamath;K.V.Subrahmanyam,amitdesh@microsoft.com;ksandeshk@cmi.ac.in;kv@cmi.ac.in,3;4;5,5;4;3,Reject,0,3,0.0,yes,9/27/18,Microsoft;Chennai Mathematical Institute;Chennai Mathematical Institute,-1;478;478,-1;1103;1103,4
2516,2516,2516,2516,2516,2516,2516,2516,ICLR,2019,Knowledge Distillation from Few Samples,Tianhong Li;Jianguo Li;Zhuang Liu;Changshui Zhang,tianhong@mit.edu;jianguo.li@intel.com;zhuangl@berkeley.edu;zcs@mail.tsinghua.edu.cn,4;6;6,4;4;3,Reject,2,12,0.0,yes,9/27/18,Massachusetts Institute of Technology;Intel;University of California Berkeley;Tsinghua University,2;-1;5;8,5;-1;18;30,1
2517,2517,2517,2517,2517,2517,2517,2517,ICLR,2019,An Automatic Operation Batching Strategy for the Backward Propagation of Neural Networks Having Dynamic Computation Graphs,Yuchen Qiao;Kenjiro Taura,qiao@eidos.ic.i.u-tokyo.ac.jp;tau@eidos.ic.i.u-tokyo.ac.jp,5;6;4,3;5;4,Reject,0,1,0.0,yes,9/27/18,The University of Tokyo;The University of Tokyo,54;54,45;45,10
2518,2518,2518,2518,2518,2518,2518,2518,ICLR,2019,Graph Learning Network: A Structure Learning Algorithm,Darwin Danilo Saire Pilco;Adín Ramírez Rivera,darwin.pilco@ic.unicamp.br;adin@ic.unicamp.br,4;3;4,5;4;4,Reject,0,4,0.0,yes,9/27/18,University of Campinas;University of Campinas,386;386,441;441,10
2519,2519,2519,2519,2519,2519,2519,2519,ICLR,2019,Remember and Forget for Experience Replay,Guido Novati;Petros Koumoutsakos,novatig@ethz.ch;petros@ethz.ch,7;6;6,3;3;3,Reject,0,6,0.0,yes,9/27/18,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10,10;10,
2520,2520,2520,2520,2520,2520,2520,2520,ICLR,2019,Probabilistic Model-Based Dynamic Architecture Search,Nozomu Yoshinari;Kento Uchida;Shota Saito;Shinichi Shirakawa;Youhei Akimoto,yoshinari-nozomu-ry@ynu.jp;uchida-kento-nc@ynu.jp;saito-shota-bt@ynu.jp;shirakawa-shinichi-bg@ynu.ac.jp;akimoto@cs.tsukuba.ac.jp,5;6;5,4;4;4,Reject,0,4,0.0,yes,9/27/18,Yokohama National University;Yokohama National University;Yokohama National University;Yokohama National University;University of Tsukuba,478;478;478;478;478,827;827;827;827;468,2
2521,2521,2521,2521,2521,2521,2521,2521,ICLR,2019,Talk The Walk: Navigating Grids in New York City through Grounded Dialogue,Harm de Vries;Kurt Shuster;Dhruv Batra;Devi Parikh;Jason Weston;Douwe Kiela,mail@harmdevries.com;kshuster@fb.com;dbatra@gatech.edu;parikh@gatech.edu;jase@fb.com;dkiela@fb.com,6;7;4,4;4;3,Reject,2,0,3.0,yes,9/27/18,University of Montreal;Facebook;Georgia Institute of Technology;Georgia Institute of Technology;Facebook;Facebook,123;-1;13;13;-1;-1,108;-1;33;33;-1;-1,3
2522,2522,2522,2522,2522,2522,2522,2522,ICLR,2019,Large-scale classification of structured objects using a CRF with deep class embedding,Eran Goldman;Jacob Goldberger,eg4000@gmail.com;jacob.goldberger@biu.ac.il,4;3;3,4;3;5,Reject,0,0,0.0,yes,9/27/18,;Bar Ilan University,-1;95,-1;456,
2523,2523,2523,2523,2523,2523,2523,2523,ICLR,2019,Deep Ensemble Bayesian Active Learning : Adressing the Mode Collapse issue in Monte Carlo dropout via Ensembles,Remus Pop;Patric Fulop,remus.p.pop@gmail.com;patric.fulop@ed.ac.uk,4;4;5,4;4;4,Reject,0,9,0.0,yes,9/27/18,;University of Edinburgh,-1;33,-1;27,11
2524,2524,2524,2524,2524,2524,2524,2524,ICLR,2019,MixFeat: Mix Feature in Latent Space Learns Discriminative Space,Yoichi Yaguchi;Fumiyuki Shiratani;Hidekazu Iwaki,yoichi_yaguchi@ot.olympus.co.jp;f_shiratani@ot.olympus.co.jp;h_iwaki@ot.olympus.co.jp,6;4;4,4;3;4,Reject,0,10,0.0,yes,9/27/18,Olympus Corporation;Olympus Corporation;Olympus Corporation,-1;-1;-1,-1;-1;-1,
2525,2525,2525,2525,2525,2525,2525,2525,ICLR,2019,Question Generation using a Scratchpad Encoder,Ryan Y Benmalek;Madian Khabsa;Suma Desu;Claire Cardie;Michele Banko,ryanai3@cs.cornell.edu;me@madiankhabsa.com;desuma24@gmail.com;cardie@cs.cornell.edu;mbanko@apple.com,4;3;4,4;5;5,Reject,0,3,0.0,yes,9/27/18,Cornell University;Madiankhabsa;;Cornell University;Apple,7;-1;-1;7;-1,19;-1;-1;19;-1,3
2526,2526,2526,2526,2526,2526,2526,2526,ICLR,2019,Interpreting Adversarial Robustness: A View from Decision Surface in Input Space,Fuxun Yu;Chenchen Liu;Yanzhi Wang;Xiang Chen,fyu2@gmu.edu;chliu@clarkson.edu;yanz.wang@northeastern.edu;xchen26@gmu.com,3;6;5,5;4;5,Reject,1,8,1.0,yes,9/27/18,George Mason University;Clarkson University;Northeastern University;Gmu,99;478;16;99,336;1103;839;336,4;8
2527,2527,2527,2527,2527,2527,2527,2527,ICLR,2019,Sequenced-Replacement Sampling for Deep Learning,Chiu Man Ho;Dae Hoon Park;Wei Yang;Yi Chang,chiuman100@gmail.com;pdhvip@gmail.com;wei.yang2@huawei.com;yichang@acm.org,3;5;4,4;5;4,Reject,0,0,0.0,yes,9/27/18,;Huawei Technologies Ltd.;Huawei Technologies Ltd.;,-1;-1;-1;-1,-1;-1;-1;-1,8
2528,2528,2528,2528,2528,2528,2528,2528,ICLR,2019,Representation-Constrained Autoencoders and an Application to Wireless Positioning,Pengzhi Huang;Emre Gonultas;Said Medjkouh;Oscar Castaneda;Olav Tirkkonen;Tom Goldstein;Christoph Studer,ph448@cornell.edu;eg566@cornell.edu;sm2685@cornell.edu;oc66@cornell.edu;olav.tirkkonen@aalto.fi;tomg@cs.umd.edu;studer@cornell.edu,5;4;6,4;4;2,Reject,0,4,0.0,yes,9/27/18,"Cornell University;Cornell University;Cornell University;Cornell University;Aalto University;University of Maryland, College Park;Cornell University",7;7;7;7;140;12;7,19;19;19;19;190;69;19,
2529,2529,2529,2529,2529,2529,2529,2529,ICLR,2019,Learn From Neighbour: A Curriculum That Train Low Weighted Samples By Imitating,Benyuan Sun;Yizhou Wang,sunbenyuan@pku.edu.cn;yizhou.wang@pku.edu.cn,2;3;4,5;3;4,Reject,0,0,0.0,yes,9/27/18,Peking University;Peking University,24;24,27;27,
2530,2530,2530,2530,2530,2530,2530,2530,ICLR,2019,On the Statistical and Information Theoretical Characteristics of DNN Representations,Daeyoung Choi;Wonjong Rhee;Kyungeun Lee;Changho Shin,choid@snu.ac.kr;wrhee@snu.ac.kr;ruddms0415@snu.ac.kr;chshin@encoredtech.com,5;4;3,3;4;3,Reject,0,8,0.0,yes,9/27/18,Seoul National University;Seoul National University;Seoul National University;Encoredtech,41;41;41;-1,74;74;74;-1,8
2531,2531,2531,2531,2531,2531,2531,2531,ICLR,2019,Trajectory VAE for multi-modal imitation,Xiaoyu Lu;Jan Stuehmer;Katja Hofmann,xiaoyu.lu@stats.ox.ac.uk;t-jastuh@microsoft.com;katja.hofmann@microsoft.com,4;4;4,4;4;4,Reject,0,3,0.0,yes,9/27/18,University of Oxford;Microsoft;Microsoft,50;-1;-1,1;-1;-1,5
2532,2532,2532,2532,2532,2532,2532,2532,ICLR,2019,SIMILE: Introducing Sequential Information towards More Effective Imitation Learning,Yutong Bai;Lingxi Xie,ytongbai@gmail.com;198808xc@gmail.com,6;4;4,3;5;4,Reject,0,3,0.0,yes,9/27/18,;Huawei Technologies Ltd.,-1;-1,-1;-1,
2533,2533,2533,2533,2533,2533,2533,2533,ICLR,2019,A   RECURRENT NEURAL CASCADE-BASED MODEL FOR CONTINUOUS-TIME DIFFUSION PROCESS,Sylvain Lamprier,sylvain.lamprier@lip6.fr,7;4;4,4;4;4,Reject,0,4,0.0,yes,9/27/18,LIP6,-1,-1,10
2534,2534,2534,2534,2534,2534,2534,2534,ICLR,2019,Multi-task Learning with Gradient Communication,Pengfei Liu;Xuanjing Huang,pfliu14@fudan.edu.cn;xjhuang@fudan.edu.cn,5;4;7,4;4;3,Reject,0,3,0.0,yes,9/27/18,Fudan University;Fudan University,78;78,116;116,
2535,2535,2535,2535,2535,2535,2535,2535,ICLR,2019,Improving Composition of Sentence Embeddings through the Lens of Statistical Relational Learning,Damien Sileo;Tim Van de Cruys;Camille Pradel;Philippe Muller,damien.sileo@synapse-fr.com;tim.van-de-cruys@irit.fr;camille.pradel@synapse-fr.com;philippe.muller@irit.fr,5;5;6,3;3;4,Reject,0,4,0.0,yes,9/27/18,"Synapse-fr;IRIT, University of Toulouse;Synapse-fr;IRIT, University of Toulouse",-1;-1;-1;-1,-1;-1;-1;-1,3
2536,2536,2536,2536,2536,2536,2536,2536,ICLR,2019,A Variational Autoencoder for Probabilistic Non-Negative Matrix Factorisation,Steven Squires;Adam Prugel-Bennett;Mahesan Niranjan,ses2g14@ecs.soton.ac.uk;apb@ecs.soton.ac.uk;mn@ecs.soton.ac.uk,4;4;7,5;3;5,Reject,0,5,0.0,yes,9/27/18,University of Southampton;University of Southampton;University of Southampton,169;169;169,126;126;126,5
2537,2537,2537,2537,2537,2537,2537,2537,ICLR,2019,Inferring Reward Functions from Demonstrators with Unknown Biases,Rohin Shah;Noah Gundotra;Pieter Abbeel;Anca Dragan,rohinmshah@berkeley.edu;noah.gundotra@berkeley.edu;pabbeel@cs.berkeley.edu;anca@berkeley.edu,5;5;5,4;3;4,Reject,0,0,0.0,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5,18;18;18;18,
2538,2538,2538,2538,2538,2538,2538,2538,ICLR,2019,Low Latency Privacy Preserving Inference,Alon Brutzkus;Oren Elisha;Ran Gilad-Bachrach,brutzkus@gmail.com;oren.elisha@microsoft.com;rani.gb@gmail.com,5;6;5,3;2;4,Reject,5,4,0.0,yes,9/27/18,Tel Aviv University;Microsoft;Microsoft,37;-1;-1,217;-1;-1,
2539,2539,2539,2539,2539,2539,2539,2539,ICLR,2019,Ain't Nobody Got Time for Coding: Structure-Aware Program Synthesis from Natural Language,Jakub Bednarek;Karol Piaskowski;Krzysztof Krawiec,jakub.bednarek@put.poznan.pl;kar.piaskowski@gmail.com;krawiec@cs.put.poznan.pl,4;4;4,4;3;5,Reject,0,9,0.0,yes,9/27/18,Poznan University of Technology;Poznan University of Technology;Poznan University of Technology,478;478;478,1103;1103;1103,3
2540,2540,2540,2540,2540,2540,2540,2540,ICLR,2019,Likelihood-based Permutation Invariant Loss Function for Probability Distributions,Masataro Asai,masataro.asai@ibm.com,5;6;4,4;3;4,Reject,0,11,0.0,yes,9/27/18,International Business Machines,-1,-1,
2541,2541,2541,2541,2541,2541,2541,2541,ICLR,2019,Learning to Progressively Plan,Xinyun Chen;Yuandong Tian,xinyun.chen@berkeley.edu;yuandong@fb.com,5;5;5,3;3;3,Reject,0,6,0.0,yes,9/27/18,University of California Berkeley;Facebook,5;-1,18;-1,
2542,2542,2542,2542,2542,2542,2542,2542,ICLR,2019,"CNNSAT: Fast, Accurate Boolean Satisfiability using Convolutional Neural Networks",Yu Wang;Fengjuan Gao;Amin Alipour;Linzhang Wang;Xuandong Li;Zhendong Su,yuwang@seg.nju.edu.cn;fjgao@seg.nju.edu.cn;alipour@cs.uh.edu;lzwang@nju.edu.cn;lxd@nju.edu.cn;zhendong.su@inf.ethz.ch,5;6;5,4;2;4,Reject,1,20,0.0,yes,9/27/18,Zhejiang University;Zhejiang University;University of Houston;Zhejiang University;Zhejiang University;Swiss Federal Institute of Technology,57;57;169;57;57;10,177;177;330;177;177;10,
2543,2543,2543,2543,2543,2543,2543,2543,ICLR,2019,Transferring SLU Models in Novel Domains,Yaohua Tang;Kaixiang Mo;Qian Xu;Chao Zhang;Qiang Yang,yaohuatang@webank.com;kxmo@connect.ust.hk;fleurxq@outlook.com;carlzzhang@webank.com;qyang@cse.ust.hk,6;5;4,4;3;3,Reject,0,0,5.0,yes,9/27/18,webank;The Hong Kong University of Science and Technology;;webank;The Hong Kong University of Science and Technology,-1;39;-1;-1;39,-1;44;-1;-1;44,3;6
2544,2544,2544,2544,2544,2544,2544,2544,ICLR,2019,Fake Sentence Detection as a Training Task for Sentence Encoding,Viresh Ranjan;Heeyoung Kwon;Niranjan Balasubramanian;Minh Hoai,vranjan@cs.stonybrook.edu;heekwon@cs.stonybrook.edu;niranjan@cs.stonybrook.edu;minhhoai@cs.stonybrook.edu,5;3;3,3;4;5,Reject,0,0,0.0,yes,9/27/18,"State University of New York, Stony Brook;State University of New York, Stony Brook;State University of New York, Stony Brook;State University of New York, Stony Brook",41;41;41;41,258;258;258;258,3;5
2545,2545,2545,2545,2545,2545,2545,2545,ICLR,2019,Q-map: a Convolutional Approach for Goal-Oriented Reinforcement Learning,Fabio Pardo;Vitaly Levdik;Petar Kormushev,f.pardo@imperial.ac.uk;v.levdik@imperial.ac.uk;p.kormushev@imperial.ac.uk,4;5;4,3;4;5,Reject,0,7,1.0,yes,9/27/18,Imperial College London;Imperial College London;Imperial College London,72;72;72,8;8;8,8
2546,2546,2546,2546,2546,2546,2546,2546,ICLR,2019,Inter-BMV: Interpolation with Block Motion Vectors for Fast Semantic Segmentation on Video,Samvit Jain;Joseph Gonzalez,samvit@eecs.berkeley.edu;jegonzal@cs.berkeley.edu,5;3;5,4;5;4,Reject,0,4,0.0,yes,9/27/18,University of California Berkeley;University of California Berkeley,5;5,18;18,2
2547,2547,2547,2547,2547,2547,2547,2547,ICLR,2019,3D-RelNet: Joint Object and Relational Network for 3D Prediction,Nilesh Kulkarni;Ishan Misra;Shubham Tulsiani;Abhinav Gupta,nileshk@cs.cmu.edu;ishan@cmu.edu;shubhtuls@fb.com;abhinavg@cs.cmu.edu,6;5;3,4;5;5,Reject,0,6,0.0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;Facebook;Carnegie Mellon University,1;1;-1;1,24;24;-1;24,
2548,2548,2548,2548,2548,2548,2548,2548,ICLR,2019,Image Score: how to select useful samples,Simiao Zuo;Jialin Wu,zsmx1996@utexas.edu;jialinwu@utexas.edu,4;4;3,3;4;3,Reject,0,0,0.0,yes,9/27/18,"University of Texas, Austin;University of Texas, Austin",22;22,49;49,
2549,2549,2549,2549,2549,2549,2549,2549,ICLR,2019,Learning Neuron Non-Linearities with Kernel-Based Deep Neural Networks,Giuseppe Marra;Dario Zanca;Alessandro Betti;Marco Gori,g.marra@unifi.it;dario.zanca@unifi.it;alessandro.betti@unifi.it;marco.gori@unisi.it,5;4;6,3;3;4,Reject,0,0,0.0,yes,9/27/18,University of Florence;University of Florence;University of Florence;University of Siena,478;478;478;169,489;489;489;161,1
2550,2550,2550,2550,2550,2550,2550,2550,ICLR,2019,DelibGAN: Coarse-to-Fine Text Generation via Adversarial Network,Ke Wang;Xiaojun Wan,wangke17@pku.edu.cn;wanxiaojun@pku.edu.cn,4;3;4,4;4;4,Reject,0,0,0.0,yes,9/27/18,Peking University;Peking University,24;24,27;27,4
2551,2551,2551,2551,2551,2551,2551,2551,ICLR,2019,High Resolution and Fast Face Completion via Progressively Attentive GANs,Zeyuan Chen;Shaoliang Nie;Tianfu Wu;Christopher G. Healey,zchen23@ncsu.edu;snie@ncsu.edu;tianfu_wu@ncsu.edu;healey@ncsu.edu,5;5;5,5;2;5,Reject,0,4,0.0,yes,9/27/18,North Carolina State University;North Carolina State University;North Carolina State University;North Carolina State University,89;89;89;89,275;275;275;275,5;4
2552,2552,2552,2552,2552,2552,2552,2552,ICLR,2019,Unsupervised Disentangling Structure and Appearance,Wayne Wu;Kaidi Cao;Cheng Li;Chen Qian;Chen Change Loy,wuwenyan@sensetime.com;kaidicao@cs.stanford.edu;chengli@sensetime.com;qianchen@sensetime.com;ccloy225@gmail.com,6;5;3,4;4;4,Reject,0,0,0.0,yes,9/27/18,SenseTime Group Limited;Stanford University;SenseTime Group Limited;SenseTime Group Limited;the Chinese University of Hong Kong,-1;4;-1;-1;57,-1;3;-1;-1;58,5
2553,2553,2553,2553,2553,2553,2553,2553,ICLR,2019,Language Model Pre-training for Hierarchical Document Representations,Ming-Wei Chang;Kristina Toutanova;Kenton Lee;Jacob Devlin,mingweichang@google.com;kristout@google.com;kentonl@google.com;jacobdevlin@google.com,6;6;6,4;4;4,Reject,0,6,0.0,yes,9/27/18,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,3;2
2554,2554,2554,2554,2554,2554,2554,2554,ICLR,2019,Human Action Recognition Based on Spatial-Temporal Attention,Wensong Chan;Zhiqiang Tian;Xuguang Lan,2489925838@qq.com;zhiqiangtian@xjtu.edu.cn;xglan@xjtu.edu.cn,4;3;3,4;5;4,Reject,0,0,0.0,yes,9/27/18,Xi'an Jiaotong University;Xi'an Jiaotong University;Xi'an Jiaotong University,478;478;478,565;565;565,
2555,2555,2555,2555,2555,2555,2555,2555,ICLR,2019,Adversarial Attacks for Optical Flow-Based Action Recognition Classifiers,Nathan Inkawhich;Matthew Inkawhich;Hai Li;Yiran Chen,nathan.inkawhich@duke.edu;matthew.inkawhich@duke.edu;hai.li@duke.edu;yiran.chen@duke.edu,4;3;5,5;4;4,Reject,0,4,0.0,yes,9/27/18,Duke University;Duke University;Duke University;Duke University,44;44;44;44,17;17;17;17,4
2556,2556,2556,2556,2556,2556,2556,2556,ICLR,2019,Computing committor functions for the study of rare events using deep learning with importance sampling,Qianxiao Li;Bo Lin;Weiqing Ren,liqix@ihpc.a-star.edu.sg;linbo94@u.nus.edu;matrw@nus.edu.sg,6;6;5;7,4;4;4;4,Reject,0,6,0.0,yes,9/27/18,"Institute of High Performance Computing, Singapore, A*STAR;National University of Singapore;National University of Singapore",-1;16;16,-1;22;22,
2557,2557,2557,2557,2557,2557,2557,2557,ICLR,2019,(Unconstrained) Beam Search is Sensitive to Large Search Discrepancies,Eldan Cohen;J. Christopher Beck,ecohen@mie.utoronto.ca;jcb@mie.utoronto.ca,5;5;7,4;5;5,Reject,0,10,0.0,yes,9/27/18,Toronto University;Toronto University,18;18,22;22,
2558,2558,2558,2558,2558,2558,2558,2558,ICLR,2019,Multi-turn Dialogue Response Generation in an Adversarial Learning Framework,Oluwatobi O. Olabiyi;Alan Salimov;Anish Khazane;Erik T. Mueller,oluwatobi.olabiyi@capitalone.com;alan.salimov@capitalone.com;anish.khazan@capitalone.com;erik.mueller@capitalone.com,4;4;6;5,4;4;4;5,Reject,0,5,0.0,yes,9/27/18,Capital One Bank;Capital One Bank;Capital One Bank;Capital One Bank,-1;-1;-1;-1,-1;-1;-1;-1,5;4
2559,2559,2559,2559,2559,2559,2559,2559,ICLR,2019,Zero-Resource Multilingual Model Transfer: Learning What to Share,Xilun Chen;Ahmed Hassan Awadallah;Hany Hassan;Wei Wang;Claire Cardie,xlchen@cs.cornell.edu;hassanam@microsoft.com;hanyh@microsoft.com;wei.wang@microsoft.com;cardie@cs.cornell.edu,6;5;6,4;5;4,Reject,0,6,0.0,yes,9/27/18,Cornell University;Microsoft;Microsoft;Microsoft;Cornell University,7;-1;-1;-1;7,19;-1;-1;-1;19,3;4;6
2560,2560,2560,2560,2560,2560,2560,2560,ICLR,2019,Inducing Cooperation via Learning to reshape rewards in semi-cooperative multi-agent reinforcement learning,David Earl Hostallero;Daewoo Kim;Kyunghwan Son;Yung Yi,ddhostallero@kaist.ac.kr;kdw2139@gmail.com;khson@lanada.kaist.ac.kr;yiyung@kaist.edu,5;5;5,3;4;4,Reject,0,4,0.0,yes,9/27/18,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;KAIST,20;20;20;20,95;95;95;95,
2561,2561,2561,2561,2561,2561,2561,2561,ICLR,2019,VECTORIZATION METHODS IN RECOMMENDER SYSTEM,Qiang Sun;Bin Wang;Zizhou Gu;Yanwei Fu,sunqiang85@gmail.com;vborisw@gmail.com;2470569@qq.com;yanweifu@fudan.edu.cn,2;2;3,5;5;4,Reject,0,0,0.0,yes,9/27/18,Fudan University;;;Fudan University,78;-1;-1;78,116;-1;-1;116,3
2562,2562,2562,2562,2562,2562,2562,2562,ICLR,2019,The Forward-Backward Embedding of Directed Graphs,Thomas Bonald;Nathan De Lara,thomas.bonald@telecom-paristech.fr;nathan.delara@telecom-paristech.fr,5;3;4,5;5;4,Reject,2,4,0.0,yes,9/27/18,Télécom ParisTech;Télécom ParisTech,478;478,188;188,1;10
2563,2563,2563,2563,2563,2563,2563,2563,ICLR,2019,Provable Guarantees on Learning Hierarchical Generative Models with Deep CNNs,Eran Malach;Shai Shalev-Shwartz,eran.malach@mail.huji.ac.il;shais@cs.huji.ac.il,6;4;6,3;4;3,Reject,0,3,0.0,yes,9/27/18,Hebrew University of Jerusalem;Hebrew University of Jerusalem,65;65,205;205,5;9
2564,2564,2564,2564,2564,2564,2564,2564,ICLR,2019,Contextualized Role Interaction for Neural Machine Translation,Dirk Weissenborn;Douwe Kiela;Jason Weston;Kyunghyun Cho,dirk.weissenborn@gmail.com;dkiela@fb.com;jase@fb.com;kyunghyun.cho@nyu.edu,4;5;4,5;4;4,Reject,0,5,0.0,yes,9/27/18,German Research Center for AI;Facebook;Facebook;New York University,-1;-1;-1;26,-1;-1;-1;27,3
2565,2565,2565,2565,2565,2565,2565,2565,ICLR,2019,Dual Skew Divergence Loss for Neural Machine Translation,Yingting Wu;Hai Zhao;Rui Wang,wuyingting@sjtu.edu.cn;zhaohai@cs.sjtu.edu.cn;wangrui.nlp@gmail.com,3;6;5,4;4;4,Reject,0,0,0.0,yes,9/27/18,Shanghai Jiao Tong University;Shanghai Jiao Tong University;,52;52;-1,188;188;-1,3;8
2566,2566,2566,2566,2566,2566,2566,2566,ICLR,2019,On Accurate Evaluation of GANs for Language Generation,Stanislau Semeniuta;Aliaksei Severyn;Sylvain Gelly,stas@inb.uni-luebeck.de;severyn@google.com;sylvaingelly@google.com,6;5;3,4;4;4,Reject,0,4,0.0,yes,9/27/18,University of Luebeck;Google;Google,261;-1;-1,1103;-1;-1,3;4;5
2567,2567,2567,2567,2567,2567,2567,2567,ICLR,2019,Zero-shot Dual Machine Translation,Lierni Sestorain;Massimiliano Ciaramita;Christian Buck;Thomas Hofmann,lierni@google.com;massi@google.com;cbuck@google.com;thomas.hofmann@inf.ethz.ch,5;4;6,4;5;3,Reject,0,6,0.0,yes,9/27/18,Google;Google;Google;Swiss Federal Institute of Technology,-1;-1;-1;10,-1;-1;-1;10,3;6
2568,2568,2568,2568,2568,2568,2568,2568,ICLR,2019,Towards the Latent Transcriptome,Assya Trofimov;Francis Dutil;Claude Perreault;Sebastien Lemieux;Yoshua Bengio;Joseph Paul Cohen,trofimov.assya@gmail.com;frdutil@gmail.com;claude.perreault@umontreal.ca;s.lemieux@umontreal.ca;yoshua.bengio@mila.quebec;joseph@josephpcohen.com,4;2;5,4;5;4,Reject,0,4,1.0,yes,9/27/18,University of Montreal;;University of Montreal;University of Montreal;University of Montreal;University of Montreal,123;-1;123;123;123;123,108;-1;108;108;108;108,
2569,2569,2569,2569,2569,2569,2569,2569,ICLR,2019,On the Relationship between Neural Machine Translation and Word Alignment,Xintong Li;Lemao Liu;Guanlin Li;Max Meng;Shuming Shi,znculee@gmail.com;redmondliu@tencent.com;epsilonlee.green@gmail.com;max.meng@ieee.org;shumingshi@tencent.com,4;5;6,4;4;4,Reject,0,9,0.0,yes,9/27/18,The Chinese University of Hong Kong;Tencent AI Lab;Harbin Institute of Technology;;Tencent AI Lab,57;-1;199;-1;-1,40;-1;522;-1;-1,3
2570,2570,2570,2570,2570,2570,2570,2570,ICLR,2019,"FROM DEEP LEARNING TO DEEP DEDUCING: AUTOMATICALLY TRACKING DOWN NASH EQUILIBRIUM THROUGH AUTONOMOUS NEURAL AGENT, A POSSIBLE MISSING STEP TOWARD GENERAL A.I.",Brown Wang,brownwang0426@gmail.com,3;2;4,3;4;5,Reject,0,1,0.0,yes,9/27/18,National Taiwan University,85,197,
2571,2571,2571,2571,2571,2571,2571,2571,ICLR,2019,Neural Predictive Belief Representations,Zhaohan Daniel Guo;Mohammad Gheshlaghi Azar;Bilal Piot;Bernardo Avila Pires;Rémi Munos,z.daniel.guo@gmail.com;mazar@google.com;piot@google.com;bavilapires@google.com;munos@google.com,4;7;5,3;4;3,Reject,0,4,0.0,yes,9/27/18,Carnegie Mellon University;Google;Google;Google;Google,1;-1;-1;-1;-1,24;-1;-1;-1;-1,
2572,2572,2572,2572,2572,2572,2572,2572,ICLR,2019,Accidental exploration through value predictors,Tomasz Kisielewski;Damian Leśniak;Maia Pasek,tymorl@gmail.com;damian.lesniak@doctoral.uj.edu.pl;maiapasek@gmail.com,3;5;4,4;4;4,Reject,0,6,0.0,yes,9/27/18,Jagiellonian University;Jagiellonian University;,478;478;-1,695;695;-1,
2573,2573,2573,2573,2573,2573,2573,2573,ICLR,2019,Neural Model-Based Reinforcement Learning for Recommendation,Xinshi Chen;Shuang Li;Hui Li;Shaohua Jiang;Le Song,xinshi.chen@gatech.edu;sli370@gatech.edu;ken.lh@alibaba-inc.com;shaohua.jsh@alipay.com;lsong@cc.gatech.edu,5;6;5,4;3;5,Reject,0,5,0.0,yes,9/27/18,Georgia Institute of Technology;Georgia Institute of Technology;Alibaba Group;Alipay;Georgia Institute of Technology,13;13;-1;-1;13,33;33;-1;-1;33,5;4
2574,2574,2574,2574,2574,2574,2574,2574,ICLR,2019,PPO-CMA: Proximal Policy Optimization with Covariance Matrix Adaptation,Perttu Hämäläinen;Amin Babadi;Xiaoxiao Ma;Jaakko Lehtinen,perttu.hamalainen@aalto.fi;amin.babadi@aalto.fi;xiaoxiao.ma@aalto.fi;jaakko.lehtinen@aalto.fi,4;9;4,4;3;2,Reject,0,6,0.0,yes,9/27/18,Aalto University;Aalto University;Aalto University;Aalto University,140;140;140;140,190;190;190;190,
2575,2575,2575,2575,2575,2575,2575,2575,ICLR,2019,Unsupervised Meta-Learning for Reinforcement Learning,Abhishek Gupta;Benjamin Eysenbach;Chelsea Finn;Sergey Levine,abhigupta@berkeley.edu;eysenbachbe@gmail.com;cbfinn@eecs.berkeley.edu;svlevine@eecs.berkeley.edu,3;6;4,4;3;2,Reject,0,4,0.0,yes,9/25/19,University of California Berkeley;Carnegie Mellon University;University of California Berkeley;University of California Berkeley,5;1;5;5,18;24;18;18,6
2576,2576,2576,2576,2576,2576,2576,2576,ICLR,2019,Set Transformer,Juho Lee;Yoonho Lee;Jungtaek Kim;Adam R. Kosiorek;Seungjin Choi;Yee Whye Teh,juho.lee@stats.ox.ac.uk;einet89@gmail.com;jtkim@postech.ac.kr;adamk@robots.ox.ac.uk;seungjin@postech.ac.kr;y.w.teh@stats.ox.ac.uk,6;5;6,5;4;3,Reject,0,5,0.0,yes,9/27/18,University of Oxford;POSTECH;POSTECH;University of Oxford;POSTECH;University of Oxford,50;123;123;50;123;50,1;137;137;1;137;1,6
2577,2577,2577,2577,2577,2577,2577,2577,ICLR,2019,Deep Neuroevolution: Genetic Algorithms are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning,Felipe Petroski Such;Vashisht Madhavan;Edoardo Conti;Joel Lehman;Kenneth O. Stanley;Jeff Clune,felipe.such@uber.com;vashisht@uber.com;edoardo@uber.com;joel.lehman@uber.com;kstanley@uber.com;jeffclune@uber.com,6;7;6;4;3,4;5;2;4;4,Reject,2,10,0.0,yes,9/27/18,Uber;Uber;Uber;Uber;Uber;Uber,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,
2578,2578,2578,2578,2578,2578,2578,2578,ICLR,2019,A Rotation and a Translation Suffice: Fooling CNNs with Simple Transformations,Logan Engstrom;Brandon Tran;Dimitris Tsipras;Ludwig Schmidt;Aleksander Madry,engstrom@mit.edu;btran115@mit.edu;tsipras@mit.edu;ludwigs@mit.edu;madry@mit.edu,8;6;5,3;2;4,Reject,0,6,0.0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2;2,5;5;5;5;5,4
2579,2579,2579,2579,2579,2579,2579,2579,ICLR,2019,Deep Recurrent Gaussian Process with Variational Sparse Spectrum Approximation,Roman Föll;Bernard Haasdonk;Markus Hanselmann;Holger Ulmer,foell@mathematik.uni-stuttgart.de;haasdonk@mathematik.uni-stuttgart.de;markus.hanselmann@etas.com;holger.ulmer@etas.com,7;5;5,2;4;4,Reject,0,5,0.0,yes,9/27/18,University of Stuttgart;University of Stuttgart;ETAS GmbH;ETAS GmbH,95;95;-1;-1,219;219;-1;-1,
2580,2580,2580,2580,2580,2580,2580,2580,ICLR,2019,Analysis of Memory Organization for Dynamic Neural Networks,Ying Ma;Jose Principe,mayingbit2011@gmail.com;principe@cnel.ufl.edu,7;5;3,3;5;5,Reject,0,14,0.0,yes,9/27/18,University of Florida;University of Florida,123;123,143;143,3
2581,2581,2581,2581,2581,2581,2581,2581,ICLR,2019,Generative Ensembles for Robust Anomaly Detection,Hyunsun Choi;Eric Jang,hyunsunchoi@kaist.ac.kr;ejang@google.com,5;4;6,4;5;3,Reject,0,8,0.0,yes,9/27/18,Korea Advanced Institute of Science and Technology;Google,20;-1,95;-1,3;5
2582,2582,2582,2582,2582,2582,2582,2582,ICLR,2019,SHAMANN: Shared Memory Augmented Neural Networks,Cosmin I. Bercea;Olivier Pauly;Andreas K. Maier;Florin C. Ghesu,cosmin.bercea@fau.de;olivier.pauly@gmail.com;andreas.maier@fau.de;florin.ghesu@siemens-healthineers.com,4;5;4,5;3;5,Reject,0,0,0.0,yes,9/27/18,University of Erlangen-Nuremberg;;University of Erlangen-Nuremberg;Siemens Healthineers,199;-1;199;-1,162;-1;162;-1,2
2583,2583,2583,2583,2583,2583,2583,2583,ICLR,2019,Novel positional encodings to enable tree-structured transformers,Vighnesh Leonardo Shiv;Chris Quirk,vishiv@microsoft.com;chrisq@microsoft.com,4;6;5,3;3;3,Reject,0,7,0.0,yes,9/27/18,Microsoft;Microsoft,-1;-1,-1;-1,
2584,2584,2584,2584,2584,2584,2584,2584,ICLR,2019,Learning Joint Wasserstein Auto-Encoders for Joint Distribution Matching,Jiezhang Cao;Yong Guo;Langyuan Mo;Peilin Zhao;Junzhou Huang;Mingkui Tan,secaojiezhang@mail.scut.edu.cn;guoyongcs@gmail.com;selangyuanmo@mail.scut.edu.cn;peilinzhao@hotmail.com;jzhuang@uta.edu;mingkuitan@scut.edu.cn,6;4;5,4;4;4,Reject,0,4,0.0,yes,9/27/18,"South China University of Technology;South China University of Technology;South China University of Technology;;University of Texas, Arlington;South China University of Technology",478;478;478;-1;115;478,576;576;576;-1;601;576,1;8
2585,2585,2585,2585,2585,2585,2585,2585,ICLR,2019,Meta Learning with Fast/Slow Learners,zhuoyuan@fb.com,chengzhuoyuan07@gmail.com,5;5;6,3;4;3,Reject,0,0,0.0,yes,9/27/18,,,,6
2586,2586,2586,2586,2586,2586,2586,2586,ICLR,2019,Domain Generalization via Invariant Representation under Domain-Class Dependency,Kei Akuzawa;Yusuke Iwasawa;Yutaka Matsuo,akuzawa-kei@weblab.t.u-tokyo.ac.jp;iwasawa@weblab.t.u-tokyo.ac.jp;matsuo@weblab.t.u-tokyo.ac.jp,4;7;5,5;5;4,Reject,0,8,0.0,yes,9/27/18,The University of Tokyo;The University of Tokyo;The University of Tokyo,54;54;54,45;45;45,8
2587,2587,2587,2587,2587,2587,2587,2587,ICLR,2019,Hierarchical Attention: What Really Counts in Various NLP Tasks,Zehao Dou;Zhihua Zhang,zehaodou@pku.edu.cn;zhzhang@math.pku.edu.cn,4;3;4,4;5;3,Reject,1,0,0.0,yes,9/27/18,Peking University;Peking University,24;24,27;27,3;1;8
2588,2588,2588,2588,2588,2588,2588,2588,ICLR,2019,Learning with Reflective Likelihoods,Adji B. Dieng;Kyunghyun Cho;David M. Blei;Yann LeCun,abd2141@columbia.edu;kyunghyun.cho@nyu.edu;david.blei@columbia.edu;yann@fb.com,4;2;3,4;4;4,Reject,2,13,0.0,yes,9/27/18,Columbia University;New York University;Columbia University;Facebook,15;26;15;-1,14;27;14;-1,5
2589,2589,2589,2589,2589,2589,2589,2589,ICLR,2019,Adapting Auxiliary Losses Using Gradient Similarity,Yunshu Du;Wojciech M. Czarnecki;Siddhant M. Jayakumar;Razvan Pascanu;Balaji Lakshminarayanan,yunshu.du@wsu.edu;lejlot@google.com;sidmj@google.com;razp@google.com;balajiln@google.com,4;6;6,5;4;3,Reject,0,11,0.0,yes,9/27/18,SUN YAT-SEN UNIVERSITY;Google;Google;Google;Google,478;-1;-1;-1;-1,352;-1;-1;-1;-1,
2590,2590,2590,2590,2590,2590,2590,2590,ICLR,2019,Model Comparison for Semantic Grouping,Francisco Vargas;Kamen Brestnichki;Nils Hammerla,francisco.vargas@babylonhealth.com;kamen.brestnichki@babylonhealth.com;nils.hammerla@babylonhealth.com,5;5;5,3;3;1,Reject,0,8,0.0,yes,9/27/18,babylon health;babylon health;babylon health,-1;-1;-1,-1;-1;-1,5;11
2591,2591,2591,2591,2591,2591,2591,2591,ICLR,2019,Connecting the Dots Between MLE and RL for Sequence Generation,Bowen Tan*;Zhiting Hu*;Zichao Yang;Ruslan Salakhutdinov;Eric P. Xing,tanbowen@sjtu.edu.cn;zhitinghu@gmail.com;yangtze2301@gmail.com;rsalakhu@cs.cmu.edu;epxing@cs.cmu.edu,5;6;5,5;3;4,Reject,0,5,0.0,yes,9/27/18,Shanghai Jiao Tong University;Carnegie Mellon University;;Carnegie Mellon University;Carnegie Mellon University,52;1;-1;1;1,188;24;-1;24;24,3
2592,2592,2592,2592,2592,2592,2592,2592,ICLR,2019,DeepTwist: Learning Model Compression via Occasional Weight Distortion,Dongsoo Lee;Parichay Kapoor;Byeongwook Kim,dslee3@gmail.com;kparichay@gmail.com;quddnr145@gmail.com,4;5;4,4;3;3,Reject,0,17,0.0,yes,9/27/18,Samsung;;Samsung,-1;-1;-1,-1;-1;-1,
2593,2593,2593,2593,2593,2593,2593,2593,ICLR,2019,Excitation Dropout: Encouraging Plasticity in Deep Neural Networks,Andrea Zunino;Sarah Adel Bargal;Pietro Morerio;Jianming Zhang;Stan Sclaroff;Vittorio Murino,andrea.zunino@iit.it;sbargal@bu.edu;pietro.morerio@iit.it;jianmzha@adobe.com;sclaroff@bu.edu;vittorio.murino@iit.it,5;5;5,4;3;4,Reject,0,9,0.0,yes,9/27/18,Istituto Italiano di Tecnologia;Boston University;Istituto Italiano di Tecnologia;Adobe Systems;Boston University;Istituto Italiano di Tecnologia,478;65;478;-1;65;478,1103;70;1103;-1;70;1103,8
2594,2594,2594,2594,2594,2594,2594,2594,ICLR,2019,Measuring Density and Similarity of Task Relevant Information in Neural Representations,Danish Pruthi;Mansi Gupta;Nitish Kumar Kulkarni;Graham Neubig;Eduard Hovy,ddanish@cs.cmu.edu;mansig1@cs.cmu.edu;nitishkk@andrew.cmu.edu;gneubig@cs.cmu.edu;hovy@cmu.edu,4;5;5,4;4;3,Reject,0,7,0.0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1;1,24;24;24;24;24,3;6
2595,2595,2595,2595,2595,2595,2595,2595,ICLR,2019,ATTENTIVE EXPLAINABILITY FOR PATIENT TEMPORAL EMBEDDING,Daby Sow;Mohamed Ghalwash;Zach Shahn;Sanjoy Dey;Moulay Draidia;Li-wei Lehmann,sowdaby@us.ibm.com;mohamed.ghalwash@ibm.com;zach.shahn@ibm.com;deysa@us.ibm.com;mzdraidia@berkeley.edu;lilehman@mit.edu,4;3;2,4;4;3,Reject,0,3,1.0,yes,9/27/18,International Business Machines;International Business Machines;International Business Machines;International Business Machines;University of California Berkeley;Massachusetts Institute of Technology,-1;-1;-1;-1;5;2,-1;-1;-1;-1;18;5,
2596,2596,2596,2596,2596,2596,2596,2596,ICLR,2019,Feature prioritization and regularization improve standard accuracy and adversarial robustness,Chihuang Liu;Joseph JaJa,chliu@umd.edu;joseph@umiacs.umd.edu,5;5;4,5;2;3,Reject,0,4,0.0,yes,9/27/18,"University of Maryland, College Park;University of Maryland, College Park",12;12,69;69,4
2597,2597,2597,2597,2597,2597,2597,2597,ICLR,2019,A Study of Robustness of Neural Nets Using Approximate Feature Collisions,Ke Li*;Tianhao Zhang*;Jitendra Malik,ke.li@eecs.berkeley.edu;bryanzhang@berkeley.edu;malik@eecs.berkeley.edu,6;4;4,3;4;4,Reject,0,4,0.0,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,18;18;18,4
2598,2598,2598,2598,2598,2598,2598,2598,ICLR,2019,DEEP HIERARCHICAL MODEL FOR HIERARCHICAL SELECTIVE CLASSIFICATION AND ZERO SHOT LEARNING,Eliyahu Sason;Koby Crammer,sasonil@gmail.com;koby@ee.technion.ac.il,4;5;2,4;3;4,Reject,0,5,0.0,yes,9/27/18,Technion;Technion,25;25,327;327,6;8
2599,2599,2599,2599,2599,2599,2599,2599,ICLR,2019,Advocacy Learning,Ian Fox;Jenna Wiens,ifox@umich.edu;wiensj@umich.edu,4;4;8,4;4;2,Reject,0,4,0.0,yes,9/27/18,University of Michigan;University of Michigan,8;8,21;21,
2600,2600,2600,2600,2600,2600,2600,2600,ICLR,2019,Learning with Random Learning Rates.,Léonard Blier;Pierre Wolinski;Yann Ollivier,leonardb@fb.com;pierre.wolinski@u-psud.fr;yol@fb.com,4;6;5,4;4;4,Reject,0,5,0.0,yes,9/27/18,Facebook;UPSud/INRIA University Paris-Saclay;Facebook,-1;478;-1,-1;1103;-1,
2601,2601,2601,2601,2601,2601,2601,2601,ICLR,2019,BLISS in Non-Isometric Embedding Spaces,Barun Patra;Joel Ruben Antony Moniz;Sarthak Garg;Matthew R Gormley;Graham Neubig,bpatra@andrew.cmu.edu;jrmoniz@andrew.cmu.edu;sarthakg@andrew.cmu.edu;mgormley@andrew.cmu.edu;gneubig@andrew.cmu.edu,4;6;6,5;5;4,Reject,0,6,0.0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1;1,24;24;24;24;24,3
2602,2602,2602,2602,2602,2602,2602,2602,ICLR,2019,Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels,Bo Han;Gang Niu;Jiangchao Yao;Xingrui Yu;Miao Xu;Ivor Tsang;Masashi Sugiyama,bo.han@riken.jp;gang.niu@riken.jp;jiangchao.yao@student.uts.edu.au;xingrui.yu@student.uts.edu.au;miao.xu@riken.jp;ivor.tsang@uts.edu.au;sugi@k.u-tokyo.ac.jp,6;5;3,3;5;4,Reject,12,11,0.0,yes,9/27/18,RIKEN;RIKEN;University of Technology Sydney;University of Technology Sydney;RIKEN;University of Technology Sydney;The University of Tokyo,-1;-1;106;106;-1;106;54,-1;-1;216;216;-1;216;45,8
2603,2603,2603,2603,2603,2603,2603,2603,ICLR,2019,HAPPIER: Hierarchical Polyphonic Music Generative RNN,Tianyang Zhao;Xiaoxuan Ma;Honglin Ma;Yizhou Wang,zhaotianyang@pku.edu.cn;maxiaoxuan@pku.edu.cn;mahonglin_pku@outlook.com;yizhou.wang@pku.edu.cn,2;3;3,4;4;5,Reject,1,0,0.0,yes,9/27/18,Peking University;Peking University;;Peking University,24;24;-1;24,27;27;-1;27,5
2604,2604,2604,2604,2604,2604,2604,2604,ICLR,2019,A Modern Take on the Bias-Variance Tradeoff in Neural Networks,Brady Neal;Sarthak Mittal;Aristide Baratin;Vinayak Tantia;Matthew Scicluna;Simon Lacoste-Julien;Ioannis Mitliagkas,bradyneal11@gmail.com;sarthmit@gmail.com;aristidebaratin@hotmail.com;tantia.vinayak1@gmail.com;mattcscicluna@gmail.com;slacoste@iro.umontreal.ca;ioannis@iro.umontreal.ca,5;7;4,3;4;4,Reject,0,11,0.0,yes,9/27/18,University of Montreal;IIT Kanpur;University of Montreal;;;University of Montreal;University of Montreal,123;123;123;-1;-1;123;123,108;578;108;-1;-1;108;108,8
2605,2605,2605,2605,2605,2605,2605,2605,ICLR,2019,Learning a Neural-network-based Representation for Open Set Recognition,Mehadi Hassen;Philip K. Chan,mhassen2005@my.fit.edu;pkc@cs.fit.edu,4;4;5,4;4;4,Reject,0,5,0.0,yes,9/27/18,Florida Institute of Technology;Florida Institute of Technology,478;478,750;750,
2606,2606,2606,2606,2606,2606,2606,2606,ICLR,2019,Faster Training by Selecting Samples Using Embeddings,Santiago Gonzalez;Joshua Landgraf;Risto Miikkulainen,slgonzalez@utexas.edu;jland@cs.utexas.edu;risto@cs.utexas.edu,3;3;2,5;3;5,Reject,0,0,0.0,yes,9/27/18,"University of Texas, Austin;University of Texas, Austin;University of Texas, Austin",22;22;22,49;49;49,
2607,2607,2607,2607,2607,2607,2607,2607,ICLR,2019,Effective Path: Know the Unknowns of Neural Network,Yuxian Qiu;Jingwen Leng;Yuhao Zhu;Quan Chen;Chao Li;Minyi Guo,qiuyuxian@sjtu.edu.cn;leng-jw@sjtu.edu.cn;yzhu@rochester.edu;chen-quan@sjtu.edu.cn;lichao@cs.sjtu.edu.cn;guo-my@cs.sjtu.edu.cn,4;4;6,4;3;5,Reject,0,0,0.0,yes,9/27/18,Shanghai Jiao Tong University;Shanghai Jiao Tong University;University of Rochester;Shanghai Jiao Tong University;Shanghai Jiao Tong University;Shanghai Jiao Tong University,52;52;106;52;52;52,188;188;153;188;188;188,4
2608,2608,2608,2608,2608,2608,2608,2608,ICLR,2019,Pseudosaccades: A simple ensemble scheme for improving classification performance of deep nets,Jin Sean Lim;Robert John Durrant,me@nicklim.com;bobd@waikato.ac.nz,5;4;4,4;4;5,Reject,0,0,0.0,yes,9/27/18,The University of Waikato;The University of Waikato,314;314,393;393,
2609,2609,2609,2609,2609,2609,2609,2609,ICLR,2019,ADAPTIVE NETWORK SPARSIFICATION VIA DEPENDENT VARIATIONAL BETA-BERNOULLI DROPOUT,Juho Lee;Saehoon Kim;Jaehong Yoon;Hae Beom Lee;Eunho Yang;Sung Ju Hwang,juho.lee@stats.ox.ac.uk;shkim@aitrics.com;jaehong.yoon@kaist.ac.kr;haebeom.lee@kaist.ac.kr;eunhoy@kaist.ac.kr;sjhwang82@kaist.ac.kr,5;5;7,4;4;4,Reject,0,6,0.0,yes,9/27/18,University of Oxford;AITRICS;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,50;-1;20;20;20;20,1;-1;95;95;95;95,11
2610,2610,2610,2610,2610,2610,2610,2610,ICLR,2019,Curiosity-Driven Experience Prioritization via Density Estimation,Rui Zhao;Volker Tresp,zhaorui.in.germany@gmail.com;volker.tresp@siemens.com,6;4;6,3;4;4,Reject,0,4,0.0,yes,9/27/18,Siemens Corporate Research;Siemens Corporate Research,-1;-1,-1;-1,
2611,2611,2611,2611,2611,2611,2611,2611,ICLR,2019,SEQUENCE MODELLING WITH AUTO-ADDRESSING AND RECURRENT MEMORY INTEGRATING NETWORKS,Zhangheng Li;Jia-Xing Zhong;Jingjia Huang;Tao Zhang;Thomas Li;Ge Li,zhanghengli@pku.edu.cn;jxzhong@pku.edu.cn;jjhuang@pku.edu.cn;t_zhang@pku.edu.cn;thomasli@pkusz.edu.cn;geli@ece.pku.edu.cn,5;4;4,4;5;4,Reject,0,6,0.0,yes,9/27/18,Peking University;Peking University;Peking University;Peking University;Tsinghua University;Peking University,24;24;24;24;8;24,27;27;27;27;30;27,
2612,2612,2612,2612,2612,2612,2612,2612,ICLR,2019,Log Hyperbolic Cosine Loss Improves Variational Auto-Encoder,Pengfei Chen;Guangyong Chen;Shengyu Zhang,chenpf.cuhk@gmail.com;gycchen@tencent.com;shengyuzhang@gmail.com,4;4;5,4;4;4,Reject,0,2,0.0,yes,9/27/18,The Chinese University of Hong Kong;Tencent AI Lab;Chinese University of Hong Kong,57;-1;57,40;-1;58,5
2613,2613,2613,2613,2613,2613,2613,2613,ICLR,2019,Computation-Efficient Quantization Method for Deep Neural Networks,Parichay Kapoor;Dongsoo Lee;Byeongwook Kim;Saehyung Lee,kparichay@gmail.com;dslee3@gmail.com;guddnr145@gmail.com;halo8218@gmail.com,4;5;5,4;4;4,Reject,0,8,1.0,yes,9/27/18,;Samsung;;Samsung,-1;-1;-1;-1,-1;-1;-1;-1,
2614,2614,2614,2614,2614,2614,2614,2614,ICLR,2019,EFFICIENT SEQUENCE LABELING WITH ACTOR-CRITIC TRAINING,Saeed Najafi;Colin Cherry;Greg Kondrak,snajafi@ualberta.ca;colin.a.cherry@gmail.com;gkondrak@ualberta.ca,5;4;4,4;3;5,Reject,0,4,0.0,yes,9/27/18,University of Alberta;Google;University of Alberta,99;-1;99,119;-1;119,
2615,2615,2615,2615,2615,2615,2615,2615,ICLR,2019,Unsupervised Exploration with Deep Model-Based Reinforcement Learning,Kurtland Chua;Rowan McAllister;Roberto Calandra;Sergey Levine,kchua@berkeley.edu;rmcallister@berkeley.edu;roberto.calandra@berkeley.edu;svlevine@eecs.berkeley.edu,4;4;4,4;3;4,Reject,0,4,0.0,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5,18;18;18;18,
2616,2616,2616,2616,2616,2616,2616,2616,ICLR,2019,Unification of  Recurrent   Neural Network Architectures and Quantum Inspired Stable Design ,Murphy Yuezhen Niu;Lior Horesh;Michael O'Keeffe;Isaac Chuang,yzniu@mit.edu;lhoresh@us.ibm.com;michael.okeeffe@ll.mit.edu;ichuang@mit.edu,5;4;4;5,2;2;3;2,Reject,0,0,0.0,yes,9/27/18,Massachusetts Institute of Technology;International Business Machines;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;-1;2;2,5;-1;5;5,1
2617,2617,2617,2617,2617,2617,2617,2617,ICLR,2019,Text Infilling,Wanrong Zhu;Zhiting Hu;Eric P. Xing,zhuwr56@gmail.com;zhitinghu@gmail.com;epxing@cs.cmu.edu,3;5;6,4;4;4,Reject,0,0,0.0,yes,9/27/18,Peking University;Carnegie Mellon University;Carnegie Mellon University,24;1;1,27;24;24,
2618,2618,2618,2618,2618,2618,2618,2618,ICLR,2019,Skip-gram word embeddings in hyperbolic space,Matthias Leimeister;Benjamin J. Wilson,matthias@lateral.io;benjamin@lateral.io,6;5;5,3;3;3,Reject,0,3,0.0,yes,9/27/18,Lateral GmbH;Lateral GmbH,-1;-1,-1;-1,3;10
2619,2619,2619,2619,2619,2619,2619,2619,ICLR,2019,EXPLORATION OF EFFICIENT ON-DEVICE ACOUSTIC MODELING WITH NEURAL NETWORKS,Wonyong Sung;Lukas Lee;Jinwhan Park,wysung@snu.ac.kr;proboscis@snu.ac.kr;bnoo@snu.ac.kr,4;4;4,5;4;4,Reject,0,0,0.0,yes,9/27/18,Seoul National University;Seoul National University;Seoul National University,41;41;41,74;74;74,
2620,2620,2620,2620,2620,2620,2620,2620,ICLR,2019,The Variational Deficiency Bottleneck,Pradeep Kr. Banerjee;Guido Montufar,pradeep@mis.mpg.de;montufar@math.ucla.edu,5;7;6,5;2;2,Reject,0,5,0.0,yes,9/27/18,"Max-Planck Institute;University of California, Los Angeles",-1;20,-1;15,1;8
2621,2621,2621,2621,2621,2621,2621,2621,ICLR,2019,Dataset Distillation,Tongzhou Wang;Jun-Yan Zhu;Antonio Torralba;Alexei A. Efros,tongzhou.wang.1994@gmail.com;junyanz@mit.edu;torralba@mit.edu;efros@eecs.berkeley.edu,6;5;5,4;4;4,Reject,4,4,0.0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;University of California Berkeley,2;2;2;5,5;5;5;18,
2622,2622,2622,2622,2622,2622,2622,2622,ICLR,2019,DecayNet: A Study on the Cell States of Long Short Term Memories,Nicholas I.H. Kuo;Mehrtash T. Harandi;Hanna Suominen;Nicolas Fourrier;Christian Walder;Gabriela Ferraro,u6424547@anu.edu.au;mehrtash.harandi@monash.edu;hanna.suominen@anu.edu.au;nicolas.fourrier@devinci.fr;christian.walder@data61.csiro.au;gabriela.ferraro@csiro.au,8;4;4,3;4;4,Reject,0,10,2.0,yes,9/27/18,"Australian National University;Monash University;Australian National University;Ecole Superieur d'Ingenieurs Leonard de Vinci;, CSIRO;CSIRO",106;123;106;-1;-1;-1,48;80;48;-1;-1;-1,
2623,2623,2623,2623,2623,2623,2623,2623,ICLR,2019,k-Nearest Neighbors by Means of Sequence to Sequence Deep Neural Networks and Memory Networks,Yiming Xu;Diego Klabjan,yimingxu2020@u.northwestern.edu;d-klabjan@northwestern.edu,6;5;4,4;4;4,Reject,0,7,0.0,yes,9/27/18,Northwestern University;Northwestern University,44;44,20;20,
2624,2624,2624,2624,2624,2624,2624,2624,ICLR,2019,Towards More Theoretically-Grounded Particle Optimization Sampling for Deep Learning,Jianyi Zhang;Ruiyi Zhang;Changyou Chen,15300180019@fudan.edu.cn;rz68@duke.edu;cchangyou@gmail.com,5;3;4,4;4;3,Reject,0,12,0.0,yes,9/27/18,"Fudan University;Duke University;State University of New York, Buffalo",78;44;81,116;17;270,11
2625,2625,2625,2625,2625,2625,2625,2625,ICLR,2019,Gaussian-gated LSTM: Improved convergence by reducing state updates,Matthew Thornton;Jithendar Anumula;Shih-Chii Liu,mattsthornton@gmail.com;anumula@ini.uzh.ch;shih@ini.uzh.ch,5;5;6,5;4;4,Reject,0,8,0.0,yes,9/27/18,Swiss Federal Institute of Technology;University of Zurich;University of Zurich,10;140;140,10;136;136,
2626,2626,2626,2626,2626,2626,2626,2626,ICLR,2019,Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data,Puyudi Yang;Jianbo Chen;Cho-Jui Hsieh;Jane-Ling Wang;Michael I. Jordan,pydyang@ucdavis.edu;jianbochen@berkeley.edu;chohsieh@ucdavis.edu;janelwang@ucdavis.edu;jordan@cs.berkeley.edu,3;6;8;7,4;4;2;4,Reject,0,22,5.0,yes,9/27/18,"University of California, Davis;University of California Berkeley;University of California, Davis;University of California, Davis;University of California Berkeley",81;5;81;81;5,54;18;54;54;18,4
2627,2627,2627,2627,2627,2627,2627,2627,ICLR,2019,Characterizing Malicious Edges targeting on Graph Neural Networks,Xiaojun Xu;Yue Yu;Bo Li;Le Song;Chengfeng Liu;Carl Gunter,xuxiaojun1005@gmail.com;yue9yu@gmail.com;lxbosky@gmail.com;lsong@cc.gatech.edu;windsonliu@tencent.com;cgunter@illinois.edu,5;5;5,5;3;3,Reject,0,5,0.0,yes,9/27/18,";Tsinghua University;University of California Berkeley;Georgia Institute of Technology;Tencent AI Lab;University of Illinois, Urbana Champaign",-1;8;5;13;-1;3,-1;30;18;33;-1;37,4;10
2628,2628,2628,2628,2628,2628,2628,2628,ICLR,2019,Graph2Seq: Scalable Learning Dynamics for Graphs,Shaileshh Bojja Venkatakrishnan;Mohammad Alizadeh;Pramod Viswanath,bjjvnkt@csail.mit.edu;alizadeh@csail.mit.edu;pramodv@illinois.edu,6;5;4,3;5;4,Reject,0,17,2.0,yes,9/27/18,"Massachusetts Institute of Technology;Massachusetts Institute of Technology;University of Illinois, Urbana Champaign",2;2;3,5;5;37,10;8
2629,2629,2629,2629,2629,2629,2629,2629,ICLR,2019,Unsupervised Word Discovery with Segmental Neural Language Models,Kazuya Kawakami;Chris Dyer;Phil Blunsom,kawakamik@google.com;cdyer@google.com;pblunsom@google.com,6;4;3,4;3;5,Reject,8,5,0.0,yes,9/27/18,Google;Google;Google,-1;-1;-1,-1;-1;-1,3;11;2;8
2630,2630,2630,2630,2630,2630,2630,2630,ICLR,2019,"A bird's eye view on coherence, and a worm's eye view on cohesion",Woon Sang Cho;Pengchuan Zhang;Yizhe Zhang;Xiujun Li;Mengdi Wang;Jianfeng Gao,woonsang@princeton.edu;penzhan@microsoft.com;yizhe.zhang@microsoft.com;xiul@microsoft.com;mengdiw@princeton.edu;jfgao@microsoft.com,2;2;4,4;4;4,Reject,0,5,0.0,yes,9/27/18,Princeton University;Microsoft;Microsoft;Microsoft;Princeton University;Microsoft,30;-1;-1;-1;30;-1,7;-1;-1;-1;7;-1,3
2631,2631,2631,2631,2631,2631,2631,2631,ICLR,2019,State-Regularized Recurrent Networks,Cheng Wang;Mathias Niepert,dr.rer.nat.chengwang@gmail.com;mathias.niepert@neclab.eu,6;6;5,4;5;5,Reject,0,4,0.0,yes,9/27/18,NEC Labs Europe;NEC Labs Europe,-1;-1,-1;-1,
2632,2632,2632,2632,2632,2632,2632,2632,ICLR,2019,DEEP GRAPH TRANSLATION,Xiaojie Guo;Lingfei Wu;Liang Zhao,xguo7@gmu.edu;lwu@email.wm.edu;lzhao9@gmu.edu,5;5;6,2;4;4,Reject,0,14,0.0,yes,9/27/18,George Mason University;College of William and Mary;George Mason University,99;169;99,336;261;336,5;4;10
2633,2633,2633,2633,2633,2633,2633,2633,ICLR,2019,Canonical Correlation Analysis with Implicit Distributions,Yaxin Shi;Donna Xu;Yuangang Pan;Ivor Tsang,yaxin.shi@student.uts.edu.au;donna.xu@student.uts.edu.au;yuangang.pan@student.uts.edu.au;ivor.tsang@uts.edu.au,5;6;4,5;4;5,Reject,0,12,0.0,yes,9/27/18,University of Technology Sydney;University of Technology Sydney;University of Technology Sydney;University of Technology Sydney,106;106;106;106,216;216;216;216,5;4
2634,2634,2634,2634,2634,2634,2634,2634,ICLR,2019,Neural Distribution Learning for generalized time-to-event prediction,Egil Martinsson;Adrian Kim;Jaesung Huh;Jaegul Choo;Jung-Woo Ha,egil.martinsson@gmail.com;adrian.kim@navercorp.com;jaesung.huh@navercorp.com;jchoo@korea.ac.kr;jungwoo.ha@navercorp.com,4;3;3,5;4;3,Reject,0,7,0.0,yes,9/27/18,Chalmers University;NAVER;NAVER;Korea University;NAVER,169;-1;-1;314;-1,240;-1;-1;244;-1,
2635,2635,2635,2635,2635,2635,2635,2635,ICLR,2019,Inference of unobserved event streams with neural Hawkes particle smoothing,Hongyuan Mei;Guanghui Qin;Jason Eisner,hmei@cs.jhu.edu;ghq@pku.edu.cn;jason@cs.jhu.edu,5;4;5,4;5;3,Reject,0,11,0.0,yes,9/27/18,Johns Hopkins University;Peking University;Johns Hopkins University,72;24;72,13;27;13,
2636,2636,2636,2636,2636,2636,2636,2636,ICLR,2019,Countdown Regression: Sharp and Calibrated Survival Predictions,Anand Avati;Tony Duan;Sharon Zhou;Kenneth Jung;Nigam Shah;Andrew Ng,avati@cs.stanford.edu;tonyduan@cs.stanford.edu;sharonz@cs.stanford.edu,4;4;5;4,5;4;3;4,Reject,0,4,0.0,yes,9/27/18,Stanford University;Stanford University;Stanford University,4;4;4,3;3;3,8
2637,2637,2637,2637,2637,2637,2637,2637,ICLR,2019,Discrete Structural Planning for Generating Diverse Translations,Raphael Shu;Hideki Nakayama,shu@nlab.ci.i.u-tokyo.ac.jp;nakayama@ci.i.u-tokyo.ac.jp,4;5;2,5;3;5,Reject,0,7,0.0,yes,9/27/18,The University of Tokyo;The University of Tokyo,54;54,45;45,3
2638,2638,2638,2638,2638,2638,2638,2638,ICLR,2019,Feature quantization for parsimonious and interpretable predictive models,Adrien EHRHARDT;Vincent VANDEWALLE;Christophe BIERNACKI;Philippe HEINRICH,adrien.ehrhardt@inria.fr;vincent.vandewalle@inria.fr;christophe.biernacki@inria.fr;philippe.heinrich@univ-lille.fr,2;3;4,4;3;2,Reject,0,4,0.0,yes,9/27/18,INRIA;INRIA;INRIA;Université de Lille,-1;-1;-1;478,-1;-1;-1;468,
2639,2639,2639,2639,2639,2639,2639,2639,ICLR,2019,Hierarchical Bayesian Modeling for Clustering Sparse Sequences in the Context of Group Profiling,Ishani Chakraborty,ishani.chakrab@gmail.com,2;2;1;3;2,5;5;5;4;4,Reject,0,0,0.0,yes,9/27/18,University of Southern California,30,66,11
2640,2640,2640,2640,2640,2640,2640,2640,ICLR,2019,Clinical Risk: wavelet reconstruction networks for marked point processes,Jeremy C. Weiss,jeremy.weiss@gmail.com,7;4;5,4;4;4,Reject,0,11,0.0,yes,9/27/18,Carnegie Mellon University,1,24,
2641,2641,2641,2641,2641,2641,2641,2641,ICLR,2019,Interpreting Layered Neural Networks via Hierarchical Modular Representation,Chihiro Watanabe,watanabe.chihiro@lab.ntt.co.jp,4;3;3,3;4;4,Reject,0,0,0.0,yes,9/27/18,NTT,-1,-1,
2642,2642,2642,2642,2642,2642,2642,2642,ICLR,2019,Learning Graph Representations by Dendrograms,Thomas Bonald;Bertrand Charpentier,thomas.bonald@telecom-paristech.fr;bertrand.charpentier@live.fr,4;5;5,4;4;3,Reject,0,3,0.0,yes,9/27/18,Télécom ParisTech;,478;-1,188;-1,10
2643,2643,2643,2643,2643,2643,2643,2643,ICLR,2019,Fast Binary Functional Search on Graph,Shulong Tan;Zhixin Zhou;Zhaozhuo Xu;Ping Li,laos1984@gmail.com;zhixin0825@gmail.com;zhaozhuoxu@gmail.com;pingli98@gmail.com,5;4,5;4,Reject,0,6,0.0,yes,9/27/18,";University of California, Los Angeles;;Rutgers University New Brunswick",-1;20;-1;34,-1;15;-1;172,10
2644,2644,2644,2644,2644,2644,2644,2644,ICLR,2019,HyperGAN:  Exploring the Manifold of Neural Networks,Neale Ratzlaff;Li  Fuxin,ratzlafn@oregonstate.edu;lif@oregonstate.edu,5;6;4,4;5;3,Reject,2,5,0.0,yes,9/27/18,Oregon State University;Oregon State University,76;76,318;318,5;4
2645,2645,2645,2645,2645,2645,2645,2645,ICLR,2019,Correction Networks: Meta-Learning for Zero-Shot Learning,R. Lily Hu;Caiming Xiong;Richard Socher,rlilyhu@gmail.com;cxiong@salesforce.com;rsocher@salesforce.com,7;4;4,4;5;4,Reject,0,7,1.0,yes,9/27/18,SalesForce.com;SalesForce.com;SalesForce.com,-1;-1;-1,-1;-1;-1,3;6
2646,2646,2646,2646,2646,2646,2646,2646,ICLR,2019,NUTS: Network for Unsupervised Telegraphic Summarization,Chanakya Malireddy;Tirth Maniar;Sajal Maheshwari;Manish Shrivastava,chanakya.malireddy@gmail.com;tirthmaniar1998@gmail.com;sajalmaheshwari624@gmail.com;m.shrivastava@iiit.ac.in,4;4;4,4;4;4,Reject,0,3,0.0,yes,9/27/18,International Institute of Information Technology Hyderabad;;;International Institute of Information Technology Hyderabad,199;-1;-1;199,1103;-1;-1;1103,
2647,2647,2647,2647,2647,2647,2647,2647,ICLR,2019,The wisdom of the crowd: reliable deep reinforcement learning through ensembles of Q-functions,Daniel Elliott;Charles Anderson,daniel.elliott18@alumni.colostate.edu;chuck.anderson@colostate.edu,4;5;3,5;3;4,Reject,0,0,0.0,yes,9/27/18,Colorado State University;Colorado State University,314;314,356;356,
2648,2648,2648,2648,2648,2648,2648,2648,ICLR,2019,Co-manifold learning with missing data,Gal Mishne;Eric C. Chi;Ronald R. Coifman,gal.mishne@yale.edu;eric_chi@ncsu.edu;coifman.ronald@yale.edu,7;4;4,4;4;3,Reject,0,8,0.0,yes,9/27/18,Yale University;North Carolina State University;Yale University,62;89;62,12;275;12,
2649,2649,2649,2649,2649,2649,2649,2649,ICLR,2019,Training generative latent models  by variational f-divergence minimization,Mingtian Zhang;Thomas Bird;Raza Habib;Tianlin Xu;David Barber,mingtian.zhang.17@ucl.ac.uk;thomas.bird@cs.ucl.ac.uk;raza.habib@cs.ucl.ac.uk;t.xu12@lse.ac.uk;david.barber@ucl.ac.uk,6;5;5,3;4;3,Reject,0,3,0.0,yes,9/27/18,University College London;University College London;University College London;London School of Economics;University College London,50;50;50;478;50,16;16;16;26;16,5;1
2650,2650,2650,2650,2650,2650,2650,2650,ICLR,2019,Improved Gradient Estimators for Stochastic Discrete Variables,Evgeny Andriyash;Arash Vahdat;Bill Macready,eandriyash@dwavesys.com;avahdat@dwavesys.com;wgm@dwavesys.com,7;6;6,4;3;4,Reject,0,3,0.0,yes,9/27/18,D-Wave Systems;D-Wave Systems;D-Wave Systems,-1;-1;-1,-1;-1;-1,
2651,2651,2651,2651,2651,2651,2651,2651,ICLR,2019,AutoLoss: Learning Discrete Schedule for Alternate Optimization,Haowen Xu;Hao Zhang;Zhiting Hu;Xiaodan Liang;Ruslan Salakhutdinov;Eric Xing,haowen.will.xu@gmail.com;hao@cs.cmu.edu;zhitingh@cs.cmu.edu;xiaodan1@cs.cmu.edu;rsalakhu@cs.cmu.edu;eric.xing@petuum.com,6;7;7,3;4;3,Accept (Poster),0,9,2.0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Petuum Inc.,1;1;1;1;1;-1,24;24;24;24;24;-1,3;5;6
2652,2652,2652,2652,2652,2652,2652,2652,ICLR,2019,Spread Divergences,David Barber;Mingtian Zhang;Raza Habib;Thomas Bird,d.barber@cs.ucl.ac.uk;mingtian.zhang.17@ucl.ac.uk;raza.habib.15@ucl.ac.uk;thomas.bird.17@ucl.ac.uk,5;4;6,4;4;4,Reject,0,3,0.0,yes,9/27/18,University College London;University College London;University College London;University College London,50;50;50;50,16;16;16;16,5
2653,2653,2653,2653,2653,2653,2653,2653,ICLR,2019,Explicit Information Placement on Latent Variables using Auxiliary Generative Modelling Task,Nat Dilokthanakul;Nick Pawlowski;Murray Shanahan,n.dilokthanakul14@imperial.ac.uk;n.pawlowski16@imperial.ac.uk;m.shanahan@imperial.ac.uk,5;6;7,4;4;4,Reject,0,7,0.0,yes,9/27/18,Imperial College London;Imperial College London;Imperial College London,72;72;72,8;8;8,5
2654,2654,2654,2654,2654,2654,2654,2654,ICLR,2019,Convergence Properties of Deep Neural Networks on Separable Data,Remi Tachet des Combes;Mohammad Pezeshki;Samira Shabanian;Aaron Courville;Yoshua Bengio,remi.tachet@microsoft.com;mohammad.pezeshki@umontreal.ca;s.shabanian@gmail.com;aaron.courville@gmail.com;yoshua.umontreal@gmail.com,5;5;5,4;4;3,Reject,0,5,0.0,yes,9/27/18,Microsoft;University of Montreal;Microsoft;University of Montreal;University of Montreal,-1;123;-1;123;123,-1;108;-1;108;108,5;4;1;8
2655,2655,2655,2655,2655,2655,2655,2655,ICLR,2019,Meta-Learning with Individualized Feature Space for Few-Shot Classification,Chunrui Han;Shiguang Shan;Meina Kan;Shuzhe Wu;Xilin Chen,chunrui.han@vipl.ict.ac.cn;sgshan@ict.ac.cn;kanmeina@ict.ac.cn;shuzhe.wu@vipl.ict.ac.cn;xlchen@ict.ac.cn,5;5;3,4;4;3,Reject,0,0,0.0,yes,9/27/18,"Institute of Computing Technology, Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences",62;62;62;62;62,1103;1103;1103;1103;1103,6
2656,2656,2656,2656,2656,2656,2656,2656,ICLR,2019,Neural Regression Tree,Wenbo Zhao;Shahan Ali Memon;Bhiksha Raj;Rita Singh,wzhao1@andrew.cmu.ecu;samemon@cs.cmu.edu;bhikshar@cs.cmu.edu;rsingh@cs.cmu.edu,5;3;4,3;4;5,Reject,0,0,0.0,yes,9/27/18,;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,-1;1;1;1,-1;24;24;24,
2657,2657,2657,2657,2657,2657,2657,2657,ICLR,2019,Learning Corresponded Rationales for Text Matching,Mo Yu;Shiyu Chang;Tommi S Jaakkola,shiyu.chang@ibm.com;yum@us.ibm.com;tommi@csail.mit.edu,6;4;3,4;4;5,Reject,0,3,0.0,yes,9/27/18,International Business Machines;International Business Machines;Massachusetts Institute of Technology,-1;-1;2,-1;-1;5,3
2658,2658,2658,2658,2658,2658,2658,2658,ICLR,2019,Differentiable Expected BLEU for Text Generation,Wentao Wang;Zhiting Hu;Zichao Yang;Haoran Shi;Eric P. Xing,wwt10@pku.edu.cn;zhitinghu@gmail.com;yangtze2301@gmail.com;shr970423@gmail.com;epxing@cs.cmu.edu,4;4;6,4;5;4,Reject,2,0,0.0,yes,9/27/18,Peking University;Carnegie Mellon University;;;Carnegie Mellon University,24;1;-1;-1;1,27;24;-1;-1;24,3
2659,2659,2659,2659,2659,2659,2659,2659,ICLR,2019,Cautious Deep Learning,Yotam Hechtlinger;Barnabas Poczos;Larry Wasserman,yhechtli@andrew.cmu.edu;bapoczos@cs.cmu.edu;larry@cmu.edu,4;7;4,3;2;5,Reject,0,5,0.0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1,24;24;24,
2660,2660,2660,2660,2660,2660,2660,2660,ICLR,2019,Success at any cost: value constrained model-free continuous control,Steven Bohez;Abbas Abdolmaleki;Michael Neunert;Jonas Buchli;Nicolas Heess;Raia Hadsell,sbohez@google.com;aabdolmaleki@google.com;neunertm@google.com;buchli@google.com;heess@google.com;raia@google.com,6;7;5,4;4;4,Reject,0,11,0.0,yes,9/27/18,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,1
2661,2661,2661,2661,2661,2661,2661,2661,ICLR,2019,Learning Hash Codes via Hamming Distance Targets,Martin Loncaric;Ryan Weber;Bowei Liu,martin@thehive.ai;ryan@thehive.ai;liubowei@gmail.com,6;4;4,3;3;5,Reject,4,6,0.0,yes,9/27/18,;;,-1;-1;-1,-1;-1;-1,
2662,2662,2662,2662,2662,2662,2662,2662,ICLR,2019,Interactive Parallel Exploration for Reinforcement Learning in Continuous Action Spaces,Whiyoung Jung;Giseung Park;Youngchul Sung,wy.jung@kaist.ac.kr;gs.park@kaist.ac.kr;ycsung@kaist.ac.kr,7;6;4,4;4;4,Reject,0,5,0.0,yes,9/27/18,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,20;20;20,95;95;95,
2663,2663,2663,2663,2663,2663,2663,2663,ICLR,2019,A Proposed Hierarchy of Deep Learning Tasks,Joel Hestness;Sharan Narang;Newsha Ardalani;Heewoo Jun;Hassan Kianinejad;Md. Mostofa Ali Patwary;Yang Yang;Yanqi Zhou;Gregory Diamos;Kenneth Church,joel@baidu.com;sharan@baidu.com;ardalaninewsha@baidu.com;junheewoo@baidu.com;hassankianinejad@baidu.com;patwarymostofa@baidu.com;yangyang62@baidu.com;zhouyanqi@baidu.com;gregdiamos@baidu.com;kennethchurch@baidu.com,6;4;4,3;5;2,Reject,0,0,0.0,yes,9/27/18,Baidu;Baidu;Baidu;Baidu;Baidu;Baidu;Baidu;Baidu;Baidu;Baidu,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,3
2664,2664,2664,2664,2664,2664,2664,2664,ICLR,2019,Multi-Grained Entity Proposal Network for Named Entity Recognition,Congying Xia;Chenwei Zhang;Tao Yang;Yaliang Li;Nan Du;Xian Wu;Wei Fan;Fenglong Ma;Philip S. Yu,cxia8@uic.edu;czhang99@uic.edu;tytaoyang@tencent.com;yaliangli@tencent.com;ndu@tencent.com;kevinxwu@tencent.com;davidwfan@tencent.com;fenglong@buffalo.edu;psyu@uic.edu,5;5;4,4;3;4,Reject,0,3,0.0,yes,9/27/18,"University of Illinois, Chicago;University of Illinois, Chicago;Tencent AI Lab;Tencent AI Lab;Tencent AI Lab;Tencent AI Lab;Tencent AI Lab;State University of New York, Buffalo;University of Illinois, Chicago",57;57;-1;-1;-1;-1;-1;81;57,255;255;-1;-1;-1;-1;-1;270;255,
2665,2665,2665,2665,2665,2665,2665,2665,ICLR,2019,Massively Parallel Hyperparameter Tuning,Liam Li;Kevin Jamieson;Afshin Rostamizadeh;Ekaterina Gonina;Moritz Hardt;Ben Recht;Ameet Talwalkar,jamieson@cs.washington.edu;rostami@google.com;kgonina@google.com;hardt@berkeley.edu;brecht@berkeley.edu;talwalkar@cmu.edu,5;6;5,4;4;4,Reject,0,5,0.0,yes,9/27/18,University of Washington;Google;Google;University of California Berkeley;University of California Berkeley;Carnegie Mellon University,6;-1;-1;5;5;1,25;-1;-1;18;18;24,
2666,2666,2666,2666,2666,2666,2666,2666,ICLR,2019,Dynamic Graph Representation Learning via Self-Attention Networks,Aravind Sankar;Yanhong Wu;Liang Gou;Wei Zhang;Hao Yang,asankar3@illinois.edu;yanwu@visa.com;ligou@visa.com;wzhan@visa.com;haoyang@visa.com,4;6;5,5;4;4,Reject,2,7,0.0,yes,9/27/18,"University of Illinois, Urbana Champaign;VISA;VISA;VISA;VISA",3;-1;-1;-1;-1,37;-1;-1;-1;-1,10
2667,2667,2667,2667,2667,2667,2667,2667,ICLR,2019,Denoise while Aggregating: Collaborative Learning in Open-Domain Question Answering,Haozhe Ji;Yankai Lin;Zhiyuan Liu;Maosong Sun,jihaozhe@gmail.com;mrlyk423@gmail.com;liuzy@tsinghua.edu.cn;sms@tsinghua.edu.cn,4;6;5,4;4;4,Reject,0,0,0.0,yes,9/27/18,Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University,8;8;8;8,30;30;30;30,
2668,2668,2668,2668,2668,2668,2668,2668,ICLR,2019,Are Generative Classifiers More Robust to Adversarial Attacks?,Yingzhen Li;John Bradshaw;Yash Sharma,yl494@cam.ac.uk;jab255@cam.ac.uk;ysharma1126@gmail.com,4;6;4;8,4;3;5;3,Reject,0,13,2.0,yes,9/27/18,University of Cambridge;University of Cambridge;The Cooper Union,71;71;-1,2;2;-1,5;4
2669,2669,2669,2669,2669,2669,2669,2669,ICLR,2019,Sentence Encoding with Tree-Constrained Relation Networks,Lei Yu;Cyprien de Masson d'Autume;Chris Dyer;Phil Blunsom;Lingpeng Kong;Wang Ling,leiyu@google.com;cyprien@google.com;cdyer@google.com;pblunsom@google.com;lingpenk@google.com;lingwang@google.com,5;5;3,4;4;4,Reject,0,4,0.0,yes,9/27/18,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,3
2670,2670,2670,2670,2670,2670,2670,2670,ICLR,2019,Exploiting Cross-Lingual Subword Similarities in Low-Resource Document Classification,Mozhi Zhang;Yoshinari Fujinuma;Jordan Boyd-Graber,mozhi@cs.umd.edu;yoshinari.fujinuma@colorado.edu;jbg@umiacs.umd.edu,6;4;6,3;3;4,Reject,0,4,0.0,yes,9/27/18,"University of Maryland, College Park;University of Colorado, Boulder;University of Maryland, College Park",12;44;12,69;100;69,6
2671,2671,2671,2671,2671,2671,2671,2671,ICLR,2019,Exploiting Environmental Variation to Improve Policy Robustness in  Reinforcement Learning,Siddharth Mysore;Robert Platt;Kate Saenko,sidmys@bu.edu;rplatt@ccs.neu.edu;saenko@bu.edu,5;3;6,3;4;4,Reject,0,3,0.0,yes,9/27/18,Boston University;Northeastern University;Boston University,65;16;65,70;839;70,8
2672,2672,2672,2672,2672,2672,2672,2672,ICLR,2019,DOMAIN ADAPTATION VIA DISTRIBUTION AND REPRESENTATION MATCHING: A CASE STUDY ON TRAINING DATA SELECTION VIA REINFORCEMENT LEARNING,Miaofeng Liu;Yan Song;Hongbin Zou;Tong Zhang,water3er@gmail.com;clksong@gmail.com;hbzou@xdu.edu.cn;bradymzhang@tencent.com,4;7;5,2;3;4,Reject,0,1,1.0,yes,9/27/18,;Tencent AI Lab;Shandong University;Tencent AI Lab,-1;-1;4;-1,-1;-1;3;-1,3
2673,2673,2673,2673,2673,2673,2673,2673,ICLR,2019,Assumption Questioning: Latent Copying and Reward Exploitation in Question Generation,Tom Hosking;Sebastian Riedel,thomas.hosking.17@ucl.ac.uk;sebastian.riedel@gmail.com,3;4;5,4;4;4,Reject,0,3,0.0,yes,9/27/18,University College London;University College London,50;50,16;16,3;4
2674,2674,2674,2674,2674,2674,2674,2674,ICLR,2019,Using Word Embeddings to Explore the Learned Representations of Convolutional Neural Networks,Dhanush Dharmaretnam;Chris Foster;Alona Fyshe,dhanush987@gmail.com;chris.james.foster@gmail.com;alona@ualberta.ca,4;3;4,4;4;2,Reject,0,0,0.0,yes,9/27/18,University of Victoria;University of Victoria;University of Alberta,169;169;99,346;346;119,3;4
2675,2675,2675,2675,2675,2675,2675,2675,ICLR,2019,Learning to control self-assembling morphologies: a study of generalization via modularity,Deepak Pathak;Chris Lu;Trevor Darrell;Philip Isola;Alexei A. Efros,pathak@berkeley.edu;chris.lu@berkeley.edu;trevor@eecs.berkeley.edu;phillip.isola@gmail.com;efros@eecs.berkeley.edu,4;4;7,4;3;3,Reject,2,8,0.0,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley;;University of California Berkeley,5;5;5;-1;5,18;18;18;-1;18,10
2676,2676,2676,2676,2676,2676,2676,2676,ICLR,2019,End-to-End Hierarchical Text Classification with Label Assignment Policy,Yuning Mao;Jingjing Tian;Jiawei Han;Xiang Ren,yuningm2@illinois.edu;tianjj97@pku.edu.cn;hanj@illinois.edu;xiangren@usc.edu,5;4;4,4;5;4,Reject,1,8,0.0,yes,9/27/18,"University of Illinois, Urbana Champaign;Peking University;University of Illinois, Urbana Champaign;University of Southern California",3;24;3;30,37;27;37;66,
2677,2677,2677,2677,2677,2677,2677,2677,ICLR,2019,SynonymNet: Multi-context Bilateral Matching for Entity Synonyms,Chenwei Zhang;Yaliang Li;Nan Du;Wei Fan;Philip S. Yu,czhang99@uic.edu;yaliangli@tencent.com;ndu@tencent.com;davidwfan@tencent.com;psyu@uic.edu,5;7;4,4;5;4,Reject,0,4,0.0,yes,9/27/18,"University of Illinois, Chicago;Tencent AI Lab;Tencent AI Lab;Tencent AI Lab;University of Illinois, Chicago",57;-1;-1;-1;57,255;-1;-1;-1;255,
2678,2678,2678,2678,2678,2678,2678,2678,ICLR,2019,TopicGAN: Unsupervised Text Generation from Explainable Latent Topics,Yau-Shian Wang;Yun-Nung Chen;Hung-Yi Lee,king6101@gmail.com;y.v.chen@ieee.org;tlkagkb93901106@gmail.com,4;4;5,2;4;4,Reject,0,3,0.0,yes,9/27/18,National Taiwan University;National Taiwan University;,85;85;-1,197;197;-1,3;4;5
2679,2679,2679,2679,2679,2679,2679,2679,ICLR,2019,ON THE EFFECTIVENESS OF TASK GRANULARITY FOR TRANSFER LEARNING,Farzaneh Mahdisoltani;Guillaume Berger;Waseem Gharbieh;David Fleet;Roland Memisevic,farzaneh@cs.toronto.edu;guillaume.berger@twentybn.com;waseem.gharbieh@twentybn.com;fleet@cs.toronto.edu;roland.memisevic@twentybn.com,5;5;5,4;4;4,Reject,0,0,0.0,yes,9/27/18,"Department of Computer Science, University of Toronto;Twenty Billion Neurons;Twenty Billion Neurons;Department of Computer Science, University of Toronto;Twenty Billion Neurons",18;-1;-1;18;-1,22;-1;-1;22;-1,6
2680,2680,2680,2680,2680,2680,2680,2680,ICLR,2019,Neural Networks for Modeling Source Code Edits,Rui Zhao;David Bieber;Kevin Swersky;Daniel Tarlow,oahziur@gmail.com;dbieber@google.com;kswersky@google.com;dtarlow@google.com,5;6;6;6,4;2;4;4,Reject,0,5,0.0,yes,9/27/18,;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,5;8
2681,2681,2681,2681,2681,2681,2681,2681,ICLR,2019,COMPOSITION AND DECOMPOSITION OF GANS,Yeu-Chern Harn;Zhenghao Chen;Vladimir Jojic,ycharn@cs.unc.edu;chen.zhenghao@gmail.com;vjojic@gmail.com,4;5;4,5;5;4,Reject,0,5,0.0,yes,9/27/18,"University of North Carolina, Chapel Hill;Calico Labs;University of North Carolina, Chapel Hill",76;-1;76,56;-1;56,5;4
2682,2682,2682,2682,2682,2682,2682,2682,ICLR,2019,"Unicorn: Continual learning with a universal, off-policy agent",Daniel J. Mankowitz;Augustin Žídek;André Barreto;Dan Horgan;Matteo Hessel;John Quan;Junhyuk Oh;Hado van Hasselt;David Silver;Tom Schaul,dmankowitz@google.com;augustinzidek@google.com;andrebarreto@google.com;horgan@google.com;mtthss@google.com;johnquan@google.com;junhyuk@google.com;hado@google.com;davidsilver@google.com;schaul@google.com,4;5;6,5;4;4,Reject,0,3,0.0,yes,9/27/18,Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,
2683,2683,2683,2683,2683,2683,2683,2683,ICLR,2019,Understanding the Asymptotic Performance of Model-Based RL Methods,William Whitney;Rob Fergus,wfwhitney@gmail.com;fergus@cs.nyu.edu,5;6;4;2,3;4;3;4,Reject,0,5,0.0,yes,9/27/18,New York University;New York University,26;26,27;27,
2684,2684,2684,2684,2684,2684,2684,2684,ICLR,2019,Dynamic Pricing on E-commerce Platform with Deep Reinforcement Learning,Jiaxi Liu;Yidong Zhang;Xiaoqing Wang;Yuming Deng;Xingyu Wu;Miaolan Xie,galiliu.ljx@alibaba-inc.com;tanfu.zyd@alibaba-inc.com;robin.wxq@alibaba-inc.com;yuming.dym@alibaba-inc.com;zhuyang.wxy@alibaba-inc.com;miaolan.xml@alibaba-inc.com,4;4;4,4;5;3,Reject,2,4,0.0,yes,9/27/18,Alibaba Group;Alibaba Group;Alibaba Group;Alibaba Group;Alibaba Group;Alibaba Group,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,
2685,2685,2685,2685,2685,2685,2685,2685,ICLR,2019,Coupled Recurrent Models for Polyphonic Music Composition,John Thickstun;Zaid Harchaoui;Dean P. Foster;Sham M. Kakade,thickstn@cs.washington.edu;zaid@uw.edu;sham@cs.washington.edu;dean@foster.net,7;4;3,3;4;4,Reject,1,4,0.0,yes,9/27/18,"University of Washington;University of Washington, Seattle;University of Washington;",6;6;6;-1,25;25;25;-1,5
2686,2686,2686,2686,2686,2686,2686,2686,ICLR,2019,From Nodes to Networks: Evolving Recurrent Neural Networks,Aditya Rawal;Jason Liang;Risto Miikkulainen,aditya@cs.utexas.edu;jasonzliang@utexas.edu;risto@cs.utexas.edu,5;4;4,4;4;4,Reject,0,0,0.0,yes,9/27/18,"University of Texas, Austin;University of Texas, Austin;University of Texas, Austin",22;22;22,49;49;49,3
2687,2687,2687,2687,2687,2687,2687,2687,ICLR,2019,Lyapunov-based Safe Policy Optimization,Yinlam Chow;Ofir Nachum;Mohammad Ghavamzadeh;Edgar Guzman-Duenez,yinlamchow@google.com;ofirnachum@google.com;mohammad.ghavamzadeh@inria.fr;duenez@google.com,6;5;6;8,2;3;2;3,Reject,0,6,0.0,yes,9/27/18,Google;Google;INRIA;Google,-1;-1;-1;-1,-1;-1;-1;-1,
2688,2688,2688,2688,2688,2688,2688,2688,ICLR,2019,Composing Entropic Policies using Divergence Correction,Jonathan J Hunt;Andre Barreto;Timothy P Lillicrap;Nicolas Heess,jjhunt@google.com;andrebarreto@google.com;countzero@google.com;heess@google.com,4;5;7,3;4;3,Reject,0,9,0.0,yes,9/27/18,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,
2689,2689,2689,2689,2689,2689,2689,2689,ICLR,2019,Accelerated Gradient Flow for Probability Distributions,Amirhossein Taghvaei;Prashant G. Mehta,amirhoseintghv@gmail.com;mehtapg@illinois.edu,4;5;6,4;3;4,Reject,0,7,0.0,yes,9/27/18,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",3;3,37;37,9
2690,2690,2690,2690,2690,2690,2690,2690,ICLR,2019,Constraining Action Sequences with Formal Languages for Deep Reinforcement Learning,Dong Xu;Eleanor Quint;Zeynep Hakguder;Haluk Dogan;Stephen Scott;Matthew Dwyer,dx@virginia.edu;pquint@cse.unl.edu;zeynep.hakguder@huskers.unl.edu;haluk.dogan@huskers.unl.edu;sscott@cse.unl.edu;matthewbdwyer@virginia.edu,5;4;3,4;3;4,Reject,0,1,0.0,yes,9/27/18,"University of Virginia;University of Nebraska, Lincoln;University of Nebraska, Lincoln;University of Nebraska, Lincoln;University of Nebraska, Lincoln;University of Virginia",65;228;228;228;228;65,113;337;337;337;337;113,
2691,2691,2691,2691,2691,2691,2691,2691,ICLR,2019,What Would pi* Do?: Imitation Learning via Off-Policy Reinforcement Learning,Siddharth Reddy;Anca D. Dragan;Sergey Levine,sgr@berkeley.edu;anca@berkeley.edu;svlevine@eecs.berkeley.edu,5;6;5,3;4;4,Reject,3,12,0.0,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,18;18;18,4
2692,2692,2692,2692,2692,2692,2692,2692,ICLR,2019,Boosting Trust Region Policy Optimization by Normalizing flows Policy,Yunhao Tang;Shipra Agrawal,yt2541@columbia.edu;sa3305@columbia.edu,4;6;4,4;4;4,Reject,0,3,0.0,yes,9/27/18,Columbia University;Columbia University,15;15,14;14,
2693,2693,2693,2693,2693,2693,2693,2693,ICLR,2019,Architecture Compression,Anubhav Ashok,anubhava@alumni.cmu.edu,4;6;4,3;4;4,Reject,0,5,0.0,yes,9/27/18,Carnegie Mellon University,1,24,
2694,2694,2694,2694,2694,2694,2694,2694,ICLR,2019,Super-Resolution via Conditional Implicit Maximum Likelihood Estimation,Ke Li*;Shichong Peng*;Jitendra Malik,ke.li@eecs.berkeley.edu;shichong.peng@mail.utoronto.ca;malik@eecs.berkeley.edu,5;6;6,3;5;5,Reject,6,5,0.0,yes,9/27/18,University of California Berkeley;Toronto University;University of California Berkeley,5;18;5,18;22;18,
2695,2695,2695,2695,2695,2695,2695,2695,ICLR,2019,A Better Baseline for Second Order Gradient Estimation in Stochastic Computation Graphs,Jingkai Mao;Jakob Foerster;Tim Rocktäschel;Gregory Farquhar;Maruan Al-Shedivat;Shimon Whiteson,jingkai.mao@gmail.com;jakobfoerster@gmail.com;tim.rocktaeschel@gmail.com;gregory.farquhar@cs.ox.ac.uk;alshedivat@cs.cmu.edu;shimon.whitesone@cs.ox.ac.uk,3;6;5;6,4;3;4;3,Reject,0,5,0.0,yes,9/27/18,University of Oxford;University of Oxford;Facebook AI Research;University of Oxford;Carnegie Mellon University;University of Oxford,50;50;-1;50;1;50,1;1;-1;1;24;1,6;10
2696,2696,2696,2696,2696,2696,2696,2696,ICLR,2019,Reinforced Pipeline Optimization: Behaving Optimally with Non-Differentiabilities,Aijun Bai;Dongdong Chen;Gang Hua;Lu Yuan,aijunbai@gmail.com;cd722522@mail.ustc.edu.cn;ganghua@gmail.com;luyuan@microsoft.com,4;5;3,5;2;4,Reject,0,0,0.0,yes,9/27/18,Microsoft;University of Science and Technology of China;Wormpex AI Research;Microsoft,-1;478;-1;-1,-1;132;-1;-1,2
2697,2697,2697,2697,2697,2697,2697,2697,ICLR,2019,An Active Learning Framework for Efficient Robust Policy Search,Sai Kiran Narayanaswami;Nandan Sudarsanam;Balaraman Ravindran,saikirann94@gmail.com;nandan@iitm.ac.in;ravi@cse.iitm.ac.in,5;6;5,3;3;4,Reject,0,4,0.0,yes,9/27/18,;Indian Institute of Technology Madras;Indian Institute of Technology Madras,-1;153;153,-1;625;625,
2698,2698,2698,2698,2698,2698,2698,2698,ICLR,2019,Model-Agnostic Meta-Learning for Multimodal Task Distributions,Risto Vuorio;Shao-Hua Sun;Hexiang Hu;Joseph J. Lim,vuoristo@gmail.com;shaohuas@usc.edu;hexiangh@usc.edu;limjj@usc.edu,5;3;5,3;5;4,Reject,0,7,0.0,yes,9/27/18,;University of Southern California;University of Southern California;University of Southern California,-1;30;30;30,-1;66;66;66,6
2699,2699,2699,2699,2699,2699,2699,2699,ICLR,2019,Teaching to Teach by Structured Dark Knowledge,Ziliang Chen;Keze Wang;Liang Lin,c.ziliang@yahoo.com;kezewang@gmail.com;linliang@ieee.org,4;3;6,1;4;5,Reject,0,0,0.0,yes,9/27/18,"SUN YAT-SEN UNIVERSITY;University of California, Los Angeles;SUN YAT-SEN UNIVERSITY",478;20;478,352;15;352,
2700,2700,2700,2700,2700,2700,2700,2700,ICLR,2019,Constrained Bayesian Optimization for Automatic Chemical Design,Ryan-Rhys Griffiths;José Miguel Hernández-Lobato,rrg27@cam.ac.uk;jmh233@cam.ac.uk,3;4;5,4;3;4,Reject,0,0,0.0,yes,9/27/18,University of Cambridge;University of Cambridge,71;71,2;2,11
2701,2701,2701,2701,2701,2701,2701,2701,ICLR,2019,Transfer Value or Policy? A Value-centric Framework Towards Transferrable Continuous Reinforcement Learning,Xingchao Liu;Tongzhou Mu;Hao Su,liuxc1996@gmail.com;t3mu@eng.ucsd.edu;haosu@eng.ucsd.edu,5;4;5,3;4;2,Reject,0,10,0.0,yes,9/27/18,"Beihang University;University of California, San Diego;University of California, San Diego",115;11;11,658;31;31,6
2702,2702,2702,2702,2702,2702,2702,2702,ICLR,2019,Understanding GANs via Generalization Analysis for Disconnected Support,Masaaki Imaizumi;Kenji Fukumizu,insou11@hotmail.com;fukumizu@ism.ac.jp,5;6;6,4;4;3,Reject,0,4,0.0,yes,9/27/18,"The Institute of Statistical Mathematics, Japan;The Institute of Statistical Mathematics, Japan",-1;-1,-1;-1,5;4;8
2703,2703,2703,2703,2703,2703,2703,2703,ICLR,2019,Ergodic Measure Preserving Flows,Yichuan Zhang;José Miguel Hernández-Lobato;Zoubin Ghahramani,yichuan.zhang@eng.cam.ac.uk;jmh233@cam.ac.uk;zoubin@eng.cam.ac.uk,5;5;4,4;3;5,Reject,4,16,0.0,yes,9/27/18,University of Cambridge;University of Cambridge;University of Cambridge,71;71;71,2;2;2,5;11
2704,2704,2704,2704,2704,2704,2704,2704,ICLR,2019,Sparse Binary Compression: Towards Distributed Deep Learning with minimal Communication,Felix Sattler;Simon Wiedemann;Klaus-Robert Müller;Wojciech Samek,felix.sattler@hhi.fraunhofer.de;simon.wiedemann@hhi.fraunhofer.de;klaus-robert.mueller@tu-berlin.de;wojciech.samek@hhi.fraunhofer.de,6;3;5,4;4;4,Reject,0,2,0.0,yes,9/27/18,Fraunhofer IIS;Fraunhofer IIS;TU Berlin;Fraunhofer IIS,-1;-1;106;-1,-1;-1;92;-1,
2705,2705,2705,2705,2705,2705,2705,2705,ICLR,2019,"S-System, Geometry, Learning, and Optimization: A Theory of Neural Networks",Shuai Li;Kui Jia,lishuai918@gmail.com;kuijia@scut.edu.cn,4;4,2;1,Reject,0,0,0.0,yes,9/27/18,;South China University of Technology,-1;478,-1;576,1
2706,2706,2706,2706,2706,2706,2706,2706,ICLR,2019,Lorentzian Distance Learning,Marc T Law;Jake Snell;Richard S Zemel,law@cs.toronto.edu;jsnell@cs.toronto.edu;zemel@cs.toronto.edu,6;5;5,4;4;4,Reject,0,9,0.0,yes,9/27/18,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",18;18;18,22;22;22,
2707,2707,2707,2707,2707,2707,2707,2707,ICLR,2019,W2GAN: RECOVERING AN OPTIMAL TRANSPORT MAP WITH A GAN,Leygonie Jacob*;Jennifer She*;Amjad Almahairi;Sai Rajeswar;Aaron Courville,jacob.leygonie@gmail.com;jennifershe123@gmail.com;amjadmahayri@gmail.com;rajsai24@gmail.com;aaron.courville@gmail.com,6;3;4,3;4;3,Reject,0,9,0.0,yes,9/27/18,University of Oxford;Stanford University;Element AI;University of Montreal;University of Montreal,50;4;-1;123;123,1;3;-1;108;108,5;4
2708,2708,2708,2708,2708,2708,2708,2708,ICLR,2019,A Differentiable Self-disambiguated Sense Embedding Model via Scaled Gumbel Softmax,Fenfei Guo;Mohit Iyyer;Leah Findlater;Jordan Boyd-Graber,fenfeigo@cs.umd.edu;miyyer@cs.umass.edu;leahkf@uw.edu;jbg@umiacs.umd.edu,6;7;6,5;3;4,Reject,0,8,0.0,yes,9/27/18,"University of Maryland, College Park;University of Massachusetts, Amherst;University of Washington, Seattle;University of Maryland, College Park",12;30;6;12,69;191;25;69,
2709,2709,2709,2709,2709,2709,2709,2709,ICLR,2019,On the Computational Inefficiency of Large Batch Sizes for Stochastic Gradient Descent,Noah Golmant;Nikita Vemuri;Zhewei Yao;Vladimir Feinberg;Amir Gholami;Kai Rothauge;Michael Mahoney;Joseph Gonzalez,noah.golmant@berkeley.edu;nikitavemuri@berkeley.edu;zheweiy@berkeley.edu;vladf@berkeley.edu;amirgh@berkeley.edu;kai.rothauge@berkeley.edu;mmahoney@stat.berkeley.edu;jegonzal@cs.berkeley.edu,5;8;5,3;4;3,Reject,0,6,0.0,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5;5;5;5;5,18;18;18;18;18;18;18;18,3;2
2710,2710,2710,2710,2710,2710,2710,2710,ICLR,2019,Three continual learning scenarios and a case for generative replay,Gido M. van de Ven;Andreas S. Tolias,gidovandeven@gmail.com;astolias@bcm.edu,4;4;6,4;4;5,Reject,0,6,2.0,yes,9/27/18,Baylor College of Medicine;Baylor College of Medicine,-1;-1,-1;-1,5
2711,2711,2711,2711,2711,2711,2711,2711,ICLR,2019,A More Globally Accurate Dimensionality Reduction Method Using Triplets,Ehsan Amid;Manfred K. Warmuth,eamid@ucsc.edu;manfred@ucsc.edu,5;6;6,4;5;3,Reject,0,8,0.0,yes,9/27/18,University of Southern California;University of Southern California,30;30,66;66,
2712,2712,2712,2712,2712,2712,2712,2712,ICLR,2019,Learning Kolmogorov Models for Binary Random Variables,Hadi Ghauch;Hossein S. Ghadikolaei;Mikael Skoglund;Carlo Fischione,ghauch@kth.se;hshokri@kth.se;skoglund@kth.se;carlofi@kth.se,5;5;8,4;2;4,Reject,0,5,0.0,yes,9/27/18,"KTH Royal Institute of Technology, Stockholm, Sweden;KTH Royal Institute of Technology, Stockholm, Sweden;KTH Royal Institute of Technology, Stockholm, Sweden;KTH Royal Institute of Technology, Stockholm, Sweden",140;140;140;140,173;173;173;173,
2713,2713,2713,2713,2713,2713,2713,2713,ICLR,2019,Laplacian Smoothing Gradient Descent,Stanley J. Osher;Bao Wang;Penghang Yin;Xiyang Luo;Minh Pham;Alex T. Lin,sjo@math.ucla.edu;wangbaonj@gmail.com;yph@g.ucla.edu;xylmath@gmail.com;minhrose@ucla.edu;atlin@math.ucla.edu,5;6;6,4;4;4,Reject,4,7,1.0,yes,9/27/18,"University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles;;University of California, Los Angeles;University of California, Los Angeles",20;20;20;-1;20;20,15;15;15;-1;15;15,9;8
2714,2714,2714,2714,2714,2714,2714,2714,ICLR,2019,Language Modeling Teaches You More Syntax than Translation Does: Lessons Learned Through Auxiliary Task Analysis,Kelly W. Zhang;Samuel R. Bowman,kellywzhang@seas.harvard.edu;bowman@nyu.edu,6;5;7,4;4;4,Reject,0,7,0.0,yes,9/27/18,Harvard University;New York University,39;26,6;27,3;6
2715,2715,2715,2715,2715,2715,2715,2715,ICLR,2019,Conscious Inference for Object Detection,Jiahuan Zhou;Nikolaos Karianakis;Ying Wu;Gang Hua,zhoujh09@gmail.com;nikolaos.karianakis@microsoft.com;yingwu@eecs.northwestern.edu;ganghua@gmail.com,4;6;4,4;4;5,Reject,0,3,0.0,yes,9/27/18,Northwestern University;Microsoft;Northwestern University;Wormpex AI Research,44;-1;44;-1,20;-1;20;-1,2
2716,2716,2716,2716,2716,2716,2716,2716,ICLR,2019,Learning Gibbs-regularized GANs with variational discriminator reparameterization,Nicholas Rhinehart;Anqi Liu;Kihyuk Sohn;Paul Vernaza,nrhineha@cs.cmu.edu;anqiliu@caltech.edu;ksohn@nec-labs.com;pvernaza@nec-labs.com,5;5;4,5;3;4,Reject,0,4,0.0,yes,9/27/18,Carnegie Mellon University;California Institute of Technology;NEC-Labs;NEC-Labs,1;140;-1;-1,24;3;-1;-1,5;4;10
2717,2717,2717,2717,2717,2717,2717,2717,ICLR,2019,DynCNN: An Effective Dynamic Architecture on Convolutional Neural Network for Surveillance Videos,De-Qin Gao;Ping-Chen Tsai;Shanq-Jang Ruan,b10113120@gmail.com;pctsainb@gmail.com;sjruan@mail.ntust.edu.tw,3;4;4,4;4;3,Reject,2,0,0.0,yes,9/27/18,Facebook;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology,-1;39;39,-1;44;44,
2718,2718,2718,2718,2718,2718,2718,2718,ICLR,2019,Expressiveness in Deep Reinforcement Learning,Xufang Luo;Qi Meng;Di He;Wei Chen;Yunhong Wang;Tie-Yan Liu,luoxufang@buaa.edu.cn;meq@microsoft.com;dihe@microsoft.com;wche@microsoft.com;yhwang@buaa.edu.cn;tyliu@microsoft.com,6;4;4,4;3;4,Reject,0,7,0.0,yes,9/27/18,Beihang University;Microsoft;Microsoft;Microsoft;Beihang University;Microsoft,115;-1;-1;-1;115;-1,658;-1;-1;-1;658;-1,
2719,2719,2719,2719,2719,2719,2719,2719,ICLR,2019,Exploring Curvature Noise in Large-Batch Stochastic Optimization,Yeming Wen;Kevin Luk;Maxime Gazeau;Guodong Zhang;Harris Chan;Jimmy Ba,ywen@cs.toronto.edu;kevin.luk@borealisai.com;maxime.gazeau@borealisai.com;gdzhang.cs@gmail.com;hchan@cs.toronto.edu;jba@cs.toronto.edu,5;6;5,4;4;5,Reject,0,20,0.0,yes,9/27/18,"Department of Computer Science, University of Toronto;Borealis AI;Borealis AI;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",18;-1;-1;18;18;18,22;-1;-1;22;22;22,8
2720,2720,2720,2720,2720,2720,2720,2720,ICLR,2019,Layerwise Recurrent Autoencoder for General Real-world Traffic Flow Forecasting,Peize Zhao;Danfeng Cai;Shaokun Zhang;Feng Chen;Zhemin Zhang;Cheng Wang;Jonathan Li,zhaopeize@sensetime.com;caidanfeng@sensetime.com;zhangshaokun@sensetime.com;chenfeng@xmu.edu.cn;zhangzhemin@xmu.edu.cn;cwang@xmu.edu.cn;junli@xmu.edu.cn,4;5;3,3;3;4,Reject,1,3,0.0,yes,9/27/18,SenseTime Group Limited;SenseTime Group Limited;SenseTime Group Limited;Xiamen University;Xiamen University;Xiamen University;Xiamen University,-1;-1;-1;62;62;62;62,-1;-1;-1;12;12;12;12,1
2721,2721,2721,2721,2721,2721,2721,2721,ICLR,2019,An experimental study of layer-level training speed and its impact on generalization,Simon Carbonnelle;Christophe De Vleeschouwer,simon.carbonnelle@uclouvain.be;christophe.devleeschouwer@uclouvain.be,6;5;5,3;4;2,Reject,0,10,0.0,yes,9/27/18,UCL;UCL,261;261,16;16,8
2722,2722,2722,2722,2722,2722,2722,2722,ICLR,2019,DEEP ADVERSARIAL FORWARD MODEL,Morgan Funtowicz;Tomi Silander;Arnaud Sors;Julien Perez,morgan.funtowicz@naverlabs.com;tomi.silander@naverlabs.com;arnaud.sors@naverlabs.com;julien.perez@naverlabs.com,4;4;4,5;5;4,Reject,0,0,0.0,yes,9/27/18,Naver Labs Europe;Naver Labs Europe;Naver Labs Europe;Naver Labs Europe,-1;-1;-1;-1,-1;-1;-1;-1,4
2723,2723,2723,2723,2723,2723,2723,2723,ICLR,2019,Learning models for visual 3D localization with implicit mapping,Dan Rosenbaum;Frederic Besse;Fabio Viola;Danilo J. Rezende;S. M. Ali Eslami,danro@google.com;fbesse@google.com;fviola@google.com;danilor@google.com;aeslami@google.com,6;7;5,3;4;4,Reject,0,7,0.0,yes,9/27/18,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,5
2724,2724,2724,2724,2724,2724,2724,2724,ICLR,2019,End-to-End Learning of Video Compression Using Spatio-Temporal Autoencoders,Jorge Pessoa;Helena Aidos;Pedro Tomás;Mário A. T. Figueiredo,jorge.pessoa@tecnico.ulisboa.pt;haidos@lx.it.pt;pedro.tomas@inesc-id.pt;mario.figueiredo@lx.it.pt,3;3;2,3;4;5,Reject,0,3,0.0,yes,9/27/18,"Instituto Superior Técnico;Instituto de Telecomunicações, Portugal;INESC-ID;Instituto de Telecomunicações, Portugal",478;478;-1;478,1103;1103;-1;1103,
2725,2725,2725,2725,2725,2725,2725,2725,ICLR,2019,Augment your batch: better training with larger batches,Elad Hoffer;Itay Hubara;Niv Giladi;Daniel Soudry,elad.hoffer@gmail.com;itayhubara@gmail.com;giladiniv@gmail.com;daniel.soudry@gmail.com,4;4;8,4;4;3,Reject,9,4,0.0,yes,9/27/18,Technion;;Technion;Technion,25;-1;25;25,327;-1;327;327,8
2726,2726,2726,2726,2726,2726,2726,2726,ICLR,2019,Unsupervised Neural Multi-Document Abstractive Summarization of Reviews,Eric Chu;Peter J. Liu,echu@mit.edu;peterjliu@google.com,4;5;9,4;4;4,Reject,1,6,0.0,yes,9/27/18,Massachusetts Institute of Technology;Google,2;-1,5;-1,
2727,2727,2727,2727,2727,2727,2727,2727,ICLR,2019,Automatic generation of object shapes with desired functionalities,Mihai Andries;Atabak Dehban;Jose Santos-Victor,mandries@isr.tecnico.ulisboa.pt;adehban@isr.tecnico.ulisboa.pt;jasv@isr.tecnico.ulisboa.pt,5;3;3,3;4;4,Reject,0,4,0.0,yes,9/27/18,Instituto Superior Técnico;Instituto Superior Técnico;Instituto Superior Técnico,478;478;478,1103;1103;1103,
2728,2728,2728,2728,2728,2728,2728,2728,ICLR,2019,Hybrid Policies Using Inverse Rewards for Reinforcement Learning,Yao Shi;Tian Xia;Guanjun Zhao;Xin Gao,yao.shi@huawei.com;xiatian14@huawei.com;zhaoguanjun1@huawei.com;gaoxin17@huawei.com,3;2;4,4;5;5,Reject,0,0,0.0,yes,9/27/18,Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.,-1;-1;-1;-1,-1;-1;-1;-1,
2729,2729,2729,2729,2729,2729,2729,2729,ICLR,2019,Sample Efficient Deep Neuroevolution in Low Dimensional Latent Space,Bin Zhou;Jiashi Feng,bin.zhou@u.nus.edu;elefjia@u.nus.edu,4;5;4,4;5;4,Reject,0,0,0.0,yes,9/27/18,National University of Singapore;National University of Singapore,16;16,22;22,5
2730,2730,2730,2730,2730,2730,2730,2730,ICLR,2019,Semi-supervised Learning with Multi-Domain Sentiment Word Embeddings,Ran Tian;Yash Agrawal;Kento Watanabe;Hiroya Takamura,robin.tianran@gmail.com;yashagrawal@iitkgp.ac.in;kento.watanabe@aist.go.jp;takamura.hiroya@aist.go.jp,6;6;6,3;3;3,Reject,0,3,0.0,yes,9/27/18,AIST;Indian Institute of Technology Kharagpur;AIST;AIST,-1;261;-1;-1,-1;506;-1;-1,3
2731,2731,2731,2731,2731,2731,2731,2731,ICLR,2019,Teacher Guided Architecture Search,Pouya Bashivan;Mark Tensen;James J DiCarlo,bashivan@mit.edu;mark.tensen@student.uva.nl;dicarlo@mit.edu,6;6;5,4;4;4,Reject,0,8,0.0,yes,9/27/18,Massachusetts Institute of Technology;University of Amsterdam;Massachusetts Institute of Technology,2;169;2,5;59;5,
2732,2732,2732,2732,2732,2732,2732,2732,ICLR,2019,A Convergent Variant of the Boltzmann Softmax Operator in Reinforcement Learning,Ling Pan;Qingpeng Cai;Qi Meng;Wei Chen;Tie-Yan Liu,v-lip@microsoft.com;cqp14@mails.tsinghua.edu.cn;v-qimeng@microsoft.com;wche@microsoft.com;tie-yan.liu@microsoft.com,4;4;5,5;4;4,Reject,0,6,2.0,yes,9/27/18,Microsoft;Tsinghua University;Microsoft;Microsoft;Microsoft,-1;8;-1;-1;-1,-1;30;-1;-1;-1,1;9
2733,2733,2733,2733,2733,2733,2733,2733,ICLR,2019,GraphSeq2Seq: Graph-Sequence-to-Sequence for Neural Machine Translation,Guoshuai Zhao;Jun Li;Lu Wang;Xueming Qian;Yun Fu,zgs2012@stu.xjtu.edu.cn;junl.mldl@gmail.com;luwang@ccs.neu.edu;qianxm@mail.xjtu.edu.cn;yunfu@ece.neu.edu,6;6;6,5;4;3,Reject,0,5,0.0,yes,9/27/18,Xi'an Jiaotong University;Massachusetts Institute of Technology;Northeastern University;Xi'an Jiaotong University;Northeastern University,478;2;16;478;16,565;5;839;565;839,3;10
2734,2734,2734,2734,2734,2734,2734,2734,ICLR,2019,Understanding the Effectiveness of Lipschitz-Continuity in Generative Adversarial Nets,Zhiming Zhou;Yuxuan Song;Lantao Yu;Hongwei Wang;Weinan Zhang;Zhihua Zhang;Yong Yu,heyohai@apex.sjtu.edu.cn;songyuxuan@apex.sjtu.edu.cn;yulantao@apex.sjtu.edu.cn;wanghongwei55@gmail.com;wnzhang@sjtu.edu.cn;zhzhang@math.pku.edu.cn;yyu@apex.sjtu.edu.cn,6;4;5,4;4;4,Reject,0,21,1.0,yes,9/27/18,Shanghai Jiao Tong University;Shanghai Jiao Tong University;Shanghai Jiao Tong University;Shanghai Jiao Tong University;Shanghai Jiao Tong University;Peking University;Shanghai Jiao Tong University,52;52;52;52;52;24;52,188;188;188;188;188;27;188,5
2735,2735,2735,2735,2735,2735,2735,2735,ICLR,2019,Hierarchically-Structured Variational Autoencoders for Long Text Generation,Dinghan Shen;Asli Celikyilmaz;Yizhe Zhang;Liqun Chen;Xin Wang;Lawrence Carin,dinghan.shen@duke.edu;asli@ieee.org;yizhe.zhang@microsoft.com;liqun.chen@duke.edu;xwang@cs.ucsb.edu;lcarin@duke.edu,5;5;7,4;4;4,Reject,0,8,0.0,yes,9/27/18,Duke University;Microsoft;Microsoft;Duke University;UC Santa Barbara;Duke University,44;-1;-1;44;37;44,17;-1;-1;17;53;17,3;5
2736,2736,2736,2736,2736,2736,2736,2736,ICLR,2019,Learning Representations in Model-Free Hierarchical Reinforcement Learning,Jacob Rafati;David Noelle,jrafatiheravi@ucmerced.edu;dnoelle@ucmerced.edu,5;4;3,4;4;5,Reject,0,0,0.0,yes,9/27/18,University of California at Merced;University of California at Merced,478;478,1103;1103,
2737,2737,2737,2737,2737,2737,2737,2737,ICLR,2019,Towards Decomposed Linguistic Representation with Holographic Reduced Representation,Jiaming Luo;Yuan Cao;Yonghui Wu,j_luo@csail.mit.edu;yuancao@google.com;yonghui@google.com,5;5;6,4;4;3,Reject,0,12,1.0,yes,9/27/18,Massachusetts Institute of Technology;Google;Google,2;-1;-1,5;-1;-1,3
2738,2738,2738,2738,2738,2738,2738,2738,ICLR,2019,Quality Evaluation of GANs Using Cross Local Intrinsic Dimensionality,Sukarna Barua;Xingjun Ma;Sarah Monazam Erfani;Michael Houle;James Bailey,sukarnab@student.unimelb.edu.au;xingjun.ma@unimelb.edu.au;sarah.erfani@unimelb.edu.au;meh@nii.ac.jp;baileyj@unimelb.edu.au,6;4;6,5;3;4,Reject,0,9,0.0,yes,9/27/18,The University of Melbourne;The University of Melbourne;The University of Melbourne;Meiji University;The University of Melbourne,123;123;123;478;123,32;32;32;334;32,5;4
2739,2739,2739,2739,2739,2739,2739,2739,ICLR,2019,Deep processing of structured data,Łukasz Maziarka;Marek Śmieja;Aleksandra Nowak;Jacek Tabor;Łukasz Struski;Przemysław Spurek,l.maziarka@gmail.com;marek.smieja@uj.edu.pl;aknoow@gmail.com;jacek.tabor@uj.edu.pl;lukasz.struski@uj.edu.pl;przemyslaw.spurek@uj.edu.pl,4;4;4,3;4;3,Reject,0,1,0.0,yes,9/27/18,Ardigen;Jagiellonian University;;Jagiellonian University;Jagiellonian University;Jagiellonian University,-1;478;-1;478;478;478,-1;695;-1;695;695;695,10
2740,2740,2740,2740,2740,2740,2740,2740,ICLR,2019,Predictive Uncertainty through Quantization,Bastiaan S. Veeling;Rianne van den Berg;Max Welling,basveeling@gmail.com;welling.max@gmail.com,5;4;5,3;4;4,Reject,0,3,0.0,yes,9/27/18,University of Amsterdam;University of California - Irvine,169;35,59;99,
2741,2741,2741,2741,2741,2741,2741,2741,ICLR,2019,TherML: The Thermodynamics of Machine Learning,Alexander A. Alemi;Ian Fischer,alemi@google.com;iansf@google.com,7;3;5,3;4;3,Reject,0,3,0.0,yes,9/27/18,Google;Google,-1;-1,-1;-1,
2742,2742,2742,2742,2742,2742,2742,2742,ICLR,2019,Countering Language Drift via Grounding,Jason Lee;Kyunghyun Cho;Douwe Kiela,jason@cs.nyu.edu;kyunghyun.cho@nyu.edu;dkiela@fb.com,6;6;6,3;4;4,Reject,0,10,9.0,yes,9/27/18,New York University;New York University;Facebook,26;26;-1,27;27;-1,3
2743,2743,2743,2743,2743,2743,2743,2743,ICLR,2019,Amortized Context Vector Inference for Sequence-to-Sequence Networks,Sotirios Chatzis;Kyriacos Tolias;Aristotelis Charalampous,sotirios.chatzis@cut.ac.cy;k.v.tolias@edu.cut.ac.cy;aristotelis.charalampous@edu.cut.ac.cy,6;6;5,3;3;4,Reject,0,4,0.0,yes,9/27/18,Cyprus University of Technology;Cyprus University of Technology;Cyprus University of Technology,261;261;261,354;354;354,3;8
2744,2744,2744,2744,2744,2744,2744,2744,ICLR,2019,Coverage and Quality Driven Training of Generative Image Models,Thomas LUCAS;Konstantin SHMELKOV;Karteek ALAHARI;Cordelia SCHMID;Jakob VERBEEK,thomas.lucas@inria.fr;konstantin.shmelkov@inria.fr;karteek.alahari@inria.fr;cordelia.schmid@inria.fr,7;5;4,4;4;5,Reject,0,19,0.0,yes,9/27/18,INRIA;INRIA;INRIA;INRIA,-1;-1;-1;-1,-1;-1;-1;-1,5;4;8
2745,2745,2745,2745,2745,2745,2745,2745,ICLR,2019,Switching Linear Dynamics for Variational Bayes Filtering,Philip Becker-Ehmck;Jan Peters;Patrick van der Smagt,philip.becker-ehmck@volkswagen.de;peters@ias.tu-darmstadt.de;smagt@volkswagen.de,6;4;7,3;4;5,Reject,0,5,0.0,yes,9/27/18,"Data Lab, Volkswagen Group;TU Darmstadt;Data Lab, Volkswagen Group",-1;65;-1,-1;244;-1,5;11
2746,2746,2746,2746,2746,2746,2746,2746,ICLR,2019,SupportNet: solving catastrophic forgetting in class incremental learning with support data,Yu Li;Zhongxiao Li;Lizhong Ding;Yijie Pan;Chao Huang;Yuhui Hu;Wei Chen;Xin Gao,yu.li@kaust.edu.sa;zhongxiao.li@kaust.edu.sa;lizhong.ding@inceptioniai.org;pyj@nbicc.com;chuang@ict.ac.cn;huyh@sustc.edu.cn;chenw@sustc.edu.cn;xin.gao@kaust.edu.sa,5;6;4,4;4;4,Reject,0,12,0.0,yes,9/27/18,"KAUST;KAUST;Inception Institute of Artificial Intelligence;Nbicc;Institute of Computing Technology, Chinese Academy of Sciences;University of Science and Technology of China;University of Science and Technology of China;KAUST",123;123;-1;-1;62;478;478;123,1103;1103;-1;-1;1103;132;132;1103,
2747,2747,2747,2747,2747,2747,2747,2747,ICLR,2019,Unsupervised Learning  of Sentence Representations Using Sequence Consistency,Siddhartha Brahma,sidbrahma@gmail.com,7;5;5,4;4;4,Reject,0,8,0.0,yes,9/27/18,International Business Machines,-1,-1,3;6
2748,2748,2748,2748,2748,2748,2748,2748,ICLR,2019,Recurrent Kalman Networks: Factorized Inference in High-Dimensional Deep Feature Spaces,Philipp Becker;Harit Pandya;Gregor H.W. Gebhardt;Cheng Zhao;Gerhard Neumann,philippbecker93@googlemail.com;hpandya@lincoln.ac.uk;gebhardt@ias.tu-darmstadt.de;irobotcheng@gmail.com;gneumann@lincoln.ac.uk,6;6;6,4;4;3,Reject,0,3,0.0,yes,9/27/18,TU Darmstadt;University of Lincoln;TU Darmstadt;Birmingham University;University of Lincoln,65;478;65;123;478,244;757;244;141;757,5
2749,2749,2749,2749,2749,2749,2749,2749,ICLR,2019,Shallow Learning For Deep Networks,Eugene Belilovsky;Michael Eickenberg;Edouard Oyallon,belilove@iro.umontreal.ca;michael.eickenberg@berkeley.edu;edouard.oyallon@centralesupelec.fr,6;5;7,4;4;4,Reject,5,20,1.0,yes,9/27/18,University of Montreal;University of California Berkeley;CentraleSupelec,123;5;478,108;18;452,
2750,2750,2750,2750,2750,2750,2750,2750,ICLR,2019,A Solution to China Competitive Poker Using Deep Learning,Zhenxing Liu;Maoyu Hu;Zhangfei Zhang,liuzx@smzy.cc;humaoyu@smzy.cc;zzf@smzy.cc,3;2,4;3,Reject,28,7,1.0,yes,9/27/18,;;,-1;-1;-1,-1;-1;-1,
2751,2751,2751,2751,2751,2751,2751,2751,ICLR,2019,A NOVEL VARIATIONAL FAMILY FOR HIDDEN NON-LINEAR MARKOV MODELS,Daniel Hernandez Diaz;Antonio Khalil Moretti;Ziqiang Wei;Shreya Saxena;John Cunningham;Liam Paninski,dh2832@columbia.edu;amoretti@cs.columbia.edu;weiz@janelia.hhmi.org;ss5513@columbia.edu;jpcunni@gmail.com;liam.paninski@gmail.com,5;8;6,3;5;3,Reject,0,6,0.0,yes,9/27/18,Columbia University;Columbia University;HHMI Janelia Research Campus;Columbia University;;,15;15;-1;15;-1;-1,14;14;-1;14;-1;-1,
2752,2752,2752,2752,2752,2752,2752,2752,ICLR,2019,GenEval: A Benchmark Suite for Evaluating Generative Models,Anton Bakhtin;Arthur Szlam;Marc'Aurelio Ranzato,yolo@fb.com;aszlam@fb.com;ranzato@fb.com,5;5;6,3;4;4,Reject,0,11,0.0,yes,9/27/18,Facebook;Facebook;Facebook,-1;-1;-1,-1;-1;-1,5
2753,2753,2753,2753,2753,2753,2753,2753,ICLR,2019,Making Convolutional Networks Shift-Invariant Again,Richard Zhang,rich.zhang@eecs.berkeley.edu,6;5;5,4;4;4,Reject,1,7,1.0,yes,9/27/18,University of California Berkeley,5,18,
2754,2754,2754,2754,2754,2754,2754,2754,ICLR,2019,Deconfounding Reinforcement Learning in Observational Settings,Chaochao Lu;José Miguel Hernández Lobato,cl641@cam.ac.uk;jmh233@cam.ac.uk,4;4;2,3;4;4,Reject,19,18,0.0,yes,9/27/18,University of Cambridge;University of Cambridge,71;71,2;2,
2755,2755,2755,2755,2755,2755,2755,2755,ICLR,2019,Incremental Hierarchical Reinforcement Learning with Multitask LMDPs,Adam C Earle;Andrew M Saxe;Benjamin Rosman,adamchristopherearle@gmail.com;andrew.saxe@psy.ox.ac.uk;benjros@gmail.com,3;4;5,4;4;4,Reject,0,0,0.0,yes,9/27/18,;University of Oxford;University of the Witwatersrand,-1;50;478,-1;1;293,
2756,2756,2756,2756,2756,2756,2756,2756,ICLR,2019,Exploration by Uncertainty in Reward Space,Wei-Yang Qu;Yang Yu;Tang-Jie Lv;Ying-Feng Chen;Chang-Jie Fan,nju_qwy@163.com;yuy@nju.edu.cn;hzlvtangjie@corp.netease.com;chenyingfeng1@corp.netease.com;fanchangjie@corp.netease.com,5;5;3,3;2;5,Reject,0,0,0.0,yes,9/27/18,Zhejiang University;Zhejiang University;Corp.netease;University of Science and Technology of China;Corp.netease,57;57;-1;478;-1,177;177;-1;132;-1,
2757,2757,2757,2757,2757,2757,2757,2757,ICLR,2019,Shrinkage-based Bias-Variance Trade-off for Deep Reinforcement Learning,Yihao Feng;Hao Liu;Jian Peng;Qiang Liu,yihao@cs.utexas.edu;uestcliuhao@gmail.com;jianpeng@illinois.edu;lqiang@cs.utexas.edu,5;4;4,3;2;4,Reject,0,4,0.0,yes,9/27/18,"University of Texas, Austin;University of California Berkeley;University of Illinois, Urbana Champaign;University of Texas, Austin",22;5;3;22,49;18;37;49,
2758,2758,2758,2758,2758,2758,2758,2758,ICLR,2019,Improving On-policy Learning with Statistical Reward Accumulation,Yubin Deng;Ke Yu;Dahua Lin;Xiaoou Tang;Chen Change Loy,dy015@ie.cuhk.edu.hk;yk017@ie.cuhk.edu.hk;dhlin@ie.cuhk.edu.hk;xtang@ie.cuhk.edu.hk;ccloy@ieee.org,4;5,3;3,Reject,0,5,0.0,yes,9/27/18,The Chinese University of Hong Kong;The Chinese University of Hong Kong;The Chinese University of Hong Kong;The Chinese University of Hong Kong;,57;57;57;57;-1,40;40;40;40;-1,
2759,2759,2759,2759,2759,2759,2759,2759,ICLR,2019,Guided Exploration in Deep Reinforcement Learning,Sahisnu Mazumder;Bing Liu;Shuai Wang;Yingxuan Zhu;Xiaotian Yin;Lifeng Liu;Jian Li;Yongbing Huang,sahisnumazumder@gmail.com;liub@cs.uic.edu;gshuaishuai@gmail.com;yingxuan.zhu@huawei.com;xiaotian.yin@huawei.com;lifeng.liu1@huawei.com;jian.li1@huawei.com;huangyongbing@huawei.com,7;5;3,5;4;3,Reject,0,4,0.0,yes,9/27/18,"University of Illinois, Chicago;University of Illinois, Chicago;;Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.",57;57;-1;-1;-1;-1;-1;-1,255;255;-1;-1;-1;-1;-1;-1,
2760,2760,2760,2760,2760,2760,2760,2760,ICLR,2019,Probabilistic Knowledge Graph Embeddings,Farnood Salehi;Robert Bamler;Stephan Mandt,farnood.salehi@epfl.ch;robert.bamler@gmail.com;stephan.mandt@gmail.com,5;6;5,2;3;3,Reject,0,3,0.0,yes,9/27/18,"Swiss Federal Institute of Technology Lausanne;University of California, Irvine;University of California, Irvine",478;35;35,38;99;99,11;1;10
2761,2761,2761,2761,2761,2761,2761,2761,ICLR,2019,Accelerated Value Iteration via Anderson Mixing,Yujun Li;Chengzhuo Ni;Guangzeng Xie;Wenhao Yang;Shuchang Zhou;Zhihua Zhang,liyujun145@gmail.com;hzxsncz@pku.edu.cn;smsxgz@pku.edu.cn;yangwenhaosms@pku.edu.cn;zsc@megvii.com;zhzhang@math.pku.edu.cn,7;4;4,4;4;3,Reject,0,8,0.0,yes,9/27/18,Shanghai Jiao Tong University;Peking University;Peking University;Peking University;Megvii Technology Inc.;Peking University,52;24;24;24;-1;24,188;27;27;27;-1;27,
2762,2762,2762,2762,2762,2762,2762,2762,ICLR,2019,Safe Policy Learning from Observations,Elad Sarafian;Aviv Tamar;Sarit Kraus,elad.sarafian@gmail.com;avivt@berkeley.edu;sarit@cs.biu.ac.il,5;5;5,4;4;3,Reject,0,5,0.0,yes,9/27/18,Bar Ilan University;University of California Berkeley;Bar Ilan University,95;5;95,456;18;456,
2763,2763,2763,2763,2763,2763,2763,2763,ICLR,2019,Importance Resampling for Off-policy Policy Evaluation,Matthew Schlegel;Wesley Chung;Daniel Graves;Martha White,mkschleg@ualberta.ca;wchung@ualberta.ca;daniel.graves@huawei.com;whitem@ualberta.ca,6;5;5,4;3;3,Reject,0,10,0.0,yes,9/27/18,University of Alberta;University of Alberta;Huawei Technologies Ltd.;University of Alberta,99;99;-1;99,119;119;-1;119,
2764,2764,2764,2764,2764,2764,2764,2764,ICLR,2019,Mimicking actions is a good strategy for beginners: Fast Reinforcement Learning with Expert Action Sequences,Tharun Medini;Anshumali Shrivastava,tharun.medini@rice.edu;anshumali@rice.edu,5;5;6,3;2;4,Reject,0,5,0.0,yes,9/27/18,Rice University;Rice University,85;85,86;86,
2765,2765,2765,2765,2765,2765,2765,2765,ICLR,2019,Convergent Reinforcement Learning with Function Approximation: A Bilevel Optimization Perspective,Zhuoran Yang;Zuyue Fu;Kaiqing Zhang;Zhaoran Wang,zy6@princeton.edu;zuyuefu2022@u.northwestern.edu;kzhang66@illinois.edu;zhaoranwang@gmail.com,6;5;5;6,4;3;4;4,Reject,0,10,0.0,yes,9/27/18,"Princeton University;Northwestern University;University of Illinois, Urbana Champaign;Northwestern University",30;44;3;44,7;20;37;20,
2766,2766,2766,2766,2766,2766,2766,2766,ICLR,2019,Unsupervised Emergence of Spatial Structure from Sensorimotor Prediction,Alban Laflaquière;Michael Garcia Ortiz,alban.laflaquiere@gmail.com;mgarciaortiz@softbankrobotics.com,6;7;4,3;3;4,Reject,0,21,2.0,yes,9/27/18,SoftBank Robotics Europe;SoftBank Robotics Europe,-1;-1,-1;-1,
2767,2767,2767,2767,2767,2767,2767,2767,ICLR,2019,Where Off-Policy Deep Reinforcement Learning Fails,Scott Fujimoto;David Meger;Doina Precup,scott.fujimoto@mail.mcgill.ca;david.meger@mcgill.ca;dprecup@cs.mcgill.ca,7;5;5,4;4;3,Reject,5,10,0.0,yes,9/27/18,McGill University;McGill University;McGill University,85;85;85,42;42;42,
2768,2768,2768,2768,2768,2768,2768,2768,ICLR,2019,Exploration by random network distillation,Yuri Burda;Harrison Edwards;Amos Storkey;Oleg Klimov,yburda@openai.com;h.l.edwards@sms.ed.ac.uk;a.storkey@ed.ac.uk;oleg@openai.com,7;4;9;10,4;4;5;4,Accept (Poster),0,16,0.0,yes,9/27/18,OpenAI;University of Edinburgh;University of Edinburgh;OpenAI,-1;33;33;-1,-1;27;27;-1,
2769,2769,2769,2769,2769,2769,2769,2769,ICLR,2019,Learning State Representations in Complex Systems with Multimodal Data,Pavel Solovev;Vladimir Aliev;Pavel Ostyakov;Gleb Sterkin;Elizaveta Logacheva;Stepan Troeshestov;Roman Suvorov;Anton Mashikhin;Oleg Khomenko;Sergey I. Nikolenko,pavel.solovev.ilich@gmail.com;vldr.aliev@gmail.com;pavelosta@gmail.com;sterkin.gleb@gmail.com;elimohl@gmail.com;troeshust96@gmail.com;windj007@gmail.com;antonagoo@gmail.com;olegkhomenkoru@gmail.com;snikolenko@gmail.com,6;6;5,3;3;4,Reject,0,5,0.0,yes,9/27/18,;;Samsung;Samsung;;Lomonosov Moscow State University;Samsung;;;,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,
2770,2770,2770,2770,2770,2770,2770,2770,ICLR,2019,EMI: Exploration with Mutual Information Maximizing State and Action Embeddings,Hyoungseok Kim;Jaekyeom Kim;Yeonwoo Jeong;Sergey Levine;Hyun Oh Song,harry2636@mllab.snu.ac.kr;jaekyeom@mllab.snu.ac.kr;yeonwoo@mllab.snu.ac.kr;svlevine@eecs.berkeley.edu;hyunoh@snu.ac.kr,5;7;7,4;4;3,Reject,1,9,0.0,yes,9/27/18,Seoul National University;Seoul National University;Seoul National University;University of California Berkeley;Seoul National University,41;41;41;5;41,74;74;74;18;74,5
2771,2771,2771,2771,2771,2771,2771,2771,ICLR,2019,P^2IR: Universal Deep Node Representation via Partial Permutation Invariant Set Functions,Shupeng Gui;Xiangliang Zhang;Shuang Qiu;Mingrui Wu;Jieping Ye;Ji Liu,sgui2@ur.rochester.edu;xiangliang.zhang@kaust.edu.sa;qiush@umich.edu;mingrui.wu@alibaba-inc.com;jieping@gmail.com;ji.liu.uwisc@gmail.com,4;5;7;5,4;3;4;5,Reject,0,0,0.0,yes,9/27/18,University of Rochester;KAUST;University of Michigan;Alibaba Group;;University of Rochester,106;123;8;-1;-1;106,153;1103;21;-1;-1;153,10
2772,2772,2772,2772,2772,2772,2772,2772,ICLR,2019,Auto-Encoding Knockoff Generator for FDR  Controlled Variable Selection,Ying Liu;Cheng Zheng,summeryingl@gmail.com;zzhengccheng@gmail.com,3;4;6,4;4;3,Reject,0,9,0.0,yes,9/27/18,Medical College of Wisconsin;,-1;-1,-1;-1,
2773,2773,2773,2773,2773,2773,2773,2773,ICLR,2019,Learning Cross-Lingual Sentence Representations via a Multi-task Dual-Encoder Model,Muthuraman Chidambaram;Yinfei Yang;Daniel Cer;Steve Yuan;Yun-Hsuan Sung;Brian Strope;Ray Kurzweil,mc4xf@virginia.edu;yinfeiy@google.com;cer@google.com;steveyuan@google.com;yhsung@google.com;bps@google.com;raykurzweil@google.com,7;4;6,4;4;5,Reject,0,5,0.0,yes,9/27/18,University of Virginia;Google;Google;Google;Google;Google;Google,65;-1;-1;-1;-1;-1;-1,113;-1;-1;-1;-1;-1;-1,3;6
2774,2774,2774,2774,2774,2774,2774,2774,ICLR,2019,Graph Generation via Scattering,Dongmian Zou;Gilad Lerman,dzou@umn.edu;lerman@umn.edu,4;4;4,4;4;3,Reject,0,6,0.0,yes,9/27/18,"University of Minnesota, Minneapolis;University of Minnesota, Minneapolis",57;57,56;56,5;10
2775,2775,2775,2775,2775,2775,2775,2775,ICLR,2019,Context Mover's Distance & Barycenters: Optimal transport of contexts for building representations,Sidak Pal Singh;Andreas Hug;Aymeric Dieuleveut;Martin Jaggi,sidak.singh@epfl.ch;andreas.hug@epfl.ch;aymeric.dieuleveut@epfl.ch;martin.jaggi@epfl.ch,7;4;6,4;4;4,Reject,0,14,1.0,yes,9/27/18,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne,478;478;478;478,38;38;38;38,
2776,2776,2776,2776,2776,2776,2776,2776,ICLR,2019,Pixel Chem: A Representation for Predicting Material Properties with Neural Network,Shuqian Ye;Yanheng Xu;Jiechun Liang;Hao Xu;Shuhong Cai;Shixin Liu;Xi Zhu,115010269@link.cuhk.edu.cn;115010252@link.cuhk.edu.cn;116010125@link.cuhk.edu.cn;115010250@link.cuhk.edu.cn;115010111@link.cuhk.edu.cn;115010194@link.cuhk.edu.cn;zhuxi@cuhk.edu.cn,3;1;3,3;5;5,Reject,0,7,0.0,yes,9/27/18,Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University,8;8;8;8;8;8;8,30;30;30;30;30;30;30,
2777,2777,2777,2777,2777,2777,2777,2777,ICLR,2019,Using Deep Siamese Neural Networks to Speed up Natural Products Research,Nicholas Roberts;Poornav S. Purushothama;Vishal T. Vasudevan;Siddarth Ravichandran;Chen Zhang;William H. Gerwick;Garrison W. Cottrell,n3robert@ucsd.edu;poornavsargoor@gmail.com;vthanvan@eng.ucsd.edu;s2ravich@eng.ucsd.edu;beowulf.zc@gmail.com;wgerwick@ucsd.edu;gary@ucsd.edu,4;3;4,4;2;4,Reject,0,0,0.0,yes,9/27/18,"University of California, San Diego;;University of California, San Diego;University of California, San Diego;;University of California, San Diego;University of California, San Diego",11;-1;11;11;-1;11;11,31;-1;31;31;-1;31;31,
2778,2778,2778,2778,2778,2778,2778,2778,ICLR,2019,Modeling Dynamics of Biological Systems with Deep Generative Neural Networks,Scott Gigante;David van Dijk;Kevin R. Moon;Alexander Strzalkowski;Katie Ferguson;Guy Wolf;Smita Krishnaswamy,scott.gigante@yale.edu;david.vandijk@yale.edu;kevin.moon@usu.edu;alexander.strzalkowski@yale.edu;katie.ferguson@yale.edu;jess.cardin@yale.edu;guy.wolf@yale.edu;smita.krishnaswamy@yale.edu,6;4;3,2;5;5,Reject,0,0,0.0,yes,9/27/18,Yale University;Yale University;SUN YAT-SEN UNIVERSITY;Yale University;Yale University;Yale University;Yale University;Yale University,62;62;478;62;62;62;62;62,12;12;352;12;12;12;12;12,5
2779,2779,2779,2779,2779,2779,2779,2779,ICLR,2019,Zero-shot Learning for Speech Recognition with Universal Phonetic Model,Xinjian Li;Siddharth Dalmia;David R. Mortensen;Florian Metze;Alan W Black,xinjianl@andrew.cmu.edu;sdalmia@cs.cmu.edu;dmortens@cs.cmu.edu;fmetze@cs.cmu.edu;awb@cs.cmu.edu,7;5;4,4;4;4,Reject,0,9,0.0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1;1,24;24;24;24;24,6
2780,2780,2780,2780,2780,2780,2780,2780,ICLR,2019,CAML: Fast Context Adaptation via Meta-Learning,Luisa M Zintgraf;Kyriacos Shiarlis;Vitaly Kurin;Katja Hofmann;Shimon Whiteson,lmzintgraf@gmail.com;kyriacos@latentlogic.com;vitaly.kurin@eng.ox.ac.uk;katja.hofmann@microsoft.com;shimon.whiteson@cs.ox.ac.uk,6;4;6;6,2;5;4;2,Reject,2,7,0.0,yes,9/27/18,University of Oxford;University of Amsterdam;University of Oxford;Microsoft;University of Oxford,50;169;50;-1;50,1;59;1;-1;1,6
2781,2781,2781,2781,2781,2781,2781,2781,ICLR,2019,Interactive Agent Modeling by Learning to Probe,Tianmin Shu;Caiming Xiong;Ying Nian Wu;Song-Chun Zhu,tianmin.shu@ucla.edu;cxiong@salesforce.com;ywu@stat.ucla.edu;sczhu@stat.ucla.edu,6;6;6;6,4;4;3;4,Reject,0,9,0.0,yes,9/27/18,"University of California, Los Angeles;SalesForce.com;University of California, Los Angeles;University of California, Los Angeles",20;-1;20;20,15;-1;15;15,
2782,2782,2782,2782,2782,2782,2782,2782,ICLR,2019,Unified recurrent network for many feature types,Alexander Stec;Diego Klabjan;Jean Utke,stec@u.northwestern.edu;d-klabjan@northwestern.edu;jutke@allstate.com,4;6;4;7,4;3;4;2,Reject,0,8,0.0,yes,9/25/19,Northwestern University;Northwestern University;Allstate,44;44;-1,20;20;-1,
2783,2783,2783,2783,2783,2783,2783,2783,ICLR,2019,Differential Equation Networks,MohamadAli Torkamani;Phillip Wallis,torkamani@gmail.com;wallis.phillip@gmail.com,4;5;5,4;3;4,Reject,0,0,0.0,yes,9/27/18,;Oregon Health and Science University,-1;478,-1;272,
2784,2784,2784,2784,2784,2784,2784,2784,ICLR,2019,Adversarial Audio Super-Resolution with Unsupervised Feature Losses,Sung Kim;Visvesh Sathe,sungmk@umich.edu;sathe@uw.edu,4;5;6,4;4;4,Reject,0,6,1.0,yes,9/27/18,"University of Michigan;University of Washington, Seattle",8;6,21;25,5;4
2785,2785,2785,2785,2785,2785,2785,2785,ICLR,2019,Learning powerful policies and better dynamics models by encouraging consistency,Shagun Sodhani;Anirudh Goyal;Tristan Deleu;Yoshua Bengio;Jian Tang,sshagunsodhani@gmail.com;anirudhgoyal9119@gmail.com;tristan.deleu@gmail.com;yoshua.bengio@mila.quebec;tangjianpku@gmail.com,2;5;3,4;3;5,Reject,0,23,0.0,yes,9/27/18,University of Montreal;University of Montreal;University of Montreal;University of Montreal;HEC Montreal,123;123;123;123;-1,108;108;108;108;-1,
2786,2786,2786,2786,2786,2786,2786,2786,ICLR,2019,Selectivity metrics can overestimate the selectivity of units: a case study on AlexNet,Ella M. Gale;Anh Nguyen;Ryan Blything;Nicholas Martin;Jeffrey S. Bowers,ella.gale@gmail.com;anhnguyen@auburn.edu;ryan.blything@bristol.ac.uk;nm13850@bristol.ac.uk;j.bowers@bristol.ac.uk,5;6;3,3;3;5,Reject,0,6,0.0,yes,9/27/18,University of Bristol;Auburn University;University of Bristol;University of Bristol;University of Bristol,123;386;123;123;123,76;652;76;76;76,
2787,2787,2787,2787,2787,2787,2787,2787,ICLR,2019,Decoupling feature extraction from policy learning: assessing benefits of state representation learning in goal based robotics,Antonin Raffin;Ashley Hill;René Traoré;Timothée Lesort;Natalia Díaz-Rodríguez;David Filliat,antonin.raffin@ensta-paristech.fr;ashley.hill@u-psud.fr;krb.traore@protonmail.com;timothee.lesort@ensta-paristech.fr;diaz.rodriguez.natalia@gmail.com;david.filliat@ensta-paristech.fr,5;3;4,4;4;4,Reject,0,4,0.0,yes,9/27/18,"ENSTA ParisTech;UPSud/INRIA University Paris-Saclay;Computer Science Lab  - Pierre and Marie Curie University, Paris, France;ENSTA ParisTech;ENSTA ParisTech;ENSTA ParisTech",478;478;478;478;478;478,1103;1103;123;1103;1103;1103,
2788,2788,2788,2788,2788,2788,2788,2788,ICLR,2019,Transferrable End-to-End Learning for Protein Interface Prediction,Raphael J. L. Townshend;Rishi Bedi;Ron O. Dror,raphael@cs.stanford.edu;rbedi@cs.stanford.edu;rondror@cs.stanford.edu,5;5;5,4;3;3,Reject,0,4,0.0,yes,9/27/18,Stanford University;Stanford University;Stanford University,4;4;4,3;3;3,6
2789,2789,2789,2789,2789,2789,2789,2789,ICLR,2019,Visual Imitation Learning with Recurrent Siamese Networks,Glen Berseth;Christopher J. Pal,gberseth@gmail.com;christopher.pal@polymtl.ca,4;4;5,4;3;4,Reject,0,6,0.0,yes,9/27/18,University of British Columbia;Polytechnique Montreal,36;386,34;108,
2790,2790,2790,2790,2790,2790,2790,2790,ICLR,2019,Self-Supervised Generalisation with Meta Auxiliary Learning,Shikun Liu;Edward Johns;Andrew Davison,shikun.liu17@imperial.ac.uk;e.johns@imperial.ac.uk;a.davison@imperial.ac.uk,4;4;6,3;4;4,Reject,0,3,0.0,yes,9/27/18,Imperial College London;Imperial College London;Imperial College London,72;72;72,8;8;8,
2791,2791,2791,2791,2791,2791,2791,2791,ICLR,2019,GRAPH TRANSFORMATION POLICY NETWORK FOR CHEMICAL REACTION PREDICTION,Kien Do;Truyen Tran;Svetha Venkatesh,dkdo@deakin.edu.au;truyen.tran@deakin.edu.au;svetha.venkatesh@deakin.edu.au,6;5;5,5;4;4,Reject,0,11,0.0,yes,9/27/18,Deakin University;Deakin University;Deakin University,478;478;478,334;334;334,10
2792,2792,2792,2792,2792,2792,2792,2792,ICLR,2019,DiffraNet: Automatic Classification of Serial Crystallography Diffraction Patterns,Artur Souza;Leonardo B. Oliveira;Sabine Hollatz;Matt Feldman;Kunle Olukotun;James M. Holton;Aina E. Cohen;Luigi Nardi,arturluis@dcc.ufmg.br;leob@dcc.ufmg.br;shollatz@slac.stanford.edu;mattfel@stanford.edu;kunle@stanford.edu;jmholton@slac.stanford.edu;acohen@slac.stanford.edu;lnardi@stanford.edu,5;3;8,4;5;4,Reject,0,9,0.0,yes,9/27/18,Universidade Federal de Minas Gerais;Universidade Federal de Minas Gerais;Stanford University;Stanford University;Stanford University;Stanford University;Stanford University;Stanford University,478;478;4;4;4;4;4;4,715;715;3;3;3;3;3;3,2
2793,2793,2793,2793,2793,2793,2793,2793,ICLR,2019,Exploring the interpretability of LSTM neural networks over multi-variable data,Tian Guo;Tao Lin,tian.guo@gess.ethz.ch;tao.lin@epfl.ch,6;6;5,5;5;3,Reject,0,5,0.0,yes,9/27/18,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology Lausanne,10;478,10;38,
2794,2794,2794,2794,2794,2794,2794,2794,ICLR,2019,Parameter efficient training of deep convolutional neural networks by dynamic sparse reparameterization,Hesham Mostafa;Xin Wang,hesham.mostafa@intel.com;xin3.wang@intel.com,4;4;6,4;4;4,Reject,16,11,0.0,yes,9/27/18,Intel;Intel,-1;-1,-1;-1,8
2795,2795,2795,2795,2795,2795,2795,2795,ICLR,2019,An Energy-Based Framework for Arbitrary Label Noise Correction,Jaspreet Sahota;Divya Shanmugam;Janahan Ramanan;Sepehr Eghbali;Marcus Brubaker,sahotaj1@gmail.com;divyas@mit.edu;janahan.ramanan@borealisai.com;sepehr3pehr@gmail.com;mbrubake@cs.toronto.edu,5;5;5,4;4;5,Reject,0,2,0.0,yes,9/27/18,";Massachusetts Institute of Technology;Borealis AI;University of Waterloo;Department of Computer Science, University of Toronto",-1;2;-1;26;18,-1;5;-1;207;22,5;1
2796,2796,2796,2796,2796,2796,2796,2796,ICLR,2019,Incremental Few-Shot Learning with Attention Attractor Networks,Mengye Ren;Renjie Liao;Ethan Fetaya;Richard S. Zemel,mren@cs.toronto.edu;rjliao@cs.toronto.edu;ethanf@cs.toronto.edu;zemel@cs.toronto.edu,5;5;5,4;5;3,Reject,4,9,0.0,yes,9/27/18,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",18;18;18;18,22;22;22;22,6
2797,2797,2797,2797,2797,2797,2797,2797,ICLR,2019,Targeted Adversarial Examples for Black Box Audio Systems,Rohan Taori;Amog Kamsetty;Brenton Chu;Nikita Vemuri,rohantaori@berkeley.edu;amogkamsetty@berkeley.edu;brentonlongchu@berkeley.edu;nikitavemuri@berkeley.edu,3;4;6,4;3;4,Reject,0,3,0.0,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5,18;18;18;18,4
2798,2798,2798,2798,2798,2798,2798,2798,ICLR,2019,End-to-End Multi-Lingual Multi-Speaker Speech Recognition,Hiroshi Seki;Takaaki Hori;Shinji Watanabe;Jonathan Le Roux;John R. Hershey,seki@slp.cs.tut.ac.jp;thori@merl.com;shinjiw@ieee.org;leilujp@gmail.com;johnhershey@google.com,3;3;3,4;5;4,Reject,0,0,0.0,yes,9/27/18,"Toyohashi University of Technology,;Mitsubishi Electric Research Labs;Johns Hopkins University;;Google",-1;-1;72;-1;-1,-1;-1;13;-1;-1,
2799,2799,2799,2799,2799,2799,2799,2799,ICLR,2019,Policy Generalization In Capacity-Limited Reinforcement Learning,Rachel A. Lerch;Chris R. Sims,lerchr2@rpi.edu;simsc3@rpi.edu,7;7;5,4;3;4,Reject,0,4,0.0,yes,9/27/18,Rensselaer Polytechnic Institute;Rensselaer Polytechnic Institute,169;169,304;304,8
2800,2800,2800,2800,2800,2800,2800,2800,ICLR,2019,Learning Neural Random Fields with Inclusive Auxiliary Generators,Yunfu Song;Zhijian Ou,769414284@qq.com;ozjthu@gmail.com,5;6;6,3;3;2,Reject,0,7,1.0,yes,9/27/18,Tsinghua University;Tsinghua University,8;8,30;30,5
2801,2801,2801,2801,2801,2801,2801,2801,ICLR,2019,Learning Disentangled Representations with Reference-Based Variational Autoencoders,Adria Ruiz;Oriol Martinez;Xavier Binefa;Jakob Verbeek,adria.ruiz-ovejero@inria.fr;oriol.martinez@upf.edu;xavier.binefa@upf.edu;jakob.verbeek@inria.fr,7;6;6,4;4;3,Reject,0,8,1.0,yes,9/27/18,INRIA;Universitat Pompeu Fabra;Universitat Pompeu Fabra;INRIA,-1;-1;-1;-1,-1;-1;-1;-1,5;4;2
2802,2802,2802,2802,2802,2802,2802,2802,ICLR,2019,Recycling the discriminator for improving the inference mapping of GAN,Duhyeon Bang;Hyunjung Shim,duhyeonbang@yonsei.ac.kr;kateshim@yonsei.ac.kr,3;3;7,5;4;4,Reject,0,4,0.0,yes,9/27/18,Yonsei University;Yonsei University,478;478,231;231,5;4
2803,2803,2803,2803,2803,2803,2803,2803,ICLR,2019,On Difficulties of Probability Distillation,Chin-Wei Huang;Faruk Ahmed;Kundan Kumar;Alexandre Lacoste;Aaron Courville,chin-wei.huang@umontreal.ca;faruk.ahmed.91@gmail.com;kundankumar2510@gmail.com;allac@elementai.com;aaron.courville@gmail.com,5;7;5,5;2;4,Reject,0,3,0.0,yes,9/27/18,University of Montreal;;University of Montreal;Element AI;University of Montreal,123;-1;123;-1;123,108;-1;108;-1;108,
2804,2804,2804,2804,2804,2804,2804,2804,ICLR,2019,AIM: Adversarial Inference by Matching Priors and Conditionals,Hanbo Li;Yaqing Wang;Changyou Chen;Jing Gao,alexanderhanboli@gmail.com;yaqingwa@buffalo.edu;cchangyou@gmail.com;jing@buffalo.edu,6;7;4,4;4;5,Reject,0,4,1.0,yes,9/27/18,"Amazon;State University of New York, Buffalo;State University of New York, Buffalo;State University of New York, Buffalo",-1;81;81;81,-1;270;270;270,5;4
2805,2805,2805,2805,2805,2805,2805,2805,ICLR,2019,Deep Reinforcement Learning of Universal Policies with Diverse Environment Summaries,Felix Berkenkamp;Debadeepta Dey;Ashish Kapoor,befelix@inf.ethz.ch;dedey@microsoft.com;akapoor@microsoft.com,4;6;5,5;4;4,Reject,0,0,0.0,yes,9/27/18,Swiss Federal Institute of Technology;Microsoft;Microsoft,10;-1;-1,10;-1;-1,
2806,2806,2806,2806,2806,2806,2806,2806,ICLR,2019,Gradient Acceleration in Activation Functions,Sangchul Hahn;Heeyoul Choi,s.hahn@handong.edu;hchoi@handong.edu,3;5;2,4;3;5,Reject,0,4,0.0,yes,9/27/18,Handong Global University;Handong Global University,478;478,1103;1103,
2807,2807,2807,2807,2807,2807,2807,2807,ICLR,2019,COLLABORATIVE MULTIAGENT REINFORCEMENT LEARNING  IN HOMOGENEOUS SWARMS,Arbaaz Khan;Clark Zhang;Vijay Kumar;Alejandro Ribeiro,arbaazk@seas.upenn.edu;vijay.kumar@seas.upenn.edu;aribeiro@seas.upenn.edu,6;4;5,3;4;4,Reject,0,0,0.0,yes,9/27/18,University of Pennsylvania;University of Pennsylvania;University of Pennsylvania,19;19;19,10;10;10,
2808,2808,2808,2808,2808,2808,2808,2808,ICLR,2019,Generative adversarial interpolative autoencoding: adversarial training on latent space interpolations encourages convex latent distributions,Tim Sainburg;Marvin Thielk;Brad Thielman;Benjamin Migliori;Timothy Gentner,tsainbur@ucsd.edu;marvin.thielk@gmail.com;ben.migliori@lanl.gov;tgentner@ucsd.edu,4;5;4,4;4;5,Reject,2,12,0.0,yes,9/27/18,"University of California, San Diego;University of California, San Diego;Los Alamos National Laboratory;University of California, San Diego",11;11;-1;11,31;31;-1;31,5;4
2809,2809,2809,2809,2809,2809,2809,2809,ICLR,2019,Beyond Winning and Losing: Modeling Human Motivations and Behaviors with Vector-valued Inverse Reinforcement Learning,Baoxiang Wang;Tongfang Sun;Xianjun Sam Zheng,wangbx66@gmail.com;tongfs@uw.edu;sam.zheng@deephow.com,5;4;4,3;4;4,Reject,0,3,0.0,yes,9/27/18,"The Chinese University of Hong Kong;University of Washington, Seattle;Deephow",57;6;-1,40;25;-1,
2810,2810,2810,2810,2810,2810,2810,2810,ICLR,2019,MLPrune: Multi-Layer Pruning for Automated Neural Network Compression,Wenyuan Zeng;Raquel Urtasun,zengwenyuan1995@gmail.com;urtasun@uber.com,5;6;4,5;4;4,Reject,0,5,0.0,yes,9/27/18,Uber;Uber,-1;-1,-1;-1,
2811,2811,2811,2811,2811,2811,2811,2811,ICLR,2019,Offline Deep models calibration with bayesian neural networks,Juan Maroñas;Roberto Paredes;Daniel Ramos,jmaronasm@gmail.com;rparedes@dsic.upv.es;daniel.ramos@uam.es,4;3;3,4;4;4,Reject,1,12,0.0,yes,9/27/18,Universidad Politecnica de Valencia;Universidad Politecnica de Valencia;Universidad Autónoma de Madrid,478;478;-1,561;561;-1,11;2
2812,2812,2812,2812,2812,2812,2812,2812,ICLR,2019,Successor Uncertainties: exploration and uncertainty in temporal difference learning,David Janz;Jiri Hron;José Miguel Hernández-Lobato;Katja Hofmann;Sebastian Tschiatschek,david.janz93@gmail.com;jh2084@cam.ac.uk;jmh233@cam.ac.uk;katja.hofmann@microsoft.com;sebastian.tschiatschek@microsoft.com,4;5;4,3;4;5,Reject,0,8,0.0,yes,9/27/18,University of Cambridge;University of Cambridge;University of Cambridge;Microsoft;Microsoft,71;71;71;-1;-1,2;2;2;-1;-1,
2813,2813,2813,2813,2813,2813,2813,2813,ICLR,2019,Adversarially Robust Training through Structured Gradient Regularization,Kevin Roth;Aurelien Lucchi;Sebastian Nowozin;Thomas Hofmann,kevin.roth@inf.ethz.ch;aurelien.lucchi@inf.ethz.ch;sebastian.nowozin@microsoft.com;thomas.hofmann@inf.ethz.ch,4;3;4,4;4;4,Reject,10,10,0.0,yes,9/27/18,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Microsoft;Swiss Federal Institute of Technology,10;10;-1;10,10;10;-1;10,4
2814,2814,2814,2814,2814,2814,2814,2814,ICLR,2019,MERCI: A NEW METRIC TO EVALUATE THE CORRELATION BETWEEN PREDICTIVE UNCERTAINTY AND TRUE ERROR,michel moukari;loïc simon;sylvaine picard;frédéric jurie,michel.moukari@unicaen.fr;loic.simon@ensicaen.fr;sylvaine.picard@safrangroup.com;frederic.jurie@unicaen.fr,4;5;3,4;3;4,Reject,0,5,0.0,yes,9/27/18,University of Caen Normandie;ENSICAEN;SAFRAN;University of Caen Normandie,-1;-1;-1;-1,-1;-1;-1;-1,11
2815,2815,2815,2815,2815,2815,2815,2815,ICLR,2019,Unsupervised Document Representation using Partition Word-Vectors Averaging,Vivek Gupta;Ankit Kumar Saw;Partha Pratim Talukdar;Praneeth Netrapalli,vgupta@cs.utah.edu;ankit.kgpian@gmail.com;ppt@iisc.ac.in;praneeth@microsoft.com,7;6;4,4;3;4,Reject,0,12,1.0,yes,9/27/18,University of Utah;;Indian Institute of Science;Microsoft,52;-1;478;-1,200;-1;273;-1,3
2816,2816,2816,2816,2816,2816,2816,2816,ICLR,2019,Graph U-Net,Hongyang Gao;Shuiwang Ji,hongyang.gao@tamu.edu;sji@tamu.edu,7;4;7,5;4;4,Reject,6,12,2.0,yes,9/27/18,Texas A&M;Texas A&M,44;44,160;160,2;10
2817,2817,2817,2817,2817,2817,2817,2817,ICLR,2019,Fast Exploration with Simplified Models and Approximately Optimistic Planning in Model Based Reinforcement Learning,Ramtin Keramati;Jay Whang;Patrick Cho;Emma Brunskill,keramati@stanford.edu;jaywhang@cs.stanford.edu;patcho@cs.stanford.edu;ebrun@cs.stanford.edu,5;4,4;4,Reject,0,0,0.0,yes,9/27/18,Stanford University;Stanford University;Stanford University;Stanford University,4;4;4;4,3;3;3;3,
2818,2818,2818,2818,2818,2818,2818,2818,ICLR,2019,Evolutionary-Neural Hybrid Agents for Architecture Search,Krzysztof Maziarz;Andrey Khorlin;Quentin de Laroussilhe;Andrea Gesmundo,kmaziarz@google.com;akhorlin@google.com;underflow@google.com;agesmundo@google.com,4;5;4,4;2;4,Reject,0,1,0.0,yes,9/27/18,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,
2819,2819,2819,2819,2819,2819,2819,2819,ICLR,2019,Deep Curiosity Search: Intra-Life Exploration Can Improve Performance on Challenging Deep Reinforcement Learning Problems,Christopher Stanton;Jeff Clune,cstanto3@uwyo.edu;jeffclune@uwyo.edu,5;5;5,3;1;3,Reject,0,4,0.0,yes,9/27/18,University of Wyoming;University of Wyoming,386;386,1103;1103,
2820,2820,2820,2820,2820,2820,2820,2820,ICLR,2019,Deep Probabilistic Video Compression,Jun Han;Salvator Lombardo;Christopher Schroers;Stephan Mandt,jun.han.gr@dartmouth.edu;sal.lombardo@disneyresearch.com;christopher.schroers@disneyresearch.com;stephan.mandt@gmail.com,6;5;6,5;4;5,Reject,0,5,0.0,yes,9/27/18,"Dartmouth College;Disney Research, Disney;Disney Research, Disney;University of California, Irvine",153;-1;-1;35,89;-1;-1;99,5
2821,2821,2821,2821,2821,2821,2821,2821,ICLR,2019,Reduced-Gate Convolutional LSTM Design Using Predictive Coding for Next-Frame Video Prediction,Nelly Elsayed;Anthony S. Maida;Magdy Bayoumi,nelly.elsayed5@gmail.com;maida@louisiana.edu;mab0778@louisiana.edu,3;5;7,5;4;4,Reject,0,7,0.0,yes,9/27/18,University of Arizona;University of Arizona;University of Arizona,169;169;169,161;161;161,
2822,2822,2822,2822,2822,2822,2822,2822,ICLR,2019,Playing the Game of Universal Adversarial Perturbations,Julien Perolet;Mateusz Malinowski;Bilal Piot;Olivier Pietquin,perolat@google.com;mateuszm@google.com;piot@google.com;pietquin@google.com,6;5;5,1;4;3,Reject,3,5,0.0,yes,9/27/18,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,4
2823,2823,2823,2823,2823,2823,2823,2823,ICLR,2019,Unsupervised Expectation Learning for Multisensory Binding,Pablo Barros;German I. Parisi;Manfred Eppe;Stefan Wermter,barros@informatik.uni-hamburg.de;parisi@informatik.uni-hamburg.de;eppe@informatik.uni-hamburg.de;wermter@informatik.uni-hamburg.de,4;5;5,4;3;2,Reject,0,5,0.0,yes,9/27/18,University of Hamburg;University of Hamburg;University of Hamburg;University of Hamburg,228;228;228;228,207;207;207;207,
2824,2824,2824,2824,2824,2824,2824,2824,ICLR,2019,"The meaning of most"" for visual question answering models""",Alexander Kuhnle;Ann Copestake,aok25@cam.ac.uk;aac10@cam.ac.uk,7;5;5,4;5;4,Reject,0,8,0.0,yes,9/27/18,University of Cambridge;University of Cambridge,71;71,2;2,
2825,2825,2825,2825,2825,2825,2825,2825,ICLR,2019,Unsupervised Image to Sequence Translation with Canvas-Drawer Networks,Kevin Frans;Chin-Yi Cheng,kevinfrans2@gmail.com;chin-yi.cheng@autodesk.com,4;4;6,4;4;5,Reject,0,9,0.0,yes,9/27/18,OpenAI;Autodesk Inc,-1;-1,-1;-1,2
2826,2826,2826,2826,2826,2826,2826,2826,ICLR,2019,NSGA-Net: A Multi-Objective Genetic Algorithm for Neural Architecture Search,Zhichao Lu;Ian Whalen;Vishnu Boddeti;Yashesh Dhebar;Kalyanmoy Deb;Erik Goodman;Wolfgang Banzhaf,mikelzc1990@gmail.com;whalenia@msu.edu;vishnu@msu.edu;dhebarya@egr.msu.edu;kdeb@egr.msu.edu;goodman@egr.msu.edu;banzhafw@msu.edu,5;6;5,4;4;3,Reject,0,5,0.0,yes,9/27/18,SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY,478;478;478;478;478;478;478,352;352;352;352;352;352;352,11
2827,2827,2827,2827,2827,2827,2827,2827,ICLR,2019,PA-GAN: Improving GAN Training by Progressive Augmentation,Dan Zhang;Anna Khoreva,dan.zhang2@de.bosch.com;anna.khoreva@de.bosch.com,5;4;5,5;2;4,Reject,0,9,3.0,yes,9/27/18,Bosch;Bosch,-1;-1,-1;-1,5;4
2828,2828,2828,2828,2828,2828,2828,2828,ICLR,2019,Point Cloud GAN,Chun-Liang Li;Manzil Zaheer;Yang Zhang;Barnabás Póczos;Ruslan Salakhutdinov,chunlial@cs.cmu.edu;manzilz@cs.cmu.edu;yz6@andrew.cmu.edu;bapoczos@cs.cmu.edu;rsalakhu@cs.cmu.edu,5;5;6,4;4;4,Reject,4,7,0.0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1;1,24;24;24;24;24,5;4;11;8
2829,2829,2829,2829,2829,2829,2829,2829,ICLR,2019,Uncertainty-guided Lifelong Learning in Bayesian Networks,Sayna Ebrahimi;Mohamed Elhoseiny;Trevor Darrell;Marcus Rohrbach,sayna@eecs.berkeley.edu;elhoseiny@fb.com;trevor@eecs.berkeley.edu;maroffm@gmail.com,4;4;4,4;4;4,Reject,0,4,0.0,yes,9/27/18,University of California Berkeley;Facebook;University of California Berkeley;Facebook,5;-1;5;-1,18;-1;18;-1,11
2830,2830,2830,2830,2830,2830,2830,2830,ICLR,2019,Rectified Gradient: Layer-wise Thresholding for Sharp and Coherent Attribution Maps,Beomsu Kim;Junghoon Seo;Jeongyeol Choe;Jamyoung Koo;Seunghyeon Jeon;Taegyun Jeon,1202kbs@gmail.com;sjh@satreci.com;cjy@si-analytics.ai;jmkoo@si-analytics.ai;jsh@satreci.com;tgjeon@si-analytics.ai,5;5;4,5;4;4,Reject,0,15,0.0,yes,9/27/18,KAIST;Satrec Initiative co. Itd;SI Analytics;SI Analytics;Satrec Initiative co. Itd;SI Analytics,20;-1;-1;-1;-1;-1,95;-1;-1;-1;-1;-1,
2831,2831,2831,2831,2831,2831,2831,2831,ICLR,2019,Successor Options : An Option Discovery Algorithm for Reinforcement Learning,Manan Tomar*;Rahul Ramesh*;Balaraman Ravindran,manan.tomar@gmail.com;rahul13ramesh@gmail.com;ravi@cse.iitm.ac.in,4;5;6;4,5;4;4;5,Reject,3,8,0.0,yes,9/27/18,Indian Institute of Technology Madras;Indian Institute of Technology Madras;Indian Institute of Technology Madras,153;153;153,625;625;625,
2832,2832,2832,2832,2832,2832,2832,2832,ICLR,2019,SnapQuant: A Probabilistic and Nested Parameterization for Binary Networks,Kuan Wang;Hao Zhao;Anbang Yao;Aojun Zhou;Dawei Sun;Yurong Chen,wangkuan15@mails.tsinghua.edu.cn;hao.zhao@intel.com;anbang.yao@intel.com;aojun.zhou@intel.com;dawei.sun@intel.com;yurong.chen@intel.com,4;6;5,5;3;4,Reject,0,3,0.0,yes,9/27/18,Tsinghua University;Intel;Intel;Intel;Intel;Intel,8;-1;-1;-1;-1;-1,30;-1;-1;-1;-1;-1,11
2833,2833,2833,2833,2833,2833,2833,2833,ICLR,2019,Learning Physics Priors for Deep Reinforcement Learing,Yilun Du;Karthik Narasimhan,yilundu@openai.com;karthikn@cs.princeton.edu,4;5;5,3;4;5,Reject,0,10,0.0,yes,9/27/18,OpenAI;Princeton University,-1;30,-1;7,6;8
2834,2834,2834,2834,2834,2834,2834,2834,ICLR,2019,HC-Net: Memory-based Incremental Dual-Network System for Continual learning,Jangho Kim;Jeesoo Kim;Nojun Kwak,kjh91@snu.ac.kr;kimjiss0305@snu.ac.kr;nojunk@snu.ac.kr,4;4;4,3;4;5,Reject,0,5,0.0,yes,9/27/18,Seoul National University;Seoul National University;Seoul National University,41;41;41,74;74;74,
2835,2835,2835,2835,2835,2835,2835,2835,ICLR,2019,Feature Attribution As Feature Selection,Satoshi Hara;Koichi Ikeno;Tasuku Soma;Takanori Maehara,satohara@ar.sanken.osaka-u.ac.jp;k1keno@ar.sanken.osaka-u.ac.jp;tasuku_soma@mist.i.u-tokyo.ac.jp;takanori.maehara@riken.jp,4;4;3,2;4;3,Reject,0,3,0.0,yes,9/27/18,Osaka University;Osaka University;The University of Tokyo;RIKEN,478;478;54;-1,236;236;45;-1,
2836,2836,2836,2836,2836,2836,2836,2836,ICLR,2019,NECST: Neural Joint Source-Channel Coding,Kristy Choi;Kedar Tatwawadi;Tsachy Weissman;Stefano Ermon,kechoi@cs.stanford.edu;kedart@stanford.edu;tsachy@stanford.edu;ermon@cs.stanford.edu,6;4;7,5;3;4,Reject,0,8,0.0,yes,9/27/18,Stanford University;Stanford University;Stanford University;Stanford University,4;4;4;4,3;3;3;3,
2837,2837,2837,2837,2837,2837,2837,2837,ICLR,2019,Generating Images from Sounds Using Multimodal Features and GANs,Jeonghyun Lyu;Takashi Shinozaki;Kaoru Amano,app@live.jp;tshino@nict.go.jp;kaoruamano@nict.go.jp,3;4;4,4;4;5,Reject,0,0,0.0,yes,9/27/18,"Osaka University;National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology;National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology",478;-1;-1,236;-1;-1,5;4
2838,2838,2838,2838,2838,2838,2838,2838,ICLR,2019,Improving Generative Adversarial Imitation Learning with Non-expert Demonstrations,Voot Tangkaratt;Masashi Sugiyama,voot.tangkaratt@riken.jp;sugi@k.u-tokyo.ac.jp,5;5;7;4,3;4;3;5,Reject,0,9,0.0,yes,9/27/18,RIKEN;The University of Tokyo,-1;54,-1;45,5;4
2839,2839,2839,2839,2839,2839,2839,2839,ICLR,2019,Step-wise Sensitivity Analysis: Identifying Partially Distributed Representations for Interpretable Deep Learning,Botty Dimanov;Mateja Jamnik,botty.dimanov@cl.cam.ac.uk;mateja.jamnik@cl.cam.ac.uk,3;4;3,5;4;4,Reject,0,2,0.0,yes,9/27/18,University of Cambridge;University of Cambridge,71;71,2;2,10
2840,2840,2840,2840,2840,2840,2840,2840,ICLR,2019,Noisy Information Bottlenecks for Generalization,Julius Kunze;Louis Kirsch;Hippolyt Ritter;David Barber,juliuskunze@gmail.com;mail@louiskirsch.com;j.ritter@cs.ucl.ac.uk;d.barber@cs.ucl.ac.uk,7;5;3,2;3;4,Reject,0,7,0.0,yes,9/27/18,University College London;IDSIA;University College London;University College London,50;-1;50;50,16;-1;16;16,5;8
2841,2841,2841,2841,2841,2841,2841,2841,ICLR,2019,GENERALIZED ADAPTIVE MOMENT ESTIMATION,Guoqiang Zhang;Kenta Niwa;W. Bastiaan Kleijn,guoqiang.zhang@uts.edu.au;niwa.kenta@lab.ntt.co.jp;bastiaan.kleijn@ecs.vuw.ac.nz,3;4;7,4;3;4,Reject,0,6,0.0,yes,9/27/18,University of Technology Sydney;NTT;Victoria University Wellington,106;-1;314,216;-1;346,9
2842,2842,2842,2842,2842,2842,2842,2842,ICLR,2019,q-Neurons: Neuron Activations based on Stochastic Jackson's Derivative Operators,Frank Nielsen;Ke Sun,frank.nielsen@acm.org;sunk.edu@gmail.com,2;6;5,5;3;3,Reject,0,1,0.0,yes,9/27/18,"Ecole Polytechnique;University of Nebraska, Kearney",478;-1,115;-1,8
2843,2843,2843,2843,2843,2843,2843,2843,ICLR,2019,Gradient-based Training of Slow Feature Analysis by Differentiable Approximate Whitening,Merlin Schüler;Hlynur Davíð Hlynsson;Laurenz Wiskott,merlin.schueler@ini.rub.de;hlynur.hlynsson@ini.rub.de;laurenz.wiskott@ini.rub.de,5;6;6,2;4;4,Reject,0,3,0.0,yes,9/27/18,Ruhr-Universtät Bochum;Ruhr-Universtät Bochum;Ruhr-Universtät Bochum,261;261;261,248;248;248,
2844,2844,2844,2844,2844,2844,2844,2844,ICLR,2019,Unsupervised Conditional Generation using noise engineered mode matching GAN,Deepak Mishra;Prathosh AP;Aravind J;Prashant Pandey;Santanu Chaudhury,deemishra21@gmail.com;prathoshap@gmail.com;maxaravind@gmail.com;getprashant57@gmail.com;santanuc@ee.iitd.ac.in,5;5;6,3;3;4,Reject,0,7,0.0,yes,9/27/18,Indian Institute of Technology Delhi;Indian Institute of Technology Delhi;Indian Institute of Technology Delhi;Indian Institute of Technology Delhi;Indian Institute of Technology Delhi,123;123;123;123;123,529;529;529;529;529,5;4
2845,2845,2845,2845,2845,2845,2845,2845,ICLR,2019,Variadic Learning by Bayesian Nonparametric Deep Embedding,Kelsey R Allen;Hanul Shin;Evan Shelhamer;Josh B. Tenenbaum,krallen@mit.edu;skyshin@mit.edu;shelhamer@cs.berkeley.edu;jbt@mit.edu,5;4;4,4;2;4,Reject,0,13,0.0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology;University of California Berkeley;Massachusetts Institute of Technology,2;2;5;2,5;5;18;5,11;6;8
2846,2846,2846,2846,2846,2846,2846,2846,ICLR,2019,Fast adversarial training for semi-supervised learning,Dongha Kim;Yongchan Choi;Jae-Joon Han;Changkyu Choi;Yongdai Kim,dongha0718@hanmail.net;pminer32@gmail.com;jae-joon.han@samsung.com;changkyu_choi@samsung.com;ydkim0903@gmail.com,7;5;5,4;4;4,Reject,0,6,0.0,yes,9/27/18,Seoul National University;Seoul National University;Samsung;Samsung;,41;41;-1;-1;-1,74;74;-1;-1;-1,5;4
2847,2847,2847,2847,2847,2847,2847,2847,ICLR,2019,Adversarially Learned Mixture Model,Andrew Jesson;Cécile Low-Kam;Tanya Nair;Florian Soudan;Florent Chandelier;Nicolas Chapados,andrew.jesson@imagia.com;cecile.low-kam@imagia.com;tanya.nair@imagia.com;fsoudan21@gmail.com;florent.chandelier@imagia.com;nicolas.chapados@imagia.com,6;5;6,1;4;2,Reject,0,1,0.0,yes,9/27/18,Imagia;Imagia;Imagia;;Imagia;Imagia,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,5;4
2848,2848,2848,2848,2848,2848,2848,2848,ICLR,2019,Theoretical and Empirical Study of Adversarial Examples,Fuchen Liu;Hongwei Shang;Hong Zhang,fuchenl@andrew.cmu.edu;shanghongwei@oath.com;hongz@oath.com,5;5;4,2;4;4,Reject,5,2,0.0,yes,9/27/18,Carnegie Mellon University;Oath;Oath,1;-1;-1,24;-1;-1,4
2849,2849,2849,2849,2849,2849,2849,2849,ICLR,2019,microGAN: Promoting Variety through Microbatch Discrimination,Goncalo Mordido;Haojin Yang;Christoph Meinel,goncalo.mordido@hpi.de;haojin.yang@hpi.de;christoph.meinel@hpi.de,3;3;6,3;3;3,Reject,0,1,0.0,yes,9/27/18,Hasso Plattner Institute;Hasso Plattner Institute;Hasso Plattner Institute,261;261;261,1103;1103;1103,5;4
2850,2850,2850,2850,2850,2850,2850,2850,ICLR,2019,Unifying Bilateral Filtering and Adversarial Training for Robust Neural Networks,Neale Ratzlaff;Li Fuxin,ratzlafn@oregonstate.edu;lif@oregonstate.edu,4;5;5,5;5;3,Reject,10,9,0.0,yes,9/27/18,Oregon State University;Oregon State University,76;76,318;318,4
2851,2851,2851,2851,2851,2851,2851,2851,ICLR,2019,PPD: Permutation Phase Defense Against Adversarial Examples in Deep Learning,Mehdi Jafarnia-Jahromi;Tasmin Chowdhury;Hsin-Tai Wu;Sayandev Mukherjee,mjafarni@usc.edu;chowdt1@unlv.nevada.edu;hwu@docomoinnovations.com;sayandev.mukherjee@huawei.com,6;7;4,3;4;5,Reject,24,6,0.0,yes,9/27/18,University of Southern California;Nevada System of Higher Education;Docomoinnovations;Huawei Technologies Ltd.,30;-1;-1;-1,66;-1;-1;-1,4
2852,2852,2852,2852,2852,2852,2852,2852,ICLR,2019,Universal Attacks on Equivariant Networks,Amit Deshpande;Sandesh Kamath;K V Subrahmanyam,amitdesh@microsoft.com;ksandeshk@cmi.ac.in;kv@cmi.ac.in,5;4;4,4;5;5,Reject,0,3,0.0,yes,9/27/18,Microsoft;Chennai Mathematical Institute;Chennai Mathematical Institute,-1;478;478,-1;1103;1103,4
2853,2853,2853,2853,2853,2853,2853,2853,ICLR,2019,Detecting Adversarial Examples Via Neural Fingerprinting,Sumanth Dathathri;Stephan Zheng;Yisong Yue;Richard M. Murray,sdathath@caltech.edu;st.t.zheng@gmail.com;yyue@caltech.edu;murray@cds.caltech.edu,6;5;9,3;4;4,Reject,5,38,0.0,yes,9/27/18,California Institute of Technology;SalesForce.com;California Institute of Technology;California Institute of Technology,140;-1;140;140,3;-1;3;3,4
2854,2854,2854,2854,2854,2854,2854,2854,ICLR,2019,Sorting out Lipschitz function approximation,Cem Anil;James Lucas;Roger B. Grosse,cem.anil@mail.utoronto.ca;jlucas@cs.toronto.edu;rgrosse@cs.toronto.edu,7;5;4,3;4;4,Reject,0,6,4.0,yes,9/27/18,"Toronto University;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",18;18;18,22;22;22,4;8
2855,2855,2855,2855,2855,2855,2855,2855,ICLR,2019,Adaptive Mixture of Low-Rank Factorizations for Compact Neural Modeling,Ting Chen;Ji Lin;Tian Lin;Song Han;Chong Wang;Denny Zhou,iamtingchen@gmail.com;lin-j14@mails.tsinghua.edu.cn;tianlin@google.com;chongw@google.com;dennyzhou@google.com;hansong8811@gmail.com,7;4;6,5;5;4,Reject,0,12,0.0,yes,9/27/18,"University of California, Los Angeles;Tsinghua University;Google;Google;Google;",20;8;-1;-1;-1;-1,15;30;-1;-1;-1;-1,3
2856,2856,2856,2856,2856,2856,2856,2856,ICLR,2019,ChoiceNet: Robust Learning by  Revealing Output Correlations,Sungjoon Choi;Sanghoon Hong;Kyungjae Lee;Sungbin Lim,sungjoon.s.choi@gmail.com;sanghoon.hong@kakaobrain.com;kyungjae.lee@cpslab.snu.ac.kr;sungbin.lim@kakaobrain.com,4;6;5,4;4;5,Reject,0,7,0.0,yes,9/27/18,"Disney Research, Disney;Kakao Brain;Seoul National University;Kakao Brain",-1;-1;41;-1,-1;-1;74;-1,
2857,2857,2857,2857,2857,2857,2857,2857,ICLR,2019,Iteratively Learning from the Best,Yanyao Shen;Sujay Sanghavi,shenyanyao@utexas.edu;sanghavi@mail.utexas.edu,6;3;6,3;5;4,Reject,0,3,0.0,yes,9/27/18,"University of Texas, Austin;University of Texas, Austin",22;22,49;49,1
2858,2858,2858,2858,2858,2858,2858,2858,ICLR,2019,Distilled Agent DQN for Provable Adversarial Robustness,Matthew Mirman;Marc Fischer;Martin Vechev,matthew.mirman@inf.ethz.ch;marcfisc@student.ethz.ch;martin.vechev@inf.ethz.ch,5;3;4,4;2;2,Reject,0,5,0.0,yes,9/27/18,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10,10;10;10,4
2859,2859,2859,2859,2859,2859,2859,2859,ICLR,2019,On Regularization and Robustness of Deep Neural Networks,Alberto Bietti*;Grégoire Mialon*;Julien Mairal,alberto.bietti@inria.fr;gregoire.mialon@inria.fr;julien.mairal@inria.fr,5;4;6,3;4;2,Reject,0,8,0.0,yes,9/27/18,INRIA;INRIA;INRIA,-1;-1;-1,-1;-1;-1,5;4;8
2860,2860,2860,2860,2860,2860,2860,2860,ICLR,2019,Prior Networks for Detection of Adversarial Attacks,Andrey Malinin;Mark Gales,am969@cam.ac.uk;mjfg@eng.cam.ac.uk,3;4;4,4;4;5,Reject,4,0,0.0,yes,9/27/18,University of Cambridge;University of Cambridge,71;71,2;2,4
2861,2861,2861,2861,2861,2861,2861,2861,ICLR,2019,A Rate-Distortion Theory of Adversarial Examples,Angus Galloway;Anna Golubeva;Graham W. Taylor,gallowaa@uoguelph.ca;agolubeva@perimeterinstitute.ca;gwtaylor@uoguelph.ca,4;3;2,4;3;3,Reject,0,1,0.0,yes,9/27/18,University of Guelph;Perimeter Institute;University of Guelph,261;-1;261,1103;-1;1103,4;8
2862,2862,2862,2862,2862,2862,2862,2862,ICLR,2019,Generalization and Regularization in DQN,Jesse Farebrother;Marlos C. Machado;Michael Bowling,jfarebro@ualberta.ca;machado@ualberta.ca;mbowling@ualberta.ca,6;5;5,3;5;5,Reject,0,3,0.0,yes,9/27/18,University of Alberta;University of Alberta;University of Alberta,99;99;99,119;119;119,8
2863,2863,2863,2863,2863,2863,2863,2863,ICLR,2019,Multi-agent Deep Reinforcement Learning with Extremely Noisy Observations,Ozsel Kilinc;Giovanni Montana,ozsel.kilinc@warwick.ac.uk;g.montana@warwick.ac.uk,6;7;3,3;2;4,Reject,0,9,0.0,yes,9/27/18,The university of Warwick;The university of Warwick,115;115,90;90,
2864,2864,2864,2864,2864,2864,2864,2864,ICLR,2019,Characterizing Attacks on Deep Reinforcement Learning,Chaowei Xiao;Xinlei Pan;Warren He;Bo Li;Jian Peng;Mingjie Sun;Jinfeng Yi;Mingyan Liu;Dawn Song.,xiaocw@umich.edu;xinleipan@berkeley.edu;_w@eecs.berkeley.edu;lxbosky@gmail.com;jianpeng@illinois.edu;sunmj15@mails.tsinghua.com;jinfengyi.ustc@gmail.com;mingyan@umich.edu;dawnsong@gmail.com,5;6;5,4;3;4,Reject,0,8,3.0,yes,9/27/18,"University of Michigan;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of Illinois, Urbana Champaign;Mails.tsinghua;JD AI Research;University of Michigan;University of California Berkeley",8;5;5;5;3;8;-1;8;5,21;18;18;18;37;30;-1;21;18,4
2865,2865,2865,2865,2865,2865,2865,2865,ICLR,2019,Dopamine: A Research Framework for Deep Reinforcement Learning,Pablo Samuel Castro;Subhodeep Moitra;Carles Gelada;Saurabh Kumar;Marc G. Bellemare,psc@google.com;smoitra@google.com;cgel@google.com;kumasaurabh@google.com;bellemare@google.com,3;3;3,4;2;3,Reject,0,2,0.0,yes,9/27/18,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,
2866,2866,2866,2866,2866,2866,2866,2866,ICLR,2019,Probabilistic Program Induction for Intuitive Physics Game Play,Fahad Alhasoun,fha@mit.edu,3;4;2,4;2;4,Reject,0,0,0.0,yes,9/27/18,Massachusetts Institute of Technology,2,5,
2867,2867,2867,2867,2867,2867,2867,2867,ICLR,2019,Hallucinations in Neural Machine Translation,Katherine Lee;Orhan Firat;Ashish Agarwal;Clara Fannjiang;David Sussillo,katherinelee@google.com;orhanf@google.com;agarwal@google.com;clarafy@berkeley.edu;sussillo@google.com,6;4;7,5;4;4,Reject,0,12,0.0,yes,9/27/18,Google;Google;Google;University of California Berkeley;Google,-1;-1;-1;5;-1,-1;-1;-1;18;-1,3
2868,2868,2868,2868,2868,2868,2868,2868,ICLR,2019,On Inductive Biases in Deep Reinforcement Learning,Matteo Hessel;Hado van Hasselt;Joseph Modayil;David Silver,mtthss@google.com;hado@google.com;modayil@google.com;davidsilver@google.com,3;3;7,4;4;2,Reject,0,4,0.0,yes,9/27/18,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,
2869,2869,2869,2869,2869,2869,2869,2869,ICLR,2019,Exponentially Decaying Flows for Optimization in Deep Learning,Mitsuharu Takeori;Kenta Nakamura,takeori.mitsuharu.d5s@jp.nssol.nssmc.com;nakamura.kenta.4n4@jp.nssol.nssmc.com,3;3;2,5;3;5,Withdrawn,0,0,,yes,9/27/18,NS Solutions Corporation;NS Solutions Corporation,-1;-1,-1;-1,8
2870,2870,2870,2870,2870,2870,2870,2870,ICLR,2019,In search of theoretically grounded pruning,Filip Svoboda;Edgar Liberis;Nicholas D. Lane,filip.svoboda@stx.ox.ac.uk;edgar.liberis@chch.ox.ac.uk;nicholas.lane@cs.ox.ac.uk,4;3;5,3;4;3,Withdrawn,0,3,,yes,9/27/18,University of Oxford;University of Oxford;University of Oxford,50;50;50,1;1;1,
2871,2871,2871,2871,2871,2871,2871,2871,ICLR,2019,Structured Content Preservation for Unsupervised Text Style Transfer,Youzhi Tian;Zhiting Hu;Zhou Yu,yztian@ucdavis.edu;zhitingh@cs.cmu.edu;joyu@ucdavis.edu,5;6;4,4;3;5,Withdrawn,0,0,,yes,9/27/18,"University of California, Davis;Carnegie Mellon University;University of California, Davis",81;1;81,54;24;54,3
2872,2872,2872,2872,2872,2872,2872,2872,ICLR,2019,Advanced Neuroevolution: A gradient-free algorithm to train Deep Neural Networks,Ahmed Aly;David Weikersdorfer;Claire Delaunay,aaa2cn@virginia.edu;dweikersdorfer@nvidia.com;cdelaunay@nvidia.com,1;1;5,5;5;4,Withdrawn,2,11,,yes,9/27/18,University of Virginia;NVIDIA;NVIDIA,65;-1;-1,113;-1;-1,
2873,2873,2873,2873,2873,2873,2873,2873,ICLR,2019,Bridging HMMs and RNNs through Architectural Transformations,Jan Buys;Yonatan Bisk;Yejin Choi,jbuys@cs.washington.edu;ybisk@yonatanbisk.com;yejin@cs.washington.edu,3;5;5,3;4;4,Withdrawn,0,7,,yes,9/27/18,University of Washington;University of Washington;University of Washington,6;6;6,25;25;25,3
2874,2874,2874,2874,2874,2874,2874,2874,ICLR,2019,Learning with Little Data: Evaluation of Deep Learning Algorithms,Andreas Look;Stefan Riedelbauch,andreas.look@ihs.uni-stuttgart.de;stefan.riedelbauch@ihs.uni-stuttgart.de,6;4;4,4;3;5,Withdrawn,0,3,,yes,9/27/18,University of Stuttgart;University of Stuttgart,95;95,219;219,5;4;6;8
2875,2875,2875,2875,2875,2875,2875,2875,ICLR,2019,Hierarchical Deep Reinforcement Learning Agent with Counter Self-play  on Competitive Games ,Huazhe Xu;Keiran Paster;Qibin Chen;Haoran Tang;Pieter Abbeel;Trevor Darrell;Sergey Levine,huazhe_xu@berkeley.edu;keirp@berkeley.edu;cqb@tsinghua.edu.cn;hrtang@math.berkeley.edu;pabbeel@cs.berkeley.edu;trevor@eecs.berkeley.edu;svlevine@eecs.berkeley.edu,3;2;2,3;4;3,Withdrawn,0,0,,yes,9/27/18,University of California Berkeley;University of California Berkeley;Tsinghua University;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;8;5;5;5;5,18;18;30;18;18;18;18,
2876,2876,2876,2876,2876,2876,2876,2876,ICLR,2019,Label Smoothing and Logit Squeezing: A Replacement for Adversarial Training?,Ali Shafahi;Amin Ghiasi;Furong Huang;Tom Goldstein,ashafahi@cs.umd.edu;amin@cs.umd.edu;furongh@cs.umd.edu;tomg@cs.umd.edu,7;4;2,5;3;5,Withdrawn,13,9,,yes,9/27/18,"University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park",12;12;12;12,69;69;69;69,4
2877,2877,2877,2877,2877,2877,2877,2877,ICLR,2019,Rotation Equivariant Networks via Conic Convolution and the DFT,Benjamin Chidester;Minh N. Do;Jian Ma,bchidest@andrew.cmu.edu;minhdo@illinois.edu;jianma@cs.cmu.edu,4;7;6,4;2;3,Withdrawn,0,1,,yes,9/27/18,"Carnegie Mellon University;University of Illinois, Urbana Champaign;Carnegie Mellon University",1;3;1,24;37;24,
2878,2878,2878,2878,2878,2878,2878,2878,ICLR,2019,GradMix: Multi-source Transfer across Domains and Tasks,Junnan Li;Ziwei Xu;Yongkang Wong;Qi Zhao;Mohan S. Kankanhalli,lijunnan@u.nus.edu;ziwei-xu@comp.nus.edu.sg;yongkang.wong@nus.edu.sg;qzhao@cs.umn.edu;mohan@comp.nus.edu.sg,3;5;3,5;4;5,Withdrawn,0,4,,yes,9/27/18,"National University of Singapore;National University of Singapore;National University of Singapore;University of Minnesota, Minneapolis;National University of Singapore",16;16;16;57;16,22;22;22;56;22,6;2
2879,2879,2879,2879,2879,2879,2879,2879,ICLR,2019,Understanding and Improving Sequence-Labeling NER with Self-Attentive LSTMs,Peng-Hsuan Li;Wei-Yun Ma,jacobvsdanniel@iis.sinica.edu.tw;ma@iis.sinica.edu.tw,4;3;3,4;5;4,Withdrawn,0,4,,yes,9/27/18,Academia Sinica;Academia Sinica,-1;-1,-1;-1,
2880,2880,2880,2880,2880,2880,2880,2880,ICLR,2019,Differentiable Greedy Networks,Thomas Powers;Rasool Fakoor;Siamak Shakeri;Abhinav Sethy;Amanjit Kainth;Abdel-rahman Mohamed;Ruhi Sarikaya,tcpowers@uw.edu;rasool.fakoor@mavs.uta.edu;siamaks@amazon.com;sethya@amazon.com;amanjitsingh.kainth@mail.utoronto.ca;asamir@cs.toronto.edu;rsarikay@amazon.com,5;2;4,4;5;4,Withdrawn,0,4,,yes,9/27/18,"University of Washington, Seattle;University of Texas, Arlington;Amazon;Amazon;Toronto University;Department of Computer Science, University of Toronto;Amazon",6;115;-1;-1;18;18;-1,25;601;-1;-1;22;22;-1,10
2881,2881,2881,2881,2881,2881,2881,2881,ICLR,2019,Efficient Federated Learning via Variational Dropout,Wei Du;Xiao Zeng;Ming Yan;Mi Zhang,duwei1@msu.edu;zengxia6@msu.edu;myan@msu.edu;mizhang@msu.edu,4;4;3,4;3;4,Withdrawn,0,1,,yes,9/27/18,SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY,478;478;478;478,352;352;352;352,
2882,2882,2882,2882,2882,2882,2882,2882,ICLR,2019,Applications of Gaussian Processes in Finance,Rajbir S. Nirwan;Nils Bertschinger,nirwan@fias.uni-frankfurt.de;bertschinger@fias.uni-frankfurt.de,4;5;3,5;4;4,Withdrawn,0,2,,yes,9/27/18,Goethe University;Goethe University,65;65,70;70,11
2883,2883,2883,2883,2883,2883,2883,2883,ICLR,2019,An Attention-Based Model for Learning Dynamic Interaction Networks,Sandro Cavallari;Vincent W Zheng;Hongyun Cai;Erik Cambria,sandro001@e.ntu.edu.sg;vincent.zheng@adsc-create.edu.sg;hongyun.c@adsc.com.sg;cambria@ntu.edu.sg,4;3;4,3;5;4,Withdrawn,0,0,,yes,9/27/18,National Taiwan University;ADSC;Advanced Digital Sciences Center;National Taiwan University,85;-1;-1;85,197;-1;-1;197,10
2884,2884,2884,2884,2884,2884,2884,2884,ICLR,2019,Modeling Evolution of Language Through Time with Neural Networks,Edouard Delasalles;Sylvain Lamprier;Ludovic Denoyer,edouard.delasalles@lip6.fr;sylvain.lamprier@lip6.fr;ludovic.denoyer@lip6.fr,3;4;4,5;5;4,Withdrawn,0,0,,yes,9/27/18,LIP6;LIP6;LIP6,-1;-1;-1,-1;-1;-1,3
2885,2885,2885,2885,2885,2885,2885,2885,ICLR,2019,Knowledge Representation for Reinforcement Learning using General Value Functions,Gheorghe Comanici;Doina Precup;Andre Barreto;Daniel Kenji Toyama;Eser Aygün;Philippe Hamel;Sasha Vezhnevets;Shaobo Hou;Shibl Mourad,gcomanici@google.com;doinap@google.com;andrebarreto@google.com;kenjitoyama@google.com;eser@google.com;hamelphi@google.com;vezhnick@google.com;shaobohou@google.com;shibl@google.com,6;7;4,3;3;4,Withdrawn,0,0,,yes,9/27/18,Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1,
2886,2886,2886,2886,2886,2886,2886,2886,ICLR,2019,,,vladymyrov@gmail.com,5;4;4,4;4;4,Withdrawn,0,0,,yes,9/27/18,,,,
2887,2887,2887,2887,2887,2887,2887,2887,ICLR,2019,Geometric Operator Convolutional Neural Network,Yangling Ma;Yixin Luo;Zhouwang Yang,yangma@mail.ustc.edu.cn;seeing@mail.ustc.edu.cn;yangzw@ustc.edu.cn,2;5;3,5;5;4,Withdrawn,2,0,,yes,9/27/18,University of Science and Technology of China;University of Science and Technology of China;University of Science and Technology of China,478;478;478,132;132;132,4;1;8
2888,2888,2888,2888,2888,2888,2888,2888,ICLR,2019,Online Bellman Residue Minimization via Saddle Point Optimization,Zhuoran Yang;Cheng Zhou;Tong Zhang;Han Liu,zy6@princeton.edu;mikechzhou@tencent.com;tongzhang@tongzhang-ml.org;hanliu.cmu@gmail.com,5;5;4,4;4;4,Withdrawn,0,3,,yes,9/27/18,Princeton University;Tencent AI Lab;;,30;-1;-1;-1,7;-1;-1;-1,
2889,2889,2889,2889,2889,2889,2889,2889,ICLR,2019,Dual Importance Weight GAN,Gahye Lee;Seungkyu Lee,waldstein94@gmail.com;seungkyu@khu.ac.kr,4;3;5,4;5;4,Withdrawn,0,0,,yes,9/27/18,KyungHee univ.;Kyung Hee University,-1;-1,-1;-1,5;4
2890,2890,2890,2890,2890,2890,2890,2890,ICLR,2019,Explainable Adversarial Learning: Implicit Generative Modeling of Random Noise during Training for Adversarial Robustness,Priyadarshini Panda;Kaushik Roy,pandap@purdue.edu;kaushik@purdue.edu,3;5;5,4;4;4,Withdrawn,0,7,,yes,9/27/18,Purdue University;Purdue University,26;26,60;60,5;4;1
2891,2891,2891,2891,2891,2891,2891,2891,ICLR,2019,Nonlinear Channels Aggregation Networks for Deep Action Recognition,Zhigang Zhu;Hongbing Ji;Wenbo Zhang;Cheng Ouyang,zgzhu_xidian@163.com;hbji@xidian.edu.cn;zwbsoul@163.com;ouoyc@aliyun.com,3;3;3,3;4;5,Withdrawn,0,0,,yes,9/27/18,Tsinghua University;Tsinghua University;163;Aliyun,8;8;-1;-1,30;30;-1;-1,8
2892,2892,2892,2892,2892,2892,2892,2892,ICLR,2019,A SINGLE SHOT PCA-DRIVEN ANALYSIS OF NETWORK STRUCTURE TO REMOVE REDUNDANCY,Isha Garg;Priyadarshini Panda;Kaushik Roy,gargi@purdue.edu;pandap@purdue.edu;kaushik@purdue.edu,4;4;5,5;4;5,Withdrawn,0,0,,yes,9/27/18,Purdue University;Purdue University;Purdue University,26;26;26,60;60;60,3;2
2893,2893,2893,2893,2893,2893,2893,2893,ICLR,2019,D2KE: From Distance to Kernel and Embedding via Random Features For Structured Inputs,Lingfei Wu;Ian E.H. Yen;Fangli Xu;Pradeep Ravikumar;Michael J. Witbrock,lwu@email.wm.edu;eyan@cs.cmu.edu;fxu02@email.wm.edu;pradeepr@cs.cmu.edu;witbrock@us.ibm.com,4;3;5,4;4;4,Withdrawn,0,0,,yes,9/27/18,College of William and Mary;Carnegie Mellon University;College of William and Mary;Carnegie Mellon University;International Business Machines,169;1;169;1;-1,261;24;261;24;-1,10
2894,2894,2894,2894,2894,2894,2894,2894,ICLR,2019,Latent Transformations for Object  View Points Synthesis,Sangpil Kim;Nick Winovich;Hyung-gun Chi;Guang Lin;Karthik Ramani,kim2030@purdue.edu;nwinovic@purdue.edu;chi45@purdue.edu;guanglin@purdue.edu;ramani@purdue.edu,4;2;5,4;4;2,Withdrawn,0,2,,yes,9/27/18,Purdue University;Purdue University;Purdue University;Purdue University;Purdue University,26;26;26;26;26,60;60;60;60;60,5;4
2895,2895,2895,2895,2895,2895,2895,2895,ICLR,2019,Network Reparameterization for Unseen Class Categorization,Kai Li;Martin Renqiang Min;Bing Bai;Yun Fu;Hans Peter Graf,li.kai.gml@gmail.com;renqiang@nec-labs.com;bbai@nec-labs.com;yunfu@ece.neu.edu;hpg@nec-labs.com,5;3;5,5;5;3,Withdrawn,7,2,,yes,9/27/18,Northeastern University;NEC-Labs;NEC-Labs;Northeastern University;NEC-Labs,16;-1;-1;16;-1,839;-1;-1;839;-1,6
2896,2896,2896,2896,2896,2896,2896,2896,ICLR,2019,Explaining Neural Networks Semantically and Quantitatively,Hao Chen;Runjin Chen;Quanshi Zhang,bridgechen@hust.edu.cn;chenrunjin@sjtu.edu.cn;zqs1022@sjtu.edu.cn,4;4;4,4;5;4,Withdrawn,0,4,,yes,9/27/18,Hong Kong University of Science and Technology;Shanghai Jiao Tong University;Shanghai Jiao Tong University,39;52;52,44;188;188,
2897,2897,2897,2897,2897,2897,2897,2897,ICLR,2019,Explaining AlphaGo: Interpreting Contextual Effects in Neural Networks,Zenan Ling;Haotian Ma;Yu Yang;Robert C. Qiu;Song-Chun Zhu;Quanshi Zhang,lingzenan@sjtu.edu.cn;11612807@mail.sustc.edu.cn;yy19970901@ucla.edu;rqiu@tntech.edu;sczhu@stat.ucla.edu;zqs1022@sjtu.edu.cn,3;4;4,5;4;5,Withdrawn,1,3,,yes,9/27/18,"Shanghai Jiao Tong University;University of Science and Technology of China;University of California, Los Angeles;Tennessee Technological University;University of California, Los Angeles;Shanghai Jiao Tong University",52;478;20;-1;20;52,188;132;15;-1;15;188,
2898,2898,2898,2898,2898,2898,2898,2898,ICLR,2019,Deepström Networks,Luc Giffon;Hachem Kadri;Stéphane Ayache;Thierry Artières,luc.giffon@lis-lab.fr;hachem.kadri@lis-lab.fr;stephane.ayache@lis-lab.fr;thierry.artieres@lis-lab.fr,4;5;3,4;4;5,Withdrawn,0,1,,yes,9/27/18,Aix Marseille Université;Aix Marseille Université;Aix Marseille Université;Aix Marseille Université,478;478;478;478,297;297;297;297,
2899,2899,2899,2899,2899,2899,2899,2899,ICLR,2019,One Bit Matters: Understanding Adversarial Examples as the Abuse of Redundancy,Jingkang Wang;Ruoxi Jia;Gerald Friedland;Bo Li;Costas Spanos,wangjksjtu_01@sjtu.edu.cn;ruoxijia@berkeley.edu;fractor@eecs.berkeley.edu;lxbosky@gmail.com;spanos@berkeley.edu,3;3;3,4;4;4,Withdrawn,2,1,,yes,9/27/18,Shanghai Jiao Tong University;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,52;5;5;5;5,188;18;18;18;18,4;1
2900,2900,2900,2900,2900,2900,2900,2900,ICLR,2019,,,samitha.herath@data61.csiro.au;u5505348@anu.edu.au;mehrtash.harandi@monash.edu,4;3;5,4;4;5,Withdrawn,0,3,,yes,9/27/18,", CSIRO;Australian National University;Monash University",-1;106;123,-1;48;80,
2901,2901,2901,2901,2901,2901,2901,2901,ICLR,2019,Context-aware Forecasting for Multivariate Stationary Time-series,Valentin Guiguet;Nicolas Baskiotis;Vincent Guigue;Patrick Gallinari,guiguetvalentin@gmail.com;nicolas.baskiotis@lip6.fr;vincent.guigue@lip6.fr;patrick.gallinari@lip6.fr,5;5;4,3;5;4,Withdrawn,0,1,,yes,9/27/18,LIP6;LIP6;LIP6;LIP6,-1;-1;-1;-1,-1;-1;-1;-1,
2902,2902,2902,2902,2902,2902,2902,2902,ICLR,2019,HANDLING CONCEPT DRIFT  IN WIFI-BASED INDOOR LOCALIZATION USING REPRESENTATION LEARNING,Raihan Seraj;Negar Ghourchian;Michel Allegue-Martinez,raihan.seraj@mail.mcgill.ca;negar.gh@aerial.ai;michel.allegue@aerial.ai,2;3;4,1;4;4,Withdrawn,0,0,,yes,9/27/18,McGill University;Aerial Technologies Inc.;Aerial Technologies Inc.,85;-1;-1,42;-1;-1,
2903,2903,2903,2903,2903,2903,2903,2903,ICLR,2019,Transfer Learning for Estimating Causal Effects Using Neural Networks,Sören R. Künzel;Bradly C. Stadie;Nikita Vemuri;Varsha Ramakrishnan;Jasjeet S. Sekhon;Pieter Abbeel,srk@berkeley.edu;bstadie@berkeley.edu;nikitavemuri@berkeley.edu;vio@berkeley.edu;sekhon@berkeley.edu;pabbeel@cs.berkeley.edu,7;5;3,3;4;3,Withdrawn,0,0,,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5;5;5,18;18;18;18;18;18,6
2904,2904,2904,2904,2904,2904,2904,2904,ICLR,2019,Variational Autoencoders for Text Modeling without Weakening the Decoder,Ryo Kamoi;Hiroyasu Fukutomi,ryo_kamoi_st@keio.jp;hiroyasu.fukutomi@datasection.co.jp,4;4;1,3;5;4,Withdrawn,6,1,,yes,9/27/18,Keio University;,62;-1,12;-1,5
2905,2905,2905,2905,2905,2905,2905,2905,ICLR,2019,A PRIVACY-PRESERVING IMAGE CLASSIFICATION FRAMEWORK WITH A LEARNABLE OBFUSCATOR,Xiangyi Meng;Zixuan Huang;Yuefeng Du;Antoni Chan;Cong Wang,xy.meng@my.cityu.edu.hk;zixuhuang3-c@my.cityu.edu.hk;yf.du@my.cityu.edu.hk;abchan@cityu.edu.hk;congwang@cityu.edu.hk,5;5;5,4;4;5,Withdrawn,2,0,,yes,9/27/18,City University of Hong Kong;City University of Hong Kong;City University of Hong Kong;City University of Hong Kong;City University of Hong Kong,89;89;89;89;89,40;40;40;40;40,4
2906,2906,2906,2906,2906,2906,2906,2906,ICLR,2019,ODIN: Outlier Detection In Neural Networks,Rickard Sjögren;Johan Trygg,rickard.sjoegren@sartorius-stedim.com;johan.trygg@sartorius-stedim.com,5;4;4,4;4;4,Withdrawn,1,4,,yes,9/27/18,Computational Life Science Cluster;Sartorius-stedim,-1;-1,-1;-1,
2907,2907,2907,2907,2907,2907,2907,2907,ICLR,2019,Improving latent variable descriptiveness by modelling rather than ad-hoc factors,Alex Mansbridge;Roberto Fierimonte;Ilya Feige;David Barber,amansbridge@turing.ac.uk;roberto.fierimonte@gmail.com;ilya@asidatascience.com;david.barber@ucl.ac.uk,4;4;6,4;4;3,Withdrawn,0,3,,yes,9/27/18,Alan Turing Institute;;University College London;University College London,-1;-1;50;50,-1;-1;16;16,3;5;1
2908,2908,2908,2908,2908,2908,2908,2908,ICLR,2019,Capacity of Deep Neural Networks under Parameter Quantization,Yoonho Boo;Sungho Shin;and Wonyong Sung,dnsgh337@snu.ac.kr;ssh9919@snu.ac.kr;wysung@snu.ac.kr,5;5;5,3;4;3,Withdrawn,0,0,,yes,9/27/18,Seoul National University;Seoul National University;Seoul National University,41;41;41,74;74;74,3
2909,2909,2909,2909,2909,2909,2909,2909,ICLR,2019,Learning of Sophisticated Curriculums by viewing them as Graphs over Tasks,Lucas Willems;Yoshua Bengio,lcswillems@gmail.com;yoshua.bengio@umontreal.ca,3;2;4,1;2;4,Withdrawn,0,3,,yes,9/27/18,Ecole Normale Superieure;University of Montreal,99;123,603;108,
2910,2910,2910,2910,2910,2910,2910,2910,ICLR,2019,RNNs with Private and Shared Representations for Semi-Supervised Sequence Learning,Ge Ya Luo;Jie Fu;Pengfei Liu;Zhi Hao Luo;Chris Pal,olga.xu@umontreal.ca;jie.fu@polymtl.ca;pfliu14@fudan.edu.cn;zhi-hao.luo@polymtl.ca;christopher.pal@polymtl.ca,3;5;4,5;5;4,Withdrawn,0,3,,yes,9/27/18,University of Montreal;Polytechnique Montreal;Fudan University;Polytechnique Montreal;Polytechnique Montreal,123;386;78;386;386,108;108;116;108;108,
2911,2911,2911,2911,2911,2911,2911,2911,ICLR,2019,MAJOR-MINOR LSTMS FOR WORD-LEVEL LANGUAGE MODEL,Kai Shuang;Rui Li;Mengyu Gu;Qianqian Yang;Jonathan;Sen Su,shuangk@bupt.edu.cn;lirui@bupt.edu.cn;pattygu0622@bupt.edu.cn;echo_yang@bupt.edu.cn;jonathan.loo@uwl.ac.uk;susen@bupt.edu.cn,4;3;3,5;4;5,Withdrawn,2,4,,yes,9/27/18,Beijing University of Post and Telecommunication;Beijing University of Post and Telecommunication;Beijing University of Post and Telecommunication;Beijing University of Post and Telecommunication;;Beijing University of Post and Telecommunication,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,3
2912,2912,2912,2912,2912,2912,2912,2912,ICLR,2019,SALSA-TEXT : SELF ATTENTIVE LATENT SPACE BASED ADVERSARIAL TEXT GENERATION,Jules Gagnon-Marchand;Hamed Sadeghi;Mehdi Rezagholizadeh;Md. Akmal Haider,jgagnonmarchand@gmail.com;haamed.sadeghi@gmail.com;mehdi.rezagholizadeh@gmail.com;md.akmal.haidar@huawei.com,4;4;5,3;4;4,Withdrawn,0,0,,yes,9/27/18,Huawei Technologies Ltd.;Huawei Technologies Ltd.;;Huawei Technologies Ltd.,-1;-1;-1;-1,-1;-1;-1;-1,5;4
2913,2913,2913,2913,2913,2913,2913,2913,ICLR,2019,,Qingpeng Cai;Ling Pan;Pingzhong Tang,cqpcurry@gmail.com;penny.ling.pan@gmail.com;kenshinping@gmail.com,4;5;1,4;3;4,Withdrawn,0,2,,yes,9/27/18,Tsinghua University;Tsinghua University;Tsinghua University,8;8;8,30;30;30,
2914,2914,2914,2914,2914,2914,2914,2914,ICLR,2019,Neuron Hierarchical Networks,Han Yue;De-An Wu;Lei Wu;Ji Xie,johnhany@163.com;wudean.cn@uestc.edu.cn;wulei@uestc.edu.cn;zonghengxs@163.com,5;4;4,4;4;4,Withdrawn,0,0,,yes,9/27/18,University of Electronic Science and Technology of China;University of Electronic Science and Technology of China;University of Electronic Science and Technology of China;163,169;169;169;-1,843;843;843;-1,10
2915,2915,2915,2915,2915,2915,2915,2915,ICLR,2019,Linearizing Visual Processes with Deep Generative Models,Alexander Sagel;Hao Shen,a.sagel@tum.de;shen@fortiss.org,3;3;4,4;4;3,Withdrawn,0,3,,yes,9/27/18,Technical University Munich;Fortiss,54;-1,41;-1,5;4
2916,2916,2916,2916,2916,2916,2916,2916,ICLR,2019,Inhibited Softmax for Uncertainty Estimation in Neural Networks,Marcin Możejko;Mateusz Susik;Rafał Karczewski,marcin@sigmoidal.io;msusik@sigmoidal.io;rafal@sigmoidal.io,4;4;3,4;3;4,Withdrawn,0,1,,yes,9/27/18,;;,-1;-1;-1,-1;-1;-1,
2917,2917,2917,2917,2917,2917,2917,2917,ICLR,2019,Improving Gaussian mixture latent variable model convergence with Optimal Transport,Benoit Gaujac;Ilya Feige;David Barber,benoit.gaujac.16@ucl.ac.uk;ilya@asidatascience.com;david.barber@ucl.ac.uk,5;5;5,3;4;4,Withdrawn,0,3,,yes,9/27/18,University College London;University College London;University College London,50;50;50,16;16;16,5
2918,2918,2918,2918,2918,2918,2918,2918,ICLR,2019,From Amortised to Memoised Inference: Combining Wake-Sleep and Variational-Bayes for Unsupervised Few-Shot Program Learning,Luke B. Hewitt;Joshua B. Tenenbaum,lbh@mit.edu;jbt@mit.edu,3;3;3,5;5;4,Withdrawn,0,1,,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2,5;5,5;11
2919,2919,2919,2919,2919,2919,2919,2919,ICLR,2019,,,v-ziclin@microsoft.com;lizo@microsoft.com,6;2;4,2;5;3,Withdrawn,3,0,,yes,9/27/18,Microsoft;Microsoft,-1;-1,-1;-1,
2920,2920,2920,2920,2920,2920,2920,2920,ICLR,2019,Encoder Discriminator Networks for Unsupervised Representation Learning,Nils Wandel,nils.wandel@ais.uni-bonn.de,3;4;3,4;4;5,Withdrawn,0,4,,yes,9/27/18,University of Bonn,123,100,
2921,2921,2921,2921,2921,2921,2921,2921,ICLR,2019,Geometry of Deep Convolutional Networks,Stefan Carlsson,stefanc@kth.se,2;4;3,5;4;2,Withdrawn,0,0,,yes,9/27/18,"KTH Royal Institute of Technology, Stockholm, Sweden",140,173,
2922,2922,2922,2922,2922,2922,2922,2922,ICLR,2019,Learning and Data Selection in Big Datasets,Hossein S. Ghadikolaei;Hadi Ghauch;Carlo Fischione;Mikael Skoglund,hshokri@kth.se;ghauch@kth.se;carlofi@kth.se;skoglund@kth.se,4;3;3,3;4;5,Withdrawn,0,0,,yes,9/27/18,"KTH Royal Institute of Technology, Stockholm, Sweden;KTH Royal Institute of Technology, Stockholm, Sweden;KTH Royal Institute of Technology, Stockholm, Sweden;KTH Royal Institute of Technology, Stockholm, Sweden",140;140;140;140,173;173;173;173,8
2923,2923,2923,2923,2923,2923,2923,2923,ICLR,2019,Data Poisoning Attack against Unsupervised Node Embedding Methods,Mingjie Sun;Jian Tang;Huichen Li;Bo Li;Chaowei Xiao;Yao Chen;Dawn Song,sunmj15@gmail.com;tangjianpku@gmail.com;huichen3@illinois.edu;lxbosky@gmail.com;xiaocw@umich.edu;antoniechen@tencent.com;dawnsong@gmail.com,4;4;4,5;4;3,Withdrawn,0,0,,yes,9/27/18,"Tsinghua University;HEC Montreal;University of Illinois, Urbana Champaign;University of California Berkeley;University of Michigan;Tencent AI Lab;University of California Berkeley",8;-1;3;5;8;-1;5,30;-1;37;18;21;-1;18,4;10
2924,2924,2924,2924,2924,2924,2924,2924,ICLR,2019,Shaping representations through communication,Olivier Tieleman;Angeliki Lazaridou;Shibl Mourad;Charles Blundell;Doina Precup,tieleman@google.com;angeliki@google.com;shibl@google.com;cblundell@google.com;doinap@google.com,5;4;5,4;4;4,Withdrawn,0,1,,yes,9/27/18,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,6
2925,2925,2925,2925,2925,2925,2925,2925,ICLR,2019,,,hongyang.gao@wsu.edu,5;4;4,4;5;4,Withdrawn,6,3,,yes,9/27/18,SUN YAT-SEN UNIVERSITY,478,352,
2926,2926,2926,2926,2926,2926,2926,2926,ICLR,2019,,,na@na.edu,3;5;5,4;4;4,Withdrawn,0,3,,yes,9/27/18,University of Arizona,169,161,
2927,2927,2927,2927,2927,2927,2927,2927,ICLR,2019,Exploiting Invariant Structures for Compression in Neural Networks,Jiahao Su;Jingling Li;Bobby Bhattacharjee;Furong Huang,jiahaosu@terpmail.umd.edu;jingling@cs.umd.edu;bobby@cs.umd.edu;furongh@cs.umd.edu,4;4;4,4;4;4,Withdrawn,0,1,,yes,9/27/18,"University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park",12;12;12;12,69;69;69;69,9
2928,2928,2928,2928,2928,2928,2928,2928,ICLR,2019,Evading Defenses to Transferable Adversarial Examples by Mitigating Attention Shift,Yinpeng Dong;Tianyu Pang;Hang Su;Jun Zhu,dyp17@mails.tsinghua.edu.cn;pty17@mails.tsinghua.edu.cn;suhangss@mail.tsinghua.edu.cn;dcszj@mail.tsinghua.edu.cn,4;4;4,3;4;3,Withdrawn,0,5,,yes,9/27/18,Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University,8;8;8;8,30;30;30;30,4
2929,2929,2929,2929,2929,2929,2929,2929,ICLR,2019,Classification of Building Noise Type/Position via Supervised Learning,Hwiyong Choi;Haesang Yang;Seungjun Lee;Woojae Seong,its_me_chy@snu.ac.kr;coupon3@snu.ac.kr;tl7qns7ch@snu.ac.kr;wseong@snu.ac.kr,4;4;4,2;4;4,Withdrawn,0,0,,yes,9/27/18,Seoul National University;Seoul National University;Seoul National University;Seoul National University,41;41;41;41,74;74;74;74,
2930,2930,2930,2930,2930,2930,2930,2930,ICLR,2019,Nesterov's method is the discretization of a differential equation with Hessian damping,Adam M. Oberman;Maxime Laborde,adam.oberman@mcgill.ca;maxime.laborde@mcgill.ca,4;5;6,5;4;5,Withdrawn,5,0,,yes,9/27/18,McGill University;McGill University,85;85,42;42,1
2931,2931,2931,2931,2931,2931,2931,2931,ICLR,2019,"A Forensic Representation to Detect Non-Trivial Image Duplicates, and How it Applies to Semantic Segmentation",M. Cicconet;H. Elliott;D.L. Richmond;D. Wainstock;M. Walsh,cicconet@gmail.com;elliott.hunter@gmail.com;daverichmond@gmail.com;daniel_wainstock@hms.harvard.edu;mary_walsh@hms.harvard.edu,4;3;2,4;5;5,Withdrawn,0,0,,yes,9/27/18,Harvard University;;;Harvard University;Harvard University,39;-1;-1;39;39,6;-1;-1;6;6,2
2932,2932,2932,2932,2932,2932,2932,2932,ICLR,2019,End-to-end Learning of a Convolutional Neural Network via Deep Tensor Decomposition,Samet Oymak;Mahdi Soltanolkotabi,sametoymak@gmail.com;soltanol@usc.edu,5;5;5,3;3;3,Withdrawn,0,0,,yes,9/27/18,"University of California, Riverside;University of Southern California",57;30,197;66,
2933,2933,2933,2933,2933,2933,2933,2933,ICLR,2019,Domain Adaptive Transfer Learning,Jiquan Ngiam;Daiyi Peng;Vijay Vasudevan;Simon Kornblith;Quoc Le;Ruoming Pang,jngiam@google.com;daiyip@google.com;vrv@google.com;skornblith@google.com;qvl@google.com;rpang@google.com,3;4;7,5;4;4,Withdrawn,0,3,,yes,9/27/18,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,6;2
2934,2934,2934,2934,2934,2934,2934,2934,ICLR,2019,Deep clustering based on a mixture of autoencoders,Shlomo E. Chazan;Sharon Gannot;Jacob Goldberger,shlomi.chazan@biu.ac.il;sharon.gannot@biu.ac.il;jacob.goldberger@biu.ac.il,6;4;5,3;3;5,Withdrawn,0,1,,yes,9/27/18,Bar Ilan University;Bar Ilan University;Bar Ilan University,95;95;95,456;456;456,
2935,2935,2935,2935,2935,2935,2935,2935,ICLR,2019,Live Face De-Identification in Video,Oran Gafni;Lior Wolf;Yaniv Taigman,oran@fb.com;wolf@fb.com;yaniv@fb.com,6;4;6,4;4;4,Withdrawn,0,4,,yes,9/27/18,Facebook;Facebook;Facebook,-1;-1;-1,-1;-1;-1,
2936,2936,2936,2936,2936,2936,2936,2936,ICLR,2019,IMAGE DEFORMATION META-NETWORK FOR ONE-SHOT LEARNING,Zitian Chen;Yanwei Fu;Yu-Xiong Wang;Lin Ma;Wei Liu;Martial Hebert,tankche2@gmail.com;yanweifu@fudan.edu.cn;yuxiongw@cs.cmu.edu;forest.linma@gmail.com;wl2223@columbia.edu;hebert@ri.cmu.edu,5;7;6,4;1;4,Withdrawn,0,3,,yes,9/27/18,Fudan University;Fudan University;Carnegie Mellon University;Tencent AI Lab;Columbia University;Carnegie Mellon University,78;78;1;-1;15;1,116;116;24;-1;14;24,6
2937,2937,2937,2937,2937,2937,2937,2937,ICLR,2019,Towards Resisting Large Data Variations via Introspective Learning,Yunhan Zhao;Ye Tian;Wei Shen;Alan Yuille,yzhao83@jhu.edu;tytian@outlook.com;shenwei1231@gmail.com;alan.l.yuille@gmail.com,4;5;6,4;4;3,Withdrawn,0,7,,yes,9/27/18,Johns Hopkins University;Verb Surgical;Johns Hopkins University;Johns Hopkins University,72;-1;72;72,13;-1;13;13,5
2938,2938,2938,2938,2938,2938,2938,2938,ICLR,2019,Realistic Adversarial Examples in 3D Meshes,Chaowei Xiao;Dawei Yang;Bo Li;Jia Deng;Mingyan Liu,xiaocw@umich.edu;ydawei@umich.edu;lxbosky@gmail.com;jiadeng@cs.princeton.edu;mingyan@umich.edu,5;3;5,3;3;3,Withdrawn,0,0,,yes,9/27/18,University of Michigan;University of Michigan;University of California Berkeley;Princeton University;University of Michigan,8;8;5;30;8,21;21;18;7;21,4
2939,2939,2939,2939,2939,2939,2939,2939,ICLR,2019,Representation Flow for Action Recognition,AJ Piergiovanni;Michael S. Ryoo,ajpiergi@indiana.edu;mryoo@indiana.edu,3;5;5,5;5;4,Withdrawn,2,10,,yes,9/27/18,University of Arizona;University of Arizona,169;169,161;161,
2940,2940,2940,2940,2940,2940,2940,2940,ICLR,2019,PointGrow: Autoregressively Learned Point Cloud Generation with Self-Attention,Yongbin Sun;Yue Wang;Ziwei Liu;Joshua E. Siegel;Sanjay Sarma,yb_sun@mit.edu;yuewang@csail.mit.edu;zwliu.hust@gmail.com;j_siegel@mit.edu;sesarma@mit.edu,3;6;6,4;4;5,Withdrawn,0,0,,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology;The Chinese University of Hong Kong;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;57;2;2,5;5;40;5;5,
2941,2941,2941,2941,2941,2941,2941,2941,ICLR,2019,Stacked U-Nets: A No-Frills Approach to Natural Image Segmentation,Sohil Shah;Pallabi Ghosh;Larry S Davis;Tom Goldstein,sohilas@umd.edu;tomg@cs.umd.edu;pallabig@umd.edu;lsd@umiacs.umd.edu,5;3;5,5;5;5,Withdrawn,0,1,,yes,9/27/18,"University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park",12;12;12;12,69;69;69;69,2
2942,2942,2942,2942,2942,2942,2942,2942,ICLR,2019,UNSUPERVISED CONVOLUTIONAL NEURAL NETWORKS FOR ACCURATE VIDEO FRAME INTERPOLATION WITH INTEGRATION OF MOTION COMPONENTS,Thang Van Nguyen;Kyu-Joong Lee;Hyuk-Jae Lee,itmanhieu@snu.ac.kr;kyujoonglee@sunmoon.ac.kr;hjlee@capp.snu.ac.kr,3;5;4,4;4;5,Withdrawn,0,0,,yes,9/27/18,Seoul National University;Kyung Hee;Seoul National University,41;-1;41,74;-1;74,10
2943,2943,2943,2943,2943,2943,2943,2943,ICLR,2019,Compositional GAN: Learning Conditional Image Composition,Samaneh Azadi;Deepak Pathak;Sayna Ebrahimi;Trevor Darrell,sazadi@berkeley.edu;pathak@berkeley.edu;sayna@berkeley.edu;trevor@eecs.berkeley.edu,4;4;5,5;4;4,Withdrawn,0,5,,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5,18;18;18;18,5;4
2944,2944,2944,2944,2944,2944,2944,2944,ICLR,2019,PolyCNN: Learning Seed Convolutional Filters,Felix Juefei-Xu;Vishnu Naresh Boddeti;Marios Savvides,juefei.xu@gmail.com;vishnu@msu.edu;msavvide@ri.cmu.edu,3;4;4,4;2;3,Withdrawn,0,0,,yes,9/27/18,Alibaba Group;SUN YAT-SEN UNIVERSITY;Carnegie Mellon University,-1;478;1,-1;352;24,
2945,2945,2945,2945,2945,2945,2945,2945,ICLR,2019,A Main/Subsidiary Network Framework for Simplifying Binary Neural Networks,Yinghao Xu;Xin Dong;Yudian Li;Hao Su,justimyhxu@zju.edu.cn;xindong@g.harvard.edu;daniellee2519@gmail.com;haosu@eng.ucsd.edu,5,4,Withdrawn,0,0,,yes,9/27/18,"Zhejiang University;Harvard University;University of Electronic Science and Technology of China;University of California, San Diego",57;39;169;11,177;6;843;31,
2946,2946,2946,2946,2946,2946,2946,2946,ICLR,2019,A Teacher Student Network For Faster Video Classification,Shweta Bhardwaj;Mukundhan Srinivasan;Mitesh M. Khapra,cs16s003@cse.iitm.ac.in;msrinivasan@nvidia.com;miteshk@cse.iitm.ac.in,4;4;4,4;5;5,Withdrawn,0,0,,yes,9/27/18,Indian Institute of Technology Madras;NVIDIA;Indian Institute of Technology Madras,153;-1;153,625;-1;625,
2947,2947,2947,2947,2947,2947,2947,2947,ICLR,2019,Data Interpretation and Reasoning Over Scientific Plots,Pritha Ganguly;Nitesh Methani;Mitesh M. Khapra,prithag@cse.iitm.ac.in;nmethani@cse.iitm.ac.in,6;6;3,4;4;4,Withdrawn,0,0,,yes,9/27/18,Indian Institute of Technology Madras;Indian Institute of Technology Madras,153;153,625;625,10
2948,2948,2948,2948,2948,2948,2948,2948,ICLR,2019,Logit Regularization Methods for Adversarial Robustness,Cecilia Summers;Michael J. Dinneen,ceciliasummers07@gmail.com;mjd@cs.auckland.ac.nz,3;5;2,5;5;5,Withdrawn,10,5,,yes,9/27/18,University of Auckland;University of Auckland,261;261,191;191,4
2949,2949,2949,2949,2949,2949,2949,2949,ICLR,2019,Feature Matters: A Stage-by-Stage Approach for Task Independent Knowledge Transfer,Mengya Gao;Yujun Shen;Quanquan Li;Liang Wan;Xiaoou Tang,daisy@tju.edu.cn;sy116@ie.cuhk.edu.hk;liquanquan@sensetime.com;lwan@tju.edu.cn;xtang@ie.cuhk.edu.hk,5;4;4,4;4;4,Withdrawn,0,0,,yes,9/27/18,Zhejiang University;The Chinese University of Hong Kong;SenseTime Group Limited;Zhejiang University;The Chinese University of Hong Kong,57;57;-1;57;57,177;40;-1;177;40,
2950,2950,2950,2950,2950,2950,2950,2950,ICLR,2019,Parametrizing Fully Convolutional Nets with a Single High-Order Tensor,Jean Kossaifi;Adrian Bulat;Georgios Tzimiropoulos;Maja Pantic,jean.kossaifi@gmail.com;bulat.adrian@gmail.com;yorgos.tzimiropoulos@nottingham.ac.uk;maja.pantic@gmail.com,4;3;4,4;4;5,Withdrawn,0,4,,yes,9/27/18,Imperial College London;Samsung;The University of Nottingham;Imperial College London,72;-1;228;72,8;-1;146;8,2
2951,2951,2951,2951,2951,2951,2951,2951,ICLR,2019,Associate Normalization,Song-Hao Jia;Ding-Jie Chen;Hwann-Tzong Chen,gasoonjia@icloud.com;djchen.tw@gmail.com;htchen@cs.nthu.edu.tw,3;5;2,5;4;5,Withdrawn,0,0,,yes,9/27/18,National Tsing Hua University;Academia Sinica;National Tsing Hua University,199;-1;199,323;-1;323,
2952,2952,2952,2952,2952,2952,2952,2952,ICLR,2019,Online abstraction with MDP homomorphisms for Deep Learning,Ondrej Biza;Robert Platt,bizaondr@fit.cvut.cz;rplatt@ccs.neu.edu,4;5,3;3,Withdrawn,0,0,,yes,9/27/18,Czech Technical University in Prague;Northeastern University,314;16,740;839,
2953,2953,2953,2953,2953,2953,2953,2953,ICLR,2019,Generalized Label Propagation Methods for Semi-Supervised Learning,Qimai Li;Xiao-Ming Wu;Zhichao Guan.,csqmli@comp.polyu.edu.hk;xiao-ming.wu@polyu.edu.hk;zcguan@zju.edu.cn,4;3;6,4;4;5,Withdrawn,2,3,,yes,9/27/18,The Hong Kong Polytechnic University;The Hong Kong Polytechnic University;Zhejiang University,169;169;57,182;182;177,10
2954,2954,2954,2954,2954,2954,2954,2954,ICLR,2019,Rethinking Knowledge Graph Propagation for Zero-Shot Learning,Michael Kampffmeyer;Yinbo Chen;Xiaodan Liang;Hao Wang;Yujia Zhang;Eric P. Xing,michael.c.kampffmeyer@uit.no;cyvius96@gmail.com;xdliang328@gmail.com;hwang87@mit.edu;zhangyujia2014@ia.ac.cn;epxing@cs.cmu.edu,7;5;5,4;3;4,Withdrawn,0,3,,yes,9/27/18,"UiT The Arctic University of Norway;Tsinghua University;SUN YAT-SEN UNIVERSITY;Massachusetts Institute of Technology;Institute of automation, Chinese academy of science, Chinese Academy of Sciences;Carnegie Mellon University",-1;8;478;2;62;1,-1;30;352;5;1103;24,10;6;8
2955,2955,2955,2955,2955,2955,2955,2955,ICLR,2019,A Unified View of Deep Metric Learning via Gradient Analysis,Xun Wang;Xintong Han;Weilin Huang;Dengke Dong;Matthew R. Scott,xunwang@malong.com;xinhan@malong.com;whuang@malong.com,3;6;5,4;4;4,Withdrawn,2,3,,yes,9/27/18,Malong Technologies;Malong Technologies;Malong Technologies,-1;-1;-1,-1;-1;-1,
2956,2956,2956,2956,2956,2956,2956,2956,ICLR,2019,Learning Spatio-Temporal Representations Using Spike-Based Backpropagation,Deboleena Roy;Priyadarshini Panda;Kaushik Roy,roy77@purdue.edu;pandap@purdue.edu;kaushik@purdue.edu,3;4;3,5;5;4,Withdrawn,0,0,,yes,9/27/18,Purdue University;Purdue University;Purdue University,26;26;26,60;60;60,5
2957,2957,2957,2957,2957,2957,2957,2957,ICLR,2019,withdrawn,withdrawn,aaron.chadha.14@ucl.ac.uk;i.andreopoulos@ucl.ac.uk,4;4;3,5;4;5,Withdrawn,0,0,,yes,9/27/18,University College London;University College London,50;50,16;16,
2958,2958,2958,2958,2958,2958,2958,2958,ICLR,2019,Cosine similarity-based Adversarial process,Hee-Soo Heo;Hye-Jin Shim;Jee-Weon Jung;IL-Ho Yang;Sung-Hyun Yoon;Ha-Jin Yu,zhasgone@naver.com;shimhyejin930615@gmail.com;aberforth19@naver.com;heisco@hanmail.net;ysh901108@naver.com;hjyu@uos.ac.kr,4;3;5,3;5;4,Withdrawn,0,0,,yes,9/27/18,"School of Computer Science, University of Seoul;;Naver;;Naver;School of Computer Science, University of Seoul",478;-1;-1;-1;-1;478,798;-1;-1;-1;-1;798,4
2959,2959,2959,2959,2959,2959,2959,2959,ICLR,2019,Low-Cost Parameterizations of Deep Convolutional Neural Networks,Eran Treister;Lars Ruthotto;Michal Sharoni;Sapir Zafrani;Eldad Haber,erant@bgu.ac.il;lruthotto@emory.edu;sharmic@post.bgu.ac.il;sapirza@post.bgu.ac.il;ehaber@eos.ubc.ca,4;4;5,3;4;5,Withdrawn,0,0,,yes,9/27/18,Ben Gurion University of the Negev;Emory University;Ben Gurion University of the Negev;Ben Gurion University of the Negev;University of British Columbia,-1;65;-1;-1;36,-1;50;-1;-1;34,
2960,2960,2960,2960,2960,2960,2960,2960,ICLR,2019,Engaging Image Captioning Via Personality,Kurt Shuster;Samuel Humeau;Hexiang Hu;Antoine Bordes;Jason Weston,kshuster@fb.com;samuelhumeau@fb.com;hexianghu@fb.com;abordes@fb.com;jaseweston@gmail.com,5;5;5,5;5;5,Withdrawn,0,1,,yes,9/27/18,Facebook;Facebook;Facebook;Facebook;,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,
2961,2961,2961,2961,2961,2961,2961,2961,ICLR,2019,Spectral Convolutional Networks on Hierarchical Multigraphs,Boris Knyazev;Xiao Lin;Mohamed R. Amer;Graham W. Taylor,bknyazev@uoguelph.ca;xiao.lin@sri.com;mohamed.amer@sri.com;gwtaylor@uoguelph.ca,4;3;4,4;5;4,Withdrawn,0,0,,yes,9/27/18,University of Guelph;SRI International;SRI International;University of Guelph,261;-1;-1;261,1103;-1;-1;1103,10;8
2962,2962,2962,2962,2962,2962,2962,2962,ICLR,2019,,,youngjoon.yoo@navercorp.com,4;4;6,4;5;5,Withdrawn,0,0,,yes,9/27/18,NAVER,-1,-1,
2963,2963,2963,2963,2963,2963,2963,2963,ICLR,2019,,Dai Quoc Nguyen;Tu Dinh Nguyen;Dinh Phung,dai.nguyen@monash.edu;tu.dinh.nguyen@monash.edu;dinh.phung@monash.edu,4;5;5,4;3;4,Withdrawn,0,3,,yes,9/27/18,Monash University;Monash University;Monash University,123;123;123,80;80;80,
2964,2964,2964,2964,2964,2964,2964,2964,ICLR,2019,MCTSBug: Generating Adversarial Text Sequences via Monte Carlo Tree Search and Homoglyph Attack,Ji Gao;Jack Lanchantin;Yanjun Qi,jg6yd@virginia.edu;jjl5sw@virginia.edu;yanjun@virginia.edu,3;4,4;3,Withdrawn,0,1,,yes,9/27/18,University of Virginia;University of Virginia;University of Virginia,65;65;65,113;113;113,4
2965,2965,2965,2965,2965,2965,2965,2965,ICLR,2019,Bamboo: Ball-Shape Data Augmentation Against Adversarial Attacks from All Directions,Huanrui Yang;Jingchi Zhang;Hsin-Pai Cheng;Wenhan Wang;Yiran Chen;Hai Li,huanrui.yang@duke.edu;jingchi.zhang@duke.edu;hc218@duke.edu;wenhanw@microsoft.com;yiran.chen@duke.edu;hai.li@duke.edu,4;3,3;5,Withdrawn,3,0,,yes,9/27/18,Duke University;Duke University;Duke University;Microsoft;Duke University;Duke University,44;44;44;-1;44;44,17;17;17;-1;17;17,4
2966,2966,2966,2966,2966,2966,2966,2966,ICLR,2019,Learning Grounded Sentence Representations by Jointly Using Video and Text Information,Patrick Bordes;Eloi Zablocki;Laure Soulier;Benjamin Piwowarski;Patrick Gallinari,patrick.bordes@lip6.fr;eloi.zablocki@gmail.com;laure.soulier@lip6.fr;benjamin.piwowarski@lip6.fr;patrick.gallinari@lip6.fr,4;3;6,4;5;4,Withdrawn,0,1,,yes,9/27/18,LIP6;LIP6;LIP6;LIP6;LIP6,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,
2967,2967,2967,2967,2967,2967,2967,2967,ICLR,2019,Isolating effects of age with fair representation learning when assessing dementia,Zining Zhu;Jekaterina Novikova;Frank Rudzicz,zining.zhu@mail.utoronto.ca;jekaterina@winterlightlabs.com;frank@spoclab.com,4;4;5,3;3;4,Withdrawn,0,4,,yes,9/27/18,Toronto University;Winterlight Labs;University of Toronto,18;-1;18,22;-1;22,7
2968,2968,2968,2968,2968,2968,2968,2968,ICLR,2019,Diagnosing Language Inconsistency in Cross-Lingual Word Embeddings,Yoshinari Fujinuma;Jordan Boyd-Graber;Michael J. Paul,yoshinari.fujinuma@colorado.edu;jbg@umiacs.umd.edu;michael.j.paul@colorado.edu,6;4;4,4;4;5,Withdrawn,0,0,,yes,9/27/18,"University of Colorado, Boulder;University of Maryland, College Park;University of Colorado, Boulder",44;12;44,100;69;100,10
2969,2969,2969,2969,2969,2969,2969,2969,ICLR,2019,Bilingual-GAN: Neural Text Generation and Neural Machine Translation as Two Sides of the Same Coin,Ahmad Rashid;Alan Do-Omri;Mehdi Rezagholizadeh;Md. Akmal Haidar;Hamed Sadeghi,ahmadrash@gmail.com;alan.do-omri@mail.mcgill.ca;mehdi.rezagholizadeh@gmail.com;md.akmal.haidar@huawei.com;haamed.sadeghi@gmail.com,3;4;4,5;3;5,Withdrawn,0,0,,yes,9/27/18,Huawei Technologies Ltd.;McGill University;;Huawei Technologies Ltd.;Huawei Technologies Ltd.,-1;85;-1;-1;-1,-1;42;-1;-1;-1,5;4
2970,2970,2970,2970,2970,2970,2970,2970,ICLR,2019,"Learning Robust, Transferable Sentence Representations for Text Classification",Wasi Uddin Ahmad;Xueying Bai;Nanyun Peng;Kai-Wei Chang,wasiahmad@cs.ucla.edu;xubai@cs.stonybrook.edu;npeng@isi.edu;kwchang@cs.ucla.edu,4;3;4,4;2;4,Withdrawn,0,2,,yes,9/27/18,"University of California, Los Angeles;State University of New York, Stony Brook;USC/ISI;University of California, Los Angeles",20;41;-1;20,15;258;-1;15,6
2971,2971,2971,2971,2971,2971,2971,2971,ICLR,2019,,Masoud Faraki;Mahsa Baktashmotlagh;Tom Drummond;Mathieu Salzmann,masoud.faraki@monash.edu;m.baktashmotlagh@qut.edu.au;tom.drummond@monash.edu;mathieu.salzmann@epfl.ch,4;4;3,4;4;5,Withdrawn,2,0,,yes,9/27/18,Monash University;South China University of Technology;Monash University;Swiss Federal Institute of Technology Lausanne,123;478;123;478,80;576;80;38,
2972,2972,2972,2972,2972,2972,2972,2972,ICLR,2019,Empirical observations on the instability of aligning word vector spaces with GANs,Mareike Hartmann;Yova Kementchedjhieva;Anders Søgaard,hartmann@di.ku.dk;yova@di.ku.dk;soegaard@di.ku.dk,4;6;5,4;3;4,Withdrawn,0,0,,yes,9/27/18,University of Copenhagen;University of Copenhagen;University of Copenhagen,99;99;99,109;109;109,3;4;5
2973,2973,2973,2973,2973,2973,2973,2973,ICLR,2019,Low-Rank Matrix Factorization of LSTM as Effective Model Compression,Genta Indra Winata;Andrea Madotto;Jamin Shin;Elham J. Barezi,giwinata@connect.ust.hk;amadotto@connect.ust.hk;jay.shin@connect.ust.hk;ejs@connect.ust.hk,5;5;4,4;2;4,Withdrawn,2,3,,yes,9/27/18,The Hong Kong University of Science and Technology;The Hong Kong University of Science and Technology;The Hong Kong University of Science and Technology;The Hong Kong University of Science and Technology,39;39;39;39,44;44;44;44,3
2974,2974,2974,2974,2974,2974,2974,2974,ICLR,2019,Combining Global Sparse Gradients with Local Gradients,Alham Fikri Aji;Kenneth Heafield,a.fikri@ed.ac.uk;kheafiel@inf.ed.ac.uk,5;5;3,4;3;4,Withdrawn,0,0,,yes,9/27/18,University of Edinburgh;University of Edinburgh,33;33,27;27,3
2975,2975,2975,2975,2975,2975,2975,2975,ICLR,2019,KNOWLEDGE DISTILL VIA LEARNING NEURON MANIFOLD,Zeyi Tao;Qi Xia;Qun Li,ztao@email.wm.edu;qxia01@email.wm.edu;liqun@cs.wm.edu,5;1;3,3;5;4,Withdrawn,0,0,,yes,9/27/18,College of William and Mary;College of William and Mary;College of William and Mary,169;169;169,261;261;261,6
2976,2976,2976,2976,2976,2976,2976,2976,ICLR,2019,Adversarial Decomposition of Text Representation,Alexey Romanov;Anna Rumshisky;Anna Rogers;David Donahue,jgc128@outlook.com;arum@cs.uml.edu;arogers@cs.uml.edu;david_donahue@student.uml.edu,3;6;4,4;3;3,Withdrawn,0,4,,yes,9/27/18,"University of Massachusetts, Lowell;University of Massachusetts, Lowell;University of Massachusetts, Lowell;University of Massachusetts, Lowell",-1;-1;-1;-1,-1;-1;-1;-1,4
2977,2977,2977,2977,2977,2977,2977,2977,ICLR,2019,How to learn (and how not to learn) multi-hop reasoning with memory networks,Jifan Chen;Greg Durrett,jf_chen@utexas.edu;gdurrett@cs.utexas.edu,3;5;5,5;5;4,Withdrawn,0,0,,yes,9/27/18,"University of Texas, Austin;University of Texas, Austin",22;22,49;49,
2978,2978,2978,2978,2978,2978,2978,2978,ICLR,2019,Learning to Attend On Essential Terms: An Enhanced Retriever-Reader Model for Open-domain Question Answering,Jianmo Ni;Chenguang Zhu;Weizhu Chen;Julian McAuley,jin018@ucsd.edu;chezhu@microsoft.com;wzchen@microsoft.com;jmcauley@cs.ucsd.edu,4;5;5,4;4;4,Withdrawn,2,0,,yes,9/27/18,"University of California, San Diego;Microsoft;Microsoft;University of California, San Diego",11;-1;-1;11,31;-1;-1;31,
2979,2979,2979,2979,2979,2979,2979,2979,ICLR,2019,The Missing Ingredient in Zero-Shot Neural Machine Translation,Naveen Arivazhagan;Ankur Bapna;Orhan Firat;Roee Aharoni;Melvin Johnson;Wolfgang Macherey,naveenariva@gmail.com;ankurbpn@google.com;orhanf@google.com;roee.aharoni@gmail.com;melvinp@google.com;wmach@google.com,5;4;3,5;3;3,Withdrawn,0,6,,yes,9/27/18,Google;Google;Google;Bar Ilan University;Google;Google,-1;-1;-1;95;-1;-1,-1;-1;-1;456;-1;-1,3;6;8
2980,2980,2980,2980,2980,2980,2980,2980,ICLR,2019,Iterative Binary Decisions,Stephan Alaniz;Zeynep Akata,s.alaniz@uva.nl;z.akata@uva.nl,4;4;4,4;4;3,Withdrawn,0,1,,yes,9/27/18,University of Amsterdam;University of Amsterdam,169;169,59;59,6
2981,2981,2981,2981,2981,2981,2981,2981,ICLR,2019,What Is in a Translation Unit?  Comparing Character and Subword Representations Beyond Translation,Nadir Durrani;Fahim Dalvi;Hassan Sajjad;Yonatan Belinkov;Preslav Nakov,ndurrani@qf.org.qa;faimaduddin@qf.org.qa;hsajjad@qf.org.qa;belinkov@mit.edu;pnakov@hbku.edu.qa,5;5;5,4;4;3,Withdrawn,0,0,,yes,9/27/18,QCRI;QCRI;QCRI;Massachusetts Institute of Technology;Peking University,199;199;199;2;24,1103;1103;1103;5;27,3;2
2982,2982,2982,2982,2982,2982,2982,2982,ICLR,2019,Robust Text Classifier on Test-Time Budgets,Md Rizwan Parvez;Tolga Bolukbasi;Kai-Wei Chang;Venkatesh Saligrama,rizwan@cs.ucla.edu;tolgab@bu.edu;kwchang@cs.ucla.edu;srv@bu.edu,4;4;5,4;4;3,Withdrawn,0,0,,yes,9/27/18,"University of California, Los Angeles;Boston University;University of California, Los Angeles;Boston University",20;65;20;65,15;70;15;70,
2983,2983,2983,2983,2983,2983,2983,2983,ICLR,2019,Hiding Objects from Detectors: Exploring Transferrable Adversarial Patterns,Shangbang Long;Jie Fu;Chris Pal,longlongsb@pku.edu.cn;jie.fu@polymtl.ca;christopher.pal@polymtl.ca,6;4;3,4;4;3,Withdrawn,0,3,,yes,9/27/18,Peking University;Polytechnique Montreal;Polytechnique Montreal,24;386;386,27;108;108,4;1
2984,2984,2984,2984,2984,2984,2984,2984,ICLR,2019,Unsupervised Latent Tree Induction with Deep Inside-Outside Recursive Auto-Encoders ,Andrew Drozdov;Patrick Verga;Mohit Yadev;Mohit Iyyer;Andrew McCallum,adrozdov@cs.umass.edu;pat@cs.umass.edu;ymohit@cs.umass.edu;miyyer@cs.umass.edu;mccallum@cs.umass.edu,5;6;2,4;3;4,Withdrawn,0,5,,yes,9/27/18,"University of Massachusetts, Amherst;University of Massachusetts, Amherst;University of Massachusetts, Amherst;University of Massachusetts, Amherst;University of Massachusetts, Amherst",30;30;30;30;30,191;191;191;191;191,2
2985,2985,2985,2985,2985,2985,2985,2985,ICLR,2019,Tangent-Normal Adversarial Regularization for Semi-supervised Learning,Bing Yu;Jingfeng Wu;Jinwen Ma;Zhanxing Zhu,byu@pku.edu.cn;pkuwjf@pku.edu.cn;jwma@math.pku.edu.cn;zhanxing.zhu@pku.edu.cn,5;4;7,3;5;4,Withdrawn,0,1,,yes,9/27/18,Peking University;Peking University;Peking University;Peking University,24;24;24;24,27;27;27;27,4
2986,2986,2986,2986,2986,2986,2986,2986,ICLR,2019,Answer-based Adversarial Training for Generating Clarification Questions,Sudha Rao;Hal Daumé III,raosudha@cs.umd.edu;hal@umiacs.umd.edu,4;4;6,4;5;4,Withdrawn,0,0,,yes,9/27/18,"University of Maryland, College Park;University of Maryland, College Park",12;12,69;69,5;4
2987,2987,2987,2987,2987,2987,2987,2987,ICLR,2019,IncSQL: Training Incremental Text-to-SQL Parsers with Non-Deterministic Oracles,Tianze Shi;Kedar Tatwawadi;Kaushik Chakrabarti;Yi Mao;Oleksandr Polozov;Weizhu Chen,tianze@cs.cornell.edu;kedart@stanford.edu;kaushik@microsoft.com;maoyi@microsoft.com;polozov@microsoft.com;wzchen@microsoft.com,4;6;3,4;3;5,Withdrawn,0,0,,yes,9/27/18,Cornell University;Stanford University;Microsoft;Microsoft;Microsoft;Microsoft,7;4;-1;-1;-1;-1,19;3;-1;-1;-1;-1,3
2988,2988,2988,2988,2988,2988,2988,2988,ICLR,2019,Mitigating Bias in Natural Language Inference Using Adversarial Learning,Yonatan Belinkov;Adam Poliak;Stuart M. Shieber;Benjamin Van Durme,belinkov@seas.harvard.edu;azpoliak@cs.jhu.edu;shieber@seas.harvard.edu;vandurme@cs.jhu.edu,4;4;8,5;4;4,Withdrawn,0,5,,yes,9/27/18,Harvard University;Johns Hopkins University;Harvard University;Johns Hopkins University,39;72;39;72,6;13;6;13,3;4
2989,2989,2989,2989,2989,2989,2989,2989,ICLR,2019,Multi-Modal Generative Adversarial Networks for Diverse Datasets,Matan Ben-Yosef;Daphna Weinshall,matan.benyosef@mail.huji.ac.il;daphna@cs.huji.ac.il,4;6,4;4,Withdrawn,0,0,,yes,9/27/18,Hebrew University of Jerusalem;Hebrew University of Jerusalem,65;65,205;205,5;4
2990,2990,2990,2990,2990,2990,2990,2990,ICLR,2019,Few-Shot Learning by Exploiting Object Relation,Liangqu Long;Wei Wang;Jun Wen;Meihui  Zhang;Qian  Lin,liangqu.long@gmail.com;wangwei@comp.nus.edu.sg;jungel2star@gmail.com;meihui_zhang@bit.edu.cn;linqian@comp.nus.edu.sg,6;4;4,4;4;3,Withdrawn,0,0,,yes,9/27/18,;National University of Singapore;;BIT;National University of Singapore,-1;16;-1;-1;16,-1;22;-1;-1;22,6
2991,2991,2991,2991,2991,2991,2991,2991,ICLR,2019,CrystalGAN: Learning to Discover Crystallographic Structures with Generative Adversarial Networks,Asma Nouira;Nataliya Sokolovska;Jean-Claude Crivello,asma.nouira.91@gmail.com;nataliya.sokolovska@upmc.fr;jccrivello@icmpe.cnrs.fr,3;7;4,4;2;2,Withdrawn,0,0,,yes,9/27/18,";Computer Science Lab  - Pierre and Marie Curie University, Paris, France;CNRS",-1;478;-1,-1;123;-1,5;4
2992,2992,2992,2992,2992,2992,2992,2992,ICLR,2019,Exploration using Distributional RL and UCB,Borislav Mavrin;Hengshuai Yao;Linglong Kong;ShangtongZhang,mavrin@ualberta.ca;hengshuai.yao@huawei.com;lkong@ualberta.ca;zhangshangtong.cpp@gmail.com,4;4;4,3;5;4,Withdrawn,0,3,,yes,9/27/18,University of Alberta;Huawei Technologies Ltd.;University of Alberta;University of Oxford,99;-1;99;50,119;-1;119;1,1
2993,2993,2993,2993,2993,2993,2993,2993,ICLR,2019,The Body is not a Given: Joint Agent Policy Learning and Morphology Evolution,Dylan Banarse;Yoram Bachrach;Siqi Liu;Chrisantha Fernando;Nicolas Heess;Pushmeet Kohli;Guy Lever;Thore Graepel,dylski@google.com;yorambac@google.com;guylever@google.com;heess@google.com;pushmeet@google.com;liusiqi@google.com;chrisantha@google.com;thore@google.com,4;4;3;4,4;4;4;3,Withdrawn,0,1,,yes,9/27/18,Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1,
2994,2994,2994,2994,2994,2994,2994,2994,ICLR,2019,From Adversarial Training to Generative Adversarial Networks,Xuanqing Liu;Cho-Jui Hsieh,xqliu@cs.ucla.edu;chohsieh@cs.ucla.edu,3;6;4,3;3;4,Withdrawn,0,1,,yes,9/27/18,"University of California, Los Angeles;University of California, Los Angeles",20;20,15;15,5;4
2995,2995,2995,2995,2995,2995,2995,2995,ICLR,2019,An Efficient Network for Predicting Time-Varying Distributions,Connie Kou;Hwee Kuan Lee;Teck Khim Ng;Jorge Sanz,koukl@comp.nus.edu.sg;leehk@bii.a-star.edu.sg;ngtk@comp.nus.edu.sg;jorges@nus.edu.sg,5;4;5,4;3;4,Withdrawn,0,0,,yes,9/27/18,National University of Singapore;A*STAR;National University of Singapore;National University of Singapore,16;-1;16;16,22;-1;22;22,
2996,2996,2996,2996,2996,2996,2996,2996,ICLR,2019,Quantile Regression Reinforcement Learning with State Aligned Vector Rewards,Oliver Richter;Roger Wattenhofer,richtero@ethz.ch;wattenhofer@ethz.ch,4;3;4,4;3;4,Withdrawn,0,8,,yes,9/27/18,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10,10;10,
2997,2997,2997,2997,2997,2997,2997,2997,ICLR,2019,Fatty and Skinny: A Joint Training Method of Watermark Encoder and Decoder,Sanghyun Hong;Mahmoud Mohammadi;Noseong Park,shhong@cs.umd.edu;mmoham12@uncc.edu;npark9@gmu.edu,4;4;4,4;4;4,Withdrawn,0,0,,yes,9/27/18,"University of Maryland, College Park;University of North Carolina, Charlotte;George Mason University",12;314;99,69;1103;336,5;4;1
2998,2998,2998,2998,2998,2998,2998,2998,ICLR,2019,Transfer Learning via Unsupervised Task Discovery for Visual Question Answering,Hyeonwoo Noh;Taehoon Kim;Jonghwan Mun;Bohyung Han,shgusdngogo@postech.ac.kr;carpedm20@gmail.com;choco1916@postech.ac.kr;bhhan@snu.ac.kr,4;5;8,5;5;5,Withdrawn,0,0,,yes,9/27/18,POSTECH;OpenAI;POSTECH;Seoul National University,123;-1;123;41,137;-1;137;74,
2999,2999,2999,2999,2999,2999,2999,2999,ICLR,2019,Confidence Calibration in Deep Neural Networks through Stochastic Inferences,Seonguk Seo;Paul Hongsuck Seo;Bohyung Han,seonguk@snu.ac.kr;hsseo@postech.ac.kr;bhhan@snu.ac.kr,5;3;5,4;2;4,Withdrawn,0,0,,yes,9/27/18,Seoul National University;POSTECH;Seoul National University,41;123;41,74;137;74,11
3000,3000,3000,3000,3000,3000,3000,3000,ICLR,2019,Noise-Tempered Generative Adversarial Networks,Simon Jenni;Paolo Favaro,jenni@inf.unibe.ch;paolo.favaro@inf.unibe.ch,4;5;5,5;4;4,Withdrawn,0,4,,yes,9/27/18,University of Bern;University of Bern,386;386,105;105,5;4
3001,3001,3001,3001,3001,3001,3001,3001,ICLR,2019,SpaMHMM: Sparse Mixture of Hidden Markov Models for Graph Connected Entities,Diogo Pernes;Jaime S. Cardoso,dpc@inesctec.pt;jaime.cardoso@inesctec.pt,3;3;3,4;4;4,Withdrawn,0,3,,yes,9/27/18,University of Porto;University of Porto,18;18,22;22,5;10
3002,3002,3002,3002,3002,3002,3002,3002,ICLR,2019,Self-Binarizing Networks,Fayez Lahoud;Radhakrishna Achanta;Pablo Márquez-Neila;Sabine Süsstrunk,fayez.lahoud@epfl.ch;radhakrishna.achanta@epfl.ch;pablo.marquez@artorg.unibe.ch;sabine.susstrunk@epfl.ch,5;5;5,4;4;4,Withdrawn,4,1,,yes,9/27/18,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;University of Bern;Swiss Federal Institute of Technology Lausanne,478;478;386;478,38;38;105;38,
3003,3003,3003,3003,3003,3003,3003,3003,ICLR,2019,UNSUPERVISED MONOCULAR DEPTH ESTIMATION WITH CLEAR BOUNDARIES,Yihan Hu;Heng Luo;Yifeng Geng,y4hu@eng.ucsd.edu;heng.luo@horizon.ai;yifeng.geng@horizon.ai,4;4;3,3;5;4,Withdrawn,0,0,,yes,9/27/18,"University of California, San Diego;Horizon Robotics;Horizon Robotics",11;-1;-1,31;-1;-1,
3004,3004,3004,3004,3004,3004,3004,3004,ICLR,2019,Object-Contrastive Networks: Unsupervised Object Representations,Soeren Pirk;Mohi Khansari;Yunfei Bai;Corey Lynch;Pierre Sermanet,pirk@google.com;khansari@google.com;yunfeibai@google.com;coreylynch@google.com;sermanet@google.com,3;3;5,5;5;4,Withdrawn,0,0,,yes,9/27/18,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,
3005,3005,3005,3005,3005,3005,3005,3005,ICLR,2019,TFGAN: Improving Conditioning for Text-to-Video Synthesis,Yogesh Balaji;Martin Renqiang Min;Bing Bai;Rama Chellappa;Hans Peter Graf,yogesh@cs.umd.edu;renqiang@nec-labs.com;bbai@nec-labs.com;rama@umiacs.umd.edu;hpg@nec-labs.com,6;3;5,3;5;4,Withdrawn,0,0,,yes,9/27/18,"University of Maryland, College Park;NEC-Labs;NEC-Labs;University of Maryland, College Park;NEC-Labs",12;-1;-1;12;-1,69;-1;-1;69;-1,5;4
3006,3006,3006,3006,3006,3006,3006,3006,ICLR,2019,Learning Graph Decomposition,Jie Song;Bjoern Andres;Michael Black;Otmar Hilliges;Siyu Tang,jsong@inf.ethz.ch;bjoern.andres@de.bosch.com;black@tuebingen.mpg.de;otmar.hilliges@inf.ethz.ch;stang@tuebingen.mpg.de,7;4;5,4;4;4,Withdrawn,0,0,,yes,9/27/18,"Swiss Federal Institute of Technology;Bosch;Max Planck Institute for Intelligent Systems, Max-Planck Institute;Swiss Federal Institute of Technology;Max Planck Institute for Intelligent Systems, Max-Planck Institute",10;-1;-1;10;-1,10;-1;-1;10;-1,2;10
3007,3007,3007,3007,3007,3007,3007,3007,ICLR,2019,Logically-Constrained Neural Fitted Q-iteration,Mohammadhosein Hasanbeig;Alessandro Abate;Daniel Kroening,hosein.hasanbeig@cs.ox.ac.uk;aabate@cs.ox.ac.uk;kroening@cs.ox.ac.uk,5;4;5,2;5;4,Withdrawn,0,5,,yes,9/27/18,University of Oxford;University of Oxford;University of Oxford,50;50;50,1;1;1,
3008,3008,3008,3008,3008,3008,3008,3008,ICLR,2019,Distributed Deep Policy Gradient for Competitive Adversarial Environment,Denis Osipychev;Girish Chowdhary,deniso2@illinois.edu;girishc@illinois.edu,4;4;3,4;3;5,Withdrawn,0,0,,yes,9/27/18,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",3;3,37;37,
3009,3009,3009,3009,3009,3009,3009,3009,ICLR,2019,Found by NEMO: Unsupervised Object Detection from Negative Examples and Motion,Rico Jonschkowski,rjon@google.com,5;3;4,4;4;4,Withdrawn,0,1,,yes,9/27/18,Google,-1,-1,2
3010,3010,3010,3010,3010,3010,3010,3010,ICLR,2019,Visualizing and Discovering Behavioural Weaknesses in Deep Reinforcement Learning,Christian Rupprecht;Cyril Ibrahim;Chris Pal,christian.rupprecht@in.tum.de;cyril.ibrahim@elementai.com;christopher.pal@polymtl.ca,5;5;4,4;4;5,Withdrawn,0,2,,yes,9/27/18,Technical University Munich;Element AI;Polytechnique Montreal,54;-1;386,41;-1;108,5
3011,3011,3011,3011,3011,3011,3011,3011,ICLR,2019,Estimating Heterogeneous Treatment Effects Using Neural Networks With The Y-Learner,Bradly C. Stadie;Sören R. Künzel;Nikita Vemuri;Jasjeet S. Sekhon,bstadie@berkeley.edu;srk@berkeley.edu;nikitavemuri@berkeley.edu;sekhon@berkeley.edu,5;5;4,3;4;4,Withdrawn,0,0,,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5,18;18;18;18,
3012,3012,3012,3012,3012,3012,3012,3012,ICLR,2019,Generative Model For Material Irradiation Experiments Based On Prior Knowledge And Attention Mechanism,MinCong Luo;Li Liu,luomincong@foxmail.com;1920148271@qq.com,3;3,4;4,Withdrawn,0,0,,yes,9/27/18,Chinese Academy of Sciences;,62;-1,1103;-1,5;4
3013,3013,3013,3013,3013,3013,3013,3013,ICLR,2019, Generating Text through Adversarial Training using Skip-Thought Vectors,Afroz Ahamad,afrozsahamad@gmail.com,3;2;2,5;5;5,Withdrawn,0,0,,yes,9/27/18,"BITS Pilani, BITS Pilani",-1,-1,3;4;5
3014,3014,3014,3014,3014,3014,3014,3014,ICLR,2019,Evolving intrinsic motivations for altruistic behavior,Jane X. Wang;Edward Hughes;Chrisantha Fernando;Wojciech M. Czarnecki;Edgar A. Duenez-Guzman;Joel Z. Leibo,wangjane@google.com;edwardhughes@google.com;chrisantha@google.com;lejlot@google.com;duenez@google.com;jzl@google.com,5;6;3,3;2;4,Withdrawn,0,0,,yes,9/27/18,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,
3015,3015,3015,3015,3015,3015,3015,3015,ICLR,2020,Pitfalls of In-Domain Uncertainty Estimation and Ensembling in Deep Learning,Arsenii Ashukha;Alexander Lyzhov;Dmitry Molchanov;Dmitry Vetrov,ars.ashuha@gmail.com;alex.grig.lyzhov@gmail.com;dmolch111@gmail.com;vetrovd@yandex.ru,6;8;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,1.0,yes,9/25/19,Samsung;Skolkovo Institute of Science and Technology;Samsung;Higher School of Economics,-1;-1;-1;481,-1;-1;-1;251,
3016,3016,3016,3016,3016,3016,3016,3016,ICLR,2020,A Theory of Usable Information under Computational Constraints,Yilun Xu;Shengjia Zhao;Jiaming Song;Russell Stewart;Stefano Ermon,xuyilun@pku.edu.cn;sjzhao@stanford.edu;tsong@cs.stanford.edu;russell.sb.nebel@gmail.com;ermon@cs.stanford.edu,8;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,3,0.0,yes,9/25/19,Peking University;Stanford University;Stanford University;;Stanford University,22;4;4;-1;4,24;4;4;-1;4,
3017,3017,3017,3017,3017,3017,3017,3017,ICLR,2020,An Inductive Bias for Distances: Neural Nets that Respect the Triangle Inequality,Silviu Pitis;Harris Chan;Kiarash Jamali;Jimmy Ba,spitis@cs.toronto.edu;hchan@cs.toronto.edu;kiarash.jamali@mail.utoronto.ca;jba@cs.toronto.edu,8;3;8;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0.0,yes,9/25/19,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Toronto University;Department of Computer Science, University of Toronto",18;18;18;18,18;18;18;18,1;10
3018,3018,3018,3018,3018,3018,3018,3018,ICLR,2020,Stochastic AUC Maximization with Deep Neural Networks,Mingrui Liu;Zhuoning Yuan;Yiming Ying;Tianbao Yang,mingrui-liu@uiowa.edu;zhuoning-yuan@uiowa.edu;yying@albany.edu;tianbao-yang@uiowa.edu,6;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,6,0.0,yes,9/25/19,"University of Iowa;University of Iowa;State University of New York, Albany;University of Iowa",154;154;266;154,227;227;350;227,9
3019,3019,3019,3019,3019,3019,3019,3019,ICLR,2020,Convolutional Conditional Neural Processes,Jonathan Gordon;Wessel P. Bruinsma;Andrew Y. K. Foong;James Requeima;Yann Dubois;Richard E. Turner,jg801@cam.ac.uk;wpb23@cam.ac.uk;ykf21@cam.ac.uk;jrr41@cam.ac.uk;yanndubois96@gmail.com;ret26@cam.ac.uk,8;8;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,7,0.0,yes,9/25/19,University of Cambridge;University of Cambridge;University of Cambridge;University of Cambridge;Facebook;University of Cambridge,71;71;71;71;-1;71,3;3;3;3;-1;3,6;8
3020,3020,3020,3020,3020,3020,3020,3020,ICLR,2020,On the interaction between supervision and self-play in emergent communication,Ryan Lowe*;Abhinav Gupta*;Jakob Foerster;Douwe Kiela;Joelle Pineau,rlowe1@cs.mcgill.ca;abhinav.gupta@umontreal.ca;jakobfoerster@gmail.com;dkiela@fb.com;jpineau@cs.mcgill.ca,6;8;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0.0,yes,9/25/19,McGill University;University of Montreal;Facebook;Facebook;McGill University,86;128;-1;-1;86,42;85;-1;-1;42,3
3021,3021,3021,3021,3021,3021,3021,3021,ICLR,2020,Towards Better Understanding of Adaptive Gradient Algorithms in Generative Adversarial Nets,Mingrui Liu;Youssef Mroueh;Jerret Ross;Wei Zhang;Xiaodong Cui;Payel Das;Tianbao Yang,mingrui-liu@uiowa.edu;mroueh@us.ibm.com;rossja@us.ibm.com;weiz@us.ibm.com;cuix@us.ibm.com;daspa@us.ibm.com;tianbao-yang@uiowa.edu,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Accept (Poster),0,7,0.0,yes,9/25/19,University of Iowa;International Business Machines;International Business Machines;International Business Machines;International Business Machines;International Business Machines;University of Iowa,154;-1;-1;-1;-1;-1;154,227;-1;-1;-1;-1;-1;227,5;9
3022,3022,3022,3022,3022,3022,3022,3022,ICLR,2020,TabFact: A Large-scale Dataset for Table-based Fact Verification,Wenhu Chen;Hongmin Wang;Jianshu Chen;Yunkai Zhang;Hong Wang;Shiyang Li;Xiyou Zhou;William Yang Wang,wenhuchen@ucsb.edu;hongmin@ucsb.edu;chenjianshu@gmail.com;yunkai_zhang@ucsb.edu;hongwang600@ucsb.edu;shiyangli@ucsb.edu;xiyou@ucsb.edu;william@cs.ucsb.edu,6;6;8,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:N/A:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Poster),0,6,1.0,yes,9/25/19,UC Santa Barbara;UC Santa Barbara;Tencent AI Lab;UC Santa Barbara;UC Santa Barbara;UC Santa Barbara;UC Santa Barbara;UC Santa Barbara,38;38;-1;38;38;38;38;38,57;57;-1;57;57;57;57;57,3;10
3023,3023,3023,3023,3023,3023,3023,3023,ICLR,2020,Keep Doing What Worked: Behavior Modelling Priors for Offline Reinforcement Learning,Noah Siegel;Jost Tobias Springenberg;Felix Berkenkamp;Abbas Abdolmaleki;Michael Neunert;Thomas Lampe;Roland Hafner;Nicolas Heess;Martin Riedmiller,siegeln@google.com;springenberg@google.com;befelix@inf.ethz.ch;aabdolmaleki@google.com;neunertm@google.com;thomaslampe@google.com;rhafner@google.com;heess@google.com;riedmiller@google.com,6;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,Google;Google;Swiss Federal Institute of Technology;Google;Google;Google;Google;Google;Google,-1;-1;10;-1;-1;-1;-1;-1;-1,-1;-1;13;-1;-1;-1;-1;-1;-1,
3024,3024,3024,3024,3024,3024,3024,3024,ICLR,2020,Scale-Equivariant Steerable Networks,Ivan Sosnovik;Michał Szmaja;Arnold Smeulders,sosnovikivan@gmail.com;szmajamichal@gmail.com;a.w.m.smeulders@uva.nl,8;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,5,0.0,yes,9/25/19,University of Amsterdam;University of Amsterdam;University of Amsterdam,172;172;172,62;62;62,
3025,3025,3025,3025,3025,3025,3025,3025,ICLR,2020,Learning Hierarchical Discrete Linguistic Units from Visually-Grounded Speech,David Harwath*;Wei-Ning Hsu*;James Glass,dharwath@csail.mit.edu;wnhsu@mit.edu;glass@mit.edu,8;8;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,6,0.0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2,5;5;5,
3026,3026,3026,3026,3026,3026,3026,3026,ICLR,2020,Inductive and Unsupervised Representation Learning on Graph Structured Objects,Lichen Wang;Bo Zong;Qianqian Ma;Wei Cheng;Jingchao Ni;Wenchao Yu;Yanchi Liu;Dongjin Song;Haifeng Chen;Yun Fu,wanglichenxj@gmail.com;bzong@nec-labs.com;maqq@bu.edu;weicheng@nec-labs.com;jni@nec-labs.com;wyu@nec-labs.com;yanchi@nec-labs.com;dsong@nec-labs.com;haifeng@nec-labs.com;yunfu@ece.neu.edu,6;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,7,0.0,yes,9/25/19,Northeastern University;NEC-Labs;Boston University;NEC-Labs;NEC-Labs;NEC-Labs;NEC-Labs;NEC-Labs;NEC-Labs;Northeastern University,16;-1;67;-1;-1;-1;-1;-1;-1;16,906;-1;61;-1;-1;-1;-1;-1;-1;906,10
3027,3027,3027,3027,3027,3027,3027,3027,ICLR,2020,Online and stochastic optimization beyond Lipschitz continuity: A Riemannian approach,Kimon Antonakopoulos;E. Veronica Belmega;Panayotis Mertikopoulos,kimon.antonakopoulos@inria.fr;veronica.belmega@ensea.fr;panayotis.mertikopoulos@imag.fr,8;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,3,0.0,yes,9/25/19,INRIA;ETIS;Imag Montpellier Université,-1;-1;-1,-1;-1;-1,9
3028,3028,3028,3028,3028,3028,3028,3028,ICLR,2020,HOPPITY: LEARNING GRAPH TRANSFORMATIONS TO DETECT AND FIX BUGS IN PROGRAMS,Elizabeth Dinella;Hanjun Dai;Ziyang Li;Mayur Naik;Le Song;Ke Wang,edinella@seas.upenn.edu;hadai@google.com;liby99@seas.upenn.edu;mhnaik@cis.upenn.edu;lsong@cc.gatech.edu;kewang@visa.com,6;8;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Spotlight),0,5,0.0,yes,9/25/19,University of Pennsylvania;Google;University of Pennsylvania;University of Pennsylvania;Georgia Institute of Technology;VISA,19;-1;19;19;13;-1,11;-1;11;11;38;-1,10
3029,3029,3029,3029,3029,3029,3029,3029,ICLR,2020,Learning to Retrieve Reasoning Paths over Wikipedia Graph for Question Answering,Akari Asai;Kazuma Hashimoto;Hannaneh Hajishirzi;Richard Socher;Caiming Xiong,akari@cs.washington.edu;k.hashimoto@salesforce.com;hannaneh@washington.edu;richard@socher.org;cxiong@salesforce.com,6;8;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),1,8,3.0,yes,9/25/19,University of Washington;SalesForce.com;University of Washington;SalesForce.com;SalesForce.com,6;-1;6;-1;-1,26;-1;26;-1;-1,10
3030,3030,3030,3030,3030,3030,3030,3030,ICLR,2020,Drawing Early-Bird Tickets: Toward More Efficient Training of Deep Networks,Haoran You;Chaojian Li;Pengfei Xu;Yonggan Fu;Yue Wang;Xiaohan Chen;Richard G. Baraniuk;Zhangyang Wang;Yingyan Lin,hy34@rice.edu;cl114@rice.edu;px5@rice.edu;yf22@rice.edu;yw68@rice.edu;chernxh@tamu.edu;richb@rice.edu;atlaswang@tamu.edu;yingyan.lin@rice.edu,8;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,6,0.0,yes,9/25/19,Rice University;Rice University;Rice University;Rice University;Rice University;Texas A&M;Rice University;Texas A&M;Rice University,84;84;84;84;84;44;84;44;84,105;105;105;105;105;177;105;177;105,
3031,3031,3031,3031,3031,3031,3031,3031,ICLR,2020,Learning from Rules Generalizing Labeled Exemplars,Abhijeet Awasthi;Sabyasachi Ghosh;Rasna Goyal;Sunita Sarawagi,awasthi@cse.iitb.ac.in;sghosh@cse.iitb.ac.in;rasna.goyal66@gmail.com;sunita@iitb.ac.in,6;8;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),1,5,0.0,yes,9/25/19,Indian Institute of Technology Bombay;Indian Institute of Technology Bombay;;Indian Institute of Technology Bombay,118;118;-1;118,480;480;-1;480,
3032,3032,3032,3032,3032,3032,3032,3032,ICLR,2020,Mirror-Generative Neural Machine Translation,Zaixiang Zheng;Hao Zhou;Shujian Huang;Lei Li;Xin-Yu Dai;Jiajun Chen,zhengzx.142857@gmail.com;zhouhao.nlp@bytedance.com;huangsj@nju.edu.cn;lilei.02@bytedance.com;daixinyu@nju.edu.cn;chenjj@nju.edu.cn,8;8;8,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,7,1.0,yes,9/25/19,Zhejiang University;Bytedance;Zhejiang University;Bytedance;Zhejiang University;Zhejiang University,56;-1;56;-1;56;56,107;-1;107;-1;107;107,3;5
3033,3033,3033,3033,3033,3033,3033,3033,ICLR,2020,Phase Transitions for the Information Bottleneck in Representation Learning,Tailin Wu;Ian Fischer,tailin@cs.stanford.edu;iansf@google.com,6;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,Stanford University;Google,4;-1,4;-1,
3034,3034,3034,3034,3034,3034,3034,3034,ICLR,2020,Model-based reinforcement learning for biological sequence design,Christof Angermueller;David Dohan;David Belanger;Ramya Deshpande;Kevin Murphy;Lucy Colwell,christofa@google.com;ddohan@google.com;dbelanger@google.com;ramyadeshpande@google.com;lcolwell@google.com;kpmurphy@google.com,6;3;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,5
3035,3035,3035,3035,3035,3035,3035,3035,ICLR,2020,GAT: Generative Adversarial Training for Adversarial Example Detection and Robust Classification,Xuwang Yin;Soheil Kolouri;Gustavo K Rohde,xy4cm@virginia.edu;skolouri@hrl.com;gustavo@virginia.edu,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),1,5,0.0,yes,9/25/19,University of Virginia;HRL Labs;University of Virginia,59;-1;59,107;-1;107,5;4
3036,3036,3036,3036,3036,3036,3036,3036,ICLR,2020,StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding,Wei Wang;Bin Bi;Ming Yan;Chen Wu;Jiangnan Xia;Zuyi Bao;Liwei Peng;Luo Si,hebian.ww@alibaba-inc.com;b.bi@alibaba-inc.com;ym119608@alibaba-inc.com;wuchen.wc@alibaba-inc.com;jiangnan.xjn@alibaba-inc.com;zuyi.bzy@alibaba-inc.com;liwei.peng@alibaba-inc.com;luo.si@alibaba-inc.com,3;8;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,3,0.0,yes,9/25/19,Alibaba Group;Alibaba Group;Alibaba Group;Alibaba Group;Alibaba Group;Alibaba Group;Alibaba Group;Alibaba Group,-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1,3
3037,3037,3037,3037,3037,3037,3037,3037,ICLR,2020,Restricting the Flow: Information Bottlenecks for Attribution,Karl Schulz;Leon Sixt;Federico Tombari;Tim Landgraf,karl.schulz@tum.de;leon.sixt@fu-berlin.de;tombari@in.tum.de;tim.landgraf@fu-berlin.de,8;8;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Talk),1,4,2.0,yes,9/25/19,Technical University Munich;Freie Universität Berlin;Technical University Munich;Freie Universität Berlin,53;-1;53;-1,43;-1;43;-1,
3038,3038,3038,3038,3038,3038,3038,3038,ICLR,2020,Oblique Decision Trees from Derivatives of ReLU Networks,Guang-He Lee;Tommi S. Jaakkola,guanghe@csail.mit.edu;tommi@csail.mit.edu,3;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),0,7,0.0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2,5;5,
3039,3039,3039,3039,3039,3039,3039,3039,ICLR,2020,DBA: Distributed Backdoor Attacks against Federated Learning,Chulin Xie;Keli Huang;Pin-Yu Chen;Bo Li,chulinxie@zju.edu.cn;nick_cooper@sjtu.edu.cn;pin-yu.chen@ibm.com;lbo@illinois.edu,6;6;8,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Accept (Poster),0,3,0.0,yes,9/25/19,"Zhejiang University;Shanghai Jiao Tong University;International Business Machines;University of Illinois, Urbana Champaign",56;53;-1;3,107;157;-1;48,4
3040,3040,3040,3040,3040,3040,3040,3040,ICLR,2020,Understanding and Improving Information Transfer in Multi-Task Learning,Sen Wu;Hongyang R. Zhang;Christopher Ré,senwu@cs.stanford.edu;hongyang@cs.stanford.edu;chrismre@stanford.edu,6;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0.0,yes,9/25/19,Stanford University;Stanford University;Stanford University,4;4;4,4;4;4,6
3041,3041,3041,3041,3041,3041,3041,3041,ICLR,2020,FSPool: Learning Set Representations with Featurewise Sort Pooling,Yan Zhang;Jonathon Hare;Adam Prügel-Bennett,yz5n12@ecs.soton.ac.uk;jsh2@ecs.soton.ac.uk;apb@ecs.soton.ac.uk,6;8;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0.0,yes,9/25/19,University of Southampton;University of Southampton;University of Southampton,172;172;172,122;122;122,
3042,3042,3042,3042,3042,3042,3042,3042,ICLR,2020,Cyclical Stochastic Gradient MCMC for Bayesian Deep Learning,Ruqi Zhang;Chunyuan Li;Jianyi Zhang;Changyou Chen;Andrew Gordon Wilson,rz297@cornell.edu;chunyuan.li@duke.edu;jz318@duke.edu;cchangyou@gmail.com;andrewgw@cims.nyu.edu,8;8;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,5,0.0,yes,9/25/19,"Cornell University;Duke University;Duke University;State University of New York, Buffalo;New York University",7;47;47;84;25,19;20;20;263;29,11;1
3043,3043,3043,3043,3043,3043,3043,3043,ICLR,2020,RIDE: Rewarding Impact-Driven Exploration for Procedurally-Generated Environments,Roberta Raileanu;Tim Rocktäschel,raileanu@cs.nyu.edu;tim.rocktaeschel@gmail.com,6;8;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,New York University;Facebook AI Research,25;-1,29;-1,
3044,3044,3044,3044,3044,3044,3044,3044,ICLR,2020,Counterfactuals uncover the modular structure of deep generative models,Michel Besserve;Arash Mehrjou;Rémy Sun;Bernhard Schölkopf,michel.besserve@tuebingen.mpg.de;mehrjou.arash@gmail.com;remy.sun@ens-rennes.fr;bs@tuebingen.mpg.de,8;8;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,"Max Planck Institute for Intelligent Systems, Max-Planck Institute;Max Planck Institute for Intelligent Systems, Max-Planck Institute;Ecole Normale Superieure de Rennes;Max Planck Institute for Intelligent Systems, Max-Planck Institute",-1;-1;100;-1,-1;-1;1397;-1,5
3045,3045,3045,3045,3045,3045,3045,3045,ICLR,2020,Neural Machine Translation with Universal Visual Representation,Zhuosheng Zhang;Kehai Chen;Rui Wang;Masao Utiyama;Eiichiro Sumita;Zuchao Li;Hai Zhao,zhangzs@sjtu.edu.cn;khchen@nict.go.jp;wangrui@nict.go.jp;mutiyama@nict.go.jp;eiichiro.sumita@nict.go.jp;charlee@sjtu.edu.cn;zhaohai@cs.sjtu.edu.cn,6;8;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A,Accept (Spotlight),1,4,0.0,yes,9/25/19,"Shanghai Jiao Tong University;National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology;National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology;National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology;National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology;Shanghai Jiao Tong University;Shanghai Jiao Tong University",53;-1;-1;-1;-1;53;53,157;-1;-1;-1;-1;157;157,3
3046,3046,3046,3046,3046,3046,3046,3046,ICLR,2020,Once-for-All: Train One Network and Specialize it for Efficient Deployment,Han Cai;Chuang Gan;Tianzhe Wang;Zhekai Zhang;Song Han,hancai@mit.edu;ganchuang1990@gmail.com;usedtobe@mit.edu;zhangzk@mit.edu;songhan@mit.edu,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,7,0.0,yes,9/25/19,Massachusetts Institute of Technology;International Business Machines;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;-1;2;2;2,5;-1;5;5;5,2
3047,3047,3047,3047,3047,3047,3047,3047,ICLR,2020,Learning Nearly Decomposable Value Functions Via Communication Minimization,Tonghan Wang*;Jianhao Wang*;Chongyi Zheng;Chongjie Zhang,tonghanwang1996@gmail.com;1040594377@qq.com;chongyeezheng@gmail.com;chongjie@tsinghua.edu.cn,3;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0.0,yes,9/25/19,Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University,8;8;8;8,23;23;23;23,
3048,3048,3048,3048,3048,3048,3048,3048,ICLR,2020,Bridging Mode Connectivity in Loss Landscapes and Adversarial Robustness,Pu Zhao;Pin-Yu Chen;Payel Das;Karthikeyan Natesan Ramamurthy;Xue Lin,zhao.pu@husky.neu.edu;pin-yu.chen@ibm.com;daspa@us.ibm.com;knatesa@us.ibm.com;xue.lin@northeastern.edu,8;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,Northeastern University;International Business Machines;International Business Machines;International Business Machines;Northeastern University,16;-1;-1;-1;16,906;-1;-1;-1;906,4
3049,3049,3049,3049,3049,3049,3049,3049,ICLR,2020,The Break-Even Point on Optimization Trajectories of Deep Neural Networks,Stanislaw Jastrzebski;Maciej Szymczak;Stanislav Fort;Devansh Arpit;Jacek Tabor;Kyunghyun Cho*;Krzysztof Geras*,staszek.jastrzebski@gmail.com;msz93@o2.pl;stanislav.fort@gmail.com;devansharpit@gmail.com;jcktbr@gmail.com;kyunghyun.cho@nyu.edu;k.j.geras@nyu.edu,6;8;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Spotlight),1,7,0.0,yes,9/25/19,New York University;Jagiellonian University;Google;SalesForce.com;Jagiellonian University;New York University;New York University,25;481;-1;-1;481;25;25,29;610;-1;-1;610;29;29,8
3050,3050,3050,3050,3050,3050,3050,3050,ICLR,2020,Empirical Studies on the Properties of Linear Regions in Deep Neural Networks,Xiao Zhang;Dongrui Wu,xiao_zhang@hust.edu.cn;drwu@hust.edu.cn,6;3;8,I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),2,6,0.0,yes,9/25/19,Hong Kong University of Science and Technology;Hong Kong University of Science and Technology,39;39,47;47,
3051,3051,3051,3051,3051,3051,3051,3051,ICLR,2020,Learning to Group: A Bottom-Up Framework for 3D Part Discovery in Unseen Categories,Tiange Luo;Kaichun Mo;Zhiao Huang;Jiarui Xu;Siyu Hu;Liwei Wang;Hao Su,luotg@pku.edu.cn;kaichun@cs.stanford.edu;z2huang@eng.ucsd.edu;jxuat@connect.ust.hk;sy89128@mail.ustc.edu.cn;wanglw@cis.pku.edu.cn;haosu@eng.ucsd.edu,8;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),1,5,0.0,yes,9/25/19,"Peking University;Stanford University;University of California, San Diego;The Hong Kong University of Science and Technology;University of Science and Technology of China;Peking University;University of California, San Diego",22;4;11;39;481;22;11,24;4;31;47;80;24;31,6;2
3052,3052,3052,3052,3052,3052,3052,3052,ICLR,2020,Learning Compositional Koopman Operators for Model-Based Control,Yunzhu Li;Hao He;Jiajun Wu;Dina Katabi;Antonio Torralba,liyunzhu@mit.edu;haohe@mit.edu;jiajunwu.cs@gmail.com;dina@csail.mit.edu;torralba@csail.mit.edu,6;8;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,7,0.0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Google;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;-1;2;2,5;5;-1;5;5,10;8
3053,3053,3053,3053,3053,3053,3053,3053,ICLR,2020,Meta-Learning without Memorization,Mingzhang Yin;George Tucker;Mingyuan Zhou;Sergey Levine;Chelsea Finn,mzyin@utexas.edu;gjt@google.com;mingyuan.zhou@mccombs.utexas.edu;svlevine@eecs.berkeley.edu;cbfinn@cs.stanford.edu,6;8;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,5,1.0,yes,9/25/19,"University of Texas, Austin;Google;University of Texas, Austin;University of California Berkeley;Stanford University",22;-1;22;5;4,38;-1;38;13;4,6
3054,3054,3054,3054,3054,3054,3054,3054,ICLR,2020,Guiding Program Synthesis by Learning to Generate Examples,Larissa Laich;Pavol Bielik;Martin Vechev,llaich@ethz.ch;pavol.bielik@inf.ethz.ch;martin.vechev@inf.ethz.ch,8;3;8,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,9,0.0,yes,9/25/19,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10,13;13;13,
3055,3055,3055,3055,3055,3055,3055,3055,ICLR,2020,Truth or backpropaganda? An empirical investigation of deep learning theory,Micah Goldblum;Jonas Geiping;Avi Schwarzschild;Michael Moeller;Tom Goldstein,goldblumcello@gmail.com;jonas.geiping@uni-siegen.de;avi1@umd.edu;michael.moeller@uni-siegen.de;tomg@cs.umd.edu,8;8;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),3,4,0.0,yes,9/25/19,"University of Maryland, College Park;University of Siegen;University of Maryland, College Park;University of Siegen;University of Maryland, College Park",12;323;12;323;12,91;570;91;570;91,1;8
3056,3056,3056,3056,3056,3056,3056,3056,ICLR,2020,What Can Neural Networks Reason About?,Keyulu Xu;Jingling Li;Mozhi Zhang;Simon S. Du;Ken-ichi Kawarabayashi;Stefanie Jegelka,keyulu@mit.edu;jingling@cs.umd.edu;mozhi@cs.umd.edu;ssdu@ias.edu;k_keniti@nii.ac.jp;stefje@mit.edu,8;8;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,10,0.0,yes,9/25/19,"Massachusetts Institute of Technology;University of Maryland, College Park;University of Maryland, College Park;Institue for Advanced Study, Princeton;Meiji University;Massachusetts Institute of Technology",2;12;12;-1;481;2,5;91;91;-1;332;5,1;10
3057,3057,3057,3057,3057,3057,3057,3057,ICLR,2020,U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation,Junho Kim;Minjae Kim;Hyeonwoo Kang;Kwang Hee Lee,takis0112@gmail.com;minjaekim@ncsoft.com;hwkang0131@ncsoft.com;lkwanghee@gmail.com,8;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,NCSOFT;NCSOFT;NCSOFT;Boeing Korea Engineering and Technology Center,-1;-1;-1;-1,-1;-1;-1;-1,
3058,3058,3058,3058,3058,3058,3058,3058,ICLR,2020,Learning to Learn by Zeroth-Order Oracle,Yangjun Ruan;Yuanhao Xiong;Sashank Reddi;Sanjiv Kumar;Cho-Jui Hsieh,ruanyj3107@zju.edu.cn;yhxiong@cs.ucla.edu;sashank@google.com;sanjivk@google.com;chohsieh@cs.ucla.edu,6;6;8,I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,5,0.0,yes,9/25/19,"Zhejiang University;University of California, Los Angeles;Google;Google;University of California, Los Angeles",56;20;-1;-1;20,107;17;-1;-1;17,4;9
3059,3059,3059,3059,3059,3059,3059,3059,ICLR,2020,Lite Transformer with Long-Short Range Attention,Zhanghao Wu*;Zhijian Liu*;Ji Lin;Yujun Lin;Song Han,zhanghao.wu@outlook.com;zhijian@mit.edu;jilin@mit.edu;yujunlin@mit.edu;songhan@mit.edu,6;8;6,I have read many papers in this area.:N/A:N/A:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2;2,5;5;5;5;5,3
3060,3060,3060,3060,3060,3060,3060,3060,ICLR,2020,Regularizing activations in neural networks via distribution matching with the Wasserstein metric,Taejong Joo;Donggu Kang;Byunghoon Kim,tjoo@estsoft.com;emppunity@gmail.com;byungkim@hanyang.ac.kr,6;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:N/A:N/A:N/A,Accept (Poster),0,4,0.0,yes,9/25/19,Estsoft;;Hanyang University,-1;-1;233,-1;-1;393,3;1;8
3061,3061,3061,3061,3061,3061,3061,3061,ICLR,2020,The Ingredients of Real World Robotic Reinforcement Learning,Henry Zhu;Justin Yu;Abhishek Gupta;Dhruv Shah;Kristian Hartikainen;Avi Singh;Vikash Kumar;Sergey Levine,henryzhu@berkeley.edu;justinvyu@berkeley.edu;abhigupta@berkeley.edu;shah@eecs.berkeley.edu;kristian.hartikainen@gmail.com;avisingh@cs.berkeley.edu;vikashplus@gmail.com;svlevine@eecs.berkeley.edu,8;8;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Spotlight),0,5,0.0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of Oxford;University of California Berkeley;;University of California Berkeley,5;5;5;5;50;5;-1;5,13;13;13;13;1;13;-1;13,
3062,3062,3062,3062,3062,3062,3062,3062,ICLR,2020,Relational State-Space Model for Stochastic Multi-Object Systems,Fan Yang;Ling Chen;Fan Zhou;Yusong Gao;Wei Cao,fanyang01@zju.edu.cn;lingchen@cs.zju.edu.cn;fanzhou@zju.edu.cn;jianchuan.gys@alibaba-inc.com;mingsong.cw@alibaba-inc.com,6;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,7,0.0,yes,9/25/19,Zhejiang University;Zhejiang University;Zhejiang University;Alibaba Group;Alibaba Group,56;56;56;-1;-1,107;107;107;-1;-1,10
3063,3063,3063,3063,3063,3063,3063,3063,ICLR,2020,Empirical Bayes Transductive Meta-Learning with Synthetic Gradients,Shell Xu Hu;Pablo Garcia Moreno;Yang Xiao;Xi Shen;Guillaume Obozinski;Neil Lawrence;Andreas Damianou,dom343@gmail.com;morepabl@amazon.com;yang.xiao@enpc.fr;xi.shen@enpc.fr;guillaume.obozinski@epfl.ch;n.lawrence@sheffield.ac.uk;damianou@amazon.com,6;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,1.0,yes,9/25/19,ENPC;Amazon;ENPC;ENPC;Swiss Federal Institute of Technology Lausanne;University of Sheffield;Amazon,-1;-1;-1;-1;481;205;-1,264;-1;264;264;38;117;-1,8;1;6
3064,3064,3064,3064,3064,3064,3064,3064,ICLR,2020,Implicit Bias of Gradient Descent based Adversarial Training on Separable Data,Yan Li;Ethan X.Fang;Huan Xu;Tuo Zhao,yli939@gatech.edu;xxf13@psu.edu;huan.xu@isye.gatech.edu;tourzhao@gatech.edu,3;8;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,4,0.0,yes,9/25/19,Georgia Institute of Technology;Pennsylvania State University;Georgia Institute of Technology;Georgia Institute of Technology,13;41;13;13,38;78;38;38,4
3065,3065,3065,3065,3065,3065,3065,3065,ICLR,2020,word2ket: Space-efficient Word Embeddings inspired by Quantum Entanglement,Aliakbar Panahi;Seyran Saeedi;Tom Arodz,panahia@vcu.edu;saeedis@vcu.edu;tarodz@vcu.edu,8;8;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,3,0.0,yes,9/25/19,Virginia Commonwealth University;Virginia Commonwealth University;Virginia Commonwealth University,266;266;266,1397;1397;1397,3
3066,3066,3066,3066,3066,3066,3066,3066,ICLR,2020,Towards Fast Adaptation of Neural Architectures with Meta Learning,Dongze Lian;Yin Zheng;Yintao Xu;Yanxiong Lu;Leyu Lin;Peilin Zhao;Junzhou Huang;Shenghua Gao,liandz@shanghaitech.edu.cn;yzheng3xg@gmail.com;xuyt@shanghaitech.edu.cn;alanlu@tencent.com;goshawklin@tencent.com;masonzhao@tencent.com;jzhuang@uta.edu;gaoshh@shanghaitech.edu.cn,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,"ShanghaiTech University;Tencent AI Lab;ShanghaiTech University;Tencent AI Lab;Tencent AI Lab;Tencent AI Lab;University of Texas, Arlington;ShanghaiTech University",481;-1;481;-1;-1;-1;118;481,1397;-1;1397;-1;-1;-1;708;1397,6
3067,3067,3067,3067,3067,3067,3067,3067,ICLR,2020,Hamiltonian Generative Networks,Peter Toth;Danilo J. Rezende;Andrew Jaegle;Sébastien Racanière;Aleksandar Botev;Irina Higgins,petertoth@google.com;danilor@google.com;drewjaegle@google.com;sracaniere@google.com;botev@google.com;irinah@google.com,6;6;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,9,0.0,yes,9/25/19,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,5
3068,3068,3068,3068,3068,3068,3068,3068,ICLR,2020,Residual Energy-Based Models for Text Generation,Yuntian Deng;Anton Bakhtin;Myle Ott;Arthur Szlam;Marc'Aurelio Ranzato,dengyuntian@seas.harvard.edu;yolo@fb.com;aszlam@fb.com;ranzato@fb.com,6;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,1.0,yes,9/25/19,Harvard University;Facebook;Facebook;Facebook,39;-1;-1;-1,7;-1;-1;-1,3
3069,3069,3069,3069,3069,3069,3069,3069,ICLR,2020,Plug and Play Language Models: A Simple Approach to Controlled Text Generation,Sumanth Dathathri;Andrea Madotto;Janice Lan;Jane Hung;Eric Frank;Piero Molino;Jason Yosinski;Rosanne Liu,dathathris@gmail.com;amadotto@connect.ust.hk;lan.janice.j@gmail.com;jane.hung@uber.com;mysterefrank@uber.com;piero@uber.com;yosinski@uber.com;rosanne@uber.com,6;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),6,7,1.0,yes,9/25/19,California Institute of Technology;The Hong Kong University of Science and Technology;Uber;Uber;Uber;Uber;Uber;Uber,143;39;-1;-1;-1;-1;-1;-1,2;47;-1;-1;-1;-1;-1;-1,3
3070,3070,3070,3070,3070,3070,3070,3070,ICLR,2020,"Deep 3D Pan via local adaptive t-shaped"" convolutions with global and local adaptive dilations""",Juan Luis Gonzalez Bello;Munchurl Kim,juanluisgb@kaist.ac.kr;mkimee@kaist.ac.kr,6;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,481;481,110;110,1
3071,3071,3071,3071,3071,3071,3071,3071,ICLR,2020,Conservative Uncertainty Estimation By Fitting  Prior Networks,Kamil Ciosek;Vincent Fortuin;Ryota Tomioka;Katja Hofmann;Richard Turner,kamil.ciosek@microsoft.com;fortuin@inf.ethz.ch;ryoto@microsoft.com;katja.hofmann@microsoft.com;ret26@cam.ac.uk,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),2,13,0.0,yes,9/25/19,Microsoft;Swiss Federal Institute of Technology;Microsoft;Microsoft;University of Cambridge,-1;10;-1;-1;71,-1;13;-1;-1;3,11;2
3072,3072,3072,3072,3072,3072,3072,3072,ICLR,2020,Dynamics-Aware Unsupervised Discovery of Skills,Archit Sharma;Shixiang Gu;Sergey Levine;Vikash Kumar;Karol Hausman,architsh@google.com;shanegu@google.com;slevine@google.com;vikashplus@google.com;karolhausman@google.com,8;8;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,3,0.0,yes,9/25/19,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,6
3073,3073,3073,3073,3073,3073,3073,3073,ICLR,2020,Implementation Matters in Deep RL: A Case Study on PPO and TRPO,Logan Engstrom;Andrew Ilyas;Shibani Santurkar;Dimitris Tsipras;Firdaus Janoos;Larry Rudolph;Aleksander Madry,ailyas@mit.edu;engstrom@mit.edu;shibani@mit.edu;tsipras@mit.edu;firdaus.janoos@twosigma.com;rudolph@csail.mit.edu;madry@mit.edu,8;8;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,10,1.0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Two Sigma;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2;-1;2;2,5;5;5;5;-1;5;5,
3074,3074,3074,3074,3074,3074,3074,3074,ICLR,2020,Mutual Information Gradient Estimation for  Representation Learning,Liangjian Wen;Yiji Zhou;Lirong He;Mingyuan Zhou;Zenglin Xu,wlj6816@gmail.com;zhouyiji@outlook.com;ronghe1217@gmail.com;mingyuan.zhou@mccombs.utexas.edu;zenglin@gmail.com,8;6;3;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,14,0.0,yes,9/25/19,"University of Electronic Science and Technology of China;University of Electronic Science and Technology of China;University of Electronic Science and Technology of China;University of Texas, Austin;University of Electronic Science and Technology of China",481;481;481;22;481,628;628;628;38;628,
3075,3075,3075,3075,3075,3075,3075,3075,ICLR,2020,Minimizing FLOPs to Learn Efficient Sparse Representations,Biswajit Paria;Chih-Kuan Yeh;Ian E.H. Yen;Ning Xu;Pradeep Ravikumar;Barnabás Póczos,bparia@cs.cmu.edu;cjyeh@cs.cmu.edu;a061105@gmail.com;ningxu01@gmail.com;pradeepr@cs.cmu.edu;bapoczos@cs.cmu.edu,8;3;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,7,0.0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;;Amazon;Carnegie Mellon University;Carnegie Mellon University,1;1;-1;-1;1;1,27;27;-1;-1;27;27,
3076,3076,3076,3076,3076,3076,3076,3076,ICLR,2020,Data-Independent Neural Pruning via Coresets,Ben Mussay;Margarita Osadchy;Vladimir Braverman;Samson Zhou;Dan Feldman,bengordoncshaifa@gmail.com;rita@cs.haifa.ac.il;vova@cs.jhu.edu;samsonzhou@gmail.com;dannyf.post@gmail.co,8;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,3,0.0,yes,9/25/19,University of Haifa;University of Haifa;Johns Hopkins University;Carnegie Mellon University;,172;172;73;1;-1,544;544;12;27;-1,4
3077,3077,3077,3077,3077,3077,3077,3077,ICLR,2020,LEARNED STEP SIZE QUANTIZATION,Steven K. Esser;Jeffrey L. McKinstry;Deepika Bablani;Rathinakumar Appuswamy;Dharmendra S. Modha,sesser@us.ibm.com;jlmckins@us.ibm.com;deepika.bablani@ibm.com;rappusw@us.ibm.com;dmodha@us.ibm.com,6;8;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,4,1.0,yes,9/25/19,International Business Machines;International Business Machines;International Business Machines;International Business Machines;International Business Machines,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,
3078,3078,3078,3078,3078,3078,3078,3078,ICLR,2020,Learning To Explore Using Active Neural SLAM,Devendra Singh Chaplot;Dhiraj Gandhi;Saurabh Gupta;Abhinav Gupta;Ruslan Salakhutdinov,chaplot@cs.cmu.edu;dhirajgandhi@fb.com;saurabhg@illinois.edu;abhinavg@cs.cmu.edu;rsalakhu@cs.cmu.edu,6;3;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0.0,yes,9/25/19,"Carnegie Mellon University;Facebook;University of Illinois, Urbana Champaign;Carnegie Mellon University;Carnegie Mellon University",1;-1;3;1;1,27;-1;48;27;27,
3079,3079,3079,3079,3079,3079,3079,3079,ICLR,2020,Towards Stabilizing Batch Statistics in Backward Propagation of Batch Normalization,Junjie Yan;Ruosi Wan;Xiangyu Zhang;Wei Zhang;Yichen Wei;Jian Sun,jjyan17@fudan.edu.cn;wanruosi@megvii.com;zhangxiangyu@megvii.com;weizh@fudan.edu.cn;weiyichen@megvii.com;sunjian@megvii.com,6;6;8,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,1.0,yes,9/25/19,Fudan University;Megvii Technology Inc.;Megvii Technology Inc.;Fudan University;Megvii Technology Inc.;Megvii Technology Inc.,79;-1;-1;79;-1;-1,109;-1;-1;109;-1;-1,1;2
3080,3080,3080,3080,3080,3080,3080,3080,ICLR,2020,Tensor Decompositions for Temporal Knowledge Base Completion,Timothée Lacroix;Guillaume Obozinski;Nicolas Usunier,timothee.lax@gmail.com;guillaume.obozinski@epfl.ch;usunier@fb.com,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,Facebook;Swiss Federal Institute of Technology Lausanne;Facebook,-1;481;-1,-1;38;-1,10
3081,3081,3081,3081,3081,3081,3081,3081,ICLR,2020,Generalization bounds for deep convolutional neural networks,Philip M. Long;Hanie Sedghi,plong@google.com;hsedghi@google.com,6;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),0,8,0.0,yes,9/25/19,Google;Google,-1;-1,-1;-1,1;8
3082,3082,3082,3082,3082,3082,3082,3082,ICLR,2020,Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples,Eleni Triantafillou;Tyler Zhu;Vincent Dumoulin;Pascal Lamblin;Utku Evci;Kelvin Xu;Ross Goroshin;Carles Gelada;Kevin Swersky;Pierre-Antoine Manzagol;Hugo Larochelle,eleni@cs.toronto.edu;tylerzhu@google.com;vdumoulin@google.com;lamblinp@google.com;evcu@google.com;kelvinxu@berkeley.edu;goroshin@google.com;cgel@google.com;kswersky@google.com;manzagop@google.com;hugolarochelle@google.com,3;8;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0.0,yes,9/25/19,"Department of Computer Science, University of Toronto;Google;Google;Google;Google;University of California Berkeley;Google;Google;Google;Google;Google",18;-1;-1;-1;-1;5;-1;-1;-1;-1;-1,18;-1;-1;-1;-1;13;-1;-1;-1;-1;-1,6;8
3083,3083,3083,3083,3083,3083,3083,3083,ICLR,2020,Latent Normalizing Flows for Many-to-Many Cross-Domain Mappings,Shweta Mahajan;Iryna Gurevych;Stefan Roth,mahajan@aiphes.tu-darmstadt.de;gurevych@ukp.informatik.tu-darmstadt.de;stefan.roth@visinf.tu-darmstadt.de,8;8;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0.0,yes,9/25/19,TU Darmstadt;TU Darmstadt;TU Darmstadt,64;64;64,289;289;289,5
3084,3084,3084,3084,3084,3084,3084,3084,ICLR,2020,Robustness Verification for Transformers,Zhouxing Shi;Huan Zhang;Kai-Wei Chang;Minlie Huang;Cho-Jui Hsieh,zhouxingshichn@gmail.com;huan@huan-zhang.com;kw@kwchang.net;aihuang@tsinghua.edu.cn;chohsieh@cs.ucla.edu,3;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,"Tsinghua University;University of California, Los Angeles;University of Virginia Main Campus;Tsinghua University;University of California, Los Angeles",8;20;59;8;20,23;17;107;23;17,1
3085,3085,3085,3085,3085,3085,3085,3085,ICLR,2020,Explain Your Move: Understanding Agent Actions Using Specific and Relevant Feature Attribution,Nikaash Puri;Sukriti Verma;Piyush Gupta;Dhruv Kayastha;Shripad Deshmukh;Balaji Krishnamurthy;Sameer Singh,nikpuri@adobe.com;dce.sukriti@gmail.com;piygupta@adobe.com;dhruvkayastha@iitkgp.ac.in;shripad@smail.iitm.ac.in;kbalaji@adobe.com;sameer@uci.edu,8;6;8,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Poster),0,11,1.0,yes,9/25/19,"Adobe Systems;;Adobe Systems;Indian Institute of Technology Kharagpur;Indian Institute of Technology Madras;Adobe Systems;University of California, Irvine",-1;-1;-1;266;154;-1;35,-1;-1;-1;476;641;-1;96,
3086,3086,3086,3086,3086,3086,3086,3086,ICLR,2020,AdvectiveNet: An Eulerian-Lagrangian Fluidic Reservoir for Point Cloud Processing     ,Xingzhe He;Helen Lu Cao;Bo Zhu,xingzhe.he95@gmail.com;helen.l.cao.22@dartmouth.edu;bo.zhu@dartmouth.edu,6;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,1,1.0,yes,9/25/19,;Dartmouth College;Dartmouth College,-1;154;154,-1;94;94,2
3087,3087,3087,3087,3087,3087,3087,3087,ICLR,2020,Batch-shaping for learning conditional channel gated networks,Babak Ehteshami Bejnordi;Tijmen Blankevoort;Max Welling,behtesha@qti.qualcomm.com;tijmen@qti.qualcomm.com;mwelling@qti.qualcomm.com,8;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),1,3,0.0,yes,9/25/19,"Qualcomm Inc, QualComm;Qualcomm Inc, QualComm;Qualcomm Inc, QualComm",-1;-1;-1,-1;-1;-1,2
3088,3088,3088,3088,3088,3088,3088,3088,ICLR,2020,A Closer Look at Deep Policy Gradients,Andrew Ilyas;Logan Engstrom;Shibani Santurkar;Dimitris Tsipras;Firdaus Janoos;Larry Rudolph;Aleksander Madry,ailyas@mit.edu;engstrom@mit.edu;shibani@mit.edu;tsipras@mit.edu;firdaus.janoos@twosigma.com;rudolph@csail.mit.edu;madry@mit.edu,8;6;8,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Talk),0,3,1.0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Two Sigma;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2;-1;2;2,5;5;5;5;-1;5;5,
3089,3089,3089,3089,3089,3089,3089,3089,ICLR,2020,Provable robustness against all adversarial $l_p$-perturbations for $p\geq 1$,Francesco Croce;Matthias Hein,francesco91.croce@gmail.com;matthias.hein@uni-tuebingen.de,6;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,University of Tuebingen;University of Tuebingen,154;154,91;91,4
3090,3090,3090,3090,3090,3090,3090,3090,ICLR,2020,Distributionally Robust Neural Networks,Shiori Sagawa*;Pang Wei Koh*;Tatsunori B. Hashimoto;Percy Liang,ssagawa@cs.stanford.edu;koh.pangwei@gmail.com;thashim@stanford.edu;pliang@cs.stanford.edu,8;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,4,0.0,yes,9/25/19,Stanford University;Stanford University;Stanford University;Stanford University,4;4;4;4,4;4;4;4,3;8
3091,3091,3091,3091,3091,3091,3091,3091,ICLR,2020,Nesterov Accelerated Gradient and Scale Invariance for Adversarial Attacks,Jiadong Lin;Chuanbiao Song;Kun He;Liwei Wang;John E. Hopcroft,jdlin@hust.edu.cn;cbsong@hust.edu.cn;brooklet60@hust.edu.cn;wanglw@cis.pku.edu.cn;jeh@cs.cornell.edu,6;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Peking University;Cornell University,39;39;39;22;7,47;47;47;24;19,4
3092,3092,3092,3092,3092,3092,3092,3092,ICLR,2020,Fast Neural Network Adaptation via Parameter Remapping and Architecture Search,Jiemin Fang*;Yuzhu Sun*;Kangjian Peng*;Qian Zhang;Yuan Li;Wenyu Liu;Xinggang Wang,jaminfong@hust.edu.cn;yzsun@hust.edu.cn;kangjian.peng@horizon.ai;qian01.zhang@horizon.ai;yuan.li@horizon.ai;liuwy@hust.edu.cn;xgwang@hust.edu.cn,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,8,0.0,yes,9/25/19,Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Horizon Robotics;Horizon Robotics;Horizon Robotics;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology,39;39;-1;-1;-1;39;39,47;47;-1;-1;-1;47;47,2
3093,3093,3093,3093,3093,3093,3093,3093,ICLR,2020,Understanding Generalization in Recurrent Neural Networks,Zhuozhuo Tu;Fengxiang He;Dacheng Tao,zhtu3055@uni.sydney.edu.au;fengxiang.he@sydney.edu.au;dacheng.tao@sydney.edu.au,6;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,University of Sydney;University of Sydney;University of Sydney,86;86;86,60;60;60,1;8
3094,3094,3094,3094,3094,3094,3094,3094,ICLR,2020,Piecewise linear activations substantially shape the loss surfaces of neural networks,Fengxiang He;Bohan Wang;Dacheng Tao,fengxiang.he@sydney.edu.au;bhwangfy@gmail.com;dacheng.tao@sydney.edu.au,6;6;3,I do not know much about this area.:N/A:N/A:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,19,0.0,yes,9/25/19,University of Sydney;University of Science and Technology of China;University of Sydney,86;481;86,60;80;60,1
3095,3095,3095,3095,3095,3095,3095,3095,ICLR,2020,ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning,Weihao Yu;Zihang Jiang;Yanfei Dong;Jiashi Feng,weihaoyu6@gmail.com;jzihang@u.nus.edu;yanfei.dong43@gmail.com;elefjia@nus.edu.sg,6;8;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0.0,yes,9/25/19,National University of Singapore;National University of Singapore;National University of Singapore;National University of Singapore,16;16;16;16,25;25;25;25,3
3096,3096,3096,3096,3096,3096,3096,3096,ICLR,2020,Poly-encoders: Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring,Samuel Humeau;Kurt Shuster;Marie-Anne Lachaux;Jason Weston,samuelhumeau@fb.com;kshuster@fb.com;malachaux@fb.com;jaseweston@gmail.com,8;8;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,1,0.0,yes,9/25/19,Facebook;Facebook;Facebook;,-1;-1;-1;-1,-1;-1;-1;-1,
3097,3097,3097,3097,3097,3097,3097,3097,ICLR,2020,Understanding the Limitations of Variational Mutual Information Estimators,Jiaming Song;Stefano Ermon,jiaming.tsong@gmail.com;ermon@cs.stanford.edu,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Accept (Poster),0,0,0.0,yes,9/25/19,Stanford University;Stanford University,4;4,4;4,
3098,3098,3098,3098,3098,3098,3098,3098,ICLR,2020,A Baseline for Few-Shot Image Classification,Guneet Singh Dhillon;Pratik Chaudhari;Avinash Ravichandran;Stefano Soatto,guneetdhillon@utexas.edu;pratikac@seas.upenn.edu;avinash.a.ravichandran@gmail.com;soattos@amazon.com,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,5,0.0,yes,9/25/19,"University of Texas, Austin;University of Pennsylvania;Amazon;Amazon",22;19;-1;-1,38;11;-1;-1,6
3099,3099,3099,3099,3099,3099,3099,3099,ICLR,2020,Hierarchical Foresight: Self-Supervised Learning of Long-Horizon Tasks via Visual Subgoal Generation,Suraj Nair;Chelsea Finn,surajn@stanford.edu;chelseaf@google.com,6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.,Accept (Poster),1,2,0.0,yes,9/25/19,Stanford University;Google,4;-1,4;-1,
3100,3100,3100,3100,3100,3100,3100,3100,ICLR,2020,Pseudo-LiDAR++: Accurate Depth for 3D Object Detection in Autonomous Driving,Yurong You;Yan Wang;Wei-Lun Chao;Divyansh Garg;Geoff Pleiss;Bharath Hariharan;Mark Campbell;Kilian Q. Weinberger,yy785@cornell.edu;yw763@cornell.edu;weilunchao760414@gmail.com;dg595@cornell.edu;gp346@cornell.edu;bharathh@cs.cornell.edu;mc288@cornell.edu;kqw4@cornell.edu,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,Cornell University;Cornell University;Ohio State University;Cornell University;Cornell University;Cornell University;Cornell University;Cornell University,7;7;77;7;7;7;7;7,19;19;373;19;19;19;19;19,2
3101,3101,3101,3101,3101,3101,3101,3101,ICLR,2020,V4D: 4D Convolutional Neural Networks for Video-level Representation Learning,Shiwen Zhang;Sheng Guo;Weilin Huang;Matthew R. Scott;Limin Wang,shizhang@malong.com;sheng@malong.com;whuang@malong.com;mscott@malong.com;07wanglimin@gmail.com,6;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Accept (Poster),0,4,0.0,yes,9/25/19,Malong Technologies;Malong Technologies;Malong Technologies;Malong Technologies;Zhejiang University,-1;-1;-1;-1;56,-1;-1;-1;-1;107,
3102,3102,3102,3102,3102,3102,3102,3102,ICLR,2020,Certified Defenses for Adversarial Patches,Ping-yeh Chiang*;Renkun Ni*;Ahmed Abdelkader;Chen Zhu;Christoph Studor;Tom Goldstein,pchiang@cs.umd.edu;rn9zm@cs.umd.edu;akader@cs.umd.edu;chenzhu@cs.umd.edu;studer@cornell.edu;tomg@cs.umd.edu,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,9,0.0,yes,9/25/19,"University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park;Cornell University;University of Maryland, College Park",12;12;12;12;7;12,91;91;91;91;19;91,4;2
3103,3103,3103,3103,3103,3103,3103,3103,ICLR,2020,Chameleon: Adaptive Code Optimization for Expedited Deep Neural Network Compilation,Byung Hoon Ahn;Prannoy Pilligundla;Amir Yazdanbakhsh;Hadi Esmaeilzadeh,bhahn@eng.ucsd.edu;ppilligu@eng.ucsd.edu;ayazdan@google.com;hadi@eng.ucsd.edu,6;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,"University of California, San Diego;University of California, San Diego;Google;University of California, San Diego",11;11;-1;11,31;31;-1;31,
3104,3104,3104,3104,3104,3104,3104,3104,ICLR,2020,Semantically-Guided Representation Learning for Self-Supervised Monocular Depth,Vitor Guizilini;Rui Hou;Jie Li;Rares Ambrus;Adrien Gaidon,vitor.guizilini@tri.global;rayhou@umich.edu;jie.li@tri.global;rares.ambrus@tri.global;adrien.gaidon@tri.global,6;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,8,0.0,yes,9/25/19,Toyota Research Institute;University of Michigan;Toyota Research Institute;Toyota Research Institute;Toyota Research Institute,-1;8;-1;-1;-1,-1;21;-1;-1;-1,2
3105,3105,3105,3105,3105,3105,3105,3105,ICLR,2020,Target-Embedding Autoencoders for Supervised Representation Learning,Daniel Jarrett;Mihaela van der Schaar,daniel.jarrett@eng.ox.ac.uk;mv472@damtp.cam.ac.uk,8;6;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,19,0.0,yes,9/25/19,University of Oxford;University of Cambridge,50;71,1;3,8
3106,3106,3106,3106,3106,3106,3106,3106,ICLR,2020,MetaPix: Few-Shot Video Retargeting,Jessica Lee;Deva Ramanan;Rohit Girdhar,jl5@cs.cmu.edu;deva@cs.cmu.edu;rgirdhar@cs.cmu.edu,6;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Poster),0,3,0.0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1,27;27;27,5;6
3107,3107,3107,3107,3107,3107,3107,3107,ICLR,2020,Lazy-CFR: fast and near-optimal regret minimization for extensive games with imperfect information,Yichi Zhou;Tongzheng Ren;Jialian Li;Dong Yan;Jun Zhu,vofhqn@gmail.com;rtz19970824@gmail.com;lijialia16@mails.tsinghua.edu.cn;sproblvem@gmail.com;dcszj@mail.tsinghua.edu.cn,8;8;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,5,0.0,yes,9/25/19,";University of Texas, Austin;Tsinghua University;;Tsinghua University",-1;22;8;-1;8,-1;38;23;-1;23,1
3108,3108,3108,3108,3108,3108,3108,3108,ICLR,2020,Neural Module Networks for Reasoning over Text,Nitish Gupta;Kevin Lin;Dan Roth;Sameer Singh;Matt Gardner,gnnitish@gmail.com;kevinlin@eecs.berkeley.edu;danroth@seas.upenn.edu;sameer@uci.edu;mattg@allenai.org,6;6;8,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,1.0,yes,9/25/19,"University of Pennsylvania;University of California Berkeley;University of Pennsylvania;University of California, Irvine;Allen Institute for Artificial Intelligence",19;5;19;35;-1,11;13;11;96;-1,3
3109,3109,3109,3109,3109,3109,3109,3109,ICLR,2020,Shifted and Squeezed 8-bit Floating Point format for Low-Precision Training of Deep Neural Networks,Leopold Cambier;Anahita Bhiwandiwalla;Ting Gong;Oguz H. Elibol;Mehran Nekuii;Hanlin Tang,lcambier@stanford.edu;anahita.bhiwandiwalla@intel.com;ting.gong@intel.com;oguz.h.elibol@intel.com;mehran.nekuii@intel.com;hanlin.tang@intel.com,6;6;1;8,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,Stanford University;Intel;Intel;Intel;Intel;Intel,4;-1;-1;-1;-1;-1,4;-1;-1;-1;-1;-1,
3110,3110,3110,3110,3110,3110,3110,3110,ICLR,2020,ProxSGD: Training Structured Neural Networks under Regularization and Constraints,Yang Yang;Yaxiong Yuan;Avraam Chatzimichailidis;Ruud JG van Sloun;Lei Lei;Symeon Chatzinotas,yang.yang@itwm.fraunhofer.de;yaxiong.yuan@uni.lu;avraam.chatzimichailidis@itwm.fraunhofer.de;r.j.g.v.sloun@tue.nl;lei.lei@uni.lu;symeon.chatzinotas@uni.lu,3;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,1.0,yes,9/25/19,"Fraunhofer IIS;Interdisciplinary Centre for Security, Reliability and Trust (SnT);Fraunhofer IIS;Eindhoven University of Technology;Interdisciplinary Centre for Security, Reliability and Trust (SnT);Interdisciplinary Centre for Security, Reliability and Trust (SnT)",-1;-1;-1;205;-1;-1,-1;-1;-1;185;-1;-1,9
3111,3111,3111,3111,3111,3111,3111,3111,ICLR,2020,BayesOpt Adversarial Attack,Binxin Ru;Adam Cobb;Arno Blaas;Yarin Gal,robin@robots.ox.ac.uk;adam.cobb@worc.ox.ac.uk;arno@robots.ox.ac.uk;yarin@cs.ox.ac.uk,6;3;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,4,0.0,yes,9/25/19,University of Oxford;University of Oxford;University of Oxford;University of Oxford,50;50;50;50,1;1;1;1,4;11
3112,3112,3112,3112,3112,3112,3112,3112,ICLR,2020,Distance-Based Learning from Errors for Confidence Calibration,Chen Xing;Sercan Arik;Zizhao Zhang;Tomas Pfister,xingchen1113@gmail.com;soarik@google.com;zizhaoz@google.com;tpfister@google.com,6;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,Nankai University;Google;Google;Google,481;-1;-1;-1,366;-1;-1;-1,
3113,3113,3113,3113,3113,3113,3113,3113,ICLR,2020,Compressive Transformers for Long-Range Sequence Modelling,Jack W. Rae;Anna Potapenko;Siddhant M. Jayakumar;Chloe Hillier;Timothy P. Lillicrap,jwrae@google.com;apotapenko@google.com;sidmj@google.com;chillier@google.com;countzero@google.com,8;8;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,8,1.0,yes,9/25/19,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,3
3114,3114,3114,3114,3114,3114,3114,3114,ICLR,2020,VL-BERT: Pre-training of Generic Visual-Linguistic Representations,Weijie Su;Xizhou Zhu;Yue Cao;Bin Li;Lewei Lu;Furu Wei;Jifeng Dai,jackroos@mail.ustc.edu.cn;ezra0408@mail.ustc.edu.cn;yuecao@microsoft.com;binli@ustc.edu.cn;lewlu@microsoft.com;fuwei@microsoft.com;jifdai@microsoft.com,3;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,4,0.0,yes,9/25/19,University of Science and Technology of China;University of Science and Technology of China;Microsoft;University of Science and Technology of China;Microsoft;Microsoft;Microsoft,481;481;-1;481;-1;-1;-1,80;80;-1;80;-1;-1;-1,
3115,3115,3115,3115,3115,3115,3115,3115,ICLR,2020,Symplectic Recurrent Neural Networks,Zhengdao Chen;Jianyu Zhang;Martin Arjovsky;Léon Bottou,zc1216@nyu.edu;edzhang@tju.edu.cn;martinarjovsky@gmail.com;leonb@fb.com,8;8;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,4,1.0,yes,9/25/19,New York University;Zhejiang University;New York University;Facebook,25;56;25;-1,29;107;29;-1,
3116,3116,3116,3116,3116,3116,3116,3116,ICLR,2020,Learning to Coordinate Manipulation Skills via Skill Behavior Diversification,Youngwoon Lee;Jingyun Yang;Joseph J. Lim,lee504@usc.edu;jingyuny@usc.edu;limjj@usc.edu,6;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,5,0.0,yes,9/25/19,University of Southern California;University of Southern California;University of Southern California,31;31;31,62;62;62,
3117,3117,3117,3117,3117,3117,3117,3117,ICLR,2020,Learning Expensive Coordination: An Event-Based Deep RL Approach,Zhenyu Shi*;Runsheng Yu*;Xinrun Wang*;Rundong Wang;Youzhi Zhang;Hanjiang Lai;Bo An,shizhy6@mail2.sysu.edu.cn;runsheng.yu@ntu.edu.sg;xwang033@e.ntu.edu.sg;rundong001@e.ntu.edu.sg;yzhang137@e.ntu.edu.sg;laihanj3@mail.sysu.edu.cn;boan@ntu.edu.sg,6;8;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Accept (Poster),0,5,0.0,yes,9/25/19,SUN YAT-SEN UNIVERSITY;National Taiwan University;National Taiwan University;National Taiwan University;National Taiwan University;SUN YAT-SEN UNIVERSITY;National Taiwan University,481;86;86;86;86;481;86,299;120;120;120;120;299;120,
3118,3118,3118,3118,3118,3118,3118,3118,ICLR,2020,Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML,Aniruddh Raghu;Maithra Raghu;Samy Bengio;Oriol Vinyals,aniruddhraghu@gmail.com;maithrar@gmail.com;bengio@google.com;vinyals@google.com,3;8;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Poster),0,5,0.0,yes,9/25/19,Massachusetts Institute of Technology;Cornell University;Google;Google,2;7;-1;-1,5;19;-1;-1,6
3119,3119,3119,3119,3119,3119,3119,3119,ICLR,2020,Iterative energy-based projection on a normal data manifold for anomaly localization,David Dehaene;Oriel Frigo;Sébastien Combrexelle;Pierre Eline,david@anotherbrain.ai;oriel@anotherbrain.ai;sebastien@anotherbrain.ai;pierre@anotherbrain.ai,3;6;8,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,AnotherBrain;AnotherBrain;AnotherBrain;AnotherBrain,-1;-1;-1;-1,-1;-1;-1;-1,2
3120,3120,3120,3120,3120,3120,3120,3120,ICLR,2020,Harnessing Structures for Value-Based Planning and Reinforcement Learning,Yuzhe Yang;Guo Zhang;Zhi Xu;Dina Katabi,yuzhe@mit.edu;guozhang@mit.edu;zhixu@mit.edu;dina@csail.mit.edu,8;6;8,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,8,0.0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2,5;5;5;5,
3121,3121,3121,3121,3121,3121,3121,3121,ICLR,2020,A closer look at the approximation capabilities of neural networks,Kai Fong Ernest Chong,ernest_chong@sutd.edu.sg,8;6;6;6,I have read many papers in this area.:N/A:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:N/A:N/A:I did not assess the derivations or theory.;I have read many papers in this area.:N/A:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:N/A:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,1.0,yes,9/25/19,Singapore University of Technology and Design,481,1397,1
3122,3122,3122,3122,3122,3122,3122,3122,ICLR,2020,Spectral  Embedding of Regularized Block Models,Nathan De Lara;Thomas Bonald,ndelara@enst.fr;bonald@enst.fr,6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,1,0.0,yes,9/25/19,Télécom ParisTech;Télécom ParisTech,481;481,187;187,10
3123,3123,3123,3123,3123,3123,3123,3123,ICLR,2020,MMA Training: Direct Input Space Margin Maximization through Adversarial Training,Gavin Weiguang Ding;Yash Sharma;Kry Yik Chau Lui;Ruitong Huang,gavin.w.ding@gmail.com;yash.sharma@bethgelab.org;yikchau.y.lui@borealisai.com;ruitong.huang@borealisai.com,3;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Accept (Poster),1,10,0.0,yes,9/25/19,"Borealis AI;Centre for Integrative Neuroscience, AG Bethge;Borealis AI;Borealis AI",-1;-1;-1;-1,-1;-1;-1;-1,4;1
3124,3124,3124,3124,3124,3124,3124,3124,ICLR,2020,Monotonic Multihead Attention,Xutai Ma;Juan Miguel Pino;James Cross;Liezl Puzon;Jiatao Gu,xutai_ma@jhu.edu;juancarabina@fb.com;jcross@fb.com;lie@fb.com;jgu@fb.com,6;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,9,0.0,yes,9/25/19,Johns Hopkins University;Facebook;Facebook;Facebook;Facebook,73;-1;-1;-1;-1,12;-1;-1;-1;-1,3
3125,3125,3125,3125,3125,3125,3125,3125,ICLR,2020,Learning representations for binary-classification without backpropagation,Mathias Lechner,mathias.lechner@ist.ac.at,6;6;8,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,Institute of Science and Technology Austria,481,1397,1
3126,3126,3126,3126,3126,3126,3126,3126,ICLR,2020,Action Semantics Network: Considering the Effects of Actions in Multiagent Systems,Weixun Wang;Tianpei Yang;Yong Liu;Jianye Hao;Xiaotian Hao;Yujing Hu;Yingfeng Chen;Changjie Fan;Yang Gao,wxwang@tju.edu.cn;tpyang@tju.edu.cn;lucasliunju@gmail.com;jianye.hao@tju.edu.cn;xiaotianhao@tju.edu.cn;huyujing@corp.netease.com;chenyingfeng1@corp.netease.com;fanchangjie@corp.netease.com;gaoy@nju.edu.cn,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,Zhejiang University;Zhejiang University;;Zhejiang University;Zhejiang University;Corp.netease;University of Science and Technology of China;Corp.netease;Zhejiang University,56;56;-1;56;56;-1;481;-1;56,107;107;-1;107;107;-1;80;-1;107,
3127,3127,3127,3127,3127,3127,3127,3127,ICLR,2020,Finite Depth and Width Corrections to the Neural Tangent Kernel,Boris Hanin;Mihai Nica,bhanin@math.tamu.edu;mnica@math.utoronto.ca,6;8;8,I have published in this field for several years.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,3,0.0,yes,9/25/19,Texas A&M;Toronto University,44;18,177;18,1
3128,3128,3128,3128,3128,3128,3128,3128,ICLR,2020,What graph neural networks cannot learn: depth vs width,Andreas Loukas,andreas.loukas@epfl.ch,6;8;8,I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,Swiss Federal Institute of Technology Lausanne,481,38,10
3129,3129,3129,3129,3129,3129,3129,3129,ICLR,2020,Neural Oblivious Decision Ensembles for Deep Learning on Tabular Data,Sergei Popov;Stanislav Morozov;Artem Babenko,sapopov@yandex-team.ru;stanis-morozov@yandex.ru;artem.babenko@phystech.edu,6;8;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,Yandex;Yandex;Moscow Institute of Physics and Technology,-1;-1;481,-1;-1;234,3
3130,3130,3130,3130,3130,3130,3130,3130,ICLR,2020,Gradient $\ell_1$ Regularization for Quantization Robustness,Milad Alizadeh;Arash Behboodi;Mart van Baalen;Christos Louizos;Tijmen Blankevoort;Max Welling,milada@qti.qualcomm.com;behboodi@qti.qualcomm.com;mart@qti.qualcomm.com;clouizos@qti.qualcomm.com;tijmen@qti.qualcomm.com;mwelling@qti.qualcomm.com,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,"Qualcomm Inc, QualComm;Qualcomm Inc, QualComm;Qualcomm Inc, QualComm;Qualcomm Inc, QualComm;Qualcomm Inc, QualComm;Qualcomm Inc, QualComm",-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,
3131,3131,3131,3131,3131,3131,3131,3131,ICLR,2020,PairNorm: Tackling Oversmoothing in GNNs,Lingxiao Zhao;Leman Akoglu,lingxiao@cmu.edu;lakoglu@andrew.cmu.edu,8;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),0,2,0.0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University,1;1,27;27,10
3132,3132,3132,3132,3132,3132,3132,3132,ICLR,2020,Cross-lingual Alignment vs Joint Training: A Comparative Study and A Simple Unified Framework,Zirui Wang*;Jiateng Xie*;Ruochen Xu;Yiming Yang;Graham Neubig;Jaime G. Carbonell,ziruiw@cs.cmu.edu;jiatengx@cs.cmu.edu;ruochenx@cs.cmu.edu;yiming@cs.cmu.edu;gneubig@cs.cmu.edu;jgc@cs.cmu.edu,8;8;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),2,5,0.0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1;1;1,27;27;27;27;27;27,6
3133,3133,3133,3133,3133,3133,3133,3133,ICLR,2020,Generalization of Two-layer Neural Networks: An Asymptotic Viewpoint,Jimmy Ba;Murat Erdogdu;Taiji Suzuki;Denny Wu;Tianzong Zhang,jba@cs.toronto.edu;erdogdu@cs.toronto.edu;taiji@mist.i.u-tokyo.ac.jp;dennywu@cs.toronto.edu;ztz16@mails.tsinghua.edu.cn,8;6;8,I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,3,0.0,yes,9/25/19,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;The University of Tokyo;Department of Computer Science, University of Toronto;Tsinghua University",18;18;56;18;8,18;18;36;18;23,8
3134,3134,3134,3134,3134,3134,3134,3134,ICLR,2020,LAMOL: LAnguage MOdeling for Lifelong Language Learning,Fan-Keng Sun*;Cheng-Hao Ho*;Hung-Yi Lee,fankeng@mit.edu;jojotenya@gmail.com;hungyilee@ntu.edu.tw,6;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I made a quick assessment of this paper.:N/A,Accept (Poster),0,4,0.0,yes,9/25/19,Massachusetts Institute of Technology;;National Taiwan University,2;-1;86,5;-1;120,3
3135,3135,3135,3135,3135,3135,3135,3135,ICLR,2020,Neural Stored-program Memory,Hung Le;Truyen Tran;Svetha Venkatesh,lethai@deakin.edu.au;truyen.tran@deakin.edu.au;svetha.venkatesh@deakin.edu.au,8;6;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,8,0.0,yes,9/25/19,Deakin University;Deakin University;Deakin University,481;481;481,332;332;332,6
3136,3136,3136,3136,3136,3136,3136,3136,ICLR,2020,BatchEnsemble: an Alternative Approach to Efficient Ensemble and Lifelong Learning,Yeming Wen;Dustin Tran;Jimmy Ba,ywen@cs.toronto.edu;trandustin@google.com;jba@cs.toronto.edu,6;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,9,1.0,yes,9/25/19,"Department of Computer Science, University of Toronto;Google;Department of Computer Science, University of Toronto",18;-1;18,18;-1;18,
3137,3137,3137,3137,3137,3137,3137,3137,ICLR,2020,Neural Arithmetic Units,Andreas Madsen;Alexander Rosenberg Johansen,amwebdk@gmail.com;alexander@herhjemme.dk,6;8;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,11,0.0,yes,9/25/19,;Technical University of Denmark,-1;481,-1;182,
3138,3138,3138,3138,3138,3138,3138,3138,ICLR,2020,"To Relieve Your Headache of Training an MRF, Take AdVIL",Chongxuan Li;Chao Du;Kun Xu;Max Welling;Jun Zhu;Bo Zhang,chongxuanli1991@gmail.com;duchao0726@gmail.com;kunxu.thu@gmail.com;m.welling@uva.nl;dcszj@mail.tsinghua.edu.cn;dcszb@mail.tsinghua.edu.cn,6;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,6,0.0,yes,9/25/19,Tsinghua University;Tsinghua University;Tsinghua University;University of Amsterdam;Tsinghua University;Tsinghua University,8;8;8;172;8;8,23;23;23;62;23;23,4
3139,3139,3139,3139,3139,3139,3139,3139,ICLR,2020,State Alignment-based Imitation Learning,Fangchen Liu;Zhan Ling;Tongzhou Mu;Hao Su,fliu@eng.ucsd.edu;z6ling@eng.ucsd.edu;t3mu@eng.ucsd.edu;haosu@eng.ucsd.edu,6;3;8,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,12,0.0,yes,9/25/19,"University of California, San Diego;University of California, San Diego;University of California, San Diego;University of California, San Diego",11;11;11;11,31;31;31;31,
3140,3140,3140,3140,3140,3140,3140,3140,ICLR,2020,Influence-Based Multi-Agent Exploration,Tonghan Wang*;Jianhao Wang*;Yi Wu;Chongjie Zhang,tonghanwang1996@gmail.com;1040594377@qq.com;jxwuyi@openai.com;chongjie@tsinghua.edu.cn,6;8;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Spotlight),3,4,0.0,yes,9/25/19,Tsinghua University;Tsinghua University;OpenAI;Tsinghua University,8;8;-1;8,23;23;-1;23,
3141,3141,3141,3141,3141,3141,3141,3141,ICLR,2020,AE-OT: A NEW GENERATIVE MODEL BASED ON EXTENDED SEMI-DISCRETE OPTIMAL TRANSPORT,Dongsheng An;Yang Guo;Na Lei;Zhongxuan Luo;Shing-Tung Yau;Xianfeng Gu,doan@cs.stonybrook.edu;yangguo@cs.stonybrook.edu;nalei@dlut.edu.cn;zxluo@dlut.edu.cn;yau@math.harvard.edu;gu@cs.stonybrook.edu,3;3;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0.0,yes,9/25/19,"State University of New York, Stony Brook;State University of New York, Stony Brook;South China University of Technology;South China University of Technology;Harvard University;State University of New York, Stony Brook",41;41;481;481;39;41,304;304;501;501;7;304,5;4;9
3142,3142,3142,3142,3142,3142,3142,3142,ICLR,2020,CoPhy: Counterfactual Learning of Physical Dynamics,Fabien Baradel;Natalia Neverova;Julien Mille;Greg Mori;Christian Wolf,fabien.baradel@insa-lyon.fr;nneverova@fb.com;julien.mille@insa-cvl.fr;mori@cs.sfu.ca;christian.wolf@insa-lyon.fr,6;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,6,0.0,yes,9/25/19,INSA de Lyon;Facebook;;Simon Fraser University;INSA de Lyon,481;-1;-1;64;481,1397;-1;-1;272;1397,
3143,3143,3143,3143,3143,3143,3143,3143,ICLR,2020,Disentangling Factors of Variations Using Few Labels,Francesco Locatello;Michael Tschannen;Stefan Bauer;Gunnar Rätsch;Bernhard Schölkopf;Olivier Bachem,flocatello@tuebingen.mpg.de;tschannen@google.com;stefan.bauer@tuebingen.mpg.de;raetsch@inf.ethz.ch;bs@tuebingen.mpg.de;bachem@google.com,1;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,5,0.0,yes,9/25/19,"Max Planck Institute for Intelligent Systems, Max-Planck Institute;Google;Max Planck Institute for Intelligent Systems, Max-Planck Institute;Swiss Federal Institute of Technology;Max Planck Institute for Intelligent Systems, Max-Planck Institute;Google",-1;-1;-1;10;-1;-1,-1;-1;-1;13;-1;-1,
3144,3144,3144,3144,3144,3144,3144,3144,ICLR,2020,Uncertainty-guided Continual Learning with Bayesian Neural Networks,Sayna Ebrahimi;Mohamed Elhoseiny;Trevor Darrell;Marcus Rohrbach,sayna@berkeley.edu;mohamed.elhoseiny@gmail.com;trevor@eecs.berkeley.edu;maroffm@gmail.com,6;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0.0,yes,9/25/19,University of California Berkeley;KAUST;University of California Berkeley;Facebook,5;128;5;-1,13;1397;13;-1,11
3145,3145,3145,3145,3145,3145,3145,3145,ICLR,2020,Composing Task-Agnostic Policies with Deep Reinforcement Learning,Ahmed H. Qureshi;Jacob J. Johnson;Yuzhe Qin;Taylor Henderson;Byron Boots;Michael C. Yip,a1qureshi@ucsd.edu;jjj025@eng.ucsd.edu;y1qin@eng.ucsd.edu;tjwest@ucsd.edu;bboots@cs.washington.edu;yip@ucsd.edu,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,13,0.0,yes,9/25/19,"University of California, San Diego;University of California, San Diego;University of California, San Diego;University of California, San Diego;University of Washington;University of California, San Diego",11;11;11;11;6;11,31;31;31;31;26;31,6
3146,3146,3146,3146,3146,3146,3146,3146,ICLR,2020,Making Sense of Reinforcement Learning and Probabilistic Inference,Brendan O'Donoghue;Ian Osband;Catalin Ionescu,bodonoghue85@gmail.com;iosband@google.com;cdi@google.com,6;8;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,5,1.0,yes,9/25/19,;Google;Google,-1;-1;-1,-1;-1;-1,
3147,3147,3147,3147,3147,3147,3147,3147,ICLR,2020,The Logical Expressiveness of Graph Neural Networks,Pablo Barceló;Egor V. Kostylev;Mikael Monet;Jorge Pérez;Juan Reutter;Juan Pablo Silva,pbarcelo@gmail.com;egor.kostylev@cs.ox.ac.uk;mikael.monet@imfd.cl;jorge.perez.rojas@gmail.com;juan.reutter@gmail.com;jpsilvapena@gmail.com,8;8;8,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,4,0.0,yes,9/25/19,Universidad de Chile;University of Oxford;Millenium Instititute for Foundational Research on Data;Universidad de Chile;Pontificia Universidad Católica;Universidad de Chile,323;50;-1;323;481;323,848;1;-1;848;570;848,10
3148,3148,3148,3148,3148,3148,3148,3148,ICLR,2020,Language GANs Falling Short,Massimo Caccia;Lucas Caccia;William Fedus;Hugo Larochelle;Joelle Pineau;Laurent Charlin,massimo.p.caccia@gmail.com;lucas.page-caccia@mail.mcgill.ca;liam.fedus@gmail.com;hugolarochelle@google.com;jpineau@cs.mcgill.ca;lcharlin@gmail.com,6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,10,1.0,yes,9/25/19,University of Montreal;McGill University;University of Montreal;Google;McGill University;University of Montreal,128;86;128;-1;86;128,85;42;85;-1;42;85,3;4;5
3149,3149,3149,3149,3149,3149,3149,3149,ICLR,2020,Directional Message Passing for Molecular Graphs,Johannes Klicpera;Janek Groß;Stephan Günnemann,klicpera@in.tum.de;grossja@in.tum.de;guennemann@in.tum.de,8;8;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,3,0.0,yes,9/25/19,Technical University Munich;Technical University Munich;Technical University Munich,53;53;53,43;43;43,10
3150,3150,3150,3150,3150,3150,3150,3150,ICLR,2020,A Target-Agnostic Attack on Deep Models: Exploiting Security Vulnerabilities of Transfer Learning,Shahbaz Rezaei;Xin Liu,srezaei@ucdavis.edu;xinliu@ucdavis.edu,6;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),0,5,0.0,yes,9/25/19,"University of California, Davis;University of California, Davis",79;79,55;55,4;6;2
3151,3151,3151,3151,3151,3151,3151,3151,ICLR,2020,At Stability's Edge: How to Adjust Hyperparameters to Preserve Minima Selection in Asynchronous Training of Neural Networks?,Niv Giladi;Mor Shpigel Nacson;Elad Hoffer;Daniel Soudry,giladiniv@gmail.com;mor.shpigel@gmail.com;elad.hoffer@gmail.com;daniel.soudry@gmail.com,6;8;8,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,3,0.0,yes,9/25/19,Technion;Technion;Technion;Technion,26;26;26;26,412;412;412;412,9;8
3152,3152,3152,3152,3152,3152,3152,3152,ICLR,2020,Smoothness and Stability in GANs,Casey Chu;Kentaro Minami;Kenji Fukumizu,caseychu@stanford.edu;minami@preferred.jp;fukumizu@ism.ac.jp,8;6;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0.0,yes,9/25/19,"Stanford University;Preferred Networks, Inc.;The Institute of Statistical Mathematics, Japan",4;-1;-1,4;-1;-1,5;4
3153,3153,3153,3153,3153,3153,3153,3153,ICLR,2020,Infinite-Horizon Differentiable Model Predictive Control,Sebastian East;Marco Gallieri;Jonathan Masci;Jan Koutnik;Mark Cannon,sebastian.east@bath.edu;marco@nnaisense.com;jonathan@nnaisense.com;jan@nnaisense.com;mark.cannon@eng.ox.ac.uk,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,7,0.0,yes,9/25/19,University of Oxford;NNAISENSE;NNAISENSE;NNAISENSE;University of Oxford,50;-1;-1;-1;50,1;-1;-1;-1;1,
3154,3154,3154,3154,3154,3154,3154,3154,ICLR,2020,Observational Overfitting in Reinforcement Learning,Xingyou Song;Yiding Jiang;Stephen Tu;Yilun Du;Behnam Neyshabur,xsong@berkeley.edu;ydjiang@google.com;stephentu@google.com;yilundu@mit.edu;neyshabur@google.com,6;8;8,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0.0,yes,9/25/19,University of California Berkeley;Google;Google;Massachusetts Institute of Technology;Google,5;-1;-1;2;-1,13;-1;-1;5;-1,8
3155,3155,3155,3155,3155,3155,3155,3155,ICLR,2020,Improved Sample Complexities for Deep Neural Networks and Robust Classification via an All-Layer Margin,Colin Wei;Tengyu Ma,colinwei@stanford.edu;tengyuma@cs.stanford.edu,3;8;8;6,I have published in this field for several years.:I did not assess the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,7,0.0,yes,9/25/19,Stanford University;Stanford University,4;4,4;4,4;1;8
3156,3156,3156,3156,3156,3156,3156,3156,ICLR,2020,SQIL: Imitation Learning via Reinforcement Learning with Sparse Rewards,Siddharth Reddy;Anca D. Dragan;Sergey Levine,sgr@berkeley.edu;anca@berkeley.edu;svlevine@eecs.berkeley.edu,6;6;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,13;13;13,5;4;1
3157,3157,3157,3157,3157,3157,3157,3157,ICLR,2020,Feature Interaction Interpretability: A Case for Explaining Ad-Recommendation Systems via Neural Interaction Detection,Michael Tsang;Dehua Cheng;Hanpeng Liu;Xue Feng;Eric Zhou;Yan Liu,tsangm@usc.edu;dehuacheng@fb.com;hanpengl@usc.edu;xfeng@fb.com;hanningz@fb.com;yanliu.cs@usc.edu,6;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,University of Southern California;Facebook;University of Southern California;Facebook;Facebook;University of Southern California,31;-1;31;-1;-1;31,62;-1;62;-1;-1;62,
3158,3158,3158,3158,3158,3158,3158,3158,ICLR,2020,Selection via Proxy: Efficient Data Selection for Deep Learning,Cody Coleman;Christopher Yeh;Stephen Mussmann;Baharan Mirzasoleiman;Peter Bailis;Percy Liang;Jure Leskovec;Matei Zaharia,cody@cs.stanford.edu;chrisyeh@stanford.edu;mussmann@stanford.edu;baharanm@stanford.edu;pbailis@cs.stanford.edu;pliang@cs.stanford.edu;jure@cs.stanford.edu;matei@cs.stanford.edu,6;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,8,0.0,yes,9/25/19,Stanford University;Stanford University;Stanford University;Stanford University;Stanford University;Stanford University;Stanford University;Stanford University,4;4;4;4;4;4;4;4,4;4;4;4;4;4;4;4,
3159,3159,3159,3159,3159,3159,3159,3159,ICLR,2020,Your classifier is secretly an energy based model and you should treat it like one,Will Grathwohl;Kuan-Chieh Wang;Joern-Henrik Jacobsen;David Duvenaud;Mohammad Norouzi;Kevin Swersky,wgrathwohl@cs.toronto.edu;wangkua1@cs.toronto.edu;j.jacobsen@vectorinstitute.ai;duvenaud@cs.toronto.edu;mnorouzi@google.com;kswersky@google.com,6;8;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,10,3.0,yes,9/25/19,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Vector Institute;Department of Computer Science, University of Toronto;Google;Google",18;18;-1;18;-1;-1,18;18;-1;18;-1;-1,5
3160,3160,3160,3160,3160,3160,3160,3160,ICLR,2020,Padé Activation Units: End-to-end Learning of Flexible Activation Functions in Deep Networks,Alejandro Molina;Patrick Schramowski;Kristian Kersting,molina@cs.tu-darmstadt.de;schramowski@cs.tu-darmstadt.de;kersting@cs.tu-darmstadt.de,6;6;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),0,7,0.0,yes,9/25/19,TU Darmstadt;TU Darmstadt;TU Darmstadt,64;64;64,289;289;289,
3161,3161,3161,3161,3161,3161,3161,3161,ICLR,2020,Lipschitz constant estimation of Neural Networks via sparse polynomial optimization,Fabian Latorre;Paul Rolland;Volkan Cevher,fabian.latorre@epfl.ch;paul.rolland@epfl.ch;volkan.cevher@epfl.ch,8;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,5,0.0,yes,9/25/19,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne,481;481;481,38;38;38,1
3162,3162,3162,3162,3162,3162,3162,3162,ICLR,2020,AMRL: Aggregated Memory For Reinforcement Learning,Jacob Beck;Kamil Ciosek;Sam Devlin;Sebastian Tschiatschek;Cheng Zhang;Katja Hofmann,jacob_beck@alumni.brown.edu;kamil.ciosek@microsoft.com;sam.devlin@microsoft.com;sebastian.tschiatschek@microsoft.com;cheng.zhang@microsoft.com;katja.hofmann@microsoft.com,6;8;6,I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0.0,yes,9/25/19,Brown University;Microsoft;Microsoft;Microsoft;Microsoft;Microsoft,67;-1;-1;-1;-1;-1,53;-1;-1;-1;-1;-1,3
3163,3163,3163,3163,3163,3163,3163,3163,ICLR,2020,Memory-Based Graph Networks,Amir Hosein Khasahmadi;Kaveh Hassani;Parsa Moradi;Leo Lee;Quaid Morris,amirhosein.khasahmadi@mail.utoronto.ca;kaveh.hassani@autodesk.com;parsa.moradi73@gmail.com;ljlee@psi.toronto.edu;quaid.morris@utoronto.ca,6;6;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,8,0.0,yes,9/25/19,Toronto University;Autodesk Inc;;University of Toronto;Toronto University,18;-1;-1;18;18,18;-1;-1;18;18,10
3164,3164,3164,3164,3164,3164,3164,3164,ICLR,2020,One-Shot Pruning of Recurrent Neural Networks by Jacobian Spectrum Evaluation,Shunshi Zhang;Bradly C. Stadie,matthew.zhang@mail.utoronto.ca;bstadie@berkeley.edu,6;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,6,0.0,yes,9/25/19,Toronto University;University of California Berkeley,18;5,18;13,
3165,3165,3165,3165,3165,3165,3165,3165,ICLR,2020,Network Randomization: A Simple Technique for Generalization in Deep Reinforcement Learning,Kimin Lee;Kibok Lee;Jinwoo Shin;Honglak Lee,kiminlee@kaist.ac.kr;kibok@umich.edu;jinwoos@kaist.ac.kr;honglak@eecs.umich.edu,6;3;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Poster),0,4,0.0,yes,9/25/19,Korea Advanced Institute of Science and Technology;University of Michigan;Korea Advanced Institute of Science and Technology;University of Michigan,481;8;481;8,110;21;110;21,8
3166,3166,3166,3166,3166,3166,3166,3166,ICLR,2020,On Mutual Information Maximization for Representation Learning,Michael Tschannen;Josip Djolonga;Paul K. Rubenstein;Sylvain Gelly;Mario Lucic,mi.tschannen@gmail.com;josip@djolonga.com;paruby@gmail.com;sylvaingelly@google.com;lucic@google.com,6;8;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,Google;Google;University of Cambridge;Google;Google,-1;-1;71;-1;-1,-1;-1;3;-1;-1,
3167,3167,3167,3167,3167,3167,3167,3167,ICLR,2020,Hypermodels for Exploration,Vikranth Dwaracherla;Xiuyuan Lu;Morteza Ibrahimi;Ian Osband;Zheng Wen;Benjamin Van Roy,vikranthd@google.com;lxlu@google.com;mibrahimi@google.com;iosband@google.com;zhengwen@google.com;benvanroy@google.com,6;8;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,7,0.0,yes,9/25/19,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,1
3168,3168,3168,3168,3168,3168,3168,3168,ICLR,2020,On the Equivalence between Positional Node Embeddings and Structural Graph Representations,Balasubramaniam Srinivasan;Bruno Ribeiro,bsriniv@purdue.edu;ribeiro@cs.purdue.edu,8;8;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,8,2.0,yes,9/25/19,Purdue University;Purdue University,27;27,88;88,1;10
3169,3169,3169,3169,3169,3169,3169,3169,ICLR,2020,GraphSAINT: Graph Sampling Based Inductive Learning Method,Hanqing Zeng;Hongkuan Zhou;Ajitesh Srivastava;Rajgopal Kannan;Viktor Prasanna,zengh@usc.edu;hongkuaz@usc.edu;ajiteshs@usc.edu;rajgopal.kannan.civ@mail.mil;prasanna@usc.edu,6;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),1,5,0.0,yes,9/25/19,University of Southern California;University of Southern California;University of Southern California;Army Reserach laboratory;University of Southern California,31;31;31;-1;31,62;62;62;-1;62,10
3170,3170,3170,3170,3170,3170,3170,3170,ICLR,2020,Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow ReLU networks,Ziwei Ji;Matus Telgarsky,ziweiji2@illinois.edu;mjt@illinois.edu,6;8;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:N/A:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:N/A:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,13,0.0,yes,9/25/19,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",3;3,48;48,1
3171,3171,3171,3171,3171,3171,3171,3171,ICLR,2020,Cross-Lingual Ability of Multilingual BERT: An Empirical Study,Karthikeyan K;Zihan Wang;Stephen Mayhew;Dan Roth,kkarthi@seas.upenn.edu;zihanw2@illinois.edu;mayhew@seas.upenn.edu;danroth@seas.upenn.edu,3;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,5,0.0,yes,9/25/19,"University of Pennsylvania;University of Illinois, Urbana Champaign;University of Pennsylvania;University of Pennsylvania",19;3;19;19,11;48;11;11,3
3172,3172,3172,3172,3172,3172,3172,3172,ICLR,2020,Model Based Reinforcement Learning for Atari,Łukasz Kaiser;Mohammad Babaeizadeh;Piotr Miłos;Błażej Osiński;Roy H Campbell;Konrad Czechowski;Dumitru Erhan;Chelsea Finn;Piotr Kozakowski;Sergey Levine;Afroz Mohiuddin;Ryan Sepassi;George Tucker;Henryk Michalewski,lukaszkaiser@google.com;mbz@google.com;pmilos@mimuw.edu.pl;blazej.osinski@gmail.com;rhc@illinois.edu;konrad.czechowski@gmail.com;dumitru@google.com;chelseaf@google.com;kozak000@gmail.com;slevine@google.com;afrozm@google.com;rsepassi@google.com;gjt@google.com;henrykmichalewski@gmail.com,6;8;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Spotlight),0,4,0.0,yes,9/25/19,"Google;Google;University of Washington, Seattle;University of Washington, Seattle;University of Illinois, Urbana Champaign;University of Washington, Seattle;Google;Google;University of Washington, Seattle;Google;Google;Google;Google;",-1;-1;6;6;3;6;-1;-1;6;-1;-1;-1;-1;-1,-1;-1;26;26;48;26;-1;-1;26;-1;-1;-1;-1;-1,
3173,3173,3173,3173,3173,3173,3173,3173,ICLR,2020,InfoGraph: Unsupervised and Semi-supervised Graph-Level Representation Learning via Mutual Information Maximization,Fan-Yun Sun;Jordan Hoffman;Vikas Verma;Jian Tang,sunfanyun@gmail.com;jhoffmann@g.harvard.edu;vikasverma.iitm@gmail.com;jian.tang@hec.ca,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,5,0.0,yes,9/25/19,National Taiwan University;Harvard University;;HEC Montreal,86;39;-1;128,120;7;-1;85,3;10;8
3174,3174,3174,3174,3174,3174,3174,3174,ICLR,2020,Black-Box Adversarial Attack with Transferable Model-based Embedding,Zhichao Huang;Tong Zhang,zhuangbx@connect.ust.hk;tongzhang@tongzhang-ml.org,6;8;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,The Hong Kong University of Science and Technology;The Hong Kong University of Science and Technology,39;39,47;47,4
3175,3175,3175,3175,3175,3175,3175,3175,ICLR,2020,Inductive Matrix Completion Based on Graph Neural Networks,Muhan Zhang;Yixin Chen,muhan@wustl.edu;chen@cse.wustl.edu,6;8;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Spotlight),0,5,0.0,yes,9/25/19,"Washington University, St. Louis;Washington University, St. Louis",100;100,52;52,6;10
3176,3176,3176,3176,3176,3176,3176,3176,ICLR,2020,Probability Calibration for Knowledge Graph Embedding Models,Pedro Tabacof;Luca Costabello,tabacof@gmail.com;luca.costabello@accenture.com,6;8;6;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,5,0.0,yes,9/25/19,;Accenture,-1;-1,-1;-1,10
3177,3177,3177,3177,3177,3177,3177,3177,ICLR,2020,Intensity-Free Learning of Temporal Point Processes,Oleksandr Shchur;Marin Biloš;Stephan Günnemann,shchur@in.tum.de;bilos@in.tum.de;guennemann@in.tum.de,8;6;8,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,5,4.0,yes,9/25/19,Technical University Munich;Technical University Munich;Technical University Munich,53;53;53,43;43;43,
3178,3178,3178,3178,3178,3178,3178,3178,ICLR,2020,Certified Robustness for Top-k Predictions against Adversarial Perturbations via Randomized Smoothing,Jinyuan Jia;Xiaoyu Cao;Binghui Wang;Neil Zhenqiang Gong,jinyuan.jia@duke.edu;xiaoyu.cao@duke.edu;binghui.wang@duke.edu;neil.gong@duke.edu,6;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0.0,yes,9/25/19,Duke University;Duke University;Duke University;Duke University,47;47;47;47,20;20;20;20,4;1
3179,3179,3179,3179,3179,3179,3179,3179,ICLR,2020,Simple and Effective Regularization Methods for Training on Noisily Labeled Data with Generalization Guarantee,Wei Hu;Zhiyuan Li;Dingli Yu,huwei@cs.princeton.edu;zhiyuanli@cs.princeton.edu;dingliy@cs.princeton.edu,6;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,Princeton University;Princeton University;Princeton University,31;31;31,6;6;6,1;8
3180,3180,3180,3180,3180,3180,3180,3180,ICLR,2020,"Understanding l4-based Dictionary Learning: Interpretation, Stability, and Robustness",Yuexiang Zhai;Hermish Mehta;Zhengyuan Zhou;Yi Ma,ysz@berkeley.edu;hermish@berkeley.edu;zyzhou@stanford.edu;yima@eecs.berkeley.edu,8;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,2,0.0,yes,9/25/19,University of California Berkeley;University of California Berkeley;Stanford University;University of California Berkeley,5;5;4;5,13;13;4;13,
3181,3181,3181,3181,3181,3181,3181,3181,ICLR,2020,RGBD-GAN: Unsupervised 3D Representation Learning From Natural Image Datasets via RGBD Image Synthesis,Atsuhiro Noguchi;Tatsuya Harada,noguchi@mi.t.u-tokyo.ac.jp;harada@mi.t.u-tokyo.ac.jp,6;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,The University of Tokyo;The University of Tokyo,56;56,36;36,5
3182,3182,3182,3182,3182,3182,3182,3182,ICLR,2020,Estimating counterfactual treatment outcomes over time through adversarially balanced representations,Ioana Bica;Ahmed M Alaa;James Jordon;Mihaela van der Schaar,ioana.bica@eng.ox.ac.uk;a7med3laa@hotmail.com;james.jordon@wolfson.ox.ac.uk;mschaar@turing.ac.uk,6;6;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Spotlight),0,5,0.0,yes,9/25/19,University of Oxford;;University of Oxford;Alan Turing Institute,50;-1;50;-1,1;-1;1;-1,4
3183,3183,3183,3183,3183,3183,3183,3183,ICLR,2020,Exploring Model-based Planning with Policy Networks,Tingwu Wang;Jimmy Ba,tingwuwang@cs.toronto.edu;jba@cs.toronto.edu,8;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",18;18,18;18,
3184,3184,3184,3184,3184,3184,3184,3184,ICLR,2020,LambdaNet: Probabilistic Type Inference using Graph Neural Networks,Jiayi Wei;Maruth Goyal;Greg Durrett;Isil Dillig,jiayi@cs.utexas.edu;maruth@utexas.edu;gdurrett@cs.utexas.edu;isil@cs.utexas.edu,6;8;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Poster),0,8,0.0,yes,9/25/19,"University of Texas, Austin;University of Texas, Austin;University of Texas, Austin;University of Texas, Austin",22;22;22;22,38;38;38;38,10
3185,3185,3185,3185,3185,3185,3185,3185,ICLR,2020,Precision Gating: Improving Neural Network Efficiency with Dynamic Dual-Precision Activations,Yichi Zhang;Ritchie Zhao;Weizhe Hua;Nayun Xu;G. Edward Suh;Zhiru Zhang,yz2499@cornell.edu;rz252@cornell.edu;wh399@cornell.edu;nx38@cornell.edu;edward.suh@cornell.edu;zhiruz@cornell.edu,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A,Accept (Poster),0,6,0.0,yes,9/25/19,Cornell University;Cornell University;Cornell University;Cornell University;Cornell University;Cornell University,7;7;7;7;7;7,19;19;19;19;19;19,
3186,3186,3186,3186,3186,3186,3186,3186,ICLR,2020,Variational Template Machine for Data-to-Text Generation,Rong Ye;Wenxian Shi;Hao Zhou;Zhongyu Wei;Lei Li,rye18@fudan.edu.cn;shiwenxian@bytedance.com;zhouhao.nlp@bytedance.com;zywei@fudan.edu.cn;lileilab@bytedance.com,8;3;8,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Accept (Poster),0,4,0.0,yes,9/25/19,Fudan University;Bytedance;Bytedance;Fudan University;Bytedance,79;-1;-1;79;-1,109;-1;-1;109;-1,
3187,3187,3187,3187,3187,3187,3187,3187,ICLR,2020,On Universal Equivariant Set Networks,Nimrod Segol;Yaron Lipman,nimrod.segol@weizmann.ac.il;yaron.lipman@weizmann.ac.il,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0.0,yes,9/25/19,Weizmann Institute;Weizmann Institute,108;108,1397;1397,1;2
3188,3188,3188,3188,3188,3188,3188,3188,ICLR,2020,The Gambler's Problem and Beyond,Baoxiang Wang;Shuai Li;Jiajin Li;Siu On Chan,bxwang@cse.cuhk.edu.hk;shuaili8@sjtu.edu.cn;jjli@se.cuhk.edu.hk;siuon@cse.cuhk.edu.hk,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,9,0.0,yes,9/25/19,The Chinese University of Hong Kong;Shanghai Jiao Tong University;The Chinese University of Hong Kong;The Chinese University of Hong Kong,59;53;59;59,35;157;35;35,
3189,3189,3189,3189,3189,3189,3189,3189,ICLR,2020,Learning to Represent Programs with Property Signatures,Augustus Odena;Charles Sutton,augustusodena@google.com;csutton@inf.ed.ac.uk,6;6;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,8,0.0,yes,9/25/19,Google;University of Edinburgh,-1;33,-1;30,
3190,3190,3190,3190,3190,3190,3190,3190,ICLR,2020,Learning to Balance: Bayesian Meta-Learning for Imbalanced and Out-of-distribution Tasks,Hae Beom Lee;Hayeon Lee;Donghyun Na;Saehoon Kim;Minseop Park;Eunho Yang;Sung Ju Hwang,haebeom.lee@kaist.ac.kr;hayeon926@kaist.ac.kr;donghyun.na@kaist.ac.kr;shkim@aitrics.com;mike_seop@aitrics.com;eunhoy@kaist.ac.kr;sjhwang82@kaist.ac.kr,8;8;8,I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Accept (Talk),0,4,0.0,yes,9/25/19,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;AITRICS;AITRICS;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,481;481;481;-1;-1;481;481,110;110;110;-1;-1;110;110,11;6
3191,3191,3191,3191,3191,3191,3191,3191,ICLR,2020,Smooth markets: A basic mechanism for organizing gradient-based learners,David Balduzzi;Wojciech M. Czarnecki;Tom Anthony;Ian Gemp;Edward Hughes;Joel Leibo;Georgios Piliouras;Thore Graepel,dbalduzzi@google.com;lejlot@google.com;edwardhughes@google.com;jzl@google.com;imgemp@google.com;twa@google.com;georgios.piliouras@gmail.com;thore@google.com,8;8,I have read many papers in this area.:N/A:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:N/A:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,2,0.0,yes,9/25/19,Google;Google;Google;Google;Google;Google;Singapore University of Technology and Design;Google,-1;-1;-1;-1;-1;-1;481;-1,-1;-1;-1;-1;-1;-1;1397;-1,5;4
3192,3192,3192,3192,3192,3192,3192,3192,ICLR,2020,Thinking While Moving: Deep Reinforcement Learning with Concurrent Control,Ted Xiao;Eric Jang;Dmitry Kalashnikov;Sergey Levine;Julian Ibarz;Karol Hausman;Alexander Herzog,tedxiao@google.com;ejang@google.com;dkalashnikov@google.com;slevine@google.com;julianibarz@google.com;karolhausman@google.com;alexherzog@google.com,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Accept (Poster),0,5,0.0,yes,9/25/19,Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,
3193,3193,3193,3193,3193,3193,3193,3193,ICLR,2020,Demystifying Inter-Class Disentanglement,Aviv Gabbay;Yedid Hoshen,avivga@gmail.com;yedid@cs.huji.ac.il,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,7,0.0,yes,9/25/19,Hebrew University of Jerusalem;Hebrew University of Jerusalem,67;67,216;216,4
3194,3194,3194,3194,3194,3194,3194,3194,ICLR,2020,Meta-Learning Deep Energy-Based Memory Models,Sergey Bartunov;Jack Rae;Simon Osindero;Timothy Lillicrap,bartunov@google.com;jwrae@google.com;osindero@google.com;countzero@google.com,6;6;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,8,0.0,yes,9/25/19,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,6
3195,3195,3195,3195,3195,3195,3195,3195,ICLR,2020,Self-Supervised Learning of Appliance Usage,Chen-Yu Hsu;Abbas Zeitoun;Guang-He Lee;Dina Katabi;Tommi Jaakkola,cyhsu@mit.edu;zeitoun@mit.edu;guanghe@csail.mit.edu;dina@csail.mit.edu;tommi@csail.mit.edu,6;3;8,I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,6,0.0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2;2,5;5;5;5;5,
3196,3196,3196,3196,3196,3196,3196,3196,ICLR,2020,Higher-Order Function Networks for Learning Composable 3D Object Representations,Eric Mitchell;Selim Engin;Volkan Isler;Daniel D Lee,eric.anthony.mitchell95@gmail.com;engin003@umn.edu;isler@umn.edu;ddlee@seas.upenn.edu,6;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,"Stanford University;University of Minnesota, Minneapolis;University of Minnesota, Minneapolis;University of Pennsylvania",4;59;59;19,4;79;79;11,
3197,3197,3197,3197,3197,3197,3197,3197,ICLR,2020,Learning to solve the credit assignment problem,Benjamin James Lansdell;Prashanth Ravi Prakash;Konrad Paul Kording,ben.lansdell@gmail.com;prprak@seas.upenn.edu;koerding@gmail.com,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,6,0.0,yes,9/25/19,University of Pennsylvania;University of Pennsylvania;University of Pennsylvania,19;19;19,11;11;11,1;9
3198,3198,3198,3198,3198,3198,3198,3198,ICLR,2020,Prediction Poisoning: Towards Defenses Against DNN Model Stealing Attacks,Tribhuvanesh Orekondy;Bernt Schiele;Mario Fritz,orekondy@mpi-inf.mpg.de;schiele@mpi-inf.mpg.de;fritz@cispa.saarland,8;3;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,1.0,yes,9/25/19,"Saarland Informatics Campus, Max-Planck Institute;Saarland Informatics Campus, Max-Planck Institute;CISPA Helmholtz Center for Information Security",-1;-1;143,-1;-1;1397,4
3199,3199,3199,3199,3199,3199,3199,3199,ICLR,2020,Difference-Seeking Generative Adversarial Network--Unseen Sample Generation,Yi Lin Sung;Sung-Hsien Hsieh;Soo-Chang Pei;Chun-Shien Lu,r06942076@ntu.edu.tw;parvaty316@hotmail.com;peisc@ntu.edu.tw;lcs@iis.sinica.edu.tw,6;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0.0,yes,9/25/19,National Taiwan University;;National Taiwan University;Academia Sinica,86;-1;86;-1,120;-1;120;-1,5;4
3200,3200,3200,3200,3200,3200,3200,3200,ICLR,2020,Reducing Transformer Depth on Demand with Structured Dropout,Angela Fan;Edouard Grave;Armand Joulin,angelafan@fb.com;egrave@fb.com;ajoulin@fb.com,8;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),1,10,0.0,yes,9/25/19,Facebook;Facebook;Facebook,-1;-1;-1,-1;-1;-1,3
3201,3201,3201,3201,3201,3201,3201,3201,ICLR,2020,Actor-Critic Provably Finds Nash Equilibria of Linear-Quadratic Mean-Field Games,Zuyue Fu;Zhuoran Yang;Yongxin Chen;Zhaoran Wang,zuyuefu2022@u.northwestern.edu;zy6@princeton.edu;yongchen@gatech.edu;zhaoranwang@gmail.com,8;6;6,I have read many papers in this area.:N/A:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:N/A:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,16,0.0,yes,9/25/19,Northwestern University;Princeton University;Georgia Institute of Technology;Northwestern University,44;31;13;44,22;6;38;22,1;9
3202,3202,3202,3202,3202,3202,3202,3202,ICLR,2020,Quantifying the Cost of Reliable Photo Authentication via High-Performance Learned Lossy Representations,Pawel Korus;Nasir Memon,pkorus@nyu.edu;memon@nyu.edu,6;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,New York University;New York University,25;25,29;29,
3203,3203,3203,3203,3203,3203,3203,3203,ICLR,2020,Multi-agent Reinforcement Learning for Networked System Control,Tianshu Chu;Sandeep Chinchali;Sachin Katti,cts198859@hotmail.com;csandeep@stanford.edu;skatti@stanford.edu,6;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0.0,yes,9/25/19,VMware Inc;Stanford University;Stanford University,-1;4;4,-1;4;4,
3204,3204,3204,3204,3204,3204,3204,3204,ICLR,2020,Learning Space Partitions for Nearest Neighbor Search,Yihe Dong;Piotr Indyk;Ilya Razenshteyn;Tal Wagner,yihedong@gmail.com;indyk@mit.edu;ilyaraz@microsoft.com;tal.wagner@gmail.com,6;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,4,0.0,yes,9/25/19,Microsoft;Massachusetts Institute of Technology;Microsoft;Massachusetts Institute of Technology,-1;2;-1;2,-1;5;-1;5,10
3205,3205,3205,3205,3205,3205,3205,3205,ICLR,2020,On Computation and Generalization of Generative Adversarial Imitation Learning,Minshuo Chen;Yizhou Wang;Tianyi Liu;Zhuoran Yang;Xingguo Li;Zhaoran Wang;Tuo Zhao,mchen393@gatech.edu;wyzjack990122@gmail.com;tianyiliu@gatech.edu;zy6@princeton.edu;xingguol@princeton.edu;zhaoran.wang@northwestern.edu;tourzhao@gatech.edu,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Accept (Poster),0,4,0.0,yes,9/25/19,Georgia Institute of Technology;Xi'an Jiaotong University;Georgia Institute of Technology;Princeton University;Princeton University;Northwestern University;Georgia Institute of Technology,13;481;13;31;31;44;13,38;555;38;6;6;22;38,5;4;8
3206,3206,3206,3206,3206,3206,3206,3206,ICLR,2020,Evaluating The Search Phase of Neural Architecture Search,Kaicheng Yu;Christian Sciuto;Martin Jaggi;Claudiu Musat;Mathieu Salzmann,kaicheng.yu@epfl.ch;sciutochristian@gmail.com;martin.jaggi@epfl.ch;claudiu.musat@swisscom.com;mathieu.salzmann@epfl.ch,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0.0,yes,9/25/19,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swisscom;Swiss Federal Institute of Technology Lausanne,481;481;481;-1;481,38;38;38;-1;38,
3207,3207,3207,3207,3207,3207,3207,3207,ICLR,2020,Critical initialisation in continuous approximations of binary neural networks,George Stamatescu;Federica Gerace;Carlo Lucibello;Ian Fuss;Langford White,george.stamatescu@gmail.com;federicagerace91@gmail.com;carlo.lucibello@gmail.com;ian.fuss@adelaide.edu.au;lang.white@adelaide.edu.au,6;3;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,4,0.0,yes,9/25/19,The University of Adelaide;;Bocconi University;The University of Adelaide;The University of Adelaide,128;-1;323;128;128,120;-1;1397;120;120,
3208,3208,3208,3208,3208,3208,3208,3208,ICLR,2020,Evolutionary Population Curriculum for Scaling Multi-Agent Reinforcement Learning,Qian Long*;Zihan Zhou*;Abhinav Gupta;Fei Fang;Yi Wu†;Xiaolong Wang†,qianlong@cs.cmu.edu;footoredo@sjtu.edu.cn;abhinavg@cs.cmu.edu;feif@cs.cmu.edu;jxwuyi@gmail.com;dragonwxl123@gmail.com,6;6;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Accept (Poster),0,4,0.0,yes,9/25/19,Carnegie Mellon University;Shanghai Jiao Tong University;Carnegie Mellon University;Carnegie Mellon University;OpenAI;University of California Berkeley,1;53;1;1;-1;5,27;157;27;27;-1;13,
3209,3209,3209,3209,3209,3209,3209,3209,ICLR,2020,Probabilistic Connection Importance Inference and Lossless Compression of Deep Neural Networks,Xin Xing;Long Sha;Pengyu Hong;Zuofeng Shang;Jun S. Liu,xin_xing@fas.harvard.edu;longsha@brandeis.edu;hongpeng@brandeis.edu;zuofeng.shang@njit.edu;jliu@stat.harvard.edu,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,10,0.0,yes,9/25/19,Harvard University;Brandeis University;Brandeis University;New Jersey Institute of Technology;Harvard University,39;323;323;172;39,7;244;244;564;7,
3210,3210,3210,3210,3210,3210,3210,3210,ICLR,2020,Picking Winning Tickets Before Training by Preserving Gradient Flow,Chaoqi Wang;Guodong Zhang;Roger Grosse,cqwang@cs.toronto.edu;gdzhang@cs.toronto.edu;rgrosse@cs.toronto.edu,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,12,0.0,yes,9/25/19,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",18;18;18,18;18;18,8
3211,3211,3211,3211,3211,3211,3211,3211,ICLR,2020,CAQL: Continuous Action Q-Learning,Moonkyung Ryu;Yinlam Chow;Ross Anderson;Christian Tjandraatmadja;Craig Boutilier,mkryu@google.com;yinlamchow@google.com;rander@google.com;ctjandra@google.com;cboutilier@google.com,6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:N/A:N/A,Accept (Poster),1,2,0.0,yes,9/25/19,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,
3212,3212,3212,3212,3212,3212,3212,3212,ICLR,2020,Stable Rank Normalization for Improved Generalization in Neural Networks and GANs,Amartya Sanyal;Philip H. Torr;Puneet K. Dokania,amartya.sanyal@cs.ox.ac.uk;philip.torr@eng.ox.ac.uk;puneet@robots.ox.ac.uk,6;8;8,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,9,1.0,yes,9/25/19,University of Oxford;University of Oxford;University of Oxford,50;50;50,1;1;1,5;1;8
3213,3213,3213,3213,3213,3213,3213,3213,ICLR,2020,Dynamic Model Pruning with Feedback,Tao Lin;Sebastian U. Stich;Luis Barba;Daniil Dmitriev;Martin Jaggi,tao.lin@epfl.ch;sebastian.stich@epfl.ch;luis.barba@inf.ethz.ch;daniil.dmitriev@epfl.ch;martin.jaggi@epfl.ch,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),2,5,1.0,yes,9/25/19,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne,481;481;10;481;481,38;38;13;38;38,
3214,3214,3214,3214,3214,3214,3214,3214,ICLR,2020,Emergent Tool Use From Multi-Agent Autocurricula,Bowen Baker;Ingmar Kanitscheider;Todor Markov;Yi Wu;Glenn Powell;Bob McGrew;Igor Mordatch,bowen@openai.com;ingmar@openai.com;todor@openai.com;jxwuyi@openai.com;glenn@openai.com;bmcgrew@openai.com;imordatch@google.com,6;8;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:N/A:N/A:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:N/A:N/A,Accept (Spotlight),1,5,0.0,yes,9/25/19,OpenAI;OpenAI;OpenAI;OpenAI;OpenAI;OpenAI;Google,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,
3215,3215,3215,3215,3215,3215,3215,3215,ICLR,2020,Conditional Learning of Fair Representations,Han Zhao;Amanda Coston;Tameem Adel;Geoffrey J. Gordon,han.zhao@cs.cmu.edu;acoston@cs.cmu.edu;tah47@cam.ac.uk;ggordon@cs.cmu.edu,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,6,0.0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;University of Cambridge;Carnegie Mellon University,1;1;71;1,27;27;3;27,7
3216,3216,3216,3216,3216,3216,3216,3216,ICLR,2020,You CAN Teach an Old Dog New Tricks! On Training Knowledge Graph Embeddings,Daniel Ruffinelli;Samuel Broscheit;Rainer Gemulla,daniel@informatik.uni-mannheim.de;broscheit@informatik.uni-mannheim.de;rgemulla@uni-mannheim.de,6;6;8,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),6,4,0.0,yes,9/25/19,University of Mannheim;University of Mannheim;University of Mannheim,233;233;233,157;157;157,10
3217,3217,3217,3217,3217,3217,3217,3217,ICLR,2020,Disagreement-Regularized Imitation Learning,Kiante Brantley;Wen Sun;Mikael Henaff,kdbrant@cs.umd.edu;wen.sun@microsoft.com;mihenaff@microsoft.com,8;8;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),1,4,1.0,yes,9/25/19,"University of Maryland, College Park;Microsoft;Microsoft",12;-1;-1,91;-1;-1,5;4;1
3218,3218,3218,3218,3218,3218,3218,3218,ICLR,2020,Physics-as-Inverse-Graphics: Unsupervised Physical Parameter Estimation from Video,Miguel Jaques;Michael Burke;Timothy Hospedales,m.a.m.jaques@sms.ed.ac.uk;michael.burke@ed.ac.uk;t.hospedales@ed.ac.uk,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,9,0.0,yes,9/25/19,University of Edinburgh;University of Edinburgh;University of Edinburgh,33;33;33,30;30;30,
3219,3219,3219,3219,3219,3219,3219,3219,ICLR,2020,Model-Augmented Actor-Critic: Backpropagating through Paths,Ignasi Clavera;Yao Fu;Pieter Abbeel,iclavera@berkeley.edu;violetfuyao@berkeley.edu;pabbeel@cs.berkeley.edu,3;8;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,13;13;13,
3220,3220,3220,3220,3220,3220,3220,3220,ICLR,2020,Disentanglement by Nonlinear ICA with General Incompressible-flow Networks (GIN),Peter Sorrenson;Carsten Rother;Ullrich Köthe,peter.sorrenson@gmail.com;carsten.rother@iwr.uni-heidelberg.de;ullrich.koethe@iwr.uni-heidelberg.de,6;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,5,0.0,yes,9/25/19,Heidelberg University;Heidelberg University;Heidelberg University,205;205;205,44;44;44,5;1
3221,3221,3221,3221,3221,3221,3221,3221,ICLR,2020,Generative Ratio Matching Networks,Akash Srivastava;Kai Xu;Michael U. Gutmann;Charles Sutton,akash.srivastava@me.com;kai.xu@ed.ac.uk;michael.gutmann@ed.ac.uk;charlessutton@google.com,6;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,10,0.0,yes,9/25/19,International Business Machines;University of Edinburgh;University of Edinburgh;Google,-1;33;33;-1,-1;30;30;-1,5;4
3222,3222,3222,3222,3222,3222,3222,3222,ICLR,2020,CM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement Learning,Jiachen Yang;Alireza Nakhaei;David Isele;Kikuo Fujimura;Hongyuan Zha,yjiachen@gmail.com;anakhaei@honda-ri.com;disele@honda-ri.com;kfujimura@honda-ri.com;zha@cc.gatech.edu,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Accept (Poster),0,4,0.0,yes,9/25/19,Georgia Institute of Technology;Honda Research Institute;Honda Research Institute;Honda Research Institute;Georgia Institute of Technology,13;-1;-1;-1;13,38;-1;-1;-1;38,
3223,3223,3223,3223,3223,3223,3223,3223,ICLR,2020,Order Learning and Its Application to Age Estimation,Kyungsun Lim;Nyeong-Ho Shin;Young-Yoon Lee;Chang-Su Kim,kslim@mcl.korea.ac.kr;nhshin@mcl.korea.ac.kr;yy77lee@gmail.com;changsukim@korea.ac.kr,6;8;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,Korea University;Korea University;Samsung;Korea University,323;323;-1;323,179;179;-1;179,7;2;10
3224,3224,3224,3224,3224,3224,3224,3224,ICLR,2020,Learning to Link,Maria-Florina Balcan;Travis Dick;Manuel Lang,ninamf@cs.cmu.edu;tdick@ttic.edu;manuel.lang@student.kit.edu,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,Carnegie Mellon University;Toyota Technological Institute at Chicago;Karlsruhe Institute of Technology,1;-1;154,27;-1;174,
3225,3225,3225,3225,3225,3225,3225,3225,ICLR,2020,DiffTaichi: Differentiable Programming for Physical Simulation,Yuanming Hu;Luke Anderson;Tzu-Mao Li;Qi Sun;Nathan Carr;Jonathan Ragan-Kelley;Fredo Durand,yuanmhu@gmail.com;lukea@mit.edu;tzumao@berkeley.edu;qisu@adobe.com;ncarr@adobe.com;jrk@berkeley.edu;fredo@mit.edu,6;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;University of California Berkeley;Adobe Systems;Adobe Systems;University of California Berkeley;Massachusetts Institute of Technology,2;2;5;-1;-1;5;2,5;5;13;-1;-1;13;5,
3226,3226,3226,3226,3226,3226,3226,3226,ICLR,2020,Adaptive Structural Fingerprints for Graph Attention Networks,Kai Zhang;Yaokang Zhu;Jun Wang;Jie Zhang,kzhang980@gmail.com;52184501026@stu.ecnu.edu.cn;wongjun@gmail.com;jzhang080@gmail.com,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,7,0.0,yes,9/25/19,;Australian National University;;,-1;108;-1;-1,-1;50;-1;-1,10
3227,3227,3227,3227,3227,3227,3227,3227,ICLR,2020,Kernelized Wasserstein Natural Gradient,M Arbel;A Gretton;W Li;G Montufar,michael.n.arbel@gmail.com;arthur.gretton@gmail.com;wcli@math.ucla.edu;guidomontufar@gmail.com,8;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,6,0.0,yes,9/25/19,"University College London;;University of California, Los Angeles;Max Planck Institute MIS",50;-1;20;-1,15;-1;17;-1,
3228,3228,3228,3228,3228,3228,3228,3228,ICLR,2020,DeepV2D: Video to Depth with Differentiable Structure from Motion,Zachary Teed;Jia Deng,zteed@princeton.edu;jiadeng@princeton.edu,8;6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,Princeton University;Princeton University,31;31,6;6,
3229,3229,3229,3229,3229,3229,3229,3229,ICLR,2020,Measuring the Reliability of Reinforcement Learning Algorithms,Stephanie C.Y. Chan;Samuel Fishman;Anoop Korattikara;John Canny;Sergio Guadarrama,scychan@google.com;sfishman@google.com;kbanoop@google.com;canny@google.com;sguada@google.com,8;8;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Spotlight),0,5,0.0,yes,9/25/19,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,
3230,3230,3230,3230,3230,3230,3230,3230,ICLR,2020,SELF: Learning to Filter Noisy Labels with Self-Ensembling,Duc Tam Nguyen;Chaithanya Kumar Mummadi;Thi Phuong Nhung Ngo;Thi Hoai Phuong Nguyen;Laura Beggel;Thomas Brox,ductam.nguyen08@gmail.com;chaithanyakumar.mummadi@de.bosch.com;thiphuongnhung.ngo@de.bosch.com;hoai.phuong.nguyen198@gmail.com;laura.beggel@de.bosch.com;brox@cs.uni-freiburg.de,6;8;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,6,1.0,yes,9/25/19,Universität Freiburg;Bosch;Bosch;Karlsruhe Institute of Technology;Bosch;Universität Freiburg,118;-1;-1;154;-1;118,85;-1;-1;174;-1;85,
3231,3231,3231,3231,3231,3231,3231,3231,ICLR,2020,Incorporating BERT into Neural Machine Translation,Jinhua Zhu;Yingce Xia;Lijun Wu;Di He;Tao Qin;Wengang Zhou;Houqiang Li;Tieyan Liu,teslazhu@mail.ustc.edu.cn;yingce.xia@gmail.com;wulijun3@mail2.sysu.edu.cn;di_he@pku.edu.cn;taoqin@microsoft.com;zhwg@ustc.edu.cn;lihq@ustc.edu.cn;tyliu@microsoft.com,6;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Poster),0,4,0.0,yes,9/25/19,University of Science and Technology of China;Microsoft;SUN YAT-SEN UNIVERSITY;Peking University;Microsoft;University of Science and Technology of China;University of Science and Technology of China;Microsoft,481;-1;481;22;-1;481;481;-1,80;-1;299;24;-1;80;80;-1,3
3232,3232,3232,3232,3232,3232,3232,3232,ICLR,2020,Mogrifier LSTM,Gábor Melis;Tomáš Kočiský;Phil Blunsom,melisgl@google.com;tkocisky@google.com;pblunsom@google.com,6;8;8,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Talk),0,3,0.0,yes,9/25/19,Google;Google;Google,-1;-1;-1,-1;-1;-1,3;8
3233,3233,3233,3233,3233,3233,3233,3233,ICLR,2020,Transferring Optimality Across Data Distributions via Homotopy Methods,Matilde Gargiani;Andrea Zanelli;Quoc Tran Dinh;Moritz Diehl;Frank Hutter,gargiani@informatik.uni-freiburg.de;andrea.zanelli@imtek.uni-freiburg.de;quoctd@email.unc.edu;moritz.diehl@imtek.uni-freiburg.de;fh@cs.uni-freiburg.de,3;6;8,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,"Universität Freiburg;Universität Freiburg;University of North Carolina, Chapel Hill;Universität Freiburg;Universität Freiburg",118;118;73;118;118,85;85;54;85;85,9
3234,3234,3234,3234,3234,3234,3234,3234,ICLR,2020,Quantum Algorithms for Deep Convolutional Neural Networks,Iordanis Kerenidis;Jonas Landman;Anupam Prakash,jkeren@gmail.com;landman@irif.fr;anupamprakash1@gmail.com,6;8;8;6,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0.0,yes,9/25/19,Université Paris Diderot;Universite Paris Diderot;Universite Paris Diderot,481;481;481,1397;1397;1397,
3235,3235,3235,3235,3235,3235,3235,3235,ICLR,2020,Unrestricted Adversarial Examples via Semantic Manipulation,Anand Bhattad;Min Jin Chong;Kaizhao Liang;Bo Li;D. A. Forsyth,bhattad2@illinois.edu;mchong6@illinois.edu;kl2@illinois.edu;lbo@illinois.edu;daf@illinois.edu,6;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",3;3;3;3;3,48;48;48;48;48,4
3236,3236,3236,3236,3236,3236,3236,3236,ICLR,2020,A Stochastic Derivative Free Optimization Method with Momentum,Eduard Gorbunov;Adel Bibi;Ozan Sener;El Houcine Bergou;Peter Richtarik,eduard.gorbunov@phystech.edu;adel.bibi@kaust.edu.sa;ozan.sener@intel.com;houcine.bergou@kaust.edu.sa;peter.richtarik@kaust.edu.sa,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,Moscow Institute of Physics and Technology;KAUST;Intel;KAUST;KAUST,481;128;-1;128;128,234;1397;-1;1397;1397,
3237,3237,3237,3237,3237,3237,3237,3237,ICLR,2020,Learning The Difference That Makes A Difference With Counterfactually-Augmented Data,Divyansh Kaushik;Eduard Hovy;Zachary Lipton,dkaushik@cs.cmu.edu;hovy@cmu.edu;zlipton@cmu.edu,8;8;1;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Accept (Spotlight),0,5,0.0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1,27;27;27,3
3238,3238,3238,3238,3238,3238,3238,3238,ICLR,2020,Fast is better than free: Revisiting adversarial training,Eric Wong;Leslie Rice;J. Zico Kolter,ericwong@cs.cmu.edu;larice@cs.cmu.edu;zkolter@cs.cmu.edu,6;6;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),10,11,0.0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1,27;27;27,4
3239,3239,3239,3239,3239,3239,3239,3239,ICLR,2020,Disentangling neural mechanisms for perceptual grouping,Junkyung Kim*;Drew Linsley*;Kalpit Thakkar;Thomas Serre,junkyung_kim@brown.edu;drew_linsley@brown.edu;kalpit_thakkar@brown.edu;thomas_serre@brown.edu,8;6;8,I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Spotlight),0,5,0.0,yes,9/25/19,Brown University;Brown University;Brown University;Brown University,67;67;67;67,53;53;53;53,
3240,3240,3240,3240,3240,3240,3240,3240,ICLR,2020,How to 0wn the NAS in Your Spare Time,Sanghyun Hong;Michael Davinroy;Yiǧitcan Kaya;Dana Dachman-Soled;Tudor Dumitraş,shhong@cs.umd.edu;michael.davinroy@gmail.com;cankaya@umiacs.umd.edu;danadach@ece.umd.edu;tdumitra@umiacs.umd.edu,6;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Accept (Poster),0,6,0.0,yes,9/25/19,"University of Maryland, College Park;Swarthmore College;University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park",12;-1;12;12;12,91;-1;91;91;91,4;10
3241,3241,3241,3241,3241,3241,3241,3241,ICLR,2020,Option Discovery using Deep Skill Chaining,Akhil Bagaria;George Konidaris,akhil_bagaria@brown.edu;gdk@cs.brown.edu,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,7,0.0,yes,9/25/19,Brown University;Brown University,67;67,53;53,
3242,3242,3242,3242,3242,3242,3242,3242,ICLR,2020,Optimal Strategies Against Generative Attacks,Roy Mor;Erez Peterfreund;Matan Gavish;Amir Globerson,roy16mor@gmail.com;erezpeter@cs.huji.ac.il;matan.gavish@mail.huji.ac.il;amir.globerson@gmail.com,8;8;8;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,4,0.0,yes,9/25/19,Tel Aviv University;Hebrew University of Jerusalem;Hebrew University of Jerusalem;Tel Aviv University,35;67;67;35,188;216;216;188,5;4
3243,3243,3243,3243,3243,3243,3243,3243,ICLR,2020,Massively Multilingual Sparse Word Representations,Gábor Berend,berendg@inf.u-szeged.hu,8;8;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,5,0.0,yes,9/25/19,University of Szeged,323,874,3
3244,3244,3244,3244,3244,3244,3244,3244,ICLR,2020,RaPP: Novelty Detection with Reconstruction along Projection Pathway,Ki Hyun Kim;Sangwoo Shim;Yongsub Lim;Jongseob Jeon;Jeongwoo Choi;Byungchan Kim;Andre S. Yoon,khkim@makinarocks.ai;sangwoo@makinarocks.ai;yongsub@makinarocks.ai;jongseob.jeon@makinarocks.ai;jeongwoo@makinarocks.ai;kbc8894@makinarocks.ai;andre@makinarocks.ai,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,8,1.0,yes,9/25/19,MakinaRocks;MakinaRocks;MakinaRocks;MakinaRocks;MakinaRocks;MakinaRocks;MakinaRocks,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,
3245,3245,3245,3245,3245,3245,3245,3245,ICLR,2020,Domain Adaptive Multibranch Networks,Róger Bermúdez-Chacón;Mathieu Salzmann;Pascal Fua,roger.bermudez@epfl.ch;mathieu.salzmann@epfl.ch;pascal.fua@epfl.ch,6;3;8,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne,481;481;481,38;38;38,10
3246,3246,3246,3246,3246,3246,3246,3246,ICLR,2020,Continual learning with hypernetworks,Johannes von Oswald;Christian Henning;João Sacramento;Benjamin F. Grewe,voswaldj@ethz.ch;henningc@ethz.ch;sacramento@ini.ethz.ch;bgrewe@ethz.ch,6;8;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0.0,yes,9/25/19,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10;10,13;13;13;13,6
3247,3247,3247,3247,3247,3247,3247,3247,ICLR,2020,Distributed Bandit Learning: Near-Optimal Regret with Efficient Communication,Yuanhao Wang;Jiachen Hu;Xiaoyu Chen;Liwei Wang,yuanhao-16@mails.tsinghua.edu.cn;nickh@pku.edu.cn;cxy30@pku.edu.cn;wanglw@cis.pku.edu.cn,6;6;8,I have published in this field for several years.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:N/A:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,Tsinghua University;Peking University;Peking University;Peking University,8;22;22;22,23;24;24;24,1
3248,3248,3248,3248,3248,3248,3248,3248,ICLR,2020,SVQN: Sequential Variational Soft Q-Learning Networks,Shiyu Huang;Hang Su;Jun Zhu;Ting Chen,huangsy1314@163.com;suhangss@mail.tsinghua.edu.cn;dcszj@tsinghua.edu.cn;tingchen@tsinghua.edu.cn,8;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,2,0.0,yes,9/25/19,Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University,8;8;8;8,23;23;23;23,10;8
3249,3249,3249,3249,3249,3249,3249,3249,ICLR,2020,PROGRESSIVE LEARNING AND DISENTANGLEMENT OF HIERARCHICAL REPRESENTATIONS,Zhiyuan Li;Jaideep Vitthal Murkute;Prashnna Kumar Gyawali;Linwei Wang,zl7904@rit.edu;jvm6526@rit.edu;pkg2182@rit.edu;linwei.wang@rit.edu,8;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,6,0.0,yes,9/25/19,Rochester Institute of Technology;Rochester Institute of Technology;Rochester Institute of Technology;Rochester Institute of Technology,128;128;128;128,843;843;843;843,5
3250,3250,3250,3250,3250,3250,3250,3250,ICLR,2020,Learning transport cost from subset correspondence,Ruishan Liu;Akshay Balsubramani;James Zou,ruishan@stanford.edu;akshay7@gmail.com;jamesyzou@gmail.com,3;6;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,Stanford University;;Stanford University,4;-1;4,4;-1;4,
3251,3251,3251,3251,3251,3251,3251,3251,ICLR,2020,Meta Dropout: Learning to Perturb Latent Features for Generalization,Hae Beom Lee;Taewook Nam;Eunho Yang;Sung Ju Hwang,haebeom.lee@kaist.ac.kr;namsan@kaist.ac.kr;eunhoy@kaist.ac.kr;sjhwang82@kaist.ac.kr,8;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:N/A:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0.0,yes,9/25/19,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,481;481;481;481,110;110;110;110,6;8
3252,3252,3252,3252,3252,3252,3252,3252,ICLR,2020,On Robustness of Neural Ordinary Differential Equations,Hanshu YAN;Jiawei DU;Vincent TAN;Jiashi FENG,hanshu.yan@u.nus.edu;dujiawei@u.nus.edu;vtan@nus.edu.sg;elefjia@nus.edu.sg,6;8;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,11,1.0,yes,9/25/19,National University of Singapore;National University of Singapore;National University of Singapore;National University of Singapore,16;16;16;16,25;25;25;25,4
3253,3253,3253,3253,3253,3253,3253,3253,ICLR,2020,Towards neural networks that provably know when they don't know,Alexander Meinke;Matthias Hein,alexander.meinke@uni-tuebingen.de;matthias.hein@uni-tuebingen.de,6;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,University of Tuebingen;University of Tuebingen,154;154,91;91,
3254,3254,3254,3254,3254,3254,3254,3254,ICLR,2020,Exploration in Reinforcement Learning with Deep Covering Options,Yuu Jinnai;Jee Won Park;Marlos C. Machado;George Konidaris,yuu_jinnai@brown.edu;jee_won_park@brown.edu;marlosm@google.com;gdk@cs.brown.edu,6;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,8,0.0,yes,9/25/19,Brown University;Brown University;Google;Brown University,67;67;-1;67,53;53;-1;53,1;10
3255,3255,3255,3255,3255,3255,3255,3255,ICLR,2020,Mixed Precision DNNs: All you need is a good parametrization,Stefan Uhlich;Lukas Mauch;Fabien Cardinaux;Kazuki Yoshiyama;Javier Alonso Garcia;Stephen Tiedemann;Thomas Kemp;Akira Nakamura,stefan.uhlich@sony.com;lukas.mauch@sony.com;fabien.cardinaux@sony.com;kazuki.yoshiyama@sony.com;javier.alonso@sony.com;stephen.tiedemann@sony.com;thomas.kemp@sony.com;akira.b.nakamura@sony.com,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:N/A:N/A,Accept (Poster),1,7,1.0,yes,9/25/19,Sony Europe Ltd.;Sony Europe Ltd.;Sony Europe Ltd.;Sony Europe Ltd.;Sony Europe Ltd.;Sony Europe Ltd.;Sony Europe Ltd.;Sony Europe Ltd.,-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1,
3256,3256,3256,3256,3256,3256,3256,3256,ICLR,2020,Depth-Width Trade-offs for ReLU Networks via Sharkovsky's Theorem,Vaggos Chatziafratis;Sai Ganesh Nagarajan;Ioannis Panageas;Xiao Wang,vaggos@cs.stanford.edu;sai_nagarajan@mymail.sutd.edu.sg;ioannis@sutd.edu.sg;xiao_wang@sutd.edu.sg,8;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,3,0.0,yes,9/25/19,Stanford University;Singapore University of Technology and Design;Singapore University of Technology and Design;Singapore University of Technology and Design,4;481;481;481,4;1397;1397;1397,1
3257,3257,3257,3257,3257,3257,3257,3257,ICLR,2020,Learning to Move with Affordance Maps,William Qi;Ravi Teja Mullapudi;Saurabh Gupta;Deva Ramanan,wq@cs.cmu.edu;raviteja.mullapudi@gmail.com;saurabhg@illinois.edu;deva@cs.cmu.edu,6;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,8,0.0,yes,9/25/19,"Carnegie Mellon University;Carnegie Mellon University;University of Illinois, Urbana Champaign;Carnegie Mellon University",1;1;3;1,27;27;48;27,
3258,3258,3258,3258,3258,3258,3258,3258,ICLR,2020,Data-dependent Gaussian Prior Objective for Language Generation,Zuchao Li;Rui Wang;Kehai Chen;Masso Utiyama;Eiichiro Sumita;Zhuosheng Zhang;Hai Zhao,charlee@sjtu.edu.cn;wangrui@nict.go.jp;khchen@nict.go.jp;mutiyama@nict.go.jp;eiichiro.sumita@nict.go.jp;zhangzs@sjtu.edu.cn;zhaohai@cs.sjtu.edu.cn,8;8;8,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,8,0.0,yes,9/25/19,"Shanghai Jiao Tong University;National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology;National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology;National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology;National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology;Shanghai Jiao Tong University;Shanghai Jiao Tong University",53;-1;-1;-1;-1;53;53,157;-1;-1;-1;-1;157;157,3;7
3259,3259,3259,3259,3259,3259,3259,3259,ICLR,2020,Sign Bits Are All You Need for Black-Box Attacks,Abdullah Al-Dujaili;Una-May O'Reilly,ash.aldujaili@gmail.com;unamay@csail.mit.edu,6;6;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0.0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2,5;5,4
3260,3260,3260,3260,3260,3260,3260,3260,ICLR,2020,Biologically inspired sleep algorithm for increased generalization and adversarial robustness in deep neural networks,Timothy Tadros;Giri Krishnan;Ramyaa Ramyaa;Maxim Bazhenov,tttadros@ucsd.edu;gkrishnan@ucsd.edu;ramyaa.ramyaa@gmail.com;mbazhenov@ucsd.edu,8;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Accept (Poster),0,3,0.0,yes,9/25/19,"University of California, San Diego;University of California, San Diego;;University of California, San Diego",11;11;-1;11,31;31;-1;31,4;8
3261,3261,3261,3261,3261,3261,3261,3261,ICLR,2020,SNODE: Spectral Discretization of Neural ODEs for System Identification,Alessio Quaglino;Marco Gallieri;Jonathan Masci;Jan Koutník,alessio@nnaisense.com;marco@nnaisense.com;jonathan@nnaisense.com;jan@nnaisense.com,8;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,7,0.0,yes,9/25/19,NNAISENSE;NNAISENSE;NNAISENSE;NNAISENSE,-1;-1;-1;-1,-1;-1;-1;-1,8
3262,3262,3262,3262,3262,3262,3262,3262,ICLR,2020,Query2box: Reasoning over Knowledge Graphs in Vector Space Using Box Embeddings,Hongyu Ren*;Weihua Hu*;Jure Leskovec,hyren@cs.stanford.edu;weihuahu@stanford.edu;jure@cs.stanford.edu,8;6;6,I do not know much about this area.:I carefully checked the experiments.:I made a quick assessment of this paper.:N/A;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,Stanford University;Stanford University;Stanford University,4;4;4,4;4;4,1;10
3263,3263,3263,3263,3263,3263,3263,3263,ICLR,2020,Theory and Evaluation Metrics for Learning Disentangled Representations,Kien Do;Truyen Tran,dkdo@deakin.edu.au;truyen.tran@deakin.edu.au,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,Deakin University;Deakin University,481;481,332;332,5
3264,3264,3264,3264,3264,3264,3264,3264,ICLR,2020,Learning to Plan in High Dimensions via Neural Exploration-Exploitation Trees,Binghong Chen;Bo Dai;Qinjie Lin;Guo Ye;Han Liu;Le Song,binghong@gatech.edu;bodai@google.com;qinjielin2018@u.northwestern.edu;guoye2018@u.northwestern.edu;hanliu@northwestern.edu;lsong@cc.gatech.edu,8;6;8,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,3,0.0,yes,9/25/19,Georgia Institute of Technology;Google;Northwestern University;Northwestern University;Northwestern University;Georgia Institute of Technology,13;-1;44;44;44;13,38;-1;22;22;22;38,
3265,3265,3265,3265,3265,3265,3265,3265,ICLR,2020,And the Bit Goes Down: Revisiting the Quantization of Neural Networks,Pierre Stock;Armand Joulin;Rémi Gribonval;Benjamin Graham;Hervé Jégou,pstock@fb.com;ajoulin@fb.com;remi.gribonval@inria.fr;benjamingraham@fb.com;rvj@fb.com,6;8;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,5,3.0,yes,9/25/19,Facebook;Facebook;INRIA;Facebook;Facebook,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,
3266,3266,3266,3266,3266,3266,3266,3266,ICLR,2020,Continual Learning with Adaptive Weights (CLAW),Tameem Adel;Han Zhao;Richard E. Turner,tah47@cam.ac.uk;han.zhao@cs.cmu.edu;ret26@cam.ac.uk,3;8;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,1.0,yes,9/25/19,University of Cambridge;Carnegie Mellon University;University of Cambridge,71;1;71,3;27;3,6
3267,3267,3267,3267,3267,3267,3267,3267,ICLR,2020,Depth-Adaptive Transformer,Maha Elbayad;Jiatao Gu;Edouard Grave;Michael Auli,maha.elbayad@inria.fr;thomagram@gmail.com;egrave@fb.com;michael.auli@gmail.com,6;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Poster),0,7,1.0,yes,9/25/19,INRIA;Facebook;Facebook;Facebook,-1;-1;-1;-1,-1;-1;-1;-1,
3268,3268,3268,3268,3268,3268,3268,3268,ICLR,2020,Robust Local Features for Improving the Generalization of Adversarial Training,Chuanbiao Song;Kun He;Jiadong Lin;Liwei Wang;John E. Hopcroft,cbsong@hust.edu.cn;brooklet60@hust.edu.cn;jdlin@hust.edu.cn;wanglw@cis.pku.edu.cn;jeh@cs.cornell.edu,6;3;8,I do not know much about this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,7,0.0,yes,9/25/19,Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Peking University;Cornell University,39;39;39;22;7,47;47;47;24;19,4;8
3269,3269,3269,3269,3269,3269,3269,3269,ICLR,2020,Learning deep graph matching with channel-independent embedding and Hungarian attention,Tianshu Yu;Runzhong Wang;Junchi Yan;Baoxin Li,tianshuy@asu.edu;runzhong.wang@sjtu.edu.cn;yanjunchi@sjtu.edu.cn;baoxin.li@asu.edu,3;6;6,I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,3,0.0,yes,9/25/19,Arizona State University;Shanghai Jiao Tong University;Shanghai Jiao Tong University;Arizona State University,95;53;53;95,155;157;157;155,10
3270,3270,3270,3270,3270,3270,3270,3270,ICLR,2020,Large Batch Optimization for Deep Learning: Training BERT in 76 minutes,Yang You;Jing Li;Sashank Reddi;Jonathan Hseu;Sanjiv Kumar;Srinadh Bhojanapalli;Xiaodan Song;James Demmel;Kurt Keutzer;Cho-Jui Hsieh,youyang@cs.berkeley.edu;jingli@google.com;sashank@google.com;jhseu@google.com;sanjivk@google.com;bsrinadh@google.com;xiaodansong@google.com;demmel@berkeley.edu;keutzer@berkeley.edu;chohsieh@cs.ucla.edu,3;8;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,3,0.0,yes,9/25/19,"University of California Berkeley;Google;Google;Google;Google;Google;Google;University of California Berkeley;University of California Berkeley;University of California, Los Angeles",5;-1;-1;-1;-1;-1;-1;5;5;20,13;-1;-1;-1;-1;-1;-1;13;13;17,9
3271,3271,3271,3271,3271,3271,3271,3271,ICLR,2020,Reanalysis of Variance Reduced Temporal Difference Learning,Tengyu Xu;Zhe Wang;Yi Zhou;Yingbin Liang,xu.3260@osu.edu;wang.10982@osu.edu;yi.zhou@utah.edu;liang.889@osu.edu,3;8;8;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0.0,yes,9/25/19,Ohio State University;Ohio State University;University of Utah;Ohio State University,77;77;233;77,373;373;366;373,9
3272,3272,3272,3272,3272,3272,3272,3272,ICLR,2020,Unsupervised Model Selection for Variational Disentangled Representation Learning,Sunny Duan;Loic Matthey;Andre Saraiva;Nick Watters;Chris Burgess;Alexander Lerchner;Irina Higgins,sunnyd@google.com;lmatthey@google.com;andresnds@google.com;nwatters@google.com;cpburgess@google.com;lerchner@google.com;irinah@google.com,6;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),2,14,0.0,yes,9/25/19,Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,5;7
3273,3273,3273,3273,3273,3273,3273,3273,ICLR,2020,On the Weaknesses of Reinforcement Learning for Neural Machine Translation,Leshem Choshen;Lior Fox;Zohar Aizenbud;Omri Abend,leshem.choshen@mail.huji.ac.il;lior.fox@mail.huji.ac.il;zohar.aizenbud@mail.huji.ac.il;oabend@cs.huji.ac.il,6;3;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0.0,yes,9/25/19,Hebrew University of Jerusalem;Hebrew University of Jerusalem;Hebrew University of Jerusalem;Hebrew University of Jerusalem,67;67;67;67,216;216;216;216,3;4;5;1
3274,3274,3274,3274,3274,3274,3274,3274,ICLR,2020,Training binary neural networks with real-to-binary convolutions,Brais Martinez;Jing Yang;Adrian Bulat;Georgios Tzimiropoulos,brais.mart@gmail.com;psxjy3@nottingham.ac.uk;adrian@adrianbulat.com;yorgos.tzimiropoulos@nottingham.ac.uk,6;6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,8,0.0,yes,9/25/19,Imperial College London;The University of Nottingham;Samsung;The University of Nottingham,73;233;-1;233,10;152;-1;152,
3275,3275,3275,3275,3275,3275,3275,3275,ICLR,2020,Mixed-curvature Variational Autoencoders,Ondrej Skopek;Octavian-Eugen Ganea;Gary Bécigneul,oskopek@oskopek.com;oct@mit.edu;garyb@mit.edu,8;8;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,6,0.0,yes,9/25/19,Google;Massachusetts Institute of Technology;Massachusetts Institute of Technology,-1;2;2,-1;5;5,5
3276,3276,3276,3276,3276,3276,3276,3276,ICLR,2020,Program Guided Agent,Shao-Hua Sun;Te-Lin Wu;Joseph J. Lim,shaohuas@usc.edu;telinwu@usc.edu;limjj@usc.edu,6;8;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,16,0.0,yes,9/25/19,University of Southern California;University of Southern California;University of Southern California,31;31;31,62;62;62,3;6;8
3277,3277,3277,3277,3277,3277,3277,3277,ICLR,2020,Revisiting Self-Training for Neural Sequence Generation,Junxian He;Jiatao Gu;Jiajun Shen;Marc'Aurelio Ranzato,junxianh@cs.cmu.edu;thomagram@gmail.com;jiajunshen@fb.com;ranzato@fb.com,8;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:N/A:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,Carnegie Mellon University;Facebook;Facebook;Facebook,1;-1;-1;-1,27;-1;-1;-1,3
3278,3278,3278,3278,3278,3278,3278,3278,ICLR,2020,"Ridge Regression: Structure, Cross-Validation, and Sketching",Sifan Liu;Edgar Dobriban,sfliu@stanford.edu;dobribanedgar@gmail.com,6;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,2,0.0,yes,9/25/19,Stanford University;University of Pennsylvania,4;19,4;11,
3279,3279,3279,3279,3279,3279,3279,3279,ICLR,2020,Deep Network Classification by Scattering and Homotopy Dictionary Learning,John Zarka;Louis Thiry;Tomas Angles;Stephane Mallat,john.zarka@ens.fr;louis.thiry@ens.fr;tomas.angles@ens.fr;stephane.mallat@ens.fr,8;6;8,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,Ecole Normale Superieure;Ecole Normale Superieure;Ecole Normale Superieure;Ecole Normale Superieure,100;100;100;100,45;45;45;45,1
3280,3280,3280,3280,3280,3280,3280,3280,ICLR,2020,FreeLB: Enhanced Adversarial Training for Natural Language Understanding,Chen Zhu;Yu Cheng;Zhe Gan;Siqi Sun;Tom Goldstein;Jingjing Liu,chenzhu@cs.umd.edu;yu.cheng@microsoft.com;zhe.gan@microsoft.com;siqi.sun@microsoft.com;tomg@cs.umd.edu;jingjl@microsoft.com,8;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,3,0.0,yes,9/25/19,"University of Maryland, College Park;Microsoft;Microsoft;Microsoft;University of Maryland, College Park;Microsoft",12;-1;-1;-1;12;-1,91;-1;-1;-1;91;-1,3;4;8
3281,3281,3281,3281,3281,3281,3281,3281,ICLR,2020,Reinforcement Learning with Competitive  Ensembles of Information-Constrained Primitives,Anirudh Goyal;Shagun Sodhani;Jonathan Binas;Xue Bin Peng;Sergey Levine;Yoshua Bengio,anirudhgoyal9119@gmail.com;sshagunsodhani@gmail.com;jbinas@gmail.com;xbpeng@berkeley.edu;svlevine@eecs.berkeley.edu;yoshua.bengio@mila.quebec,8;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,7,0.0,yes,9/25/19,University of Montreal;University of Montreal;University of Montreal;University of California Berkeley;University of California Berkeley;University of Montreal,128;128;128;5;5;128,85;85;85;13;13;85,8
3282,3282,3282,3282,3282,3282,3282,3282,ICLR,2020,Adversarially Robust Representations with Smooth Encoders,Taylan Cemgil;Sumedh Ghaisas;Krishnamurthy (Dj) Dvijotham;Pushmeet Kohli,taylancemgil@google.com;sumedhg@google.com;dvij@google.com;pushmeet@google.com,6;3;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,5;4;1
3283,3283,3283,3283,3283,3283,3283,3283,ICLR,2020,Scalable Neural Methods for Reasoning With a Symbolic Knowledge   Base,William W. Cohen;Haitian Sun;R. Alex Hofer;Matthew Siegler,wcohen@google.com;haitiansun@google.com;rofer@google.com;msiegler@google.com,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:N/A:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,
3284,3284,3284,3284,3284,3284,3284,3284,ICLR,2020,Curriculum Loss: Robust Learning and Generalization  against Label Corruption,Yueming Lyu;Ivor W. Tsang,lv_yueming@outlook.com;ivor.tsang@uts.edu.au,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,University of Technology Sydney;University of Technology Sydney,108;108,193;193,1;8
3285,3285,3285,3285,3285,3285,3285,3285,ICLR,2020,Low-dimensional statistical manifold embedding of directed graphs,Thorben Funke;Tian Guo;Alen Lancic;Nino Antulov-Fantulin,fun@biba.uni-bremen.de;tian.guo0980@gmail.com;alen.lancic@math.hr;nino.antulov@gess.ethz.ch,8;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,Universität Bremen;;;Swiss Federal Institute of Technology,154;-1;-1;10,360;-1;-1;13,10
3286,3286,3286,3286,3286,3286,3286,3286,ICLR,2020,VideoFlow: A Conditional Flow-Based Model for Stochastic Video Generation,Manoj Kumar;Mohammad Babaeizadeh;Dumitru Erhan;Chelsea Finn;Sergey Levine;Laurent Dinh;Durk Kingma,manojkumarsivaraj334@gmail.com;mb2@uiuc.edu;dumitru@google.com;cbfinn@eecs.berkeley.edu;slevine@google.com;laurentdinh@google.com;d.p.kingma@uva.nl,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0.0,yes,9/25/19,";University of Illinois, Urbana-Champaign;Google;University of California Berkeley;Google;Google;University of Amsterdam",-1;3;-1;5;-1;-1;172,-1;48;-1;13;-1;-1;62,5
3287,3287,3287,3287,3287,3287,3287,3287,ICLR,2020,Self-Adversarial Learning with Comparative Discrimination for Text Generation,Wangchunshu Zhou;Tao Ge;Ke Xu;Furu Wei;Ming Zhou,v-waz@microsoft.com;tage@microsoft.com;kexu@nlsde.buaa.edu.cn;fuwei@microsoft.com;mingzhou@microsoft.com,8;3;8,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,6,0.0,yes,9/25/19,Microsoft;Microsoft;Beihang University;Microsoft;Microsoft,-1;-1;118;-1;-1,-1;-1;594;-1;-1,5;4
3288,3288,3288,3288,3288,3288,3288,3288,ICLR,2020,Deep neuroethology of a virtual rodent,Josh Merel;Diego Aldarondo;Jesse Marshall;Yuval Tassa;Greg Wayne;Bence Olveczky,jsmerel@google.com;diegoaldarondo@g.harvard.edu;jesse_d_marshall@fas.harvard.edu;tassa@google.com;gregwayne@google.com;olveczky@fas.harvard.edu,6;8;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:N/A:N/A:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Spotlight),0,4,0.0,yes,9/25/19,Google;Harvard University;Harvard University;Google;Google;Harvard University,-1;39;39;-1;-1;39,-1;7;7;-1;-1;7,
3289,3289,3289,3289,3289,3289,3289,3289,ICLR,2020,On the Global Convergence  of Training Deep Linear ResNets,Difan Zou;Philip M. Long;Quanquan Gu,knowzou@ucla.edu;plong@google.com;qgu@cs.ucla.edu,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,9,0.0,yes,9/25/19,"University of California, Los Angeles;Google;University of California, Los Angeles",20;-1;20,17;-1;17,1;9
3290,3290,3290,3290,3290,3290,3290,3290,ICLR,2020,Measuring and Improving the Use of Graph Information in Graph Neural Networks,Yifan Hou;Jian Zhang;James Cheng;Kaili Ma;Richard T. B. Ma;Hongzhi Chen;Ming-Chang Yang,yfhou@cse.cuhk.edu.hk;jzhang@cse.cuhk.edu.hk;jcheng@cse.cuhk.edu.hk;klma@cse.cuhk.edu.hk;tbma@comp.nus.edu.sg;hzchen@cse.cuhk.edu.hk;mcyang@cse.cuhk.edu.hk,8;3;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0.0,yes,9/25/19,The Chinese University of Hong Kong;The Chinese University of Hong Kong;The Chinese University of Hong Kong;The Chinese University of Hong Kong;National University of Singapore;The Chinese University of Hong Kong;The Chinese University of Hong Kong,59;59;59;59;16;59;59,35;35;35;35;25;35;35,10
3291,3291,3291,3291,3291,3291,3291,3291,ICLR,2020,AssembleNet: Searching for Multi-Stream Neural Connectivity in Video Architectures,Michael S. Ryoo;AJ Piergiovanni;Mingxing Tan;Anelia Angelova,mryoo@google.com;ajpiergi@indiana.edu;tanmingxing@google.com;anelia@google.com,8;8;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Poster),0,4,1.0,yes,9/25/19,Google;University of Arizona;Google;Google,-1;172;-1;-1,-1;103;-1;-1,
3292,3292,3292,3292,3292,3292,3292,3292,ICLR,2020,Identity Crisis: Memorization and Generalization Under Extreme Overparameterization,Chiyuan Zhang;Samy Bengio;Moritz Hardt;Michael C. Mozer;Yoram Singer,pluskid@gmail.com;bengio@google.com;moritzhardt@gmail.com;mcmozer@google.com;y.s@cs.princeton.edu,8;6;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,Massachusetts Institute of Technology;Google;;Google;Princeton University,2;-1;-1;-1;31,5;-1;-1;-1;6,8
3293,3293,3293,3293,3293,3293,3293,3293,ICLR,2020,Spike-based causal inference for weight alignment,Jordan Guerguiev;Konrad Kording;Blake Richards,jordan.guerguiev@utoronto.ca;koerding@gmail.com;blake.richards@mcgill.ca,6;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,Toronto University;University of Pennsylvania;McGill University,18;19;86,18;11;42,
3294,3294,3294,3294,3294,3294,3294,3294,ICLR,2020,VariBAD: A Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning,Luisa Zintgraf;Kyriacos Shiarlis;Maximilian Igl;Sebastian Schulze;Yarin Gal;Katja Hofmann;Shimon Whiteson,luisa.zintgraf@cs.ox.ac.uk;kikos1988@gmail.com;maximilian.igl@gmail.com;sebastian.schulze@eng.ox.ac.uk;yarin.gal@cs.ox.ac.uk;katja.hofmann@microsoft.com;shimon.whiteson@cs.ox.ac.uk,6;8;1;8,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,7,0.0,yes,9/25/19,University of Oxford;;University of Oxford;University of Oxford;University of Oxford;Microsoft;University of Oxford,50;-1;50;50;50;-1;50,1;-1;1;1;1;-1;1,11;6
3295,3295,3295,3295,3295,3295,3295,3295,ICLR,2020,Expected Information Maximization: Using the I-Projection for Mixture Density Estimation,Philipp Becker;Oleg Arenz;Gerhard Neumann,philippbecker93@googlemail.com;oleg@robot-learning.de;geri@robot-learning.de,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,TU Darmstadt;TU Darmstadt;University of Lincoln,64;64;481,289;289;617,5;4;1
3296,3296,3296,3296,3296,3296,3296,3296,ICLR,2020,Fast Task Inference with Variational Intrinsic Successor Features,Steven Hansen;Will Dabney;Andre Barreto;David Warde-Farley;Tom Van de Wiele;Volodymyr Mnih,stevenhansen@google.com;wdabney@google.com;andrebarreto@google.com;dwf@google.com;tvdwiele@gmail.com;vmnih@google.com,6;8;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,4,0.0,yes,9/25/19,Google;Google;Google;Google;;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,8
3297,3297,3297,3297,3297,3297,3297,3297,ICLR,2020,Kernel of CycleGAN as a principal homogeneous space,Nikita Moriakov;Jonas Adler;Jonas Teuwen,nikita.moriakov@radboudumc.nl;jonasadl@kth.se;jonas.teuwen@radboudumc.nl,3;6;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,3,1.0,yes,9/25/19,"Radboud University Medical Center;KTH Royal Institute of Technology, Stockholm, Sweden;Radboud University Medical Center",390;128;390,128;222;128,4
3298,3298,3298,3298,3298,3298,3298,3298,ICLR,2020,"Generative Models for Effective ML on Private, Decentralized Datasets",Sean Augenstein;H. Brendan McMahan;Daniel Ramage;Swaroop Ramaswamy;Peter Kairouz;Mingqing Chen;Rajiv Mathews;Blaise Aguera y Arcas,saugenst@google.com;mcmahan@google.com;dramage@google.com;swaroopram@google.com;kairouz@google.com;mingqing@google.com;mathews@google.com;blaisea@google.com,6;8;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1,5
3299,3299,3299,3299,3299,3299,3299,3299,ICLR,2020,Gradient-Based Neural DAG Learning,Sébastien Lachapelle;Philippe Brouillard;Tristan Deleu;Simon Lacoste-Julien,sebastien.lachapelle@umontreal.ca;philippebrouillard@gmail.com;tristan.deleu@gmail.com;slacoste@iro.umontreal.ca,6;6;8,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,5,0.0,yes,9/25/19,University of Montreal;University of Montreal;University of Montreal;University of Montreal,128;128;128;128,85;85;85;85,10
3300,3300,3300,3300,3300,3300,3300,3300,ICLR,2020,DeepSphere: a graph-based spherical CNN,Michaël Defferrard;Martino Milani;Frédérick Gusset;Nathanaël Perraudin,michael.defferrard@epfl.ch;martino.milani@epfl.ch;frederick.gusset@epfl.ch;nathanael.perraudin@sdsc.ethz.ch,6;6;8,I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,4,0.0,yes,9/25/19,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology,481;481;481;10,38;38;38;13,10
3301,3301,3301,3301,3301,3301,3301,3301,ICLR,2020,IMPACT: Importance Weighted Asynchronous Architectures with Clipped Target Networks,Michael Luo;Jiahao Yao;Richard Liaw;Eric Liang;Ion Stoica,michael.luo@berkeley.edu;jiahaoyao@berkeley.edu;rliaw@berkeley.edu;ekhliang@gmail.com;istoica@berkeley.edu,6;6;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0.0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5;5,13;13;13;13;13,
3302,3302,3302,3302,3302,3302,3302,3302,ICLR,2020,Improving Neural Language Generation with Spectrum Control,Lingxiao Wang;Jing Huang;Kevin Huang;Ziniu Hu;Guangtao Wang;Quanquan Gu,lingxw@cs.ucla.edu;jing.huang@jd.com;kevin.huang3@jd.com;bull@cs.ucla.edu;guangtao.wang@jd.com;qgu@cs.ucla.edu,6;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,"University of California, Los Angeles;JD AI Research;JD AI Research;University of California, Los Angeles;JD AI Research;University of California, Los Angeles",20;-1;-1;20;-1;20,17;-1;-1;17;-1;17,3
3303,3303,3303,3303,3303,3303,3303,3303,ICLR,2020,Harnessing the Power of Infinitely Wide Deep Nets on Small-data Tasks,Sanjeev Arora;Simon S. Du;Zhiyuan Li;Ruslan Salakhutdinov;Ruosong Wang;Dingli Yu,arora@cs.princeton.edu;ssdu@ias.edu;zhiyuanli@cs.princeton.edu;rsalakhu@cs.cmu.edu;ruosongw@andrew.cmu.edu;dingliy@cs.princeton.edu,8;6;8,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Spotlight),0,4,0.0,yes,9/25/19,"Princeton University;Institue for Advanced Study, Princeton;Princeton University;Carnegie Mellon University;Carnegie Mellon University;Princeton University",31;-1;31;1;1;31,6;-1;6;27;27;6,6
3304,3304,3304,3304,3304,3304,3304,3304,ICLR,2020,NAS evaluation is frustratingly hard,Antoine Yang;Pedro M. Esperança;Fabio M. Carlucci,antoineyang3@gmail.com;pedro.esperanca@huawei.com;fabiom.carlucci@gmail.com,8;1;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Poster),0,4,0.0,yes,9/25/19,ENS Paris-Saclay;Huawei Technologies Ltd.;Huawei Technologies Ltd.,481;-1;-1,644;-1;-1,1
3305,3305,3305,3305,3305,3305,3305,3305,ICLR,2020,Understanding and Robustifying Differentiable Architecture Search,Arber Zela;Thomas Elsken;Tonmoy Saikia;Yassine Marrakchi;Thomas Brox;Frank Hutter,zelaa@cs.uni-freiburg.de;thomas.elsken@de.bosch.com;saikiat@cs.uni-freiburg.de;marrakch@cs.uni-freiburg.de;brox@cs.uni-freiburg.de;fh@cs.uni-freiburg.de,8;8;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Talk),1,5,0.0,yes,9/25/19,Universität Freiburg;Bosch;Universität Freiburg;Universität Freiburg;Universität Freiburg;Universität Freiburg,118;-1;118;118;118;118,85;-1;85;85;85;85,3;8
3306,3306,3306,3306,3306,3306,3306,3306,ICLR,2020,Meta-Q-Learning,Rasool Fakoor;Pratik Chaudhari;Stefano Soatto;Alexander J. Smola,rasool.fakoor@mavs.uta.edu;pratikac@seas.upenn.edu;soatto@cs.ucla.edu;alex@smola.org,6;8;8,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,10,0.0,yes,9/25/19,"University of Texas, Arlington;University of Pennsylvania;University of California, Los Angeles;Carnegie-Mellon University",118;19;20;1,708;11;17;27,
3307,3307,3307,3307,3307,3307,3307,3307,ICLR,2020,Budgeted Training: Rethinking Deep Neural Network Training Under Resource Constraints,Mengtian Li;Ersin Yumer;Deva Ramanan,mtli@cs.cmu.edu;meyumer@gmail.com;deva@cs.cmu.edu,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,1.0,yes,9/25/19,Carnegie Mellon University;Uber;Carnegie Mellon University,1;-1;1,27;-1;27,2
3308,3308,3308,3308,3308,3308,3308,3308,ICLR,2020,Training individually fair ML models with sensitive subspace robustness,Mikhail Yurochkin;Amanda Bower;Yuekai Sun,mikhail.yurochkin@ibm.com;amandarg@umich.edu;yuekai@umich.edu,6;6;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,4,0.0,yes,9/25/19,International Business Machines;University of Michigan;University of Michigan,-1;8;8,-1;21;21,4;7
3309,3309,3309,3309,3309,3309,3309,3309,ICLR,2020,Deformable Kernels: Adapting Effective Receptive Fields for Object Deformation,Hang Gao;Xizhou Zhu;Stephen Lin;Jifeng Dai,hangg@berkeley.edu;ezra0408@mail.ustc.edu.cn;stevelin@microsoft.com;jifdai@microsoft.com,6;6;6,I do not know much about this area.:N/A:N/A:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:N/A:N/A,Accept (Poster),0,4,0.0,yes,9/25/19,University of California Berkeley;University of Science and Technology of China;Microsoft;Microsoft,5;481;-1;-1,13;80;-1;-1,
3310,3310,3310,3310,3310,3310,3310,3310,ICLR,2020,From Variational to Deterministic Autoencoders,Partha Ghosh;Mehdi S. M. Sajjadi;Antonio Vergari;Michael Black;Bernhard Scholkopf,partha.ghosh@tuebingen.mpg.de;msajjadi@tue.mpg.de;antonio.vergari@tuebingen.mpg.de;black@tue.mpg.de;bs@tue.mpg.de,6;8;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,11,0.0,yes,9/25/19,"Max Planck Institute for Intelligent Systems, Max-Planck Institute;Max Planck Institute for Intelligent Systems, Max-Planck Institute;Max Planck Institute for Intelligent Systems, Max-Planck Institute;Max Planck Institute for Intelligent Systems, Max-Planck Institute;Max Planck Institute for Intelligent Systems, Max-Planck Institute",-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,5
3311,3311,3311,3311,3311,3311,3311,3311,ICLR,2020,GraphAF: a Flow-based Autoregressive Model for Molecular Graph Generation,Chence Shi*;Minkai Xu*;Zhaocheng Zhu;Weinan Zhang;Ming Zhang;Jian Tang,chenceshi@pku.edu.cn;mkxu@apex.sjtu.edu.cn;zhaocheng.zhu@umontreal.ca;wnzhang@sjtu.edu.cn;mzhang_cs@pku.edu.cn;jian.tang@hec.ca,6;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,17,0.0,yes,9/25/19,Peking University;Shanghai Jiao Tong University;University of Montreal;Shanghai Jiao Tong University;Peking University;HEC Montreal,22;53;128;53;22;128,24;157;85;157;24;85,5;10
3312,3312,3312,3312,3312,3312,3312,3312,ICLR,2020,A Theoretical Analysis of the Number of Shots in Few-Shot Learning,Tianshi Cao;Marc T Law;Sanja Fidler,tianshi.cao@mail.utoronto.ca;law@cs.toronto.edu;fidler@cs.toronto.edu,8;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,"Toronto University;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",18;18;18,18;18;18,6
3313,3313,3313,3313,3313,3313,3313,3313,ICLR,2020,Deep Learning of Determinantal Point Processes via Proper Spectral Sub-gradient,Tianshu Yu;Yikang Li;Baoxin Li,tianshuy@asu.edu;yikang.li@asu.edu;baoxin.li@asu.edu,3;8;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,Arizona State University;Arizona State University;Arizona State University,95;95;95,155;155;155,2
3314,3314,3314,3314,3314,3314,3314,3314,ICLR,2020,Augmenting Genetic Algorithms with Deep Neural Networks for Exploring the Chemical Space,AkshatKumar Nigam;Pascal Friederich;Mario Krenn;Alan Aspuru-Guzik,akshat.nigam@mail.utoronto.ca;pascal.friederich@utoronto.ca;mario.krenn@utoronto.ca;alan@aspuru.com,8;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,7,0.0,yes,9/25/19,Toronto University;Toronto University;Toronto University;Toronto University,18;18;18;18,18;18;18;18,5
3315,3315,3315,3315,3315,3315,3315,3315,ICLR,2020,Denoising and Regularization via Exploiting the Structural Bias of Convolutional Generators,Reinhard Heckel;Mahdi Soltanolkotabi,reinhard.heckel@tum.de;msoltoon@gmail.com,6;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,5,0.0,yes,9/25/19,Technical University Munich;University of Southern California,53;31,43;62,1
3316,3316,3316,3316,3316,3316,3316,3316,ICLR,2020,SNOW: Subscribing to Knowledge via Channel Pooling for Transfer & Lifelong Learning of Convolutional Neural Networks,Chungkuk Yoo;Bumsoo Kang;Minsik Cho,ckyoo@ibm.com;steve.kang@kaist.ac.kr;thyeros@gmail.com,8;8;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,6,0.0,yes,9/25/19,International Business Machines;Korea Advanced Institute of Science and Technology;,-1;481;-1,-1;110;-1,
3317,3317,3317,3317,3317,3317,3317,3317,ICLR,2020,A Meta-Transfer Objective for Learning to Disentangle Causal Mechanisms,Yoshua Bengio;Tristan Deleu;Nasim Rahaman;Nan Rosemary Ke;Sebastien Lachapelle;Olexa Bilaniuk;Anirudh Goyal;Christopher Pal,yoshua.bengio@mila.quebec;tristan.deleu@gmail.com;nasim.rahaman@tuebingen.mpg.de;rosemary.nan.ke@gmail.com;sebastien.lachapelle@umontreal.ca;obilaniu@gmail.com;anirudhgoyal9119@gmail.com;chris.j.pal@gmail.com,8;8;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,8,0.0,yes,9/25/19,"University of Montreal;University of Montreal;Max Planck Institute for Intelligent Systems, Max-Planck Institute;Polytechnique Montreal;University of Montreal;University of Montreal;University of Montreal;Ecole Polytechnique de Montreal",128;128;-1;390;128;128;128;390,85;85;-1;1397;85;85;85;1397,10;1;6
3318,3318,3318,3318,3318,3318,3318,3318,ICLR,2020,Classification-Based Anomaly Detection for General Data,Liron Bergman;Yedid Hoshen,liron.bergman@mail.huji.ac.il;yedid@cs.huji.ac.il,6;8;8,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Accept (Poster),0,3,1.0,yes,9/25/19,Hebrew University of Jerusalem;Hebrew University of Jerusalem,67;67,216;216,8
3319,3319,3319,3319,3319,3319,3319,3319,ICLR,2020,B-Spline CNNs on Lie groups,Erik J Bekkers,e.j.bekkers@tue.nl,8;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:N/A:N/A:I did not assess the derivations or theory.,Accept (Poster),0,6,0.0,yes,9/25/19,Eindhoven University of Technology,205,185,
3320,3320,3320,3320,3320,3320,3320,3320,ICLR,2020,Global Relational Models of Source Code,Vincent J. Hellendoorn;Charles Sutton;Rishabh Singh;Petros Maniatis;David Bieber,vjhellendoorn@gmail.com;charlessutton@google.com;rising@google.com;maniatis@google.com;dbieber@google.com,6;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),1,8,0.0,yes,9/25/19,"University of California, Davis;Google;Google;Google;Google",79;-1;-1;-1;-1,55;-1;-1;-1;-1,10
3321,3321,3321,3321,3321,3321,3321,3321,ICLR,2020,NeurQuRI: Neural Question Requirement Inspector for Answerability Prediction in Machine Reading Comprehension,Seohyun Back;Sai Chetan Chinthakindi;Akhil Kedia;Haejun Lee;Jaegul Choo,scv.back@samsung.com;sai.chetan@samsung.com;akhil.kedia@samsung.com;haejun82.lee@samsung.com;jchoo@korea.ac.kr,6;8;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,6,0.0,yes,9/25/19,Samsung;Samsung;Samsung;Samsung;Korea University,-1;-1;-1;-1;323,-1;-1;-1;-1;179,
3322,3322,3322,3322,3322,3322,3322,3322,ICLR,2020,Adjustable Real-time Style Transfer,Mohammad Babaeizadeh;Golnaz Ghiasi,mb2@uiuc.edu;golnazg@google.com,6;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,"University of Illinois, Urbana-Champaign;Google",3;-1,48;-1,
3323,3323,3323,3323,3323,3323,3323,3323,ICLR,2020,Ranking Policy Gradient,Kaixiang Lin;Jiayu Zhou,linkaixi@msu.edu;jiayuz@msu.edu,6;3;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY,481;481,299;299,1
3324,3324,3324,3324,3324,3324,3324,3324,ICLR,2020,Neural Epitome Search for Architecture-Agnostic Network Compression,Daquan Zhou;Xiaojie Jin;Qibin Hou;Kaixin Wang;Jianchao Yang;Jiashi Feng,zhoudaquan21@gmail.com;jinxiaojie@bytedance.com;andrewhoux@gmail.com;kaixin.wang@u.nus.edu;yangjianchao@bytedance.com;elefjia@nus.edu.sg,6;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0.0,yes,9/25/19,National University of Singapore;Bytedance;National University of Singapore;National University of Singapore;Bytedance;National University of Singapore,16;-1;16;16;-1;16,25;-1;25;25;-1;25,
3325,3325,3325,3325,3325,3325,3325,3325,ICLR,2020,Learning to Guide Random Search,Ozan Sener;Vladlen Koltun,ozansener@gmail.com;vkoltun@gmail.com,6;8;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,Intel;Intel,-1;-1,-1;-1,11
3326,3326,3326,3326,3326,3326,3326,3326,ICLR,2020,BERTScore: Evaluating Text Generation with BERT,Tianyi Zhang*;Varsha Kishore*;Felix Wu*;Kilian Q. Weinberger;Yoav Artzi,zty27x@gmail.com;vk352@cornell.edu;fw245@cornell.edu;kqw4@cornell.edu;yoav@cs.cornell.edu,8;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,Cornell University;Cornell University;Cornell University;Cornell University;Cornell University,7;7;7;7;7,19;19;19;19;19,3;4
3327,3327,3327,3327,3327,3327,3327,3327,ICLR,2020,Reinforced Genetic Algorithm Learning for Optimizing Computation Graphs,Aditya Paliwal;Felix Gimeno;Vinod Nair;Yujia Li;Miles Lubin;Pushmeet Kohli;Oriol Vinyals,adipal@google.com;fgimeno@google.com;vinair@google.com;yujiali@google.com;mlubin@google.com;pushmeet@google.com;vinyals@google.com,6;6;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,10
3328,3328,3328,3328,3328,3328,3328,3328,ICLR,2020,Meta-Learning Acquisition Functions for Transfer Learning in Bayesian Optimization,Michael Volpp;Lukas P. Fröhlich;Kirsten Fischer;Andreas Doerr;Stefan Falkner;Frank Hutter;Christian Daniel,mvolpp89@googlemail.com;lukas.froehlich@de.bosch.com;k.fischer-lotte@online.de;andreas.doerr3@de.bosch.com;stefan.falkner@de.bosch.com;fh@cs.uni-freiburg.de;christian.daniel@de.bosch.com,8;8;6,I have published one or two papers in this area.:I carefully checked the experiments.:N/A:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Spotlight),0,5,0.0,yes,9/25/19,Bosch;Bosch;RWTH Aachen University;Bosch;Bosch;Universität Freiburg;Bosch,-1;-1;95;-1;-1;118;-1,-1;-1;98;-1;-1;85;-1,11;6;8
3329,3329,3329,3329,3329,3329,3329,3329,ICLR,2020,Adaptive Correlated Monte Carlo for Contextual Categorical Sequence Generation,Xinjie Fan;Yizhe Zhang;Zhendong Wang;Mingyuan Zhou,xfan@utexas.edu;yizhe.zhang@microsoft.com;zw2533@columbia.edu;mingyuan.zhou@mccombs.utexas.edu,6;8;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,"University of Texas, Austin;Microsoft;Columbia University;University of Texas, Austin",22;-1;15;22,38;-1;16;38,
3330,3330,3330,3330,3330,3330,3330,3330,ICLR,2020,Fooling Detection Alone is Not Enough: Adversarial Attack against Multiple Object Tracking,Yunhan Jia;Yantao Lu;Junjie Shen;Qi Alfred Chen;Hao Chen;Zhenyu Zhong;Tao Wei,jack0082010@gmail.com;ylu25@syr.edu;junjies1@uci.edu;alfchen@uci.edu;chen@ucdavis.edu;edwardzhong@baidu.com;lenx.wei@gmail.com,6;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,3,0.0,yes,9/25/19,"University of Michigan;Syracuse University;University of California, Irvine;University of California, Irvine;University of California, Davis;Baidu;",8;233;35;35;79;-1;-1,21;292;96;96;55;-1;-1,4;2
3331,3331,3331,3331,3331,3331,3331,3331,ICLR,2020,Pre-training Tasks for Embedding-based Large-scale Retrieval,Wei-Cheng Chang;Felix X. Yu;Yin-Wen Chang;Yiming Yang;Sanjiv Kumar,wchang2@cs.cmu.edu;felixyu@google.com;yinwen@google.com;yiming@cs.cmu.edu;sanjivk@google.com,6;6;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,8,0.0,yes,9/25/19,Carnegie Mellon University;Google;Google;Carnegie Mellon University;Google,1;-1;-1;1;-1,27;-1;-1;27;-1,
3332,3332,3332,3332,3332,3332,3332,3332,ICLR,2020,MACER: Attack-free and Scalable Robust Training via Maximizing Certified Radius,Runtian Zhai;Chen Dan;Di He;Huan Zhang;Boqing Gong;Pradeep Ravikumar;Cho-Jui Hsieh;Liwei Wang,zhairuntian@pku.edu.cn;cdan@cs.cmu.edu;dihe@microsoft.com;huan@huan-zhang.com;boqinggo@outlook.com;pradeepr@cs.cmu.edu;chohsieh@cs.ucla.edu;wanglw@cis.pku.edu.cn,8;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0.0,yes,9/25/19,"Peking University;Carnegie Mellon University;Microsoft;University of California, Los Angeles;International Computer Science Institute;Carnegie Mellon University;University of California, Los Angeles;Peking University",22;1;-1;20;-1;1;20;22,24;27;-1;17;-1;27;17;24,4
3333,3333,3333,3333,3333,3333,3333,3333,ICLR,2020,Rényi Fair Inference,Sina Baharlouei;Maher Nouiehed;Ahmad Beirami;Meisam Razaviyayn,baharlou@usc.edu;nouiehed@usc.edu;beirami@mit.edu;razaviya@usc.edu,8;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,University of Southern California;University of Southern California;Massachusetts Institute of Technology;University of Southern California,31;31;2;31,62;62;5;62,4;7
3334,3334,3334,3334,3334,3334,3334,3334,ICLR,2020,An Exponential Learning Rate Schedule for Deep Learning,Zhiyuan Li;Sanjeev Arora,zhiyuanli@cs.princeton.edu;arora@cs.princeton.edu,6;8;8;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Spotlight),0,4,0.0,yes,9/25/19,Princeton University;Princeton University,31;31,6;6,1;8
3335,3335,3335,3335,3335,3335,3335,3335,ICLR,2020,Graph Convolutional Reinforcement Learning,Jiechuan Jiang;Chen Dun;Tiejun Huang;Zongqing Lu,jiechuan.jiang@pku.edu.cn;cd46@rice.edu;tjhuang@pku.edu.cn;zongqing.lu@pku.edu.cn,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Poster),1,12,0.0,yes,9/25/19,Peking University;Rice University;Peking University;Peking University,22;84;22;22,24;105;24;24,10
3336,3336,3336,3336,3336,3336,3336,3336,ICLR,2020,In Search for a SAT-friendly Binarized Neural Network Architecture,Nina Narodytska;Hongce Zhang;Aarti Gupta;Toby Walsh,n.narodytska@gmail.com;hongcez@princeton.edu;aartig@cs.princeton.edu;toby.walsh@data61.csiro.au,8;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,6,0.0,yes,9/25/19,";Princeton University;Princeton University;, CSIRO",-1;31;31;233,-1;6;6;-1,
3337,3337,3337,3337,3337,3337,3337,3337,ICLR,2020,A Constructive Prediction of the Generalization Error Across Scales,Jonathan S. Rosenfeld;Amir Rosenfeld;Yonatan Belinkov;Nir Shavit,jonsr@mit.edu;amir@eecs.yorku.ca;belinkov@mit.edu;shanir@csail.mit.edu,6;8;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,5,0.0,yes,9/25/19,Massachusetts Institute of Technology;York University;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;172;2;2,5;416;5;5,8
3338,3338,3338,3338,3338,3338,3338,3338,ICLR,2020,Improving Generalization in Meta Reinforcement Learning using Learned Objectives,Louis Kirsch;Sjoerd van Steenkiste;Juergen Schmidhuber,louis@idsia.ch;sjoerd@idsia.ch;juergen@idsia.ch,6;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:N/A:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Spotlight),0,7,1.0,yes,9/25/19,IDSIA;IDSIA;IDSIA,-1;-1;-1,-1;-1;-1,
3339,3339,3339,3339,3339,3339,3339,3339,ICLR,2020,Exploratory Not Explanatory: Counterfactual Analysis of Saliency Maps for Deep Reinforcement Learning,Akanksha Atrey;Kaleigh Clary;David Jensen,aatrey@cs.umass.edu;kclary@cs.umass.edu;jensen@cs.umass.edu,8;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,8,0.0,yes,9/25/19,"University of Massachusetts, Amherst;University of Massachusetts, Amherst;University of Massachusetts, Amherst",28;28;28,209;209;209,
3340,3340,3340,3340,3340,3340,3340,3340,ICLR,2020,A Generalized Training Approach for Multiagent Learning,Paul Muller;Shayegan Omidshafiei;Mark Rowland;Karl Tuyls;Julien Perolat;Siqi Liu;Daniel Hennes;Luke Marris;Marc Lanctot;Edward Hughes;Zhe Wang;Guy Lever;Nicolas Heess;Thore Graepel;Remi Munos,pmuller@google.com;somidshafiei@google.com;markrowland@google.com;karltuyls@google.com;perolat@google.com;liusiqi@google.com;hennes@google.com;marris@google.com;lanctot@google.com;edwardhughes@google.com;zhewang@google.com;guylever@google.com;heess@google.com;thore@google.com;munos@google.com,8;8;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,10,0.0,yes,9/25/19,Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,
3341,3341,3341,3341,3341,3341,3341,3341,ICLR,2020,Scalable Model Compression by Entropy Penalized Reparameterization,Deniz Oktay;Johannes Ballé;Saurabh Singh;Abhinav Shrivastava,doktay@princeton.edu;jballe@google.com;saurabhsingh@google.com;abhinav@cs.umd.edu,6;8;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,"Princeton University;Google;Google;University of Maryland, College Park",31;-1;-1;12,6;-1;-1;91,
3342,3342,3342,3342,3342,3342,3342,3342,ICLR,2020,Rotation-invariant clustering of neuronal responses in primary visual cortex,Ivan Ustyuzhaninov;Santiago A. Cadena;Emmanouil Froudarakis;Paul G. Fahey;Edgar Y. Walker;Erick Cobos;Jacob Reimer;Fabian H. Sinz;Andreas S. Tolias;Matthias Bethge;Alexander S. Ecker,ivan.ustyuzhaninov@bethgelab.org;santiago.cadena@bethgelab.org;froudara@bcm.edu;paul.fahey@bcm.edu;eywalker@bcm.edu;ecobos@bcm.edu;reimer@bcm.edu;fabian.sinz@bcm.edu;astolias@bcm.edu;matthias@bethgelab.org;alexander.ecker@uni-tuebingen.de,8;8;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,7,0.0,yes,9/25/19,"Centre for Integrative Neuroscience, AG Bethge;Centre for Integrative Neuroscience, AG Bethge;Baylor College of Medicine;Baylor College of Medicine;Baylor College of Medicine;Baylor College of Medicine;Baylor College of Medicine;Baylor College of Medicine;Baylor College of Medicine;Centre for Integrative Neuroscience, AG Bethge;University of Tuebingen",-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;154,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;91,5
3343,3343,3343,3343,3343,3343,3343,3343,ICLR,2020,Ensemble Distribution Distillation,Andrey Malinin;Bruno Mlodozeniec;Mark Gales,am969@yandex-team.ru;bkm28@cam.ac.uk;mjfg@eng.cam.ac.uk,6;6;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,5,0.0,yes,9/25/19,Yandex;University of Cambridge;University of Cambridge,-1;71;71,-1;3;3,
3344,3344,3344,3344,3344,3344,3344,3344,ICLR,2020,ES-MAML: Simple Hessian-Free Meta Learning,Xingyou Song;Wenbo Gao;Yuxiang Yang;Krzysztof Choromanski;Aldo Pacchiano;Yunhao Tang,xsong@berkeley.edu;wg2279@columbia.edu;yxyang@google.com;kchoro@google.com;pacchiano@berkeley.edu;yt2541@columbia.edu,8;8;1;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,7,0.0,yes,9/25/19,University of California Berkeley;Columbia University;Google;Google;University of California Berkeley;Columbia University,5;15;-1;-1;5;15,13;16;-1;-1;13;16,
3345,3345,3345,3345,3345,3345,3345,3345,ICLR,2020,SAdam: A Variant of Adam for Strongly Convex Functions,Guanghui Wang;Shiyin Lu;Quan Cheng;Wei-wei Tu;Lijun Zhang,guhuwang@gmail.com;lsy1116@qq.com;chengquangm@gmail.com;tuwwcn@gmail.com;zljzju@gmail.com,8;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,8,0.0,yes,9/25/19,Zhejiang University;Zhejiang University;Zhejiang University;;Zhejiang University,56;56;56;-1;56,107;107;107;-1;107,1
3346,3346,3346,3346,3346,3346,3346,3346,ICLR,2020,Continual Learning with Bayesian Neural Networks for Non-Stationary Data,Richard Kurle;Botond Cseke;Alexej Klushyn;Patrick van der Smagt;Stephan Günnemann,richard.kurle@tum.de;botond.cseke@argmax.ai;a.klushyn@tum.de;smagt@argmax.ai;guennemann@in.tum.de,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,7,0.0,yes,9/25/19,"Technical University Munich;Volkswagen Group, ML Research Lab;Technical University Munich;Volkswagen Group, ML Research Lab;Technical University Munich",53;-1;53;-1;53,43;-1;43;-1;43,11
3347,3347,3347,3347,3347,3347,3347,3347,ICLR,2020,Automated curriculum generation through setter-solver interactions,Sebastien Racaniere;Andrew Lampinen;Adam Santoro;David Reichert;Vlad Firoiu;Timothy Lillicrap,lampinen@stanford.edu;sracaniere@google.com;adamsantoro@google.com;reichert@google.com;vladfi@google.com;countzero@google.com,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,Stanford University;Google;Google;Google;Google;Google,4;-1;-1;-1;-1;-1,4;-1;-1;-1;-1;-1,
3348,3348,3348,3348,3348,3348,3348,3348,ICLR,2020,NAS-Bench-201: Extending the Scope of Reproducible Neural Architecture Search,Xuanyi Dong;Yi Yang,xuanyi.dxy@gmail.com;yi.yang@uts.edu.au,8;8;8,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:N/A:N/A,Accept (Spotlight),0,14,2.0,yes,9/25/19,University of Technology Sydney;University of Technology Sydney,108;108,193;193,10
3349,3349,3349,3349,3349,3349,3349,3349,ICLR,2020,Dynamic Time Lag Regression: Predicting What & When,Mandar Chandorkar;Cyril Furtlehner;Bala Poduval;Enrico Camporeale;Michele Sebag,mandar.chandorkar@cwi.nl;furtlehn@lri.fr;bala.poduval@unh.edu;e.camporeale@cwi.nl;michele.sebag@lri.fr,6;6;6;8,I do not know much about this area.:I did not assess the experiments.:N/A:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,"Centrum voor Wiskunde en Informatica;CNRS, Université Paris-Saclay;University of New Hampshire;Centrum voor Wiskunde en Informatica;CNRS, Université Paris-Saclay",-1;-1;266;-1;-1,-1;-1;1397;-1;-1,11;1
3350,3350,3350,3350,3350,3350,3350,3350,ICLR,2020,Double Neural Counterfactual Regret Minimization,Hui Li;Kailiang Hu;Shaohua Zhang;Yuan Qi;Le Song,ken.lh@antfin.com;hkl163251@antfin.com;yaohua.zsh@antfin.com;yuan.qi@antfin.com;lsong@cc.gatech.edu,6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,2,0.0,yes,9/25/19,Alibaba Group and Ant Financial Services Group;Antfin;Antfin;Antfin;Georgia Institute of Technology,-1;-1;-1;-1;13,-1;-1;-1;-1;38,
3351,3351,3351,3351,3351,3351,3351,3351,ICLR,2020,Robust anomaly detection and backdoor attack detection via differential privacy,Min Du;Ruoxi Jia;Dawn Song,min.du@berkeley.edu;ruoxijia@berkeley.edu;dawnsong@berkeley.edu,6;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,13;13;13,4
3352,3352,3352,3352,3352,3352,3352,3352,ICLR,2020,Optimistic Exploration even with a Pessimistic Initialisation,Tabish Rashid;Bei Peng;Wendelin Boehmer;Shimon Whiteson,tabish.rashid@cs.ox.ac.uk;bei.peng@cs.ox.ac.uk;wendelin.boehmer@cs.ox.ac.uk;shimon.whiteson@cs.ox.ac.uk,6;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,7,0.0,yes,9/25/19,University of Oxford;University of Oxford;University of Oxford;University of Oxford,50;50;50;50,1;1;1;1,
3353,3353,3353,3353,3353,3353,3353,3353,ICLR,2020,Training Generative Adversarial Networks from Incomplete Observations using Factorised Discriminators,Daniel Stoller;Sebastian Ewert;Simon Dixon,d.stoller@qmul.ac.uk;sewert@spotify.com;s.e.dixon@qmul.ac.uk,6;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,Queen Mary University London;Queen Mary University London;Queen Mary University London,233;233;233,110;110;110,5;4;2
3354,3354,3354,3354,3354,3354,3354,3354,ICLR,2020,Mathematical Reasoning in Latent Space,Dennis Lee;Christian Szegedy;Markus Rabe;Sarah Loos;Kshitij Bansal,ldennis@google.com;szegedy@google.com;mrabe@google.com;smoos@google.com;kbk@google.com,8;8;8,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,7,0.0,yes,9/25/19,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,1;10
3355,3355,3355,3355,3355,3355,3355,3355,ICLR,2020,Deep Symbolic Superoptimization Without Human Knowledge,Hui Shi;Yang Zhang;Xinyun Chen;Yuandong Tian;Jishen Zhao,hshi@ucsd.edu;yang.zhang2@ibm.com;xinyun.chen@berkeley.edu;yuandong@fb.com;jzhao@ucsd.edu,6;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0.0,yes,9/25/19,"University of California, San Diego;International Business Machines;University of California Berkeley;Facebook;University of California, San Diego",11;-1;5;-1;11,31;-1;13;-1;31,
3356,3356,3356,3356,3356,3356,3356,3356,ICLR,2020,Neural Execution of Graph Algorithms,Petar Veličković;Rex Ying;Matilde Padovano;Raia Hadsell;Charles Blundell,petarv@google.com;rexying@stanford.edu;mp861@cam.ac.uk;raia@google.com;cblundell@google.com,8;8;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,4,0.0,yes,9/25/19,Google;Stanford University;University of Cambridge;Google;Google,-1;4;71;-1;-1,-1;4;3;-1;-1,10
3357,3357,3357,3357,3357,3357,3357,3357,ICLR,2020,Encoding word order in complex embeddings,Benyou Wang;Donghao Zhao;Christina Lioma;Qiuchi Li;Peng Zhang;Jakob Grue Simonsen,wang@dei.unipd.it;zhaodh@tju.edu.cn;chrh@di.ku.dk;qiuchili@dei.unipd.it;pzhang@tju.edu.cn;simonsen@di.ku.dk,6;8;6;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,9,0.0,yes,9/25/19,Universita' degli studi di Padova;Zhejiang University;University of Copenhagen;Universita' degli studi di Padova;Zhejiang University;University of Copenhagen,-1;56;100;-1;56;100,-1;107;101;-1;107;101,3
3358,3358,3358,3358,3358,3358,3358,3358,ICLR,2020,Deep Double Descent: Where Bigger Models and More Data Hurt,Preetum Nakkiran;Gal Kaplun;Yamini Bansal;Tristan Yang;Boaz Barak;Ilya Sutskever,preetum@cs.harvard.edu;galkaplun@g.harvard.edu;ybansal@g.harvard.edu;tristanyang@college.harvard.edu;b@boazbarak.org;ilyasu@openai.com,8;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,4,1.0,yes,9/25/19,Harvard University;Harvard University;Harvard University;Harvard University;Harvard University;OpenAI,39;39;39;39;39;-1,7;7;7;7;7;-1,
3359,3359,3359,3359,3359,3359,3359,3359,ICLR,2020,The Early Phase of Neural Network Training,Jonathan Frankle;David J. Schwab;Ari S. Morcos,jfrankle@mit.edu;dschwab@gc.cuny.edu;arimorcos@gmail.com,8;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,Massachusetts Institute of Technology;The City College of New York;Facebook,2;205;-1,5;1397;-1,
3360,3360,3360,3360,3360,3360,3360,3360,ICLR,2020,"Don't Use Large Mini-batches, Use Local SGD",Tao Lin;Sebastian U. Stich;Kumar Kshitij Patel;Martin Jaggi,tao.lin@epfl.ch;sebastian.stich@epfl.ch;kumarkshitijpatel@gmail.com;martin.jaggi@epfl.ch,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),7,4,0.0,yes,9/25/19,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Toyota Technological Institute at Chicago;Swiss Federal Institute of Technology Lausanne,481;481;-1;481,38;38;-1;38,8
3361,3361,3361,3361,3361,3361,3361,3361,ICLR,2020,A Closer Look at the Optimization Landscapes of Generative Adversarial Networks,Hugo Berard;Gauthier Gidel;Amjad Almahairi;Pascal Vincent;Simon Lacoste-Julien,berard.hugo@gmail.com;gauthier.gidel@umontreal.ca;amjadmahayri@gmail.com;vincentp@iro.umontreal.ca;slacoste@iro.umontreal.ca,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,7,0.0,yes,9/25/19,University of Montreal;University of Montreal;Element AI;University of Montreal;University of Montreal,128;128;-1;128;128,85;85;-1;85;85,5;4;9
3362,3362,3362,3362,3362,3362,3362,3362,ICLR,2020,Learning to Control PDEs with Differentiable Physics,Philipp Holl;Nils Thuerey;Vladlen Koltun,philipp.holl@tum.de;nils.thuerey@tum.de;vkoltun@gmail.com,6;8;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,6,0.0,yes,9/25/19,Technical University Munich;Technical University Munich;Intel,53;53;-1,43;43;-1,
3363,3363,3363,3363,3363,3363,3363,3363,ICLR,2020,Compression based bound for non-compressed network: unified generalization error analysis of large compressible deep neural network,Taiji Suzuki;Hiroshi Abe;Tomoaki Nishimura,taiji@mist.i.u-tokyo.ac.jp;abe@ipride.co.jp;tomoaki.nishimura@nttdata.com,8;6;6,I have published in this field for several years.:N/A:N/A:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,8,0.0,yes,9/25/19,The University of Tokyo;;Nttdata,56;-1;-1,36;-1;-1,1;8
3364,3364,3364,3364,3364,3364,3364,3364,ICLR,2020,Rethinking the Hyperparameters for Fine-tuning,Hao Li;Pratik Chaudhari;Hao Yang;Michael Lam;Avinash Ravichandran;Rahul Bhotika;Stefano Soatto,hao.li.ict@gmail.com;pratikac@seas.upenn.edu;lancelot365@gmail.com;michlam@amazon.com;avinash.a.ravichandran@gmail.com;bhotikar@amazon.com;soatto@ucla.edu,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0.0,yes,9/25/19,"Amazon;University of Pennsylvania;Amazon;Amazon;Amazon;Amazon;University of California, Los Angeles",-1;19;-1;-1;-1;-1;20,-1;11;-1;-1;-1;-1;17,6;2
3365,3365,3365,3365,3365,3365,3365,3365,ICLR,2020,PAC Confidence Sets for Deep Neural Networks via Calibrated Prediction,Sangdon Park;Osbert Bastani;Nikolai Matni;Insup Lee,sangdonp@cis.upenn.edu;obastani@seas.upenn.edu;nmatni@seas.upenn.edu;lee@cis.upenn.edu,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0.0,yes,9/25/19,University of Pennsylvania;University of Pennsylvania;University of Pennsylvania;University of Pennsylvania,19;19;19;19,11;11;11;11,8
3366,3366,3366,3366,3366,3366,3366,3366,ICLR,2020,Deep Semi-Supervised Anomaly Detection,Lukas Ruff;Robert A. Vandermeulen;Nico Görnitz;Alexander Binder;Emmanuel Müller;Klaus-Robert Müller;Marius Kloft,contact@lukasruff.com;vandermeulen@cs.uni-kl.de;nico.goernitz@tu-berlin.de;alexander_binder@sutd.edu.sg;mueller@bit.uni-bonn.de;klaus-robert.mueller@tu-berlin.de;kloft@cs.uni-kl.de,6;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,TU Berlin;TU Kaiserslautern;TU Berlin;Singapore University of Technology and Design;University of Bonn;TU Berlin;TU Kaiserslautern,108;154;108;481;128;108;154,149;601;149;1397;106;149;601,
3367,3367,3367,3367,3367,3367,3367,3367,ICLR,2020,AtomNAS: Fine-Grained End-to-End Neural Architecture Search,Jieru Mei;Yingwei Li;Xiaochen Lian;Xiaojie Jin;Linjie Yang;Alan Yuille;Jianchao Yang,meijieru@gmail.com;yingwei.li@jhu.edu;xiaochen.lian@bytedance.com;jinxiaojie@bytedance.com;linjie.yang@bytedance.com;alan.l.yuille@gmail.com;yangjianchao@bytedance.com,6;6;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,4,0.0,yes,9/25/19,Johns Hopkins University;Johns Hopkins University;Bytedance;Bytedance;Bytedance;Johns Hopkins University;Bytedance,73;73;-1;-1;-1;73;-1,12;12;-1;-1;-1;12;-1,
3368,3368,3368,3368,3368,3368,3368,3368,ICLR,2020,"Triple Wins: Boosting Accuracy, Robustness and Efficiency Together by Enabling Input-Adaptive Inference",Ting-Kuei Hu;Tianlong Chen;Haotao Wang;Zhangyang Wang,tkhu@tamu.edu;wiwjp619@tamu.edu;htwang@tamu.edu;atlaswang@tamu.edu,8;8;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Accept (Poster),0,4,0.0,yes,9/25/19,Texas A&M;Texas A&M;Texas A&M;Texas A&M,44;44;44;44,177;177;177;177,4
3369,3369,3369,3369,3369,3369,3369,3369,ICLR,2020,CLEVRER: Collision Events for Video Representation and Reasoning,Kexin Yi*;Chuang Gan*;Yunzhu Li;Pushmeet Kohli;Jiajun Wu;Antonio Torralba;Joshua B. Tenenbaum,kyi@g.harvard.edu;ganchuang1990@gmail.com;liyunzhu@mit.edu;pushmeet@google.com;jiajunwu@mit.edu;torralba@mit.edu;jbt@mit.edu,6;8;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Accept (Spotlight),0,4,0.0,yes,9/25/19,Harvard University;International Business Machines;Massachusetts Institute of Technology;Google;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,39;-1;2;-1;2;2;2,7;-1;5;-1;5;5;5,
3370,3370,3370,3370,3370,3370,3370,3370,ICLR,2020,Computation Reallocation for Object Detection,Feng Liang;Chen Lin;Ronghao Guo;Ming Sun;Wei Wu;Junjie Yan;Wanli Ouyang,liangfeng@sensetime.com;linchen@sensetime.com;guoronghao@sensetime.com;sunming1@sensetime.com;wuwei@sensetime.com;yanjunjie@sensetime.com;wanli.ouyang@sydney.edu.au,3;6;6;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0.0,yes,9/25/19,SenseTime Group Limited;SenseTime Group Limited;SenseTime Group Limited;SenseTime Group Limited;SenseTime Group Limited;SenseTime Group Limited;University of Sydney,-1;-1;-1;-1;-1;-1;86,-1;-1;-1;-1;-1;-1;60,2
3371,3371,3371,3371,3371,3371,3371,3371,ICLR,2020,Making Efficient Use of Demonstrations to Solve Hard Exploration Problems,Caglar Gulcehre;Tom Le Paine;Bobak Shahriari;Misha Denil;Matt Hoffman;Hubert Soyer;Richard Tanburn;Steven Kapturowski;Neil Rabinowitz;Duncan Williams;Gabriel Barth-Maron;Ziyu Wang;Nando de Freitas;Worlds Team,caglarg@google.com;tpaine@google.com;bshahr@google.com;mdenil@google.com;mwhoffman@google.com;soyer@google.com;tanburn@google.com;skapturowski@google.com;ncr@google.com;duncanwilliams@google.com;gabrielbm@google.com;ziyu@google.com;nandodefreitas@google.com;deepmind-worlds-team@google.com,6;6;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,4,0.0,yes,9/25/19,Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,
3372,3372,3372,3372,3372,3372,3372,3372,ICLR,2020,Robust And Interpretable Blind Image Denoising Via Bias-Free Convolutional Neural Networks,Sreyas Mohan;Zahra Kadkhodaie;Eero P. Simoncelli;Carlos Fernandez-Granda,sm7582@nyu.edu;zk388@nyu.edu;eero.simoncelli@nyu.edu;cfgranda@cims.nyu.edu,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,4,1.0,yes,9/25/19,New York University;New York University;New York University;New York University,25;25;25;25,29;29;29;29,8
3373,3373,3373,3373,3373,3373,3373,3373,ICLR,2020,Permutation Equivariant Models for Compositional Generalization in Language,Jonathan Gordon;David Lopez-Paz;Marco Baroni;Diane Bouchacourt,jg801@cam.ac.uk;dlp@fb.com;mbaroni@fb.com;dianeb@fb.com,8;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0.0,yes,9/25/19,University of Cambridge;Facebook;Facebook;Facebook,71;-1;-1;-1,3;-1;-1;-1,3;8
3374,3374,3374,3374,3374,3374,3374,3374,ICLR,2020,Automated Relational Meta-learning,Huaxiu Yao;Xian Wu;Zhiqiang Tao;Yaliang Li;Bolin Ding;Ruirui Li;Zhenhui Li,huaxiuyao@psu.edu;xwu9@nd.edu;zqtao@ece.neu.edu;yaliangl.ub@gmail.com;bolin.ding@alibaba-inc.com;rrli@cs.ucla.edu;jessieli@ist.psu.edu,8;8;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,5,0.0,yes,9/25/19,"Pennsylvania State University;University of Notre Dame;Northeastern University;;Alibaba Group;University of California, Los Angeles;Pennsylvania State University",41;118;16;-1;-1;20;41,78;157;906;-1;-1;17;78,6;10
3375,3375,3375,3375,3375,3375,3375,3375,ICLR,2020,A Probabilistic Formulation of Unsupervised Text Style Transfer,Junxian He;Xinyi Wang;Graham Neubig;Taylor Berg-Kirkpatrick,junxianh@cs.cmu.edu;xinyiw1@cs.cmu.edu;gneubig@cs.cmu.edu;tberg@eng.ucsd.edu,6;6;8,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A,Accept (Spotlight),0,4,0.0,yes,9/25/19,"Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;University of California, San Diego",1;1;1;11,27;27;27;31,3;4;5
3376,3376,3376,3376,3376,3376,3376,3376,ICLR,2020,Discrepancy Ratio: Evaluating Model Performance When Even Experts Disagree on the Truth,Igor Lovchinsky;Alon Daks;Israel Malkin;Pouya Samangouei;Ardavan Saeedi;Yang Liu;Swami Sankaranarayanan;Tomer Gafner;Ben Sternlieb;Patrick Maher;Nathan Silberman,ilovchinsky@butterflynetwork.com;adaks@butterflynetwork.com;imalkin@butterflynetwork.com;psamangouei@butterflynetwork.com;asaeedi@butterflynetwork.com;yliu@butterflynetwork.com;ssankaranarayanan@butterflynetwork.com;tgafner@butterflynetwork.com;bsternlieb@butterflynetwork.com;pmaher@butterflynetwork.com;nsilberman@butterflynetwork.com,8;6;6,I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0.0,yes,9/25/19,Butterfly Network Inc;Butterfly Network Inc;Butterfly Network Inc;Butterfly Network Inc;Butterfly Network Inc;Butterfly Network Inc;Butterfly Network Inc;Butterfly Network Inc;Butterfly Network Inc;Butterfly Network Inc;Butterfly Network Inc,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,
3377,3377,3377,3377,3377,3377,3377,3377,ICLR,2020,Combining Q-Learning and Search with Amortized Value Estimates,Jessica B. Hamrick;Victor Bapst;Alvaro Sanchez-Gonzalez;Tobias Pfaff;Theophane Weber;Lars Buesing;Peter W. Battaglia,jhamrick@google.com;vbapst@google.com;alvarosg@google.com;tpfaff@google.com;theophane@google.com;lbuesing@google.com;peterbattaglia@google.com,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,10,0.0,yes,9/25/19,Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,
3378,3378,3378,3378,3378,3378,3378,3378,ICLR,2020,Generalized Convolutional Forest Networks for Domain Generalization and Visual Recognition,Jongbin Ryu;Gitaek Kwon;Ming-Hsuan Yang;Jongwoo Lim,jongbin.ryu@gmail.com;kwongitack@gmail.com;mhyang@ucmerced.edu;jlim@hanyang.ac.kr,6;3;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,Hanyang University;Hanyang University;University of California at Merced;Hanyang University,233;233;481;233,393;393;354;393,8
3379,3379,3379,3379,3379,3379,3379,3379,ICLR,2020,Graph Constrained Reinforcement Learning for Natural Language Action Spaces,Prithviraj Ammanabrolu;Matthew Hausknecht,raj.ammanabrolu@gatech.edu;matthew.hausknecht@microsoft.com,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,3,0.0,yes,9/25/19,Georgia Institute of Technology;Microsoft,13;-1,38;-1,3;10
3380,3380,3380,3380,3380,3380,3380,3380,ICLR,2020,Improved memory in recurrent neural networks with sequential non-normal dynamics,Emin Orhan;Xaq Pitkow,aeminorhan@gmail.com;xaq@rice.edu,6;8;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0.0,yes,9/25/19,New York University;Rice University,25;84,29;105,
3381,3381,3381,3381,3381,3381,3381,3381,ICLR,2020,A Learning-based Iterative Method for Solving Vehicle Routing Problems,Hao Lu;Xingwen Zhang;Shuang Yang,haolu@princeton.edu;xingwen.zhang@antfin.com;shuang.yang@antfin.com,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:N/A:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0.0,yes,9/25/19,Princeton University;Antfin;Antfin,31;-1;-1,6;-1;-1,
3382,3382,3382,3382,3382,3382,3382,3382,ICLR,2020,Emergence of functional and structural properties of the head direction system by optimization of recurrent neural networks,Christopher J. Cueva;Peter Y. Wang;Matthew Chin;Xue-Xin Wei,ccueva@gmail.com;peterwang724@gmail.com;mattchin35@gmail.com;weixxpku@gmail.com,6;8;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Accept (Spotlight),0,4,0.0,yes,9/25/19,Columbia University;Columbia University;;Columbia University,15;15;-1;15,16;16;-1;16,
3383,3383,3383,3383,3383,3383,3383,3383,ICLR,2020,GenDICE: Generalized Offline Estimation of Stationary Values,Ruiyi Zhang*;Bo Dai*;Lihong Li;Dale Schuurmans,ryzhang@cs.duke.edu;bodai@google.com;lihongli.cs@gmail.com;schuurmans@google.com,8;8;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Talk),0,6,0.0,yes,9/25/19,Duke University;Google;Google;Google,47;-1;-1;-1,20;-1;-1;-1,1
3384,3384,3384,3384,3384,3384,3384,3384,ICLR,2020,Adversarially robust transfer learning,Ali Shafahi;Parsa Saadatpanah;Chen Zhu;Amin Ghiasi;Christoph Studer;David Jacobs;Tom Goldstein,ashafahi@cs.umd.edu;parsa@cs.umd.edu;chenzhu@cs.umd.edu;amin@cs.umd.edu;studer@cornell.edu;djacobs@cs.umd.edu;tomg@cs.umd.edu,8;8;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0.0,yes,9/25/19,"University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park;Cornell University;University of Maryland, College Park;University of Maryland, College Park",12;12;12;12;7;12;12,91;91;91;91;19;91;91,4;6;8
3385,3385,3385,3385,3385,3385,3385,3385,ICLR,2020,Learning from Explanations with Neural Execution Tree,Ziqi Wang*;Yujia Qin*;Wenxuan Zhou;Jun Yan;Qinyuan Ye;Leonardo Neves;Zhiyuan Liu;Xiang Ren,ziqi-wan16@mails.tsinghua.edu.cn;qinyj16@mails.tsinghua.edu.cn;zhouwenx@usc.edu;yanjun@usc.edu;qinyuany@usc.edu;lneves@snap.com;liuzy@tsinghua.edu.cn;xiangren@usc.edu,8;8;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,6,0.0,yes,9/25/19,Tsinghua University;Tsinghua University;University of Southern California;University of Southern California;University of Southern California;Snap Inc.;Tsinghua University;University of Southern California,8;8;31;31;31;-1;8;31,23;23;62;62;62;-1;23;62,3;8
3386,3386,3386,3386,3386,3386,3386,3386,ICLR,2020,Universal Approximation with Certified Networks,Maximilian Baader;Matthew Mirman;Martin Vechev,mbaader@inf.ethz.ch;matthew.mirman@inf.ethz.ch;martin.vechev@inf.ethz.ch,3;8;6,I have published in this field for several years.:N/A:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:N/A:N/A:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),1,7,0.0,yes,9/25/19,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10,13;13;13,4;1
3387,3387,3387,3387,3387,3387,3387,3387,ICLR,2020,Extreme Classification via Adversarial Softmax Approximation,Robert Bamler;Stephan Mandt,rbamler@uci.edu;stephan.mandt@gmail.com,6;3;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,"University of California, Irvine;University of California, Irvine",35;35,96;96,4;1
3388,3388,3388,3388,3388,3388,3388,3388,ICLR,2020,Synthesizing Programmatic Policies that Inductively Generalize,Jeevana Priya Inala;Osbert Bastani;Zenna Tavares;Armando Solar-Lezama,jinala@csail.mit.edu;obastani@seas.upenn.edu;zenna@mit.edu;asolar@csail.mit.edu,8;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,11,0.0,yes,9/25/19,Massachusetts Institute of Technology;University of Pennsylvania;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;19;2;2,5;11;5;5,8
3389,3389,3389,3389,3389,3389,3389,3389,ICLR,2020,Image-guided Neural Object Rendering,Justus Thies;Michael Zollhöfer;Christian Theobalt;Marc Stamminger;Matthias Nießner,justus.thies@tum.de;michael@zollhoefer.com;marc.stamminger@fau.de;theobalt@mpi-inf.mpg.de;niessner@tum.de,6;8;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,1,0.0,yes,9/25/19,"Technical University Munich;Facebook;University of Erlangen-Nuremberg;Saarland Informatics Campus, Max-Planck Institute;Technical University Munich",53;-1;205;-1;53,43;-1;182;-1;43,5
3390,3390,3390,3390,3390,3390,3390,3390,ICLR,2020,Is a Good Representation Sufficient for Sample Efficient Reinforcement Learning?,Simon S. Du;Sham M. Kakade;Ruosong Wang;Lin F. Yang,ssdu@ias.edu;sham@cs.washington.edu;ruosongw@andrew.cmu.edu;linyang@ee.ucla.edu,6;8;8,I have read many papers in this area.:N/A:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Spotlight),0,6,0.0,yes,9/25/19,"Institue for Advanced Study, Princeton;University of Washington;Carnegie Mellon University;University of California, Los Angeles",-1;6;1;20,-1;26;27;17,
3391,3391,3391,3391,3391,3391,3391,3391,ICLR,2020,Interpretable Complex-Valued Neural Networks for Privacy Protection,Liyao Xiang;Hao Zhang;Haotian Ma;Yifan Zhang;Jie Ren;Quanshi Zhang,xiangliyao08@sjtu.edu.cn;1603023-zh@sjtu.edu.cn;11612807@mail.sustc.edu.cn;zhangyf_sjtu@sjtu.edu.cn;ariesrj@sjtu.edu.cn;zqs1022@sjtu.edu.cn,6;6;6,I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,Shanghai Jiao Tong University;Shanghai Jiao Tong University;University of Science and Technology of China;Shanghai Jiao Tong University;Shanghai Jiao Tong University;Shanghai Jiao Tong University,53;53;481;53;53;53,157;157;80;157;157;157,4
3392,3392,3392,3392,3392,3392,3392,3392,ICLR,2020,The Shape of Data: Intrinsic Distance for Data Distributions,Anton Tsitsulin;Marina Munkhoeva;Davide Mottin;Panagiotis Karras;Alex Bronstein;Ivan Oseledets;Emmanuel Mueller,tsitsulin@bit.uni-bonn.de;marina.munkhoeva@skolkovotech.ru;davide@cs.au.dk;piekarras@gmail.com;bron@cs.technion.ac.il;i.oseledets@skoltech.ru;mueller@bit.uni-bonn.de,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),1,4,0.0,yes,9/25/19,University of Bonn;Skolkovo Institute of Science and Technology;Aarhus University;Aarhus University;Technion;Skolkovo Institute of Science and Technology;University of Bonn,128;-1;108;108;26;-1;128,106;-1;115;115;412;-1;106,5
3393,3393,3393,3393,3393,3393,3393,3393,ICLR,2020,Dynamics-Aware Embeddings,William Whitney;Rajat Agarwal;Kyunghyun Cho;Abhinav Gupta,wfwhitney@gmail.com;ra2630@nyu.edu;kyunghyun.cho@nyu.edu;abhinavg@cs.cmu.edu,8;8;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,8,0.0,yes,9/25/19,New York University;New York University;New York University;Carnegie Mellon University,25;25;25;1,29;29;29;27,
3394,3394,3394,3394,3394,3394,3394,3394,ICLR,2020,Extreme Tensoring for Low-Memory Preconditioning ,Xinyi Chen;Naman Agarwal;Elad Hazan;Cyril Zhang;Yi Zhang,xinyic@google.com;namanagarwal@google.com;ehazan@cs.princeton.edu;cyril.zhang@cs.princeton.edu;y.zhang@cs.princeton.edu,8;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,5,0.0,yes,9/25/19,Google;Google;Princeton University;Princeton University;Princeton University,-1;-1;31;31;31,-1;-1;6;6;6,3
3395,3395,3395,3395,3395,3395,3395,3395,ICLR,2020,Sequential Latent Knowledge Selection for Knowledge-Grounded Dialogue,Byeongchang Kim;Jaewoo Ahn;Gunhee Kim,byeongchang.kim@vision.snu.ac.kr;jaewoo.ahn@vision.snu.ac.kr;gunhee@snu.ac.kr,8;6;8,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,3,0.0,yes,9/25/19,Seoul National University;Seoul National University;Seoul National University,41;41;41,64;64;64,
3396,3396,3396,3396,3396,3396,3396,3396,ICLR,2020,Geom-GCN: Geometric Graph Convolutional Networks,Hongbin Pei;Bingzhe Wei;Kevin Chen-Chuan Chang;Yu Lei;Bo Yang,gspeihongbing@163.com;bwei6@illinois.edu;kcchang@illinois.edu;csylei@comp.polyu.edu.hk;ybo@jlu.edu.cn,6;8;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),2,6,2.0,yes,9/25/19,"Jilin University;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;The Hong Kong Polytechnic University;Jilin University",481;3;3;172;481,952;48;48;171;952,10
3397,3397,3397,3397,3397,3397,3397,3397,ICLR,2020,Compositional languages emerge in a neural iterated learning model,Yi Ren;Shangmin Guo;Matthieu Labeau;Shay B. Cohen;Simon Kirby,y.ren-18@sms.ed.ac.uk;s.guo-16@sms.ed.ac.uk;matthieu.labeau@gmail.com;scohen@inf.ed.ac.uk;simon.kirby@ed.ac.uk,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,17,0.0,yes,9/25/19,University of Edinburgh;University of Edinburgh;Télécom ParisTech;University of Edinburgh;University of Edinburgh,33;33;481;33;33,30;30;187;30;30,3;8
3398,3398,3398,3398,3398,3398,3398,3398,ICLR,2020,Knowledge Consistency between Neural Networks and Beyond,Ruofan Liang;Tianlin Li;Longfei Li;Jing Wang;Quanshi Zhang,nexuslrf@sjtu.edu.cn;litl@act.buaa.edu.cn;1776752575@sjtu.edu.cn;wangjing215@huawei.com;zqs1022@sjtu.edu.cn,8;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,Shanghai Jiao Tong University;Beihang University;Shanghai Jiao Tong University;Huawei Technologies Ltd.;Shanghai Jiao Tong University,53;118;53;-1;53,157;594;157;-1;157,
3399,3399,3399,3399,3399,3399,3399,3399,ICLR,2020,Differentially Private Meta-Learning,Jeffrey Li;Mikhail Khodak;Sebastian Caldas;Ameet Talwalkar,jwl3@andrew.cmu.edu;khodak@cs.cmu.edu;scaldas@cs.cmu.edu;talwalkar@cmu.edu,6;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Accept (Poster),0,4,0.0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1,27;27;27;27,3;6
3400,3400,3400,3400,3400,3400,3400,3400,ICLR,2020,Stochastic Conditional Generative Networks with Basis Decomposition,Ze Wang;Xiuyuan Cheng;Guillermo Sapiro;Qiang Qiu,ze.w@duke.edu;xiuyuan.cheng@duke.edu;guillermo.sapiro@duke.edu;qiang.qiu@duke.edu,6;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0.0,yes,9/25/19,Duke University;Duke University;Duke University;Duke University,47;47;47;47,20;20;20;20,5;4
3401,3401,3401,3401,3401,3401,3401,3401,ICLR,2020,Meta-Learning with Warped Gradient Descent,Sebastian Flennerhag;Andrei A. Rusu;Razvan Pascanu;Francesco Visin;Hujun Yin;Raia Hadsell,flennerhag@google.com;andreirusu@google.com;razp@google.com;visin@google.com;hujun.yin@manchester.ac.uk;raia@google.com,8;8;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Talk),2,3,0.0,yes,9/25/19,Google;Google;Google;Google;University of Manchester;Google,-1;-1;-1;-1;266;-1,-1;-1;-1;-1;55;-1,6
3402,3402,3402,3402,3402,3402,3402,3402,ICLR,2020,Neural Text Generation With Unlikelihood Training,Sean Welleck;Ilia Kulikov;Stephen Roller;Emily Dinan;Kyunghyun Cho;Jason Weston,wellecks@nyu.edu;kulikov@cs.nyu.edu;roller@fb.com;edinan@fb.com;kyunghyun.cho@nyu.edu;jase@fb.com,6;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,New York University;New York University;Facebook;Facebook;New York University;Facebook,25;25;-1;-1;25;-1,29;29;-1;-1;29;-1,3
3403,3403,3403,3403,3403,3403,3403,3403,ICLR,2020,Duration-of-Stay Storage Assignment under Uncertainty,Michael Lingzhi Li;Elliott Wolf;Daniel Wintz,mlli@mit.edu;ewolf@lineagelogistics.com;dwintz@lineagelogistics.com,6;3;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,16,0.0,yes,9/25/19,Massachusetts Institute of Technology;Lineagelogistics;Lineagelogistics,2;-1;-1,5;-1;-1,
3404,3404,3404,3404,3404,3404,3404,3404,ICLR,2020,Non-Autoregressive Dialog State Tracking,Hung Le;Richard Socher;Steven C.H. Hoi,l.hung1610@gmail.com;rsocher@salesforce.com;shoi@salesforce.com,6;1;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Poster),1,3,0.0,yes,9/25/19,Singapore Management University;SalesForce.com;SalesForce.com,92;-1;-1,1397;-1;-1,
3405,3405,3405,3405,3405,3405,3405,3405,ICLR,2020,Scalable and Order-robust Continual Learning with Additive Parameter Decomposition,Jaehong Yoon;Saehoon Kim;Eunho Yang;Sung Ju Hwang,jaehong.yoon@kaist.ac.kr;shkim@aitrics.com;eunhoy@kaist.ac.kr;sjhwang82@kaist.ac.kr,1;6;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Accept (Poster),0,16,0.0,yes,9/25/19,Korea Advanced Institute of Science and Technology;AITRICS;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,481;-1;481;481,110;-1;110;110,7
3406,3406,3406,3406,3406,3406,3406,3406,ICLR,2020,Automatically Discovering and Learning New Visual Categories with Ranking Statistics,Kai Han;Sylvestre-Alvise Rebuffi;Sebastien Ehrhardt;Andrea Vedaldi;Andrew Zisserman,khan@robots.ox.ac.uk;srebuffi@robots.ox.ac.uk;hyenal@robots.ox.ac.uk;vedaldi@robots.ox.ac.uk;az@robots.ox.ac.uk,3;6;6,I have published in this field for several years.:I carefully checked the experiments.:N/A:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,5,0.0,yes,9/25/19,University of Oxford;University of Oxford;University of Oxford;University of Oxford;University of Oxford,50;50;50;50;50,1;1;1;1;1,
3407,3407,3407,3407,3407,3407,3407,3407,ICLR,2020,On Identifiability in Transformers,Gino Brunner;Yang Liu;Damian Pascual;Oliver Richter;Massimiliano Ciaramita;Roger Wattenhofer,brunnegi@ethz.ch;liu.yang@alumni.ethz.ch;dpascual@ethz.ch;richtero@ethz.ch;massi@google.com;wattenhofer@ethz.ch,8;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,7,1.0,yes,9/25/19,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Google;Swiss Federal Institute of Technology,10;10;10;10;-1;10,13;13;13;13;-1;13,
3408,3408,3408,3408,3408,3408,3408,3408,ICLR,2020,Robust Reinforcement Learning for Continuous Control with Model Misspecification,Daniel J. Mankowitz;Nir Levine;Rae Jeong;Abbas Abdolmaleki;Jost Tobias Springenberg;Yuanyuan Shi;Jackie Kay;Todd Hester;Timothy Mann;Martin Riedmiller,dmankowitz@google.com;nirlevine@google.com;raejeong@google.com;aabdolmaleki@google.com;springenberg@google.com;yyshi@google.com;kayj@google.com;toddhester@google.com;timothymann@google.com;riedmiller@google.com,8;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,9,0.0,yes,9/25/19,Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,
3409,3409,3409,3409,3409,3409,3409,3409,ICLR,2020,Enhancing Adversarial Defense by k-Winners-Take-All,Chang Xiao;Peilin Zhong;Changxi Zheng,chang@cs.columbia.edu;pz2225@columbia.edu;cxz@cs.columbia.edu,8;8;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Spotlight),0,5,0.0,yes,9/25/19,Columbia University;Columbia University;Columbia University,15;15;15,16;16;16,4
3410,3410,3410,3410,3410,3410,3410,3410,ICLR,2020,Are Pre-trained Language Models Aware of Phrases? Simple but Strong Baselines for Grammar Induction,Taeuk Kim;Jihun Choi;Daniel Edmiston;Sang-goo Lee,taeuk@europa.snu.ac.kr;jhchoi@europa.snu.ac.kr;danedmiston@uchicago.edu;sglee@europa.snu.ac.kr,6;8;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Accept (Poster),0,6,0.0,yes,9/25/19,Seoul National University;Seoul National University;University of Chicago;Seoul National University,41;41;48;41,64;64;9;64,3
3411,3411,3411,3411,3411,3411,3411,3411,ICLR,2020,Neural Policy Gradient Methods: Global Optimality and Rates of Convergence,Lingxiao Wang;Qi Cai;Zhuoran Yang;Zhaoran Wang,lingxiaowang2022@u.northwestern.edu;qicai2022@u.northwestern.edu;zy6@princeton.edu;zhaoranwang@gmail.com,8;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),0,5,0.0,yes,9/25/19,Northwestern University;Northwestern University;Princeton University;Northwestern University,44;44;31;44,22;22;6;22,1;9
3412,3412,3412,3412,3412,3412,3412,3412,ICLR,2020,Enhancing Transformation-Based Defenses Against Adversarial Attacks with a Distribution Classifier,Connie Kou;Hwee Kuan Lee;Ee-Chien Chang;Teck Khim Ng,conniekoukl@gmail.com;leehk@bii.a-star.edu.sg;changec@comp.nus.edu.sg;ngtk@comp.nus.edu.sg,6;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,11,0.0,yes,9/25/19,National University of Singapore;A*STAR;National University of Singapore;National University of Singapore,16;-1;16;16,25;-1;25;25,4
3413,3413,3413,3413,3413,3413,3413,3413,ICLR,2020,Sample Efficient Policy Gradient Methods with Recursive Variance Reduction,Pan Xu;Felicia Gao;Quanquan Gu,panxu@cs.ucla.edu;fxgao1160@engineering.ucla.edu;qgu@cs.ucla.edu,6;8;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,5,0.0,yes,9/25/19,"University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles",20;20;20,17;17;17,9
3414,3414,3414,3414,3414,3414,3414,3414,ICLR,2020,ALBERT: A Lite BERT for Self-supervised Learning of Language Representations,Zhenzhong Lan;Mingda Chen;Sebastian Goodman;Kevin Gimpel;Piyush Sharma;Radu Soricut,lanzhzh@google.com;mchen@ttic.edu;seabass@google.com;kgimpel@ttic.edu;piyushsharma@google.com;rsoricut@google.com,6;8;8,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A,Accept (Spotlight),8,8,0.0,yes,9/25/19,Google;Toyota Technological Institute at Chicago;Google;Toyota Technological Institute at Chicago;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,3
3415,3415,3415,3415,3415,3415,3415,3415,ICLR,2020,State-only Imitation with Transition Dynamics Mismatch,Tanmay Gangwani;Jian Peng,gangwan2@illinois.edu;jianpeng@illinois.edu,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,7,0.0,yes,9/25/19,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",3;3,48;48,4
3416,3416,3416,3416,3416,3416,3416,3416,ICLR,2020,Watch the Unobserved: A Simple Approach to Parallelizing Monte Carlo Tree Search,Anji Liu;Jianshu Chen;Mingze Yu;Yu Zhai;Xuewen Zhou;Ji Liu,anjiliu219@gmail.com;chenjianshu@gmail.com;yumingze@kuaishou.com;zhaiyu@kuaishou.com;zhouxuewen@kuaishou.com;ji.liu.uwisc@gmail.com,6;8;8,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Talk),0,9,0.0,yes,9/25/19,"University of California, Los Angeles;Tencent AI Lab;Kuaishou;Kuaishou;Kuaishou;University of Rochester",20;-1;-1;-1;-1;100,17;-1;-1;-1;-1;173,
3417,3417,3417,3417,3417,3417,3417,3417,ICLR,2020,Gradientless Descent: High-Dimensional Zeroth-Order Optimization,Daniel Golovin;John Karro;Greg Kochanski;Chansoo Lee;Xingyou Song;Qiuyi Zhang,dgg@google.com;karro@google.com;gpk@google.com;chansoo@google.com;xingyousong@google.com;qiuyiz@google.com,6;6;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,4,0.0,yes,9/25/19,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,
3418,3418,3418,3418,3418,3418,3418,3418,ICLR,2020,Black-box Off-policy Estimation for Infinite-Horizon Reinforcement Learning,Ali Mousavi;Lihong Li;Qiang Liu;Denny Zhou,ali.mousavi1988@gmail.com;lihongli.cs@gmail.com;dennyzhou@google.com;lqiang@cs.utexas.edu,6;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0.0,yes,9/25/19,"Google;Google;Google;University of Texas, Austin",-1;-1;-1;22,-1;-1;-1;38,8
3419,3419,3419,3419,3419,3419,3419,3419,ICLR,2020,Hyper-SAGNN: a self-attention based graph neural network for hypergraphs,Ruochi Zhang;Yuesong Zou;Jian Ma,ruochiz@andrew.cmu.edu;logic.zys@gmail.com;jianma@cs.cmu.edu,8;8,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,1.0,yes,9/25/19,Carnegie Mellon University;Tsinghua University;Carnegie Mellon University,1;8;1,27;23;27,10
3420,3420,3420,3420,3420,3420,3420,3420,ICLR,2020,Unpaired Point Cloud Completion on Real Scans using Adversarial Training,Xuelin Chen;Baoquan Chen;Niloy J. Mitra,xuelin.chen.sdu@gmail.com;baoquan.chen@gmail.com;n.mitra@cs.ucl.ac.uk,6;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Poster),0,4,0.0,yes,9/25/19,Shandong University;Peking University;University College London,154;22;50,658;24;15,
3421,3421,3421,3421,3421,3421,3421,3421,ICLR,2020,The intriguing role of module criticality in the generalization of deep networks,Niladri Chatterji;Behnam Neyshabur;Hanie Sedghi,niladri.chatterji@berkeley.edu;neyshabur@google.com;hsedghi@google.com,8;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,5,0.0,yes,9/25/19,University of California Berkeley;Google;Google,5;-1;-1,13;-1;-1,8
3422,3422,3422,3422,3422,3422,3422,3422,ICLR,2020,SpikeGrad: An ANN-equivalent Computation Model for Implementing Backpropagation with Spikes,Johannes C. Thiele;Olivier Bichler;Antoine Dupret,johannes.thiele@cea.fr;olivier.bichler@cea.fr;antoine.dupret@cea.fr,6;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,7,0.0,yes,9/25/19,CEA;CEA;CEA,233;233;233,1027;1027;1027,
3423,3423,3423,3423,3423,3423,3423,3423,ICLR,2020,Physics-aware Difference Graph Networks for Sparsely-Observed Dynamics,Sungyong Seo*;Chuizheng Meng*;Yan Liu,sungyons@usc.edu;chuizhem@usc.edu;yanliu.cs@usc.edu,6;8;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,University of Southern California;University of Southern California;University of Southern California,31;31;31,62;62;62,10
3424,3424,3424,3424,3424,3424,3424,3424,ICLR,2020,Neural Network Branching for Neural Network Verification ,Jingyue Lu;M. Pawan Kumar,jingyue.lu@spc.ox.ac.uk;pawan@robots.ox.ac.uk,6;8;8,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,7,0.0,yes,9/25/19,University of Oxford;University of Oxford,50;50,1;1,1;10
3425,3425,3425,3425,3425,3425,3425,3425,ICLR,2020,DivideMix: Learning with Noisy Labels as Semi-supervised Learning,Junnan Li;Richard Socher;Steven C.H. Hoi,junnan.li@salesforce.com;rsocher@salesforce.com;shoi@salesforce.com,6;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),1,4,0.0,yes,9/25/19,SalesForce.com;SalesForce.com;SalesForce.com,-1;-1;-1,-1;-1;-1,
3426,3426,3426,3426,3426,3426,3426,3426,ICLR,2020,Reinforcement Learning Based Graph-to-Sequence Model for Natural Question Generation,Yu Chen;Lingfei Wu;Mohammed J. Zaki,cheny39@rpi.edu;lwu@email.wm.edu;zaki@cs.rpi.edu,6;6;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Accept (Poster),0,6,1.0,yes,9/25/19,Rensselaer Polytechnic Institute;College of William and Mary;Rensselaer Polytechnic Institute,172;154;172,438;235;438,10
3427,3427,3427,3427,3427,3427,3427,3427,ICLR,2020,Bayesian Meta Sampling for Fast Uncertainty Adaptation,Zhenyi Wang;Yang Zhao;Ping Yu;Ruiyi Zhang;Changyou Chen,zhenyiwa@buffalo.edu;yzhao63@buffalo.edu;pingyu@buffalo.edu;ryzhang@cs.duke.edu;changyou@buffalo.edu,6;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Accept (Poster),0,6,0.0,yes,9/25/19,"State University of New York, Buffalo;State University of New York, Buffalo;State University of New York, Buffalo;Duke University;State University of New York, Buffalo",84;84;84;47;84,263;263;263;20;263,11
3428,3428,3428,3428,3428,3428,3428,3428,ICLR,2020,BREAKING  CERTIFIED  DEFENSES:  SEMANTIC  ADVERSARIAL  EXAMPLES  WITH  SPOOFED  ROBUSTNESS  CERTIFICATES,Amin Ghiasi;Ali Shafahi;Tom Goldstein,amin@cs.umd.edu;ashafahi@cs.umd.edu;tomg@cs.umd.edu,6;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Accept (Poster),0,3,0.0,yes,9/25/19,"University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park",12;12;12,91;91;91,4
3429,3429,3429,3429,3429,3429,3429,3429,ICLR,2020,Progressive Memory Banks for Incremental Domain Adaptation,Nabiha Asghar;Lili Mou;Kira A. Selby;Kevin D. Pantasdo;Pascal Poupart;Xin Jiang,nasghar@uwaterloo.ca;doublepower.mou@gmail.com;kaselby@uwaterloo.ca;kevin.pantasdo@uwaterloo.ca;ppoupart@uwaterloo.ca;jiang.xin@huawei.com,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,0,0.0,yes,9/25/19,University of Waterloo;;University of Waterloo;University of Waterloo;University of Waterloo;Huawei Technologies Ltd.,28;-1;28;28;28;-1,235;-1;235;235;235;-1,3
3430,3430,3430,3430,3430,3430,3430,3430,ICLR,2020,Deep Learning For Symbolic Mathematics,Guillaume Lample;François Charton,guillaume.lample@gmail.com;fcharton@fb.com,8;6;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Spotlight),7,8,0.0,yes,9/25/19,Facebook;Facebook,-1;-1,-1;-1,
3431,3431,3431,3431,3431,3431,3431,3431,ICLR,2020,Contrastive Learning of Structured World Models,Thomas Kipf;Elise van der Pol;Max Welling,t.n.kipf@uva.nl;e.e.vanderpol@uva.nl;m.welling@uva.nl,8;8;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Talk),0,10,1.0,yes,9/25/19,University of Amsterdam;University of Amsterdam;University of Amsterdam,172;172;172,62;62;62,10
3432,3432,3432,3432,3432,3432,3432,3432,ICLR,2020,Decentralized Deep Learning with Arbitrary Communication Compression,Anastasia Koloskova*;Tao Lin*;Sebastian U Stich;Martin Jaggi,anastasia.koloskova@epfl.ch;tao.lin@epfl.ch;sebastian.stich@epfl.ch;martin.jaggi@epfl.ch,3;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0.0,yes,9/25/19,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne,481;481;481;481,38;38;38;38,
3433,3433,3433,3433,3433,3433,3433,3433,ICLR,2020,Masked Based Unsupervised Content Transfer,Ron Mokady;Sagie Benaim;Lior Wolf;Amit Bermano,sagiebenaim@gmail.com;ron.mokady@gmail.com;wolf@fb.com;amit.bermano@gmail.com,6;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,7,0.0,yes,9/25/19,Tel Aviv University;Tel Aviv University;Facebook;,35;35;-1;-1,188;188;-1;-1,2
3434,3434,3434,3434,3434,3434,3434,3434,ICLR,2020,Fair Resource Allocation in Federated Learning,Tian Li;Maziar Sanjabi;Ahmad Beirami;Virginia Smith,tianli@cmu.edu;maziar.sanjabi@gmail.com;ahmad.beirami@gmail.com;smithv@cmu.edu,6;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0.0,yes,9/25/19,Carnegie Mellon University;University of Southern California;Facebook;Carnegie Mellon University,1;31;-1;1,27;62;-1;27,7
3435,3435,3435,3435,3435,3435,3435,3435,ICLR,2020,Gap-Aware Mitigation of Gradient Staleness,Saar Barkai;Ido Hakimi;Assaf Schuster,saarbarkai@gmail.com;idohakimi@gmail.com;assaf@cs.technion.ac.il,6;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0.0,yes,9/25/19,Technion;Technion;Technion,26;26;26,412;412;412,1;9
3436,3436,3436,3436,3436,3436,3436,3436,ICLR,2020,Graph Neural Networks Exponentially Lose Expressive Power for Node Classification,Kenta Oono;Taiji Suzuki,kenta_oono@mist.i.u-tokyo.ac.jp;taiji@mist.i.u-tokyo.ac.jp,8;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),1,3,1.0,yes,9/25/19,The University of Tokyo;The University of Tokyo,56;56,36;36,10
3437,3437,3437,3437,3437,3437,3437,3437,ICLR,2020,A FRAMEWORK  FOR ROBUSTNESS CERTIFICATION  OF SMOOTHED CLASSIFIERS USING  F-DIVERGENCES,Krishnamurthy (Dj) Dvijotham;Jamie Hayes;Borja Balle;Zico Kolter;Chongli Qin;Andras Gyorgy;Kai Xiao;Sven Gowal;Pushmeet Kohli,dvij@google.com;j.hayes@cs.ucl.ac.uk;bballe@google.com;zkolter@cs.cmu.edu;chongliqin@google.com;agyorgy@google.com;kaix@mit.edu;sgowal@google.com;pushmeet@google.com,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,7,0.0,yes,9/25/19,Google;University College London;Google;Carnegie Mellon University;Google;Google;Massachusetts Institute of Technology;Google;Google,-1;50;-1;1;-1;-1;2;-1;-1,-1;15;-1;27;-1;-1;5;-1;-1,4;1
3438,3438,3438,3438,3438,3438,3438,3438,ICLR,2020,I Am Going MAD: Maximum Discrepancy Competition for Comparing Classifiers Adaptively,Haotao Wang;Tianlong Chen;Zhangyang Wang;Kede Ma,htwang@tamu.edu;wiwjp619@tamu.edu;atlaswang@tamu.edu;kede.ma@cityu.edu.hk,3;8;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,9,0.0,yes,9/25/19,Texas A&M;Texas A&M;Texas A&M;City University of Hong Kong,44;44;44;92,177;177;177;35,
3439,3439,3439,3439,3439,3439,3439,3439,ICLR,2020,Analysis of Video Feature Learning in Two-Stream CNNs on the Example of Zebrafish Swim Bout Classification,Bennet Breier;Arno Onken,b.breier@sms.ed.ac.uk;aonken@inf.ed.ac.uk,6;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,3,0.0,yes,9/25/19,University of Edinburgh;University of Edinburgh,33;33,30;30,
3440,3440,3440,3440,3440,3440,3440,3440,ICLR,2020,"On the steerability"" of generative adversarial networks""",Ali Jahanian*;Lucy Chai*;Phillip Isola,jahanian@mit.edu;lrchai@mit.edu;phillipi@mit.edu,8;8;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,4,0.0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2,5;5;5,5;4;8
3441,3441,3441,3441,3441,3441,3441,3441,ICLR,2020,Imitation Learning via Off-Policy Distribution Matching,Ilya Kostrikov;Ofir Nachum;Jonathan Tompson,kostrikov@cs.nyu.edu;ofirnachum@google.com;tompson@google.com,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,5,1.0,yes,9/25/19,New York University;Google;Google,25;-1;-1,29;-1;-1,
3442,3442,3442,3442,3442,3442,3442,3442,ICLR,2020,A Function Space View of Bounded Norm Infinite Width ReLU Nets: The Multivariate Case,Greg Ongie;Rebecca Willett;Daniel Soudry;Nathan Srebro,gongie@uchicago.edu;willett@uchicago.edu;daniel.soudry@technion.ac.il;nati@ttic.edu,8;6;6,I have published one or two papers in this area.:N/A:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,University of Chicago;University of Chicago;Technion;Toyota Technological Institute at Chicago,48;48;26;-1,9;9;412;-1,
3443,3443,3443,3443,3443,3443,3443,3443,ICLR,2020,ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators,Kevin Clark;Minh-Thang Luong;Quoc V. Le;Christopher D. Manning,kevclark@cs.stanford.edu;thangluong@google.com;qvl@google.com;manning@cs.stanford.edu,8;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),1,6,0.0,yes,9/25/19,Stanford University;Google;Google;Stanford University,4;-1;-1;4,4;-1;-1;4,3
3444,3444,3444,3444,3444,3444,3444,3444,ICLR,2020,Quantifying Point-Prediction Uncertainty in Neural Networks via Residual Estimation with an I/O Kernel,Xin Qiu;Elliot Meyerson;Risto Miikkulainen,qiuxin.nju@gmail.com;elliot.meyerson@cognizant.com;risto@cognizant.com,6;6;8;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,11,0.0,yes,9/25/19,;Cognizant;Cognizant,-1;-1;-1,-1;-1;-1,11
3445,3445,3445,3445,3445,3445,3445,3445,ICLR,2020,Span Recovery for Deep Neural Networks with Applications to Input Obfuscation,Rajesh Jayaram;David P. Woodruff;Qiuyi Zhang,rkjayara@cs.cmu.edu;dwoodruf@andrew.cmu.edu;qiuyiz@google.com,3;6;8;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;Google,1;1;-1,27;27;-1,4
3446,3446,3446,3446,3446,3446,3446,3446,ICLR,2020,Posterior sampling for multi-agent reinforcement learning: solving extensive games with imperfect information,Yichi Zhou;Jialian Li;Jun Zhu,vofhqn@gmail.com;lijialia16@mails.tsinghua.edu.cn;dcszj@mail.tsinghua.edu.cn,6;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,8,0.0,yes,9/25/19,;Tsinghua University;Tsinghua University,-1;8;8,-1;23;23,
3447,3447,3447,3447,3447,3447,3447,3447,ICLR,2020,Co-Attentive Equivariant Neural Networks: Focusing Equivariance On Transformations Co-Occurring in Data,David W. Romero;Mark Hoogendoorn,d.w.romeroguzman@vu.nl;m.hoogendoorn@vu.nl,6;8;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,11,0.0,yes,9/25/19,VU University Toronto;VU University Toronto,18;18,18;18,
3448,3448,3448,3448,3448,3448,3448,3448,ICLR,2020,Explanation  by Progressive  Exaggeration,Sumedha Singla;Brian Pollack;Junxiang Chen;Kayhan Batmanghelich,sumedha.singla@pitt.edu;kayhan@pitt.edu;cjx880409@gmail.com;kayhan@pitt.edu,8;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Accept (Spotlight),0,3,0.0,yes,9/25/19,University of Pittsburgh;University of Pittsburgh;;University of Pittsburgh,79;79;-1;79,113;113;-1;113,
3449,3449,3449,3449,3449,3449,3449,3449,ICLR,2020,Principled Weight Initialization for Hypernetworks,Oscar Chang;Lampros Flokas;Hod Lipson,oscar.chang@columbia.edu;lamflokas@cs.columbia.edu;hod.lipson@columbia.edu,8;8;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Talk),0,6,0.0,yes,9/25/19,Columbia University;Columbia University;Columbia University,15;15;15,16;16;16,11
3450,3450,3450,3450,3450,3450,3450,3450,ICLR,2020,On the Variance of the Adaptive Learning Rate and Beyond,Liyuan Liu;Haoming Jiang;Pengcheng He;Weizhu Chen;Xiaodong Liu;Jianfeng Gao;Jiawei Han,ll2@illinois.edu;jianghm@gatech.edu;penhe@microsoft.com;wzchen@microsoft.com;xiaodl@microsoft.com;jfgao@microsoft.com;hanj@illinois.edu,6;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),4,5,0.0,yes,9/25/19,"University of Illinois, Urbana Champaign;Georgia Institute of Technology;Microsoft;Microsoft;Microsoft;Microsoft;University of Illinois, Urbana Champaign",3;13;-1;-1;-1;-1;3,48;38;-1;-1;-1;-1;48,3;8
3451,3451,3451,3451,3451,3451,3451,3451,ICLR,2020,Curvature Graph Network,Ze Ye;Kin Sum Liu;Tengfei Ma;Jie Gao;Chao Chen,yeze16159@gmail.com;kiliu@cs.stonybrook.edu;tengfei.ma1@ibm.com;jgao@cs.stonybrook.edu;chao.chen.1@stonybrook.edu,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Accept (Poster),0,3,0.0,yes,9/25/19,"State University of New York, Stony Brook;State University of New York, Stony Brook;International Business Machines;State University of New York, Stony Brook;State University of New York, Stony Brook",41;41;-1;41;41,304;304;-1;304;304,10
3452,3452,3452,3452,3452,3452,3452,3452,ICLR,2020,"Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds",Jordan T. Ash;Chicheng Zhang;Akshay Krishnamurthy;John Langford;Alekh Agarwal,jordanta@cs.princeton.edu;chichengz@cs.arizona.edu;akshay.krishnamurthy@microsoft.com;jcl@microsoft.com;alekha@microsoft.com,8;6;8,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,4,0.0,yes,9/25/19,Princeton University;University of Arizona;Microsoft;Microsoft;Microsoft,31;172;-1;-1;-1,6;103;-1;-1;-1,
3453,3453,3453,3453,3453,3453,3453,3453,ICLR,2020,StructPool: Structured Graph Pooling via Conditional Random Fields,Hao Yuan;Shuiwang Ji,hao.yuan@tamu.edu;sji@tamu.edu,6;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,1.0,yes,9/25/19,Texas A&M;Texas A&M,44;44,177;177,10
3454,3454,3454,3454,3454,3454,3454,3454,ICLR,2020,Weakly Supervised Clustering by Exploiting Unique Class Count,Mustafa Umit Oner;Hwee Kuan Lee;Wing-Kin Sung,umitoner@comp.nus.edu.sg;leehk@bii.a-star.edu.sg;ksung@comp.nus.edu.sg,6;1;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,8,0.0,yes,9/25/19,National University of Singapore;A*STAR;National University of Singapore,16;-1;16,25;-1;25,1;2
3455,3455,3455,3455,3455,3455,3455,3455,ICLR,2020,Gradients as Features for Deep Representation Learning,Fangzhou Mu;Yingyu Liang;Yin Li,fmu@cs.wisc.edu;yliang@cs.wisc.edu;yin.li@wisc.edu,6;3;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,University of Southern California;University of Southern California;University of Southern California,31;31;31,62;62;62,
3456,3456,3456,3456,3456,3456,3456,3456,ICLR,2020,Composition-based Multi-Relational Graph Convolutional Networks,Shikhar Vashishth;Soumya Sanyal;Vikram Nitin;Partha Talukdar,shikhar@iisc.ac.in;sanyal.soumya8@gmail.com;vikram.nitin@columbia.edu;ppt@iisc.ac.in,6;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,Indian Institute of Science;Indian Institute of Science;Columbia University;Indian Institute of Science,95;95;15;95,301;301;16;301,10
3457,3457,3457,3457,3457,3457,3457,3457,ICLR,2020,Behaviour Suite for Reinforcement Learning,Ian Osband;Yotam Doron;Matteo Hessel;John Aslanides;Eren Sezener;Andre Saraiva;Katrina McKinney;Tor Lattimore;Csaba Szepesvari;Satinder Singh;Benjamin Van Roy;Richard Sutton;David Silver;Hado Van Hasselt,ian.osband@gmail.com;ydoron@google.com;mtthss@google.com;jaslanides@google.com;esezener@google.com;andresnds@google.com;mckinneyk@google.com;lattimore@google.com;szepi@google.com;baveja@google.com;benvanroy@google.com;suttonr@google.com;davidsilver@google.com;hado@google.com,8;6;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Spotlight),0,11,0.0,yes,9/25/19,Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,
3458,3458,3458,3458,3458,3458,3458,3458,ICLR,2020,Identifying through Flows for Recovering Latent Representations,Shen Li;Bryan Hooi;Gim Hee Lee,maths.shenli@gmail.com;bhooi@comp.nus.edu.sg;dcslgh@nus.edu.sg,6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,National University of Singapore;National University of Singapore;National University of Singapore,16;16;16,25;25;25,5;1
3459,3459,3459,3459,3459,3459,3459,3459,ICLR,2020,Federated Adversarial Domain Adaptation,Xingchao Peng;Zijun Huang;Yizhe Zhu;Kate Saenko,xpeng@bu.edu;zijun.huang@columbia.edu;yizhe.zhu@rutgers.edu;saenko@bu.edu,6;6;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),2,5,0.0,yes,9/25/19,Boston University;Columbia University;Rutgers University;Boston University,67;15;34;67,61;16;168;61,4
3460,3460,3460,3460,3460,3460,3460,3460,ICLR,2020,High Fidelity Speech Synthesis with Adversarial Networks,Mikołaj Bińkowski;Jeff Donahue;Sander Dieleman;Aidan Clark;Erich Elsen;Norman Casagrande;Luis C. Cobo;Karen Simonyan,mikbinkowski@gmail.com;jeffdonahue@google.com;sedielem@google.com;aidanclark@google.com;eriche@google.com;ncasagrande@google.com;luisca@google.com;simonyan@google.com,8;6;8,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Talk),1,5,0.0,yes,9/25/19,Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1,5;4
3461,3461,3461,3461,3461,3461,3461,3461,ICLR,2020,Why Not to Use Zero Imputation? Correcting Sparsity Bias in Training Neural Networks,Joonyoung Yi;Juhyuk Lee;Kwang Joon Kim;Sung Ju Hwang;Eunho Yang,joonyoung.yi@kaist.ac.kr;sehkmg@kaist.ac.kr;preppie@yuhs.ac;sjhwang82@kaist.ac.kr;eunhoy@kaist.ac.kr,6;6;6,I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,12,0.0,yes,9/25/19,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,481;481;-1;481;481,110;110;-1;110;110,4
3462,3462,3462,3462,3462,3462,3462,3462,ICLR,2020,Adversarial Training and Provable Defenses: Bridging the Gap,Mislav Balunovic;Martin Vechev,bmislav@student.ethz.ch;martin.vechev@inf.ethz.ch,8;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Talk),2,8,0.0,yes,9/25/19,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10,13;13,4
3463,3463,3463,3463,3463,3463,3463,3463,ICLR,2020,Can gradient clipping mitigate label noise?,Aditya Krishna Menon;Ankit Singh Rawat;Sashank J. Reddi;Sanjiv Kumar,adityakmenon@google.com;ankitsrawat@google.com;sashank@google.com;sanjivk@google.com,6;8;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,1;9
3464,3464,3464,3464,3464,3464,3464,3464,ICLR,2020,Comparing Rewinding and Fine-tuning in Neural Network Pruning,Alex Renda;Jonathan Frankle;Michael Carbin,renda@csail.mit.edu;jfrankle@csail.mit.edu;mcarbin@csail.mit.edu,8;8;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Talk),0,4,1.0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2,5;5;5,
3465,3465,3465,3465,3465,3465,3465,3465,ICLR,2020,Meta-learning curiosity algorithms,Ferran Alet*;Martin F. Schneider*;Tomas Lozano-Perez;Leslie Pack Kaelbling,ferranalet@gmail.com;martinfs@mit.edu;tlp@csail.mit.edu;lpk@csail.mit.edu,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,4,0.0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2,5;5;5;5,6;8
3466,3466,3466,3466,3466,3466,3466,3466,ICLR,2020,Lookahead: A Far-sighted Alternative of Magnitude-based Pruning,Sejun Park*;Jaeho Lee*;Sangwoo Mo;Jinwoo Shin,sejun.park@kaist.ac.kr;jaeho-lee@kaist.ac.kr;swmo@kaist.ac.kr;jinwoos@kaist.ac.kr,6;6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,13,0.0,yes,9/25/19,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,481;481;481;481,110;110;110;110,8
3467,3467,3467,3467,3467,3467,3467,3467,ICLR,2020,Fantastic Generalization Measures and Where to Find Them,Yiding Jiang*;Behnam Neyshabur*;Hossein Mobahi;Dilip Krishnan;Samy Bengio,ydjiang@google.com;neyshabur@google.com;dilipkay@google.com;hmobahi@google.com;bengio@google.com,8;3;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),1,4,0.0,yes,9/25/19,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,8
3468,3468,3468,3468,3468,3468,3468,3468,ICLR,2020,Convergence of Gradient Methods on Bilinear Zero-Sum Games,Guojun Zhang;Yaoliang Yu,guojun.zhang@uwaterloo.ca;yaoliang.yu@uwaterloo.ca,8;6;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,4,0.0,yes,9/25/19,University of Waterloo;University of Waterloo,28;28,235;235,5;4;9
3469,3469,3469,3469,3469,3469,3469,3469,ICLR,2020,Multi-Agent Interactions Modeling with Correlated Policies,Minghuan Liu;Ming Zhou;Weinan Zhang;Yuzheng Zhuang;Jun Wang;Wulong Liu;Yong Yu,minghuanliu@sjtu.edu.cn;mingak@sjtu.edu.cn;wnzhang@sjtu.edu.cn;zhuangyuzheng@huawei.com;w.j@huawei.com;liuwulong@huawei.com;yyu@apex.sjtu.edu.cn,8;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0.0,yes,9/25/19,Shanghai Jiao Tong University;Shanghai Jiao Tong University;Shanghai Jiao Tong University;Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.;Shanghai Jiao Tong University,53;53;53;-1;-1;-1;53,157;157;157;-1;-1;-1;157,4
3470,3470,3470,3470,3470,3470,3470,3470,ICLR,2020,"Real or Not Real, that is the Question",Yuanbo Xiangli*;Yubin Deng*;Bo Dai*;Chen Change Loy;Dahua Lin,xy019@ie.cuhk.edu.hk;danny.s.deng.ds@gmail.com;doubledaibo@gmail.com;ccloy@ntu.edu.sg;dhlin@ie.cuhk.edu.hk,6;6;8,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,9,0.0,yes,9/25/19,The Chinese University of Hong Kong;;The Chinese University of Hong Kong;National Taiwan University;The Chinese University of Hong Kong,59;-1;59;86;59,35;-1;35;120;35,5;4
3471,3471,3471,3471,3471,3471,3471,3471,ICLR,2020,Structured Object-Aware Physics Prediction for Video Modeling and Planning,Jannik Kossen;Karl Stelzner;Marcel Hussing;Claas Voelcker;Kristian Kersting,kossen@stud.uni-heidelberg.de;stelzner@cs.tu-darmstadt.de;marcel.hussing@stud.tu-darmstadt.de;c.voelcker@stud.tu-darmstadt.de;kersting@cs.tu-darmstadt.de,6;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,8,0.0,yes,9/25/19,Heidelberg University;TU Darmstadt;TU Darmstadt;TU Darmstadt;TU Darmstadt,205;64;64;64;64,44;289;289;289;289,
3472,3472,3472,3472,3472,3472,3472,3472,ICLR,2020,Gradient Descent Maximizes the Margin of Homogeneous Neural Networks,Kaifeng Lyu;Jian Li,vfleaking@gmail.com;lijian83@mail.tsinghua.edu.cn,6;8;8,I do not know much about this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published in this field for several years.:N/A:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,3,0.0,yes,9/25/19,Tsinghua University;Tsinghua University,8;8,23;23,1
3473,3473,3473,3473,3473,3473,3473,3473,ICLR,2020,Implementing Inductive bias for different navigation tasks through diverse RNN attrractors,Tie XU;Omri Barak,fexutie@gmail.com;omri.barak@gmail.com,6;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0.0,yes,9/25/19,Technion;Technion,26;26,412;412,
3474,3474,3474,3474,3474,3474,3474,3474,ICLR,2020,End to End Trainable Active Contours via Differentiable Rendering,Shir Gur;Tal Shaharabany;Lior Wolf,shiretzet@gmail.com;shaharabany@mail.tau.ac.il;wolf@fb.com,8;8;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,6,2.0,yes,9/25/19,Tel Aviv University;Tel Aviv University;Facebook,35;35;-1,188;188;-1,2
3475,3475,3475,3475,3475,3475,3475,3475,ICLR,2020,ReMixMatch: Semi-Supervised Learning with Distribution Matching and Augmentation Anchoring,David Berthelot;Nicholas Carlini;Ekin D. Cubuk;Alex Kurakin;Kihyuk Sohn;Han Zhang;Colin Raffel,dberth@google.com;ncarlini@google.com;cubuk@google.com;kurakin@google.com;kihyuks@google.com;zhanghan@google.com;craffel@google.com,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:N/A:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,
3476,3476,3476,3476,3476,3476,3476,3476,ICLR,2020,DropEdge: Towards Deep Graph Convolutional Networks on Node Classification,Yu Rong;Wenbing Huang;Tingyang Xu;Junzhou Huang,yu.rong@hotmail.com;hwenbing@126.com;tingyangxu@tencent.com;jzhuang@uta.edu,3;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),7,3,2.0,yes,9/25/19,"Tencent AI Lab;Tsinghua University;Tencent AI Lab;University of Texas, Arlington",-1;8;-1;118,-1;23;-1;708,10;8
3477,3477,3477,3477,3477,3477,3477,3477,ICLR,2020,Neural Tangents: Fast and Easy Infinite Neural Networks in Python,Roman Novak;Lechao Xiao;Jiri Hron;Jaehoon Lee;Alexander A. Alemi;Jascha Sohl-Dickstein;Samuel S. Schoenholz,romann@google.com;xlc@google.com;jh2084@cam.ac.uk;jaehlee@google.com;alemi@google.com;jaschasd@google.com;schsam@google.com,8;3;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,12,0.0,yes,9/25/19,Google;Google;University of Cambridge;Google;Google;Google;Google,-1;-1;71;-1;-1;-1;-1,-1;-1;3;-1;-1;-1;-1,11
3478,3478,3478,3478,3478,3478,3478,3478,ICLR,2020,FasterSeg: Searching for Faster Real-time Semantic Segmentation,Wuyang Chen;Xinyu Gong;Xianming Liu;Qian Zhang;Yuan Li;Zhangyang Wang,wuyang.chen@tamu.edu;xy_gong@tamu.edu;xianming.liu@horizon.ai;qian01.zhang@horizon.ai;yuan.li@horizon.ai;atlaswang@tamu.edu,8;6;8,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0.0,yes,9/25/19,Texas A&M;Texas A&M;Horizon Robotics;Horizon Robotics;Horizon Robotics;Texas A&M,44;44;-1;-1;-1;44,177;177;-1;-1;-1;177,2
3479,3479,3479,3479,3479,3479,3479,3479,ICLR,2020,Understanding Knowledge Distillation in Non-autoregressive Machine Translation,Chunting Zhou;Jiatao Gu;Graham Neubig,chuntinz@andrew.cmu.edu;jgu@fb.com;gneubig@cs.cmu.edu,8;8;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,5,2.0,yes,9/25/19,Carnegie Mellon University;Facebook;Carnegie Mellon University,1;-1;1,27;-1;27,3
3480,3480,3480,3480,3480,3480,3480,3480,ICLR,2020,Building Deep Equivariant Capsule Networks,Sai Raam Venkataraman;S. Balasubramanian;R. Raghunatha Sarma,vsairaam@sssihl.edu.in;sbalasubramanian@sssihl.edu.in;rraghunathasarma@sssihl.edu.in,8;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,10,0.0,yes,9/25/19,Sri Sathya Sai Institute of Higher Learning;Sri Sathya Sai Institute of Higher Learning;Sri Sathya Sai Institute of Higher Learning,-1;-1;-1,-1;-1;-1,
3481,3481,3481,3481,3481,3481,3481,3481,ICLR,2020,Understanding Architectures Learnt by Cell-based Neural Architecture Search,Yao Shu;Wei Wang;Shaofeng Cai,shuyao@comp.nus.edu.sg;wangwei@comp.nus.edu.sg;shaofeng@comp.nus.edu.sg,8;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0.0,yes,9/25/19,National University of Singapore;National University of Singapore;National University of Singapore,16;16;16,25;25;25,3;8
3482,3482,3482,3482,3482,3482,3482,3482,ICLR,2020,Dynamically Pruned Message Passing Networks for Large-scale Knowledge Graph Reasoning,Xiaoran Xu;Wei Feng;Yunsheng Jiang;Xiaohui Xie;Zhiqing Sun;Zhi-Hong Deng,xiaoran.xu@hulu.com;wei.feng@hulu.com;yunsheng.jiang@hulu.com;xiaohui.xie@hulu.com;zhiqings@andrew.cmu.edu;zhdeng@pku.edu.cn,6;6;8,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,3,0.0,yes,9/25/19,Hulu LLC.;Hulu LLC.;Hulu LLC.;Hulu LLC.;Carnegie Mellon University;Peking University,-1;-1;-1;-1;1;22,-1;-1;-1;-1;27;24,10
3483,3483,3483,3483,3483,3483,3483,3483,ICLR,2020,Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model,Wenhan Xiong;Jingfei Du;William Yang Wang;Veselin Stoyanov,xwhan@cs.ucsb.edu;jingfeidu@fb.com;william@cs.ucsb.edu;ves@fb.com,8;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,5,0.0,yes,9/25/19,UC Santa Barbara;Facebook;UC Santa Barbara;Facebook,38;-1;38;-1,57;-1;57;-1,3;6
3484,3484,3484,3484,3484,3484,3484,3484,ICLR,2020,Linear Symmetric Quantization of Neural Networks for Low-precision Integer Hardware,Xiandong Zhao;Ying Wang;Xuyi Cai;Cheng Liu;Lei Zhang,zhaoxiandong@ict.ac.cn;wangying2009@ict.ac.cn;caixuyi18s@ict.ac.cn;liucheng@ict.ac.cn;zlei@ict.ac.cn,6;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0.0,yes,9/25/19,"Institute of Computing Technology, Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences",59;59;59;59;59,1397;1397;1397;1397;1397,2
3485,3485,3485,3485,3485,3485,3485,3485,ICLR,2020,Consistency Regularization for Generative Adversarial Networks,Han Zhang;Zizhao Zhang;Augustus Odena;Honglak Lee,zhanghan@google.com;zizhaoz@google.com;augustusodena@google.com;honglak@google.com,6;6;8,I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:N/A:N/A,Accept (Poster),0,5,1.0,yes,9/25/19,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,5;4
3486,3486,3486,3486,3486,3486,3486,3486,ICLR,2020,Short and Sparse Deconvolution --- A Geometric Approach,Yenson Lau;Qing Qu;Han-Wen Kuo;Pengcheng Zhou;Yuqian Zhang;John Wright,y.lau@columbia.edu;qq213@nyu.edu;hk2673@columbia.edu;pz2230@columbia.edu;yz2557@cornell.edu;jw2966@columbia.edu,6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,Columbia University;New York University;Columbia University;Columbia University;Cornell University;Columbia University,15;25;15;15;7;15,16;29;16;16;19;16,
3487,3487,3487,3487,3487,3487,3487,3487,ICLR,2020,GENESIS: Generative Scene Inference and Sampling with Object-Centric Latent Representations,Martin Engelcke;Adam R. Kosiorek;Oiwi Parker Jones;Ingmar Posner,martin@robots.ox.ac.uk;adamk@robots.ox.ac.uk;oiwi@robots.ox.ac.uk;ingmar@robots.ox.ac.uk,8;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,7,0.0,yes,9/25/19,University of Oxford;University of Oxford;University of Oxford;University of Oxford,50;50;50;50,1;1;1;1,5
3488,3488,3488,3488,3488,3488,3488,3488,ICLR,2020,BinaryDuo: Reducing Gradient Mismatch in Binary Activation Network by Coupling Binary Activations,Hyungjun Kim;Kyungsu Kim;Jinseok Kim;Jae-Joon Kim,hyungjun.kim@postech.ac.kr;kyungsu.kim@postech.ac.kr;jinseok.kim@postech.ac.kr;jaejoon@postech.ac.kr,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),1,7,1.0,yes,9/25/19,POSTECH;POSTECH;POSTECH;POSTECH,118;118;118;118,146;146;146;146,
3489,3489,3489,3489,3489,3489,3489,3489,ICLR,2020,Transformer-XH: Multi-Evidence Reasoning with eXtra Hop Attention,Chen Zhao;Chenyan Xiong;Corby Rosset;Xia Song;Paul Bennett;Saurabh Tiwary,chenz@cs.umd.edu;chenyan.xiong@microsoft.com;corbin.rosset@microsoft.com;xiaso@microsoft.com;paul.n.bennett@microsoft.com;satiwary@microsoft.com,8;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,8,0.0,yes,9/25/19,"University of Maryland, College Park;Microsoft;Microsoft;Microsoft;Microsoft;Microsoft",12;-1;-1;-1;-1;-1,91;-1;-1;-1;-1;-1,3;10
3490,3490,3490,3490,3490,3490,3490,3490,ICLR,2020,Sub-policy Adaptation for Hierarchical Reinforcement Learning,Alexander Li;Carlos Florensa;Ignasi Clavera;Pieter Abbeel,alexli1@berkeley.edu;florensa@berkeley.edu;iclavera@berkeley.edu;pabbeel@berkeley.edu,8;3,I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5,13;13;13;13,
3491,3491,3491,3491,3491,3491,3491,3491,ICLR,2020,"Watch, Try, Learn: Meta-Learning from Demonstrations and Rewards",Allan Zhou;Eric Jang;Daniel Kappler;Alex Herzog;Mohi Khansari;Paul Wohlhart;Yunfei Bai;Mrinal Kalakrishnan;Sergey Levine;Chelsea Finn,ayz@stanford.edu;ejang@google.com;kappler@google.com;alexherzog@google.com;khansari@google.com;wohlhart@google.com;yunfeibai@google.com;kalakris@google.com;slevine@google.com;cbfinn@cs.stanford.edu,6;3;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,Stanford University;Google;Google;Google;Google;Google;Google;Google;Google;Stanford University,4;-1;-1;-1;-1;-1;-1;-1;-1;4,4;-1;-1;-1;-1;-1;-1;-1;-1;4,
3492,3492,3492,3492,3492,3492,3492,3492,ICLR,2020,Deep Orientation Uncertainty Learning based on a Bingham Loss,Igor Gilitschenski;Roshni Sahoo;Wilko Schwarting;Alexander Amini;Sertac Karaman;Daniela Rus,igilitschenski@mit.edu;rsahoo@mit.edu;wilkos@mit.edu;amini@mit.edu;sertac@mit.edu;rus@csail.mit.edu,6;6,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2;2;2,5;5;5;5;5;5,2
3493,3493,3493,3493,3493,3493,3493,3493,ICLR,2020,Overlearning Reveals Sensitive Attributes,Congzheng Song;Vitaly Shmatikov,cs2296@cornell.edu;shmat@cs.cornell.edu,6;1;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Poster),0,3,0.0,yes,9/25/19,Cornell University;Cornell University,7;7,19;19,3;7
3494,3494,3494,3494,3494,3494,3494,3494,ICLR,2020,Understanding the Limitations of Conditional Generative Models,Ethan Fetaya;Joern-Henrik Jacobsen;Will Grathwohl;Richard Zemel,ethanf@cs.toronto.edu;j.jacobsen@vectorinstitute.ai;wgrathwohl@cs.toronto.edu;zemel@cs.toronto.edu,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,4,0.0,yes,9/25/19,"Department of Computer Science, University of Toronto;Vector Institute;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",18;-1;18;18,18;-1;18;18,5;4
3495,3495,3495,3495,3495,3495,3495,3495,ICLR,2020,Learning Disentangled Representations for CounterFactual Regression,Negar Hassanpour;Russell Greiner,hassanpo@ualberta.ca;rgreiner@ualberta.ca,8;8;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,1.0,yes,9/25/19,University of Alberta;University of Alberta,100;100,136;136,
3496,3496,3496,3496,3496,3496,3496,3496,ICLR,2020,Dynamical Distance Learning for Semi-Supervised and Unsupervised Skill Discovery,Kristian Hartikainen;Xinyang Geng;Tuomas Haarnoja;Sergey Levine,kristian.hartikainen@gmail.com;young.geng@berkeley.edu;tuomash@google.com;svlevine@eecs.berkeley.edu,6;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,9,3.0,yes,9/25/19,University of Oxford;University of California Berkeley;Google;University of California Berkeley,50;5;-1;5,1;13;-1;13,
3497,3497,3497,3497,3497,3497,3497,3497,ICLR,2020,Learning-Augmented Data Stream Algorithms,Tanqiu Jiang;Yi Li;Honghao Lin;Yisong Ruan;David P. Woodruff,taj320@lehigh.edu;yili@ntu.edu.sg;honghao_lin@sjtu.edu.cn;24320152202802@stu.xmu.edu.cn;dwoodruf@andrew.cmu.edu,8;8;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,3,0.0,yes,9/25/19,Lehigh University;National Taiwan University;Shanghai Jiao Tong University;Xiamen University;Carnegie Mellon University,266;86;53;64;1,633;120;157;8;27,
3498,3498,3498,3498,3498,3498,3498,3498,ICLR,2020,"Kaleidoscope: An Efficient, Learnable Representation For All Structured Linear Maps",Tri Dao;Nimit Sohoni;Albert Gu;Matthew Eichhorn;Amit Blonder;Megan Leszczynski;Atri Rudra;Christopher Ré,trid@stanford.edu;nims@stanford.edu;albertgu@stanford.edu;mae226@cornell.edu;amitblon@buffalo.edu;mleszczy@stanford.edu;atri@buffalo.edu;chrismre@cs.stanford.edu,6;8;8,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,5,0.0,yes,9/25/19,"Stanford University;Stanford University;Stanford University;Cornell University;State University of New York, Buffalo;Stanford University;State University of New York, Buffalo;Stanford University",4;4;4;7;84;4;84;4,4;4;4;19;263;4;263;4,1
3499,3499,3499,3499,3499,3499,3499,3499,ICLR,2020,Playing the lottery with rewards and multiple languages: lottery tickets in RL and NLP,Haonan Yu;Sergey Edunov;Yuandong Tian;Ari S. Morcos,haonanu@gmail.com;edunov@fb.com;yuandong@fb.com;arimorcos@gmail.com,6;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,2,0.0,yes,9/25/19,;Facebook;Facebook;Facebook,-1;-1;-1;-1,-1;-1;-1;-1,3
3500,3500,3500,3500,3500,3500,3500,3500,ICLR,2020,Learning the Arrow of Time for Problems in Reinforcement Learning,Nasim Rahaman;Steffen Wolf;Anirudh Goyal;Roman Remme;Yoshua Bengio,nasim.rahaman@tuebingen.mpg.de;steffen.wolf@iwr.uni-heidelberg.de;anirudhgoyal9119@gmail.com;roman.remme@iwr.uni-heidelberg.de;yoshua.bengio@mila.quebec,6;8;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,16,0.0,yes,9/25/19,"Max Planck Institute for Intelligent Systems, Max-Planck Institute;Heidelberg University;University of Montreal;Heidelberg University;University of Montreal",-1;205;128;205;128,-1;44;85;44;85,
3501,3501,3501,3501,3501,3501,3501,3501,ICLR,2020,DD-PPO: Learning Near-Perfect PointGoal Navigators from 2.5 Billion Frames,Erik Wijmans;Abhishek Kadian;Ari Morcos;Stefan Lee;Irfan Essa;Devi Parikh;Manolis Savva;Dhruv Batra,etw@gatech.edu;akadian@fb.com;arimorcos@gmail.com;leestef@oregonstate.edu;irfan@gatech.edu;parikh@gatech.edu;msavva@sfu.ca;dbatra@gatech.edu,8;8;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,Georgia Institute of Technology;Facebook;Facebook;Oregon State University;Georgia Institute of Technology;Georgia Institute of Technology;Simon Fraser University;Georgia Institute of Technology,13;-1;-1;77;13;13;64;13,38;-1;-1;373;38;38;272;38,
3502,3502,3502,3502,3502,3502,3502,3502,ICLR,2020,Information Geometry of Orthogonal Initializations and Training,Piotr Aleksander Sokół;Il Memming Park,piotr.sokol@stonybrook.edu;memming.park@stonybrook.edu,6;8;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,8,0.0,yes,9/25/19,"State University of New York, Stony Brook;State University of New York, Stony Brook",41;41,304;304,1
3503,3503,3503,3503,3503,3503,3503,3503,ICLR,2020,Strategies for Pre-training Graph Neural Networks,Weihua Hu*;Bowen Liu*;Joseph Gomes;Marinka Zitnik;Percy Liang;Vijay Pande;Jure Leskovec,weihuahu@stanford.edu;liubowen@stanford.edu;joegomes@stanford.edu;marinka@cs.stanford.edu;pliang@cs.stanford.edu;pande@stanford.edu;jure@cs.stanford.edu,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),1,3,0.0,yes,9/25/19,Stanford University;Stanford University;Stanford University;Stanford University;Stanford University;Stanford University;Stanford University,4;4;4;4;4;4;4,4;4;4;4;4;4;4,10;8
3504,3504,3504,3504,3504,3504,3504,3504,ICLR,2020,Cross-Domain Few-Shot Classification via Learned Feature-Wise Transformation,Hung-Yu Tseng;Hsin-Ying Lee;Jia-Bin Huang;Ming-Hsuan Yang,htseng6@ucmerced.edu;hlee246@ucmerced.edu;jbhuang@vt.edu;mhyang@ucmerced.edu,6;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,5,0.0,yes,9/25/19,University of California at Merced;University of California at Merced;Virginia Tech;University of California at Merced,481;481;79;481,354;354;240;354,6;8
3505,3505,3505,3505,3505,3505,3505,3505,ICLR,2020,How much Position Information Do Convolutional Neural Networks Encode?,Md Amirul Islam*;Sen Jia*;Neil D. B. Bruce,amirul@scs.ryerson.ca;sen.jia@ryerson.ca;bruce@ryerson.ca,8;8;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,3,5.0,yes,9/25/19,Ryerson University;Ryerson University;Ryerson University,323;323;323,739;739;739,
3506,3506,3506,3506,3506,3506,3506,3506,ICLR,2020,Generalization through Memorization: Nearest Neighbor Language Models,Urvashi Khandelwal;Omer Levy;Dan Jurafsky;Luke Zettlemoyer;Mike Lewis,urvashik@stanford.edu;omerlevy@gmail.com;jurafsky@stanford.edu;lsz@fb.com;mikelewis@fb.com,3;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),1,8,0.0,yes,9/25/19,Stanford University;Facebook;Stanford University;Facebook;Facebook,4;-1;4;-1;-1,4;-1;4;-1;-1,3
3507,3507,3507,3507,3507,3507,3507,3507,ICLR,2020,On the Relationship between Self-Attention and Convolutional Layers,Jean-Baptiste Cordonnier;Andreas Loukas;Martin Jaggi,jean-baptiste.cordonnier@epfl.ch;andreas.loukas@epfl.ch;martin.jaggi@epfl.ch,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0.0,yes,9/25/19,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne,481;481;481,38;38;38,1
3508,3508,3508,3508,3508,3508,3508,3508,ICLR,2020,On Bonus Based Exploration Methods In The Arcade Learning Environment,Adrien Ali Taiga;William Fedus;Marlos C. Machado;Aaron Courville;Marc G. Bellemare,adrien.alitaiga@gmail.com;liamfedus@google.com;marlosm@google.com;aaron.courville@gmail.com;bellemare@google.com,6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,2,0.0,yes,9/25/19,University of Montreal;Google;Google;University of Montreal;Google,128;-1;-1;128;-1,85;-1;-1;85;-1,
3509,3509,3509,3509,3509,3509,3509,3509,ICLR,2020,The Curious Case of Neural Text Degeneration,Ari Holtzman;Jan Buys;Li Du;Maxwell Forbes;Yejin Choi,ahai@cs.washington.edu;jbuys@cs.uct.ac.za;dul2@cs.washington.edu;mbforbes@cs.washington.edu;yejin@cs.washington.edu,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,7,0.0,yes,9/25/19,University of Washington;University of Cape Town;University of Washington;University of Washington;University of Washington,6;390;6;6;6,26;136;26;26;26,3
3510,3510,3510,3510,3510,3510,3510,3510,ICLR,2020,A Mutual Information Maximization Perspective of Language Representation Learning,Lingpeng Kong;Cyprien de Masson d'Autume;Lei Yu;Wang Ling;Zihang Dai;Dani Yogatama,lingpenk@google.com;cyprien@google.com;leiyu@google.com;lingwang@google.com;zihangd@google.com;dyogatama@google.com,8;8;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Spotlight),0,5,1.0,yes,9/25/19,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,3;1;2
3511,3511,3511,3511,3511,3511,3511,3511,ICLR,2020,Measuring Compositional Generalization: A Comprehensive Method on Realistic Data,Daniel Keysers;Nathanael Schärli;Nathan Scales;Hylke Buisman;Daniel Furrer;Sergii Kashubin;Nikola Momchev;Danila Sinopalnikov;Lukasz Stafiniak;Tibor Tihon;Dmitry Tsarkov;Xiao Wang;Marc van Zee;Olivier Bousquet,keysers@google.com;schaerli@google.com;nkscales@google.com;hylke@google.com;danielfurrer@google.com;sergik@google.com;nikola@google.com;sinopalnikov@google.com;lukstafi@google.com;ttihon@google.com;tsar@google.com;wangxiao@google.com;marcvanzee@google.com;obousquet@google.com,6;8;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,4,0.0,yes,9/25/19,Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,3;8
3512,3512,3512,3512,3512,3512,3512,3512,ICLR,2020,GLAD: Learning Sparse Graph Recovery,Harsh Shrivastava;Xinshi Chen;Binghong Chen;Guanghui Lan;Srinivas Aluru;Han Liu;Le Song,hshrivastava3@gatech.edu;xinshi.chen@gatech.edu;binghong@gatech.edu;george.lan@isye.gatech.edu;aluru@cc.gatech.edu;hanliu@northwestern.edu;lsong@cc.gatech.edu,8;8;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,9,0.0,yes,9/25/19,Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Northwestern University;Georgia Institute of Technology,13;13;13;13;13;44;13,38;38;38;38;38;22;38,9;10
3513,3513,3513,3513,3513,3513,3513,3513,ICLR,2020,Query-efficient Meta Attack to Deep Neural Networks,Jiawei Du;Hu Zhang;Joey Tianyi Zhou;Yi Yang;Jiashi Feng,dujiawei@u.nus.edu;hu.zhang-1@student.uts.edu.au;joey.tianyi.zhou@gmail.com;yi.yang@uts.edu.au;elefjia@nus.edu.sg,8;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),1,7,0.0,yes,9/25/19,National University of Singapore;University of Technology Sydney;;University of Technology Sydney;National University of Singapore,16;108;-1;108;16,25;193;-1;193;25,4
3514,3514,3514,3514,3514,3514,3514,3514,ICLR,2020,Bounds on Over-Parameterization for Guaranteed Existence of Descent Paths in Shallow ReLU Networks,Arsalan Sharifnassab;Saber Salehkaleybar;S. Jamaloddin Golestani,a.sharifnassab@gmail.com;saber.salehk@gmail.com;golestani@sharif.edu,6;6,I have read many papers in this area.:N/A:N/A:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,2,0.0,yes,9/25/19,Sharif University of Technology;Sharif University of Technology;Sharif University of Technology,323;323;323,564;564;564,
3515,3515,3515,3515,3515,3515,3515,3515,ICLR,2020,"A critical analysis of self-supervision, or what we can learn from a single image",Asano YM.;Rupprecht C.;Vedaldi A.,yuki@robots.ox.ac.uk;chrisr@robots.ox.ac.uk;vedaldi@robots.ox.ac.uk,6;6;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Poster),0,4,0.0,yes,9/25/19,University of Oxford;University of Oxford;University of Oxford,50;50;50,1;1;1,
3516,3516,3516,3516,3516,3516,3516,3516,ICLR,2020,Geometric Insights into the Convergence of Nonlinear TD Learning,David Brandfonbrener;Joan Bruna,david.brandfonbrener@nyu.edu;bruna@cims.nyu.edu,8;6;3;8,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,New York University;New York University,25;25,29;29,1;9
3517,3517,3517,3517,3517,3517,3517,3517,ICLR,2020,Geometric Analysis of Nonconvex Optimization Landscapes for Overcomplete Learning,Qing Qu;Yuexiang Zhai;Xiao Li;Yuqian Zhang;Zhihui Zhu,qingqu1006@gmail.com;ysz@berkeley.edu;xli@ee.cuhk.edu.hk;yqz.zhang@gmail.com;zzhu29@jhu.edu,8;8;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,5,0.0,yes,9/25/19,New York University;University of California Berkeley;The Chinese University of Hong Kong;Columbia University;Johns Hopkins University,25;5;59;15;73,29;13;35;16;12,9
3518,3518,3518,3518,3518,3518,3518,3518,ICLR,2020,Network Deconvolution,Chengxi Ye;Matthew Evanusa;Hua He;Anton Mitrokhin;Tom Goldstein;James A. Yorke;Cornelia Fermuller;Yiannis Aloimonos,yechengxi@gmail.com;mevanusa@umd.edu;huah@umd.edu;amitrokh@umd.edu;tomg@cs.umd.edu;yorke@umd.edu;fer@umiacs.umd.edu;yiannis@cs.umd.edu,8;8;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,3,0.0,yes,9/25/19,"Amazon;University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park",-1;12;12;12;12;12;12;12,-1;91;91;91;91;91;91;91,
3519,3519,3519,3519,3519,3519,3519,3519,ICLR,2020,Pure and Spurious Critical Points: a Geometric Study of Linear Networks,Matthew Trager;Kathlén Kohn;Joan Bruna,matthew.trager@cims.nyu.edu;kathlen.korn@gmail.com;bruna@cims.nyu.edu,8;3;3,I do not know much about this area.:N/A:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0.0,yes,9/25/19,"New York University;KTH Royal Institute of Technology, Stockholm, Sweden;New York University",25;128;25,29;222;29,
3520,3520,3520,3520,3520,3520,3520,3520,ICLR,2020,PC-DARTS: Partial Channel Connections for Memory-Efficient Architecture Search,Yuhui Xu;Lingxi Xie;Xiaopeng Zhang;Xin Chen;Guo-Jun Qi;Qi Tian;Hongkai Xiong,yuhuixu@sjtu.edu.cn;198808xc@gmail.com;zxphistory@gmail.com;1410452@tongji.edu.cn;guojunq@gmail.com;tian.qi1@huawei.com;xionghongkai@sjtu.edu.cn,6;8;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,5,0.0,yes,9/25/19,Shanghai Jiao Tong University;Huawei Technologies Ltd.;National University of Singapore;Tsinghua University;University of Central Florida;Huawei Technologies Ltd.;Shanghai Jiao Tong University,53;-1;16;8;77;-1;53,157;-1;25;23;609;-1;157,
3521,3521,3521,3521,3521,3521,3521,3521,ICLR,2020,Towards a Deep Network Architecture for Structured Smoothness,Haroun Habeeb;Oluwasanmi Koyejo,haroun7@gmail.com;sanmi@illinois.edu,6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Accept (Poster),0,2,0.0,yes,9/25/19,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",3;3,48;48,
3522,3522,3522,3522,3522,3522,3522,3522,ICLR,2020,RNNs Incrementally Evolving on an Equilibrium Manifold: A Panacea for Vanishing and Exploding Gradients?,Anil Kag;Ziming Zhang;Venkatesh Saligrama,anilkag@bu.edu;zzhang@merl.com;srv@bu.edu,8;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0.0,yes,9/25/19,Boston University;Mitsubishi Electric Research Labs;Boston University,67;-1;67,61;-1;61,
3523,3523,3523,3523,3523,3523,3523,3523,ICLR,2020,"Deep Imitative Models for Flexible Inference, Planning, and Control",Nicholas Rhinehart;Rowan McAllister;Sergey Levine,nrhineha@cs.cmu.edu;rmcallister@berkeley.edu;svlevine@eecs.berkeley.edu,8;8;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),0,7,0.0,yes,9/25/19,Carnegie Mellon University;University of California Berkeley;University of California Berkeley,1;5;5,27;13;13,
3524,3524,3524,3524,3524,3524,3524,3524,ICLR,2020,The Implicit Bias of Depth: How Incremental Learning Drives Generalization,Daniel Gissin;Shai Shalev-Shwartz;Amit Daniely,daniel.gissin@mail.huji.ac.il;shais@cs.huji.ac.il;amit.daniely@mail.huji.ac.il,6;6;6,I have published one or two papers in this area.:N/A:N/A:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,Hebrew University of Jerusalem;Hebrew University of Jerusalem;Hebrew University of Jerusalem,67;67;67,216;216;216,8
3525,3525,3525,3525,3525,3525,3525,3525,ICLR,2020,BackPACK: Packing more into Backprop,Felix Dangel;Frederik Kunstner;Philipp Hennig,felix.dangel@tuebingen.mpg.de;kunstner@cs.ubc.ca;philipp.hennig@uni-tuebingen.de,8;8;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,3,0.0,yes,9/25/19,"Max Planck Institute for Intelligent Systems, Max-Planck Institute;University of British Columbia;University of Tuebingen",-1;35;154,-1;34;91,
3526,3526,3526,3526,3526,3526,3526,3526,ICLR,2020,Economy Statistical Recurrent Units For Inferring Nonlinear Granger Causality,Saurabh Khanna;Vincent Y. F. Tan,elesaur@nus.edu.sg;vtan@nus.edu.sg,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0.0,yes,9/25/19,National University of Singapore;National University of Singapore,16;16,25;25,
3527,3527,3527,3527,3527,3527,3527,3527,ICLR,2020,Detecting Extrapolation with Local Ensembles,David Madras;James Atwood;Alexander D'Amour,david.madras@mail.utoronto.ca;atwoodj@google.com;alexdamour@google.com,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,7,0.0,yes,9/25/19,Toronto University;Google;Google,18;-1;-1,18;-1;-1,
3528,3528,3528,3528,3528,3528,3528,3528,ICLR,2020,Few-shot Text Classification with Distributional Signatures,Yujia Bao;Menghua Wu;Shiyu Chang;Regina Barzilay,yujia@csail.mit.edu;rmwu@mit.edu;shiyu.chang@ibm.com;regina@csail.mit.edu,6;1;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;International Business Machines;Massachusetts Institute of Technology,2;2;-1;2,5;5;-1;5,3;6;2
3529,3529,3529,3529,3529,3529,3529,3529,ICLR,2020,Scaling Autoregressive Video Models,Dirk Weissenborn;Oscar Täckström;Jakob Uszkoreit,diwe@google.com;oscar.tackstrom@gmail.com;usz@google.com,8;6;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Spotlight),0,6,0.0,yes,9/25/19,Google;Sana Labs;Google,-1;-1;-1,-1;-1;-1,4
3530,3530,3530,3530,3530,3530,3530,3530,ICLR,2020,Reformer: The Efficient Transformer,Nikita Kitaev;Lukasz Kaiser;Anselm Levskaya,kitaev@cs.berkeley.edu;lukaszkaiser@google.com;levskaya@google.com,8;6;8,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Talk),4,3,0.0,yes,9/25/19,University of California Berkeley;Google;Google,5;-1;-1,13;-1;-1,
3531,3531,3531,3531,3531,3531,3531,3531,ICLR,2020,Robust Subspace Recovery Layer for Unsupervised Anomaly Detection,Chieh-Hsin Lai;Dongmian Zou;Gilad Lerman,laixx313@umn.edu;dzou@umn.edu;lerman@umn.edu,8;6;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0.0,yes,9/25/19,"University of Minnesota, Minneapolis;University of Minnesota, Minneapolis;University of Minnesota, Minneapolis",59;59;59,79;79;79,
3532,3532,3532,3532,3532,3532,3532,3532,ICLR,2020,SCALOR: Generative World Models with Scalable Object Representations,Jindong Jiang*;Sepehr Janghorbani*;Gerard De Melo;Sungjin Ahn,jindong.jiang@rutgers.edu;sj620@scarletmail.rutgers.edu;gdm@demelo.org;sjn.ahn@gmail.com,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Accept (Poster),0,7,0.0,yes,9/25/19,Rutgers University;Rutgers University;Rutgers University;Rutgers University,34;34;34;34,168;168;168;168,5
3533,3533,3533,3533,3533,3533,3533,3533,ICLR,2020,Learning Robust Representations via Multi-View Information Bottleneck,Marco Federici;Anjan Dutta;Patrick Forré;Nate Kushman;Zeynep Akata,m.federici@uva.nl;duttanjan@gmail.com;patrickforre@gmail.com;nate@kushman.org;zeynepakata@gmail.com,8;6;8,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),0,5,1.0,yes,9/25/19,University of Amsterdam;University of Exeter;University of Amsterdam;Microsoft Research;University of Tuebingen,172;390;172;-1;154,62;146;62;-1;91,8
3534,3534,3534,3534,3534,3534,3534,3534,ICLR,2020,Mixup Inference: Better Exploiting Mixup to Defend Adversarial Attacks,Tianyu Pang*;Kun Xu*;Jun Zhu,pty17@mails.tsinghua.edu.cn;kunxu.thu@gmail.com;dcszj@mail.tsinghua.edu.cn,6;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,9,0.0,yes,9/25/19,Tsinghua University;Tsinghua University;Tsinghua University,8;8;8,23;23;23,4;8
3535,3535,3535,3535,3535,3535,3535,3535,ICLR,2020,Adversarial Lipschitz Regularization,Dávid Terjék,david.terjek92@gmail.com,6;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,9,0.0,yes,9/25/19,,,,5;4
3536,3536,3536,3536,3536,3536,3536,3536,ICLR,2020,Are Transformers universal approximators of sequence-to-sequence functions?,Chulhee Yun;Srinadh Bhojanapalli;Ankit Singh Rawat;Sashank Reddi;Sanjiv Kumar,chulheey@mit.edu;bsrinadh@google.com;ankitsrawat@google.com;sashank@google.com;sanjivk@google.com,6;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,5,0.0,yes,9/25/19,Massachusetts Institute of Technology;Google;Google;Google;Google,2;-1;-1;-1;-1,5;-1;-1;-1;-1,3;1
3537,3537,3537,3537,3537,3537,3537,3537,ICLR,2020,Escaping Saddle Points Faster with Stochastic Momentum,Jun-Kun Wang;Chi-Heng Lin;Jacob Abernethy,jimwang@gatech.edu;cl3385@gatech.edu;prof@gatech.edu,3;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,9,0.0,yes,9/25/19,Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology,13;13;13,38;38;38,9
3538,3538,3538,3538,3538,3538,3538,3538,ICLR,2020,SlowMo: Improving Communication-Efficient Distributed SGD with Slow Momentum,Jianyu Wang;Vinayak Tantia;Nicolas Ballas;Michael Rabbat,jianyuw1@andrew.cmu.edu;tantia@fb.com;ballasn@fb.com;mikerabbat@fb.com,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,Carnegie Mellon University;Facebook;Facebook;Facebook,1;-1;-1;-1,27;-1;-1;-1,3;9;8
3539,3539,3539,3539,3539,3539,3539,3539,ICLR,2020,SUMO: Unbiased Estimation of Log Marginal Probability for Latent Variable Models,Yucen Luo;Alex Beatson;Mohammad Norouzi;Jun Zhu;David Duvenaud;Ryan P. Adams;Ricky T. Q. Chen,luoyc15@mails.tsinghua.edu.cn;abeatson@cs.princeton.edu;mnorouzi@google.com;dcszj@mail.tsinghua.edu.cn;duvenaud@cs.toronto.edu;rpa@princeton.edu;rtqichen@cs.toronto.edu,8;8;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,5,0.0,yes,9/25/19,"Tsinghua University;Princeton University;Google;Tsinghua University;Department of Computer Science, University of Toronto;Princeton University;Department of Computer Science, University of Toronto",8;31;-1;8;18;31;18,23;6;-1;23;18;6;18,
3540,3540,3540,3540,3540,3540,3540,3540,ICLR,2020,Symplectic ODE-Net: Learning Hamiltonian Dynamics with Control,Yaofeng Desmond Zhong;Biswadip Dey;Amit Chakraborty,y.zhong@princeton.edu;biswadip.dey@siemens.com;amit.chakraborty@siemens.com,8;6;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,6,0.0,yes,9/25/19,Princeton University;Siemens Corporate Research;Siemens Corporate Research,31;-1;-1,6;-1;-1,10;8
3541,3541,3541,3541,3541,3541,3541,3541,ICLR,2020,Intrinsically Motivated Discovery of Diverse Patterns in Self-Organizing Systems,Chris Reinke;Mayalen Etcheverry;Pierre-Yves Oudeyer,chris.reinke@inria.fr;mayalen.etcheverry@inria.fr;chris.reinke@inria.fr;pierre-yves.oudeyer@inria.fr,8;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.,Accept (Talk),0,5,0.0,yes,9/25/19,INRIA;INRIA;INRIA;INRIA,-1;-1;-1;-1,-1;-1;-1;-1,
3542,3542,3542,3542,3542,3542,3542,3542,ICLR,2020,On the Convergence of FedAvg on Non-IID Data,Xiang Li;Kaixuan Huang;Wenhao Yang;Shusen Wang;Zhihua Zhang,smslixiang@pku.edu.cn;hackyhuang@pku.edu.cn;yangwhsms@gmail.com;shusen.wang@stevens.edu;zhzhang@math.pku.edu.cn,8;8;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,3,0.0,yes,9/25/19,Peking University;Peking University;Peking University;Stevens Institute of Technology;Peking University,22;22;22;154;22,24;24;24;605;24,1;9
3543,3543,3543,3543,3543,3543,3543,3543,ICLR,2020,The asymptotic spectrum of the Hessian of DNN throughout training,Arthur Jacot;Franck Gabriel;Clement Hongler,arthur.jacot@epfl.ch;franck.gabriel@epfl.ch;clement.hongler@epfl.ch,3;6;8,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne,481;481;481,38;38;38,
3544,3544,3544,3544,3544,3544,3544,3544,ICLR,2020,AutoQ: Automated Kernel-Wise Neural Network Quantization ,Qian Lou;Feng Guo;Minje Kim;Lantao Liu;Lei Jiang.,louqian@iu.edu;fengguo@iu.edu;minje@indiana.edu;lantao@iu.edu;jiang60@iu.edu,6;3;8;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:N/A:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,"Indiana University, Bloomington;Indiana University, Bloomington;University of Arizona;Indiana University, Bloomington;Indiana University, Bloomington",73;73;172;73;73,134;134;103;134;134,
3545,3545,3545,3545,3545,3545,3545,3545,ICLR,2020,Learning Self-Correctable Policies and Value Functions from Demonstrations with Negative Sampling,Yuping Luo;Huazhe Xu;Tengyu Ma,yupingl@cs.princeton.edu;huazhe_xu@eecs.berkeley.edu;tengyuma@stanford.edu,6;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0.0,yes,9/25/19,Princeton University;University of California Berkeley;Stanford University,31;5;4,6;13;4,
3546,3546,3546,3546,3546,3546,3546,3546,ICLR,2020,Projection-Based Constrained Policy Optimization,Tsung-Yen Yang;Justinian Rosca;Karthik Narasimhan;Peter J. Ramadge,ty3@princeton.edu;justinian.rosca@siemens.com;karthikn@cs.princeton.edu;ramadge@princeton.edu,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,Princeton University;Siemens Corporate Research;Princeton University;Princeton University,31;-1;31;31,6;-1;6;6,1;7
3547,3547,3547,3547,3547,3547,3547,3547,ICLR,2020,Variational Autoencoders for Highly Multivariate Spatial Point Processes Intensities,Baichuan Yuan;Xiaowei Wang;Jianxin Ma;Chang Zhou;Andrea L. Bertozzi;Hongxia Yang,ybcmath@gmail.com;daemon.wxw@alibaba-inc.com;majx13fromthu@gmail.com;ericzhou.zc@alibaba-inc.com;bertozzi@math.ucla.edu;yang.yhx@alibaba-inc.com,6;8;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.,Accept (Poster),0,3,0.0,yes,9/25/19,"University of California, Los Angeles;Alibaba Group;Tsinghua University;Alibaba Group;University of California, Los Angeles;Alibaba Group",20;-1;8;-1;20;-1,17;-1;23;-1;17;-1,5;1;8
3548,3548,3548,3548,3548,3548,3548,3548,ICLR,2020,Single Episode Policy Transfer in Reinforcement Learning,Jiachen Yang;Brenden Petersen;Hongyuan Zha;Daniel Faissol,yjiachen@gmail.com;petersen33@llnl.gov;zha@cc.gatech.edu;faissol1@llnl.gov,8;8;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,7,0.0,yes,9/25/19,Georgia Institute of Technology;Lawrence Livermore National Labs;Georgia Institute of Technology;Lawrence Livermore National Labs,13;-1;13;-1,38;-1;38;-1,
3549,3549,3549,3549,3549,3549,3549,3549,ICLR,2020,Transferable Perturbations of Deep Feature Distributions,Nathan Inkawhich;Kevin Liang;Lawrence Carin;Yiran Chen,nathan.inkawhich@duke.edu;kevin.liang@duke.edu;lcarin@duke.edu;yiran.chen@duke.edu,3;8;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,6,0.0,yes,9/25/19,Duke University;Duke University;Duke University;Duke University,47;47;47;47,20;20;20;20,4
3550,3550,3550,3550,3550,3550,3550,3550,ICLR,2020,Pruned Graph Scattering Transforms,Vassilis N. Ioannidis;Siheng Chen;Georgios B. Giannakis,ioann006@umn.edu;schen@merl.com;georgios@umn.edu,6;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,8,0.0,yes,9/25/19,"University of Minnesota, Minneapolis;Mitsubishi Electric Research Labs;University of Minnesota, Minneapolis",59;-1;59,79;-1;79,10;8
3551,3551,3551,3551,3551,3551,3551,3551,ICLR,2020,Efficient Probabilistic Logic Reasoning with Graph Neural Networks,Yuyu Zhang;Xinshi Chen;Yuan Yang;Arun Ramamurthy;Bo Li;Yuan Qi;Le Song,yuyu@gatech.edu;xinshi.chen@gatech.edu;yuanyang@gatech.edu;arun.ramamurthy@siemens.com;lbo@illinois.edu;yuan.qi@antfin.com;lsong@cc.gatech.edu,3;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,4.0,yes,9/25/19,"Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Siemens Corporate Research;University of Illinois, Urbana Champaign;Antfin;Georgia Institute of Technology",13;13;13;-1;3;-1;13,38;38;38;-1;48;-1;38,10
3552,3552,3552,3552,3552,3552,3552,3552,ICLR,2020,Decoupling Representation and Classifier for Long-Tailed Recognition,Bingyi Kang;Saining Xie;Marcus Rohrbach;Zhicheng Yan;Albert Gordo;Jiashi Feng;Yannis Kalantidis,kang@u.nus.edu;xiesaining@gmail.com;maroffm@gmail.com;zhicheng.yan@live.com;albert.gordo.s@gmail.com;elefjia@nus.edu.sg;ykalant@image.ntua.gr,6;8;6,I have read many papers in this area.:I carefully checked the experiments.:N/A:N/A;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,1.0,yes,9/25/19,National University of Singapore;Facebook;Facebook;;Facebook;National University of Singapore;National Technical University of Athens,16;-1;-1;-1;-1;16;323,25;-1;-1;-1;-1;25;776,6
3553,3553,3553,3553,3553,3553,3553,3553,ICLR,2020,Coherent Gradients: An Approach to Understanding Generalization in Gradient Descent-based Optimization,Satrajit Chatterjee,satrajit@gmail.com,8;8;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,21,0.0,yes,9/25/19,Google,-1,-1,
3554,3554,3554,3554,3554,3554,3554,3554,ICLR,2020,Environmental drivers of systematicity and generalization in a situated agent,Felix Hill;Andrew Lampinen;Rosalia Schneider;Stephen Clark;Matthew Botvinick;James L. McClelland;Adam Santoro,felixhill@google.com;lampinen@stanford.edo;rgschneider@google.com;clarkstephen@google.com;botvinick@google.com;jlmcc@google.com;adamsantoro@google.com,6;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,19,0.0,yes,9/25/19,Google;;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,8
3555,3555,3555,3555,3555,3555,3555,3555,ICLR,2020,Compositional Language Continual Learning,Yuanpeng Li;Liang Zhao;Kenneth Church;Mohamed Elhoseiny,yuanpeng16@gmail.com;lzhao4ever@gmail.com;kenneth.ward.church@gmail.com;mohamed.elhoseiny@gmail.com,3;6;8,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,4,0.0,yes,9/25/19,;;;KAUST,-1;-1;-1;128,-1;-1;-1;1397,3
3556,3556,3556,3556,3556,3556,3556,3556,ICLR,2020,Vid2Game: Controllable Characters Extracted from Real-World Videos,Oran Gafni;Lior Wolf;Yaniv Taigman,oran.gafni@gmail.com;wolf@fb.com;yaniv@fb.com,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,3,0.0,yes,9/25/19,Facebook;Facebook;Facebook,-1;-1;-1,-1;-1;-1,
3557,3557,3557,3557,3557,3557,3557,3557,ICLR,2020,Infinite-horizon Off-Policy Policy Evaluation with Multiple Behavior Policies,Xinyun Chen;Lu Wang;Yizhe Hang;Heng Ge;Hongyuan Zha,chenxinyun@cuhk.edu.cn;luwang@stu.ecnu.edu.cn;hangyhan@mail.ustc.edu.cn;hengge@mail.sdu.edu.cn;zhahy@cuhk.edu.cn,6;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,4,0.0,yes,9/25/19,Tsinghua University;Australian National University;University of Science and Technology of China;Shandong University;Tsinghua University,8;108;481;154;8,23;50;80;658;23,
3558,3558,3558,3558,3558,3558,3558,3558,ICLR,2020,Learning Efficient Parameter Server Synchronization Policies for Distributed SGD,Rong Zhu;Sheng Yang;Andreas Pfadler;Zhengping Qian;Jingren Zhou,red.zr@alibaba-inc.com;yangsheng@hit.edu.cn;andreaswernerrober@alibaba-inc.com;zhengping.qzp@alibaba-inc.com;jingren.zhou@alibaba-inc.com,6;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,9,0.0,yes,9/25/19,Alibaba Group;Harbin Institute of Technology;Alibaba Group;Alibaba Group;Alibaba Group,-1;172;-1;-1;-1,-1;424;-1;-1;-1,
3559,3559,3559,3559,3559,3559,3559,3559,ICLR,2020,Unsupervised Clustering using Pseudo-semi-supervised Learning,Divam Gupta;Ramachandran Ramjee;Nipun Kwatra;Muthian Sivathanu,divam@cmu.edu;ramjee@microsoft.com;nipun.kwatra@microsoft.com;muthian@microsoft.com,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0.0,yes,9/25/19,Carnegie Mellon University;Microsoft;Microsoft;Microsoft,1;-1;-1;-1,27;-1;-1;-1,10
3560,3560,3560,3560,3560,3560,3560,3560,ICLR,2020,Tree-Structured Attention with Hierarchical Accumulation,Xuan-Phi Nguyen;Shafiq Joty;Steven Hoi;Richard Socher,nxphi47@gmail.com;sjoty@salesforce.com,8;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,0,0.0,yes,9/25/19,National Taiwan University;SalesForce.com,86;-1,120;-1,3
3561,3561,3561,3561,3561,3561,3561,3561,ICLR,2020,Sparse Coding with Gated Learned ISTA,Kailun Wu;Yiwen Guo;Ziang Li;Changshui Zhang,wukl14@mails.tsinghua.edu.cn;guoyiwen.ai@bytedance.com;liza19@mails.tsinghua.edu.cn;zcs@mail.tsinghua.edu.cn,8;8;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Accept (Spotlight),0,4,0.0,yes,9/25/19,Tsinghua University;Bytedance;Tsinghua University;Tsinghua University,8;-1;8;8,23;-1;23;23,
3562,3562,3562,3562,3562,3562,3562,3562,ICLR,2020,A Signal Propagation Perspective for Pruning Neural Networks at Initialization,Namhoon Lee;Thalaiyasingam Ajanthan;Stephen Gould;Philip H. S. Torr,namhoon@robots.ox.ac.uk;thalaiyasingam.ajanthan@anu.edu.au;stephen.gould@anu.edu.au;phst@robots.ox.ac.uk,8;3;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Spotlight),0,3,0.0,yes,9/25/19,University of Oxford;Australian National University;Australian National University;University of Oxford,50;108;108;50,1;50;50;1,
3563,3563,3563,3563,3563,3563,3563,3563,ICLR,2020,Functional vs. parametric equivalence of ReLU networks,Mary Phuong;Christoph H. Lampert,bphuong@ist.ac.at;chl@ist.ac.at,3;8;6,I have published in this field for several years.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:N/A:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:N/A:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),1,4,0.0,yes,9/25/19,Institute of Science and Technology Austria;Institute of Science and Technology Austria,481;481,1397;1397,1
3564,3564,3564,3564,3564,3564,3564,3564,ICLR,2020,Jacobian Adversarially Regularized Networks for Robustness,Alvin Chan;Yi Tay;Yew Soon Ong;Jie Fu,guoweial001@e.ntu.edu.sg;ytay017@e.ntu.edu.sg;asysong@ntu.edu.sg;jie.fu@polymtl.ca,6;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,1.0,yes,9/25/19,National Taiwan University;National Taiwan University;National Taiwan University;Polytechnique Montreal,86;86;86;390,120;120;120;1397,5;4
3565,3565,3565,3565,3565,3565,3565,3565,ICLR,2020,Reinforced active learning for image segmentation,Arantxa Casanova;Pedro O. Pinheiro;Negar Rostamzadeh;Christopher J. Pal,arantxa.casanova-paga@polymtl.ca;pedro@opinheiro.com;negar@elementai.com;chris.j.pal@gmail.com,6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0.0,yes,9/25/19,Polytechnique Montreal;Opinheiro;Element AI;Ecole Polytechnique de Montreal,390;-1;-1;390,1397;-1;-1;1397,1;2
3566,3566,3566,3566,3566,3566,3566,3566,ICLR,2020,Maximum Likelihood Constraint Inference for Inverse Reinforcement Learning,Dexter R.R. Scobee;S. Shankar Sastry,dscobee@eecs.berkeley.edu;sastry@eecs.berkeley.edu,3;6;6;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,4,0.0,yes,9/25/19,University of California Berkeley;University of California Berkeley,5;5,13;13,
3567,3567,3567,3567,3567,3567,3567,3567,ICLR,2020,Weakly Supervised Disentanglement with Guarantees,Rui Shu;Yining Chen;Abhishek Kumar;Stefano Ermon;Ben Poole,ruishu@stanford.edu;cynnjjs@stanford.edu;abhishk@google.com;ermon@cs.stanford.edu;pooleb@google.com,8;8;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),1,10,0.0,yes,9/25/19,Stanford University;Stanford University;Google;Stanford University;Google,4;4;-1;4;-1,4;4;-1;4;-1,
3568,3568,3568,3568,3568,3568,3568,3568,ICLR,2020,PCMC-Net: Feature-based Pairwise Choice Markov Chains,Alix Lhéritier,alherit@gmail.com,8;6;6;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0.0,yes,9/25/19,Amadeus IT Group,-1,-1,
3569,3569,3569,3569,3569,3569,3569,3569,ICLR,2020,Capsules with Inverted Dot-Product Attention Routing,Yao-Hung Hubert Tsai;Nitish Srivastava;Hanlin Goh;Ruslan Salakhutdinov,yaohungt@cs.cmu.edu;nitish_srivastava@apple.com;hanlin@apple.com;rsalakhutdinov@apple.com,3;6;8,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),1,4,0.0,yes,9/25/19,Carnegie Mellon University;Apple;Apple;Apple,1;-1;-1;-1,27;-1;-1;-1,
3570,3570,3570,3570,3570,3570,3570,3570,ICLR,2020,Contrastive Representation Distillation,Yonglong Tian;Dilip Krishnan;Phillip Isola,yonglong@mit.edu;dilipkay@google.com;phillipi@mit.edu,3;6;6,I do not know much about this area.:I carefully checked the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,Massachusetts Institute of Technology;Google;Massachusetts Institute of Technology,2;-1;2,5;-1;5,
3571,3571,3571,3571,3571,3571,3571,3571,ICLR,2020,Variance Reduction With Sparse Gradients,Melih Elibol;Lihua Lei;Michael I. Jordan,elibol@cs.berkeley.edu;lihualei@stanford.edu;jordan@cs.berkeley.edu,6;3;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0.0,yes,9/25/19,University of California Berkeley;Stanford University;University of California Berkeley,5;4;5,13;4;13,3
3572,3572,3572,3572,3572,3572,3572,3572,ICLR,2020,FSNet: Compression of Deep Convolutional Neural Networks by Filter Summary,Yingzhen Yang;Jiahui Yu;Nebojsa Jojic;Jun Huan;Thomas S. Huang,superyyzg@gmail.com;jyu79@illinois.edu;jojic@microsoft.com;lukehuan@shenshangtech.com;t-huang1@illinois.edu,8;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,2,0.0,yes,9/25/19,"Independent Researcher;University of Illinois, Urbana Champaign;Microsoft;Shenshangtech;University of Illinois, Urbana Champaign",-1;3;-1;-1;3,-1;48;-1;-1;48,2
3573,3573,3573,3573,3573,3573,3573,3573,ICLR,2020,RNA Secondary Structure Prediction By Learning Unrolled Algorithms,Xinshi Chen;Yu Li;Ramzan Umarov;Xin Gao;Le Song,xinshi.chen@gatech.edu;yu.li@kaust.edu.sa;ramzan.umarov@kaust.edu.sa;xin.gao@kaust.edu.sa;lsong@cc.gatech.edu,8;6;8;8,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Talk),0,12,0.0,yes,9/25/19,Georgia Institute of Technology;KAUST;KAUST;KAUST;Georgia Institute of Technology,13;128;128;128;13,38;1397;1397;1397;38,
3574,3574,3574,3574,3574,3574,3574,3574,ICLR,2020,The Variational Bandwidth Bottleneck: Stochastic Evaluation on an Information Budget,Anirudh Goyal;Yoshua Bengio;Matthew Botvinick;Sergey Levine,anirudhgoyal9119@gmail.com;yoshua.bengio@mila.quebec;botvinick@google.com;svlevine@eecs.berkeley.edu,6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,10,1.0,yes,9/25/19,University of Montreal;University of Montreal;Google;University of California Berkeley,128;128;-1;5,85;85;-1;13,8
3575,3575,3575,3575,3575,3575,3575,3575,ICLR,2020,Mixout: Effective Regularization to Finetune Large-scale Pretrained Language Models,Cheolhyoung Lee;Kyunghyun Cho;Wanmo Kang,bloodwass@kaist.ac.kr;kyunghyun.cho@nyu.edu;wanmo.kang@kaist.edu,6;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,5,1.0,yes,9/25/19,Korea Advanced Institute of Science and Technology;New York University;KAIST,481;25;20,110;29;110,3;8
3576,3576,3576,3576,3576,3576,3576,3576,ICLR,2020,Novelty Detection Via Blurring,Sungik Choi;Sae-Young Chung,si_choi@kaist.ac.kr;schung@kaist.ac.kr,6;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,6,0.0,yes,9/25/19,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,481;481,110;110,5;4
3577,3577,3577,3577,3577,3577,3577,3577,ICLR,2020,Locality and Compositionality in Zero-Shot Learning,Tristan Sylvain;Linda Petrini;Devon Hjelm,tristan.sylvain@gmail.com;lindapetrini@gmail.com;devon.hjelm@microsoft.com,6;8;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,Microsoft;University of Amsterdam;Microsoft,-1;172;-1,-1;62;-1,6;8
3578,3578,3578,3578,3578,3578,3578,3578,ICLR,2020,"Prediction, Consistency, Curvature: Representation Learning for Locally-Linear Control",Nir Levine;Yinlam Chow;Rui Shu;Ang Li;Mohammad Ghavamzadeh;Hung Bui,nirlevine@google.com;yinlamchow@google.com;ruishu@stanford.edu;anglili@google.com;mgh@fb.com;v.hungbh1@vinai.io,6;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,3,0.0,yes,9/25/19,Google;Google;Stanford University;Google;Facebook;VinAI Research,-1;-1;4;-1;-1;-1,-1;-1;4;-1;-1;-1,1
3579,3579,3579,3579,3579,3579,3579,3579,ICLR,2020,Federated Learning with Matched Averaging,Hongyi Wang;Mikhail Yurochkin;Yuekai Sun;Dimitris Papailiopoulos;Yasaman Khazaeni,hongyiwang@cs.wisc.edu;mikhail.yurochkin@ibm.com;yuekai@umich.edu;dimitris@papail.io;yasaman.khazaeni@us.ibm.com,8;8;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,5,1.0,yes,9/25/19,University of Southern California;International Business Machines;University of Michigan;University of Wisconsin - Madison;International Business Machines,31;-1;8;14;-1,62;-1;21;51;-1,
3580,3580,3580,3580,3580,3580,3580,3580,ICLR,2020,Efficient and Information-Preserving Future Frame Prediction and Beyond,Wei Yu;Yichao Lu;Steve Easterbrook;Sanja Fidler,gnosis@cs.toronto.edu;yichao@cs.toronto.edu;sme@cs.toronto.edu;fidler@cs.toronto.edu,6;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",18;18;18;18,18;18;18;18,5;2
3581,3581,3581,3581,3581,3581,3581,3581,ICLR,2020,Tranquil Clouds: Neural Networks for Learning Temporally Coherent Features in Point Clouds,Lukas Prantl;Nuttapong Chentanez;Stefan Jeschke;Nils Thuerey,lukas.prantl@tum.de;nuttapong26@gmail.com;jeschke@stefan-jeschke.com;nils.thuerey@tum.de,6;8;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,3,0.0,yes,9/25/19,Technical University Munich;;Stefan-jeschke;Technical University Munich,53;-1;-1;53,43;-1;-1;43,5
3582,3582,3582,3582,3582,3582,3582,3582,ICLR,2020,Robust training with ensemble consensus,Jisoo Lee;Sae-Young Chung,jisoolee@kaist.ac.kr;schung@kaist.ac.kr,3;6;8,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,5,0.0,yes,9/25/19,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,481;481,110;110,
3583,3583,3583,3583,3583,3583,3583,3583,ICLR,2020,Asymptotics of Wide Networks from Feynman Diagrams,Ethan Dyer;Guy Gur-Ari,edyer@google.com;guyga@google.com,6;6;8,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,4,0.0,yes,9/25/19,Google;Google,-1;-1,-1;-1,
3584,3584,3584,3584,3584,3584,3584,3584,ICLR,2020,Unbiased Contrastive Divergence Algorithm for Training Energy-Based Latent Variable Models,Yixuan Qiu;Lingsong Zhang;Xiao Wang,yixuanq@andrew.cmu.edu;lingsong@purdue.edu;wangxiao@purdue.edu,6;8;8,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,3,2.0,yes,9/25/19,Carnegie Mellon University;Purdue University;Purdue University,1;27;27,27;88;88,
3585,3585,3585,3585,3585,3585,3585,3585,ICLR,2020,Frequency-based Search-control in Dyna,Yangchen Pan;Jincheng Mei;Amir-massoud Farahmand,pan6@ualberta.ca;jmei2@ualberta.ca;farahmand@vectorinstitute.ai,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A,Accept (Poster),0,3,0.0,yes,9/25/19,University of Alberta;University of Alberta;Vector Institute,100;100;-1,136;136;-1,1
3586,3586,3586,3586,3586,3586,3586,3586,ICLR,2020,Energy-based models for atomic-resolution protein conformations,Yilun Du;Joshua Meier;Jerry Ma;Rob Fergus;Alexander Rives,yilundu@mit.edu;jmeier@fb.com;maj@fb.com;robfergus@fb.com;arives@cs.nyu.edu,8;8;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,5,0.0,yes,9/25/19,Massachusetts Institute of Technology;Facebook;Facebook;Facebook;New York University,2;-1;-1;-1;25,5;-1;-1;-1;29,
3587,3587,3587,3587,3587,3587,3587,3587,ICLR,2020,Augmenting Non-Collaborative Dialog Systems with Explicit Semantic and Strategic Dialog History,Yiheng Zhou;Yulia Tsvetkov;Alan W Black;Zhou Yu,yihengz1@cs.cmu.edu;ytsvetko@cs.cmu.edu;awb@cs.cmu.edu;joyu@ucdavis.edu,6;3;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,"Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;University of California, Davis",1;1;1;79,27;27;27;55,
3588,3588,3588,3588,3588,3588,3588,3588,ICLR,2020,Discovering Motor Programs by Recomposing Demonstrations,Tanmay Shankar;Shubham Tulsiani;Lerrel Pinto;Abhinav Gupta,tanmayshankar@fb.com;shubhtuls@fb.com;lerrel.pinto@gmail.com;abhinavg@cs.cmu.edu,3;8;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Accept (Poster),0,9,0.0,yes,9/25/19,Facebook;Facebook;University of California Berkeley;Carnegie Mellon University,-1;-1;5;1,-1;-1;13;27,
3589,3589,3589,3589,3589,3589,3589,3589,ICLR,2020,Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers,Junjie LIU;Zhe XU;Runbin SHI;Ray C. C. Cheung;Hayden K.H. So,jjliu@eee.hku.hk;zhexu22-c@my.cityu.edu.hk;rbshi@eee.hku.hk;r.cheung@cityu.edu.hk;hso@eee.hku.hk,6;3;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,3.0,yes,9/25/19,The University of Hong Kong;City University of Hong Kong;The University of Hong Kong;City University of Hong Kong;The University of Hong Kong,92;92;92;92;92,35;35;35;35;35,
3590,3590,3590,3590,3590,3590,3590,3590,ICLR,2020,RTFM: Generalising to New Environment Dynamics via Reading,Victor Zhong;Tim Rocktäschel;Edward Grefenstette,victor@victorzhong.com;tim.rocktaeschel@gmail.com;egrefen@gmail.com,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:N/A:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),1,4,0.0,yes,9/25/19,University of Washington;Facebook AI Research;Facebook,6;-1;-1,26;-1;-1,
3591,3591,3591,3591,3591,3591,3591,3591,ICLR,2020,Causal Discovery with Reinforcement Learning,Shengyu Zhu;Ignavier Ng;Zhitang Chen,zhushengyu@huawei.com;ignavierng@cs.toronto.edu;chenzhitang2@huawei.com,8;8;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,6,2.0,yes,9/25/19,"Huawei Technologies Ltd.;Department of Computer Science, University of Toronto;Huawei Technologies Ltd.",-1;18;-1,-1;18;-1,10
3592,3592,3592,3592,3592,3592,3592,3592,ICLR,2020,FEW-SHOT LEARNING ON GRAPHS VIA SUPER-CLASSES BASED ON GRAPH SPECTRAL MEASURES,Jatin Chauhan;Deepak Nathani;Manohar Kaul,chauhanjatin100@gmail.com;deepakn1019@gmail.com;mkaul@iith.ac.in,6;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,6,0.0,yes,9/25/19,Indian Institute of Technology Hyderabad;Indian Institute of Technology Hyderabad;Indian Institute of Technology Hyderabad,205;205;205,713;713;713,6;10
3593,3593,3593,3593,3593,3593,3593,3593,ICLR,2020,Controlling generative models with continuous factors of variations,Antoine Plumerault;Hervé Le Borgne;Céline Hudelot,antoine.plumerault@cea.fr;herve.le-borgne@cea.fr;celine.hudelot@centralesupelec.fr,6;6;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,CEA;CEA;CentraleSupelec,233;233;481,1027;1027;534,3;5;2
3594,3594,3594,3594,3594,3594,3594,3594,ICLR,2020,Defending Against Physically Realizable Attacks on Image Classification,Tong Wu;Liang Tong;Yevgeniy Vorobeychik,tongwu@wustl.edu;liangtong@wustl.edu;yvorobeychik@wustl.edu,8;8;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Spotlight),0,9,0.0,yes,9/25/19,"Washington University, St. Louis;Washington University, St. Louis;Washington University, St. Louis",100;100;100,52;52;52,4
3595,3595,3595,3595,3595,3595,3595,3595,ICLR,2020,A Neural Dirichlet Process Mixture Model for Task-Free Continual Learning,Soochan Lee;Junsoo Ha;Dongsu Zhang;Gunhee Kim,soochan.lee@vision.snu.ac.kr;junsooha@hanyang.ac.kr;96lives@snu.ac.kr;gunhee@snu.ac.kr,6;6;8,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,1.0,yes,9/25/19,Seoul National University;Hanyang University;Seoul National University;Seoul National University,41;233;41;41,64;393;64;64,5;11
3596,3596,3596,3596,3596,3596,3596,3596,ICLR,2020,HiLLoC: lossless image compression with hierarchical latent variable models,James Townsend;Thomas Bird;Julius Kunze;David Barber,james.townsend@cs.ucl.ac.uk;thomas.bird@cs.ucl.ac.uk;julius.kunze@cs.ucl.ac.uk;david.barber@ucl.ac.uk,8;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,4,0.0,yes,9/25/19,University College London;University College London;University College London;University College London,50;50;50;50,15;15;15;15,5
3597,3597,3597,3597,3597,3597,3597,3597,ICLR,2020,Multilingual Alignment of Contextual Word Representations,Steven Cao;Nikita Kitaev;Dan Klein,stevencao@berkeley.edu;kitaev@berkeley.edu;klein@berkeley.edu,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),0,8,0.0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,13;13;13,6
3598,3598,3598,3598,3598,3598,3598,3598,ICLR,2020,Jelly Bean World: A Testbed for Never-Ending Learning,Emmanouil Antonios Platanios;Abulhair Saparov;Tom Mitchell,e.a.platanios@cs.cmu.edu;asaparov@cs.cmu.edu;tom.mitchell@cs.cmu.edu,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A,Accept (Poster),0,5,0.0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1,27;27;27,
3599,3599,3599,3599,3599,3599,3599,3599,ICLR,2020,V-MPO: On-Policy Maximum a Posteriori Policy Optimization for Discrete and Continuous Control,H. Francis Song;Abbas Abdolmaleki;Jost Tobias Springenberg;Aidan Clark;Hubert Soyer;Jack W. Rae;Seb Noury;Arun Ahuja;Siqi Liu;Dhruva Tirumala;Nicolas Heess;Dan Belov;Martin Riedmiller;Matthew M. Botvinick,songf@google.com;aabdolmaleki@google.com;springenberg@google.com;aidanclark@google.com;soyer@google.com;jwrae@google.com;snoury@google.com;arahuja@google.com;liusiqi@google.com;dhruvat@google.com;heess@google.com;danbelov@google.com;riedmiller@google.com;botvinick@google.com,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),1,7,0.0,yes,9/25/19,Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,
3600,3600,3600,3600,3600,3600,3600,3600,ICLR,2020,Deep Graph Matching Consensus,Matthias Fey;Jan E. Lenssen;Christopher Morris;Jonathan Masci;Nils M. Kriege,matthias.fey@tu-dortmund.de;janeric.lenssen@udo.edu;christopher.morris@tu-dortmund.de;jonathan@nnaisense.com;nils.kriege@tu-dortmund.de,6;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,6,0.0,yes,9/25/19,TU Dortmund;TU Dortmund University;TU Dortmund;NNAISENSE;TU Dortmund,233;233;233;-1;233,354;354;354;-1;354,2;10
3601,3601,3601,3601,3601,3601,3601,3601,ICLR,2020,CATER: A diagnostic dataset for Compositional Actions & TEmporal Reasoning,Rohit Girdhar;Deva Ramanan,rgirdhar@cs.cmu.edu;deva@cs.cmu.edu,8;8;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Talk),0,3,1.0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University,1;1,27;27,2
3602,3602,3602,3602,3602,3602,3602,3602,ICLR,2020,Differentiable Reasoning over a Virtual Knowledge Base,Bhuwan Dhingra;Manzil Zaheer;Vidhisha Balachandran;Graham Neubig;Ruslan Salakhutdinov;William W. Cohen,bdhingra@andrew.cmu.edu;manzilzaheer@google.com;vbalacha@andrew.cmu.edu;gneubig@cs.cmu.edu;rsalakhu@cs.cmu.edu;wcohen@google.com,8;8;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,6,0.0,yes,9/25/19,Carnegie Mellon University;Google;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Google,1;-1;1;1;1;-1,27;-1;27;27;27;-1,3
3603,3603,3603,3603,3603,3603,3603,3603,ICLR,2020,Finding and Visualizing Weaknesses of Deep Reinforcement Learning Agents,Christian Rupprecht;Cyril Ibrahim;Christopher J. Pal,christian.rupprecht@eng.ox.ac.uk;cyril.ibrahim@elementai.com;christopher.pal@polymtl.ca,6;3;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,4,0.0,yes,9/25/19,University of Oxford;Element AI;Polytechnique Montreal,50;-1;390,1;-1;1397,5
3604,3604,3604,3604,3604,3604,3604,3604,ICLR,2020,Intriguing Properties of Adversarial Training at Scale,Cihang Xie;Alan Yuille,cihangxie306@gmail.com;alan.l.yuille@gmail.com,6;8;6,I have published one or two papers in this area.:I carefully checked the experiments.:I made a quick assessment of this paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Poster),0,4,1.0,yes,9/25/19,Johns Hopkins University;Johns Hopkins University,73;73,12;12,4
3605,3605,3605,3605,3605,3605,3605,3605,ICLR,2020,Four Things Everyone Should Know to Improve Batch Normalization,Cecilia Summers;Michael J. Dinneen,ceciliasummers07@gmail.com;mjd@cs.auckland.ac.nz,6;3;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,2.0,yes,9/25/19,University of Auckland;University of Auckland,266;266,177;177,
3606,3606,3606,3606,3606,3606,3606,3606,ICLR,2020,Effect of Activation Functions on the Training of Overparametrized Neural Nets,Abhishek Panigrahi;Abhishek Shetty;Navin Goyal,abhishekpanigrahi034@gmail.com;ashetty1995@gmail.com;navingo@microsoft.com,8;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,2,0.0,yes,9/25/19,Microsoft;Cornell University;Microsoft,-1;7;-1,-1;19;-1,1
3607,3607,3607,3607,3607,3607,3607,3607,ICLR,2020,Differentiation of Blackbox Combinatorial Solvers,Marin Vlastelica Pogančić;Anselm Paulus;Vit Musil;Georg Martius;Michal Rolinek,marin.vlastelica@tue.mpg.de;anselm.paulus@tuebingen.mpg.de;vejtek@atrey.karlin.mff.cuni.cz;georg.martius@tuebingen.mpg.de;michal.rolinek@tuebingen.mpg.de,8;8;8,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,6,0.0,yes,9/25/19,"Max Planck Institute for Intelligent Systems, Max-Planck Institute;Max Planck Institute for Intelligent Systems, Max-Planck Institute;Charles University, Prague;Max Planck Institute for Intelligent Systems, Max-Planck Institute;Max Planck Institute for Intelligent Systems, Max-Planck Institute",-1;-1;481;-1;-1,-1;-1;495;-1;-1,
3608,3608,3608,3608,3608,3608,3608,3608,ICLR,2020,Learning Heuristics for Quantified Boolean Formulas through Reinforcement Learning,Gil Lederman;Markus Rabe;Sanjit Seshia;Edward A. Lee,gilled@berkeley.edu;mrabe@google.com;sshesia@eecs.berkeley.edu;eal@eecs.berkeley.edu,3;8;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,University of California Berkeley;Google;University of California Berkeley;University of California Berkeley,5;-1;5;5,13;-1;13;13,
3609,3609,3609,3609,3609,3609,3609,3609,ICLR,2020,Inductive representation learning on temporal graphs,da Xu;chuanwei ruan;evren korpeoglu;sushant kumar;kannan achan,da.xu@walmartlabs.com;ruanchuanwei@gmail.com;ekorpeoglu@walmart.com;skumar4@walmartlabs.com;kachan@walmartlabs.com,8;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),1,4,0.0,yes,9/25/19,University of California Berkeley;;Walmart;Walmartlabs;Walmartlabs,5;-1;-1;-1;-1,13;-1;-1;-1;-1,1;10
3610,3610,3610,3610,3610,3610,3610,3610,ICLR,2020,Multi-Scale Representation Learning  for Spatial Feature Distributions using Grid Cells,Gengchen Mai;Krzysztof Janowicz;Bo Yan;Rui Zhu;Ling Cai;Ni Lao,gengchen_mai@geog.ucsb.edu;janowicz@ucsb.edu;boyan1@linkedin.com;ruizhu@geog.ucsb.edu;lingcai@ucsb.edu;noon99@gmail.com,8;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,7,1.0,yes,9/25/19,UC Santa Barbara;UC Santa Barbara;LinkedIn;UC Santa Barbara;UC Santa Barbara;mosaix.ai,38;38;-1;38;38;-1,57;57;-1;57;57;-1,3
3611,3611,3611,3611,3611,3611,3611,3611,ICLR,2020,Efficient Riemannian Optimization on the Stiefel Manifold via the Cayley Transform,Jun Li;Fuxin Li;Sinisa Todorovic,liju2@oregonstate.edu;fuxin.li@oregonstate.edu;sinisa@oregonstate.edu,3;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,9,2.0,yes,9/25/19,Oregon State University;Oregon State University;Oregon State University,77;77;77,373;373;373,9
3612,3612,3612,3612,3612,3612,3612,3612,ICLR,2020,"Neural tangent kernels, transportation mappings, and universal approximation",Ziwei Ji;Matus Telgarsky;Ruicheng Xian,ziweiji2@illinois.edu;mjt@illinois.edu;rxian2@illinois.edu,6;8,I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:N/A:N/A:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,1.0,yes,9/25/19,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",3;3;3,48;48;48,1
3613,3613,3613,3613,3613,3613,3613,3613,ICLR,2020,Dream to Control: Learning Behaviors by Latent Imagination,Danijar Hafner;Timothy Lillicrap;Jimmy Ba;Mohammad Norouzi,mail@danijar.com;countzero@google.com;jba@cs.toronto.edu;mnorouzi@google.com,6;8;6;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),1,6,1.0,yes,9/25/19,"Department of Computer Science, University of Toronto;Google;Department of Computer Science, University of Toronto;Google",18;-1;18;-1,18;-1;18;-1,
3614,3614,3614,3614,3614,3614,3614,3614,ICLR,2020,From Inference to Generation: End-to-end Fully Self-supervised Generation of Human Face from Speech,Hyeong-Seok Choi;Changdae Park;Kyogu Lee,kekepa15@snu.ac.kr;cdpark@connect.ust.hk;kglee@snu.ac.kr,8;6;3,I have read many papers in this area.:I carefully checked the experiments.:N/A:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),1,5,0.0,yes,9/25/19,Seoul National University;The Hong Kong University of Science and Technology;Seoul National University,41;39;41,64;47;64,5
3615,3615,3615,3615,3615,3615,3615,3615,ICLR,2020,Neural Symbolic Reader: Scalable Integration of Distributed and Symbolic Representations for Reading Comprehension,Xinyun Chen;Chen Liang;Adams Wei Yu;Denny Zhou;Dawn Song;Quoc V. Le,xinyun.chen@berkeley.edu;crazydonkey@google.com;adamsyuwei@google.com;dennyzhou@google.com;dawnsong.travel@gmail.com;qvl@google.com,8;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Spotlight),0,8,0.0,yes,9/25/19,University of California Berkeley;Google;Google;Google;University of California Berkeley;Google,5;-1;-1;-1;5;-1,13;-1;-1;-1;13;-1,
3616,3616,3616,3616,3616,3616,3616,3616,ICLR,2020,Population-Guided Parallel Policy Search for Reinforcement Learning,Whiyoung Jung;Giseung Park;Youngchul Sung,wy.jung@kaist.ac.kr;gs.park@kaist.ac.kr;ycsung@kaist.ac.kr,6;8;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,481;481;481,110;110;110,
3617,3617,3617,3617,3617,3617,3617,3617,ICLR,2020,Abstract Diagrammatic Reasoning with Multiplex Graph Networks,Duo Wang;Mateja Jamnik;Pietro Lio,wd263@cam.ac.uk;mateja.jamnik@cl.cam.ac.uk;pietro.lio@cl.cam.ac.uk,6;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,7,0.0,yes,9/25/19,University of Cambridge;University of Cambridge;University of Cambridge,71;71;71,3;3;3,10
3618,3618,3618,3618,3618,3618,3618,3618,ICLR,2020,Logic and the 2-Simplicial Transformer,James Clift;Dmitry Doryn;Daniel Murfet;James Wallbridge,jamesedwardclift@gmail.com;dmitry.doryn@gmail.com;d.murfet@unimelb.edu.au;james.wallbridge@gmail.com,3;3;8,I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,8,0.0,yes,9/25/19,;;The University of Melbourne;,-1;-1;118;-1,-1;-1;32;-1,
3619,3619,3619,3619,3619,3619,3619,3619,ICLR,2020,Meta Reinforcement Learning with Autonomous Inference of Subtask Dependencies,Sungryull Sohn;Hyunjae Woo;Jongwook Choi;Honglak Lee,srsohn@umich.edu;hjwoo@umich.edu;jwook@umich.edu;honglak@eecs.umich.edu,6;6;6,I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0.0,yes,9/25/19,University of Michigan;University of Michigan;University of Michigan;University of Michigan,8;8;8;8,21;21;21;21,10;1;6
3620,3620,3620,3620,3620,3620,3620,3620,ICLR,2020,A Latent Morphology Model for Open-Vocabulary Neural Machine Translation,Duygu Ataman;Wilker Aziz;Alexandra Birch,duyguataman@gmail.com;will.aziz@gmail.com;a.birch@ed.ac.uk,6;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,0,0.0,yes,9/25/19,University of Zurich;University of Amsterdam;University of Edinburgh,143;172;33,90;62;30,3;2;8
3621,3621,3621,3621,3621,3621,3621,3621,ICLR,2020,Mutual Mean-Teaching: Pseudo Label Refinery for Unsupervised Domain Adaptation on Person Re-identification,Yixiao Ge;Dapeng Chen;Hongsheng Li,yxge@link.cuhk.edu.hk;chendapeng@sensetime.com;hsli@ee.cuhk.edu.hk,6;6;8,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,The Chinese University of Hong Kong;SenseTime Group Limited;The Chinese University of Hong Kong,59;-1;59,35;-1;35,
3622,3622,3622,3622,3622,3622,3622,3622,ICLR,2020,Variational Hetero-Encoder Randomized GANs for Joint Image-Text Modeling,Hao Zhang;Bo Chen;Long Tian;Zhengjue Wang;Mingyuan Zhou,zhanghao_xidian@163.com;bchen@mail.xidian.edu.cn;tianlong_xidian@163.com;zhengjuewang@163.com;mingyuan.zhou@mccombs.utexas.edu,6;3;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,"163;Tsinghua University;163;Xidian University;University of Texas, Austin",-1;8;-1;481;22,-1;23;-1;919;38,5;4;11
3623,3623,3623,3623,3623,3623,3623,3623,ICLR,2020,Toward Evaluating Robustness of Deep Reinforcement Learning with Continuous Control,Tsui-Wei Weng;Krishnamurthy (Dj) Dvijotham*;Jonathan Uesato*;Kai Xiao*;Sven Gowal*;Robert Stanforth*;Pushmeet Kohli,twweng@mit.edu;dvij@google.com;juesato@google.com;kaix@mit.edu;sgowal@google.com;stanforth@google.com;pushmeet@google.com,6;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,8,0.0,yes,9/25/19,Massachusetts Institute of Technology;Google;Google;Massachusetts Institute of Technology;Google;Google;Google,2;-1;-1;2;-1;-1;-1,5;-1;-1;5;-1;-1;-1,4
3624,3624,3624,3624,3624,3624,3624,3624,ICLR,2020,BlockSwap: Fisher-guided Block Substitution for Network Compression on a Budget,Jack Turner;Elliot J. Crowley;Michael O'Boyle;Amos Storkey;Gavin Gray,jack.turner@ed.ac.uk;elliot.j.crowley@ed.ac.uk;mob@inf.ed.ac.uk;a.storkey@ed.ac.uk;g.d.b.gray@ed.ac.uk,6;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,University of Edinburgh;University of Edinburgh;University of Edinburgh;University of Edinburgh;University of Edinburgh,33;33;33;33;33,30;30;30;30;30,
3625,3625,3625,3625,3625,3625,3625,3625,ICLR,2020,Decoding As Dynamic Programming For Recurrent Autoregressive Models,Najam Zaidi;Trevor Cohn;Gholamreza Haffari,syed.zaidi1@monash.edu;t.cohn@unimelb.edu.au;reza.haffari@gmail.com,8;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Accept (Poster),0,4,0.0,yes,9/25/19,Monash University;The University of Melbourne;,118;118;-1,75;32;-1,10
3626,3626,3626,3626,3626,3626,3626,3626,ICLR,2020,LEARNING EXECUTION THROUGH NEURAL CODE FUSION,Zhan Shi;Kevin Swersky;Daniel Tarlow;Parthasarathy Ranganathan;Milad Hashemi,zshi17@cs.utexas.edu;kswersky@google.com;dtarlow@google.com;parthas@google.com;miladh@google.com,6;8;3,I do not know much about this area.:I assessed the sensibility of the experiments.:N/A:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Accept (Poster),0,3,0.0,yes,9/25/19,"University of Texas, Austin;Google;Google;Google;Google",22;-1;-1;-1;-1,38;-1;-1;-1;-1,6;10
3627,3627,3627,3627,3627,3627,3627,3627,ICLR,2020,Detecting and Diagnosing Adversarial Images with Class-Conditional Capsule Reconstructions,Yao Qin;Nicholas Frosst;Sara Sabour;Colin Raffel;Garrison Cottrell;Geoffrey Hinton,yaq007@eng.ucsd.edu;frosst@google.com;sasabour@google.com;craffel@google.com;gary@eng.ucsd.edu;geoffhinton@google.com,6;8;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,"University of California, San Diego;Google;Google;Google;University of California, San Diego;Google",11;-1;-1;-1;11;-1,31;-1;-1;-1;31;-1,4
3628,3628,3628,3628,3628,3628,3628,3628,ICLR,2020,Reconstructing continuous distributions of 3D protein structure from cryo-EM images,Ellen D. Zhong;Tristan Bepler;Joseph H. Davis;Bonnie Berger,zhonge@mit.edu;tbepler@mit.edu;jhdavis@mit.edu;bab@mit.edu,8;8;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Spotlight),0,3,1.0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2,5;5;5;5,5
3629,3629,3629,3629,3629,3629,3629,3629,ICLR,2020,Sampling-Free Learning of Bayesian Quantized Neural Networks,Jiahao Su;Milan Cvitkovic;Furong Huang,jiahaosu@terpmail.umd.edu;mcvitkov@caltech.edu;furongh@cs.umd.edu,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,"University of Maryland, College Park;California Institute of Technology;University of Maryland, College Park",12;143;12,91;2;91,11
3630,3630,3630,3630,3630,3630,3630,3630,ICLR,2020,On Solving Minimax Optimization Locally: A Follow-the-Ridge Approach,Yuanhao Wang*;Guodong Zhang*;Jimmy Ba,yuanhao-16@mails.tsinghua.edu.cn;gdzhang@cs.toronto.edu;jba@cs.toronto.edu,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),1,21,0.0,yes,9/25/19,"Tsinghua University;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",8;18;18,23;18;18,5
3631,3631,3631,3631,3631,3631,3631,3631,ICLR,2020,Lagrangian Fluid Simulation with Continuous Convolutions,Benjamin Ummenhofer;Lukas Prantl;Nils Thuerey;Vladlen Koltun,benjamin.ummenhofer@intel.com;lukas.prantl@tum.de;nils.thuerey@tum.de;vkoltun@gmail.com,8;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0.0,yes,9/25/19,Intel;Technical University Munich;Technical University Munich;Intel,-1;53;53;-1,-1;43;43;-1,10
3632,3632,3632,3632,3632,3632,3632,3632,ICLR,2020,Stochastic Weight Averaging in Parallel: Large-Batch Training That Generalizes Well,Vipul Gupta;Santiago Akle Serrano;Dennis DeCoste,vipul_gupta@berkeley.edu;sakle@apple.com;ddecoste@apple.com,6;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,University of California Berkeley;Apple;Apple,5;-1;-1,13;-1;-1,2;8
3633,3633,3633,3633,3633,3633,3633,3633,ICLR,2020,Adversarial AutoAugment,Xinyu Zhang;Qiang Wang;Jian Zhang;Zhao Zhong,zhangxinyu10@huawei.com;wangqiang168@huawei.com;zhangjian157@huawei.com;zorro.zhongzhao@huawei.com,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,10,0.0,yes,9/25/19,Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.,-1;-1;-1;-1,-1;-1;-1;-1,4;8
3634,3634,3634,3634,3634,3634,3634,3634,ICLR,2020,Episodic Reinforcement Learning with Associative Memory,Guangxiang Zhu*;Zichuan Lin*;Guangwen Yang;Chongjie Zhang,guangxiangzhu@outlook.com;linzc16@mails.tsinghua.edu.cn;ygw@tsinghua.edu.cn;chongjie@tsinghua.edu.cn,6;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),1,4,0.0,yes,9/25/19,Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University,8;8;8;8,23;23;23;23,10
3635,3635,3635,3635,3635,3635,3635,3635,ICLR,2020,NAS-Bench-1Shot1: Benchmarking and Dissecting One-shot Neural Architecture Search,Arber Zela;Julien Siems;Frank Hutter,zelaa@cs.uni-freiburg.de;siemsj@cs.uni-freiburg.de;fh@cs.uni-freiburg.de,1;8;8,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0.0,yes,9/25/19,Universität Freiburg;Universität Freiburg;Universität Freiburg,118;118;118,85;85;85,
3636,3636,3636,3636,3636,3636,3636,3636,ICLR,2020,You Only Train Once: Loss-Conditional Training of Deep Networks,Alexey Dosovitskiy;Josip Djolonga,adosovitskiy@gmail.com;josip@djolonga.com,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),0,5,0.0,yes,9/25/19,Google;Google,-1;-1,-1;-1,5
3637,3637,3637,3637,3637,3637,3637,3637,ICLR,2020,Q-learning with UCB Exploration is Sample Efficient for Infinite-Horizon MDP,Yuanhao Wang;Kefan Dong;Xiaoyu Chen;Liwei Wang,yuanhao-16@mails.tsinghua.edu.cn;dkf16@mails.tsinghua.edu.cn;cxy30@pku.edu.cn;wanglw@cis.pku.edu.cn,6;6;6;6,I have read many papers in this area.:N/A:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:N/A:N/A:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:N/A:N/A:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,8,0.0,yes,9/25/19,Tsinghua University;Tsinghua University;Peking University;Peking University,8;8;22;22,23;23;24;24,5;1
3638,3638,3638,3638,3638,3638,3638,3638,ICLR,2020,Deep Audio Priors Emerge From Harmonic Convolutional Networks,Zhoutong Zhang;Yunyun Wang;Chuang Gan;Jiajun Wu;Joshua B. Tenenbaum;Antonio Torralba;William T. Freeman,ztzhang@mit.edu;wyy@mit.edu;ganchuang1990@gmail.com;jiajunwu@mit.edu;jbt@mit.edu;torralba@mit.edu;billf@mit.edu,6;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,6,1.0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;International Business Machines;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;-1;2;2;2;2,5;5;-1;5;5;5;5,8
3639,3639,3639,3639,3639,3639,3639,3639,ICLR,2020,Abductive Commonsense Reasoning,Chandra Bhagavatula;Ronan Le Bras;Chaitanya Malaviya;Keisuke Sakaguchi;Ari Holtzman;Hannah Rashkin;Doug Downey;Wen-tau Yih;Yejin Choi,chandrab@allenai.org;ronanlb@allenai.org;chaitanyam@allenai.org;keisukes@allenai.org;arih@allenai.org;hrashkin@uw.edu;dougd@allenai.org;scottyih@fb.com;yejinc@allenai.org,8;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,"Allen Institute for Artificial Intelligence;Allen Institute for Artificial Intelligence;Allen Institute for Artificial Intelligence;Allen Institute for Artificial Intelligence;Allen Institute for Artificial Intelligence;University of Washington, Seattle;Allen Institute for Artificial Intelligence;Facebook;Allen Institute for Artificial Intelligence",-1;-1;-1;-1;-1;6;-1;-1;-1,-1;-1;-1;-1;-1;26;-1;-1;-1,3
3640,3640,3640,3640,3640,3640,3640,3640,ICLR,2020,DeepHoyer: Learning Sparser Neural Network with Differentiable Scale-Invariant Sparsity Measures,Huanrui Yang;Wei Wen;Hai Li,huanrui.yang@duke.edu;wei.wen@duke.edu;hai.li@duke.edu,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,Duke University;Duke University;Duke University,47;47;47,20;20;20,
3641,3641,3641,3641,3641,3641,3641,3641,ICLR,2020,DeFINE: Deep Factorized Input Token Embeddings for Neural Sequence Modeling,Sachin Mehta;Rik Koncel-Kedziorski;Mohammad Rastegari;Hannaneh Hajishirzi,sacmehta@uw.edu;kedzior@uw.edu;mohammadr@allenai.org;hannaneh@washington.edu,6;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,13,1.0,yes,9/25/19,"University of Washington, Seattle;University of Washington, Seattle;Allen Institute for Artificial Intelligence;University of Washington",6;6;-1;6,26;26;-1;26,3
3642,3642,3642,3642,3642,3642,3642,3642,ICLR,2020,Never Give Up: Learning Directed Exploration Strategies,Adrià Puigdomènech Badia;Pablo Sprechmann;Alex Vitvitskyi;Daniel Guo;Bilal Piot;Steven Kapturowski;Olivier Tieleman;Martin Arjovsky;Alexander Pritzel;Andrew Bolt;Charles Blundell,adriap@google.com;psprechmann@google.com;avlife@google.com;danielguo@google.com;piot@google.com;skapturowski@google.com;tieleman@google.com;martinarjovsky@gmail.com;apritzel@google.com;abolt@google.com;cblundell@google.com,6;8;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0.0,yes,9/25/19,Google;Google;Google;Google;Google;Google;Google;New York University;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;25;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;29;-1;-1;-1,
3643,3643,3643,3643,3643,3643,3643,3643,ICLR,2020,Beyond Linearization: On Quadratic and Higher-Order Approximation of Wide Neural Networks,Yu Bai;Jason D. Lee,yubai.pku@gmail.com;jasondlee88@gmail.com,6;6;6,I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:N/A:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,1.0,yes,9/25/19,SalesForce.com;University of Southern California,-1;31,-1;62,1;8
3644,3644,3644,3644,3644,3644,3644,3644,ICLR,2020,Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity,Jingzhao Zhang;Tianxing He;Suvrit Sra;Ali Jadbabaie,jzhzhang@mit.edu;tianxing@mit.edu;suvrit@mit.edu;jadbabai@mit.edu,8;8;8,I have read many papers in this area.:I did not assess the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,8,0.0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2,5;5;5;5,1
3645,3645,3645,3645,3645,3645,3645,3645,ICLR,2020,Thieves on Sesame Street! Model Extraction of BERT-based APIs,Kalpesh Krishna;Gaurav Singh Tomar;Ankur P. Parikh;Nicolas Papernot;Mohit Iyyer,kalpesh@cs.umass.edu;gtomar@google.com;aparikh@google.com;papernot@google.com;miyyer@cs.umass.edu,8;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Accept (Poster),0,3,1.0,yes,9/25/19,"University of Massachusetts, Amherst;Google;Google;Google;University of Massachusetts, Amherst",28;-1;-1;-1;28,209;-1;-1;-1;209,3;4;6
3646,3646,3646,3646,3646,3646,3646,3646,ICLR,2020,Accelerating SGD with momentum for over-parameterized learning,Chaoyue Liu;Mikhail Belkin,liu.2656@buckeyemail.osu.edu;mbelkin@cse.ohio-state.edu,3;8;8,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,Ohio State University;Ohio State University,77;77,373;373,1;9
3647,3647,3647,3647,3647,3647,3647,3647,ICLR,2020,GraphZoom: A Multi-level Spectral Approach for Accurate and Scalable Graph Embedding,Chenhui Deng;Zhiqiang Zhao;Yongyu Wang;Zhiru Zhang;Zhuo Feng,cd574@cornell.edu;qzzhao@mtu.edu;yongyuw@mtu.edu;zhiruz@cornell.edu;zfeng12@stevens.edu,6;8;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,7,0.0,yes,9/25/19,Cornell University;Michigan Technological University;Michigan Technological University;Cornell University;Stevens Institute of Technology,7;323;323;7;154,19;1397;1397;19;605,10
3648,3648,3648,3648,3648,3648,3648,3648,ICLR,2020,Low-Resource Knowledge-Grounded Dialogue Generation,Xueliang Zhao;Wei Wu;Chongyang Tao;Can Xu;Dongyan Zhao;Rui Yan,xl.zhao@pku.edu.cn;wuwei@microsoft.com;chongyangtao@pku.edu.cn;can.xu@microsoft.com;zhaody@pku.edu.cn;ruiyan@pku.edu.cn,6;8;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,4,0.0,yes,9/25/19,Peking University;Microsoft;Peking University;Microsoft;Peking University;Peking University,22;-1;22;-1;22;22,24;-1;24;-1;24;24,
3649,3649,3649,3649,3649,3649,3649,3649,ICLR,2020,Diverse Trajectory Forecasting with Determinantal Point Processes,Ye Yuan;Kris M. Kitani,yyuan2@cs.cmu.edu;kkitani@cs.cmu.edu,6;6;8,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0.0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University,1;1,27;27,5
3650,3650,3650,3650,3650,3650,3650,3650,ICLR,2020,Estimating Gradients for Discrete Random Variables by Sampling without Replacement,Wouter Kool;Herke van Hoof;Max Welling,w.w.m.kool@uva.nl;h.c.vanhoof@uva.nl;m.welling@uva.nl,6;8;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Spotlight),0,6,0.0,yes,9/25/19,University of Amsterdam;University of Amsterdam;University of Amsterdam,172;172;172,62;62;62,
3651,3651,3651,3651,3651,3651,3651,3651,ICLR,2020,Maxmin Q-learning: Controlling the Estimation Bias of Q-learning,Qingfeng Lan;Yangchen Pan;Alona Fyshe;Martha White,qlan3@ualberta.ca;pan6@ualberta.ca;alona@ualberta.ca;whitem@ualberta.ca,6;3;8,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,11,0.0,yes,9/25/19,University of Alberta;University of Alberta;University of Alberta;University of Alberta,100;100;100;100,136;136;136;136,1;8
3652,3652,3652,3652,3652,3652,3652,3652,ICLR,2020,Editable Neural Networks,Anton Sinitsin;Vsevolod Plokhotnyuk;Dmitry Pyrkin;Sergei Popov;Artem Babenko,ant.sinitsin@gmail.com;vsevolod-pl@yandex.ru;alagaster@yandex.ru;sapopov@yandex-team.ru;artem.babenko@phystech.edu,6;3;8,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,8,0.0,yes,9/25/19,Higher School of Economics;Higher School of Economics;Higher School of Economics;Yandex;Moscow Institute of Physics and Technology,481;481;481;-1;481,251;251;251;-1;234,3
3653,3653,3653,3653,3653,3653,3653,3653,ICLR,2020,Rethinking Softmax Cross-Entropy Loss for Adversarial Robustness,Tianyu Pang;Kun Xu;Yinpeng Dong;Chao Du;Ning Chen;Jun Zhu,pty17@mails.tsinghua.edu.cn;kunxu.thu@gmail.com;dyp17@mails.tsinghua.edu.cn;duchao0726@gmail.com;ningchen@mail.tsinghua.edu.cn;dcszj@mail.tsinghua.edu.cn,6;6;8;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,14,0.0,yes,9/25/19,Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University,8;8;8;8;8;8,23;23;23;23;23;23,4;8
3654,3654,3654,3654,3654,3654,3654,3654,ICLR,2020,Self-labelling via simultaneous clustering and representation learning,Asano YM.;Rupprecht C.;Vedaldi A.,yuki@robots.ox.ac.uk;chrisr@robots.ox.ac.uk;vedaldi@robots.ox.ac.uk,8;3;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Spotlight),0,5,2.0,yes,9/25/19,University of Oxford;University of Oxford;University of Oxford,50;50;50,1;1;1,
3655,3655,3655,3655,3655,3655,3655,3655,ICLR,2020,Semi-Supervised Generative Modeling for Controllable Speech Synthesis,Raza Habib;Soroosh Mariooryad;Matt Shannon;Eric Battenberg;RJ Skerry-Ryan;Daisy Stanton;David Kao;Tom Bagby,raza.habib@cs.ucl.ac.uk;soroosh@google.com;mattshannon@google.com;ebattenberg@google.com;rjryan@google.com;daisy@google.com;davidkao@google.com;tombagby@google.com,6;8;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,9,0.0,yes,9/25/19,University College London;Google;Google;Google;Google;Google;Google;Google,50;-1;-1;-1;-1;-1;-1;-1,15;-1;-1;-1;-1;-1;-1;-1,5
3656,3656,3656,3656,3656,3656,3656,3656,ICLR,2020,EMPIR: Ensembles of Mixed Precision Deep Networks for Increased Robustness Against Adversarial Attacks,Sanchari Sen;Balaraman Ravindran;Anand Raghunathan,sen9@purdue.edu;ravi@cse.iitm.ac.in;raghunathan@purdue.edu,6;6;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,Purdue University;Indian Institute of Technology Madras;Purdue University,27;154;27,88;641;88,4
3657,3657,3657,3657,3657,3657,3657,3657,ICLR,2020,Training Recurrent Neural Networks Online by Learning Explicit State Variables,Somjit Nath;Vincent Liu;Alan Chan;Xin Li;Adam White;Martha White,somjit@ualberta.ca;vliu1@ualberta.ca;achan4@ualberta.ca;xzli@ualberta.ca;amw8@ualberta.ca;whitem@ualberta.ca,6;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,9,0.0,yes,9/25/19,University of Alberta;University of Alberta;University of Alberta;University of Alberta;University of Alberta;University of Alberta,100;100;100;100;100;100,136;136;136;136;136;136,9
3658,3658,3658,3658,3658,3658,3658,3658,ICLR,2020,MEMO: A Deep Network for Flexible Combination of Episodic Memories,Andrea Banino;Adrià Puigdomènech Badia;Raphael Köster;Martin J. Chadwick;Vinicius Zambaldi;Demis Hassabis;Caswell Barry;Matthew Botvinick;Dharshan Kumaran;Charles Blundell,abanino@google.com;adriap@google.com;rkoster@google.com;mjchadwick@google.com;vzambaldi@google.com;dhteam@google.com;caswell.barry@ucl.ac.uk;botvinick@google.com;dkumaran@google.com;cblundell@google.com,8;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,2,0.0,yes,9/25/19,Google;Google;Google;Google;Google;Google;University College London;Google;Google;Google,-1;-1;-1;-1;-1;-1;50;-1;-1;-1,-1;-1;-1;-1;-1;-1;15;-1;-1;-1,
3659,3659,3659,3659,3659,3659,3659,3659,ICLR,2020,Differentiable learning of numerical rules in knowledge graphs,Po-Wei Wang;Daria Stepanova;Csaba Domokos;J. Zico Kolter,poweiw@cs.cmu.edu;daria.stepanova@de.bosch.com;csaba.domokos@de.bosch.com;zkolter@cs.cmu.edu,3;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,6,0.0,yes,9/25/19,Carnegie Mellon University;Bosch;Bosch;Carnegie Mellon University,1;-1;-1;1,27;-1;-1;27,10
3660,3660,3660,3660,3660,3660,3660,3660,ICLR,2020,Multiplicative Interactions and Where to Find Them,Siddhant M. Jayakumar;Wojciech M. Czarnecki;Jacob Menick;Jonathan Schwarz;Jack Rae;Simon Osindero;Yee Whye Teh;Tim Harley;Razvan Pascanu,sidmj@google.com;lejlot@google.com;jmenick@google.com;schwarzjn@google.com;jwrae@google.com;osindero@google.com;ywteh@google.com;tharley@google.com;razp@google.com,8;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Poster),0,3,0.0,yes,9/25/19,Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1,
3661,3661,3661,3661,3661,3661,3661,3661,ICLR,2020,Recurrent neural circuits for contour detection,Drew Linsley*;Junkyung Kim*;Alekh Ashok;Thomas Serre,drew_linsley@brown.edu;junkyung_kim@brown.edu;alekh_karkada_ashok@brown.edu;thomas_serre@brown.edu,6;8;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,7,0.0,yes,9/25/19,Brown University;Brown University;Brown University;Brown University,67;67;67;67,53;53;53;53,2
3662,3662,3662,3662,3662,3662,3662,3662,ICLR,2020,Provable Benefit of Orthogonal Initialization in Optimizing Deep Linear Networks,Wei Hu;Lechao Xiao;Jeffrey Pennington,huwei@cs.princeton.edu;xlc@google.com;jpennin@google.com,6;8;3,I have published one or two papers in this area.:N/A:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,Princeton University;Google;Google,31;-1;-1,6;-1;-1,1
3663,3663,3663,3663,3663,3663,3663,3663,ICLR,2020,Sliced Cramer Synaptic Consolidation for Preserving Deeply Learned Representations,Soheil Kolouri;Nicholas A. Ketz;Andrea Soltoggio;Praveen K. Pilly,skolouri@hrl.com;naketz@hrl.com;a.soltoggio@lboro.ac.uk;pkpilly@hrl.com,8;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,2,0.0,yes,9/25/19,HRL Labs;HRL Labs;Loughborough University;HRL Labs,-1;-1;390;-1,-1;-1;374;-1,
3664,3664,3664,3664,3664,3664,3664,3664,ICLR,2020,Neural Outlier Rejection for Self-Supervised Keypoint Learning,Jiexiong Tang;Hanme Kim;Vitor Guizilini;Sudeep Pillai;Rares Ambrus,jiexiong@kth.se;hanme.kim@tri.global;vitor.guizilini@tri.global;sudeep.pillai@tri.global;rares.ambrus@tri.global,6;8;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,8,0.0,yes,9/25/19,"KTH Royal Institute of Technology, Stockholm, Sweden;Toyota Research Institute;Toyota Research Institute;Toyota Research Institute;Toyota Research Institute",128;-1;-1;-1;-1,222;-1;-1;-1;-1,
3665,3665,3665,3665,3665,3665,3665,3665,ICLR,2020,vq-wav2vec: Self-Supervised Learning of Discrete Speech Representations,Alexei Baevski;Steffen Schneider;Michael Auli,alexei.b@gmail.com;stes@fb.com;michael.auli@gmail.com,8;8;6;8,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,5,0.0,yes,9/25/19,Facebook;Facebook;Facebook,-1;-1;-1,-1;-1;-1,3
3666,3666,3666,3666,3666,3666,3666,3666,ICLR,2020,A Fair Comparison of Graph Neural Networks for Graph Classification,Federico Errica;Marco Podda;Davide Bacciu;Alessio Micheli,federico.errica@phd.unipi.it;marco.podda@di.unipi.it;bacciu@di.unipi.it;micheli@di.unipi.it,8;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,12,0.0,yes,9/25/19,University of Pisa;University of Pisa;University of Pisa;University of Pisa,233;233;233;233,366;366;366;366,10
3667,3667,3667,3667,3667,3667,3667,3667,ICLR,2020,Towards Hierarchical Importance Attribution: Explaining Compositional Semantics for Neural Sequence Models,Xisen Jin;Zhongyu Wei;Junyi Du;Xiangyang Xue;Xiang Ren,xisenjin@usc.edu;zywei@fudan.edu.cn;junyidu@usc.edu;xyxue@fudan.edu.cn;xiangren@usc.edu,6;8;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,7,0.0,yes,9/25/19,University of Southern California;Fudan University;University of Southern California;Fudan University;University of Southern California,31;79;31;79;31,62;109;62;109;62,3
3668,3668,3668,3668,3668,3668,3668,3668,ICLR,2020,Understanding Why Neural Networks Generalize Well Through GSNR of Parameters,Jinlong Liu;Yunzhi Bai;Guoqing Jiang;Ting Chen;Huayan Wang,ljlwykqh@126.com;yunzhi.bai@outlook.fr;jianggq@pku.edu.cn;roushi0322@sina.cn;wanghuayan@kuaishou.com,6;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,4,0.0,yes,9/25/19,126;Kuaishou;Peking University;;Kuaishou,-1;-1;22;-1;-1,-1;-1;24;-1;-1,8
3669,3669,3669,3669,3669,3669,3669,3669,ICLR,2020,Intrinsic Motivation for Encouraging Synergistic Behavior,Rohan Chitnis;Shubham Tulsiani;Saurabh Gupta;Abhinav Gupta,ronuchit@mit.edu;shubhtuls@fb.com;saurabhg@illinois.edu;abhinavg@cs.cmu.edu,6;8;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,"Massachusetts Institute of Technology;Facebook;University of Illinois, Urbana Champaign;Carnegie Mellon University",2;-1;3;1,5;-1;48;27,
3670,3670,3670,3670,3670,3670,3670,3670,ICLR,2020,Sharing Knowledge in Multi-Task Deep Reinforcement Learning,Carlo D'Eramo;Davide Tateo;Andrea Bonarini;Marcello Restelli;Jan Peters,carlo@robot-learning.de;davide@robot-learning.de;andrea.bonarini@polimi.it;marcello.restelli@polimi.it;peters@ias.tu-darmstadt.de,6;6;6,I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,TU Darmstadt;TU Darmstadt;Politecnico di Milano;Politecnico di Milano;TU Darmstadt,64;64;128;128;64,289;289;347;347;289,1
3671,3671,3671,3671,3671,3671,3671,3671,ICLR,2020,Provable Filter Pruning for Efficient Neural Networks,Lucas Liebenwein;Cenk Baykal;Harry Lang;Dan Feldman;Daniela Rus,lucasl@mit.edu;baykal@mit.edu;hlang08@gmail.com;dannyf.post@gmail.com;rus@csail.mit.edu,3;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0.0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;;University of Haifa;Massachusetts Institute of Technology,2;2;-1;172;2,5;5;-1;544;5,
3672,3672,3672,3672,3672,3672,3672,3672,ICLR,2020,Additive Powers-of-Two Quantization: An Efficient Non-uniform Discretization for Neural Networks,Yuhang Li;Xin Dong;Wei Wang,loafyuhang@gmail.com;xindong@g.harvard.edu;wangwei@comp.nus.edu.sg,6;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,4,1.0,yes,9/25/19,University of Electronic Science and Technology of China;Harvard University;National University of Singapore,481;39;16,628;7;25,
3673,3673,3673,3673,3673,3673,3673,3673,ICLR,2020,Variational Recurrent Models for Solving Partially Observable Control Tasks,Dongqi Han;Kenji Doya;Jun Tani,dongqi.han@oist.jp;doya@oist.jp;jun.tani@oist.jp,6;8;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0.0,yes,9/25/19,Okinawa Institute of Science and Technology Graduate University;Okinawa Institute of Science and Technology Graduate University;Okinawa Institute of Science and Technology Graduate University,-1;-1;-1,-1;-1;-1,
3674,3674,3674,3674,3674,3674,3674,3674,ICLR,2020,SEED RL: Scalable and Efficient Deep-RL with Accelerated Central Inference,Lasse Espeholt;Raphaël Marinier;Piotr Stanczyk;Ke Wang;Marcin Michalski‎,lespeholt@google.com;raphaelm@google.com;stanczyk@google.com;kewa@google.com;michalski@google.com,8;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Accept (Talk),0,4,0.0,yes,9/25/19,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,
3675,3675,3675,3675,3675,3675,3675,3675,ICLR,2020,Discriminative Particle Filter Reinforcement Learning for Complex Partial observations,Xiao Ma;Peter Karkus;David Hsu;Wee Sun Lee;Nan Ye,xiao-ma@comp.nus.edu.sg;karkus@comp.nus.edu.sg;dyhsu@comp.nus.edu.sg;leews@comp.nus.edu.sg;nan.ye@uq.edu.au,8;6;8,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,5,0.0,yes,9/25/19,National University of Singapore;National University of Singapore;National University of Singapore;National University of Singapore;University of Queensland,16;16;16;16;205,25;25;25;25;66,5
3676,3676,3676,3676,3676,3676,3676,3676,ICLR,2020,White Noise Analysis of Neural Networks,Ali Borji;Sikun Lin,aliborji@gmail.com;sikun@ucsb.edu,3;8;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,7,0.0,yes,9/25/19,University of Central Florida;UC Santa Barbara,77;38,609;57,4;2
3677,3677,3677,3677,3677,3677,3677,3677,ICLR,2020,RaCT: Toward Amortized Ranking-Critical Training For Collaborative Filtering ,Sam Lobel*;Chunyuan Li*;Jianfeng Gao;Lawrence Carin,samuel_lobel@brown.edu;chunyuan.li@microsoft.com;jfgao@microsoft.com;lcarin@duke.edu,8;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,3,0.0,yes,9/25/19,Brown University;Microsoft;Microsoft;Duke University,67;-1;-1;47,53;-1;-1;20,
3678,3678,3678,3678,3678,3678,3678,3678,ICLR,2020,Skip Connections Matter: On the Transferability of Adversarial Examples Generated with ResNets,Dongxian Wu;Yisen Wang;Shu-Tao Xia;James Bailey;Xingjun Ma,wu-dx16@mails.tsinghua.edu.cn;eewangyisen@gmail.com;xiast@sz.tsinghua.edu.cn;baileyj@unimelb.edu.au;xingjun.ma@unimelb.edu.au,8;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Spotlight),0,4,1.0,yes,9/25/19,Tsinghua University;Tsinghua University;Tsinghua University;The University of Melbourne;The University of Melbourne,8;8;8;118;118,23;23;23;32;32,4
3679,3679,3679,3679,3679,3679,3679,3679,ICLR,2020,The Local Elasticity of Neural Networks,Hangfeng He;Weijie Su,hangfeng@seas.upenn.edu;suw@wharton.upenn.edu,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0.0,yes,9/25/19,University of Pennsylvania;University of Pennsylvania,19;19,11;11,
3680,3680,3680,3680,3680,3680,3680,3680,ICLR,2020,Deep probabilistic subsampling for task-adaptive compressed sensing,Iris A.M. Huijben;Bastiaan S. Veeling;Ruud J.G. van Sloun,i.a.m.huijben@tue.nl;basveeling@gmail.com;r.j.g.v.sloun@tue.nl,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,8,1.0,yes,9/25/19,Eindhoven University of Technology;Google;Eindhoven University of Technology,205;-1;205,185;-1;185,
3681,3681,3681,3681,3681,3681,3681,3681,ICLR,2020,Simplified Action Decoder for Deep Multi-Agent Reinforcement Learning,Hengyuan Hu;Jakob N Foerster,hengyuan@fb.com;jakobfoerster@gmail.com,8;8;8,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Spotlight),0,9,0.0,yes,9/25/19,Facebook;Facebook,-1;-1,-1;-1,4
3682,3682,3682,3682,3682,3682,3682,3682,ICLR,2020,On the Need for Topology-Aware Generative Models for Manifold-Based Defenses,Uyeong Jang;Susmit Jha;Somesh Jha,wjang@cs.wisc.edu;susmit.jha@sri.com;jha@cs.wisc.edu,8;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0.0,yes,9/25/19,University of Southern California;SRI International;University of Southern California,31;-1;31,62;-1;62,5;4
3683,3683,3683,3683,3683,3683,3683,3683,ICLR,2020,Functional Regularisation for  Continual Learning with Gaussian Processes,Michalis K. Titsias;Jonathan Schwarz;Alexander G. de G. Matthews;Razvan Pascanu;Yee Whye Teh,mtitsias@google.com;schwarzjn@google.com;alexmatthews@google.com;razp@google.com;ywteh@google.com,3;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,5,0.0,yes,9/25/19,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,11
3684,3684,3684,3684,3684,3684,3684,3684,ICLR,2020,Graph inference learning for semi-supervised classification,Chunyan Xu;Zhen Cui;Xiaobin Hong;Tong Zhang;Jian Yang;Wei Liu,cyx@njust.edu.cn;zhen.cui@njust.edu.cn;xbhong@njust.edu.cn;tong.zhang@njust.edu.cn;csjyang@njust.edu.cn;wl2223@columbia.edu,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,0,0.0,yes,9/25/19,Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Columbia University,39;39;39;39;39;15,47;47;47;47;47;16,10
3685,3685,3685,3685,3685,3685,3685,3685,ICLR,2020,Input Complexity and Out-of-distribution Detection with Likelihood-based Generative Models,Joan Serrà;David Álvarez;Vicenç Gómez;Olga Slizovskaia;José F. Núñez;Jordi Luque,joansj@gmail.com;davidalvarezdlt@gmail.com;vicen.gomez@upf.edu;oslizovskaia@gmail.com;jfn237@nyu.edu;jordi.luqueserrano@telefonica.com,6;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,5,0.0,yes,9/25/19,Dolby Laboratories;Telefonica Research;Universitat Pompeu Fabra;Universitat Pompeu Fabra;New York University;Telefonica Research,-1;-1;481;481;25;-1,-1;-1;141;141;29;-1,5;11
3686,3686,3686,3686,3686,3686,3686,3686,ICLR,2020,AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty,Dan Hendrycks*;Norman Mu*;Ekin Dogus Cubuk;Barret Zoph;Justin Gilmer;Balaji Lakshminarayanan,hendrycks@berkeley.edu;normanmu@google.com;cubuk@google.com;barretzoph@google.com;gilmer@google.com;balajiln@google.com,8;8;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,14,0.0,yes,9/25/19,University of California Berkeley;Google;Google;Google;Google;Google,5;-1;-1;-1;-1;-1,13;-1;-1;-1;-1;-1,
3687,3687,3687,3687,3687,3687,3687,3687,ICLR,2020,Learn to Explain Efficiently via Neural Logic Inductive Learning,Yuan Yang;Le Song,yyang754@gatech.edu;lsong@cc.gatech.edu,6;8;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,14,0.0,yes,9/25/19,Georgia Institute of Technology;Georgia Institute of Technology,13;13,38;38,
3688,3688,3688,3688,3688,3688,3688,3688,ICLR,2020,Doubly Robust Bias Reduction in Infinite Horizon Off-Policy Estimation,Ziyang Tang*;Yihao Feng*;Lihong Li;Dengyong Zhou;Qiang Liu,ztang@cs.utexas.edu;yihao@cs.utexas.edu;lihongli.cs@gmail.com;dennyzhou@google.com;lqiang@cs.utexas.edu,8;8;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Spotlight),0,10,0.0,yes,9/25/19,"University of Texas, Austin;University of Texas, Austin;Google;Google;University of Texas, Austin",22;22;-1;-1;22,38;38;-1;-1;38,
3689,3689,3689,3689,3689,3689,3689,3689,ICLR,2020,Towards Stable and Efficient Training of Verifiably Robust Neural Networks,Huan Zhang;Hongge Chen;Chaowei Xiao;Sven Gowal;Robert Stanforth;Bo Li;Duane Boning;Cho-Jui Hsieh,huan@huan-zhang.com;chenhg@mit.edu;xiaocw@umich.edu;sgowal@google.com;stanforth@google.com;lbo@illinois.edu;boning@mtl.mit.edu;chohsieh@cs.ucla.edu,6;3;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),3,7,0.0,yes,9/25/19,"University of California, Los Angeles;Massachusetts Institute of Technology;University of Michigan;Google;Google;University of Illinois, Urbana Champaign;Massachusetts Institute of Technology;University of California, Los Angeles",20;2;8;-1;-1;3;2;20,17;5;21;-1;-1;48;5;17,4;1
3690,3690,3690,3690,3690,3690,3690,3690,ICLR,2020,Adversarial Policies: Attacking Deep Reinforcement Learning,Adam Gleave;Michael Dennis;Cody Wild;Neel Kant;Sergey Levine;Stuart Russell,gleave@berkeley.edu;michael_dennis@berkeley.edu;codywild@berkeley.edu;kantneel@berkeley.edu;svlevine@eecs.berkeley.edu;russell@cs.berkeley.edu,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,8,0.0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5;5;5,13;13;13;13;13;13,4
3691,3691,3691,3691,3691,3691,3691,3691,ICLR,2020,DDSP: Differentiable Digital Signal Processing,Jesse Engel;Lamtharn (Hanoi) Hantrakul;Chenjie Gu;Adam Roberts,jesseengel@google.com;hanoih@google.com;gcj@google.com;adarob@google.com,6;8;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Accept (Spotlight),0,9,2.0,yes,9/25/19,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,5;4
3692,3692,3692,3692,3692,3692,3692,3692,ICLR,2020,Enabling Deep Spiking Neural Networks with Hybrid Conversion and Spike Timing Dependent Backpropagation,Nitin Rathi;Gopalakrishnan Srinivasan;Priyadarshini Panda;Kaushik Roy,rathi2@purdue.edu;srinivg@purdue.edu;priya.panda@yale.edu;kaushik@purdue.edu,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,Purdue University;Purdue University;Yale University;Purdue University,27;27;64;27,88;88;8;88,
3693,3693,3693,3693,3693,3693,3693,3693,ICLR,2020,Towards Verified Robustness under Text Deletion Interventions,Johannes Welbl;Po-Sen Huang;Robert Stanforth;Sven Gowal;Krishnamurthy (Dj) Dvijotham;Martin Szummer;Pushmeet Kohli,johannes.welbl.14@ucl.ac.uk;posenhuang@google.com;stanforth@google.com;sgowal@google.com;dvij@google.com;szummer@google.com;pushmeet@google.com,8;6;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:N/A:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0.0,yes,9/25/19,University College London;Google;Google;Google;Google;Google;Google,50;-1;-1;-1;-1;-1;-1,15;-1;-1;-1;-1;-1;-1,3;1
3694,3694,3694,3694,3694,3694,3694,3694,ICLR,2020,N-BEATS: Neural basis expansion analysis for interpretable time series forecasting,Boris N. Oreshkin;Dmitri Carpov;Nicolas Chapados;Yoshua Bengio,boris@elementai.com;dmitri.carpov@elementai.com;chapados@elementai.com;yoshua.bengio@mila.quebec,6;8;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,8,0.0,yes,9/25/19,Element AI;Element AI;Element AI;University of Montreal,-1;-1;-1;128,-1;-1;-1;85,
3695,3695,3695,3695,3695,3695,3695,3695,ICLR,2020,Improving Adversarial Robustness Requires Revisiting Misclassified Examples,Yisen Wang;Difan Zou;Jinfeng Yi;James Bailey;Xingjun Ma;Quanquan Gu,eewangyisen@gmail.com;knowzou@ucla.edu;jinfengyi.ustc@gmail.com;baileyj@unimelb.edu.au;xingjun.ma@unimelb.edu.au;qgu@cs.ucla.edu,6;6;8,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,1.0,yes,9/25/19,"Tsinghua University;University of California, Los Angeles;JD AI Research;The University of Melbourne;The University of Melbourne;University of California, Los Angeles",8;20;-1;118;118;20,23;17;-1;32;32;17,4
3696,3696,3696,3696,3696,3696,3696,3696,ICLR,2020,On Generalization Error Bounds of Noisy Gradient Methods for Non-Convex Learning,Jian Li;Xuanyuan Luo;Mingda Qiao,ljiian83@mail.tsinghua.edu.cn;luo-xy19@mails.tsinghua.edu.cn;mqiao@stanford.edu,6;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Accept (Poster),1,9,1.0,yes,9/25/19,Tsinghua University;Tsinghua University;Stanford University,8;8;4,23;23;4,11;8
3697,3697,3697,3697,3697,3697,3697,3697,ICLR,2020,SPACE: Unsupervised Object-Oriented Scene Representation via Spatial Attention and Decomposition,Zhixuan Lin;Yi-Fu Wu;Skand Vishwanath Peri;Weihao Sun;Gautam Singh;Fei Deng;Jindong Jiang;Sungjin Ahn,zxlin@zju.edu.cn;yifu.wu@gmail.com;pvskand@protonmail.com;ws383@scarletmail.rutgers.edu;singh.gautam.iitg@gmail.com;fei.deng@rutgers.edu;jindong.jiang@rutgers.edu;sjn.ahn@gmail.com,6;6;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,8,0.0,yes,9/25/19,Zhejiang University;Rutgers University;Rutgers University;Rutgers University;Rutgers University;Rutgers University;Rutgers University;Rutgers University,56;34;34;34;34;34;34;34,107;168;168;168;168;168;168;168,5
3698,3698,3698,3698,3698,3698,3698,3698,ICLR,2020,Learning from Unlabelled Videos Using Contrastive Predictive Neural 3D Mapping,Adam W. Harley;Shrinidhi K. Lakshmikanth;Fangyu Li;Xian Zhou;Hsiao-Yu Fish Tung;Katerina Fragkiadaki,aharley@cmu.edu;kowshika@cmu.edu;fangyul@cmu.edu;zhouxian@cmu.edu;htung@cs.cmu.edu;katef@cs.cmu.edu,6;6;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,8,0.0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1;1;1,27;27;27;27;27;27,2
3699,3699,3699,3699,3699,3699,3699,3699,ICLR,2020,CLN2INV: Learning Loop Invariants with Continuous Logic Networks,Gabriel Ryan;Justin Wong;Jianan Yao;Ronghui Gu;Suman Jana,gabe@cs.columbia.edu;justin.wong@columbia.edu;jy3022@columbia.edu;ronghui.gu@columbia.edu;suman@cs.columbia.edu,3;8,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,1.0,yes,9/25/19,Columbia University;Columbia University;Columbia University;Columbia University;Columbia University,15;15;15;15;15,16;16;16;16;16,
3700,3700,3700,3700,3700,3700,3700,3700,ICLR,2020,"Pay Attention to Features, Transfer Learn Faster CNNs",Kafeng Wang;Xitong Gao;Yiren Zhao;Xingjian Li;Dejing Dou;Cheng-Zhong Xu,kf.wang@siat.ac.cn;xt.gao@siat.ac.cn;yiren.zhao@cl.cam.ac.uk;lixingjian@baidu.com;doudejing@baidu.com;czxu@um.edu.mo,6;6;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0.0,yes,9/25/19,Chinese Academy of Sciences;Chinese Academy of Sciences;University of Cambridge;Baidu;Baidu;,59;59;71;-1;-1;-1,1397;1397;3;-1;-1;-1,6
3701,3701,3701,3701,3701,3701,3701,3701,ICLR,2020,Sign-OPT: A Query-Efficient Hard-label Adversarial Attack,Minhao Cheng;Simranjit Singh;Patrick H. Chen;Pin-Yu Chen;Sijia Liu;Cho-Jui Hsieh,mhcheng@ucla.edu;simranjit@cs.ucla.edu;patrickchen@ucla.edu;pin-yu.chen@ibm.com;sijia.liu@ibm.com;chohsieh@cs.ucla.edu,6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,2,0.0,yes,9/25/19,"University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles;International Business Machines;International Business Machines;University of California, Los Angeles",20;20;20;-1;-1;20,17;17;17;-1;-1;17,4
3702,3702,3702,3702,3702,3702,3702,3702,ICLR,2020,GraphQA: Protein Model Quality Assessment using Graph Convolutional Network,Federico Baldassarre;David Menéndez Hurtado;Arne Elofsson;Hossein Azizpour,baldassarre.fe@gmail.com;david.menendez.hurtado@scilifelab.se;arne@bioinfo.se;azizpour@kth.se,3;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,6,0.0,yes,9/25/19,"KTH Royal Institute of Technology, Stockholm, Sweden;Science for Life Laboratory;;KTH Royal Institute of Technology, Stockholm, Sweden",128;-1;-1;128,222;-1;-1;222,10
3703,3703,3703,3703,3703,3703,3703,3703,ICLR,2020,Compositional Embeddings: Joint Perception and Comparison of Class Label Sets,Zeqian Li;Jacob Whitehill,zli14@wpi.edu;jrwhitehill@wpi.edu,6;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Worcester Polytechnic Institute;Worcester Polytechnic Institute,172;172,628;628,2
3704,3704,3704,3704,3704,3704,3704,3704,ICLR,2020,Generating Multi-Sentence Abstractive Summaries of Interleaved Texts,Sanjeev Kumar Karn;Francine Chen;Yan-Ying Chen;Ulli Waltinger;Hinrich Schütze,skarn@cis.lmu.de;chen@fxpal.com;yanying@fxpal.com;ulli.waltinger@siemens.com;hinrich@hotmail.com,3;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,"Institut für Informatik;FX Palo Alto Laboratory, Inc.;FX Palo Alto Laboratory, Inc.;Siemens Corporate Research;Institut für Informatik",-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,
3705,3705,3705,3705,3705,3705,3705,3705,ICLR,2020,Generalizing Reinforcement Learning to Unseen Actions,Ayush Jain*;Andrew Szot*;Jincheng Zhou;Joseph J. Lim,ayushj@usc.edu;szot@usc.edu;jinchenz@usc.edu;limjj@usc.edu,6;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,University of Southern California;University of Southern California;University of Southern California;University of Southern California,31;31;31;31,62;62;62;62,6;8
3706,3706,3706,3706,3706,3706,3706,3706,ICLR,2020,Mixture-of-Experts Variational Autoencoder for clustering and generating from similarity-based representations,Andreas Kopf;Vincent Fortuin;Vignesh Ram Somnath;Manfred Claassen,akopf@ethz.ch;fortuin@inf.ethz.ch;vsomnath@student.ethz.ch;mclaassen@ethz.ch,3;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10;10,13;13;13;13,5
3707,3707,3707,3707,3707,3707,3707,3707,ICLR,2020,On Symmetry and Initialization for Neural Networks,Ido Nachum;Amir Yehudayoff,ido0808@gmail.com;amir.yehudayoff@gmail.com,3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,Swiss Federal Institute of Technology Lausanne;,481;-1,38;-1,1;8
3708,3708,3708,3708,3708,3708,3708,3708,ICLR,2020,Training Interpretable Convolutional Neural Networks towards Class-specific Filters,Haoyu Liang;Zhihao Ouyang;Hang Su;Yuyuan Zeng;Zihao He;Shu-tao Xia;Jun Zhu;Bo Zhang,lianghy18@mails.tsinghua.edu.cn;oyzh18@mails.tsinghua.edu.cn;suhangss@mail.tsinghua.edu.cn;zengyy19@mails.tsinghua.edu.cn;zihaoh@usc.edu;xiast@sz.tsinghua.edu.cn;dcszj@mail.tsinghua.edu.cn;dcszb@mail.tsinghua.edu.cn,6;6;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0.0,yes,9/25/19,Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University;University of Southern California;Tsinghua University;Tsinghua University;Tsinghua University,8;8;8;8;31;8;8;8,23;23;23;23;62;23;23;23,
3709,3709,3709,3709,3709,3709,3709,3709,ICLR,2020,InfoCNF: Efficient Conditional Continuous Normalizing Flow Using Adaptive Solvers,Tan M. Nguyen;Animesh Garg;Richard G. Baraniuk;Anima Anandkumar,mn15@rice.edu;garg@cs.toronto.edu;richb@rice.edu;anima@caltech.edu,3;3;6;1,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,10,0.0,yes,9/25/19,"Rice University;Department of Computer Science, University of Toronto;Rice University;California Institute of Technology",84;18;84;143,105;18;105;2,5
3710,3710,3710,3710,3710,3710,3710,3710,ICLR,2020,All SMILES Variational Autoencoder for Molecular Property Prediction and Optimization,Zaccary Alperstein;Artem Cherkasov;Jason Rolfe,zalperst@gmail.com;artc@interchange.ubc.ca;rolfe22@gmail.com,3;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,2,7,0.0,yes,9/25/19,;University of British Columbia;D-Wave Systems,-1;35;-1,-1;34;-1,5;10
3711,3711,3711,3711,3711,3711,3711,3711,ICLR,2020,On Incorporating Semantic Prior Knowlegde in Deep Learning Through Embedding-Space Constraints,Damien Teney;Ehsan Abbasnejad;Anton van den Hengel,damien.teney@adelaide.edu.au;ehsan.abbasnejad@adelaide.edu.au;anton.vandenhengel@adelaide.edu.au,6;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,The University of Adelaide;The University of Adelaide;The University of Adelaide,128;128;128,120;120;120,
3712,3712,3712,3712,3712,3712,3712,3712,ICLR,2020,Learning Latent Representations for Inverse Dynamics using Generalized Experiences,Aditi Mavalankar;Sicun Gao,amavalan@eng.ucsd.edu;sicung@ucsd.edu,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,"University of California, San Diego;University of California, San Diego",11;11,31;31,
3713,3713,3713,3713,3713,3713,3713,3713,ICLR,2020,Understanding Attention Mechanisms,Bingyuan Liu;Yogesh Balaji;Lingzhou Xue;Martin Renqiang Min,bul37@psu.edu;yogesh@cs.umd.edu;lzxue@psu.edu;renqiang@nec-labs.com,6;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,5,0.0,yes,9/25/19,"Pennsylvania State University;University of Maryland, College Park;Pennsylvania State University;NEC-Labs",41;12;41;-1,78;91;78;-1,
3714,3714,3714,3714,3714,3714,3714,3714,ICLR,2020,Differentiable Architecture Compression,Shashank Singh;Ashish Khetan;Zohar Karnin,sss1@andrew.cmu.edu;khetan2@illinois.edu;zkarnin@gmail.com,3;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,"Carnegie Mellon University;University of Illinois, Urbana Champaign;Amazon",1;3;-1,27;48;-1,
3715,3715,3715,3715,3715,3715,3715,3715,ICLR,2020,On Stochastic Sign Descent Methods,Mher Safaryan;Peter Richtárik,mher.safaryan@gmail.com;peter.richtarik@kaust.edu.sa,6;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,KAUST;KAUST,128;128,1397;1397,9
3716,3716,3716,3716,3716,3716,3716,3716,ICLR,2020,The Generalization-Stability Tradeoff in Neural Network Pruning,Brian R. Bartoldson;Ari S. Morcos;Adrian Barbu;Gordon Erlebacher,bbartoldson@fsu.edu;arimorcos@gmail.com;abarbu@stat.fsu.edu;gerlebacher@fsu.edu,3;1;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,7,0.0,yes,9/25/19,SUN YAT-SEN UNIVERSITY;Facebook;SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY,481;-1;481;481,299;-1;299;299,8
3717,3717,3717,3717,3717,3717,3717,3717,ICLR,2020,Evaluations and Methods for Explanation through Robustness Analysis,Cheng-Yu Hsieh;Chih-Kuan Yeh;Xuanqing Liu;Pradeep Ravikumar;Seungyeon Kim;Sanjiv Kumar;Cho-Jui Hsieh,r05922048@ntu.edu.tw;cjyeh@cs.cmu.edu;xqliu@cs.ucla.edu;pradeepr@cs.cmu.edu;seungyeonk@google.com;sanjivk@google.com;chohsieh@cs.ucla.edu,6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,1,6,0.0,yes,9/25/19,"National Taiwan University;Carnegie Mellon University;University of California, Los Angeles;Carnegie Mellon University;Google;Google;University of California, Los Angeles",86;1;20;1;-1;-1;20,120;27;17;27;-1;-1;17,4
3718,3718,3718,3718,3718,3718,3718,3718,ICLR,2020,Unsupervised Meta-Learning for Reinforcement Learning,Abhishek Gupta;Benjamin Eysenbach;Chelsea Finn;Sergey Levine,abhigupta@berkeley.edu;beysenba@cs.cmu.edu;cbfinn@eecs.berkeley.edu;svlevine@eecs.berkeley.edu,3;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,University of California Berkeley;Carnegie Mellon University;University of California Berkeley;University of California Berkeley,5;1;5;5,13;27;13;13,6
3719,3719,3719,3719,3719,3719,3719,3719,ICLR,2020,A General Upper Bound for Unsupervised Domain Adaptation,Dexuan Zhang;Tatsuya Harada,dexuan.zhang@mi.t.u-tokyo.ac.jp;harada@mi.t.u-tokyo.ac.jp,1;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,10,0.0,yes,9/25/19,The University of Tokyo;The University of Tokyo,56;56,36;36,4;1
3720,3720,3720,3720,3720,3720,3720,3720,ICLR,2020,FoveaBox: Beyound Anchor-based Object Detection,Tao Kong;Fuchun Sun;Huaping Liu;Yuning Jiang;Lei Li;Jianbo Shi,taokongcn@gmail.com;fcsun@tsinghua.edu.cn;hpliu@tsinghua.edu.cn;jiangyuning@bytedance.com;lileilab@bytedance.com;jshi@seas.upenn.edu,6;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:N/A:N/A,Reject,1,4,0.0,yes,9/25/19,Bytedance;Tsinghua University;Tsinghua University;Bytedance;Bytedance;University of Pennsylvania,-1;8;8;-1;-1;19,-1;23;23;-1;-1;11,2;8
3721,3721,3721,3721,3721,3721,3721,3721,ICLR,2020,CloudLSTM: A Recurrent Neural Model for Spatiotemporal Point-cloud Stream Forecasting,Chaoyun Zhang;Marco Fiore;Iain Murray;Paul Patras,chaoyun.zhang@ed.ac.uk;marco.fiore@ieiit.cnr.it;i.murray@ed.ac.uk;paul.patras@ed.ac.uk,8;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,7,0.0,yes,9/25/19,University of Edinburgh;;University of Edinburgh;University of Edinburgh,33;-1;33;33,30;-1;30;30,
3722,3722,3722,3722,3722,3722,3722,3722,ICLR,2020,Step Size Optimization,Gyoung S. Na;Dongmin Hyeon;Hwanjo Yu,ngs0726@gmail.com;dmhyeon@postech.ac.kr;hwanjoyu@postech.ac.kr,3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,1,4,0.0,yes,9/25/19,POSTECH;POSTECH;POSTECH,118;118;118,146;146;146,
3723,3723,3723,3723,3723,3723,3723,3723,ICLR,2020,Neural Arithmetic Unit by reusing many small pre-trained networks,Ammar Ahmad;Oneeb Babar;Murtaza Taj,ammarahmad977@gmail.com;oneebalibabar@gmail.com;murtaza.taj@lums.edu.pk,1;1;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,0,0.0,yes,9/25/19,;Lahore University of Management Sciences;Lahore University of Management Sciences,-1;481;481,-1;932;932,
3724,3724,3724,3724,3724,3724,3724,3724,ICLR,2020,OvA-INN: Continual Learning with Invertible Neural Networks,HOCQUET Guillaume;BICHLER Olivier;QUERLIOZ Damien,guillaume.hocquet@live.fr;olivier.bichler@cea.fr;damien.querlioz@c2n.upsaclay.fr,6;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,CEA;CEA;,233;233;-1,1027;1027;-1,
3725,3725,3725,3725,3725,3725,3725,3725,ICLR,2020,Amortized Nesterov's Momentum: Robust and Lightweight  Momentum for Deep Learning,Kaiwen Zhou;Yanghua Jin;Qinghua Ding;James Cheng,kwzhou@cse.cuhk.edu.hk;jinyh@preferred.jp;qhding@cse.cuhk.edu.hk;jcheng@cse.cuhk.edu.hk,3;8;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,9,0.0,yes,9/25/19,"The Chinese University of Hong Kong;Preferred Networks, Inc.;The Chinese University of Hong Kong;The Chinese University of Hong Kong",59;-1;59;59,35;-1;35;35,1;9;8
3726,3726,3726,3726,3726,3726,3726,3726,ICLR,2020,Improving Sample Efficiency in Model-Free Reinforcement Learning from Images,Denis Yarats;Amy Zhang;Ilya Kostrikov;Brandon Amos;Joelle Pineau;Rob Fergus,denisyarats@cs.nyu.edu;amyzhang@fb.com;ik1078@nyu.edu;brandon.amos.cs@gmail.com;jpineau@fb.com;robfergus@fb.com,6;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,2,0.0,yes,9/25/19,New York University;Facebook;New York University;Facebook;Facebook;Facebook,25;-1;25;-1;-1;-1,29;-1;29;-1;-1;-1,
3727,3727,3727,3727,3727,3727,3727,3727,ICLR,2020,Curriculum Learning for Deep Generative Models with Clustering,Deli Zhao;Jiapeng Zhu;Zhenfang Guo;Bo Zhang,zhaodeli@gmail.com;jengzhu0@gmail.com;guozhenfang@pku.edu.cn;zhangbo@xiaomi.com,6;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,;;Peking University;Xiaomi,-1;-1;22;-1,-1;-1;24;-1,5;4
3728,3728,3728,3728,3728,3728,3728,3728,ICLR,2020,Autoencoder-based Initialization for Recurrent Neural Networks with a Linear Memory,Antonio Carta;Alessandro Sperduti;Davide Bacciu,antonio.carta@di.unipi.it;sperduti@math.unipd.it;bacciu@di.unipi.it,3;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,University of Pisa;Universita' degli studi di Padova;University of Pisa,233;-1;233,366;-1;366,
3729,3729,3729,3729,3729,3729,3729,3729,ICLR,2020,Universal Approximation with Deep Narrow Networks,Patrick Kidger;Terry Lyons,kidger@maths.ox.ac.uk;tlyons@maths.ox.ac.uk,6;8;3,I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,University of Oxford;University of Oxford,50;50,1;1,1
3730,3730,3730,3730,3730,3730,3730,3730,ICLR,2020,Probing Emergent Semantics in Predictive Agents via Question Answering,Abhishek Das;Federico Carnevale;Hamza Merzic;Laura Rimell;Rosalia Schneider;Alden Hung;Josh Abramson;Arun Ahuja;Stephen Clark;Greg Wayne;Felix Hill,abhshkdz@gatech.edu;fedecarnev@google.com;hamzamerzic@google.com;laurarimell@google.com;rgschneider@google.com;aldenhung@google.com;jabramson@google.com;arahuja@google.com;clarkstephen@google.com;gregwayne@google.com;felixhill@google.com,3;8;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,9,0.0,yes,9/25/19,Georgia Institute of Technology;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,13;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,38;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,
3731,3731,3731,3731,3731,3731,3731,3731,ICLR,2020,Scheduled Intrinsic Drive: A Hierarchical Take on Intrinsically Motivated Exploration,Jingwei Zhang;Niklas Wetzel;Nicolai Dorka;Joschka Boedecker;Wolfram Burgard,zhang@cs.uni-freiburg.de;wetzel@cs.uni-freiburg.de;dorka@informatik.uni-freiburg.de;jboedeck@cs.uni-freiburg.de;burgard@informatik.uni-freiburg.de,3;8;6;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,8,0.0,yes,9/25/19,Universität Freiburg;Universität Freiburg;Universität Freiburg;Universität Freiburg;Universität Freiburg,118;118;118;118;118,85;85;85;85;85,
3732,3732,3732,3732,3732,3732,3732,3732,ICLR,2020,Stein Bridging: Enabling Mutual Reinforcement between Explicit and Implicit Generative Models,Qitian Wu;Rui Gao;Hongyuan Zha,echo740@sjtu.edu.cn;rui.gao@mccombs.utexas.edu;zha@cc.gatech.edu,3;1;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,"Shanghai Jiao Tong University;University of Texas, Austin;Georgia Institute of Technology",53;22;13,157;38;38,5;4
3733,3733,3733,3733,3733,3733,3733,3733,ICLR,2020,New Loss Functions for Fast Maximum Inner Product Search,Ruiqi Guo;Quan Geng;David Simcha;Felix Chern;Phil Sun;Sanjiv Kumar,guorq@google.com;qgeng@google.com;dsimcha@google.com;fchern@google.com;sunphil@google.com;sanjivk@google.com,3;3;6,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,
3734,3734,3734,3734,3734,3734,3734,3734,ICLR,2020,End-to-end named entity recognition and relation extraction using pre-trained language models,John Giorgi;Xindi Wang;Nicola Sahar;Won Young Shin;Gary Bader;Bo Wang,john.giorgi@utoronto.ca;xindi.wang@uhnresearch.ca;nicola.sahar@mail.utoronto.ca;wonyoung.shin@mail.utoronto.ca;gary.bader@utoronto.ca;bowang@vectorinstitute.ai,6;3;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:N/A:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,4,1.0,yes,9/25/19,Toronto University;University Health Network;Toronto University;Toronto University;Toronto University;Vector Institute,18;481;18;18;18;-1,18;1397;18;18;18;-1,3
3735,3735,3735,3735,3735,3735,3735,3735,ICLR,2020,AdaGAN: Adaptive GAN for Many-to-Many Non-Parallel Voice Conversion,Maitreya Patel;Mirali Purohit;Mihir Parmar;Nirmesh J. Shah;Hemant A. Patil,maitreya_patel@daiict.ac.in;purohit_mirali@daiict.ac.in;mihirparmar@asu.edu;nirmesh88_shah@daiict.ac.in;hemant_patil@daiict.ac.in,1;1;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,1,3,0.0,yes,9/25/19,"Dhirubhai Ambani Institute of Information and Communication Technology, Gandhinagar;Dhirubhai Ambani Institute of Information and Communication Technology, Gandhinagar;Arizona State University;Dhirubhai Ambani Institute of Information and Communication Technology, Gandhinagar;Dhirubhai Ambani Institute of Information and Communication Technology, Gandhinagar",-1;-1;95;-1;-1,-1;-1;155;-1;-1,5;4;6
3736,3736,3736,3736,3736,3736,3736,3736,ICLR,2020,Goal-Conditioned Video Prediction,Oleh Rybkin;Karl Pertsch;Frederik Ebert;Dinesh Jayaraman;Chelsea Finn;Sergey Levine,oleh@seas.upenn.edu;pertsch@usc.edu;febert@berkeley.edu;dineshjayaraman@berkeley.edu;cbfinn@cs.stanford.edu;svlevine@eecs.berkeley.edu,6;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:N/A:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,4,0.0,yes,9/25/19,University of Pennsylvania;University of Southern California;University of California Berkeley;University of California Berkeley;Stanford University;University of California Berkeley,19;31;5;5;4;5,11;62;13;13;4;13,5
3737,3737,3737,3737,3737,3737,3737,3737,ICLR,2020,wMAN: WEAKLY-SUPERVISED MOMENT ALIGNMENT NETWORK FOR TEXT-BASED VIDEO SEGMENT RETRIEVAL,Reuben Tan;Huijuan Xu;Kate Saenko;Bryan A. Plummer,rxtan@bu.edu;huijuan@berkeley.edu;saenko@bu.edu;bplumme2@illinois.edu,6;6;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,"Boston University;University of California Berkeley;Boston University;University of Illinois, Urbana Champaign",67;5;67;3,61;13;61;48,10
3738,3738,3738,3738,3738,3738,3738,3738,ICLR,2020,Data augmentation instead of explicit regularization,Alex Hernandez-Garcia;Peter König,alexhg15@gmail.com;pkoenig@uos.de,3;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,9,0.0,yes,9/25/19,University of Osnabrück;University of Osnabrück,323;323,1397;1397,8
3739,3739,3739,3739,3739,3739,3739,3739,ICLR,2020,Implicit λ-Jeffreys Autoencoders: Taking the Best of Both Worlds,Aibek Alanov;Max Kochurov;Artem Sobolev;Daniil Yashkov;Dmitry Vetrov,alanov.aibek@gmail.com;maxim.v.kochurov@gmail.com;asobolev@bayesgroup.ru;daniil.yashkov@phystech.edu;vetrovd@yandex.ru,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Higher School of Economics;Skolkovo Institute of Science and Technology;Samsung;Moscow Institute of Physics and Technology;Higher School of Economics,481;-1;-1;481;481,251;-1;-1;234;251,5;4
3740,3740,3740,3740,3740,3740,3740,3740,ICLR,2020,LOSSLESS SINGLE IMAGE SUPER RESOLUTION FROM LOW-QUALITY JPG IMAGES,Yong Shi;Biao Li;Bo Wang;Zhiquan Qi;Jiabin Liu;Fan Meng,yshi@unomaha.edu;libiao17@mails.ucas.ac.cn;wangbo@uibe.edu.cn;qizhiquan@foxmail.com;liujiabin008@126.com;mengfan@cufe.edu.cn,3;6;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,1,0.0,yes,9/25/19,"University of Nebraska, Omaha;Chinese Academy of Sciences;Tsinghua University;University of Chinese Academy of Sciences;126;Tsinghua University",390;59;8;59;-1;8,1397;1397;23;1397;-1;23,2
3741,3741,3741,3741,3741,3741,3741,3741,ICLR,2020,Improving Batch Normalization with Skewness Reduction for Deep Neural Networks,Pak Lun Kevin Ding;Sarah Martin;Baoxin Li,kevinding@asu.edu;samart44@asu.edu;baoxin.li@asu.edu,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Arizona State University;Arizona State University;Arizona State University,95;95;95,155;155;155,
3742,3742,3742,3742,3742,3742,3742,3742,ICLR,2020,Cascade Style Transfer,Zhizhong Wang;Lei Zhao;Qihang Mo;Sihuan Lin;Zhiwen Zuo;Wei Xing;Dongming Lu,endywon@zju.edu.cn;cszhl@zju.edu.cn;moqihang@zju.edu.cn;linsh@zju.edu.cn;zzwcs@zju.edu.cn;wxing@zju.edu.cn;ldm@zju.edu.cn,1;1;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Zhejiang University;Zhejiang University;Zhejiang University;Zhejiang University;Zhejiang University;Zhejiang University;Zhejiang University,56;56;56;56;56;56;56,107;107;107;107;107;107;107,
3743,3743,3743,3743,3743,3743,3743,3743,ICLR,2020,Recurrent Hierarchical Topic-Guided Neural Language Models,Dandan Guo;Bo Chen;Ruiying Lu;Mingyuan Zhou,gdd_xidian@126.com;bchen@mail.xidian.edu.cn;ruiyinglu_xidian@163.com;mingyuan.zhou@mccombs.utexas.edu,1;1;8;8;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,8,4.0,yes,9/25/19,"Xidian University;Tsinghua University;Xidian University;University of Texas, Austin",481;8;481;22,919;23;919;38,3
3744,3744,3744,3744,3744,3744,3744,3744,ICLR,2020,The Frechet Distance of training and test distribution predicts the generalization gap,Julian Zilly;Hannes Zilly;Oliver Richter;Roger Wattenhofer;Andrea Censi;Emilio Frazzoli,jzilly@ethz.ch;hzilly@ethz.ch;richtero@ethz.ch;wattenhofer@ethz.ch;acensi@ethz.ch;emilio.frazzoli@idsc.mavt.ethz.ch,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10;10;10;10,13;13;13;13;13;13,8;1;6
3745,3745,3745,3745,3745,3745,3745,3745,ICLR,2020,Objective Mismatch in Model-based Reinforcement Learning,Nathan Lambert;Brandon Amos;Omry Yadan;Roberto Calandra,nol@berkeley.edu;brandon.amos.cs@gmail.com;omry@fb.com;rcalandra@fb.com,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,University of California Berkeley;Facebook;Facebook;Facebook,5;-1;-1;-1,13;-1;-1;-1,
3746,3746,3746,3746,3746,3746,3746,3746,ICLR,2020,Learning Explainable Models Using Attribution Priors,Gabriel Erion;Joseph D. Janizek;Pascal Sturmfels;Scott M. Lundberg;Su-In Lee,erion@cs.washington.edu;jjanizek@cs.washington.edu;psturm@cs.washington.edu;slund1@cs.washington.edu;suinlee@cs.washington.edu,8;3;1,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,University of Washington;University of Washington;University of Washington;University of Washington;University of Washington,6;6;6;6;6,26;26;26;26;26,8
3747,3747,3747,3747,3747,3747,3747,3747,ICLR,2020,Improved Modeling of Complex Systems Using Hybrid Physics/Machine Learning/Stochastic Models,Anand Ramakrishnan;Warren B. Jackson;Kent Evans,aramakrishnan@wpi.edu;jackson@parc.com;kent.evans@parc.com,1;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Worcester Polytechnic Institute;Xerox;Xerox,172;-1;-1,628;-1;-1,
3748,3748,3748,3748,3748,3748,3748,3748,ICLR,2020,Semi-supervised 3D Face Reconstruction with Nonlinear Disentangled Representations,Zhongpai Gao;Juyong Zhang;Yudong Guo;Chao Ma;Guangtao Zhai;Xiaokang Yang,gaozhongpai@sjtu.edu.cn;juyong@ustc.edu.cn;gyd2011@mail.ustc.edu.cn;chaoma@sjtu.edu.cn;zhaiguangtao@sjtu.edu.cn;xkyang@sjtu.edu.cn,3;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Shanghai Jiao Tong University;University of Science and Technology of China;University of Science and Technology of China;Shanghai Jiao Tong University;Shanghai Jiao Tong University;Shanghai Jiao Tong University,53;481;481;53;53;53,157;80;80;157;157;157,4;7
3749,3749,3749,3749,3749,3749,3749,3749,ICLR,2020,Efficient Exploration via State Marginal Matching,Lisa Lee;Benjain Eysenbach;Emilio Parisotto;Erix Xing;Sergey Levine;Ruslan Salakhutdinov,lslee@cs.cmu.edu;beysenba@cs.cmu.edu;eparisot@cs.cmu.edu;epxing@cs.cmu.edu;svlevine@eecs.berkeley.edu;rsalakhu@cs.cmu.edu,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;University of California Berkeley;Carnegie Mellon University,1;1;1;1;5;1,27;27;27;27;13;27,
3750,3750,3750,3750,3750,3750,3750,3750,ICLR,2020,Adversarial Filters of Dataset Biases,Ronan Le Bras;Swabha Swayamdipta;Chandra Bhagavatula;Rowan Zellers;Matthew Peters;Ashish Sabharwal;Yejin Choi,ronanlb@allenai.org;swabhas@allenai.org;chandrab@allenai.org;rowanz@cs.washington.edu;matthewp@allenai.org;ashishs@allenai.org;yejinc@allenai.org,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Allen Institute for Artificial Intelligence;Allen Institute for Artificial Intelligence;Allen Institute for Artificial Intelligence;University of Washington;Allen Institute for Artificial Intelligence;Allen Institute for Artificial Intelligence;Allen Institute for Artificial Intelligence,-1;-1;-1;6;-1;-1;-1,-1;-1;-1;26;-1;-1;-1,3;4
3751,3751,3751,3751,3751,3751,3751,3751,ICLR,2020,Insights on Visual Representations for Embodied Navigation Tasks,Erik Wijmans;Julian Straub;Irfan Essa;Dhruv Batra;Judy Hoffman;Ari Morcos,etw@gatech.edu;julian.straub@oculus.com;irfan@gatech.edu;dbatra@gatech.edu;judy@gatech.edu;arimorcos@gmail.com,3;3;3,I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,3,0.0,yes,9/25/19,Georgia Institute of Technology;Oculus;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Facebook,13;-1;13;13;13;-1,38;-1;38;38;38;-1,
3752,3752,3752,3752,3752,3752,3752,3752,ICLR,2020,Crafting Data-free Universal Adversaries with Dilate Loss,Deepak Babu Sam;ABINAYA K;Sudharsan K A;Venkatesh Babu RADHAKRISHNAN,deepaksam@iisc.ac.in;abinayak@iisc.ac.in;sudharsanka16@gmail.com;venky@iisc.ac.in,8;3;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,"Indian Institute of Science;Indian Institute of Science;National Institute of Technology, Tiruchirappalli;Indian Institute of Science",95;95;481;95,301;301;894;301,4
3753,3753,3753,3753,3753,3753,3753,3753,ICLR,2020,Stochastic Gradient Descent with Biased but Consistent Gradient Estimators,Jie Chen;Ronny Luss,chenjie@us.ibm.com;rluss@us.ibm.com,6;1;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,15,0.0,yes,9/25/19,International Business Machines;International Business Machines,-1;-1,-1;-1,10
3754,3754,3754,3754,3754,3754,3754,3754,ICLR,2020,Wasserstein Robust Reinforcement Learning,Mohammed Amin Abdullah;Hang Ren;Haitham Bou-Ammar;Vladimir Milenkovic;Rui Luo;Mingtian Zhang;Jun Wang,mohammed.abdullah@huawei.com;hang.ren1@huawei.com;haitham.ammar@huawei.com;vladimir.milenkovic@huawei.com;ruiluo@huawei.com;w.j@huawei.com,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,
3755,3755,3755,3755,3755,3755,3755,3755,ICLR,2020,A Training Scheme for the Uncertain Neuromorphic Computing Chips,Qingtian Zhang;Bin Gao;Huaqiang Wu,zhangqt0103@mail.tsinghua.edu.cn;gaob1@tsinghua.edu.cn;wuhq@tsinghua.edu.cn,1;6;1,I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,2,0.0,yes,9/25/19,Tsinghua University;Tsinghua University;Tsinghua University,8;8;8,23;23;23,
3756,3756,3756,3756,3756,3756,3756,3756,ICLR,2020,A Stochastic Trust Region Method for Non-convex Minimization,Zebang Shen;Pan Zhou;Cong Fang;Jiahao Xie;Alejandro Ribeiro,shenzebang@zju.edu.cn;pzhou@u.nus.edu;fangcong@pku.edu.cn;xiejh@zju.edu.cn;aribeiro@seas.upenn.edu,3;6;8;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,Zhejiang University;National University of Singapore;Peking University;Zhejiang University;University of Pennsylvania,56;16;22;56;19,107;25;24;107;11,1;9
3757,3757,3757,3757,3757,3757,3757,3757,ICLR,2020,MoET: Interpretable and Verifiable Reinforcement Learning via Mixture of Expert Trees,Marko Vasic;Andrija Petrovic;Kaiyuan Wang;Mladen Nikolic;Rishabh Singh;Sarfraz Khurshid,vasic@utexas.edu;aapetrovic@mas.bg.ac.rs;kaiyuanw@google.com;nikolic@matf.bg.ac.rs;rising@google.com;khurshid@ece.utexas.edu,3;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,"University of Texas, Austin;University of Belgrade;Google;University of Belgrade;Google;University of Texas, Austin",22;481;-1;481;-1;22,38;1397;-1;1397;-1;38,1
3758,3758,3758,3758,3758,3758,3758,3758,ICLR,2020,Deceptive Opponent Modeling with Proactive Network Interdiction for Stochastic Goal Recognition Control,Junren Luo;Wei Gao;Zhiyong Liao;Weilin Yuan;Wanpeng Zhang;Shaofei Chen,luojunren17@nudt.edu.cn;gaowei14@nudt.edu.cn,1;1;1,I have read many papers in this area.:N/A:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:N/A:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,0,0.0,yes,9/25/19,National University of Defense Technology;National University of Defense Technology,-1;-1,-1;-1,
3759,3759,3759,3759,3759,3759,3759,3759,ICLR,2020,Regularization Matters in Policy Optimization,Zhuang Liu;Xuanlin Li;Bingyi Kang;Trevor Darrell,zhuangl@berkeley.edu;xuanlinli17@berkeley.edu;kang@u.nus.edu;trevor@eecs.berkeley.edu,3;3;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,12,1.0,yes,9/25/19,University of California Berkeley;University of California Berkeley;National University of Singapore;University of California Berkeley,5;5;16;5,13;13;25;13,
3760,3760,3760,3760,3760,3760,3760,3760,ICLR,2020,Sentence embedding with contrastive multi-views learning,Antoine Simoulin,antoine.simoulin@gmail.com,1;1;3,I have published one or two papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Université Paris Diderot,481,1397,
3761,3761,3761,3761,3761,3761,3761,3761,ICLR,2020,Robust Domain Randomization for Reinforcement Learning,Reda Bahi Slaoui;William R. Clements;Jakob N. Foerster;Sébastien Toth,reda.bahi.slaoui@gmail.com;william.clements@unchartech.com;jakobfoerster@gmail.com;sebastien.toth@unchartech.com,3;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Ecole Normale Superieure;Uncharted Technologies;Facebook;Uncharted Technologies,100;-1;-1;-1,45;-1;-1;-1,8
3762,3762,3762,3762,3762,3762,3762,3762,ICLR,2020,Regularizing Trajectories to Mitigate Catastrophic Forgetting,Paul Michel;Elisabeth Salesky;Graham Neubig,pmichel1@cs.cmu.edu;esalesky@gmail.com;gneubig@cs.cmu.edu,6;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,Carnegie Mellon University;;Carnegie Mellon University,1;-1;1,27;-1;27,
3763,3763,3763,3763,3763,3763,3763,3763,ICLR,2020,Coloring graph neural networks for node disambiguation,George Dasoulas;Ludovic Dos Santos;Kevin Scaman;Aladin Virmaux,george.dasoulas1@gmail.com;kevin.scaman@gmail.com;ludovic.dos.santos@huawei.com;aladin.virmaux@huawei.com,6;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Ecole polytechnique;Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.,481;-1;-1;-1,93;-1;-1;-1,10
3764,3764,3764,3764,3764,3764,3764,3764,ICLR,2020,Undersensitivity in Neural Reading Comprehension,Johannes Welbl;Pasquale Minervini;Max Bartolo;Pontus Stenetorp;Sebastian Riedel,johannes.welbl.14@ucl.ac.uk;p.minervini@gmail.com;maxbartolo@gmail.com;pontus.stenetorp@gmail.com;s.riedel@ucl.ac.uk,6;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,University College London;University College London;University College London;;University College London,50;50;50;-1;50,15;15;15;-1;15,4
3765,3765,3765,3765,3765,3765,3765,3765,ICLR,2020,RotationOut as a Regularization Method for Neural Network,Kai Hu;Barnabas Poczos,kaihu@cmu.edu;bapoczos@cs.cmu.edu,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University,1;1,27;27,
3766,3766,3766,3766,3766,3766,3766,3766,ICLR,2020,Meta-Learning with Network Pruning for Overfitting Reduction,Hongduan Tian;Bo Liu;Xiao-Tong Yuan;Qingshan Liu,hongduan_tian@nuist.edu.cn;kfliubo@gmail.com;xtyuan1980@gmail.com;qsliu@nuist.edu.cn,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Nanjing University of Information Science & Technology;Rutgers University;Nanjing University of Information Science & Technology;Nanjing University of Information Science & Technology,481;34;481;481,1397;168;1397;1397,6;8
3767,3767,3767,3767,3767,3767,3767,3767,ICLR,2020,An implicit function learning approach for parametric modal regression,Yangchen Pan;Martha White;Amir-massoud Farahmand,pan6@ualberta.ca;whitem@ualberta.ca;farahmand@vectorinstitute.ai,1;3;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,3,0.0,yes,9/25/19,University of Alberta;University of Alberta;Vector Institute,100;100;-1,136;136;-1,1
3768,3768,3768,3768,3768,3768,3768,3768,ICLR,2020,On the Evaluation of Conditional GANs,Terrance DeVries;Adriana Romero;Luis Pineda;Graham W. Taylor;Michal Drozdzal,terrance@uoguelph.ca;adrianars@fb.com;lep@fb.com;gwtaylor@uoguelph.ca;mdrozdzal@fb.com,3;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,University of Guelph;Facebook;Facebook;University of Guelph;Facebook,266;-1;-1;266;-1,558;-1;-1;558;-1,5;4;1
3769,3769,3769,3769,3769,3769,3769,3769,ICLR,2020,CZ-GEM:  A  FRAMEWORK  FOR DISENTANGLED REPRESENTATION LEARNING,Akash Srivastava;Yamini Bansal;Yukun Ding;Bernhard Egger;Prasanna Sattigeri;Josh Tenenbaum;David D. Cox;Dan Gutfreund,akash.srivastava@me.com;ybansal@g.harvard.edu;yding5@nd.edu;egger@mit.edu;psattig@us.ibm.com;jbt@mit.edu;david.d.cox@ibm.com;dgutfre@us.ibm.com,1;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,4,0.0,yes,9/25/19,International Business Machines;Harvard University;University of Notre Dame;Massachusetts Institute of Technology;International Business Machines;Massachusetts Institute of Technology;International Business Machines;International Business Machines,-1;39;118;2;-1;2;-1;-1,-1;7;157;5;-1;5;-1;-1,5
3770,3770,3770,3770,3770,3770,3770,3770,ICLR,2020,On Weight-Sharing and Bilevel Optimization in Architecture Search,Mikhail Khodak;Liam Li;Maria-Florina Balcan;Ameet Talwalkar,khodak@cmu.edu;me@liamcli.com;ninamf@cs.cmu.edu;talwalkar@cmu.edu,3;3,I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,2,0.0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1,27;27;27;27,3;8
3771,3771,3771,3771,3771,3771,3771,3771,ICLR,2020,Are there any 'object detectors' in the hidden layers of CNNs trained to identify objects or scenes?,Ella M. Gale;Nicholas Martin;Ryan Blything;Anh Nguyen;Jeffrey S. Bowers,ella.gale@bristol.ac.uk;nm13850@bristol.ac.uk;ryan.blything@bristol.ac.uk;anhnguyen@auburn.edu;j.bowers@bristol.ac.uk,3;3;8,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,3,0.0,yes,9/25/19,University of Bristol;University of Bristol;University of Bristol;Auburn University;University of Bristol,128;128;128;390;128,87;87;87;651;87,
3772,3772,3772,3772,3772,3772,3772,3772,ICLR,2020,Dimensional Reweighting Graph Convolution Networks,Xu Zou;Qiuye Jia;Jianwei Zhang;Chang Zhou;Zijun Yao;Hongxia Yang;Jie Tang,zoux18@mails.tsinghua.edu.cn;jqy@stanford.edu;zhangjianwei.zjw@alibaba-inc.com;ericzhou.zc@alibaba-inc.com;yaozijun@bupt.edu.cn;yang.yhx@alibaba-inc.com;jietang@tsinghua.edu.cn,3;6;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,7,0.0,yes,9/25/19,Tsinghua University;Stanford University;Alibaba Group;Alibaba Group;Beijing University of Post and Telecommunication;Alibaba Group;Tsinghua University,8;4;-1;-1;481;-1;8,23;4;-1;-1;1397;-1;23,1;10
3773,3773,3773,3773,3773,3773,3773,3773,ICLR,2020,Leveraging inductive bias of neural networks for learning without explicit human annotations,Fatih Furkan Yilmaz;Reinhard Heckel,fy11@rice.edu;rh43@rice.edu,6;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,2,0.0,yes,9/25/19,Rice University;Rice University,84;84,105;105,4
3774,3774,3774,3774,3774,3774,3774,3774,ICLR,2020,XLDA: Cross-Lingual Data Augmentation for Natural Language Inference and Question Answering,Jasdeep Singh;Bryan McCann;Nitish Shirish Keskar;Caiming Xiong;Richard Socher,jasdeep@cs.stanford.edu;bmccann@salesforce.com;nkeskar@salesforce.com;cxiong@salesforce.com;rsocher@salesforce.com,1;8;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Stanford University;SalesForce.com;SalesForce.com;SalesForce.com;SalesForce.com,4;-1;-1;-1;-1,4;-1;-1;-1;-1,3;6
3775,3775,3775,3775,3775,3775,3775,3775,ICLR,2020,Ergodic Inference: Accelerate Convergence by Optimisation,Yichuan Zhang;José Miguel Hernández-Lobato,yichuan.zhang@eng.cam.ac.uk;jmh233@cam.ac.uk,3;8;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,2.0,yes,9/25/19,University of Cambridge;University of Cambridge,71;71,3;3,
3776,3776,3776,3776,3776,3776,3776,3776,ICLR,2020,Synthetic vs Real: Deep Learning on Controlled Noise,Lu Jiang;Di Huang;Weilong Yang,lujiang@google.com;dihuang@google.com;weilongyang@google.com,3;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,4,0.0,yes,9/25/19,Google;Google;Google,-1;-1;-1,-1;-1;-1,
3777,3777,3777,3777,3777,3777,3777,3777,ICLR,2020,Layerwise Learning Rates for Object Features in Unsupervised and Supervised Neural Networks And Consequent Predictions for the Infant Visual System,Rhodri Cusack;Cliona O'Doherty;Anna Birbeck;Anna Truzzi,cusackrh@tcd.ie;odoherc1@tcd.ie;birbecka@tcd.ie;truzzia@tcd.ie,3;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:N/A:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:N/A:N/A,Reject,0,5,0.0,yes,9/25/19,"Trinity College, Dublin;Trinity College, Dublin;Trinity College, Dublin;Trinity College, Dublin",481;481;481;481,164;164;164;164,
3778,3778,3778,3778,3778,3778,3778,3778,ICLR,2020,Non-Sequential Melody Generation,Mitchell Billard;Robert Bishop;Moustafa Elsisy;Laura Graves;Antonina Kolokolova;Vineel Nagisetty;Zachary Northcott;Heather Patey,mlb238@mun.ca;r.bishop@mun.ca;mmatelsisy@mun.ca;cmgraves@mun.ca;kol@mun.ca;vnagisetty@mun.ca;zmnorthcott@mun.ca;hpatey@gmail.com,1;3;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,3,0.0,yes,9/25/19,Memorial University of Newfoundland;Memorial University of Newfoundland;Memorial University of Newfoundland;Memorial University of Newfoundland;Memorial University of Newfoundland;Memorial University of Newfoundland;Memorial University of Newfoundland;,390;390;390;390;390;390;390;-1,557;557;557;557;557;557;557;-1,5;4
3779,3779,3779,3779,3779,3779,3779,3779,ICLR,2020,A Dynamic Approach to Accelerate Deep Learning Training,John Osorio;Adrià Armejach;Eric Petit;Marc Casas,john.osorio@bsc.es;adria.armejach@bsc.es;eric.petit@intel.com;marc.casas@bsc.es,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Barcelona Supercomputing Center;Barcelona Supercomputing Center;Intel;Barcelona Supercomputing Center,-1;-1;-1;-1,-1;-1;-1;-1,
3780,3780,3780,3780,3780,3780,3780,3780,ICLR,2020,Emergence of Compositional Language with Deep Generational Transmission,Michael Cogswell;Jiasen Lu;Stefan Lee;Devi Parikh;Dhruv Batra,cogswell@gatech.edu;jiasenlu@gatech.edu;steflee@gatech.edu;parikh@gatech.edu;dbatra@gatech.edu,6;1;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:N/A:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,6,0.0,yes,9/25/19,Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology,13;13;13;13;13,38;38;38;38;38,8
3781,3781,3781,3781,3781,3781,3781,3781,ICLR,2020,AutoGrow: Automatic Layer Growing in Deep Convolutional Networks,Wei Wen;Feng Yan;Hai Li,wei.wen@duke.edu;fyan@unr.edu;hai.li@duke.edu,3;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,1.0,yes,9/25/19,"Duke University;University of Nevada, Reno;Duke University",47;266;47,20;1397;20,
3782,3782,3782,3782,3782,3782,3782,3782,ICLR,2020,Model-Agnostic Feature Selection with Additional Mutual Information,Mukund Sudarshan;Aahlad Manas Puli;Lakshmi Subramanian;Sriram Sankararaman;Rajesh Ranganath,ms7490@nyu.edu;apm470@nyu.edu;lakshmi@cs.nyu.edu;sriram@cs.ucla.edu;rajeshr@cims.nyu.edu,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,"New York University;New York University;New York University;University of California, Los Angeles;New York University",25;25;25;20;25,29;29;29;17;29,
3783,3783,3783,3783,3783,3783,3783,3783,ICLR,2020,Certifying Neural Network Audio Classifiers,Wonryong Ryou;Mislav Balunovic;Gagandeep Singh;Martin Vechev,wryou@student.ethz.ch;bmislav@student.ethz.ch;gsingh@inf.ethz.ch;martin.vechev@inf.ethz.ch,6;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10;10,13;13;13;13,
3784,3784,3784,3784,3784,3784,3784,3784,ICLR,2020,Wasserstein-Bounded Generative Adversarial Networks,Peng Zhou;Bingbing Ni;Lingxi Xie;Xiaopeng Zhang;Hang Wang;Cong Geng;Qi Tian,zhoupengcv@sjtu.edu.cn;nibingbing@sjtu.edu.cn;198808xc@gmail.com;zxphistory@gmail.com;wang--hang@sjtu.edu.cn;gengcong@sjtu.edu.cn;tian.qi1@huawei.com,1;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Shanghai Jiao Tong University;Shanghai Jiao Tong University;Huawei Technologies Ltd.;National University of Singapore;Shanghai Jiao Tong University;Shanghai Jiao Tong University;Huawei Technologies Ltd.,53;53;-1;16;53;53;-1,157;157;-1;25;157;157;-1,5;4
3785,3785,3785,3785,3785,3785,3785,3785,ICLR,2020,An Explicitly Relational Neural Network Architecture,Murray Shanahan;Kyriacos Nikiforou;Antonia Creswell;Christos Kaplanis;David Barrett;Marta Garnelo,mshanahan@google.com;knikiforou@google.com;tonicreswell@google.com;christos.kaplanis14@imperial.ac.uk;barrettdavid@google.com;garnelo@google.com,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,4,1.0,yes,9/25/19,Google;Google;Google;Imperial College London;Google;Google,-1;-1;-1;73;-1;-1,-1;-1;-1;10;-1;-1,
3786,3786,3786,3786,3786,3786,3786,3786,ICLR,2020,Do Deep Neural Networks for Segmentation Understand Insideness?,Kimberly M Villalobos;Vilim Stih;Amineh Ahmadinejad;Jamell Dozier;Andrew Francl;Frederico Azevedo;Tomotake Sasaki;Xavier Boix,kimvc@mit.edu;vilim@neuro.mpg.de;amineh@mit.edu;jamell@mit.edu;francl@mit.edu;fazevedo@mit.edu;tomotake.sasaki@fujitsu.com;xboix@mit.edu,6;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,1.0,yes,9/25/19,Massachusetts Institute of Technology;Max Planck Institute of Neurobiology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Fujitsu Laboratories Ltd.;Massachusetts Institute of Technology,2;-1;2;2;2;2;-1;2,5;-1;5;5;5;5;-1;5,2
3787,3787,3787,3787,3787,3787,3787,3787,ICLR,2020,Split LBI for Deep Learning: Structural Sparsity via Differential Inclusion Paths,Yanwei Fu;Chen Liu;Donghao Li;Xinwei Sun;Jinshan ZENG;Yuan Yao,yanweifu@fudan.edu.cn;corwinliu9669@gmail.com;donghao.li@connect.ust.hk;xinsun@microsoft.com;jsh.zeng@gmail.com;yuany@ust.hk,3;8;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,Fudan University;Fudan University;The Hong Kong University of Science and Technology;Microsoft;Australian National University;The Hong Kong University of Science and Technology,79;79;39;-1;108;39,109;109;47;-1;50;47,9;8
3788,3788,3788,3788,3788,3788,3788,3788,ICLR,2020,Contextual Text Style Transfer,Yu Cheng;Zhe Gan;Yizhe Zhang;Oussama Elachqar;Dianqi Li;Jingjing Liu,yu.cheng@microsoft.com;zhe.gan@microsoft.com;yizhe.zhang@microsoft.com;ouelachq@microsoft.com;dianqili@uw.edu;jingjl@microsoft.com,3;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,"Microsoft;Microsoft;Microsoft;Microsoft;University of Washington, Seattle;Microsoft",-1;-1;-1;-1;6;-1,-1;-1;-1;-1;26;-1,
3789,3789,3789,3789,3789,3789,3789,3789,ICLR,2020,FLAT MANIFOLD VAES,Nutan Chen;Alexej Klushyn;Francesco Ferroni;Justin Bayer;Patrick van der Smagt,nutan.chen@gmail.com;a.klushyn@gmail.com;francescoferroni1@gmail.com;bayer.justin@googlemail.com;smagt@argmax.ai,1;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,11,0.0,yes,9/25/19,"Data Lab, Volkswagen Group;Technical University Munich;;Data Lab, Volkswagen Group;Volkswagen Group, ML Research Lab",-1;53;-1;-1;-1,-1;43;-1;-1;-1,
3790,3790,3790,3790,3790,3790,3790,3790,ICLR,2020,An Information Theoretic Approach to Distributed Representation Learning,Abdellatif Zaidi;Inaki Estella Aguerri,abdellatif.zaidi@u-pem.fr;inaki.estella@huawei.com,3;3;8;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Université Paris-Est;Huawei Technologies Ltd.,481;-1,1397;-1,1
3791,3791,3791,3791,3791,3791,3791,3791,ICLR,2020,LIA: Latently Invertible Autoencoder with Adversarial Learning,Jiapeng Zhu;Deli Zhao;Bolei Zhou;Bo Zhang,jengzhu0@gmail.com;zhaodeli@gmail.com;bzhou@ie.cuhk.edu.hk;zhangbo@xiaomi.com,3;3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,;;The Chinese University of Hong Kong;Xiaomi,-1;-1;59;-1,-1;-1;35;-1,5;4;2
3792,3792,3792,3792,3792,3792,3792,3792,ICLR,2020,Connecting the Dots Between MLE and RL for Sequence Prediction,Bowen Tan;Zhiting Hu;Zichao Yang;Ruslan Salakhutdinov;Eric Xing,bwkevintan@gmail.com;zhitinghu@gmail.com;yangtze2301@gmail.com;rsalakhu@cs.cmu.edu;epxing@cs.cmu.edu,3;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,;Carnegie Mellon University;;Carnegie Mellon University;Carnegie Mellon University,-1;1;-1;1;1,-1;27;-1;27;27,3
3793,3793,3793,3793,3793,3793,3793,3793,ICLR,2020,BEYOND SUPERVISED LEARNING: RECOGNIZING UNSEEN ATTRIBUTE-OBJECT PAIRS WITH VISION-LANGUAGE FUSION AND ATTRACTOR NETWORKS,Hui Chen;Zhixiong Nan;Nanning Zheng,chenhui0622@stu.xjtu.edu.cn;nanzhixiong@stu.xjtu.edu.cn;nnzheng@mail.xjtu.edu.cn,1;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,0,0.0,yes,9/25/19,Xi'an Jiaotong University;Xi'an Jiaotong University;Xi'an Jiaotong University,481;481;481,555;555;555,5
3794,3794,3794,3794,3794,3794,3794,3794,ICLR,2020,Distance-based Composable Representations with Neural Networks,Graham Spinks;Marie-Francine Moens,graham.spinks@cs.kuleuven.be;sien.moens@cs.kuleuven.be,3;3;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,KU Leuven;KU Leuven,118;118,45;45,
3795,3795,3795,3795,3795,3795,3795,3795,ICLR,2020,Efficient Deep Representation Learning by Adaptive Latent Space Sampling,Yuanhan Mo;Shuo Wang;Chengliang Dai;Rui Zhou;Zhongzhao Teng;Wenjia Bai;Yike Guo,y.mo16@imperial.ac.uk;shuo.wang@imperial.ac.uk;c.dai@imperial.ac.uk;rui.zhou18@imperial.ac.uk;zt215@cam.ac.uk;w.bai@imperial.ac.uk;y.guo@imperial.ac.uk,8;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Imperial College London;Imperial College London;Imperial College London;Imperial College London;University of Cambridge;Imperial College London;Imperial College London,73;73;73;73;71;73;73,10;10;10;10;3;10;10,5;2
3796,3796,3796,3796,3796,3796,3796,3796,ICLR,2020,Out-of-Distribution Detection Using Layerwise Uncertainty in Deep Neural Networks,Hirono Okamoto;Masahiro Suzuki;Yutaka Matsuo,h-okamoto@weblab.t.u-tokyo.ac.jp;masa@weblab.t.u-tokyo.ac.jp;matsuo@weblab.t.u-tokyo.ac.jp,3;1;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,The University of Tokyo;The University of Tokyo;The University of Tokyo,56;56;56,36;36;36,
3797,3797,3797,3797,3797,3797,3797,3797,ICLR,2020,Effect of top-down connections in Hierarchical Sparse Coding,Victor Boutin;Angelo Franciosini;Franck Ruffier;Laurent Perrinet,victor.boutin@univ-amu.fr;angelo.franciosini@univ-amu.fr;franck.ruffier@univ-amu.fr;laurent.perrinet@univ-amu.fr,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Aix Marseille Univ;Aix Marseille Univ;Aix Marseille Univ;Aix Marseille Univ,481;481;481;481,329;329;329;329,
3798,3798,3798,3798,3798,3798,3798,3798,ICLR,2020,Multitask Soft Option Learning,Maximilian Igl;Andrew Gambardella;Jinke He;Nantas Nardelli;N. Siddharth;Wendelin Böhmer;Shimon Whiteson,maximilian.igl@gmail.com;gambs@robots.ox.ac.uk;jinkehe1996@gmail.com;nantas@robots.ox.ac.uk;nsid@robots.ox.ac.uk;wendelin.boehmer@cs.ox.ac.uk;shimon.whiteson@cs.ox.ac.uk,8;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,University of Oxford;University of Oxford;;University of Oxford;University of Oxford;University of Oxford;University of Oxford,50;50;-1;50;50;50;50,1;1;-1;1;1;1;1,
3799,3799,3799,3799,3799,3799,3799,3799,ICLR,2020,Compositional Transfer in Hierarchical Reinforcement Learning,Markus Wulfmeier;Abbas Abdolmaleki;Roland Hafner;Jost Tobias Springenberg;Michael Neunert;Tim Hertweck;Thomas Lampe;Noah Siegel;Nicolas Heess;Martin Riedmiller,mwulfmeier@google.com;aabdolmaleki@google.com;rhafner@google.com;springenberg@google.com;neunertm@google.com;thertweck@google.com;thomaslampe@google.com;heess@google.com;riedmiller@google.com,3;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1,
3800,3800,3800,3800,3800,3800,3800,3800,ICLR,2020,Federated User Representation Learning,Duc Bui;Kshitiz Malik;Jack Goetz;Seungwhan Moon;Honglei Liu;Anuj Kumar;Kang G. Shin,ducbui@umich.edu;kmalik2@fb.com;jrgoetz@umich.edu;shanemoon@fb.com;honglei@fb.com;anujk@fb.com;kgshin@umich.edu,8;3;1,I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,University of Michigan;Facebook;University of Michigan;Facebook;Facebook;Facebook;University of Michigan,8;-1;8;-1;-1;-1;8,21;-1;21;-1;-1;-1;21,
3801,3801,3801,3801,3801,3801,3801,3801,ICLR,2020,Fairness with Wasserstein Adversarial Networks,serrurier Mathieu;Loubes Jean-Michel;Edouard Pauwels,mathieu.serrurier@irit.fr;loubes@math.univ-toulouse.fr;edouard.pauwels@irit.fr,1;1;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,"IRIT, University of Toulouse;Université de Toulouse;IRIT, University of Toulouse",-1;-1;-1,-1;-1;-1,5;4;7
3802,3802,3802,3802,3802,3802,3802,3802,ICLR,2020,Quantum Expectation-Maximization for Gaussian Mixture Models,Iordanis Kerenidis;Anupam Prakash;Alessandro Luongo,jkeren@gmail.com;anupamprakash1@gmail.com;aluongo@irif.fr,3;3;1,I have published in this field for several years.:N/A:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,7,0.0,yes,9/25/19,Université Paris Diderot;Universite Paris Diderot;Universite Paris Diderot,481;481;481,1397;1397;1397,5
3803,3803,3803,3803,3803,3803,3803,3803,ICLR,2020,Deep Evidential Uncertainty,Alexander Amini;Wilko Schwarting;Ava Soleimany;Daniela Rus,amini@mit.edu;wilkos@mit.edu;asolei@mit.edu;rus@csail.mit.edu,6;6;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,12,0.0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2,5;5;5;5,4;2
3804,3804,3804,3804,3804,3804,3804,3804,ICLR,2020,Variational Information Bottleneck for Unsupervised Clustering: Deep Gaussian Mixture Embedding,Yigit Ugur;George Arvanitakis;Abdellatif Zaidi,ygtugur@gmail.com;george.arvanitakis@huawei.com;abdellatif.zaidi@u-pem.fr,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,1,0,0.0,yes,9/25/19,Huawei Technologies Ltd.;Huawei Technologies Ltd.;Université Paris-Est,-1;-1;481,-1;-1;1397,5;1
3805,3805,3805,3805,3805,3805,3805,3805,ICLR,2020,Multi-scale Attributed Node Embedding,Benedek Rozemberczki;Carl Allen;Rik Sarkar,benedek.rozemberczki@gmail.com;carl.allen@ed.ac.uk;rsarkar@inf.ed.ac.uk,6;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,University of Edinburgh;University of Edinburgh;University of Edinburgh,33;33;33,30;30;30,1
3806,3806,3806,3806,3806,3806,3806,3806,ICLR,2020,Model Inversion Networks for Model-Based Optimization,Aviral Kumar;Sergey Levine,aviralkumar2907@gmail.com;svlevine@eecs.berkeley.edu,6;3;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,University of California Berkeley;University of California Berkeley,5;5,13;13,
3807,3807,3807,3807,3807,3807,3807,3807,ICLR,2020,Variational Autoencoders with Normalizing Flow Decoders,Rogan Morrow;Wei-Chen Chiu,rogan.o.morrow@gmail.com;walon@cs.nctu.edu.tw,3;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,4,0.0,yes,9/25/19,National Chiao Tung University;National Chiao Tung University,143;143,564;564,5
3808,3808,3808,3808,3808,3808,3808,3808,ICLR,2020,Adversarial Attacks on Copyright Detection Systems,Parsa Saadatpanah;Ali Shafahi;Tom Goldstein,parsa@cs.umd.edu;ashafahi@cs.umd.edu;tomg@cs.umd.edu,3;3;3;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,"University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park",12;12;12,91;91;91,4;1
3809,3809,3809,3809,3809,3809,3809,3809,ICLR,2020,MODiR: Multi-Objective Dimensionality Reduction for Joint Data Visualisation,Tim Repke;Ralf Krestel,tim.repke@hpi.uni-potsdam.de;ralf.krestel@hpi.de,1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,University of Potsdam;Hasso Plattner Institute,323;266,272;1397,3;10
3810,3810,3810,3810,3810,3810,3810,3810,ICLR,2020,Actor-Critic Approach for Temporal Predictive Clustering,Changhee Lee;Mihaela van der Schaar,chl8856@gmail.com;mihaela@ee.ucla.edu,3;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,"University of California, Los Angeles;University of California, Los Angeles",20;20,17;17,
3811,3811,3811,3811,3811,3811,3811,3811,ICLR,2020,NeuroFabric: Identifying Ideal Topologies for Training A Priori Sparse Networks,Mihailo Isakov;Michel A. Kinsy,mihailo@bu.edu;mkinsy@bu.edu,3;3;3;3,I do not know much about this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Boston University;Boston University,67;67,61;61,
3812,3812,3812,3812,3812,3812,3812,3812,ICLR,2020,Chart Auto-Encoders for Manifold Structured  Data,Stephan Schonsheck;Jie Chen;Rongjie Lai,schons@rpi.edu;chenjie@us.ibm.com;lair@rpi.edu,6;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,Rensselaer Polytechnic Institute;International Business Machines;Rensselaer Polytechnic Institute,172;-1;172,438;-1;438,5
3813,3813,3813,3813,3813,3813,3813,3813,ICLR,2020,Putting Machine Translation in Context with the Noisy Channel Model,Lei Yu;Laurent Sartran;Wojciech Stokowiec;Wang Ling;Lingpeng Kong;Phil Blunsom;Chris Dyer,leiyu@google.com;lsartran@google.com;wstokowiec@google.com;lingwang@google.com;lingpenk@google.com;pblunsom@google.com;cdyer@google.com,3;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,3
3814,3814,3814,3814,3814,3814,3814,3814,ICLR,2020,Optimizing Data Usage via Differentiable Rewards,Xinyi Wang;Hieu Pham;Paul Michel;Antonios Anastasopoulos;Graham Neubig;Jaime Carbonell,xinyiw1@cs.cmu.edu;hyhieu@cmu.edu;pmichel1@cs.cmu.edu;aanastas@andrew.cmu.edu;gneubig@cs.cmu.edu;jgc@cs.cmu.edu,3;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1;1;1,27;27;27;27;27;27,3
3815,3815,3815,3815,3815,3815,3815,3815,ICLR,2020,Representing Model Uncertainty of Neural Networks in Sparse Information Form,Jongseok Lee;Rudolph Triebel,jongseok.lee@dlr.de;rudolph.triebel@dlr.de,3;1;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,9,0.0,yes,9/25/19,German Aerospace Center (DLR);German Aerospace Center (DLR),-1;-1,-1;-1,11
3816,3816,3816,3816,3816,3816,3816,3816,ICLR,2020,Unsupervised Learning of Efficient and Robust Speech Representations,Kazuya Kawakami;Luyu Wang;Chris Dyer;Phil Blunsom;Aaron van den Oord,kawakamik@google.com;luyuwang@google.com;cdyer@google.com;pblunsom@google.com;avdnoord@google.com,6;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,4,0.0,yes,9/25/19,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,
3817,3817,3817,3817,3817,3817,3817,3817,ICLR,2020,A Kolmogorov Complexity Approach to Generalization in Deep Learning,Hazar Yueksel;Kush R. Varshney;Brian Kingsbury,hazar.yueksel@ibm.com;krvarshn@us.ibm.com;bedk@us.ibm.com,1;8;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,9,0.0,yes,9/25/19,International Business Machines;International Business Machines;International Business Machines,-1;-1;-1,-1;-1;-1,4;8
3818,3818,3818,3818,3818,3818,3818,3818,ICLR,2020,Learning Invariants through Soft Unification,Nuri Cingillioglu;Alessandra Russo,nuri.cingillioglu13@imperial.ac.uk;a.russo@imperial.ac.uk,1;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Imperial College London;Imperial College London,73;73,10;10,
3819,3819,3819,3819,3819,3819,3819,3819,ICLR,2020,Collapsed amortized variational inference for switching nonlinear dynamical systems,Zhe Dong;Bryan A. Seybold;Kevin P. Murphy;Hung H. Bui,zhedong@google.com;baseybold@gmail.com;kpmurphy@google.com;bui.h.hung@gmail.com,8;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,
3820,3820,3820,3820,3820,3820,3820,3820,ICLR,2020,ODE Analysis of Stochastic Gradient Methods with Optimism and Anchoring  for Minimax Problems and GANs,Ernest K. Ryu;Kun Yuan;Wotao Yin,eryu@math.ucla.edu;kunyuan@ucla.edu;wotaoyin@math.ucla.edu,1;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,"University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles",20;20;20,17;17;17,5;4;1
3821,3821,3821,3821,3821,3821,3821,3821,ICLR,2020,Event Discovery for History Representation in Reinforcement Learning,Aleksandr Ermolov;Enver Sangineto;Nicu Sebe,aleksandr.ermolov@unitn.it;enver.sangineto@unitn.it;niculae.sebe@unitn.it,3;6;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,25,0.0,yes,9/25/19,University of Trento;University of Trento;University of Trento,18;18;18,307;307;307,
3822,3822,3822,3822,3822,3822,3822,3822,ICLR,2020,Learning to Rank Learning Curves,Martin Wistuba;Tejaswini Pedapati,martin.wistuba@ibm.com;tejaswinip@us.ibm.com,6;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,International Business Machines;International Business Machines,-1;-1,-1;-1,6
3823,3823,3823,3823,3823,3823,3823,3823,ICLR,2020,Scoring-Aggregating-Planning: Learning task-agnostic priors from interactions and sparse rewards for zero-shot generalization,Huazhe Xu;Boyuan Chen;Yang Gao;Trevor Darrell,huazhe_xu@eecs.berkeley.edu;boyuanchen@berkeley.edu;yg@eecs.berkeley.edu;trevordarrell@eecs.berkeley.edu,3;6;6,I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,5,0.0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5,13;13;13;13,6
3824,3824,3824,3824,3824,3824,3824,3824,ICLR,2020,Topic Models with Survival Supervision: Archetypal Analysis and Neural Approaches,George H. Chen;Linhong Li;Ren Zuo;Amanda Coston;Jeremy C. Weiss,georgechen@cmu.edu;linhongl@andrew.cmu.edu;renzuo.wren@gmail.com;acoston@cs.cmu.edu;jeremyweiss@cmu.edu,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,3,0.0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;;Carnegie Mellon University;Carnegie Mellon University,1;1;-1;1;1,27;27;-1;27;27,
3825,3825,3825,3825,3825,3825,3825,3825,ICLR,2020,Way Off-Policy Batch Deep Reinforcement Learning of Human Preferences in Dialog,Natasha Jaques;Asma Ghandeharioun;Judy Hanwen Shen;Craig Ferguson;Agata Lapedriza;Noah Jones;Shixiang Gu;Rosalind Picard,jaquesn@mit.edu;asma_gh@mit.edu;judyshen@mit.edu;fergusoc@mit.edu;agata@mit.edu;ncjones@mit.edu;shanegu@google.com;picard@media.mit.edu,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Google;Massachusetts Institute of Technology,2;2;2;2;2;2;-1;2,5;5;5;5;5;5;-1;5,1;8
3826,3826,3826,3826,3826,3826,3826,3826,ICLR,2020,Well-Read Students Learn Better: On the Importance of Pre-training Compact Models,Iulia Turc;Ming-Wei Chang;Kenton Lee;Kristina Toutanova,iuliaturc@google.com;mingweichang@google.com;kentonl@google.com;kristout@google.com,1;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,4,0.0,yes,9/25/19,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,3
3827,3827,3827,3827,3827,3827,3827,3827,ICLR,2020,Model Imitation for Model-Based Reinforcement Learning,Yueh-Hua Wu;Ting-Han Fan;Peter J. Ramadge;Hao Su,kriswu8021@gmail.com;tinghanf@princeton.edu;ramadge@princeton.edu;haosu@eng.ucsd.edu,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,"National Taiwan University;Princeton University;Princeton University;University of California, San Diego",86;31;31;11,120;6;6;31,
3828,3828,3828,3828,3828,3828,3828,3828,ICLR,2020,Black Box Recursive Translations for Molecular Optimization,Farhan Damani;Vishnu Sresht;Stephen Ra,farhand7@gmail.com;vishnu.sresht@pfizer.com;stephen.ra@pfizer.com,6;3;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,9,0.0,yes,9/25/19,Princeton University;Pfizer R&D;Pfizer R&D,31;-1;-1,6;-1;-1,10
3829,3829,3829,3829,3829,3829,3829,3829,ICLR,2020,Improving the Generalization of Visual Navigation Policies using Invariance Regularization,Michel Aractingi;Christopher Dance;Julien Perez;Tomi Silander,michel.aractingi@naverlabs.com;christopher.dance@naverlabs.com;julien.perez@naverlabs.com;tomi.silander@naverlabs.com,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Naver Labs Europe;Naver Labs Europe;Naver Labs Europe;Naver Labs Europe,-1;-1;-1;-1,-1;-1;-1;-1,8
3830,3830,3830,3830,3830,3830,3830,3830,ICLR,2020,PatchFormer: A neural architecture for self-supervised representation learning on images,Aravind Srinivas;Pieter Abbeel,aravind@cs.berkeley.edu;pabbeel@cs.berkeley.edu,1;1;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,0,0.0,yes,9/25/19,University of California Berkeley;University of California Berkeley,5;5,13;13,5
3831,3831,3831,3831,3831,3831,3831,3831,ICLR,2020,Learning from Positive and Unlabeled Data  with Adversarial Training,Wenpeng Hu;Ran Le;Bing Liu;Feng Ji;Haiqing Chen;Dongyan Zhao;Jinwen Ma;Rui Yan,wenpeng.hu@pku.edu.cn;leran.pku@gmail.com;dcsliub@pku.edu.cn;zhongxiu.jf@alibaba-inc.com;zhaody@pku.edu.cn;jinwen.ma@pku.edu.cn;rui.yan@pku.edu.cn,3;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,Peking University;;Peking University;Alibaba Group;Peking University;Peking University;Peking University,22;-1;22;-1;22;22;22,24;-1;24;-1;24;24;24,5;4
3832,3832,3832,3832,3832,3832,3832,3832,ICLR,2020,PNAT: Non-autoregressive Transformer by Position Learning,Yu Bao;Hao Zhou;Jiangtao Feng;Mingxuan Wang;Shujian Huang;Jiajun Chen;Lei Li,baoy@smail.nju.edu.cn;zhouhao.nlp@bytedance.com;fengjiangtao@bytedance.com;wangmingxuan.89@bytedance.com;huangsj@nju.edu.cn;chenjj@nju.edu.cn;lilei.02@bytedance.com,3;3;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,2,9,0.0,yes,9/25/19,Zhejiang University;Bytedance;Bytedance;Bytedance;Zhejiang University;Zhejiang University;Bytedance,56;-1;-1;-1;56;56;-1,107;-1;-1;-1;107;107;-1,3
3833,3833,3833,3833,3833,3833,3833,3833,ICLR,2020,Learning Boolean Circuits with Neural Networks,Eran Malach;Shai Shalev-Shwartz,eran.malach@mail.huji.ac.il;shais@cs.huji.ac.il,6;3;6,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Hebrew University of Jerusalem;Hebrew University of Jerusalem,67;67,216;216,
3834,3834,3834,3834,3834,3834,3834,3834,ICLR,2020,Stagnant zone segmentation with U-net,Selam Waktola;Laurent Babout;Krzysztof Grudzien,selam.waktola@gmail.com,1;1;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:N/A:I made a quick assessment of this paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,1,0.0,yes,9/25/19,Lodz university of technology,-1,-1,2
3835,3835,3835,3835,3835,3835,3835,3835,ICLR,2020,Model-based Saliency for the Detection of Adversarial Examples,Lisa Schut;Yarin Gal,lisaschut94@gmail.com;yarin.gal@cs.ox.ac.uk,6;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,University of Oxford;University of Oxford,50;50,1;1,4
3836,3836,3836,3836,3836,3836,3836,3836,ICLR,2020,Learning Temporal Coherence via Self-Supervision for GAN-based Video Generation,Mengyu Chu;You Xie;Jonas Mayer;Laura Leal-Taixé;Nils Thürey,mengyu.chu@tum.de;you.xie@tum.de;jonas.a.mayer@tum.de;leal.taixe@tum.de;nils.thuerey@tum.de,3;8;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,9,0.0,yes,9/25/19,Technical University Munich;Technical University Munich;Technical University Munich;Technical University Munich;Technical University Munich,53;53;53;53;53,43;43;43;43;43,5;4
3837,3837,3837,3837,3837,3837,3837,3837,ICLR,2020,R-TRANSFORMER: RECURRENT NEURAL NETWORK ENHANCED TRANSFORMER,Zhiwei Wang;Yao Ma;Zitao Liu;Jiliang Tang,wangzh65@msu.edu;mayao4@msu.edu;liuzitao@100tal.com;tangjili@msu.edu,3;3;6,I have published in this field for several years.:I carefully checked the experiments.:N/A:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;100tal;SUN YAT-SEN UNIVERSITY,481;481;-1;481,299;299;-1;299,
3838,3838,3838,3838,3838,3838,3838,3838,ICLR,2020,A closer look at network resolution for efficient network design,Taojiannan Yang;Sijie Zhu;Yan Shen;Mi Zhang;Andrew Willis;Chen Chen,tyang30@uncc.edu;szhu3@uncc.edu;yanshen6@msu.edu;mizhang@msu.edu;arwillis@uncc.edu;chen.chen@uncc.edu,3;6;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,10,0.0,yes,9/25/19,"University of North Carolina, Charlotte;University of North Carolina, Charlotte;SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;University of North Carolina, Charlotte;University of North Carolina, Charlotte",73;73;481;481;73;73,1397;1397;299;299;1397;1397,6;2
3839,3839,3839,3839,3839,3839,3839,3839,ICLR,2020,Learning in Confusion: Batch Active Learning with Noisy Oracle,Gaurav Gupta;Anit Kumar Sahu;Wan-Yi Lin,ggaurav@usc.edu;anit.sahu@gmail.com;wan-yi.lin@us.bosch.com,1;1;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,4,0.0,yes,9/25/19,University of Southern California;;Bosch,31;-1;-1,62;-1;-1,
3840,3840,3840,3840,3840,3840,3840,3840,ICLR,2020,On Predictive Information Sub-optimality of RNNs,Zhe Dong;Deniz Oktay;Ben Poole;Alexander A. Alemi,zhedong@google.com;doktay@princeton.edu;pooleb@google.com;alemi@google.com,3;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Google;Princeton University;Google;Google,-1;31;-1;-1,-1;6;-1;-1,
3841,3841,3841,3841,3841,3841,3841,3841,ICLR,2020,Match prediction from group comparison data using neural networks,Sunghyun Kim;Minje jang;Changho Suh,koishkim@gmail.com;jmj427@lunit.io;chsuh@kaist.ac.kr,3;1;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,4,0.0,yes,9/25/19,;Lunit Inc.;Korea Advanced Institute of Science and Technology,-1;-1;481,-1;-1;110,
3842,3842,3842,3842,3842,3842,3842,3842,ICLR,2020,Defending Against Adversarial Examples by Regularized Deep Embedding,Yao Li;Martin Renqiang Min;Wenchao Yu;Cho-Jui Hsieh;Thomas Lee;Erik Kruus,yaoli@ucdavis.edu;renqiang@nec-labs.com;yuwenchao@ucla.edu;chohsieh@cs.ucla.edu;tcmlee@ucdavis.edu;kruus@nec-labs.com,3;1;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,2,11,0.0,yes,9/25/19,"University of California, Davis;NEC-Labs;University of California, Los Angeles;University of California, Los Angeles;University of California, Davis;NEC-Labs",79;-1;20;20;79;-1,55;-1;17;17;55;-1,4
3843,3843,3843,3843,3843,3843,3843,3843,ICLR,2020,Feature Partitioning for Efficient Multi-Task Architectures,Alejandro Newell;Lu Jiang;Chong Wang;Li-Jia Li;Jia Deng,anewell@cs.princeton.edu;lujiang@google.com;chong.wang@bytedance.com;lijiali@cs.stanford.edu;jiadeng@princeton.edu,6;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,3,0.0,yes,9/25/19,Princeton University;Google;Bytedance;Stanford University;Princeton University,31;-1;-1;4;31,6;-1;-1;4;6,
3844,3844,3844,3844,3844,3844,3844,3844,ICLR,2020,Learnable Group Transform For Time-Series,Romain Cosentino;Behnaam Aazhang,rc57@rice.edu;aaz@rice.edu,8;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,9,0.0,yes,9/25/19,Rice University;Rice University,84;84,105;105,
3845,3845,3845,3845,3845,3845,3845,3845,ICLR,2020,SesameBERT: Attention for Anywhere,Ta-Chun Su;Hsiang-Chih Cheng,gene11117@gmail.com;musicmilif@gmail.com,3;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,;,-1;-1,-1;-1,8
3846,3846,3846,3846,3846,3846,3846,3846,ICLR,2020,Towards A Unified Min-Max Framework for Adversarial Exploration and Robustness,Jingkang Wang;Tianyun Zhang;Sijia Liu;Pin-Yu Chen;Jiacen Xu;Makan Fardad;Bo Li,wangjksjtu@gmail.com;tzhan120@syr.edu;sijia.liu@ibm.com;pin-yu.chen@ibm.com;coldstudy@sjtu.edu.cn;makan@syr.edu;lxbosky@gmail.com,3;8;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0.0,yes,9/25/19,University of Toronto;Syracuse University;International Business Machines;International Business Machines;Shanghai Jiao Tong University;Syracuse University;University of California Berkeley,18;233;-1;-1;53;233;5,18;292;-1;-1;157;292;13,4
3847,3847,3847,3847,3847,3847,3847,3847,ICLR,2020,Towards Scalable Imitation Learning for Multi-Agent Systems with Graph Neural Networks,Siyu Zhou;Chaitanya Rajasekhar;Mariano J. Phielipp;Heni Ben Amor,siyu.zhou.ac@gmail.com;crajase1@asu.edu;mariano.j.phielipp@intel.com;hbenamor@asu.edu,3;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I made a quick assessment of this paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Arizona State University;Arizona State University;Intel;Arizona State University,95;95;-1;95,155;155;-1;155,6;10
3848,3848,3848,3848,3848,3848,3848,3848,ICLR,2020,Dual-module Inference for Efficient Recurrent Neural Networks,Liu Liu;Lei Deng;Shuangchen Li;Jingwei Zhang;Yihua Yang;Zhenyu Gu;Yufei Ding;Yuan Xie,liu_liu@ucsb.edu;leideng@ucsb.edu;shuangchen.li@alibaba-inc.com;jingwei.zhang@alibaba-inc.com;yihua.yang@alibaba-inc.com;zhenyu.gu@alibaba-inc.com;yufeiding@cs.ucsb.edu;yuanxie@ece.ucsb.edu,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,UC Santa Barbara;UC Santa Barbara;Alibaba Group;Alibaba Group;Alibaba Group;Alibaba Group;UC Santa Barbara;UC Santa Barbara,38;38;-1;-1;-1;-1;38;38,57;57;-1;-1;-1;-1;57;57,
3849,3849,3849,3849,3849,3849,3849,3849,ICLR,2020,On Federated Learning of Deep Networks from Non-IID Data: Parameter Divergence and the Effects of Hyperparametric Methods,Heejae Kim;Taewoo Kim;Chan-Hyun Youn,kim881019@kaist.ac.kr;taewoo_kim@kaist.ac.kr;chyoun@kaist.ac.kr,3;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,1,15,0.0,yes,9/25/19,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,481;481;481,110;110;110,
3850,3850,3850,3850,3850,3850,3850,3850,ICLR,2020,DeepXML: Scalable & Accurate Deep Extreme Classification for Matching User Queries to Advertiser Bid Phrases,Kunal Dahiya;Anshul Mittal;Deepak Saini;Kushal Dave;Himanshu Jain;Sumeet Agarwal;Manik Varma,kunalsdahiya@gmail.com;anshulmittal71@gmail.com;desaini@microsoft.com;kudave@microsoft.com;himanshu.j689@gmail.com;sumeet@iitd.ac.in;manik@microsoft.com,6;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Indian Institute of Technology Delhi;Indian Institute of Technology Delhi;Microsoft;Microsoft;;Indian Institute of Technology Delhi;Microsoft,118;118;-1;-1;-1;118;-1,441;441;-1;-1;-1;441;-1,3
3851,3851,3851,3851,3851,3851,3851,3851,ICLR,2020,Few-Shot Regression via Learning Sparsifying Basis Functions,Yi Loo;Yiluan Guo;Ngai-Man Cheung,loo_yi@sutd.edu.sg;guoyl1990@outlook.com;ngaiman_cheung@sutd.edu.sg,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,1,3,0.0,yes,9/25/19,Singapore University of Technology and Design;Singapore University of Technology and Design;Singapore University of Technology and Design,481;481;481,1397;1397;1397,6
3852,3852,3852,3852,3852,3852,3852,3852,ICLR,2020,Address2vec: Generating vector embeddings for blockchain analytics,Ali Hussein;Samiiha Nalwooga,ali.hussein@ronininstitute.org;nsamiiha@gmail.com,1;1;1,I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Ronin Institute;,-1;-1,-1;-1,10
3853,3853,3853,3853,3853,3853,3853,3853,ICLR,2020,Energy-Aware Neural Architecture Optimization with Fast Splitting Steepest Descent,Dilin Wang;Meng Li;Lemeng Wu;Vikas Chandra;Qiang Liu,dilin@cs.utexas.edu;meng.li@fb.com;lmwu@cs.utexas.edu;vchandra@fb.com;lqiang@cs.utexas.edu,3;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,5,0.0,yes,9/25/19,"University of Texas, Austin;Facebook;University of Texas, Austin;Facebook;University of Texas, Austin",22;-1;22;-1;22,38;-1;38;-1;38,
3854,3854,3854,3854,3854,3854,3854,3854,ICLR,2020,Equilibrium Propagation with Continual Weight Updates,Maxence Ernoult;Julie Grollier;Damien Querlioz;Yoshua Bengio;Benjamin Scellier,maxence.ernoult@u-psud.fr;julie.grollier@cnrs-thales.fr;damien.querlioz@u-psud.fr;yoshua.bengio@mila.quebec;benjamin.scellier@umontreal.ca,3;3;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,UPSud/INRIA University Paris-Saclay;;UPSud/INRIA University Paris-Saclay;University of Montreal;University of Montreal,481;-1;481;128;128,227;-1;227;85;85,1
3855,3855,3855,3855,3855,3855,3855,3855,ICLR,2020,IsoNN: Isomorphic Neural Network for Graph Representation Learning and Classification,Lin Meng;Jiawei Zhang,lin@ifmlab.org;jiawei@ifmlab.org,6;3;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,4,0.0,yes,9/25/19,SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY,481;481,299;299,3;2;10
3856,3856,3856,3856,3856,3856,3856,3856,ICLR,2020,LDMGAN: Reducing Mode Collapse in GANs with Latent Distribution Matching,Zhiwen Zuo;Lei Zhao;Huiming Zhang;Qihang Mo;Haibo Chen;Zhizhong Wang;AiLin Li;Lihong Qiu;Wei Xing;Dongming Lu,zzwcs@zju.edu.cn;cszhl@zju.edh.cn;qinglanwuji@zju.edu.cn;moqihang@zju.edu.cn;feng123@zju.edu.cn;endywon@zju.edu.cn;11921050@zju.edu.cn;zjusheldon@zju.edu.cn;wxing@zju.edu.cn;ldm@zju.edu.cn,1;3;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Zhejiang University;;Zhejiang University;Zhejiang University;Zhejiang University;Zhejiang University;Zhejiang University;Zhejiang University;Zhejiang University;Zhejiang University,56;-1;56;56;56;56;56;56;56;56,107;-1;107;107;107;107;107;107;107;107,5;4
3857,3857,3857,3857,3857,3857,3857,3857,ICLR,2020,Reinforcement Learning with Chromatic Networks,Xingyou Song;Krzysztof Choromanski;Jack Parker-Holder;Yunhao Tang;Wenbo Gao;Aldo Pacchiano;Tamas Sarlos;Deepali Jain;Yuxiang Yang,xingyousong@google.com;kchoro@google.com;jh3764@columbia.edu;yt2541@columbia.edu;wg2279@columbia.edu;pacchiano@berkeley.edu;stamas@google.com;jaindeepali@google.com;yxyang@google.com,3;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,5,0.0,yes,9/25/19,Google;Google;Columbia University;Columbia University;Columbia University;University of California Berkeley;Google;Google;Google,-1;-1;15;15;15;5;-1;-1;-1,-1;-1;16;16;16;13;-1;-1;-1,
3858,3858,3858,3858,3858,3858,3858,3858,ICLR,2020,Generalized Natural Language Grounded Navigation via Environment-agnostic Multitask Learning,Xin Wang;Vihan Jain;Eugene Ie;William Wang;Zornitsa Kozareva;Sujith Ravi,xwang@cs.ucsb.edu;vihanjain@google.com;eugeneie@google.com;william@cs.ucsb.edu;kozareva@google.com;sravi@google.com,6;3;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,6,0.0,yes,9/25/19,UC Santa Barbara;Google;Google;UC Santa Barbara;Google;Google,38;-1;-1;38;-1;-1,57;-1;-1;57;-1;-1,3
3859,3859,3859,3859,3859,3859,3859,3859,ICLR,2020,Mincut Pooling in Graph Neural Networks,Filippo Maria Bianchi;Daniele Grattarola;Cesare Alippi,fibi@norceresearch.no;grattd@usi.ch;alippc@usi.ch,3;8;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,5,0.0,yes,9/25/19,NORCE the Norwegian Research Center;Università della Svizzera Italiana;Università della Svizzera Italiana,-1;143;143,-1;341;341,10
3860,3860,3860,3860,3860,3860,3860,3860,ICLR,2020,Off-Policy Actor-Critic with Shared Experience Replay,Simon Schmitt;Matteo Hessel;Karen Simonyan,suschmitt@google.com;mtthss@google.com;simonyan@google.com,6;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,1.0,yes,9/25/19,Google;Google;Google,-1;-1;-1,-1;-1;-1,
3861,3861,3861,3861,3861,3861,3861,3861,ICLR,2020,Striving for Simplicity in Off-Policy Deep Reinforcement Learning,Rishabh Agarwal;Dale Schuurmans;Mohammad Norouzi,rishabhagarwal@google.com;schuurmans@google.com;mnorouzi@google.com,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,Google;Google;Google,-1;-1;-1,-1;-1;-1,
3862,3862,3862,3862,3862,3862,3862,3862,ICLR,2020,Scheduling the Learning Rate Via Hypergradients: New Insights and a New Algorithm,Michele Donini;Luca Franceschi;Orchid Majumder;Massimiliano Pontil;Paolo Frasconi,mikko108382892@gmail.com;luca.franceschi@iit.it;orchid@amazon.com;massimiliano.pontil@gmail.com;paolo.frasconi@unifi.it,6;1,I have read many papers in this area.:I carefully checked the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,Amazon;Istituto Italiano di Tecnologia;Amazon;;University of Florence,-1;481;-1;-1;481,-1;1397;-1;-1;378,8
3863,3863,3863,3863,3863,3863,3863,3863,ICLR,2020,Improving Federated Learning Personalization via Model Agnostic Meta Learning,Yihan Jiang;Jakub Konečný;Keith Rush;Sreeram Kannan,yihanrogerjiang@gmail.com;konkey@google.com;krush@google.com;ksreeram@uw.edu,1;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,1,4,0.0,yes,9/25/19,"University of Washington, Seattle;Google;Google;University of Washington, Seattle",6;-1;-1;6,26;-1;-1;26,6
3864,3864,3864,3864,3864,3864,3864,3864,ICLR,2020,Understanding and Improving Transformer From a Multi-Particle Dynamic System Point of View,Yiping Lu;Zhuohan Li;Di He;Zhiqing Sun;Bin Dong;Tao Qin;Liwei Wang;Tie-Yan Liu,yplu@stanford.edu;zhuohan@berkeley.edu;di_he@pku.edu.cn;zhiqings@andrew.cmu.edu;bindong@math.pku.edu.cn;taoqin@microsoft.com;wanglw@cis.pku.edu.cn;tyliu@microsoft.com,3;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,1,0.0,yes,9/25/19,Stanford University;University of California Berkeley;Peking University;Carnegie Mellon University;Peking University;Microsoft;Peking University;Microsoft,4;5;22;1;22;-1;22;-1,4;13;24;27;24;-1;24;-1,3
3865,3865,3865,3865,3865,3865,3865,3865,ICLR,2020,Optimising Neural Network Architectures for Provable Adversarial Robustness,Henry Gouk;Timothy M. Hospedales,hgouk@inf.ed.ac.uk;t.hospedales@ed.ac.uk,3;1;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,University of Edinburgh;University of Edinburgh,33;33,30;30,4;1
3866,3866,3866,3866,3866,3866,3866,3866,ICLR,2020,AN EFFICIENT HOMOTOPY TRAINING ALGORITHM FOR NEURAL NETWORKS,Qipin Chen;Wenrui Hao,qzc18@psu.edu;wxh64@psu.edu,3;3;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Pennsylvania State University;Pennsylvania State University,41;41,78;78,
3867,3867,3867,3867,3867,3867,3867,3867,ICLR,2020,Improving End-to-End Object Tracking Using Relational Reasoning,Fabian B. Fuchs;Adam R. Kosiorek;Li Sun;Oiwi Parker Jones;Ingmar Posner,fabian@robots.ox.ac.uk;adamk@robots.ox.ac.uk;kevin@robots.ox.ac.uk;oiwi.parkerjones@jesus.ox.ac.uk;ingmar@robots.ox.ac.uk,6;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Reject,0,5,1.0,yes,9/25/19,University of Oxford;University of Oxford;University of Oxford;University of Oxford;University of Oxford,50;50;50;50;50,1;1;1;1;1,
3868,3868,3868,3868,3868,3868,3868,3868,ICLR,2020,Bandlimiting Neural Networks Against Adversarial Attacks,Yuping Lin;Kasra Ahmadi K. A.;Hui Jiang,yuping@eecs.yorku.ca;kasraah@eecs.yorku.ca;hj@cse.yorku.ca,1;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,York University;York University;York University,172;172;172,416;416;416,4
3869,3869,3869,3869,3869,3869,3869,3869,ICLR,2020,NoiGAN: NOISE AWARE KNOWLEDGE GRAPH EMBEDDING WITH GAN,Kewei Cheng;Yikai Zhu;Ming Zhang;Yizhou Sun,viviancheng@cs.ucla.edu;zhuyikai.zyk@gmail.com;mzhang_cs@pku.edu.cn;yzsun@cs.ucla.edu,3;3;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,"University of California, Los Angeles;Peking University;Peking University;University of California, Los Angeles",20;22;22;20,17;24;24;17,5;4;10
3870,3870,3870,3870,3870,3870,3870,3870,ICLR,2020,Gradient Surgery for Multi-Task Learning,Tianhe Yu;Saurabh Kumar;Abhishek Gupta;Karol Hausman;Sergey Levine;Chelsea Finn,tianheyu@cs.stanford.edu;szk@stanford.edu;abhigupta@berkeley.edu;hausmankarol@gmail.com;svlevine@eecs.berkeley.edu;cbfinn@cs.stanford.edu,3;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,Stanford University;Stanford University;University of California Berkeley;Google;University of California Berkeley;Stanford University,4;4;5;-1;5;4,4;4;13;-1;13;4,
3871,3871,3871,3871,3871,3871,3871,3871,ICLR,2020,Filter redistribution templates for iteration-lessconvolutional model reduction,Ramon Izquierdo Cordova;Walterio Mayol Cuevas,ri16164@bristol.ac.uk;walterio.mayol-cuevas@bristol.ac.uk,3;6;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,6,0.0,yes,9/25/19,University of Bristol;University of Bristol,128;128,87;87,
3872,3872,3872,3872,3872,3872,3872,3872,ICLR,2020,Value-Driven Hindsight Modelling,Arthur Guez;Fabio Viola;Theophane Weber;Lars Buesing;Steven Kapturowski;Doina Precup;David Silver;Nicolas Heess,aguez@google.com;fviola@google.com;theophane@google.com;lbuesing@google.com;skapturowski@google.com;doinap@google.com;davidsilver@google.com;heess@google.com,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1,
3873,3873,3873,3873,3873,3873,3873,3873,ICLR,2020,JAUNE: Justified And Unified Neural language Evaluation,Hassan Kané;Yusuf Kocyigit;Ali Abdalla;Pelkins Ajanoh;Mohamed Coulibali,hassanmohamed@alum.mit.edu;yusuf.kocyigit@boun.edu.tr;aabdalla@alum.mit.edu;pelkins@alum.mit.edu;mohamed-konoufo.coulibali.1@ulaval.ca,1;1;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published in this field for several years.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A,Reject,0,3,0.0,yes,9/25/19,Massachusetts Institute of Technology;Bogazici University;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Laval university,2;323;2;2;481,5;672;5;5;272,3
3874,3874,3874,3874,3874,3874,3874,3874,ICLR,2020,Extreme Values are Accurate and Robust in Deep Networks,Jianguo Li;Mingjie Sun;Changshui Zhang,jianguo.li@intel.com;sunmj15@gmail.com;zcs@tsinghua.edu.cn,3;3;8,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Intel;Carnegie Mellon University;Tsinghua University,-1;1;8,-1;27;23,4
3875,3875,3875,3875,3875,3875,3875,3875,ICLR,2020,"Scaling Laws for the Principled Design, Initialization, and Preconditioning of ReLU Networks",Aaron Defazio;Leon Bottou,aaron.defazio@gmail.com;leon@bottou.org,3;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Facebook;Facebook,-1;-1,-1;-1,1
3876,3876,3876,3876,3876,3876,3876,3876,ICLR,2020,Overcoming Catastrophic Forgetting via Hessian-free Curvature Estimates,Leonid Butyrev;Georgios Kontes;Christoffer Löffler;Christopher Mutschler,butyreld@iis.fraunhofer.de;georgios.kontes@iis.fraunhofer.de;christoffer.loeffler@iis.fraunhofer.de;christopher.mutschler@iis.fraunhofer.de,3;1;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,1,0.0,yes,9/25/19,Fraunhofer IIS;Fraunhofer IIS;Fraunhofer IIS;Fraunhofer IIS,-1;-1;-1;-1,-1;-1;-1;-1,
3877,3877,3877,3877,3877,3877,3877,3877,ICLR,2020,Learning Functionally Decomposed Hierarchies for Continuous Navigation Tasks,Lukas Jendele;Sammy Christen;Emre Aksan;Otmar Hilliges,lukas.jendele@gmail.com;sammy.christen@inf.ethz.ch;eaksan@inf.ethz.ch;otmar.hilliges@inf.ethz.ch,6;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0.0,yes,9/25/19,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10;10,13;13;13;13,8
3878,3878,3878,3878,3878,3878,3878,3878,ICLR,2020,Hindsight Trust Region Policy Optimization,Hanbo Zhang;Site Bai;Xuguang Lan;Nanning Zheng,zhanghanbo163@stu.xjtu.edu.cn;best99317@stu.xjtu.edu.cn;xglan@xjtu.edu.cn;nnzheng@xjtu.edu.cn,3;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Xi'an Jiaotong University;Xi'an Jiaotong University;Xi'an Jiaotong University;Xi'an Jiaotong University,481;481;481;481,555;555;555;555,
3879,3879,3879,3879,3879,3879,3879,3879,ICLR,2020,The Probabilistic Fault Tolerance of Neural Networks in the Continuous Limit,El-Mahdi El-Mhamdi;Rachid Guerraoui;Andrei Kucharavy;Sergei Volodin,elmahdi.elmhamdi@epfl.ch;rachid.guerraoui@epfl.ch;andrei.kucharavy@epfl.ch;sergei.volodin@epfl.ch,1;3;8,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne,481;481;481;481,38;38;38;38,1;8
3880,3880,3880,3880,3880,3880,3880,3880,ICLR,2020,Parallel Neural Text-to-Speech,Kainan Peng;Wei Ping;Zhao Song;Kexin Zhao,pengkainan@baidu.com;weiping.thu@gmail.com;zhaosong02@baidu.com;zhaokexin01@baidu.com,3;6;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,1,3,0.0,yes,9/25/19,Baidu;Baidu;Baidu;Baidu,-1;-1;-1;-1,-1;-1;-1;-1,
3881,3881,3881,3881,3881,3881,3881,3881,ICLR,2020,"Cross-Dimensional Self-Attention for Multivariate, Geo-tagged Time Series Imputation",Jiawei Ma*;Zheng Shou*;Alireza Zareian;Hassan Mansour;Anthony Vetro;Shih-Fu Chang,jm4743@columbia.edu;zs2262@columbia.edu;alireza@cs.columbia.edu;mansour@merl.com;avetro@merl.com;sc250@columbia.edu,6;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Columbia University;Columbia University;Columbia University;Mitsubishi Electric Research Labs;Mitsubishi Electric Research Labs;Columbia University,15;15;15;-1;-1;15,16;16;16;-1;-1;16,3
3882,3882,3882,3882,3882,3882,3882,3882,ICLR,2020,Differential Privacy in Adversarial Learning with Provable Robustness,NhatHai Phan;My T. Thai;Ruoming Jin;Han Hu;Dejing Dou,phan@njit.edu;mythai@cise.ufl.edu;rjin1@kent.edu;hh255@njit.edu;dou@cs.uoregon.edu,3;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,New Jersey Institute of Technology;University of Florida;Bilkent University;New Jersey Institute of Technology;University of Oregon,172;128;323;172;205,564;174;548;564;288,4
3883,3883,3883,3883,3883,3883,3883,3883,ICLR,2020,On Layer Normalization in the Transformer Architecture,Ruibin Xiong;Yunchang Yang;Di He;Kai Zheng;Shuxin Zheng;Huishuai Zhang;Yanyan Lan;Liwei Wang;Tie-Yan Liu,xiongruibin18@mails.ucas.ac.cn;1500010650@pku.edu.cn;dihe@microsoft.com;zhengk92@pku.edu.cn;shuxin.zheng@microsoft.com;huishuai.zhang@microsoft.com;lanyanyan@ict.ac.cn;wanglw@cis.pku.edu.cn;tyliu@microsoft.com,6;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,2,11,0.0,yes,9/25/19,"Chinese Academy of Sciences;Peking University;Microsoft;Peking University;Microsoft;Microsoft;Institute of Computing Technology, Chinese Academy of Sciences;Peking University;Microsoft",59;22;-1;22;-1;-1;59;22;-1,1397;24;-1;24;-1;-1;1397;24;-1,3
3884,3884,3884,3884,3884,3884,3884,3884,ICLR,2020,Learning RNNs with Commutative State Transitions,Edo Cohen-Karlik;Amir Globerson,edocoh@gmail.com;amir.globerson@gmail.com,1;1;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Tel Aviv University;Tel Aviv University,35;35,188;188,
3885,3885,3885,3885,3885,3885,3885,3885,ICLR,2020,Deep Bayesian Structure Networks,Zhijie Deng;Yucen Luo;Jun Zhu;Bo Zhang,dzj17@mails.tsinghua.edu.cn;luoyc15@mails.tsinghua.edu.cn;dcszj@tsinghua.edu.cn;dcszb@tsinghua.edu.cn,3;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0.0,yes,9/25/19,Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University,8;8;8;8,23;23;23;23,11;2
3886,3886,3886,3886,3886,3886,3886,3886,ICLR,2020,RPGAN: random paths as a latent space for GAN interpretability,Andrey Voynov;Artem Babenko,an.voynov@gmail.com;artem.babenko@phystech.edu,3;3;8,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Yandex;Moscow Institute of Physics and Technology,-1;481,-1;234,5;4
3887,3887,3887,3887,3887,3887,3887,3887,ICLR,2020,MissDeepCausal: causal inference from incomplete data using deep latent variable models,Julie Josse;Imke Mayer;Jean-Philippe Vert,julie.josse@polytechnique.edu;imke.mayer@polytechnique.edu;jpvert@google.com,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Ecole polytechnique;Ecole polytechnique;Google,481;481;-1,93;93;-1,5
3888,3888,3888,3888,3888,3888,3888,3888,ICLR,2020,Active Learning Graph Neural Networks via Node Feature Propagation,Yuexin Wu;Yichong Xu;Aarti Singh;Artur Dubrawski;Yiming Yang,yuexinw@andrew.cmu.edu;yichongx@cs.cmu.edu;aarti@cs.cmu.edu;awd@cs.cmu.edu;yiming@cs.cmu.edu,3;1;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,2,4,0.0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1;1,27;27;27;27;27,1;10
3889,3889,3889,3889,3889,3889,3889,3889,ICLR,2020,DUAL ADVERSARIAL MODEL FOR GENERATING 3D POINT CLOUD,Yuhang Zhang;Zhenwei Miao;Tiebin Mi;Robert Caiming Qiu,hang_universe@sjtu.edu.cn;zhenwei.mzw@alibaba-inc.com;mitiebin@sjtu.edu.cn;rcqiu@sjtu.edu.cn,1;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Shanghai Jiao Tong University;Alibaba Group;Shanghai Jiao Tong University;Shanghai Jiao Tong University,53;-1;53;53,157;-1;157;157,5;4
3890,3890,3890,3890,3890,3890,3890,3890,ICLR,2020,LabelFool: A Trick in the Label Space,Yujia Liu;Tingting Jiang;Ming Jiang,yujia_liu@pku.edu.cn;ttjiang@pku.edu.cn;ming-jiang@pku.edu.cn,3;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,10,0.0,yes,9/25/19,Peking University;Peking University;Peking University,22;22;22,24;24;24,4
3891,3891,3891,3891,3891,3891,3891,3891,ICLR,2020,Unsupervised Generative 3D Shape Learning from Natural Images,Attila Szabo;Givi Meishvili;Paolo Favaro,attila.szabo@inf.unibe.ch;givi.meishvili@inf.unibe.ch;paolo.favaro@inf.unibe.ch,3;3;8,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,2,5,0.0,yes,9/25/19,University of Bern;University of Bern;University of Bern,390;390;390,113;113;113,5;4
3892,3892,3892,3892,3892,3892,3892,3892,ICLR,2020,Prestopping: How Does Early Stopping Help Generalization Against Label Noise?,Hwanjun Song;Minseok Kim;Dongmin Park;Jae-Gil Lee,songhwanjun@kaist.ac.kr;minseokkim@kaist.ac.kr;dongminpark@kaist.ac.kr;jaegil@kaist.ac.kr,3;3;6;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,2,9,0.0,yes,9/25/19,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,481;481;481;481,110;110;110;110,8
3893,3893,3893,3893,3893,3893,3893,3893,ICLR,2020,Deep Reinforcement Learning with Implicit Human Feedback,Duo Xu;Mohit Agarwal;Raghupathy Sivakumar;Faramarz Fekri,dxu3016@gatech.edu;me.agmohit@gatech.edu;siva@ece.gatech.edu;faramarz.fekri@ece.gatech.edu,3;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,1,0.0,yes,9/25/19,Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology,13;13;13;13,38;38;38;38,
3894,3894,3894,3894,3894,3894,3894,3894,ICLR,2020,"Manifold Modeling in Embedded Space: A Perspective for Interpreting Deep Image Prior""""",Tatsuya Yokota;Hidekata Hontani;Qibin Zhao;Andrzej Cichocki,t.yokota@nitech.ac.jp;hontani@nitech.ac.jp;qibin.zhao@riken.jp;a.cichocki@riken.jp,6;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,10,0.0,yes,9/25/19,Meiji University;Meiji University;RIKEN;RIKEN,481;481;-1;-1,332;332;-1;-1,2
3895,3895,3895,3895,3895,3895,3895,3895,ICLR,2020,Self-Imitation Learning via Trajectory-Conditioned Policy for Hard-Exploration Tasks,Yijie Guo;Jongwook Choi;Marcin Moczulski;Samy Bengio;Mohammad Norouzi;Honglak Lee,guoyijie@umich.edu;jwook@umich.edu;moczulski@google.com;bengio@google.com;mnorouzi@google.com;honglak@google.com,6;1;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,5,0.0,yes,9/25/19,University of Michigan;University of Michigan;Google;Google;Google;Google,8;8;-1;-1;-1;-1,21;21;-1;-1;-1;-1,
3896,3896,3896,3896,3896,3896,3896,3896,ICLR,2020,Improving Semantic Parsing with Neural Generator-Reranker Architecture,Huseyin A. Inan;Gaurav Singh Tomar;Huapu Pan,hinan1@stanford.edu;gtomar@google.com;huapupan@google.com,3;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Stanford University;Google;Google,4;-1;-1,4;-1;-1,3
3897,3897,3897,3897,3897,3897,3897,3897,ICLR,2020,Learning to Make Generalizable and Diverse Predictions for Retrosynthesis,Benson Chen;Tianxiao Shen;Tommi S. Jaakkola;Regina Barzilay,bensonc@mit.edu;tianxiao@mit.edu;tommi@csail.mit.edu;regina@csail.mit.edu,6;6;1,I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2,5;5;5;5,5
3898,3898,3898,3898,3898,3898,3898,3898,ICLR,2020,Benefit of Interpolation in Nearest Neighbor Algorithms,Yue Xing;Qifan Song;Guang Cheng,xing49@purdue.edu;qfsong@purdue.edu;chengg@purdue.edu,6;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Purdue University;Purdue University;Purdue University,27;27;27,88;88;88,
3899,3899,3899,3899,3899,3899,3899,3899,ICLR,2020,Learning to Reach Goals Without Reinforcement Learning,Dibya Ghosh;Abhishek Gupta;Justin Fu;Ashwin Reddy;Coline Devin;Benjamin Eysenbach;Sergey Levine,dibya.ghosh@berkeley.edu;abhigupta@berkeley.edu;justinjfu@eecs.berkeley.edu;adreddy@berkeley.edu;coline@berkeley.edu;beysenba@cs.cmu.edu;svlevine@eecs.berkeley.edu,6;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0.0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;Carnegie Mellon University;University of California Berkeley,5;5;5;5;5;1;5,13;13;13;13;13;27;13,
3900,3900,3900,3900,3900,3900,3900,3900,ICLR,2020,Refining the variational posterior through iterative optimization,Marton Havasi;Jasper Snoek;Dustin Tran;Jonathan Gordon;José Miguel Hernández-Lobato,mh740@cam.ac.uk;jsnoek@google.com;trandustin@google.com;jg801@cam.ac.uk;jmh233@cam.ac.uk,6;3;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0.0,yes,9/25/19,University of Cambridge;Google;Google;University of Cambridge;University of Cambridge,71;-1;-1;71;71,3;-1;-1;3;3,11;1
3901,3901,3901,3901,3901,3901,3901,3901,ICLR,2020,Off-policy Bandits with Deficient Support,Noveen Sachdeva;Yi Su;Thorsten Joachims,ernoveen@gmail.com;ys756@cornell.edu;tj@cs.cornell.edu,6;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,International Institute of Information Technology Hyderabad;Cornell University;Cornell University,205;7;7,713;19;19,
3902,3902,3902,3902,3902,3902,3902,3902,ICLR,2020,iSparse: Output Informed Sparsification of Neural Networks,Yash Garg;K. Selcuk Candan,ygarg@asu.edu;candan@asu.edu,1;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Arizona State University;Arizona State University,95;95,155;155,
3903,3903,3903,3903,3903,3903,3903,3903,ICLR,2020,Towards understanding the true loss surface of deep neural networks using random matrix theory and iterative spectral methods,Diego Granziol;Timur Garipov;Dmitry Vetrov;Stefan Zohren;Stephen Roberts;Andrew Gordon Wilson,diego@robots.ox.ac.uk;timgaripov@gmail.com;vetrovd@yandex.ru;zohren@robots.ox.ac.uk;sjrob@robots.ox.ac.uk;andrewgw@cims.nyu.edu,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,University of Oxford;Massachusetts Institute of Technology;Higher School of Economics;University of Oxford;University of Oxford;New York University,50;2;481;50;50;25,1;5;251;1;1;29,8
3904,3904,3904,3904,3904,3904,3904,3904,ICLR,2020,PROTOTYPE-ASSISTED ADVERSARIAL LEARNING FOR UNSUPERVISED DOMAIN ADAPTATION,Dapeng Hu;Jian Liang*;Qibin Hou;Hanshu Yan;Jiashi Feng,dapeng.hu@u.nus.edu;liangjian92@gmail.com;andrewhoux@gmail.com;hanshu.yan@u.nus.edu;elefjia@nus.edu.sg,3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.,Reject,0,5,0.0,yes,9/25/19,National University of Singapore;National University of Singapore;National University of Singapore;National University of Singapore;National University of Singapore,16;16;16;16;16,25;25;25;25;25,4;2;8
3905,3905,3905,3905,3905,3905,3905,3905,ICLR,2020,Likelihood Contribution based Multi-scale Architecture for Generative Flows,Hari Prasanna Das;Pieter Abbeel;Costas J. Spanos,hpdas@eecs.berkeley.edu;pabbeel@cs.berkeley.edu;spanos@eecs.berkeley.edu,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,10,0.0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,13;13;13,5
3906,3906,3906,3906,3906,3906,3906,3906,ICLR,2020,Understanding Top-k Sparsification in Distributed Deep Learning,Shaohuai Shi;Xiaowen Chu;Ka Chun Cheung;Simon See,csshshi@comp.hkbu.edu.hk;chxw@comp.hkbu.edu.hk;chcheung@nvidia.com;ssee@nvidia.com,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Boston University;Boston University;NVIDIA;NVIDIA,67;67;-1;-1,61;61;-1;-1,1
3907,3907,3907,3907,3907,3907,3907,3907,ICLR,2020,AN ATTENTION-BASED DEEP NET FOR LEARNING TO RANK,Diego Klabjan;Baiyang Wang,d-klabjan@northwestern.edu;baiyang@u.northwestern.edu,1;1;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Northwestern University;Northwestern University,44;44,22;22,
3908,3908,3908,3908,3908,3908,3908,3908,ICLR,2020,Modeling question asking using neural program generation,Ziyun Wang;Brenden M. Lake,ziyunw@nyu.edu;brenden@nyu.edu,6;1;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,5,0.0,yes,9/25/19,New York University;New York University,25;25,29;29,
3909,3909,3909,3909,3909,3909,3909,3909,ICLR,2020,Acutum: When Generalization Meets Adaptability,Xunpeng Huang;Zhengyang Liu;Zhe Wang;Yue Yu;Lei Li,huangxunpeng@bytedance.com;liuzhengyang.lozycs@bytedance.com;wang.10982@osu.edu;yuyue.elaine@bytedance.com;lilei.02@bytedance.com,3;1;6,I have published one or two papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Bytedance;Bytedance;Ohio State University;Bytedance;Bytedance,-1;-1;77;-1;-1,-1;-1;373;-1;-1,9;8
3910,3910,3910,3910,3910,3910,3910,3910,ICLR,2020,Pseudo-Labeling and Confirmation Bias in Deep Semi-Supervised Learning,Eric Arazo;Diego Ortego;Paul Albert;Noel E. O'Connor;Kevin McGuinness,eric.arazo@insight-centre.org;diego.ortego@insight-centre.org;paul.albert@insight-centre.org;noel.oconnor@dcu.ie;kevin.mcguinness@dcu.ie,3;8;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:N/A:N/A,Reject,0,5,0.0,yes,9/25/19,Insight Centre for Data Analytics;Insight Centre for Data Analytics;Insight Centre for Data Analytics;Dublin City University;Dublin City University,-1;-1;-1;481;481,-1;-1;-1;601;601,
3911,3911,3911,3911,3911,3911,3911,3911,ICLR,2020,Revisiting the Generalization of Adaptive Gradient Methods,Naman Agarwal;Rohan Anil;Elad Hazan;Tomer Koren;Cyril Zhang,namanagarwal@google.com;rohananil@google.com;ehazan@cs.princeton.edu;tkoren@google.com;cyril.zhang@princeton.edu,3;3;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Google;Google;Princeton University;Google;Princeton University,-1;-1;31;-1;31,-1;-1;6;-1;6,8
3912,3912,3912,3912,3912,3912,3912,3912,ICLR,2020,ConQUR: Mitigating Delusional Bias in Deep Q-Learning,DiJia-Andy Su;Jayden Ooi;Tyler Lu;Dale Schuurmans;Craig Boutilier‎,andy.2008.su@gmail.com;jayden@alum.mit.edu;tyler.lu@gmail.com;schuurmans@google.com;cboutilier@google.com,3;3;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,;Massachusetts Institute of Technology;Google;Google;Google,-1;2;-1;-1;-1,-1;5;-1;-1;-1,
3913,3913,3913,3913,3913,3913,3913,3913,ICLR,2020,Weight-space symmetry in neural network loss landscapes revisited,Berfin Simsek;Johanni Brea;Bernd Illing;Wulfram Gerstner,berfin.simsek@epfl.ch;johanni.brea@epfl.ch;bernd.illing@epfl.ch;wulfram.gerstner@epfl.ch,3;6;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne,481;481;481;481,38;38;38;38,1
3914,3914,3914,3914,3914,3914,3914,3914,ICLR,2020,Collaborative Filtering With A Synthetic Feedback Loop,Wenlin Wang;Hongteng Xu;Ruiyi Zhang;Wenqi Wang;Lawrence Carin,wlwang616@gmail.com;hongtengxu313@gmail.com;ryzhang@cs.duke.edu;wenqiwang@fb.com,6;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,0,0.0,yes,9/25/19,Duke University;Duke University;Duke University;Facebook,47;47;47;-1,20;20;20;-1,
3915,3915,3915,3915,3915,3915,3915,3915,ICLR,2020,Empowering Graph Representation Learning with Paired Training and Graph Co-Attention,Andreea Deac;Yu-Hsiang Huang;Petar Velickovic;Pietro Lio;Jian Tang,deacandr@mila.quebec;huang.yu-hsiang@courrier.uqam.ca;petar.velickovic@cst.cam.ac.uk;pl219@cam.ac.uk;jian.tang@hec.ca,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,University of Montreal;université du Québec à Montreal;University of Cambridge;University of Cambridge;HEC Montreal,128;128;71;71;128,85;85;3;3;85,10
3916,3916,3916,3916,3916,3916,3916,3916,ICLR,2020,Global Concavity and Optimization in a Class of Dynamic Discrete Choice Models,Yiding Feng;Ekaterina Khmelnitskaya;Denis Nekipelov,yidingfeng2021@u.northwestern.edu;eak5rf@virginia.edu;denis@virginia.edu,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Northwestern University;University of Virginia;University of Virginia,44;59;59,22;107;107,
3917,3917,3917,3917,3917,3917,3917,3917,ICLR,2020,Training Provably Robust Models by Polyhedral Envelope Regularization,Chen Liu;Mathieu Salzmann;Sabine Süsstrunk,chen.liu@epfl.ch;mathieu.salzmann@epfl.ch;sabine.susstrunk@epfl.ch,3;8;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne,481;481;481,38;38;38,4;1
3918,3918,3918,3918,3918,3918,3918,3918,ICLR,2020,Unifying Graph Convolutional Networks as Matrix Factorization,Zhaocheng Liu;Qiang Liu;Haoli Zhang;Jun Zhu,zhaocheng.liu@realai.ai;qiang.liu@realai.ai;haoli.zhang@realai.ai;dcszj@mail.tsinghua.edu.cn,1;6;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,RealAI;RealAI;RealAI;Tsinghua University,-1;-1;-1;8,-1;-1;-1;23,10
3919,3919,3919,3919,3919,3919,3919,3919,ICLR,2020,Adversarial Inductive Transfer Learning with input and output space adaptation,Hossein Sharifi-Noghabi;Shuman Peng;Olga Zolotareva;Colin C. Collins;Martin Ester,hsharifi@sfu.ca;shumanp@sfu.ca;ozolotareva@techfak.uni-bielefeld.de;ccollins@prostatecentre.com;ester@sfu.ca,3;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Simon Fraser University;Simon Fraser University;Bielefeld University;Prostatecentre;Simon Fraser University,64;64;323;-1;64,272;272;166;-1;272,4;6
3920,3920,3920,3920,3920,3920,3920,3920,ICLR,2020,Leveraging Simple Model Predictions for Enhancing its Performance,Amit Dhurandhar;Karthikeyan Shanmugam;Ronny Luss,adhuran@us.ibm.com;karthikeyan.shanmugam2@ibm.com;rluss@us.ibm.com,6;6;6;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,1,5,0.0,yes,9/25/19,International Business Machines;International Business Machines;International Business Machines,-1;-1;-1,-1;-1;-1,
3921,3921,3921,3921,3921,3921,3921,3921,ICLR,2020,Learning to Transfer via Modelling Multi-level Task Dependency,Haonan Wang;Zhenbang Wu;Ziniu Hu;Yizhou Sun,haonan3@illinois.edu;zw12@illinois.edu;bull@cs.ucla.edu;yzsun@cs.ucla.edu,1;3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,4,0.0,yes,9/25/19,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;University of California, Los Angeles;University of California, Los Angeles",3;3;20;20,48;48;17;17,
3922,3922,3922,3922,3922,3922,3922,3922,ICLR,2020,Disentangling Style and Content in Anime Illustrations,Sitao Xiang;Hao Li,sitaoxia@usc.edu;hao@hao-li.com,3;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,University of Southern California;Hao-li,31;-1,62;-1,5;4
3923,3923,3923,3923,3923,3923,3923,3923,ICLR,2020,Targeted sampling of enlarged neighborhood via Monte Carlo tree search for TSP,Zhang-Hua Fu;Kai-Bin Qiu;Meng Qiu;Hongyuan Zha,fuzhanghua@cuhk.edu.cn;20150008030@m.scnu.edu.cn;qiumeng.sz@gmail.com;zhahy@cuhk.edu.cn,1;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.,Reject,0,0,0.0,yes,9/25/19,Tsinghua University;Australian National University;The Chinese University of Hong Kong;Tsinghua University,8;108;59;8,23;50;35;23,
3924,3924,3924,3924,3924,3924,3924,3924,ICLR,2020,S2VG: Soft Stochastic Value Gradient method,Xiaoyu Tan;Chao Qu;Junwu Xiong;James Zhang,xiaoyu_tan@u.nus.edu;chaoqu.technion@gmail.com;junwu.xjw@antfin.com;james.z@antfin.com,1;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,National University of Singapore;;Antfin;Antfin,16;-1;-1;-1,25;-1;-1;-1,
3925,3925,3925,3925,3925,3925,3925,3925,ICLR,2020,Learning to Defense by Learning to Attack,Zhehui Chen;Haoming Jiang;Yuyang Shi;Bo Dai;Tuo Zhao,zhchen@gatech.edu;jianghm@gatech.edu;yyshi@gatech.edu;bodai@google.com;tourzhao@gatech.edu,6;3;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,5,0.0,yes,9/25/19,Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Google;Georgia Institute of Technology,13;13;13;-1;13,38;38;38;-1;38,5;4
3926,3926,3926,3926,3926,3926,3926,3926,ICLR,2020,Model Architecture Controls Gradient Descent Dynamics: A Combinatorial Path-Based Formula,Xin Zhou;Newsha Ardalani,chow459@gmail.com;newsha@baidu.com,3;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,;Baidu,-1;-1,-1;-1,9
3927,3927,3927,3927,3927,3927,3927,3927,ICLR,2020,Natural- to formal-language generation using Tensor Product Representations,Kezhen Chen;Qiuyuan Huang;Hamid Palangi;Paul Smolensky;Kenneth D. Forbus;Jianfeng Gao,kezhenchen2021@u.northwestern.edu;qihua@microsoft.com;hpalangi@microsoft.com;paul.smolensky@gmail.com;forbus@northwestern.edu;jfgao@microsoft.com,8;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:N/A:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,8,0.0,yes,9/25/19,Northwestern University;Microsoft;Microsoft;Microsoft;Northwestern University;Microsoft,44;-1;-1;-1;44;-1,22;-1;-1;-1;22;-1,
3928,3928,3928,3928,3928,3928,3928,3928,ICLR,2020,Unsupervised Out-of-Distribution Detection with Batch Normalization,Jiaming Song;Yang Song;Stefano Ermon,jiaming.tsong@gmail.com;yangsong@cs.stanford.edu;ermon@cs.stanford.edu,1;6;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Stanford University;Stanford University;Stanford University,4;4;4,4;4;4,5
3929,3929,3929,3929,3929,3929,3929,3929,ICLR,2020,Distribution Matching Prototypical Network for Unsupervised Domain Adaptation,Lei Zhu;Wei Wang;Mei Hui Zhang;Beng Chin Ooi;Chang Yao,e0203764@u.nus.edu;wangwei@comp.nus.edu.sg;meihui_zhang@bit.edu.cn;ooibc@comp.nus.edu.sg;yaochang@zjuici.com,3;3;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,10,0.0,yes,9/25/19,National University of Singapore;National University of Singapore;BIT;National University of Singapore;Zhejiang University,16;16;-1;16;56,25;25;-1;25;107,
3930,3930,3930,3930,3930,3930,3930,3930,ICLR,2020,Deep Hierarchical-Hyperspherical Learning (DH^2L),Youngsung Kim;Jae-Joon Han,yskim.ee@gmail.com;jae-joon.han@samsung.com,3;6;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Samsung;Samsung,-1;-1,-1;-1,8
3931,3931,3931,3931,3931,3931,3931,3931,ICLR,2020,Neural Markov Logic Networks,Giuseppe Marra;Ondřej Kuželka,g.marra@unifi.it;kuzelo1@gmail.com,6;1;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,University of Florence;Czech Technical University in Prague,481;323,378;956,1
3932,3932,3932,3932,3932,3932,3932,3932,ICLR,2020,Deep Randomized Least Squares Value Iteration,Guy Adam;Tom Zahavy;Oron Anschel;Nahum Shimkin,guyadam3@gmail.com;tomzahavy@gmail.com;oronanschel@gmail.com;shimkin@ee.technion.ac.il,1;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,6,0.0,yes,9/25/19,Technion;Technion;;Technion,26;26;-1;26,412;412;-1;412,
3933,3933,3933,3933,3933,3933,3933,3933,ICLR,2020,Safe Policy Learning for Continuous Control,Yinlam Chow;Ofir Nachum;Aleksandra Faust;Edgar Duenez-Guzman;Mohammad Ghavamzadeh,yinlamchow@google.com;ofirnachum@google.com;sandrafaust@google.com;duenez@google.com;mgh@fb.com,6;8;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,6,2.0,yes,9/25/19,Google;Google;Google;Google;Facebook,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,
3934,3934,3934,3934,3934,3934,3934,3934,ICLR,2020,Unsupervised Distillation of Syntactic Information from Contextualized Word Representations,Shauli Ravfogel;Yanai Elazar;Jacob Goldberger;Yoav Goldberg,shauli.ravfogel@gmail.com;yanaiela@gmail.com;jacob.goldberger@biu.ac.il;yogo@cs.biu.ac.il,6;8;6;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,12,0.0,yes,9/25/19,Bar Ilan University;Bar Ilan University;Bar Ilan University;Bar Ilan University,95;95;95;95,513;513;513;513,3;6
3935,3935,3935,3935,3935,3935,3935,3935,ICLR,2020,Discriminator Based Corpus Generation for General Code Synthesis,Alexander Wild;Barry Porter,a.wild3@lancaster.ac.uk;b.f.porter@lancaster.ac.uk,1;1;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:N/A:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,2,1,0.0,yes,9/25/19,Lancaster University;Lancaster University,233;233,140;140,
3936,3936,3936,3936,3936,3936,3936,3936,ICLR,2020,Situating Sentence Embedders with Nearest Neighbor Overlap,Lucy H. Lin;Noah A. Smith,lucylin@cs.washington.edu;nasmith@cs.washington.edu,3;1;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,2,0.0,yes,9/25/19,University of Washington;University of Washington,6;6,26;26,3
3937,3937,3937,3937,3937,3937,3937,3937,ICLR,2020,Lattice Representation Learning,Luis A Lastras,lastrasl@us.ibm.com,3;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,International Business Machines,-1,-1,1
3938,3938,3938,3938,3938,3938,3938,3938,ICLR,2020,Towards Understanding the Transferability of Deep Representations,Hong Liu;Mingsheng Long;Jianmin Wang;Michael I. Jordan,h-l17@mails.tsinghua.edu.cn;mingsheng@tsinghua.edu.cn;jimwang@tsinghua.edu.cn;jordan@cs.berkeley.edu,3;3;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Tsinghua University;Tsinghua University;Tsinghua University;University of California Berkeley,8;8;8;5,23;23;23;13,8
3939,3939,3939,3939,3939,3939,3939,3939,ICLR,2020,Improved Training of Certifiably Robust Models,Chen Zhu;Renkun Ni;Ping-yeh Chiang;Hengduo Li;Furong Huang;Tom Goldstein,chenzhu@cs.umd.edu;rn9zm@cs.umd.edu;pingyeh.chiang@gmail.com;hdli@cs.umd.edu;furongh@cs.umd.edu;tomg@cs.umd.edu,6;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,"University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park",12;12;12;12;12;12,91;91;91;91;91;91,4
3940,3940,3940,3940,3940,3940,3940,3940,ICLR,2020,Learning from Label Proportions with Consistency Regularization,Kuen-Han Tsai;Hsuan-Tien Lin,r06922066@csie.ntu.edu.tw;htlin@csie.ntu.edu.tw,3;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,National Taiwan University;National Taiwan University,86;86,120;120,
3941,3941,3941,3941,3941,3941,3941,3941,ICLR,2020,Learning Compact Embedding Layers via Differentiable Product Quantization,Ting Chen;Lala Li;Yizhou Sun,iamtingchen@gmail.com;lala@google.com;yzsun@cs.ucla.edu,3;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,"Google;Google;University of California, Los Angeles",-1;-1;20,-1;-1;17,
3942,3942,3942,3942,3942,3942,3942,3942,ICLR,2020,{COMPANYNAME}11K: An Unsupervised Representation Learning Dataset for Arrhythmia Subtype Discovery,Shawn Tan;Guillaume Androz;Ahmad Chamseddine;Pierre Fecteau;Aaron Courville;Yoshua Bengio;Joseph Paul Cohen,shawn@wtf.sg;guillaume.androz@icentia.com;doctor.ahmad89@gmail.com;pierre.fecteau@icentia.com;aaron.courville@gmail.com;yoshua.bengio@mila.quebec;joseph@josephpcohen.com,3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,2,0.0,yes,9/25/19,;Icentia Inc.;Polytechnique Montreal;Icentia;University of Montreal;University of Montreal;University of Montreal,-1;-1;390;-1;128;128;128,-1;-1;1397;-1;85;85;85,
3943,3943,3943,3943,3943,3943,3943,3943,ICLR,2020,NADS: Neural Architecture Distribution Search for Uncertainty Awareness,Randy Ardywibowo;Shahin Boluki;Xinyu Gong;Zhangyang Wang;Xiaoning Qian,randyardywibowo@tamu.edu;s.boluki@tamu.edu;gong1994@tamu.edu;atlaswang@tamu.edu;xqian@tamu.edu,8;1;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Texas A&M;Texas A&M;Texas A&M;Texas A&M;Texas A&M,44;44;44;44;44,177;177;177;177;177,11
3944,3944,3944,3944,3944,3944,3944,3944,ICLR,2020,Learning World Graph Decompositions To Accelerate Reinforcement Learning,Wenling Shang;Alex Trott;Stephan Zheng;Caiming Xiong;Richard Socher,w.shang@uva.nl;atrott@salesforce.com;stephan.zheng@salesforce.com;cxiong@salesforce.com;richard@socher.org,6;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0.0,yes,9/25/19,University of Amsterdam;SalesForce.com;SalesForce.com;SalesForce.com;SalesForce.com,172;-1;-1;-1;-1,62;-1;-1;-1;-1,5;10
3945,3945,3945,3945,3945,3945,3945,3945,ICLR,2020,Group-Transformer: Towards A Lightweight Character-level Language Model,Sungrae Park;Geewook Kim;Junyeop Lee;Junbum Cha;Ji-Hoon Kim Hwalsuk Lee,sungrae.park@navercorp.com;geewook@sys.i.kyoto-u.ac.jp;junyeop.lee@navercorp.com;junbum.cha@navercorp.com;genesis.kim@navercorp.com;hwalsuk.lee@navercorp.com,6;6;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,10,0.0,yes,9/25/19,NAVER;Meiji University;NAVER;NAVER;NAVER;NAVER,-1;481;-1;-1;-1;-1,-1;332;-1;-1;-1;-1,3
3946,3946,3946,3946,3946,3946,3946,3946,ICLR,2020,Learning DNA folding patterns with Recurrent Neural Networks ,Michal Rozenwald;Aleksandra Galitsyna;Ekaterina Khrameeva;Grigory Sapunov;Mikhail S. Gelfand,michal.rozenwald@gmail.com;agalitzina@gmail.com;ekhrameeva@gmail.com;grigory.sapunov@gmail.codelfm;mikhail.gelfand@gmail.com,1;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,;Skolkovo Institute of Science and Technology;;;,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,
3947,3947,3947,3947,3947,3947,3947,3947,ICLR,2020,TSInsight: A local-global attribution framework for interpretability in time-series data,Shoaib Ahmed Siddiqui;Dominique Mercier;Andreas Dengel;Sheraz Ahmed,shoaib_ahmed.siddiqui@dfki.de;dominique.mercier@dfki.de;andreas.dengel@dfki.de;sheraz.ahmed@dfki.de,3;1;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,German Research Center for AI;German Research Center for AI;German Research Center for AI;German Research Center for AI,-1;-1;-1;-1,-1;-1;-1;-1,4
3948,3948,3948,3948,3948,3948,3948,3948,ICLR,2020,Imagining the Latent Space of a Variational Auto-Encoders,Zezhen Zeng;Jonathon Hare;Adam Prügel-Bennett,zz8n17@ecs.soton.ac.uk;jsh2@ecs.soton.ac.uk;apb@ecs.soton.ac.uk,3;3;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,University of Southampton;University of Southampton;University of Southampton,172;172;172,122;122;122,5
3949,3949,3949,3949,3949,3949,3949,3949,ICLR,2020,Quantum Semi-Supervised Kernel Learning,Seyran Saeedi;Aliakbar Panahi;Tom Arodz,saeedis@vcu.edu;panahia@vcu.edu;tarodz@vcu.edu,6;6;6,I do not know much about this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:N/A:N/A:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Virginia Commonwealth University;Virginia Commonwealth University;Virginia Commonwealth University,266;266;266,1397;1397;1397,
3950,3950,3950,3950,3950,3950,3950,3950,ICLR,2020,Interpretable Network Structure for Modeling Contextual Dependency,Xindian Ma;Peng Zhang;Xiaoliu Mao;Yehua Zhang;Nan Duan;Yuexian Hou;Ming Zhou.,xindianma@tju.edu.cn;pzhang@tju.edu.cn;xiaoliumao@tju.edu.cn;yehua_zhang@tju.edu.cn;nanduan@microsoft.com;yxhou@tju.edu.cn;mingzhou@microsoft.com,3;1;3,I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,3,0.0,yes,9/25/19,Zhejiang University;Zhejiang University;Zhejiang University;Zhejiang University;Microsoft;Zhejiang University;Microsoft,56;56;56;56;-1;56;-1,107;107;107;107;-1;107;-1,3;1
3951,3951,3951,3951,3951,3951,3951,3951,ICLR,2020,Frequency Principle: Fourier Analysis Sheds Light on Deep Neural Networks,Zhi-Qin John Xu;Yaoyu Zhang;Tao Luo;Yanyang Xiao;Zheng Ma,xuzhiqin@sjtu.edu.cn;yaoyu@ias.edu;luo196@purdue.edu;xyy82148@gmail.com;ma531@purdue.edu,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,"Shanghai Jiao Tong University;Institue for Advanced Study, Princeton;Purdue University;;Purdue University",53;-1;27;-1;27,157;-1;88;-1;88,8
3952,3952,3952,3952,3952,3952,3952,3952,ICLR,2020,S-Flow GAN,Miron Yakov;Coscas Yona,yakov.miron@gmail.com;yona.coscas@gmail.com,1;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:N/A:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,0,0.0,yes,9/25/19,;,-1;-1,-1;-1,5;4;10
3953,3953,3953,3953,3953,3953,3953,3953,ICLR,2020,Independence-aware Advantage Estimation,Pushi Zhang;Li Zhao;Guoqing Liu;Jiang Bian;Minglie Huang;Tao Qin;Tie-Yan Liu,zpschang@gmail.com;lizo@microsoft.com;lgq1001@mail.ustc.edu.cn;jiang.bian@microsoft.com;aihuang@mails.tsinghua.edu.cn;taoqin@microsoft.com;tie-yan.liu@microsoft.com,6;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,Tsinghua University;Microsoft;University of Science and Technology of China;Microsoft;Tsinghua University;Microsoft;Microsoft,8;-1;481;-1;8;-1;-1,23;-1;80;-1;23;-1;-1,
3954,3954,3954,3954,3954,3954,3954,3954,ICLR,2020,Role of two learning rates in convergence of model-agnostic meta-learning,Shiro Takagi;Yoshihiro Nagano;Yuki Yoshida;Masato Okada,takagi@mns.k.u-tokyo.ac.jp;nagano@mns.k.u-tokyo.ac.jp;yoshida@mns.k.u-tokyo.ac.jp;okada@edu.k.u-tokyo.ac.jp,3;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0.0,yes,9/25/19,The University of Tokyo;The University of Tokyo;The University of Tokyo;The University of Tokyo,56;56;56;56,36;36;36;36,1;6
3955,3955,3955,3955,3955,3955,3955,3955,ICLR,2020,Neural Subgraph Isomorphism Counting,Xin Liu;Haojie Pan;Mutian He;Yangqiu Song;Xin Jiang,xliucr@cse.ust.hk;hpanad@cse.ust.hk;mhear@cse.ust.hk;yqsong@cse.ust.hk;jiang.xin@huawei.com,6;3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,5,0.0,yes,9/25/19,The Hong Kong University of Science and Technology;The Hong Kong University of Science and Technology;The Hong Kong University of Science and Technology;The Hong Kong University of Science and Technology;Huawei Technologies Ltd.,39;39;39;39;-1,47;47;47;47;-1,10
3956,3956,3956,3956,3956,3956,3956,3956,ICLR,2020,Semantic Hierarchy Emerges in the Deep Generative Representations for Scene Synthesis,Ceyuan Yang;Yujun Shen;Bolei Zhou,limbo0066@gmail.com;sy116@ie.cuhk.edu.hk;bzhou@ie.cuhk.edu.hk,6;3;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,The Chinese University of Hong Kong;The Chinese University of Hong Kong;The Chinese University of Hong Kong,59;59;59,35;35;35,5;4
3957,3957,3957,3957,3957,3957,3957,3957,ICLR,2020,Decoupling Adaptation from Modeling with Meta-Optimizers for Meta Learning,Sébastien M.R. Arnold;Shariq Iqbal;Fei Sha,arnolds@usc.edu;shariqiqbal2810@gmail.com;fsha@google.com,3;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,University of Southern California;University of Southern California;Google,31;31;-1,62;62;-1,6
3958,3958,3958,3958,3958,3958,3958,3958,ICLR,2020,Black-box Adversarial Attacks with Bayesian Optimization,Satya Narayan Shukla;Anit Kumar Sahu;Devin Willmott;J. Zico Kolter,snshukla@cs.umass.edu;anit.sahu@gmail.com;devin.willmott@uky.edu;zkolter@cs.cmu.edu,6;3;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,"University of Massachusetts, Amherst;;University of Kentucky;Carnegie Mellon University",28;-1;233;1,209;-1;490;27,4;11
3959,3959,3959,3959,3959,3959,3959,3959,ICLR,2020,Adversarial Imitation Attack,Mingyi Zhou;Jing Wu;Yipeng Liu;Xiaolin Huang;Shuaicheng Liu;Liaqat Ali;Xiang Zhang;Ce Zhu,zhoumingyi@std.uestc.edu.cn;wujing@std.uestc.edu.cn;yipengliu@uestc.edu.cn;xiaolinhuang@sjtu.edu.cn;liushuaicheng@uestc.edu.cn;engr_liaqat183@yahoo.com;uestchero@uestc.edu.cn;eczhu@uestc.edu.cn,6;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,University of Electronic Science and Technology of China;University of Electronic Science and Technology of China;University of Electronic Science and Technology of China;Shanghai Jiao Tong University;University of Electronic Science and Technology of China;;University of Electronic Science and Technology of China;University of Electronic Science and Technology of China,481;481;481;53;481;-1;481;481,628;628;628;157;628;-1;628;628,5;4
3960,3960,3960,3960,3960,3960,3960,3960,ICLR,2020,LOGAN:  Latent Optimisation for Generative Adversarial Networks,Yan Wu;Jeff Donahue;David Balduzzi;Karen Simonyan;Timothy Lillicrap,yanwu@google.com;jeffdonahue@google.com;dbalduzzi@google.com;simonyan@google.com;countzero@google.com,6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,2,2.0,yes,9/25/19,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,5;4
3961,3961,3961,3961,3961,3961,3961,3961,ICLR,2020,Searching for Stage-wise Neural Graphs In the Limit,Xin Zhou;Dejing Dou;Boyang Li,chow459@gmail.com;doudejing@baidu.com;libo0001@gmail.com,3;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,3,0.0,yes,9/25/19,;Baidu;National Taiwan University,-1;-1;86,-1;-1;120,10
3962,3962,3962,3962,3962,3962,3962,3962,ICLR,2020,Attention Interpretability Across NLP Tasks,Shikhar Vashishth;Shyam Upadhyay;Gaurav Singh Tomar;Manaal Faruqui,shikhar@iisc.ac.in;shyamupa@google.com;gtomar@google.com;mfaruqui@google.com,6;6;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,1,5,0.0,yes,9/25/19,Indian Institute of Science;Google;Google;Google,95;-1;-1;-1,301;-1;-1;-1,3
3963,3963,3963,3963,3963,3963,3963,3963,ICLR,2020,LSTOD: Latent Spatial-Temporal Origin-Destination prediction model and its applications in ride-sharing platforms,Fan Zhou;Haibo Zhou;Hongtu Zhu,zhoufan@mail.shufe.edu.cn;zhou@bios.unc.edu;zhuhongtu@didiglobal.com,1;6;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,3,0.0,yes,9/25/19,"Tsinghua University;University of North Carolina, Chapel Hill;DiDi AI Labs, Didi Chuxing",8;73;-1,23;54;-1,10
3964,3964,3964,3964,3964,3964,3964,3964,ICLR,2020,Unsupervised Universal Self-Attention Network for Graph Classification,Dai Quoc Nguyen;Tu Dinh Nguyen;Dinh Phung,dai.nguyen@monash.edu;tu.dinh.nguyen@monash.edu;dinh.phung@monash.edu,3;1;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,6,7,0.0,yes,9/25/19,Monash University;Monash University;Monash University,118;118;118,75;75;75,10
3965,3965,3965,3965,3965,3965,3965,3965,ICLR,2020,Efficacy of Pixel-Level OOD Detection for Semantic Segmentation,Matt Angus;Krzysztof Czarnecki;Rick Salay,m2angus@gsd.uwaterloo.ca;rsalay@gsd.uwaterloo.ca;k2czarne@gsd.uwaterloo.ca,3;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,University of Waterloo;University of Waterloo;University of Waterloo,28;28;28,235;235;235,2
3966,3966,3966,3966,3966,3966,3966,3966,ICLR,2020,CURSOR-BASED ADAPTIVE QUANTIZATION FOR DEEP NEURAL NETWORK,Bapu Li(*);Yanwen Fan(*);Zhiyu Cheng;Yingze Bao (* means equal contribution),baopuli@baidu.com;fanyanwen@baidu.com;zhiyucheng@baidu.com;baoyingze@baidu.com,3;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Baidu;Baidu;Baidu;Baidu,-1;-1;-1;-1,-1;-1;-1;-1,9
3967,3967,3967,3967,3967,3967,3967,3967,ICLR,2020,MMD GAN with Random-Forest Kernels,Tao Huang;Zhen Han;Xu Jia;Hanyuan Hang,tao.huang2018@ruc.edu.cn;handarkholme@ruc.edu.cn;jiayushenyang@gmail.com;hanyuan0725@gmail.com,1;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,"University of Illinois, Urbana-Champaign;University of Illinois, Urbana-Champaign;Huawei Technologies Ltd.;University of Illinois, Urbana-Champaign",3;3;-1;3,48;48;-1;48,5
3968,3968,3968,3968,3968,3968,3968,3968,ICLR,2020,Hyperparameter Tuning and Implicit Regularization in Minibatch SGD,Samuel L Smith;Erich Elsen;Soham De,slsmith@google.com;eriche@google.com;sohamde@google.com,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,1,4,0.0,yes,9/25/19,Google;Google;Google,-1;-1;-1,-1;-1;-1,
3969,3969,3969,3969,3969,3969,3969,3969,ICLR,2020,Quantum Optical Experiments Modeled by Long Short-Term Memory,Thomas Adler;Manuel Erhard;Mario Krenn;Johannes Brandstetter;Johannes Kofler;Sepp Hochreiter,adler@ml.jku.at;manuel.erhard@univie.ac.at;mario.krenn@univie.ac.at;brandstetter@ml.jku.at;kofler@ml.jku.at;hochreit@ml.jku.at,3;1;1,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Johannes Kepler University Linz;University of Vienna;University of Vienna;Johannes Kepler University Linz;Johannes Kepler University Linz;Johannes Kepler University Linz,481;205;205;481;481;481,620;134;134;620;620;620,5
3970,3970,3970,3970,3970,3970,3970,3970,ICLR,2020,Variational Hashing-based Collaborative Filtering with Self-Masking,Casper Hansen;Christian Hansen;Jakob Grue Simonsen;Stephen Alstrup;Christina Lioma,c.hansen@di.ku.dk;chrh@di.ku.dk;simonsen@di.ku.dk;s.alstrup@di.ku.dk;c.lioma@di.ku.dk,3;3;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,University of Copenhagen;University of Copenhagen;University of Copenhagen;University of Copenhagen;University of Copenhagen,100;100;100;100;100,101;101;101;101;101,
3971,3971,3971,3971,3971,3971,3971,3971,ICLR,2020,Customizing Sequence Generation with Multi-Task Dynamical Systems,Alex Bird;Christopher K. I. Williams,abird@turing.ac.uk;ckiw@inf.ed.ac.uk,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Alan Turing Institute;University of Edinburgh,-1;33,-1;30,
3972,3972,3972,3972,3972,3972,3972,3972,ICLR,2020,Batch Normalization is a Cause of Adversarial Vulnerability,Angus Galloway;Anna Golubeva;Thomas Tanay;Medhat Moussa;Graham W. Taylor,gallowaa@uoguelph.ca;agolubeva@perimeterinstitute.ca;thomas.tanay.13@ucl.ac.uk;mmoussa@uoguelph.ca;gwtaylor@uoguelph.ca,3;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,3,8,0.0,yes,9/25/19,University of Guelph;Perimeter Institute;University College London;University of Guelph;University of Guelph,266;-1;50;266;266,558;-1;15;558;558,4
3973,3973,3973,3973,3973,3973,3973,3973,ICLR,2020,Enhancing Language Emergence through Empathy,Marie Ossenkopf,mos@vs.uni-kassel.de,1;1;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,1,0.0,yes,9/25/19,University of Kassel,390,94,3
3974,3974,3974,3974,3974,3974,3974,3974,ICLR,2020,A Data-Efficient Mutual Information Neural Estimator for Statistical Dependency Testing,Xiao Lin;Indranil Sur;Samuel A. Nastase;Uri Hasson;Ajay Divakaran;Mohamed R. Amer,xiao.lin@sri.com;indranil.sur@sri.com;mohamed.rabie.amer@gmail.com;ajay.divakaran@sri.com;snastase@princeton.edu;hasson@princeton.edu,1;6;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,SRI International;SRI International;Robust.AI;SRI International;Princeton University;Princeton University,-1;-1;-1;-1;31;31,-1;-1;-1;-1;6;6,1;6
3975,3975,3975,3975,3975,3975,3975,3975,ICLR,2020,SMiRL: Surprise Minimizing RL in Entropic Environments,Glen Berseth;Daniel Geng;Coline Devin;Dinesh Jayaraman;Chelsea Finn;Sergey Levine,gberseth@gmail.com;dangengdg@berkeley.edu;coline.devin@gmail.com;dinesh.jayaraman123@gmail.com;cbfinn@eecs.berkeley.edu;svlevine@eecs.berkeley.edu,6;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,5,0.0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley;Facebook;University of California Berkeley;University of California Berkeley,5;5;5;-1;5;5,13;13;13;-1;13;13,11
3976,3976,3976,3976,3976,3976,3976,3976,ICLR,2020,The Role of Embedding Complexity in Domain-invariant Representations,Ching-Yao Chuang;Antonio Torralba;Stefanie Jegelka,cychuang@mit.edu;torralba@mit.edu;stefje@mit.edu,3;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2,5;5;5,1;8
3977,3977,3977,3977,3977,3977,3977,3977,ICLR,2020,Learning to Infer User Interface Attributes from Images,Philippe Schlattner;Pavol Bielik;Martin Vechev,pschlatt@ethz.ch;pavol.bielik@inf.ethz.ch;martin.vechev@inf.ethz.ch,1;3;8,I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0.0,yes,9/25/19,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10,13;13;13,10
3978,3978,3978,3978,3978,3978,3978,3978,ICLR,2020,Semi-supervised Learning by Coaching,Hieu Pham;Quoc V. Le,hyhieu@cmu.edu;qvl@google.com,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,Carnegie Mellon University;Google,1;-1,27;-1,
3979,3979,3979,3979,3979,3979,3979,3979,ICLR,2020,Learning Representations in Reinforcement Learning: an Information Bottleneck Approach,Yingjun Pei;Xinwen Hou,peiyingjun4@gmail.com;xwhou@nlpr.ia.ac.cn,3;6;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,"Beijing University of Post and Telecommunication;Institute of automation, Chinese academy of science, Chinese Academy of Sciences",481;59,1397;1397,1
3980,3980,3980,3980,3980,3980,3980,3980,ICLR,2020,Evaluating Lossy Compression Rates of Deep Generative Models,Sicong Huang;Alireza Makhzani;Yanshuai Cao;Roger Grosse,huang@cs.toronto.edu;a.makhzani@gmail.com;yanshuai.cao@borealisai.com;rgrosse@cs.toronto.edu,3;8;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,"Department of Computer Science, University of Toronto;;Borealis AI;Department of Computer Science, University of Toronto",18;-1;-1;18,18;-1;-1;18,5;4
3981,3981,3981,3981,3981,3981,3981,3981,ICLR,2020,Kronecker Attention Networks,Hongyang Gao;Zhengyang Wang;Shuiwang Ji,hongyang.gao@tamu.edu;zhengyang.wang@tamu.edu;sji@tamu.edu,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Texas A&M;Texas A&M;Texas A&M,44;44;44,177;177;177,
3982,3982,3982,3982,3982,3982,3982,3982,ICLR,2020,Stochastic Neural Physics Predictor,Piotr Tatarczyk;Damian Mrowca;Li Fei-Fei;Daniel L. K. Yamins;Nils Thuerey,piotr.tatarczyk@tum.de;mrowca@stanford.edu;feifeili@cs.stanford.edu;yamins@stanford.edu;nils.thuerey@tum.de,3;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Technical University Munich;Stanford University;Stanford University;Stanford University;Technical University Munich,53;4;4;4;53,43;4;4;4;43,10
3983,3983,3983,3983,3983,3983,3983,3983,ICLR,2020,Spectral Nonlocal Block for Neural Network,Lei Zhu;Qi She;Lidan Zhang;Ping guo,lei1.zhu@intel.com;qi.she@intel.com;lidan.zhang@intel.com;ping.guo@intel.com,6;6;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,Intel;Intel;Intel;Intel,-1;-1;-1;-1,-1;-1;-1;-1,2;10
3984,3984,3984,3984,3984,3984,3984,3984,ICLR,2020,Metagross: Meta Gated Recursive Controller Units for Sequence Modeling,Yi Tay;Yikang Shen;Alvin Chan;Yew Soon Ong,ytay017@e.ntu.edu.sg;yikang.shn@gmail.com;guoweial001@e.ntu.edu.sg;asysong@ntu.edu.sg,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,National Taiwan University;University of Montreal;National Taiwan University;National Taiwan University,86;128;86;86,120;85;120;120,3
3985,3985,3985,3985,3985,3985,3985,3985,ICLR,2020,Diving into Optimization of Topology in Neural Networks,Kun Yuan;Quanquan Li;Yucong Zhou;Jing Shao;Junjie Yan,yuankun@sensetime.com;liquanquan@sensetime.com;zhouyucong@sensetime.com;shaojing@sensetime.com;yanjunjie@sensetime.com,6;6;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,SenseTime Group Limited;SenseTime Group Limited;SenseTime Group Limited;SenseTime Group Limited;SenseTime Group Limited,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,8
3986,3986,3986,3986,3986,3986,3986,3986,ICLR,2020,The Usual Suspects? Reassessing Blame for VAE Posterior Collapse,Bin Dai;Ziyu Wang;David Wipf,daib13@mails.tsinghua.edu.cn;wzy196@gmail.com;davidwipf@gmail.com,3;8;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0.0,yes,9/25/19,Tsinghua University;Tsinghua University;Microsoft,8;8;-1,23;23;-1,5;1
3987,3987,3987,3987,3987,3987,3987,3987,ICLR,2020,Generative Restricted Kernel Machines,Arun Pandey;Joachim Schreurs;Johan A.K. Suykens,arun.pandey@esat.kuleuven.be;joachim.schreurs@esat.kuleuven.be;johan.suykens@esat.kuleuven.be,6;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,1.0,yes,9/25/19,KU Leuven;KU Leuven;KU Leuven,118;118;118,45;45;45,5
3988,3988,3988,3988,3988,3988,3988,3988,ICLR,2020,GNN-FiLM: Graph Neural Networks with Feature-wise Linear Modulation,Marc Brockschmidt,mabrocks@microsoft.com,6;8;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,14,0.0,yes,9/25/19,Microsoft,-1,-1,10
3989,3989,3989,3989,3989,3989,3989,3989,ICLR,2020,Which Tasks Should Be Learned Together in Multi-task Learning?,Trevor Standley;Amir R. Zamir;Dawn Chen;Leonidas Guibas;Jitendra Malik;Silvio Savarese,tstand@cs.stanford.edu;zamir@cs.stanford.edu;sdawnchen@gmail.com;guibas@cs.stanford.edu;malik@eecs.berkeley.edu;ssilvio@stanford.edu,6;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Stanford University;Stanford University;;Stanford University;University of California Berkeley;Stanford University,4;4;-1;4;5;4,4;4;-1;4;13;4,2
3990,3990,3990,3990,3990,3990,3990,3990,ICLR,2020,Distillation $\approx$ Early Stopping? Harvesting Dark Knowledge Utilizing Anisotropic Information Retrieval For Overparameterized NN,Bin Dong;Jikai Hou;Yiping Lu;Zhihua Zhang,dongbin@math.pku.edu.cn;houjikai@pku.edu.cn;yplu@stanford.edu;zhzhang@math.pku.edu.cn,3;8;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,10,0.0,yes,9/25/19,Peking University;Peking University;Stanford University;Peking University,22;22;4;22,24;24;4;24,1;8
3991,3991,3991,3991,3991,3991,3991,3991,ICLR,2020,How Does Learning Rate Decay Help Modern Neural Networks?,Kaichao You;Mingsheng Long;Jianmin Wang;Michael I. Jordan,youkaichao@gmail.com;mingsheng@tsinghua.edu.cn;jimwang@tsinghua.edu.cn;jordan@cs.berkeley.edu,3;1;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Tsinghua University;Tsinghua University;Tsinghua University;University of California Berkeley,8;8;8;5,23;23;23;13,8
3992,3992,3992,3992,3992,3992,3992,3992,ICLR,2020,Incorporating Horizontal Connections in Convolution by Spatial Shuffling,Ikki Kishida;Hideki Nakayama,kishida@nlab.ci.i.u-tokyo.ac.jp;nakayama@nlab.ci.i.u-tokyo.ac.jp,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,The University of Tokyo;The University of Tokyo,56;56,36;36,
3993,3993,3993,3993,3993,3993,3993,3993,ICLR,2020,Learning Cluster Structured Sparsity by Reweighting,Yulun Jiang;Lei Yu;Haijian Zhang;Zhou Liu,yljblues@whu.edu.cn;ly.wd@whu.edu.cn;haijian.zhang@whu.edu.cn;liuzhou@whu.edu.cn,1;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:N/A:N/A,Reject,0,3,0.0,yes,9/25/19,Wuhan University;Wuhan University;Wuhan University;Wuhan University,266;266;266;266,354;354;354;354,
3994,3994,3994,3994,3994,3994,3994,3994,ICLR,2020,Continuous Meta-Learning without Tasks,James Harrison;Apoorva Sharma;Chelsea Finn;Marco Pavone,jharrison@stanford.edu;apoorva@stanford.edu;cbfinn@cs.stanford.edu;pavone@stanford.edu,3;6;8,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Stanford University;Stanford University;Stanford University;Stanford University,4;4;4;4,4;4;4;4,11;6;2
3995,3995,3995,3995,3995,3995,3995,3995,ICLR,2020,Generative Adversarial Networks For Data Scarcity Industrial Positron Images With Attention,Mingwei Zhu;Min Zhao;Min Yao;Ruipeng Guo,zhumingwei@nuaa.edu.cn;xymzhao@126.com;ym_nuaa@163.com;rpguo@nuaa.edu.cn,1;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,0,0.0,yes,9/25/19,Tsinghua University;126;163;Tsinghua University,8;-1;-1;8,23;-1;-1;23,4;6
3996,3996,3996,3996,3996,3996,3996,3996,ICLR,2020,Support-guided Adversarial Imitation Learning,Ruohan Wang;Carlo Ciliberto;Pierluigi Amadori;Yiannis Demiris,r.wang16@ic.ac.uk;c.ciliberto@imperial.ac.uk;p.amadori@imperial.ac.uk;y.demiris@imperial.ac.uk,6;6;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Imperial College London;Imperial College London;Imperial College London;Imperial College London,73;73;73;73,10;10;10;10,4
3997,3997,3997,3997,3997,3997,3997,3997,ICLR,2020,IS THE LABEL TRUSTFUL: TRAINING BETTER DEEP LEARNING MODEL VIA UNCERTAINTY MINING NET,Yang Sun;Abhishek Kolagunda;Steven Eliuk;Xiaolong Wang,yang.sun1@ibm.com;abhishek.kolagunda@ibm.com;steven.eliuk@ibm.com;visionxiaolong@gmail.com,3;1;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,International Business Machines;International Business Machines;International Business Machines;International Business Machines,-1;-1;-1;-1,-1;-1;-1;-1,5
3998,3998,3998,3998,3998,3998,3998,3998,ICLR,2020,Learning with Social Influence through  Interior Policy Differentiation,Hao Sun;Bo Dai;Jiankai Sun;Zhenghao Peng;Guodong Xu;Dahua Lin;Bolei Zhou,sh018@ie.cuhk.edu.hk;doubledaibo@gmail.com;sunjiankai@sensetime.com;pengzh@ie.cuhk.edu.hk;xg018@ie.cuhk.edu.hk;dhlin@ie.cuhk.edu.hk;bzhou@ie.cuhk.edu.hk,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,1,4,0.0,yes,9/25/19,The Chinese University of Hong Kong;The Chinese University of Hong Kong;SenseTime Group Limited;The Chinese University of Hong Kong;The Chinese University of Hong Kong;The Chinese University of Hong Kong;The Chinese University of Hong Kong,59;59;-1;59;59;59;59,35;35;-1;35;35;35;35,
3999,3999,3999,3999,3999,3999,3999,3999,ICLR,2020,Selfish Emergent Communication,Michael Noukhovitch;Travis LaCroix;Aaron Courville,michael.noukhovitch@umontreal.ca;tlacroix@uci.edu;aaron.courville@gmail.com,3;1;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,23,0.0,yes,9/25/19,"University of Montreal;University of California, Irvine;University of Montreal",128;35;128,85;96;85,
4000,4000,4000,4000,4000,4000,4000,4000,ICLR,2020,Coordinated Exploration via Intrinsic Rewards for Multi-Agent Reinforcement Learning,Shariq Iqbal;Fei Sha,shariqiqbal2810@gmail.com;fsha@google.com,6;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,3,0.0,yes,9/25/19,University of Southern California;Google,31;-1,62;-1,
4001,4001,4001,4001,4001,4001,4001,4001,ICLR,2020,Mutual Exclusivity as a Challenge for Deep Neural Networks,Kanishk Gandhi;Brenden Lake,kanishk.gandhi@nyu.edu;brenden@nyu.edu,6;8;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0.0,yes,9/25/19,New York University;New York University,25;25,29;29,
4002,4002,4002,4002,4002,4002,4002,4002,ICLR,2020,The Differentiable Cross-Entropy Method,Brandon Amos;Denis Yarats,brandon.amos.cs@gmail.com;denisyarats@cs.nyu.edu,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,1,6,0.0,yes,9/25/19,Facebook;New York University,-1;25,-1;29,9
4003,4003,4003,4003,4003,4003,4003,4003,ICLR,2020,DS-VIC: Unsupervised Discovery of Decision States for Transfer in RL,Nirbhay Modhe;Prithvijit Chattopadhyay;Mohit Sharma;Abhishek Das;Devi Parikh;Dhruv Batra;Ramakrishna Vedantam,nirbhaym@gatech.edu;prithvijit3@gatech.edu;sharma.mohit.916@gmail.com;abhshkdz@gatech.edu;parikh@gatech.edu;dbatra@gatech.edu;ramav@fb.com,3;3;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,Georgia Institute of Technology;Georgia Institute of Technology;;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Facebook,13;13;-1;13;13;13;-1,38;38;-1;38;38;38;-1,1
4004,4004,4004,4004,4004,4004,4004,4004,ICLR,2020,Exploration Based Language Learning for Text-Based Games,Andrea Madotto;Mahdi Namazifar;Joost Huizinga;Piero Molino;Adrien Ecoffet;Huaixiu Zheng;Alexandros Papangelis;Dian Yu;Chandra Khatri;Gokhan Tur,amadotto@connect.ust.hk;mahdin@uber.com;jhuizinga@uber.com;piero@uber.com;adrienle@uber.com;huaixiu.zheng@uber.com;apapangelis@uber.com;dianyu@ucdavis.edu;chandrak@uber.com;gokhan@uber.com,6;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,"The Hong Kong University of Science and Technology;Uber;Uber;Uber;Uber;Uber;Uber;University of California, Davis;Uber;Uber",39;-1;-1;-1;-1;-1;-1;79;-1;-1,47;-1;-1;-1;-1;-1;-1;55;-1;-1,3
4005,4005,4005,4005,4005,4005,4005,4005,ICLR,2020,Representation Learning with Multisets,Vasco Portilheiro,vascop@stanford.edu,3;3;6,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Stanford University,4,4,
4006,4006,4006,4006,4006,4006,4006,4006,ICLR,2020,Deep Gradient Boosting -- Layer-wise Input Normalization of Neural Networks,Erhan Bilal,ebilal@us.ibm.com,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,1,3,0.0,yes,9/25/19,International Business Machines,-1,-1,8
4007,4007,4007,4007,4007,4007,4007,4007,ICLR,2020,Multi-Dimensional Explanation of Reviews,Diego Antognini;Claudiu Musat;Boi Faltings,diego.antognini@epfl.ch;claudiu.musat@swisscom.com;boi.faltings@epfl.ch,3;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Swiss Federal Institute of Technology Lausanne;Swisscom;Swiss Federal Institute of Technology Lausanne,481;-1;481,38;-1;38,3
4008,4008,4008,4008,4008,4008,4008,4008,ICLR,2020,ISBNet: Instance-aware Selective Branching Networks,Shaofeng Cai;Yao Shu;Wei Wang;Gang Chen;Beng Chin Ooi,shaofeng@comp.nus.edu.sg;shuyao@comp.nus.edu.sg;wangwei@comp.nus.edu.sg;cg@zju.edu.cn;ooibc@comp.nus.edu.sg,3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,2,0.0,yes,9/25/19,National University of Singapore;National University of Singapore;National University of Singapore;Zhejiang University;National University of Singapore,16;16;16;56;16,25;25;25;107;25,
4009,4009,4009,4009,4009,4009,4009,4009,ICLR,2020,Inferring Dynamical Systems with Long-Range Dependencies through Line Attractor Regularization,Dominik Schmidt;Georgia Koppe;Max Beutelspacher;Daniel Durstewitz,dominik.schmidt@zi-mannheim.de;georgia.koppe@zi-mannheim.de;max.beutelspacher@mailbox.org;daniel.durstewitz@zi-mannheim.de,3;6;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,ZI Mannheim;ZI Mannheim;;ZI Mannheim,-1;-1;-1;-1,-1;-1;-1;-1,
4010,4010,4010,4010,4010,4010,4010,4010,ICLR,2020,SoftAdam: Unifying SGD and Adam for better stochastic gradient descent,Abraham J. Fetterman;Christina H. Kim;Joshua Albrecht,abe@sourceress.co;christina@sourceress.co;josh@sourceress.co,3;1;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,1,3,0.0,yes,9/25/19,;;,-1;-1;-1,-1;-1;-1,3
4011,4011,4011,4011,4011,4011,4011,4011,ICLR,2020,BERT-AL: BERT for Arbitrarily Long Document Understanding,Ruixuan Zhang;Zhuoyu Wei;Yu Shi;Yining Chen,903276268@pku.edu.cn;zhuoyu.wei@microsoft.com;yushi@microsoft.com;yining.chen@microsoft.com,3;3;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Peking University;Microsoft;Microsoft;Microsoft,22;-1;-1;-1,24;-1;-1;-1,3
4012,4012,4012,4012,4012,4012,4012,4012,ICLR,2020,NEURAL EXECUTION ENGINES,Yujun Yan;Kevin Swersky;Danai Koutra;Parthasarathy Ranganathan;Milad Hashemi,yujunyan@umich.edu;kswersky@google.com;dkoutra@umich.edu;parthas@google.com;miladh@google.com,3;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,University of Michigan;Google;University of Michigan;Google;Google,8;-1;8;-1;-1,21;-1;21;-1;-1,10;8
4013,4013,4013,4013,4013,4013,4013,4013,ICLR,2020,Learning Good Policies By Learning Good Perceptual Models,Yilun Du;Phillip Isola,yilundu@mit.edu;phillipi@mit.edu,1;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2,5;5,
4014,4014,4014,4014,4014,4014,4014,4014,ICLR,2020,Quaternion Equivariant Capsule Networks for 3D Point Clouds,Yongheng Zhao;Tolga Birdal;Jan Eric Lenssen;Emanuele Menegatti;Leonidas Guibas;Federico Tombari,zhao@dei.unipd.it;tbirdal@stanford.edu;janeric.lenssen@udo.edu;emg@dei.unipd.it;guibas@cs.stanford.edu;tombari@google.com,3;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Universita' degli studi di Padova;Stanford University;TU Dortmund University;Universita' degli studi di Padova;Stanford University;Google,-1;4;233;-1;4;-1,-1;4;354;-1;4;-1,2
4015,4015,4015,4015,4015,4015,4015,4015,ICLR,2020,Informed Temporal Modeling via Logical Specification of Factorial LSTMs,Hongyuan Mei;Guanghui Qin;Minjie Xu;Jason Eisner,hongyuanmei@gmail.com;gqin@jhu.edu;chokkyvista06@gmail.com;jason@cs.jhu.edu,3;3;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,1,0.0,yes,9/25/19,Johns Hopkins University;Johns Hopkins University;Bloomberg LP;Johns Hopkins University,73;73;-1;73,12;12;-1;12,10;8
4016,4016,4016,4016,4016,4016,4016,4016,ICLR,2020,Needles in Haystacks: On Classifying Tiny Objects in Large Images,Nick Pawlowski;Suvrat Bhooshan;Nicolas Ballas;Francesco Ciompi;Ben Glocker;Michal Drozdzal,pawlowski.nick@gmail.com;sbh@fb.com;ballasn@fb.com;f.ciompi@gmail.com;b.glocker@imperial.ac.uk;mdrozdzal@fb.com,3;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,3,0.0,yes,9/25/19,Imperial College London;Facebook;Facebook;Radboud University Medical Center;Imperial College London;Facebook,73;-1;-1;390;73;-1,10;-1;-1;128;10;-1,2;8
4017,4017,4017,4017,4017,4017,4017,4017,ICLR,2020,Probabilistic View of Multi-agent Reinforcement Learning: A Unified Approach,Shubham Gupta;Ambedkar Dukkipati,shubhamg@iisc.ac.in;ambedkar@iisc.ac.in,3;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,1,3,0.0,yes,9/25/19,Indian Institute of Science;Indian Institute of Science,95;95,301;301,10
4018,4018,4018,4018,4018,4018,4018,4018,ICLR,2020,Hope For The Best But Prepare For The Worst: Cautious Adaptation In RL Agents,Jesse Zhang;Brian Cheung;Chelsea Finn;Dinesh Jayaraman;Sergey Levine,jessezhang@berkeley.edu;bcheung@berkeley.edu;cbfinn@cs.stanford.edu;dineshjayaraman@berkeley.edu;svlevine@eecs.berkeley.edu,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,3,0.0,yes,9/25/19,University of California Berkeley;University of California Berkeley;Stanford University;University of California Berkeley;University of California Berkeley,5;5;4;5;5,13;13;4;13;13,
4019,4019,4019,4019,4019,4019,4019,4019,ICLR,2020,Abstractive Dialog Summarization with Semantic Scaffolds,Lin Yuan;Zhou Yu,yuanlinzju@gmail.com;joyu@ucdavis.edu,1;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,0,0.0,yes,9/25/19,"Zhejiang University;University of California, Davis",56;79,107;55,
4020,4020,4020,4020,4020,4020,4020,4020,ICLR,2020,Relative Pixel Prediction For Autoregressive Image Generation,Wang Ling;Chris Dyer;Lei Yu;Lingpeng Kong;Dani Yogatama;Susannah Young,lingwang@google.com;cdyer@google.com;leiyu@google.com;lingpenk@google.com;dyogatama@google.com;susannahy@google.com,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,
4021,4021,4021,4021,4021,4021,4021,4021,ICLR,2020,Integrative Tensor-based Anomaly Detection System For Satellites,Youjin Shin;Sangyup Lee;Shahroz Tariq;Myeong Shin Lee;OkchulJung;Daewon Chung;Simon Woo,youjin.shin.1@stonybrook.edu;shahroz@g.skku.edu;sangyup.lee@g.skku.edu;mslee@kari.re.kr;ocjung@kari.re.kr;dwchung@kari.re.kr;swoo@g.skku.edu,1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,"State University of New York, Stony Brook;Peking University;Peking University;;;;Peking University",41;22;22;-1;-1;-1;22,304;24;24;-1;-1;-1;24,
4022,4022,4022,4022,4022,4022,4022,4022,ICLR,2020,Gram-Gauss-Newton Method: Learning Overparameterized Neural Networks for Regression Problems,Tianle Cai*;Ruiqi Gao*;Jikai Hou*;Siyu Chen;Dong Wang;Di He;Zhihua Zhang;Liwei Wang,caitianle1998@pku.edu.cn;grq@pku.edu.cn;1600010681@pku.edu.cn;siyuchen@pku.edu.cn;wangdongcis@pku.edu.cn;di_he@pku.edu.cn;zhzhang@math.pku.edu.cn;wanglw@cis.pku.edu.cn,3;3;3;6;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,Peking University;Peking University;Peking University;Peking University;Peking University;Peking University;Peking University;Peking University,22;22;22;22;22;22;22;22,24;24;24;24;24;24;24;24,9
4023,4023,4023,4023,4023,4023,4023,4023,ICLR,2020,A Greedy Approach to Max-Sliced Wasserstein GANs,András Horváth,horvath.andras@itk.ppke.hu,1;1;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Pazmany Peter catholic University,481,1397,5;4
4024,4024,4024,4024,4024,4024,4024,4024,ICLR,2020,Improving Sequential Latent Variable Models with Autoregressive Flows,Joseph Marino;Lei Chen;Jiawei He;Stephan Mandt,jmarino@caltech.edu;lei_chen_4@sfu.ca;jiawei_he_2@sfu.ca;stephan.mandt@gmail.com,6;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,"California Institute of Technology;Simon Fraser University;Simon Fraser University;University of California, Irvine",143;64;64;35,2;272;272;96,
4025,4025,4025,4025,4025,4025,4025,4025,ICLR,2020,Symmetry and Systematicity,Jeff Mitchell;Jeff Bowers,jeff.mitchell@bristol.ac.uk;j.bowers@bristol.ac.uk,1;1;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,9,0.0,yes,9/25/19,University of Bristol;University of Bristol,128;128,87;87,
4026,4026,4026,4026,4026,4026,4026,4026,ICLR,2020,Towards Interpretable Evaluations: A Case Study of Named Entity Recognition,Jinlan Fu;Pengfei Liu;Xuanjing Huang,fujl16@fudan.edu.cn;pfliu14@fudan.edu.cn;xjhuang@fudan.edu.cn,3;3;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,Fudan University;Fudan University;Fudan University,79;79;79,109;109;109,3
4027,4027,4027,4027,4027,4027,4027,4027,ICLR,2020,Bootstrapping the Expressivity with Model-based Planning,Kefan Dong;Yuping Luo;Tengyu Ma,dkf16@mails.tsinghua.edu.cn;yupingl@cs.princeton.edu;tengyuma@cs.stanford.edu,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Tsinghua University;Princeton University;Stanford University,8;31;4,23;6;4,
4028,4028,4028,4028,4028,4028,4028,4028,ICLR,2020,Score and Lyrics-Free Singing Voice Generation,Jen-Yu Liu;Yu-Hua Chen;Yin-Cheng Yeh;Yi-Hsuan Yang,ciauaishere@gmail.com;r08946011@ntu.edu.tw;deanyeh.ee01@g2.nctu.edu.tw;affige@gmail.com,3;3;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,7,0.0,yes,9/25/19,;National Taiwan University;National Chiao Tung University;Academia Sinica,-1;86;143;-1,-1;120;564;-1,5;4
4029,4029,4029,4029,4029,4029,4029,4029,ICLR,2020,RL-LIM: Reinforcement Learning-based Locally Interpretable Modeling,Jinsung Yoon;Sercan O. Arik;Tomas Pfister,jsyoon0823@gmail.com;soarik@google.com;tpfister@google.com,6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,"University of California, Los Angeles;Google;Google",20;-1;-1,17;-1;-1,
4030,4030,4030,4030,4030,4030,4030,4030,ICLR,2020,Characterizing Missing Information in Deep Networks Using Backpropagated Gradients,Gukyeong Kwon;Mohit Prabhushankar;Dogancan Temel;Ghassan AlRegib,gukyeong.kwon@gatech.edu;mohit.p@gatech.edu;cantemel@gatech.edu;alregib@gatech.edu,3;1;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology,13;13;13;13,38;38;38;38,
4031,4031,4031,4031,4031,4031,4031,4031,ICLR,2020,Improving Multi-Manifold GANs with a Learned Noise Prior,Matthew Amodio;Smita Krishnaswamy,matthew.amodio@yale.edu;smita.krishnaswamy@yale.edu,3;8;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,3,0.0,yes,9/25/19,Yale University;Yale University,64;64,8;8,5;4
4032,4032,4032,4032,4032,4032,4032,4032,ICLR,2020,Unsupervised Domain Adaptation through Self-Supervision,Yu Sun;Eric Tzeng;Trevor Darrell;Alexei A. Efros,yusun@berkeley.edu;etzeng@eecs.berkeley.edu;trevor@eecs.berkeley.edu;efros@eecs.berkeley.edu,6;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,2,5,0.0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5,13;13;13;13,2
4033,4033,4033,4033,4033,4033,4033,4033,ICLR,2020,Adversarial Paritial Multi-label Learning,Yan Yan;Yuhong Guo,yanyan.nwpu@gmail.com;yuhongguo.cs@gmail.com,8;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Tsinghua University;Temple University,8;205,23;311,5;4
4034,4034,4034,4034,4034,4034,4034,4034,ICLR,2020,Iterative Target Augmentation for Effective Conditional Generation,Kevin Yang;Wengong Jin;Kyle Swanson;Regina Barzilay;Tommi Jaakkola,yangk@berkeley.edu;wengong@csail.mit.edu;swansonk.14@gmail.com;regina@csail.mit.edu;tommi@csail.mit.edu,3;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,University of California Berkeley;Massachusetts Institute of Technology;;Massachusetts Institute of Technology;Massachusetts Institute of Technology,5;2;-1;2;2,13;5;-1;5;5,5
4035,4035,4035,4035,4035,4035,4035,4035,ICLR,2020,Hyperbolic Discounting and Learning Over Multiple Horizons,William Fedus;Carles Gelada;Yoshua Bengio;Marc G. Bellemare;Hugo Larochelle,liam.fedus@gmail.com;carlesgelada@hotmail.com;yoshua.bengio@mila.quebec;bellemare@google.com;hugolarochelle@google.com,6;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,University of Montreal;;University of Montreal;Google;Google,128;-1;128;-1;-1,85;-1;85;-1;-1,
4036,4036,4036,4036,4036,4036,4036,4036,ICLR,2020,Can I Trust the Explainer? Verifying Post-Hoc Explanatory Methods,Oana-Maria Camburu*;Eleonora Giunchiglia*;Jakob Foerster;Thomas Lukasiewicz;Phil Blunsom,ocamburu@gmail.com;eleonora.giunchiglia@cs.ox.ac.uk;jakobfoerster@gmail.com;thomas.lukasiewicz@gmail.com;philblunsom@gmail.com,3;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,University of Oxford;University of Oxford;Facebook;University of Oxford;,50;50;-1;50;-1,1;1;-1;1;-1,
4037,4037,4037,4037,4037,4037,4037,4037,ICLR,2020,A Fine-Grained Spectral Perspective on Neural Networks,Greg Yang;Hadi Salman,gregyang@microsoft.com;hadicsalman@gmail.com,6;3;6,I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,Microsoft;Microsoft,-1;-1,-1;-1,8
4038,4038,4038,4038,4038,4038,4038,4038,ICLR,2020,Low Rank Training of Deep Neural Networks for Emerging Memory Technology,Albert Gural;Phillip Nadeau;Mehul Tikekar;Boris Murmann,agural@stanford.edu;phillip.nadeau@analog.com;mehul.tikekar@analog.com;murmann@stanford.edu,3;3;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,5,0.0,yes,9/25/19,Stanford University;;;Stanford University,4;-1;-1;4,4;-1;-1;4,
4039,4039,4039,4039,4039,4039,4039,4039,ICLR,2020,Cyclic Graph Dynamic Multilayer Perceptron for Periodic Signals,Mikio Furokawa;Erik Gest;Takayuki Hirano;Kamal Youcef-Toumi,mikiof@mit.edu;erikgest@mit.edu;takayuki_hirano@jsw.co.jp;youcef@mit.edu,3;6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;;Massachusetts Institute of Technology,2;2;-1;2,5;5;-1;5,10
4040,4040,4040,4040,4040,4040,4040,4040,ICLR,2020,Atomic Compression Networks,Jonas Falkner;Josif Grabocka;Lars Schmidt-Thieme,falkner@ismll.uni-hildesheim.de;josif@ismll.uni-hildesheim.de;schmidt-thieme@ismll.uni-hildesheim.de,6;1;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,University of Hildesheim;University of Hildesheim;University of Hildesheim,390;390;390,1397;1397;1397,
4041,4041,4041,4041,4041,4041,4041,4041,ICLR,2020,Simplicial Complex Networks,Mohammad Firouzi;Sadra Boreiri;Hamed Firouzi,mfirouzi@alphabist.com;sadra.boreiri@epfl.ch;hfirouzi@alphabist.com,1;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,"Department of Computer Science, University of Toronto;Swiss Federal Institute of Technology Lausanne;Alphabist",18;481;-1,18;38;-1,1
4042,4042,4042,4042,4042,4042,4042,4042,ICLR,2020,A Non-asymptotic comparison of SVRG and SGD: tradeoffs between compute and speed,Qingru Zhang;Yuhuai Wu;Fartash Faghri;Tianzong Zhang;Jimmy Ba,qrzhang98@gmail.com;ywu@cs.toronto.edu;faghri@cs.toronto.edu;ztz16@mails.tsinghua.edu.cn;jba@cs.toronto.edu,6;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,"Shanghai Jiao Tong University;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Tsinghua University;Department of Computer Science, University of Toronto",53;18;18;8;18,157;18;18;23;18,
4043,4043,4043,4043,4043,4043,4043,4043,ICLR,2020,PAC-Bayesian Neural Network Bounds,Yossi Adi;Alex Schwing;Tamir Hazan,yossiadidrum@gmail.com;aschwing@illinois.edu;tamir.hazan@technion.ac.il,3;6;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,"Facebook;University of Illinois, Urbana Champaign;Technion",-1;3;26,-1;48;412,11;1;8
4044,4044,4044,4044,4044,4044,4044,4044,ICLR,2020,Promoting Coordination through Policy Regularization in Multi-Agent Deep Reinforcement Learning,Paul Barde;Julien Roy;Félix G. Harvey;Derek Nowrouzezahrai;Christopher Pal,paul.b.barde@gmail.com;jul.roy1311@gmail.com;c212.felixh@gmail.com;derek@cim.mcgill.ca;christopher.pal@polymtl.ca,6;8;3,I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,10,0.0,yes,9/25/19,University of Montreal;University of Montreal;Polytechnique Montreal;McGill University;Polytechnique Montreal,128;128;390;86;390,85;85;1397;42;1397,
4045,4045,4045,4045,4045,4045,4045,4045,ICLR,2020,Stabilizing Transformers for Reinforcement Learning,Emilio Parisotto;Francis Song;Jack Rae;Razvan Pascanu;Caglar Gulcehre;Siddhant Jayakumar;Max Jaderberg;Raphaël Lopez Kaufman;Aidan Clark;Seb Noury;Matt Botvinick;Nicolas Heess;Raia Hadsell,eparisot@cs.cmu.edu;songf@google.com;jwrae@google.com;razp@google.com;caglarg@google.com;sidmj@google.com;jaderberg@google.com;rlopezkaufman@google.com;aidanclark@google.com;snoury@google.com;botvinick@google.com;heess@google.com;raia@google.com,1;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,7,0.0,yes,9/25/19,Carnegie Mellon University;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,27;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,3
4046,4046,4046,4046,4046,4046,4046,4046,ICLR,2020,Deep k-NN for Noisy Labels,Dara Bahri;Heinrich Jiang;Maya Gupta,dbahri@google.com;heinrichj@google.com;mayagupta@google.com,1;1;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,2,0.0,yes,9/25/19,Google;Google;Google,-1;-1;-1,-1;-1;-1,
4047,4047,4047,4047,4047,4047,4047,4047,ICLR,2020,Annealed Denoising score matching: learning Energy based model in high-dimensional spaces,Zengyi Li;Yubei Chen;Friedrich T. Sommer,zengyi_li@berkeley.edu;yubeic@eecs.berkeley.edu;fsommer@berkeley.edu,3;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,13;13;13,5;11
4048,4048,4048,4048,4048,4048,4048,4048,ICLR,2020,Sparse Networks from Scratch: Faster Training without Losing Performance,Tim Dettmers;Luke Zettlemoyer,dettmers@cs.washington.edu;lsz@cs.washington.edu,6;3;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,University of Washington;University of Washington,6;6,26;26,
4049,4049,4049,4049,4049,4049,4049,4049,ICLR,2020,TPO: TREE SEARCH POLICY OPTIMIZATION FOR CONTINUOUS ACTION SPACES,Amir Yazdanbakhsh;Ebrahim Songhori;Robert Ormandi;Anna Goldie;Azalia Mirhoseini,ayazdan@google.com;esonghori@google.com;ormandi@google.com;agoldie@google.com;azalia@google.com,1;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,
4050,4050,4050,4050,4050,4050,4050,4050,ICLR,2020,Sample-Based Point Cloud Decoder Networks,Erich Merrill;Alan Fern,merriler@oregonstate.edu;alan.fern@oregonstate.edu,3;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Oregon State University;Oregon State University,77;77,373;373,
4051,4051,4051,4051,4051,4051,4051,4051,ICLR,2020,Bayesian Residual Policy Optimization: Scalable Bayesian Reinforcement Learning with Clairvoyant Experts,Gilwoo Lee;Brian Hou;Sanjiban Choudhury;Siddhartha S. Srinivasa,gilwoo@cs.uw.edu;bhou@cs.uw.edu;sanjibac@cs.uw.edu;siddh@cs.uw.edu,3;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,"University of Washington, Seattle;University of Washington, Seattle;University of Washington, Seattle;University of Washington, Seattle",6;6;6;6,26;26;26;26,11
4052,4052,4052,4052,4052,4052,4052,4052,ICLR,2020,Efficient Inference and Exploration for Reinforcement Learning,Yi Zhu;Jing Dong;Henry Lam,yizhu2020@u.northwestern.edu;jing.dong@gsb.columbia.edu;khl2114@columbia.edu,3;1;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Northwestern University;Columbia University;Columbia University,44;15;15,22;16;16,
4053,4053,4053,4053,4053,4053,4053,4053,ICLR,2020,Poisoning Attacks with Generative Adversarial Nets,Luis Muñoz-González;Bjarne Pfitzner;Matteo Russo;Javier Carnerero-Cano;Emil C. Lupu,l.munoz@imperial.ac.uk;bjarne.pfitzner@hpi.de;matteor@princeton.edu;j.carnerero-cano18@imperial.ac.uk;e.c.lupu@imperial.ac.uk,6;6;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,3,0.0,yes,9/25/19,Imperial College London;Hasso Plattner Institute;Princeton University;Imperial College London;Imperial College London,73;266;31;73;73,10;1397;6;10;10,5;4
4054,4054,4054,4054,4054,4054,4054,4054,ICLR,2020,Keyframing the Future: Discovering Temporal Hierarchy with Keyframe-Inpainter Prediction,Karl Pertsch;Oleh Rybkin;Jingyun Yang;Konstantinos G. Derpanis;Kostas Daniilidis;Joseph J. Lim;Andrew Jaegle,pertsch@usc.edu;oleh@seas.upenn.edu;jingyuny@usc.edu;kosta@ryerson.ca;kostas@seas.upenn.edu;limjj@usc.edu;ajaegle@upenn.edu,3;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,University of Southern California;University of Pennsylvania;University of Southern California;Ryerson University;University of Pennsylvania;University of Southern California;University of Pennsylvania,31;19;31;323;19;31;19,62;11;62;739;11;62;11,
4055,4055,4055,4055,4055,4055,4055,4055,ICLR,2020,An Empirical Study on Post-processing Methods for Word Embeddings,Shuai Tang;Mahta Mousavi;Virginia R. de Sa,shuaitang93@ucsd.edu;mahta@ucsd.edu;desa@ucsd.edu,1;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,1,0.0,yes,9/25/19,"University of California, San Diego;University of California, San Diego;University of California, San Diego",11;11;11,31;31;31,3
4056,4056,4056,4056,4056,4056,4056,4056,ICLR,2020,Winning the Lottery with Continuous Sparsification,Pedro Savarese;Hugo Silva;Michael Maire,savarese@ttic.edu;hugoandradesilva664@gmail.com;mmaire@uchicago.edu,3;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,Toyota Technological Institute at Chicago;;University of Chicago,-1;-1;48,-1;-1;9,6
4057,4057,4057,4057,4057,4057,4057,4057,ICLR,2020,Rethinking Curriculum Learning With Incremental Labels And Adaptive Compensation,Madan Ravi Ganesh;Jason J. Corso,madantrg@umich.edu;jjcorso@umich.edu,3;1;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,4,0.0,yes,9/25/19,University of Michigan;University of Michigan,8;8,21;21,
4058,4058,4058,4058,4058,4058,4058,4058,ICLR,2020,Zero-Shot Policy Transfer with Disentangled Attention,Josh Roy;George Konidaris,josh_roy@brown.edu;gdk@cs.brown.edu,1;1;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,3,0.0,yes,9/25/19,Brown University;Brown University,67;67,53;53,5
4059,4059,4059,4059,4059,4059,4059,4059,ICLR,2020,P-BN: Towards Effective Batch Normalization in the Path Space,Xufang Luo;Qi Meng;Wei Chen;Tie-Yan Liu,luoxufang@buaa.edu.cn;meq@microsoft.com;wche@microsoft.com;tyliu@microsoft.com,3;3;3,I have published one or two papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Beihang University;Microsoft;Microsoft;Microsoft,118;-1;-1;-1,594;-1;-1;-1,
4060,4060,4060,4060,4060,4060,4060,4060,ICLR,2020,Convolutional Tensor-Train LSTM for Long-Term Video Prediction,Jiahao Su;Wonmin Byeon;Furong Huang;Jan Kautz;Animashree Anandkumar,jiahaosu@terpmail.umd.edu;wonmin.byeon@gmail.com;furongh@cs.umd.edu;jkautz@nvidia.com;animakumar@gmail.com,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,"University of Maryland, College Park;NVIDIA;University of Maryland, College Park;NVIDIA;University of California-Irvine",12;-1;12;-1;35,91;-1;91;-1;96,
4061,4061,4061,4061,4061,4061,4061,4061,ICLR,2020,Task-Based Top-Down Modulation Network for Multi-Task-Learning Applications,Hila Levi;Shimon Ullman,hila.levi@weizmann.ac.il;shimon.ullman@weizmann.ac.il,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,6,0.0,yes,9/25/19,Weizmann Institute;Weizmann Institute,108;108,1397;1397,
4062,4062,4062,4062,4062,4062,4062,4062,ICLR,2020,Variational Constrained Reinforcement Learning with Application to Planning at Roundabout,Yuan Tian;Minghao Han;Lixian Zhang;Wulong Liu;Jun Wang;Wei Pan,yuantian013@163.com;mhhan@hit.edu.cn;lixianzhang@hit.edu.cn;liuwulong@huawei.com;jun.wang@cs.ucl.ac.uk;wei.pan@tudelft.nl,1;1;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Swiss Federal Institute of Technology;Harbin Institute of Technology;Harbin Institute of Technology;Huawei Technologies Ltd.;University College London;Delft University of Technology,10;172;172;-1;50;89,13;424;424;-1;15;67,
4063,4063,4063,4063,4063,4063,4063,4063,ICLR,2020,Learning a Spatio-Temporal Embedding for Video Instance Segmentation,Anthony Hu;Alex Kendall;Roberto Cipolla,ah2029@cam.ac.uk;alex@wayve.ai;rc10001@cam.ac.uk,3;6;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,4,0.0,yes,9/25/19,University of Cambridge;Wayve Technologies;University of Cambridge,71;-1;71,3;-1;3,2
4064,4064,4064,4064,4064,4064,4064,4064,ICLR,2020,Deep Ensembles: A Loss Landscape Perspective,Stanislav Fort;Clara Huiyi Hu;Balaji Lakshminarayanan,stanislav.fort@gmail.com;clarahu@google.com;balajiln@google.com,8;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Google;Google;Google,-1;-1;-1,-1;-1;-1,11
4065,4065,4065,4065,4065,4065,4065,4065,ICLR,2020,Adversarial Privacy Preservation under Attribute Inference Attack,Han Zhao;Jianfeng Chi;Yuan Tian;Geoffrey J. Gordon,han.zhao@cs.cmu.edu;jc6ub@virginia.edu;yuant@virginia.edu;geoff.gordon@microsoft.com,3;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,4,1.0,yes,9/25/19,Carnegie Mellon University;University of Virginia;University of Virginia;Microsoft,1;59;59;-1,27;107;107;-1,4;1
4066,4066,4066,4066,4066,4066,4066,4066,ICLR,2020,Deep Innovation Protection,Sebastian Risi;Kenneth O. Stanley,sebr@itu.dk;kstanley@uber.com,6;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,IT University;Uber,172;-1,416;-1,
4067,4067,4067,4067,4067,4067,4067,4067,ICLR,2020,Meta-Learning Runge-Kutta,Nadine Behrmann;Patrick Schramowski;Kristian Kersting,nadine.behrmann@freenet.de;schramowski@cs.tu-darmstadt.de;kersting@cs.tu-darmstadt.de,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,6,0.0,yes,9/25/19,TU Darmstadt;TU Darmstadt;TU Darmstadt,64;64;64,289;289;289,6
4068,4068,4068,4068,4068,4068,4068,4068,ICLR,2020,"Credible Sample Elicitation by Deep Learning, for Deep Learning",Yang Liu;Zuyue Fu;Zhuoran Yang;Zhaoran Wang,yangliu@ucsc.edu;zuyuefu2022@u.northwestern.edu;zy6@princeton.edu;zhaoranwang@gmail.com,6;1,I do not know much about this area.:N/A:N/A:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,University of Southern California;Northwestern University;Princeton University;Northwestern University,31;44;31;44,62;22;6;22,5;4
4069,4069,4069,4069,4069,4069,4069,4069,ICLR,2020,Continuous Graph Flow,Zhiwei Deng;Megha Nawhal;Lili Meng;Greg Mori,zhiweid@princeton.edu;mnawhal@sfu.ca;lilimeng1103@gmail.com;mori@cs.sfu.ca,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,1,3,0.0,yes,9/25/19,Princeton University;Simon Fraser University;University of British Columbia;Simon Fraser University,31;64;35;64,6;272;34;272,10;5;8
4070,4070,4070,4070,4070,4070,4070,4070,ICLR,2020,Extreme Value k-means Clustering,Sixiao Zheng;Yanxi Hou;Yanwei Fu;Jianfeng Feng,sxzheng18@fudan.edu.cn;yxhou@fudan.edu.cn;yanweifu@fudan.edu.cn;jffeng@fudan.edu.cn,3;3;1;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Fudan University;Fudan University;Fudan University;Fudan University,79;79;79;79,109;109;109;109,
4071,4071,4071,4071,4071,4071,4071,4071,ICLR,2020,Hallucinative Topological Memory for Zero-Shot Visual Planning,Kara Liu;Thanard Kurutach;Pieter Abbeel;Aviv Tamar,karamarieliu@berkeley.edu;thanard.kurutach@berkeley.edu;pabbeel@cs.berkeley.edu;aviv.tamar.mail@gmail.com,1;8;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley;Technion,5;5;5;26,13;13;13;412,10;5;6;8
4072,4072,4072,4072,4072,4072,4072,4072,ICLR,2020,On the Decision Boundaries of Deep Neural Networks: A Tropical Geometry Perspective,Motasem Alfarra;Adel Bibi;Hasan Hammoud;Mohamed Gaafar;Bernard Ghanem,motasem.alfarra@kaust.edu.sa;adel.bibi@kaust.edu.sa;hasan.hammoud@kaust.edu.sa;muhamed.gaafar@gmail.com;bernard.ghanem@kaust.edu.sa,1;8;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,9,0.0,yes,9/25/19,KAUST;KAUST;KAUST;Itemis AG;KAUST,128;128;128;-1;128,1397;1397;1397;-1;1397,4
4073,4073,4073,4073,4073,4073,4073,4073,ICLR,2020,Encoder-decoder Network as Loss Function for Summarization,Glen Jeh,glenjeh@gmail.com,1;6;1,I have published one or two papers in this area.:I carefully checked the experiments.:N/A:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I did not assess the experiments.:N/A:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,,,,
4074,4074,4074,4074,4074,4074,4074,4074,ICLR,2020,Unsupervised Learning of Automotive 3D Crash Simulations using LSTMs,Amin Abbasloo;Jochen Garcke;Rodrigo Iza-Teran,amin.abbasloo@scai.fraunhofer.de;garcke@ins.uni-bonn.de;rodrigo.iza-teran@scai.fraunhofer.de,3;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,1,0.0,yes,9/25/19,Fraunhofer SCAI;University of Bonn;Fraunhofer SCAI,-1;128;-1,-1;106;-1,
4075,4075,4075,4075,4075,4075,4075,4075,ICLR,2020,Dynamic Instance Hardness,Tianyi Zhou;Shengjie Wang;Jeff A. Bilmes,tianyizh@uw.edu;wangsj@cs.washington.edu;bilmes@uw.edu,3;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,"University of Washington, Seattle;University of Washington;University of Washington, Seattle",6;6;6,26;26;26,1
4076,4076,4076,4076,4076,4076,4076,4076,ICLR,2020,Beyond Classical Diffusion: Ballistic Graph Neural Network,Yimeng Min,minyimen@mila.quebec,3;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,University of Montreal,128,85,10
4077,4077,4077,4077,4077,4077,4077,4077,ICLR,2020,INSTANCE CROSS ENTROPY FOR DEEP METRIC LEARNING,Xinshao Wang;Elyor Kodirov;Yang Hua;Neil M. Robertson,xwang39@qub.ac.uk;elyor@anyvision.co;y.hua@qub.ac.uk;n.robertson@qub.ac.uk,3;1;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,1.0,yes,9/25/19,Queen's University Belfast;Anyvision;Queen's University Belfast;Queen's University Belfast,266;-1;266;266,204;-1;204;204,
4078,4078,4078,4078,4078,4078,4078,4078,ICLR,2020,Retrieving Signals in the Frequency Domain with Deep Complex Extractors,Chiheb Trabelsi;Olexa Bilaniuk;Ousmane Dia;Ying Zhang;Mirco Ravanelli;Jonathan Binas;Negar Rostamzadeh;Christopher  J Pal,chiheb.trabelsi@polymtl.ca;olexa.bilaniuk@umontreal.ca;ousmane@elementai.com;ying@elementai.com;mirco.ravanelli@gmail.com;jbinas@gmail.com;negar@elementai.com;christopher.pal@elementai.com,6;3;6,I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,Polytechnique Montreal;University of Montreal;Element AI;Element AI;University of Montreal;University of Montreal;Element AI;Element AI,390;128;-1;-1;128;128;-1;-1,1397;85;-1;-1;85;85;-1;-1,
4079,4079,4079,4079,4079,4079,4079,4079,ICLR,2020,A Copula approach for hyperparameter transfer learning,David Salinas;Huibin Shen;Valerio Perrone,david.salinas.pro@gmail.com;huibishe@amazon.com;vperrone@amazon.com,3;1;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,4,0.0,yes,9/25/19,Naver Labs Europe;Amazon;Amazon,-1;-1;-1,-1;-1;-1,11;6
4080,4080,4080,4080,4080,4080,4080,4080,ICLR,2020,Out-of-distribution Detection in Few-shot Classification,Kuan-Chieh Wang;Paul Vicol;Eleni Triantafillou;Chia-Cheng Liu;Richard Zemel,wangkua1@cs.toronto.edu;pvicol@cs.toronto.edu;eleni@cs.toronto.edu;cc.liu2018@gmail.com;zemel@cs.toronto.edu,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;;Department of Computer Science, University of Toronto",18;18;18;-1;18,18;18;18;-1;18,6
4081,4081,4081,4081,4081,4081,4081,4081,ICLR,2020,GQ-Net: Training Quantization-Friendly Deep Networks,Rundong Li;Rui Fan,lird@shanghaitech.edu.cn;fanrui@shanghaitech.edu.cn,3;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,8,0.0,yes,9/25/19,ShanghaiTech University;ShanghaiTech University,481;481,1397;1397,
4082,4082,4082,4082,4082,4082,4082,4082,ICLR,2020,The divergences minimized by non-saturating GAN training,Matt Shannon,matt.shannon.personal@gmail.com,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Google,-1,-1,5;4
4083,4083,4083,4083,4083,4083,4083,4083,ICLR,2020,"Translation Between Waves,  wave2wave",Tsuyoshi Okita;Hirotaka Hachiya;Sozo Inoue;Naonori Ueda,tsuyoshi.okita@gmail.com;hirotaka.hachiya@riken.jp;sozo.inoue@riken.jp;naonori.ueda@riken.jp,1;1;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Meiji University;RIKEN;RIKEN;RIKEN,481;-1;-1;-1,332;-1;-1;-1,3
4084,4084,4084,4084,4084,4084,4084,4084,ICLR,2020,MixUp as Directional Adversarial Training,Guillaume Perrault-Archambault;Yongyi Mao;Hongyu Guo;Richong Zhang,gperr050@uottawa.ca;yymao@eecs.uottawa.ca;hongyu.guo@nrc-cnrc.gc.ca;zhangrc@act.buaa.edu.cn,1;3;3,I have published in this field for several years.:I carefully checked the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0.0,yes,9/25/19,University of Ottawa;University of Ottawa;National Research Council Canada;Beihang University,266;266;-1;118,141;141;-1;594,4;8
4085,4085,4085,4085,4085,4085,4085,4085,ICLR,2020,Deep Multiple Instance Learning with Gaussian Weighting,Basura Fernando;Hakan Bilen,basura.fernando@anu.edu.au;hbilen@ed.ac.uk,8;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Australian National University;University of Edinburgh,108;33,50;30,
4086,4086,4086,4086,4086,4086,4086,4086,ICLR,2020,DyNet: Dynamic Convolution for Accelerating Convolution Neural Networks,Kane Zhang;Jian Zhang;Qiang Wang;Zhao Zhong,zhangyikang5@huawei.com;zhangjian157@huawei.com;wangqiang168@huawei.com;zorro.zhongzhao@huawei.com,6;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0.0,yes,9/25/19,Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.,-1;-1;-1;-1,-1;-1;-1;-1,2
4087,4087,4087,4087,4087,4087,4087,4087,ICLR,2020,Growing Action Spaces,Gregory Farquhar;Laura Gustafson;Zeming Lin;Shimon Whiteson;Nicolas Usunier;Gabriel Synnaeve,gregory.farquhar@cs.ox.ac.uk;lgustafson@fb.com;zlin@fb.com;shimon.whiteson@cs.ox.ac.uk;usunier@fb.com;gab@fb.com,3;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,University of Oxford;Facebook;Facebook;University of Oxford;Facebook;Facebook,50;-1;-1;50;-1;-1,1;-1;-1;1;-1;-1,1
4088,4088,4088,4088,4088,4088,4088,4088,ICLR,2020,Model Ensemble-Based Intrinsic Reward for Sparse Reward Reinforcement Learning,Giseung Park;Whiyoung Jung;Sungho Choi;Youngchul Sung,gs.park@kaist.ac.kr;wy.jung@kaist.ac.kr;sungho.choi@kaist.ac.kr;ycsung@kaist.ac.kr,3;3;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,481;481;481;481,110;110;110;110,
4089,4089,4089,4089,4089,4089,4089,4089,ICLR,2020,Differentiable Hebbian Consolidation for Continual Learning,Vithursan Thangarasa;Thomas Miconi;Graham W. Taylor,vthangar@uoguelph.ca;tmiconi@uber.com;gwtaylor@uoguelph.ca,3;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,7,0.0,yes,9/25/19,University of Guelph;Uber;University of Guelph,266;-1;266,558;-1;558,
4090,4090,4090,4090,4090,4090,4090,4090,ICLR,2020,Meta-Graph: Few shot Link Prediction via Meta Learning,Avishek Joey Bose;Ankit Jain;Piero Molino;William L. Hamilton,joey.bose@mail.mcgill.ca;ankit.jain@uber.com;piero.molino@uber.com;wlh@cs.mcgill.ca,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,McGill University;Uber;Uber;McGill University,86;-1;-1;86,42;-1;-1;42,6;10
4091,4091,4091,4091,4091,4091,4091,4091,ICLR,2020,EXPLOITING SEMANTIC COHERENCE TO IMPROVE PREDICTION IN SATELLITE SCENE IMAGE ANALYSIS: APPLICATION TO DISEASE DENSITY ESTIMATION,Rahman Sanya;Gilbert Maiga;Ernest Mwebaze,hbasanya@gmail.com;gilmaiga@gmail.com;emwebaze@gmail.com,1;3;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,1,0.0,yes,9/25/19,Makerere University;;,481;-1;-1,605;-1;-1,
4092,4092,4092,4092,4092,4092,4092,4092,ICLR,2020,VAENAS: Sampling Matters in Neural Architecture Search,Shizheng Qin;Yichen Zhu;Pengfei Hou;Xiangyu Zhang;Wenqiang Zhang;Jian Sun,szqin17@fudan.edu.cn;k.zhu@mail.utoronto.ca;houpengfei@megvii.com;zhangxiangyu@megvii.com;wqzhang@fudan.edu.cn;sunjian@megvii.com,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Fudan University;Toronto University;Megvii Technology Inc.;Megvii Technology Inc.;Fudan University;Megvii Technology Inc.,79;18;-1;-1;79;-1,109;18;-1;-1;109;-1,5;2
4093,4093,4093,4093,4093,4093,4093,4093,ICLR,2020,Aggregating explanation methods for neural networks stabilizes explanations,Laura Rieger;Lars Kai Hansen,lauri@dtu.dk,8;3;8,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,Technical University of Denmark,481,182,
4094,4094,4094,4094,4094,4094,4094,4094,ICLR,2020,Towards Stable and comprehensive Domain Alignment: Max-Margin Domain-Adversarial Training,Jianfei Yang;Han Zou;Yuxun Zhou;Lihua Xie,yang0478@e.ntu.edu.sg;hanzou@berkeley.edu;yxzhou@berkeley.edu;elhxie@ntu.edu.sg,6;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,National Taiwan University;University of California Berkeley;University of California Berkeley;National Taiwan University,86;5;5;86,120;13;13;120,4
4095,4095,4095,4095,4095,4095,4095,4095,ICLR,2020,In-Domain Representation Learning For Remote Sensing,Maxim Neumann;Andre Susano Pinto;Xiaohua Zhai;Neil Houlsby,maximneumann@google.com;andresp@google.com;xzhai@google.com;neilhoulsby@google.com,3;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,
4096,4096,4096,4096,4096,4096,4096,4096,ICLR,2020,Ternary MobileNets via Per-Layer Hybrid Filter Banks,Dibakar Gope;Jesse G Beu;Urmish Thakker;Matthew Mattina,dibakar.gope@arm.com;jesse.beu@arm.com;urmish.thakker@arm.com;matthew.mattina@arm.com,3;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,arm;arm;arm;arm,-1;-1;-1;-1,-1;-1;-1;-1,2
4097,4097,4097,4097,4097,4097,4097,4097,ICLR,2020,Avoiding Negative Side-Effects and Promoting Safe Exploration with Imaginative Planning,Dhruv Ramani;Benjamin Eysenbach,dhruvramani98@gmail.com;beysenba@cs.cmu.edu,1;1;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,0,0.0,yes,9/25/19,National Institute of Technology Warangal;Carnegie Mellon University,-1;1,-1;27,10
4098,4098,4098,4098,4098,4098,4098,4098,ICLR,2020,Exploring the Correlation between Likelihood of Flow-based Generative Models and Image Semantics,Xin WANG;SiuMing Yiu,xwang@cs.hku.hk;smyiu@cs.hku.hk,3;1;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,8,0.0,yes,9/25/19,The University of Hong Kong;The University of Hong Kong,92;92,35;35,5
4099,4099,4099,4099,4099,4099,4099,4099,ICLR,2020,Provably Communication-efficient Data-parallel SGD via Nonuniform Quantization,Ali Ramezani-Kebrya;Fartash Faghri;Ilya Markov;Vitalii Aksenov;Dan Alistarh;Daniel M. Roy,alir@vectorinstitute.ai;faghri@cs.toronto.edu;droy@utstat.toronto.edu;dan.alistarh@ist.ac.at;markovilya197@gmail.com;vitalii.aksenov@ist.ac.at,3;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,"Vector Institute;Department of Computer Science, University of Toronto;University of Toronto;Institute of Science and Technology Austria;;Institute of Science and Technology Austria",-1;18;18;481;-1;481,-1;18;18;1397;-1;1397,
4100,4100,4100,4100,4100,4100,4100,4100,ICLR,2020,Scalable Neural Learning for Verifiable Consistency with Temporal Specifications,Sumanth Dathathri;Johannes Welbl;Krishnamurthy (Dj) Dvijotham;Ramana Kumar;Aditya Kanade;Jonathan Uesato;Sven Gowal;Po-Sen Huang;Pushmeet Kohli,sdathath@caltech.edu;johannes.welbl.14@ucl.ac.uk;dvij@google.com;ramanakumar@google.com;akanade@google.com;juesato@google.com;sgowal@google.com;posenhuang@google.com;pushmeet@google.com,3;6;8;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,9,0.0,yes,9/25/19,California Institute of Technology;University College London;Google;Google;Google;Google;Google;Google;Google,143;50;-1;-1;-1;-1;-1;-1;-1,2;15;-1;-1;-1;-1;-1;-1;-1,3;4
4101,4101,4101,4101,4101,4101,4101,4101,ICLR,2020,EvoNet: A Neural Network for Predicting the Evolution of Dynamic Graphs,Changmin Wu;Giannis Nikolentzos;Michalis Vazirgiannis,changmin.wu@polytechnique.edu;giannisnik@hotmail.com;mvazirg@lix.polytechnique.fr,3;3;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,0,0.0,yes,9/25/19,"Ecole polytechnique;;Ecole Polytechnique, France",481;-1;481,93;-1;93,5;10
4102,4102,4102,4102,4102,4102,4102,4102,ICLR,2020,SGD Learns One-Layer Networks in WGANs,Qi Lei;Jason D. Lee;Alexandros G. Dimakis;Constantinos Daskalakis,leiqi@ices.utexas.edu;jasondlee88@gmail.com;dimakis@austin.utexas.edu;costis@csail.mit.edu,3;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,"University of Texas, Austin;University of Southern California;University of Texas, Austin;Massachusetts Institute of Technology",22;31;22;2,38;62;38;5,5;4
4103,4103,4103,4103,4103,4103,4103,4103,ICLR,2020,Improving Robustness Without Sacrificing Accuracy with Patch Gaussian Augmentation,Raphael Gontijo Lopes;Dong Yin;Ben Poole;Justin Gilmer;Ekin D. Cubuk,iraphael@google.com;dongyin@berkeley.edu;pooleb@google.com;gilmer@google.com;cubuk@google.com,3;3;8,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,5,0.0,yes,9/25/19,Google;University of California Berkeley;Google;Google;Google,-1;5;-1;-1;-1,-1;13;-1;-1;-1,4;8
4104,4104,4104,4104,4104,4104,4104,4104,ICLR,2020,Tensorized Embedding Layers for Efficient Model Compression,Oleksii Hrinchuk;Valentin Khrulkov;Leyla Mirvakhabova;Ivan Oseledets,oleksii.hrinchuk@skoltech.ru;khrulkov.v@gmail.com;leyla.mirvakhabova@skoltech.ru;i.oseledets@skoltech.ru,8;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Skolkovo Institute of Science and Technology;Skolkovo Institute of Science and Technology;Skolkovo Institute of Science and Technology;Skolkovo Institute of Science and Technology,-1;-1;-1;-1,-1;-1;-1;-1,3
4105,4105,4105,4105,4105,4105,4105,4105,ICLR,2020,Training Deep Networks with Stochastic Gradient Normalized by Layerwise Adaptive Second Moments,Boris Ginsburg;Patrice Castonguay;Oleksii Hrinchuk;Oleksii Kuchaiev;Vitaly Lavrukhin;Ryan Leary;Jason Li;Huyen Nguyen;Yang Zhang;Jonathan M. Cohen,boris.ginsburg@gmail.com;pcastonguay@nvidia.com;grinchuk.alexey@gmail.com;kuchaev@gmail.com;vlavrukhin@yahoo.com;rleary@nvidia.com;jasoli@nvidia.com;huyenntkvn@gmail.com;yangzhang@nvidia.com;jocohen@nvidia.com,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,3,0.0,yes,9/25/19,NVIDIA;NVIDIA;Moscow Institute of Physics and Technology;NVIDIA;;NVIDIA;NVIDIA;;NVIDIA;NVIDIA,-1;-1;481;-1;-1;-1;-1;-1;-1;-1,-1;-1;234;-1;-1;-1;-1;-1;-1;-1,3
4106,4106,4106,4106,4106,4106,4106,4106,ICLR,2020,Learning Time-Aware Assistance Functions for Numerical Fluid Solvers,Kiwon Um;Yun (Raymond) Fei;Philipp Holl;Nils Thuerey,kiwon.um@tum.de;yf2320@columbia.edu;philipp.holl@tum.de;nils.thuerey@tum.de,6;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,3,0.0,yes,9/25/19,Technical University Munich;Columbia University;Technical University Munich;Technical University Munich,53;15;53;53,43;16;43;43,
4107,4107,4107,4107,4107,4107,4107,4107,ICLR,2020,Why do These Match? Explaining the Behavior of Image Similarity Models,Bryan A. Plummer;Mariya I. Vasileva;Vitali Petsiuk;Kate Saenko;David Forsyth,bplumme2@illinois.edu;mvasile2@illinois.edu;vpetsiuk@bu.edu;saenko@bu.edu;daf@illinois.edu,6;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,5,0.0,yes,9/25/19,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;Boston University;Boston University;University of Illinois, Urbana Champaign",3;3;67;67;3,48;48;61;61;48,
4108,4108,4108,4108,4108,4108,4108,4108,ICLR,2020,Towards an Adversarially Robust Normalization Approach,Muhammad Awais;Fahad Shamshad;Sung-Ho Bae,awais@khu.ac.kr;fahad.shamshad@itu.edu.pk;shbae@khu.ac.kr,3;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,3,0,0.0,yes,9/25/19,"Kyung Hee University;ITU of Punjab Lahore, Pakistan;Kyung Hee University",481;-1;481,319;-1;319,4
4109,4109,4109,4109,4109,4109,4109,4109,ICLR,2020,When Does Self-supervision Improve Few-shot Learning?,Jong-Chyi Su;Subhransu Maji;Bharath Hariharan,jcsu@cs.umass.edu;smaji@cs.umass.edu;bharathh@cs.cornell.edu,3;3;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:N/A:N/A,Reject,0,4,0.0,yes,9/25/19,"University of Massachusetts, Amherst;University of Massachusetts, Amherst;Cornell University",28;28;7,209;209;19,6;8
4110,4110,4110,4110,4110,4110,4110,4110,ICLR,2020,D3PG: Deep Differentiable Deterministic Policy Gradients,Tao Du;Yunfei Li;Jie Xu;Andrew Spielberg;Kui Wu;Daniela Rus;Wojciech Matusik,taodu@csail.mit.edu;l-yf16@mails.tsinghua.edu.cn;jiex@csail.mit.edu;aespielberg@csail.mit.edu;walker.kui.wu@gmail.com;rus@csail.mit.edu;wojciech@csail.mit.edu,3;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,Massachusetts Institute of Technology;Tsinghua University;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;8;2;2;2;2;2,5;23;5;5;5;5;5,9
4111,4111,4111,4111,4111,4111,4111,4111,ICLR,2020,Sparse Transformer: Concentrated Attention Through Explicit Selection,Guangxiang Zhao;Junyang Lin;Zhiyuan Zhang;Xuancheng Ren;Xu Sun,1701214310@pku.edu.cn;junyang.ljy@alibaba-inc.com;zzy1210@pku.edu.cn;renxc@pku.edu.cn;xusun@pku.edu.cn,1;3;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,1,7,0.0,yes,9/25/19,Peking University;Alibaba Group;Peking University;Peking University;Peking University,22;-1;22;22;22,24;-1;24;24;24,3
4112,4112,4112,4112,4112,4112,4112,4112,ICLR,2020,SAFE-DNN: A Deep Neural Network with Spike Assisted Feature Extraction for Noise Robust Inference,Xueyuan She;Priyabrata Saha;Daehyun Kim;Yun Long;Saibal Mukhopadhyay,xshe6@gatech.edu;priyabratasaha@gatech.edu;daehyun.kim@gatech.edu;yunlong@gatech.edu;saibal.mukhopadhyay@ece.gatech.edu,3;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,4,0.0,yes,9/25/19,Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology,13;13;13;13;13,38;38;38;38;38,
4113,4113,4113,4113,4113,4113,4113,4113,ICLR,2020,TWIN GRAPH CONVOLUTIONAL NETWORKS: GCN WITH DUAL GRAPH SUPPORT FOR SEMI-SUPERVISED LEARNING,Feng Shi;Yizhou Zhao;Ziheng Xu;Tianyang Liu;Song-Chun Zhu,shi.feng@cs.ucla.edu;yizhouzhao@ucla.edu;lawrencexu@ucla.edu,3;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,"University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles",20;20;20,17;17;17,10
4114,4114,4114,4114,4114,4114,4114,4114,ICLR,2020,"Calibration, Entropy Rates, and Memory in Language Models",Mark Braverman;Xinyi Chen;Sham Kakade;Karthik Narasimhan;Cyril Zhang;Yi Zhang,mbraverm@cs.princeton.edu;xinyic@google.com;sham@cs.washington.edu;karthikn@cs.princeton.edu;cyril.zhang@cs.princeton.edu;y.zhang@cs.princeton.edu,6;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Princeton University;Google;University of Washington;Princeton University;Princeton University;Princeton University,31;-1;6;31;31;31,6;-1;26;6;6;6,3;5
4115,4115,4115,4115,4115,4115,4115,4115,ICLR,2020,Deep End-to-end Unsupervised Anomaly Detection ,Li Tangqing;Wang Zheng;Liu Siying;Daniel Lin Wen-Yan,li_tangqing@u.nus.edu;sliu50@illinois.edu;zhwang@i2r.a-star.edu.sg;daniellin@smu.edu.sg,3;6;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,"National University of Singapore;University of Illinois, Urbana Champaign;A*STAR;Singapore Management University",16;3;-1;92,25;48;-1;1397,
4116,4116,4116,4116,4116,4116,4116,4116,ICLR,2020,Role-Wise Data Augmentation for Knowledge Distillation,Jie Fu;Xue Geng;Bohan Zhuang;Xingdi Yuan;Adam Trischler;Jie Lin;Vijay Chandrasekhar;Chris Pal,jie.fu@polymtl.ca;geng_xue@i2r.a-star.edu.sg;bohan.zhuang@adelaide.edu.au;eryua@microsoft.com;adam.trischler@microsoft.com;lin-j@i2r.a-star.edu.sg;vijay@i2r.a-star.edu.sg;christopher.pal@polymtl.ca,3;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,4,0.0,yes,9/25/19,Polytechnique Montreal;A*STAR;The University of Adelaide;Microsoft;Microsoft;A*STAR;A*STAR;Polytechnique Montreal,390;-1;128;-1;-1;-1;-1;390,1397;-1;120;-1;-1;-1;-1;1397,
4117,4117,4117,4117,4117,4117,4117,4117,ICLR,2020,HighRes-net: Multi-Frame Super-Resolution by Recursive Fusion,Michel Deudon;Alfredo Kalaitzis;Md Rifat Arefin;Israel Goytom;Zhichao Lin;Kris Sankaran;Vincent Michalski;Samira E Kahou;Julien Cornebise;Yoshua Bengio,michel.deudon@elementai.com;freddie@element.ai;rifat.arefin515@gmail.com;isrugeek@gmail.com;zhichao.lin@elementai.com;sankaran.kris@gmail.com;vincent.michalski@gmx.de;samira.ebrahimi-kahou@polymtl.ca;julien@elementai.com;yoshua.bengio@mila.quebec,8;3;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,17,1.0,yes,9/25/19,Element AI;Element AI;University of Montreal;Boston University;Element AI;University of Montreal;Goethe University;Polytechnique Montreal;Element AI;University of Montreal,-1;-1;128;67;-1;128;233;390;-1;128,-1;-1;85;61;-1;85;305;1397;-1;85,5
4118,4118,4118,4118,4118,4118,4118,4118,ICLR,2020,Sparse and Structured Visual Attention,Pedro Henrique Martins;Vlad Niculae;Zita Marinho;André F.T. Martins,pedrohenriqueamartins@gmail.com;vlad@vene.ro;zita.marinho@priberam.pt;andre.martins@unbabel.com,3;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,"Instituto Superior Técnico;Instituto de Telecomunicações, Portugal;;Unbabel",481;-1;-1;-1,1397;-1;-1;-1,
4119,4119,4119,4119,4119,4119,4119,4119,ICLR,2020,Policy Tree Network,Zac Wellmer;Sepanta Zeighami;James Kwok,zac@1984.ai;szeighami@connect.ust.hk;jamesk@cse.ust.hk,3;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0.0,yes,9/25/19,Hong Kong University of Science and Technology;The Hong Kong University of Science and Technology;The Hong Kong University of Science and Technology,39;39;39,47;47;47,1
4120,4120,4120,4120,4120,4120,4120,4120,ICLR,2020,Bias-Resilient Neural Network,Ehsan Adeli;Qingyu Zhao;Adolf Pfefferbaum;Edith V. Sullivan;Fei-Fei Li;Juan Carlos Niebles;Kilian M. Pohl,eadeli@stanford.edu;qingyuz@stanford.edu;edie@stanford.edu;dolfp@stanford.edu;feifeili@cs.stanford.edu;jniebles@cs.stanford.edu;kilian.pohl@stanford.edu,8;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0.0,yes,9/25/19,Stanford University;Stanford University;Stanford University;Stanford University;Stanford University;Stanford University;Stanford University,4;4;4;4;4;4;4,4;4;4;4;4;4;4,7;4;2
4121,4121,4121,4121,4121,4121,4121,4121,ICLR,2020,ASYNCHRONOUS MULTI-AGENT GENERATIVE ADVERSARIAL IMITATION LEARNING,Xin Zhang;Weixiao Huang;Renjie Liao;Yanhua Li,xzhang17@wpi.edu;whuang2@wpi.edu;rjliao@cs.toronto.edu;yli15@wpi.edu,1;6;6,I have published in this field for several years.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,20,0.0,yes,9/25/19,"Worcester Polytechnic Institute;Worcester Polytechnic Institute;Department of Computer Science, University of Toronto;Worcester Polytechnic Institute",172;172;18;172,628;628;18;628,5;4
4122,4122,4122,4122,4122,4122,4122,4122,ICLR,2020,Efficient Training of Robust and Verifiable Neural Networks,Akhilan Boopathy;Lily Weng;Sijia Liu;Pin-Yu Chen;Luca Daniel,akhilan@mit.edu;twweng@mit.edu;sijia.liu@ibm.com;pin-yu.chen@ibm.com;dluca@mit.edu,1;1;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,6,0.0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;International Business Machines;International Business Machines;Massachusetts Institute of Technology,2;2;-1;-1;2,5;5;-1;-1;5,4;8
4123,4123,4123,4123,4123,4123,4123,4123,ICLR,2020,Adversarial Video Generation on Complex Datasets,Aidan Clark;Jeff Donahue;Karen Simonyan,aidanclark@google.com;jeffdonahue@google.com;simonyan@google.com,3;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,1,4,0.0,yes,9/25/19,Google;Google;Google,-1;-1;-1,-1;-1;-1,5;4
4124,4124,4124,4124,4124,4124,4124,4124,ICLR,2020,Data Augmentation in Training CNNs: Injecting Noise to Images,Murtaza Eren Akbiyik,erenakbiyik@gmail.com,3;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,2,0,0.0,yes,9/25/19,International Business Machines,-1,-1,
4125,4125,4125,4125,4125,4125,4125,4125,ICLR,2020,Variational Diffusion Autoencoders with Random Walk Sampling,Henry Li;Ofir Lindenbaum;Xiuyuan Cheng;Alexander Cloninger,henryli@eng.ucsd.edu;ofir.lindenbaum@yale.edu;xiuyuan.cheng@duke.edu;acloninger@ucsd.edu,8;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,"University of California, San Diego;Yale University;Duke University;University of California, San Diego",11;64;47;11,31;8;20;31,5;1
4126,4126,4126,4126,4126,4126,4126,4126,ICLR,2020,"INFERENCE, PREDICTION, AND ENTROPY RATE OF CONTINUOUS-TIME, DISCRETE-EVENT PROCESSES",Sarah Marzen;James P. Crutchfield,smarzen@cmc.edu;chaos@cse.ucdavis.edu,1;3;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:N/A:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,"Central Methodist College;University of California, Davis",-1;79,-1;55,11
4127,4127,4127,4127,4127,4127,4127,4127,ICLR,2020,Scale-Equivariant Neural Networks with Decomposed Convolutional Filters,Wei Zhu;Qiang Qiu;Robert Calderbank;Guillermo Sapiro;Xiuyuan Cheng,zhu@math.duke.edu;qiang.qiu@duke.edu;robert.calderbank@duke.edu;guillermo.sapiro@duke.edu;xiuyuan.cheng@duke.edu,6;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,1,5,0.0,yes,9/25/19,Duke University;Duke University;Duke University;Duke University;Duke University,47;47;47;47;47,20;20;20;20;20,
4128,4128,4128,4128,4128,4128,4128,4128,ICLR,2020,Dynamic Scale Inference by Entropy Minimization,Dequan Wang;Evan Shelhamer;Bruno Olshausen;Trevor Darrell,dqwang@eecs.berkeley.edu;shelhamer@cs.berkeley.edu;baolshausen@berkeley.edu;trevor@eecs.berkeley.edu,3;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5,13;13;13;13,2
4129,4129,4129,4129,4129,4129,4129,4129,ICLR,2020,The Effect of Residual Architecture on the Per-Layer Gradient of Deep Networks,Etai Littwin;Lior Wolf,etai.littwin@gmail.com;wolf@fb.com,1;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,1.0,yes,9/25/19,Tel Aviv University;Facebook,35;-1,188;-1,
4130,4130,4130,4130,4130,4130,4130,4130,ICLR,2020,City Metro Network Expansion with Reinforcement Learning,Yu Wei;Minjia Mao;Xi Zhao;Jianhua Zou,weiyu123112@163.com;maominjia@foxmail.com;zhaoxi1@mail.xjtu.edu.cn;jhzou@sei.xjtu.edu.cn,3;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Xi'an Jiaotong University;Foxmail;Xi'an Jiaotong University;Xi'an Jiaotong University,481;-1;481;481,555;-1;555;555,
4131,4131,4131,4131,4131,4131,4131,4131,ICLR,2020,Dropout: Explicit Forms and Capacity Control,Raman Arora;Peter L. Bartlett;Poorya Mianjy;Nathan Srebro,arora@cs.jhu.edu;bartlett@cs.berkeley.edu;mianjy@jhu.edu;nati@ttic.edu,1;1;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Johns Hopkins University;University of California Berkeley;Johns Hopkins University;Toyota Technological Institute at Chicago,73;5;73;-1,12;13;12;-1,8
4132,4132,4132,4132,4132,4132,4132,4132,ICLR,2020,Verification of Generative-Model-Based Visual Transformations,Matthew Mirman;Timon Gehr;Martin Vechev,matthew.mirman@inf.ethz.ch;timon.gehr@inf.ethz.ch;martin.vechev@inf.ethz.ch,3;3;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,6,0.0,yes,9/25/19,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10,13;13;13,5
4133,4133,4133,4133,4133,4133,4133,4133,ICLR,2020,Beyond GANs: Transforming without a Target Distribution,Matthew Amodio;David van Dijk;Ruth Montgomery;Guy Wolf;Smita Krishnaswamy,matthew.amodio@yale.edu;david.vandijk@yale.edu;ruth.montgomery@yale.edu;guy.wolf@umontreal.ca;smita.krishnaswamy@yale.edu,6;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,3,0.0,yes,9/25/19,Yale University;Yale University;Yale University;University of Montreal;Yale University,64;64;64;128;64,8;8;8;85;8,5;4;7
4134,4134,4134,4134,4134,4134,4134,4134,ICLR,2020,On Variational Learning of Controllable Representations for Text without Supervision,Peng Xu;Yanshuai Cao;Jackie Chi Kit Cheung,pxu4@ualberta.ca;yanshuaicao@gmail.com;jcheung@cs.mcgill.ca,8;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,1.0,yes,9/25/19,University of Alberta;;McGill University,100;-1;86,136;-1;42,5
4135,4135,4135,4135,4135,4135,4135,4135,ICLR,2020,Demystifying Graph Neural Network Via Graph Filter Assessment,Yewen Wang;Ziniu Hu;Yusong Ye;Yizhou Sun,wyw10804@gmail.com;bull@cs.ucla.edu;yusongye@g.ucla.edu;yzsun@cs.ucla.edu,8;1;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,3,6,0.0,yes,9/25/19,"University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles",20;20;20;20,17;17;17;17,10
4136,4136,4136,4136,4136,4136,4136,4136,ICLR,2020,Individualised Dose-Response Estimation using Generative Adversarial Nets,Ioana Bica;James Jordon;Mihaela van der Schaar,ioana.bica@eng.ox.ac.uk;james.jordon@wolfson.ox.ac.uk;mschaar@turing.ac.uk,1;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,5,0.0,yes,9/25/19,University of Oxford;University of Oxford;Alan Turing Institute,50;50;-1,1;1;-1,5;4
4137,4137,4137,4137,4137,4137,4137,4137,ICLR,2020,"``Best-of-Many-Samples"" Distribution Matching""",Apratim Bhattacharyya;Mario Fritz;Bernt Schiele,abhattac@mpi-inf.mpg.de;fritz@cispa.saarland;schiele@mpi-inf.mpg.de,3;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,0,0.0,yes,9/25/19,"Saarland Informatics Campus, Max-Planck Institute;CISPA Helmholtz Center for Information Security;Saarland Informatics Campus, Max-Planck Institute",-1;143;-1,-1;1397;-1,5;4
4138,4138,4138,4138,4138,4138,4138,4138,ICLR,2020,Swoosh! Rattle! Thump! - Actions that Sound,Dhiraj Gandhi;Abhinav Gupta;Lerrel Pinto,g.prakashchand@gmail.com;abhinavg@cs.cmu.edu;lerrel.pinto@gmail.com,6;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;University of California Berkeley,1;1;5,27;27;13,
4139,4139,4139,4139,4139,4139,4139,4139,ICLR,2020,Near-Zero-Cost Differentially Private Deep Learning with Teacher Ensembles,Lichao Sun;Yingbo Zhou;Jia Li;Richard Socher;Philip S. Yu;Caiming Xiong,james.lichao.sun@gmail.com;yingbo.zhou@salesforce.com;jia.li@salesforce.com;rsocher@salesforce.com;psyu@uic.edu;cxiong@salesforce.com,1;1;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,3,0.0,yes,9/25/19,"University of Illinois, Chicago;SalesForce.com;SalesForce.com;SalesForce.com;University of Illinois, Chicago;SalesForce.com",56;-1;-1;-1;56;-1,254;-1;-1;-1;254;-1,
4140,4140,4140,4140,4140,4140,4140,4140,ICLR,2020,On the Tunability of Optimizers in Deep Learning,Prabhu Teja S*;Florian Mai*;Thijs Vogels;Martin Jaggi;Francois Fleuret,prabhu.teja@idiap.ch;florian.mai@idiap.ch;thijs.vogels@epfl.ch;martin.jaggi@epfl.ch;francois.fleuret@idiap.ch,3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,10,0.0,yes,9/25/19,Idiap Research Institute;Idiap Research Institute;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Idiap Research Institute,-1;-1;481;481;-1,-1;-1;38;38;-1,
4141,4141,4141,4141,4141,4141,4141,4141,ICLR,2020,GUIDEGAN:  ATTENTION  BASED  SPATIAL  GUIDANCE FOR  IMAGE-TO-IMAGE TRANSLATION,Yu Lin;Yigong Wang;Yifan Li;Zhuoyi Wang;Yang Gao;Latifur Khan,yxl163430@utdallas.edu;yxw158830@utdallas.edu;yli@utdallas.edu;zhuoyi.wang1@utdallas.edu;yxg122530@utdallas.edu;lkhan@utdallas.edu,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,5,0.0,yes,9/25/19,"University of Texas, Dallas;University of Texas, Dallas;University of Texas, Dallas;University of Texas, Dallas;University of Texas, Dallas;University of Texas, Dallas",86;86;86;86;86;86,319;319;319;319;319;319,5;4
4142,4142,4142,4142,4142,4142,4142,4142,ICLR,2020,ROBUST GENERATIVE ADVERSARIAL NETWORK,Shufei Zhang;Zhuang Qian;Kaizhu Huang;Rui Zhang;Jimin Xiao,shufei.zhang@xjtlu.edu.cn;qz2009425@gmail.com;kaizhu.huang@xjtlu.edu.cn;rui.zhang02@xjtlu.edu.cn;jimin.xiao@xjtlu.edu.cn,1;3;3,I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:N/A:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Tsinghua University;Xi'an Jiaotong-Liverpool University;Tsinghua University;Tsinghua University;Tsinghua University,8;-1;8;8;8,23;-1;23;23;23,5;4;8
4143,4143,4143,4143,4143,4143,4143,4143,ICLR,2020,Decoupling Hierarchical Recurrent Neural Networks With Locally Computable Losses,Asier Mujika;Felix Weissenberger;Angelika Steger,asierm@inf.ethz.ch;felix.weissenberger@inf.ethz.ch;steger@inf.ethz.ch,1;1;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,1,0.0,yes,9/25/19,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10,13;13;13,
4144,4144,4144,4144,4144,4144,4144,4144,ICLR,2020,Branched Multi-Task Networks: Deciding What Layers To Share,Simon Vandenhende;Stamatios Georgoulis;Bert De Brabandere;Luc Van Gool,simon.vandenhende@kuleuven.be;georgous@ee.ethz.ch;bert.debrabandere@esat.kuleuven.be;vangool@vision.ee.ethz.ch,3;6;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,KU Leuven;Swiss Federal Institute of Technology;KU Leuven;Swiss Federal Institute of Technology,118;10;118;10,45;13;45;13,
4145,4145,4145,4145,4145,4145,4145,4145,ICLR,2020,Variational pSOM: Deep Probabilistic Clustering with Self-Organizing Maps,Laura Manduchi;Matthias Hüser;Gunnar Rätsch;Vincent Fortuin,lauraman@student.ethz.ch;matthias.hueser@inf.ethz.ch;gunnar.ratsch@ratschlab.org;fortuin@inf.ethz.ch,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;MSKCC New York;Swiss Federal Institute of Technology,10;10;-1;10,13;13;-1;13,
4146,4146,4146,4146,4146,4146,4146,4146,ICLR,2020,Tensor Graph Convolutional Networks for Prediction on Dynamic Graphs,Osman Asif Malik;Shashanka Ubaru;Lior Horesh;Misha E. Kilmer;Haim Avron,osman.malik.87@gmail.com;shashanka.ubaru@ibm.com;lhoresh@us.ibm.com;misha.kilmer@tufts.edu;haimav@tauex.tau.ac.il,3;6;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,"University of Colorado, Boulder;International Business Machines;International Business Machines;Tufts University;Tel Aviv University",44;-1;-1;172;35,123;-1;-1;139;188,3;10
4147,4147,4147,4147,4147,4147,4147,4147,ICLR,2020,Mint: Matrix-Interleaving for Multi-Task Learning,Tianhe Yu;Saurabh Kumar;Eric Mitchell;Abhishek Gupta;Karol Hausman;Sergey Levine;Chelsea Finn,tianheyu@cs.stanford.edu;szk@stanford.edu;eric.anthony.mitchell95@gmail.com;abhigupta@berkeley.edu;hausmankarol@gmail.com;svlevine@eecs.berkeley.edu;cbfinn@cs.stanford.edu,3;3;3;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Stanford University;Stanford University;Stanford University;University of California Berkeley;Google;University of California Berkeley;Stanford University,4;4;4;5;-1;5;4,4;4;4;13;-1;13;4,8
4148,4148,4148,4148,4148,4148,4148,4148,ICLR,2020,Effective and Robust Detection of Adversarial Examples via Benford-Fourier Coefficients,Chengcheng Ma;Baoyuan Wu;Shibiao Xu;Yanbo Fan;Yong Zhang;Xiaopeng Zhang;Zhifeng Li,machengcheng2016@gmail.com;wubaoyuan1987@gmail.com;shibiao.xu@ia.ac.cn;fanyanbo0124@gmail.com;zhangyong201303@gmail.com;xiaopeng.zhang@ia.ac.cn;michaelzfli@tencent.com,6;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,"Institute of automation, Chinese academy of science, Chinese Academy of Sciences;Tencent AI Lab;Institute of automation, Chinese academy of science, Chinese Academy of Sciences;Tencent AI Lab;;Institute of automation, Chinese academy of science, Chinese Academy of Sciences;Tencent AI Lab",59;-1;59;-1;-1;59;-1,1397;-1;1397;-1;-1;1397;-1,4
4149,4149,4149,4149,4149,4149,4149,4149,ICLR,2020,Multi-objective Neural Architecture Search via Predictive Network Performance Optimization,Han Shi;Renjie Pi;Hang Xu;Zhenguo Li;James T. Kwok;Tong Zhang,hshiac@cse.ust.hk;pipilu@connect.hku.hk;xbjxh@live.com;li.zhenguo@huawei.com;jamesk@cse.ust.hk;tongzhang@tongzhang-ml.org,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,8,0.0,yes,9/25/19,The Hong Kong University of Science and Technology;The University of Hong Kong;Huawei Technologies Ltd.;Huawei Technologies Ltd.;The Hong Kong University of Science and Technology;The Hong Kong University of Science and Technology,39;92;-1;-1;39;39,47;35;-1;-1;47;47,11;10
4150,4150,4150,4150,4150,4150,4150,4150,ICLR,2020,CONFEDERATED MACHINE LEARNING ON HORIZONTALLY AND VERTICALLY SEPARATED MEDICAL DATA FOR LARGE-SCALE HEALTH SYSTEM INTELLIGENCE,Dianbo Liu;Tim Miller;Kenneth Mandl,dianbo.liu@childrens.harvard.edu;timothy.miller@childrens.harvard.edu;kenneth.mandl@childrens.harvard.edu,3;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Harvard University;Harvard University;Harvard University,39;39;39,7;7;7,
4151,4151,4151,4151,4151,4151,4151,4151,ICLR,2020,On Evaluating Explainability Algorithms,Gokula Krishnan Santhanam;Ali Alami-Idrissi;Nuno Mota;Anika Schumann;Ioana Giurgiu,gst@zurich.ibm.com;aai@zurich.ibm.com;nuno.motagoncalves@epfl.ch;ikh@zurich.ibm.com;igi@zurich.ibm.com,1;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,International Business Machines;International Business Machines;Swiss Federal Institute of Technology Lausanne;International Business Machines;International Business Machines,-1;-1;481;-1;-1,-1;-1;38;-1;-1,
4152,4152,4152,4152,4152,4152,4152,4152,ICLR,2020,FRICATIVE PHONEME DETECTION WITH ZERO DELAY,Metehan Yurt;Alberto N. Escalante B.;Veniamin I. Morgenshtern,metehan.yurt@fau.de;alberto.escalante@sivantos.com;veniamin.morgenshtern@fau.de,3;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,3,0.0,yes,9/25/19,University of Erlangen-Nuremberg;Sivantos;University of Erlangen-Nuremberg,205;-1;205,182;-1;182,
4153,4153,4153,4153,4153,4153,4153,4153,ICLR,2020,Deep Mining: Detecting Anomalous Patterns in Neural Network Activations with Subset Scanning,Skyler Speakman;Celia Cintas;Victor Akinwande;Srihari Sridharan;Edward McFowland III,skyler@ke.ibm.com;celia.cintas@ibm.com;victor.akinwande1@ibm.com;sriharis.sridharan@ke.ibm.com;mcfowland@umn.edu,3;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,"International Business Machines;International Business Machines;International Business Machines;International Business Machines;University of Minnesota, Minneapolis",-1;-1;-1;-1;59,-1;-1;-1;-1;79,4
4154,4154,4154,4154,4154,4154,4154,4154,ICLR,2020,"GRAPHS, ENTITIES, AND STEP MIXTURE",Kyuyong Shin;Wonyoung Shin;Jung-Woo Ha;Sunyoung Kwon,p37329@gmail.com;wyshin@kaist.ac.kr;jungwoo.ha@navercorp.com;sunny.kwon@navercorp.com,3;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,NAVER;Korea Advanced Institute of Science and Technology;NAVER;NAVER,-1;481;-1;-1,-1;110;-1;-1,10;8
4155,4155,4155,4155,4155,4155,4155,4155,ICLR,2020,Learning Human Postural Control with Hierarchical Acquisition Functions,Nils Rottmann;Tjasa Kunavar;Jan Babic;Jan Peters;Elmar Rueckert,rottmann@rob.uni-luebeck.de;tjasa.kunavar@ijs.si;jan.babic@ijs.si;mail@jan-peters.net;rueckert@ai-lab.science,1;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A,Reject,0,3,0.0,yes,9/25/19,University of Luebeck;Jozef Stefan institute;Jozef Stefan institute;TU Darmstadt;Universität zu Lübeck,266;-1;-1;64;266,1397;-1;-1;289;1397,11
4156,4156,4156,4156,4156,4156,4156,4156,ICLR,2020,Unknown-Aware Deep Neural Network,Lei Cao;Yizhou Yan;Samuel Madden;Elke Rundensteiner,lcao@csail.mit.edu;yyan2@wpi.edu;madden@csail.mit.edu;rundenst@cs.wpi.edu,8;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Massachusetts Institute of Technology;Worcester Polytechnic Institute;Massachusetts Institute of Technology;Worcester Polytechnic Institute,2;172;2;172,5;628;5;628,8
4157,4157,4157,4157,4157,4157,4157,4157,ICLR,2020,Low Bias Gradient Estimates for Very Deep Boolean Stochastic Networks,Adeel Pervez;Taco Cohen;Efstratios Gavves,a.a.pervez@uva.nl;tacos@qti.qualcomm.com;efstratios.gavves@gmail.com,3;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,11,0.0,yes,9/25/19,"University of Amsterdam;Qualcomm Inc, QualComm;University of Amsterdam",172;-1;172,62;-1;62,
4158,4158,4158,4158,4158,4158,4158,4158,ICLR,2020,SoftLoc: Robust Temporal Localization under Label Misalignment,Julien Schroeter;Kirill Sidorov;Dave Marshall,schroeterj1@cardiff.ac.uk;sidorovk@cardiff.ac.uk;marshallad@cardiff.ac.uk,3;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,Cardiff University;Cardiff University;Cardiff University,172;172;172,196;196;196,
4159,4159,4159,4159,4159,4159,4159,4159,ICLR,2020,Learning Video Representations using Contrastive Bidirectional Transformer,Chen Sun;Fabien Baradel;Kevin Murphy;Cordelia Schmid,chensun@google.com;fabien.baradel@insa-lyon.fr;kpmurphy@google.com;cordelias@google.com,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,3,0.0,yes,9/25/19,Google;INSA de Lyon;Google;Google,-1;481;-1;-1,-1;1397;-1;-1,2
4160,4160,4160,4160,4160,4160,4160,4160,ICLR,2020,ShardNet: One Filter Set to Rule Them All,Saumya Jetley;Tommaso Cavallari;Philip Torr;Stuart Golodetz,sjetley@robots.ox.ac.uk;tommaso.cavallari@five.ai;phil@five.ai;stuart@five.ai,3;3;1,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,7,0.0,yes,9/25/19,University of Oxford;FiveAI;FiveAI;FiveAI,50;-1;-1;-1,1;-1;-1;-1,2
4161,4161,4161,4161,4161,4161,4161,4161,ICLR,2020,Explaining Time Series by Counterfactuals,Sana Tonekaboni;Shalmali Joshi;David Duvenaud;Anna Goldenberg,stonekaboni@cs.toronto.edu;shalmali@vectorinstitute.ai;duvenaud@cs.toronto.edu;anna.goldenberg@utoronto.ca,6;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,"Department of Computer Science, University of Toronto;Vector Institute;Department of Computer Science, University of Toronto;Toronto University",18;-1;18;18,18;-1;18;18,
4162,4162,4162,4162,4162,4162,4162,4162,ICLR,2020,On the Linguistic Capacity of Real-time Counter Automata,William Merrill,vikingarnir.will@gmail.com,6;1;6,I do not know much about this area.:N/A:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Allen Institute for Artificial Intelligence,-1,-1,3;1
4163,4163,4163,4163,4163,4163,4163,4163,ICLR,2020,The fairness-accuracy landscape of neural classifiers,Susan Wei;Marc Niethammer,susan.wei@unimelb.edu.au;mn@cs.unc.edu,1;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,"The University of Melbourne;University of North Carolina, Chapel Hill",118;73,32;54,4;7
4164,4164,4164,4164,4164,4164,4164,4164,ICLR,2020,Corpus Based Amharic Sentiment Lexicon Generation,Girma Neshir;Andeas Rauber;and Solomon Atnafu,girma1978@gmail.com;rauber@ifs.tuwien.ac.at;solomon.atnafu@aau.edu.et,1;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,0,0.0,yes,9/25/19,Addis Ababa University;TU Wien Vienna University of Technology;Addis Ababa University,481;100;481,1397;360;1397,
4165,4165,4165,4165,4165,4165,4165,4165,ICLR,2020,MaskConvNet: Training Efficient ConvNets from Scratch via Budget-constrained Filter Pruning,Raden Mu'az Mun'im;Jie Lin;Vijay Chandrasekhar;Koichi Shinoda,raden.m.muaz@gmail.com;lin-j@i2r.a-star.edu.sg;vijay@i2r.a-star.edu.sg;shinoda@ks.cs.titech.ac.jp,3;3;3,I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0.0,yes,9/25/19,Universiti Teknologi Malaysia;A*STAR;A*STAR;Tokyo Institute of Technology,481;-1;-1;172,674;-1;-1;299,
4166,4166,4166,4166,4166,4166,4166,4166,ICLR,2020,Curvature-based Robustness Certificates against Adversarial Examples,Sahil Singla;Soheil Feizi,ssingla@cs.umd.edu;sfeizi@cs.umd.edu,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0.0,yes,9/25/19,"University of Maryland, College Park;University of Maryland, College Park",12;12,91;91,4;1;9
4167,4167,4167,4167,4167,4167,4167,4167,ICLR,2020,Pipelined Training with Stale Weights of Deep Convolutional Neural Networks,Lifu Zhang;Tarek S. Abdelrahman,lifu.zhang@mail.utoronto.ca;tsa@ece.utoronto.ca,3;6;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,6,0.0,yes,9/25/19,Toronto University;Toronto University,18;18,18;18,
4168,4168,4168,4168,4168,4168,4168,4168,ICLR,2020,Information Plane Analysis of Deep Neural Networks via Matrix--Based Renyi's Entropy and Tensor Kernels,Kristoffer Wickstrøm;Sigurd Løkse;Michael Kampffmeyer;Shujian Yu;Jose Principe;Robert Jenssen,kristoffer.k.wickstrom@uit.no;sigurd.lokse@uit.no;michael.c.kampffmeyer@uit.no;yusjlcy9011@cnel.ufl.edu;principe@cnel.ufl.edu;robert.jenssen@uit.no,3;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,UiT The Arctic University of Norway;UiT The Arctic University of Norway;UiT The Arctic University of Norway;University of Florida;University of Florida;UiT The Arctic University of Norway,-1;-1;-1;128;128;-1,419;419;419;174;174;419,8
4169,4169,4169,4169,4169,4169,4169,4169,ICLR,2020,Disentangled Representation Learning with Sequential Residual Variational Autoencoder,Nanxiang Li;Shabnam Ghaffarzadegan;Liu Ren,nanxiang.li@us.bosch.com;shabnam.ghaffarzadegan@us.bosch.com;liu.ren@us.bosch.com,3;8;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Bosch;Bosch;Bosch,-1;-1;-1,-1;-1;-1,5
4170,4170,4170,4170,4170,4170,4170,4170,ICLR,2020,MULTIPOLAR: Multi-Source Policy Aggregation for Transfer Reinforcement Learning between Diverse Environmental Dynamics,Mohammadamin Barekatain;Ryo Yonetani;Masashi Hamaya,m.barekatain@tum.de;ryo.yonetani@sinicx.com;masashi.hamaya@sinicx.com,1;8;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,Technical University Munich;OMRON SINIC X;OMRON SINIC X,53;-1;-1,43;-1;-1,
4171,4171,4171,4171,4171,4171,4171,4171,ICLR,2020,Simultaneous Classification and Out-of-Distribution Detection Using Deep Neural Networks,Aristotelis-Angelos Papadopoulos;Nazim Shaikh;Jiamian Wang;Mohammad Reza Rajati,aristotp@usc.edu;nshaikh@usc.edu;jiamianw@usc.edu;rajati@usc.edu,6;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,11,0.0,yes,9/25/19,University of Southern California;University of Southern California;University of Southern California;University of Southern California,31;31;31;31,62;62;62;62,
4172,4172,4172,4172,4172,4172,4172,4172,ICLR,2020,"Learning vector representation of local content and matrix representation of local motion, with implications for V1",Ruiqi Gao;Jianwen Xie;Siyuan Huang;Yufan Ren;Song-Chun Zhu;Ying Nian Wu,ruiqigao@ucla.edu;jianwen@ucla.edu;huangsiyuan@ucla.edu;3160104704@zju.edu.cn;sczhu@stat.ucla.edu;ywu@stat.ucla.edu,3;1;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,"University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles;Zhejiang University;University of California, Los Angeles;University of California, Los Angeles",20;20;20;56;20;20,17;17;17;107;17;17,
4173,4173,4173,4173,4173,4173,4173,4173,ICLR,2020,Data Valuation using Reinforcement Learning,Jinsung Yoon;Sercan O. Arik;Tomas Pfister,jsyoon0823@gmail.com;soarik@google.com;tpfister@google.com,6;6;3,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,"University of California, Los Angeles;Google;Google",20;-1;-1,17;-1;-1,
4174,4174,4174,4174,4174,4174,4174,4174,ICLR,2020,Reject Illegal Inputs: Scaling Generative Classifiers with Supervised Deep Infomax,Xin WANG;SiuMing Yiu,xwang@cs.hku.hk;smyiu@cs.hku.hk,3;8;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,3,1.0,yes,9/25/19,The University of Hong Kong;The University of Hong Kong,92;92,35;35,5;4
4175,4175,4175,4175,4175,4175,4175,4175,ICLR,2020,Surrogate-Based Constrained Langevin Sampling With Applications to Optimal Material Configuration Design,Thanh V Nguyen;Youssef Mroueh;Samuel C. Hoffman;Payel Das;Pierre Dognin;Giuseppe Romano;Chinmay Hegde,thanhng@iastate.edu;mroueh@us.ibm.com;shoffman@ibm.com;daspa@us.ibm.com;pdognin@us.ibm.com;romanog@mit.edu;chinmay@iastate.edu,3;6;6,I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Iowa State University;International Business Machines;International Business Machines;International Business Machines;International Business Machines;Massachusetts Institute of Technology;Iowa State University,172;-1;-1;-1;-1;2;172,399;-1;-1;-1;-1;5;399,1
4176,4176,4176,4176,4176,4176,4176,4176,ICLR,2020,VILD: Variational Imitation Learning with Diverse-quality Demonstrations,Voot Tangkaratt;Bo Han;Mohammad Emtiyaz Khan;Masashi Sugiyama,voot.tangkaratt@riken.jp;bo.han@riken.jp;emtiyaz.khan@riken.jp;sugi@k.u-tokyo.ac.jp,6;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,4,0.0,yes,9/25/19,RIKEN;RIKEN;RIKEN;The University of Tokyo,-1;-1;-1;56,-1;-1;-1;36,10
4177,4177,4177,4177,4177,4177,4177,4177,ICLR,2020,OPTIMAL BINARY QUANTIZATION FOR DEEP NEURAL NETWORKS,Hadi Pouransari;Oncel Tuzel,mpouransari@apple.com;onceltuzel@gmail.com,3;3;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,1.0,yes,9/25/19,Apple;MERL,-1;-1,-1;-1,1
4178,4178,4178,4178,4178,4178,4178,4178,ICLR,2020,Learning Through Limited Self-Supervision: Improving Time-Series Classification Without Additional Data via Auxiliary Tasks,Ian Fox;Harry Rubin-Falcone;Jenna Wiens,ifox@umich.edu;hrf@umich.edu;wiensj@umich.edu,1;3;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,4,0.0,yes,9/25/19,University of Michigan;University of Michigan;University of Michigan,8;8;8,21;21;21,
4179,4179,4179,4179,4179,4179,4179,4179,ICLR,2020,Robust Natural Language Representation Learning for Natural Language Inference by Projecting Superficial Words out,Wanyun Cui;Guangyu Zheng;Wei Wang,cui.wanyun@sufe.edu.cn;simonzgy@outlook.com;weiwang1@fudan.edu.cn,1;3;1,I have published in this field for several years.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Shanghai University of Finance and Economics;Fudan University;Fudan University,266;79;79,1397;109;109,3
4180,4180,4180,4180,4180,4180,4180,4180,ICLR,2020,DSReg: Using Distant Supervision as a Regularizer,Yuxian Meng;Muyu Li;Xiaoya Li;Wei Wu;Fei Wu;Jiwei Li,yuxian_meng@shannonai.com;muyu_li@shannonai.com;xiaoya_li@shannonai.com;wei_wu@shannonai.com;wufei@zju.edu.cn;jiwei_li@shannonai.com,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,2,0.0,yes,9/25/19,Shannon.AI;Shannon.AI;Shannon.AI;Shannon.AI;Zhejiang University;Shannon.AI,-1;-1;-1;-1;56;-1,-1;-1;-1;-1;107;-1,3
4181,4181,4181,4181,4181,4181,4181,4181,ICLR,2020,MxPool: Multiplex Pooling for Hierarchical Graph Representation Learning,Yanyan Liang;Yanfeng Zhang;Fangjing Wang;Qian Xu,13354227340@163.com;zhangyf@mail.neu.edu.cn;inggraph@qq.com;xuqian1286@163.com,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,163;Northeastern University;;163,-1;16;-1;-1,-1;906;-1;-1,10
4182,4182,4182,4182,4182,4182,4182,4182,ICLR,2020,Non-linear System Identification from Partial Observations via Iterative Smoothing and Learning,Kunal Menda;Jean de Becdelièvre;Jayesh K Gupta;Ilan Kroo;Mykel J. Kochenderfer;Zachary Manchester,kmenda@stanford.edu;jeandb@stanford.edu;jkg@cs.stanford.edu;kroo@stanford.edu;mykel@stanford.edu;zacmanchester@stanford.edu,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,Stanford University;Stanford University;Stanford University;Stanford University;Stanford University;Stanford University,4;4;4;4;4;4,4;4;4;4;4;4,1
4183,4183,4183,4183,4183,4183,4183,4183,ICLR,2020,Policy Message Passing: A New Algorithm for Probabilistic Graph Inference,Zhiwei Deng;Greg Mori,zhiweid@princeton.edu;mori@cs.sfu.ca,1;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Princeton University;Simon Fraser University,31;64,6;272,10
4184,4184,4184,4184,4184,4184,4184,4184,ICLR,2020,Learning Mahalanobis Metric Spaces via Geometric Approximation Algorithms,Diego Ihara;Neshat Mohammadi;Anastasios Sidiropoulos,dihara@gmail.com;nmoham24@uic.edu;sidiropo@uic.edu,3;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,"University of Illinois, Chicago;University of Illinois, Chicago;University of Illinois, Chicago",56;56;56,254;254;254,4
4185,4185,4185,4185,4185,4185,4185,4185,ICLR,2020,On Understanding Knowledge Graph Representation,Carl Allen*;Ivana Balazevic*;Timothy M Hospedales,carl.allen@ed.ac.uk;ivana.balazevic@ed.ac.uk;t.hospedales@ed.ac.uk,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,6,1.0,yes,9/25/19,University of Edinburgh;University of Edinburgh;University of Edinburgh,33;33;33,30;30;30,3;10
4186,4186,4186,4186,4186,4186,4186,4186,ICLR,2020,Learning to Prove Theorems by Learning to Generate Theorems,Mingzhe Wang;Jia Deng,mingzhew@cs.princeton.edu;jiadeng@princeton.edu,6;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,Princeton University;Princeton University,31;31,6;6,1
4187,4187,4187,4187,4187,4187,4187,4187,ICLR,2020,Augmenting Self-attention with Persistent Memory,Sainbayar Sukhbaatar;Edouard Grave;Guillaume Lample;Herve Jegou;Armand Joulin,sainbar@fb.com;egrave@fb.com;guismay@fb.com;rvj@fb.com;ajoulin@fb.com,3;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,2,0.0,yes,9/25/19,Facebook;Facebook;Facebook;Facebook;Facebook,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,3
4188,4188,4188,4188,4188,4188,4188,4188,ICLR,2020,Realism Index: Interpolation in Generative Models With Arbitrary Prior,Łukasz Struski;Jacek Tabor;Igor Podolak;Aleksandra Nowak;Krzysztof Maziarz,lukasz.struski@uj.edu.pl;jacek.tabor@uj.edu.pl;igor.podolak@uj.edu.pl;aknoow@gmail.com;krzysztof.s.maziarz@gmail.com,3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,Jagiellonian University;Jagiellonian University;Jagiellonian University;;Jagiellonian University,481;481;481;-1;481,610;610;610;-1;610,5
4189,4189,4189,4189,4189,4189,4189,4189,ICLR,2020,Fully Convolutional Graph Neural Networks using Bipartite Graph Convolutions,Marcel Nassar;Xin Wang;Evren Tumer,nassar.marcel@gmail.com;caseus.viridis@gmail.com;nervetumer@gmail.com,3;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,3,0.0,yes,9/25/19,"Intel;Cerebras Systems, Inc;",-1;-1;-1,-1;-1;-1,10
4190,4190,4190,4190,4190,4190,4190,4190,ICLR,2020,Selective sampling for accelerating  training of deep neural networks,Berry Weinstein;Shai Fine;Yacov Hel-Or,berry.weinstein@post.idc.ac.il;shai.fine@idc.ac.il;toky@idc.ac.il,1;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,2,0.0,yes,9/25/19,interdisciplinary center herzliya;interdisciplinary center herzliya;interdisciplinary center herzliya,-1;-1;-1,-1;-1;-1,8
4191,4191,4191,4191,4191,4191,4191,4191,ICLR,2020,Hierarchical Graph-to-Graph Translation for Molecules,Wengong Jin;Regina Barzilay;Tommi Jaakkola,wengong@csail.mit.edu;regina@csail.mit.edu;tommi@csail.mit.edu,3;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2,5;5;5,10
4192,4192,4192,4192,4192,4192,4192,4192,ICLR,2020,MelNet: A Generative Model for Audio in the Frequency Domain,Sean Vasquez;Mike Lewis,seanjv@mit.edu;mikelewis@fb.com,6;3;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,3,0.0,yes,9/25/19,Massachusetts Institute of Technology;Facebook,2;-1,5;-1,5
4193,4193,4193,4193,4193,4193,4193,4193,ICLR,2020,End-to-end learning of energy-based representations for irregularly-sampled signals and images,Ronan Fablet;Lucas Drumetz;François Rousseau,ronan.fablet@imt-atlantique.fr;lucas.drumetz@imt-atlantique.fr;francois.rousseau@imt-atlantique.fr,3;1;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,IMT Atlantique;IMT Atlantique;IMT Atlantique,481;481;481,393;393;393,11
4194,4194,4194,4194,4194,4194,4194,4194,ICLR,2020,Reparameterized Variational Divergence Minimization for Stable Imitation,Dilip Arumugam;Debadeepta Dey;Alekh Agarwal;Asli Celikyilmaz;Elnaz Nouri;Eric Horvitz;Bill Dolan,dilip@cs.stanford.edu;dedey@microsoft.com;alekha@microsoft.com;aslicel@microsoft.com;elnouri@microsoft.com;horvitz@microsoft.com;billdol@microsoft.com,1;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,9,0.0,yes,9/25/19,Stanford University;Microsoft;Microsoft;Microsoft;Microsoft;Microsoft;Microsoft,4;-1;-1;-1;-1;-1;-1,4;-1;-1;-1;-1;-1;-1,5;4
4195,4195,4195,4195,4195,4195,4195,4195,ICLR,2020,TrojanNet: Exposing the Danger of Trojan Horse Attack on Neural Networks,Chuan Guo;Ruihan Wu;Kilian Q. Weinberger,cg563@cornell.edu;rw565@cornell.edu;kqw4@cornell.edu,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,1,0.0,yes,9/25/19,Cornell University;Cornell University;Cornell University,7;7;7,19;19;19,4;1
4196,4196,4196,4196,4196,4196,4196,4196,ICLR,2020,SRDGAN: learning the noise prior for Super Resolution with Dual Generative Adversarial Networks,Jingwei GUAN;Cheng PAN;Songnan LI;Dahai YU,jwguan37@gmail.com;pancheng@tcl.com;lisn@tcl.com;dahai.yu@tcl.com,3;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,;Tcl;Tcl;Tcl,-1;-1;-1;-1,-1;-1;-1;-1,5;4
4197,4197,4197,4197,4197,4197,4197,4197,ICLR,2020,Detecting malicious PDF using CNN,Raphael Fettaya;Yishay Mansour,raphaelfettaya@gmail.com;mansour.yishay@gmail.com,1;1;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,1,0.0,yes,9/25/19,Tel Aviv University;Tel Aviv University,35;35,188;188,
4198,4198,4198,4198,4198,4198,4198,4198,ICLR,2020,Novelty Search in representational space for sample efficient exploration,Ruo Yu Tao;Vincent François-Lavet;Joelle Pineau,ruo.tao@mail.mcgill.ca;vincent.francois-lavet@mail.mcgill.ca;jpineau@cs.mcgill.ca,1;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,McGill University;McGill University;McGill University,86;86;86,42;42;42,
4199,4199,4199,4199,4199,4199,4199,4199,ICLR,2020,"Feature-Robustness, Flatness and Generalization Error for Deep Neural Networks",Henning Petzka;Linara Adilova;Michael Kamp;Cristian Sminchisescu,henning.petzka@gmail.com;adylova.linara.r@gmail.com;info@michaelkamp.org;cristian.sminchisescu@math.lth.se,1;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,8,1.0,yes,9/25/19,Lund University;;Monash University;Lund University,390;-1;118;390,98;-1;75;98,8
4200,4200,4200,4200,4200,4200,4200,4200,ICLR,2020,Robust Cross-lingual Embeddings from Parallel Sentences ,Ali Sabet;Prakhar Gupta;Jean-Baptiste Cordonnier;Robert West;Martin Jaggi,asabet@uwaterloo.ca;prakhar.gupta@epfl.ch;jean-baptiste.cordonnier@epfl.ch;robert.west@epfl.ch;martin.jaggi@epfl.ch,3;3;8,I have published in this field for several years.:I carefully checked the experiments.:N/A:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,4,0.0,yes,9/25/19,University of Waterloo;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne,28;481;481;481;481,235;38;38;38;38,3;6
4201,4201,4201,4201,4201,4201,4201,4201,ICLR,2020,PowerSGD: Powered Stochastic Gradient Descent Methods for Accelerated Non-Convex Optimization,Jun Liu;Beitong Zhou;Weigao Sun;Ruijuan Chen;Claire J. Tomlin;Ye Yuan,j.liu@uwaterloo.ca;zhoubt@hust.edu.cn;sunweigao@outlook.com;ruijuanchen@hust.edu.cn;tomlin@eecs.berkeley.edu;yye@hust.edu.cn,3;3;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,5,0.0,yes,9/25/19,University of Waterloo;Hong Kong University of Science and Technology;;Hong Kong University of Science and Technology;University of California Berkeley;Hong Kong University of Science and Technology,28;39;-1;39;5;39,235;47;-1;47;13;47,9;8
4202,4202,4202,4202,4202,4202,4202,4202,ICLR,2020,Unaligned Image-to-Sequence Transformation with Loop Consistency,Siyang Wang;Justin Lazarow;Kwonjoon Lee;Zhuowen Tu,siw030@ucsd.edu;jlazarow@ucsd.edu;kwl042@ucsd.edu;ztu@ucsd.edu,1;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:N/A:N/A;I have published one or two papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,0,0.0,yes,9/25/19,"University of California, San Diego;University of California, San Diego;University of California, San Diego;University of California, San Diego",11;11;11;11,31;31;31;31,
4203,4203,4203,4203,4203,4203,4203,4203,ICLR,2020,Progressive Compressed Records: Taking a Byte Out of Deep Learning Data,Michael Kuchnik;George Amvrosiadis;Virginia Smith,mkuchnik@andrew.cmu.edu;gamvrosi@cmu.edu;smithv@cmu.edu,3;6;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,5,0.0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1,27;27;27,
4204,4204,4204,4204,4204,4204,4204,4204,ICLR,2020,Training a Constrained Natural Media Painting Agent using Reinforcement Learning ,Biao Jia;Jonathan Brandt;Radomir Mech;Ning Xu;Byungmoon Kim;Dinesh Manocha,biao@cs.umd.edu;jbrandt@adobe.com;rmech@adobe.com;nxu@adobe.com;bmkim@adobe.com;dm@cs.umd.edu,3;1;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,0,0.0,yes,9/25/19,"University of Maryland, College Park;Adobe Systems;Adobe Systems;Adobe Systems;Adobe Systems;University of Maryland, College Park",12;-1;-1;-1;-1;12,91;-1;-1;-1;-1;91,
4205,4205,4205,4205,4205,4205,4205,4205,ICLR,2020,Disentangling Improves VAEs' Robustness to Adversarial Attacks,Matthew Willetts;Alexander Camuto;Stephen Roberts;Chris Holmes,mwilletts@turing.ac.uk;acamuto@turing.ac.uk;sroberts@turing.ac.uk;cholmes@turing.ac.uk,3;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,1.0,yes,9/25/19,Alan Turing Institute;Alan Turing Institute;Alan Turing Institute;Alan Turing Institute,-1;-1;-1;-1,-1;-1;-1;-1,5;4
4206,4206,4206,4206,4206,4206,4206,4206,ICLR,2020,Exploring Cellular Protein Localization Through Semantic Image Synthesis,Daniel Li;Qiang Ma;Andrew Liu;Justin Cheung;Dana Pe’er;Itsik Pe’er,daniel.li@columbia.edu;ma.qiang@columbia.edu;andrew@ml.berkeley.edu;justin.cheung@stonybrookmedicine.edu;peerster@gmail.com;itsik@cs.columbia.edu,3;3;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,6,0.0,yes,9/25/19,Columbia University;Columbia University;University of California Berkeley;Renaissance School of Medicine at Stony Brook University;;Columbia University,15;15;5;41;-1;15,16;16;13;304;-1;16,5;2
4207,4207,4207,4207,4207,4207,4207,4207,ICLR,2020,Encoder-Agnostic Adaptation for Conditional Language Generation,Zachary M. Ziegler;Luke Melas-Kyriazi;Sebastian Gehrmann;Alexander M. Rush,zziegler@g.harvard.edu;lmelaskyriazi@college.harvard.edu;gehrmann@seas.harvard.edu;srush@seas.harvard.edu,8;8;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Harvard University;Harvard University;Harvard University;Harvard University,39;39;39;39,7;7;7;7,3
4208,4208,4208,4208,4208,4208,4208,4208,ICLR,2020,Better Knowledge Retention through Metric Learning,Ke Li*;Shichong Peng*;Kailas Vodrahalli*;Jitendra Malik,ke.li@eecs.berkeley.edu;shichong.peng@mail.utoronto.ca;kailasv@berkeley.edu;malik@eecs.berkeley.edu,3;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,6,0.0,yes,9/25/19,University of California Berkeley;Toronto University;University of California Berkeley;University of California Berkeley,5;18;5;5,13;18;13;13,
4209,4209,4209,4209,4209,4209,4209,4209,ICLR,2020,Robust Learning with Jacobian Regularization,Judy Hoffman;Daniel A. Roberts;Sho Yaida,judy@gatech.edu;dan@diffeo.com;shoyaida@fb.com,6;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Georgia Institute of Technology;Diffeo;Facebook,13;-1;-1,38;-1;-1,4;8
4210,4210,4210,4210,4210,4210,4210,4210,ICLR,2020,CAN ALTQ LEARN FASTER: EXPERIMENTS AND THEORY,Bowen Weng;Huaqing Xiong;Yingbin Liang;Wei Zhang,weng.172@buckeyemail.osu.edu;xiong.309@buckeyemail.osu.edu;liang.889@osu.edu;zhangw3@sustech.edu.cn,3;3;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Ohio State University;Ohio State University;Ohio State University;Tsinghua University,77;77;77;8,373;373;373;23,9
4211,4211,4211,4211,4211,4211,4211,4211,ICLR,2020,A NEW POINTWISE CONVOLUTION IN DEEP NEURAL NETWORKS THROUGH EXTREMELY FAST AND NON PARAMETRIC TRANSFORMS,Joonhyun Jeong;Sung-Ho Bae,doublejtoh@khu.ac.kr;shbae@khu.ac.kr,3;8;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Kyung Hee University;Kyung Hee University,481;481,319;319,
4212,4212,4212,4212,4212,4212,4212,4212,ICLR,2020,"Making the Shoe Fit: Architectures, Initializations, and Tuning for Learning with Privacy",Nicolas Papernot;Steve Chien;Shuang Song;Abhradeep Thakurta;Ulfar Erlingsson,papernot@google.com;schien@google.com;athakurta@google.com;shuangsong@google.com;ulfar@google.com,6;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,7,0.0,yes,9/25/19,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,
4213,4213,4213,4213,4213,4213,4213,4213,ICLR,2020,Toward Understanding Generalization of Over-parameterized Deep ReLU network trained with SGD in Student-teacher Setting,Yuandong Tian,yuandong.tian@gmail.com,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0.0,yes,9/25/19,Facebook,-1,-1,1;8
4214,4214,4214,4214,4214,4214,4214,4214,ICLR,2020,Ordinary differential equations on graph networks,Juntang Zhuang;Nicha Dvornek;Xiaoxiao Li;James S. Duncan,j.zhuang@yale.edu;nicha.dvornek@yale.edu;xiaoxiao.li@yale.edu;james.duncan@yale.edu,1;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,13,0.0,yes,9/25/19,Yale University;Yale University;Yale University;Yale University,64;64;64;64,8;8;8;8,10
4215,4215,4215,4215,4215,4215,4215,4215,ICLR,2020,Poincaré Wasserstein Autoencoder,Ivan Ovinnikov,ivan.ovinnikov@inf.ethz.ch,3;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Swiss Federal Institute of Technology,10,13,10
4216,4216,4216,4216,4216,4216,4216,4216,ICLR,2020,Learning Neural Surrogate Model for Warm-Starting Bayesian Optimization,Haotian Zhang;Jian Sun;Zongben Xu,zht570795275@stu.xjtu.edu.cn;jiansun@xjtu.edu.cn;zbxu@xjtu.edu.cn,3;3;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,3,0.0,yes,9/25/19,Xi'an Jiaotong University;Xi'an Jiaotong University;Xi'an Jiaotong University,481;481;481,555;555;555,11
4217,4217,4217,4217,4217,4217,4217,4217,ICLR,2020,Music Source Separation in the Waveform Domain,Alexandre Defossez;Nicolas Usunier;Leon Bottou;Francis Bach,defossez@fb.com;usunier@fb.com;leonb@fb.com;francis.bach@inria.fr,8;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0.0,yes,9/25/19,Facebook;Facebook;Facebook;INRIA,-1;-1;-1;-1,-1;-1;-1;-1,
4218,4218,4218,4218,4218,4218,4218,4218,ICLR,2020,Self-Attentional Credit Assignment for Transfer in Reinforcement Learning,Johan Ferret;Raphaël Marinier;Matthieu Geist;Olivier Pietquin,jferret@google.com;raphaelm@google.com;mfgeist@google.com;pietquin@google.com,8;3;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,1,5,0.0,yes,9/25/19,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,6
4219,4219,4219,4219,4219,4219,4219,4219,ICLR,2020,Gradient Descent can Learn Less Over-parameterized Two-layer Neural Networks on Classification Problems,Atsushi Nitanda;Geoffrey Chinot;Taiji Suzuki,nitanda@mist.i.u-tokyo.ac.jp;geoffreychinot@gmail.com;taiji@mist.i.u-tokyo.ac.jp,8;3;3,I do not know much about this area.:N/A:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:N/A:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,The University of Tokyo;;The University of Tokyo,56;-1;56,36;-1;36,9;8
4220,4220,4220,4220,4220,4220,4220,4220,ICLR,2020,Learning to Discretize: Solving 1D Scalar Conservation Laws via Deep Reinforcement Learning,Yufei Wang*;Ziju Shen*;Zichao Long;Bin Dong,wang.yufei@pku.edu.cn;zjshen@pku.edu.cn;zlong@pku.edu.cn;dongbin@math.pku.edu.cn,3;3;6,I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Peking University;Peking University;Peking University;Peking University,22;22;22;22,24;24;24;24,3;1;2;6
4221,4221,4221,4221,4221,4221,4221,4221,ICLR,2020,Discovering the compositional structure of vector representations with Role Learning Networks,Paul Soulos;Tom McCoy;Tal Linzen;Paul Smolensky,psoulos1@jhu.edu;tom.mccoy@jhu.edu;tal.linzen@jhu.edu;paul.smolensky@gmail.com,6;3;6,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Johns Hopkins University;Johns Hopkins University;Johns Hopkins University;Microsoft,73;73;73;-1,12;12;12;-1,
4222,4222,4222,4222,4222,4222,4222,4222,ICLR,2020,A Graph Neural Network Assisted Monte Carlo Tree Search Approach to Traveling Salesman Problem,Zhihao Xing;Shikui Tu,xingzhihao@sjtu.edu.cn;tushikui@sjtu.edu.cn,6;6;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Shanghai Jiao Tong University;Shanghai Jiao Tong University,53;53,157;157,10
4223,4223,4223,4223,4223,4223,4223,4223,ICLR,2020,A bi-diffusion based layer-wise sampling method for deep learning in large graphs,Yu He;Shiyang Wen;Wenjin Wu;Yan Zhang;Siran Yang;Yuan Wei;Di Zhang;Guojie  Song;Wei Lin;Liang Wang;Bo Zheng,herve.hy@alibaba-inc.com;shiyang.wsy@alibaba-inc.com;kevin.wwj@alibaba-inc.com;zy143424@alibaba-inc.com;siran.ysr@alibaba-inc.com;yuanxi.wy@alibaba-inc.com;di.zhangd@alibaba-inc.com;gjsong@pku.edu.cn;yangkun.lw@alibaba-inc.com;liangbo.wl@alibaba-inc.com;bozheng@alibaba-inc.com,6;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Alibaba Group;Alibaba Group;Alibaba Group;Alibaba Group;Alibaba Group;Alibaba Group;Alibaba Group;Peking University;Alibaba Group;Alibaba Group;Alibaba Group,-1;-1;-1;-1;-1;-1;-1;22;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;24;-1;-1;-1,10
4224,4224,4224,4224,4224,4224,4224,4224,ICLR,2020,Noise Regularization for Conditional Density Estimation,Jonas Rothfuss;Fabio Ferreira;Simon Boehm;Simon Walther;Maxim Ulrich;Tamim Asfour;Andreas Krause,jonas.rothfuss@gmail.com;fabioferreira@mailbox.org;simonboehm@gmx.de;simon.walther@kit.edu;maxim.ulrich@kit.edu;asfour@kit.edu;krausea@ethz.ch,6;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,University of California Berkeley;Stanford University;Swiss Federal Institute of Technology;Karlsruhe Institute of Technology;Karlsruhe Institute of Technology;Karlsruhe Institute of Technology;Swiss Federal Institute of Technology,5;4;10;154;154;154;10,13;4;13;174;174;174;13,1
4225,4225,4225,4225,4225,4225,4225,4225,ICLR,2020,Learning with Protection: Rejection of Suspicious Samples under Adversarial Environment,Masahiro Kato;Yoshihiro Fukuhara;Hirokatsu Kataoka;Shigeo Morishima,mkato.csecon@gmail.com;gatheluck@gmail.com;hirokatsu.kataoka@aist.go.jp;shigeo@waseda.jp,3;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,RIKEN;Waseda University;AIST;Waseda University,-1;266;-1;266,-1;790;-1;790,4
4226,4226,4226,4226,4226,4226,4226,4226,ICLR,2020,Transition Based Dependency Parser for Amharic Language Using Deep Learning,Mizanu Zelalem;Million Meshesha (PhD),mizatmymail@gmail.com;meshe84@gmail.com,1;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Shandong University;,154;-1,658;-1,
4227,4227,4227,4227,4227,4227,4227,4227,ICLR,2020,V1Net: A computational model of cortical horizontal connections,Vijay Veerabadran;Virginia R. de Sa,vveeraba@ucsd.edu;desa@ucsd.edu,1;1;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,"University of California, San Diego;University of California, San Diego",11;11,31;31,2
4228,4228,4228,4228,4228,4228,4228,4228,ICLR,2020,HOW IMPORTANT ARE NETWORK WEIGHTS? TO WHAT EXTENT DO THEY NEED AN UPDATE?,Fawaz Sammani;Mahmoud Elsayed;Abdelsalam Hamdi,fawaz.sammani@aol.com;elsayedmahmoud@aol.com;abdelsalam.h.a.a@gmail.com,1;3;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,3,0.0,yes,9/25/19,Multimedia University;Multimedia University;,481;481;-1,1285;1285;-1,
4229,4229,4229,4229,4229,4229,4229,4229,ICLR,2020,INTERNAL-CONSISTENCY CONSTRAINTS FOR EMERGENT COMMUNICATION,Charles Lovering;Ellie Pavlick,charles_lovering@brown.edu;ellie_pavlick@brown.edu,6;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Brown University;Brown University,67;67,53;53,
4230,4230,4230,4230,4230,4230,4230,4230,ICLR,2020,Not All Features Are Equal: Feature Leveling Deep Neural Networks for Better Interpretation,Yingjing Lu;Runde Yang,yingjinl@andrew.cmu.edu;ry82@cornell.edu,1;3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Carnegie Mellon University;Cornell University,1;7,27;19,
4231,4231,4231,4231,4231,4231,4231,4231,ICLR,2020,Neural Networks for Principal Component Analysis: A New Loss Function Provably Yields Ordered Exact Eigenvectors ,Reza Oftadeh;Jiayi Shen;Zhangyang Wang;Dylan Shell,oftadeh.reza@gmail.com;asjyjya-617@tamu.edu;atlaswang@tamu.edu;dshell@tamu.edu,6;6;3,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Texas A&M;Texas A&M;Texas A&M;Texas A&M,44;44;44;44,177;177;177;177,1
4232,4232,4232,4232,4232,4232,4232,4232,ICLR,2020,GENN: Predicting Correlated Drug-drug Interactions with Graph Energy Neural Networks,Tengfei Ma;Junyuan Shang;Cao Xiao;Jimeng Sun,tengfei.ma1@ibm.com;sjy1203@pku.edu.cn;cao.xiao@iqvia.com;sun@cc.gatech.edu,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,International Business Machines;Peking University;IQVIA;Georgia Institute of Technology,-1;22;-1;13,-1;24;-1;38,10
4233,4233,4233,4233,4233,4233,4233,4233,ICLR,2020,Efficient Systolic Array Based on Decomposable MAC for Quantized Deep Neural Networks,Ning-Chi Huang;Huan-Jan Chou;Kai-Chiang Wu,nchuang@cs.nctu.edu.tw;kulugu2.cs07g@nctu.edu.tw;kcw@cs.nctu.edu.tw,1;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,National Chiao Tung University;National Chiao Tung University;National Chiao Tung University,143;143;143,564;564;564,
4234,4234,4234,4234,4234,4234,4234,4234,ICLR,2020,DeepEnFM: Deep neural networks with Encoder enhanced Factorization Machine,Qiang Sun;Zhinan Cheng;Yanwei Fu;Wenxuan Wang;Yu-Gang Jiang;Xiangyang Xue,sunqiang85@gmail.com;zhinancheng.bryan@gmail.com;yanweifu@fudan.edu.cn;wxwang.iris@gmail.com;ygj@fudan.edu.cn;xyxue@fudan.edu.cn,1;3;1,I do not know much about this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Fudan University;Fudan University;Fudan University;Fudan University;Fudan University;Fudan University,79;79;79;79;79;79,109;109;109;109;109;109,
4235,4235,4235,4235,4235,4235,4235,4235,ICLR,2020,Statistically Consistent Saliency Estimation,Emre Barut;Shunyan Luo,barut@gwu.edu;shine_lsy@gwu.edu,8;8;3;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,George Washington University;George Washington University,205;205,198;198,1;2
4236,4236,4236,4236,4236,4236,4236,4236,ICLR,2020,Towards Physics-informed Deep Learning for Turbulent Flow Prediction,Rui Wang;Karthik Kashinath;Mustafa Mustafa;Adrian Albert;Rose Yu,wang.rui4@husky.neu.edu;kkashinath@lbl.gov;mmustafa@lbl.gov;aalbert@lbl.gov;roseyu@northeastern.edu,6;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,3,7,0.0,yes,9/25/19,Northeastern University;Lawrence Berkeley National Lab;Lawrence Berkeley National Lab;Lawrence Berkeley National Lab;Northeastern University,16;-1;-1;-1;16,906;-1;-1;-1;906,
4237,4237,4237,4237,4237,4237,4237,4237,ICLR,2020,Domain-Agnostic Few-Shot Classification by Learning Disparate Modulators,Yongseok Choi;Junyoung Park;Subin Yi;Dong-Yeon Cho,yschoi@sktbrain.com;jypark@sktbrain.com;yisubin@sktbrain.com;dycho24@sktbrain.com,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,SK T-Brain;SK T-Brain;SK T-Brain;SK T-Brain,-1;-1;-1;-1,-1;-1;-1;-1,6
4238,4238,4238,4238,4238,4238,4238,4238,ICLR,2020,Online Meta-Critic Learning for Off-Policy Actor-Critic Methods,Wei Zhou;Yiying Li;Yongxin Yang;Huaimin Wang;Timothy M. Hospedales,zhouwei14@nudt.edu.cn;liyiying10@nudt.edu.cn;yongxin.yang@ed.ac.uk;hmwang@nudt.edu.cn;t.hospedales@ed.ac.uk,3;3;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,National University of Defense Technology;National University of Defense Technology;University of Edinburgh;National University of Defense Technology;University of Edinburgh,-1;-1;33;-1;33,-1;-1;30;-1;30,6
4239,4239,4239,4239,4239,4239,4239,4239,ICLR,2020,Using Explainabilty to Detect Adversarial Attacks,Ohad Amosy;Gal Chechik,amosy3@gmail.com;gal.chechik@gmail.com,3;3;1;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,2,0,0.0,yes,9/25/19,Bar Ilan University;,95;-1,513;-1,4
4240,4240,4240,4240,4240,4240,4240,4240,ICLR,2020,Random Bias Initialization Improving Binary Neural Network Training,Xinlin Li;Vahid Partovi Nia,xinlin.li1@huawei.com;vahid.partovinia@huawei.com,1;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Huawei Technologies Ltd.;Huawei Technologies Ltd.,-1;-1,-1;-1,
4241,4241,4241,4241,4241,4241,4241,4241,ICLR,2020,Bayesian Inference for Large Scale Image Classification,Jonathan Heek;Nal Kalchbrenner,jheek@google.com;nalk@google.com,6;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,3,0.0,yes,9/25/19,Google;Google,-1;-1,-1;-1,11;7
4242,4242,4242,4242,4242,4242,4242,4242,ICLR,2020,Self-Supervised GAN Compression,Chong Yu;Jeff Pool,chongy@nvidia.com;jpool@nvidia.com,3;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:N/A:N/A,Reject,0,4,0.0,yes,9/25/19,NVIDIA;NVIDIA,-1;-1,-1;-1,3;4;5
4243,4243,4243,4243,4243,4243,4243,4243,ICLR,2020,"Generative Hierarchical Models for Parts, Objects, and Scenes",Fei Deng;Zhuo Zhi;Sungjin Ahn,fei.deng@rutgers.edu;zhizz001@stu.xjtu.edu.cn;sjn.ahn@gmail.com,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,Rutgers University;Xi'an Jiaotong University;Rutgers University,34;481;34,168;555;168,5
4244,4244,4244,4244,4244,4244,4244,4244,ICLR,2020,SDGM: Sparse Bayesian Classifier Based on a Discriminative Gaussian Mixture Model,Hideaki Hayashi;Seiichi Uchida,hayashi@ait.kyushu-u.ac.jp;uchida@ait.kyushu-u.ac.jp,3;3;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Meiji University;Meiji University,481;481,332;332,11;8
4245,4245,4245,4245,4245,4245,4245,4245,ICLR,2020,Analytical Moment Regularizer for Training Robust Networks,Modar Alfadly;Adel Bibi;Muhammed Kocabas;Bernard Ghanem,modar.alfadly@kaust.edu.sa;adel.bibi@kaust.edu.sa;muhammed.kocabas@tue.mpg.de;bernard.ghanem@kaust.edu.sa,3;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,3,0.0,yes,9/25/19,"KAUST;KAUST;Max Planck Institute for Intelligent Systems, Max-Planck Institute;KAUST",128;128;-1;128,1397;1397;-1;1397,4
4246,4246,4246,4246,4246,4246,4246,4246,ICLR,2020,ROBUST DISCRIMINATIVE REPRESENTATION LEARNING VIA GRADIENT RESCALING: AN EMPHASIS REGULARISATION PERSPECTIVE,Xinshao Wang;Yang Hua;Elyor Kodirov;Neil M. Robertson,xwang39@qub.ac.uk;y.hua@qub.ac.uk;elyor@anyvision.co;n.robertson@qub.ac.uk,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,7,1.0,yes,9/25/19,Queen's University Belfast;Queen's University Belfast;Anyvision;Queen's University Belfast,266;266;-1;266,204;204;-1;204,
4247,4247,4247,4247,4247,4247,4247,4247,ICLR,2020,Multi-step Greedy Policies in Model-Free Deep Reinforcement Learning,Yonathan Efroni;Manan Tomar;Mohammad Ghavamzadeh,jonathan.efroni@gmail.com;manan.tomar@gmail.com;mgh@fb.com,3;3;6,I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,8,0.0,yes,9/25/19,Technion;Indian Institute of Technology Madras;Facebook,26;154;-1,412;641;-1,
4248,4248,4248,4248,4248,4248,4248,4248,ICLR,2020,Hidden incentives for self-induced distributional shift,David Scott Krueger;Tegan Maharaj;Shane Legg;Jan Leike,davidscottkrueger@gmail.com;tegan.jrm@gmail.com;legg@google.com;leike@google.com,6;1;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,University of Montreal;Polytechnique Montreal;Google;Google,128;390;-1;-1,85;1397;-1;-1,6
4249,4249,4249,4249,4249,4249,4249,4249,ICLR,2020,Learning Temporal Abstraction with Information-theoretic Constraints for Hierarchical Reinforcement Learning,Wenshan Wang;Yaoyu Hu;Sebastian Scherer,wenshanw@andrew.cmu.edu;yaoyuh@andrew.cmu.edu;basti@andrew.cmu.edu,3;1;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,12,0.0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1,27;27;27,9
4250,4250,4250,4250,4250,4250,4250,4250,ICLR,2020,Generalized Clustering by Learning to Optimize Expected Normalized Cuts,Azade Nazi;Will Hang;Anna Goldie;Sujith Ravi;Azalia Mirhoseini,azade@google.com;agoldie@google.com;sravi@google.com;azalia@google.com;willhang@stanford.edu,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Google;Google;Google;Google;Stanford University,-1;-1;-1;-1;4,-1;-1;-1;-1;4,8
4251,4251,4251,4251,4251,4251,4251,4251,ICLR,2020,Adversarially Robust Neural Networks via Optimal Control: Bridging Robustness with Lyapunov Stability,Zhiyang Chen;Hang Su,zy-chen17@mails.tsinghua.edu.cn;suhangss@mail.tsinghua.edu.cn,1;6;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,2,0,0.0,yes,9/25/19,Tsinghua University;Tsinghua University,8;8,23;23,4
4252,4252,4252,4252,4252,4252,4252,4252,ICLR,2020,Temporal Difference Weighted Ensemble For Reinforcement Learning,Takuma Seno;Michita Imai,seno@ailab.ics.keio.ac.jp;michita@ailab.ics.keio.ac.jp,1;3;8,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,6,0.0,yes,9/25/19,Meiji University;Meiji University,481;481,332;332,
4253,4253,4253,4253,4253,4253,4253,4253,ICLR,2020,Learning Generative Image Object Manipulations from Language Instructions,Martin Längkvist;Andreas Persson;Amy Loutfi,martin.langkvist@oru.se;andreas.persson@oru.se;amy.loutfi@oru.se,3;3;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,0,0.0,yes,9/25/19,Centre for Applied Autonomous Sensor Systems;Centre for Applied Autonomous Sensor Systems;Centre for Applied Autonomous Sensor Systems,-1;-1;-1,-1;-1;-1,3;4;5
4254,4254,4254,4254,4254,4254,4254,4254,ICLR,2020,A Simple Technique to Enable Saliency Methods to Pass the Sanity Checks,Arushi Gupta;Sanjeev Arora,arushig@princeton.edu;arora@cs.princeton.edu,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Princeton University;Princeton University,31;31,6;6,
4255,4255,4255,4255,4255,4255,4255,4255,ICLR,2020,Impact of the latent space on the ability of GANs to fit the distribution,Thomas Pinetz;Daniel Soukup;Thomas Pock,thomas.pinetz@ait.ac.at;daniel.soukup@ait.ac.at;pock@icg.tugraz.at,1;1;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,;;Graz University of Technology,-1;-1;108,-1;-1;542,5;4;1
4256,4256,4256,4256,4256,4256,4256,4256,ICLR,2020,At Your Fingertips: Automatic Piano Fingering Detection,Amit Moryossef;Yanai Elazar;Yoav Goldberg,amitmoryossef@gmail.com;yanaiela@gmail.com;yoav.goldberg@gmail.com,1;3;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:N/A:N/A;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,2,0.0,yes,9/25/19,Bar Ilan University;Bar Ilan University;Bar-Ilan University,95;95;95,513;513;513,5;4
4257,4257,4257,4257,4257,4257,4257,4257,ICLR,2020,Latent Question Reformulation and Information Accumulation for Multi-Hop Machine Reading,Quentin Grail;Julien Perez;Eric Gaussier,quentin.grail@naverlabs.com;julien.perez@naverlabs.com;eric.gaussier@imag.fr,8;3;3,I have published one or two papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,4,0.0,yes,9/25/19,Naver Labs Europe;Naver Labs Europe;Imag Montpellier Université,-1;-1;-1,-1;-1;-1,3
4258,4258,4258,4258,4258,4258,4258,4258,ICLR,2020,Deep Variational Semi-Supervised Novelty Detection,Tal Daniel;Thanard Kurutach;Aviv Tamar,taldanielm@campus.technion.ac.il;thanard.kurutach@berkeley.edu;avivt@technion.ac.il,6;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Technion;University of California Berkeley;Technion,26;5;26,412;13;412,5
4259,4259,4259,4259,4259,4259,4259,4259,ICLR,2020,Occlusion  resistant  learning  of  intuitive physics from videos,Ronan Riochet;Josef Sivic;Ivan Laptev;Emmanuel Dupoux,ronan.riochet@inria.fr;josef.sivic@ens.fr;ivan.laptev@inria.fr;emmanuel.dupoux@gmail.com,3;6;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,4,0.0,yes,9/25/19,INRIA;Ecole Normale Superieure;INRIA;,-1;100;-1;-1,-1;45;-1;-1,2
4260,4260,4260,4260,4260,4260,4260,4260,ICLR,2020,"Compressive Recovery Defense: A Defense Framework for $\ell_0, \ell_2$ and $\ell_\infty$ norm attacks.",Jasjeet Dhaliwal;Kyle Hambrook,jasjeet.dhaliwal@sjsu.edu;kyle.hambrook@sjsu.edu,3;3;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY,481;481,299;299,4
4261,4261,4261,4261,4261,4261,4261,4261,ICLR,2020,Causally Correct Partial Models for Reinforcement Learning,Danilo J. Rezende;Ivo Danihelka;George Papamakarios;Nan Rosemary Ke;Ray Jiang;Theophane Weber;Karol Gregor;Hamza Merzic;Fabio Viola;Jane Wang;Jovana Mitrovic;Frederic Besse;Ioannis Antonoglou;Lars Buesing;Julian Schrittwieser;Thomas Hubert;David Silver,danilor@google.com;danihelka@google.com;gpapamak@google.com;rosemary.nan.ke@gmail.com;rayjiang@google.com;theophane@google.com;karolg@google.com;hamzamerzic@google.com;fviola@google.com;wangjane@google.com;mitrovic@google.com;fbesse@google.com;ioannisa@google.com;lbuesing@google.com;swj@google.com;tkhubert@google.com;davidsilver@google.com,8;1;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Reject,0,7,0.0,yes,9/25/19,Google;Google;Google;Polytechnique Montreal;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;390;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;1397;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,
4262,4262,4262,4262,4262,4262,4262,4262,ICLR,2020,Learning scalable and transferable multi-robot/machine sequential assignment planning via graph embedding,Hyunwook Kang;Aydar Mynbay;James R. Morrison;Jinkyoo Park,hwkang@tamu.edu;aydar.mynbay@bluehole.net;james.morrison@kaist.edu;jinkyoo.park@kaist.ac.kr,3;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,3,0.0,yes,9/25/19,Texas A&M;;KAIST;Korea Advanced Institute of Science and Technology,44;-1;20;481,177;-1;110;110,10
4263,4263,4263,4263,4263,4263,4263,4263,ICLR,2020,Improving Evolutionary Strategies with Generative Neural Networks,Louis Faury;Clément Calauzènes;Olivier Fercoq,l.faury@criteo.com;c.calauzenes@criteo.com;olivier.fercoq@telecom-paris.fr,8;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Criteo;Criteo;Télécom Paris,-1;-1;481,-1;-1;187,5
4264,4264,4264,4264,4264,4264,4264,4264,ICLR,2020,Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning,Mitchell A Gordon;Kevin Duh;Nicholas Andrews,mgordo37@jhu.edu;kevinduh@cs.jhu.edu;noa@jhu.edu,6;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,3,0.0,yes,9/25/19,Johns Hopkins University;Johns Hopkins University;Johns Hopkins University,73;73;73,12;12;12,3;6;2
4265,4265,4265,4265,4265,4265,4265,4265,ICLR,2020,REFINING MONTE CARLO TREE SEARCH AGENTS BY MONTE CARLO TREE SEARCH,Katsuki Ohto,katsuki.ohto@gmail.com,1;1;1,I have published in this field for several years.:N/A:N/A:N/A;I have published in this field for several years.:I did not assess the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have read many papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,3,0.0,yes,9/25/19,,,,
4266,4266,4266,4266,4266,4266,4266,4266,ICLR,2020,Smooth Kernels Improve Adversarial Robustness and Perceptually-Aligned Gradients,Haohan Wang;Xindi Wu;Songwei Ge;Zachary C. Lipton;Eric P. Xing,haohanw@cs.cmu.edu;xindiw@andrew.cmu.edu;songweig@andrew.cmu.edu;zlipton@cmu.edu;epxing@cs.cmu.edu,1;1;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1;1,27;27;27;27;27,4
4267,4267,4267,4267,4267,4267,4267,4267,ICLR,2020,Modeling treatment events in disease progression,Guanyang Wang;Yumeng Zhang;Yong Deng;Xuxin Huang;Lukasz Kidzinski,guanyang@stanford.edu;zym3008@gmail.com;yongdeng@stanford.edu;xxhuang@stanford.edu;lukasz.kidzinski@stanford.edu,1;1;1,I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Stanford University;;Stanford University;Stanford University;Stanford University,4;-1;4;4;4,4;-1;4;4;4,
4268,4268,4268,4268,4268,4268,4268,4268,ICLR,2020,Collaborative Generated Hashing for Market Analysis and Fast Cold-start Recommendation,Yan Zhang;Ivor W. Tsang;Lixin Duan;Guowu Yang,yixianqianzy@gmail.com;ivor.tsang@uts.edu.au;lxduan@gmail.com;guowu@uestc.edu.cn,1;1;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,University of Technology Sydney;University of Technology Sydney;University of Electronic Science and Technology of China;University of Electronic Science and Technology of China,108;108;481;481,193;193;628;628,5
4269,4269,4269,4269,4269,4269,4269,4269,ICLR,2020,Are Few-shot Learning Benchmarks Too Simple ?,Gabriel Huang;Hugo Larochelle;Simon Lacoste-Julien,gbxhuang@gmail.com;hugolarochelle@google.com;slacoste@iro.umontreal.ca,6;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,University of Montreal;Google;University of Montreal,128;-1;128,85;-1;85,6
4270,4270,4270,4270,4270,4270,4270,4270,ICLR,2020,Towards trustworthy predictions from deep neural networks with fast adversarial calibration,Christian Tomani;Florian Buettner,christian.tomani@gmail.com;fbuettner.phys@gmail.com,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,6,0.0,yes,9/25/19,Technical University Munich;Siemens Corporate Research,53;-1,43;-1,4;11
4271,4271,4271,4271,4271,4271,4271,4271,ICLR,2020,Collaborative Training of Balanced Random Forests for Open Set Domain Adaptation,Jongbin Ryu;Jiun Bae;Jongwoo Lim,jongbin.ryu@gmail.com;maybe@hanyang.ac.kr;jlim@hanyang.ac.kr,3;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Hanyang University;Hanyang University;Hanyang University,233;233;233,393;393;393,
4272,4272,4272,4272,4272,4272,4272,4272,ICLR,2020,Accelerated Variance Reduced Stochastic Extragradient Method for Sparse Machine Learning Problems,Fanhua Shang;Lin Kong;Yuanyuan Liu;Hua Huang;Hongying Liu,fhshang@xidian.edu.cn;xdkonglin0511@163.com;yyliu@xidian.edu.cn;huanghua1115@outlook.com;hyliu@xidian.edu.cn,8;1;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Tsinghua University;Xidian University;Tsinghua University;;Tsinghua University,8;481;8;-1;8,23;919;23;-1;23,9;2
4273,4273,4273,4273,4273,4273,4273,4273,ICLR,2020,Convergence Analysis of a Momentum Algorithm with Adaptive Step Size for Nonconvex Optimization,Anas Barakat;Pascal Bianchi,anas.barakat@telecom-paristech.fr;pascal.bianchi@telecom-paristech.fr,3;3;3,I have read many papers in this area.:N/A:N/A:I did not assess the derivations or theory.;I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Télécom ParisTech;Télécom ParisTech,481;481,187;187,1;9
4274,4274,4274,4274,4274,4274,4274,4274,ICLR,2020,RTC-VAE: HARNESSING THE PECULIARITY OF TOTAL CORRELATION  IN LEARNING DISENTANGLED REPRESENTATIONS,Ze Cheng;Juncheng B Li;Chenxu Wang;Jixuan Gu;Hao Xu;Xinjian Li;Florian Metze,ze.cheng@cn.bosch.com;junchenl@cs.cmu.edu;chenxujwang@gmail.com;jixuan.gu@sjtu.edu.cn;hao.xu-1@colorado.edu;xinjianl@cs.cmu.edu;fmetze@cs.cmu.edu,3;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,"Bosch;Carnegie Mellon University;Bosch;Shanghai Jiao Tong University;University of Colorado, Boulder;Carnegie Mellon University;Carnegie Mellon University",-1;1;-1;53;44;1;1,-1;27;-1;157;123;27;27,5;1
4275,4275,4275,4275,4275,4275,4275,4275,ICLR,2020,Generalized Bayesian Posterior Expectation Distillation for Deep Neural Networks,Meet P. Vadera;Benjamin M. Marlin,mvadera@cs.umass.edu;marlin@cs.umass.edu,3;6;6,I have published one or two papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,7,0.0,yes,9/25/19,"University of Massachusetts, Amherst;University of Massachusetts, Amherst",28;28,209;209,11
4276,4276,4276,4276,4276,4276,4276,4276,ICLR,2020,Towards Simplicity in Deep Reinforcement Learning: Streamlined Off-Policy Learning,Che Wang;Yanqiu Wu;Quan Vuong;Keith Ross,cw1681@nyu.edu;yanqiu.wu@nyu.edu;quan.hovuong@gmail.com;keithwross@nyu.edu,6;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,11,0.0,yes,9/25/19,"New York University;New York University;University of California, San Diego;New York University",25;25;11;25,29;29;31;29,
4277,4277,4277,4277,4277,4277,4277,4277,ICLR,2020,A shallow feature extraction network with a large receptive field for stereo matching tasks,Jianguo Liu;Yunjian Feng;Guo Ji;Fuwu Yan,ljg424@163.com;1029515027@whut.edu.cn;18754806756@163.com;yanfw@whut.edu.cn,3;1;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,0,0.0,yes,9/25/19,163;South China University of Technology;163;South China University of Technology,-1;481;-1;481,-1;501;-1;501,2
4278,4278,4278,4278,4278,4278,4278,4278,ICLR,2020,Evaluating Semantic Representations of Source Code,Yaza Wainakh;Moiz Rauf;Michael Pradel,yaza.wainakh@gmail.com;moiz.rauf@iste.uni-stuttgart.de;michael@binaervarianz.de,6;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,;University of Stuttgart;University of Stuttgart,-1;95;95,-1;292;292,3
4279,4279,4279,4279,4279,4279,4279,4279,ICLR,2020,UW-NET: AN INCEPTION-ATTENTION NETWORK FOR UNDERWATER IMAGE CLASSIFICATION,Miao Yang;Ke Hu;Chongyi Li;Zhiqiang Wei,lemonmiao@gmial.com;kexisibest@outlook.com;lichongyi@tju.edu.cn;weizhiqiang@ouc.edu.cn,3;1;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,0,0.0,yes,9/25/19,"Gmial;;Zhejiang University;University of Illinois, Urbana-Champaign",-1;-1;56;3,-1;-1;107;48,
4280,4280,4280,4280,4280,4280,4280,4280,ICLR,2020,Deep Generative Classifier for Out-of-distribution Sample Detection,Dongha Lee;Sehun Yu;Hwanjo Yu,dongha0914@postech.ac.kr;hunu12@postech.ac.kr;hwanjoyu@postech.ac.kr,3;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,1,3,0.0,yes,9/25/19,POSTECH;POSTECH;POSTECH,118;118;118,146;146;146,5
4281,4281,4281,4281,4281,4281,4281,4281,ICLR,2020,Adaptive Loss Scaling for Mixed Precision Training,Ruizhe Zhao;Brian Vogel;Tanvir Ahmed,ruizhe.zhao15@imperial.ac.uk;vogel@preferred.jp;tanvira@preferred.jp,3;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,3,0.0,yes,9/25/19,"Imperial College London;Preferred Networks, Inc.;Preferred Networks, Inc.",73;-1;-1,10;-1;-1,
4282,4282,4282,4282,4282,4282,4282,4282,ICLR,2020,On importance-weighted autoencoders,Axel Finke;Alexandre H. Thiery,axelfinke42@gmail.com;a.h.thiery@nus.edu.sg,8;3;6,I have published one or two papers in this area.:N/A:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,National University of Singapore;National University of Singapore,16;16,25;25,5;1
4283,4283,4283,4283,4283,4283,4283,4283,ICLR,2020,Mixed Precision Training With 8-bit Floating Point,Naveen Mellempudi;Sudarshan Srinivasan;Dipankar Das;Bharat Kaul,naveen.k.mellempudi@intel.com;sudarshan.srinivasan@intel.com;dipankar.das@intel.com;bharat.kaul@intel.com,6;1;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,7,0.0,yes,9/25/19,Intel;Intel;Intel;Intel,-1;-1;-1;-1,-1;-1;-1;-1,8
4284,4284,4284,4284,4284,4284,4284,4284,ICLR,2020,A Functional Characterization of Randomly Initialized Gradient Descent in Deep ReLU Networks,Justin Sahs;Aneel Damaraju;Ryan Pyle;Onur Tavaslioglu;Josue Ortega Caro;Hao Yang Lu;Ankit Patel,justin.sahs@bcm.edu;amd18@rice.edu;ryan.pyle@bcm.edu;onur.tavaslioglu@bcm.edu;josue.ortegacaro@bcm.edu;hl61@rice.edu;ankitp@bcm.edu,3;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Baylor College of Medicine;Rice University;Baylor College of Medicine;Baylor College of Medicine;Baylor College of Medicine;Rice University;Baylor College of Medicine,-1;84;-1;-1;-1;84;-1,-1;105;-1;-1;-1;105;-1,8
4285,4285,4285,4285,4285,4285,4285,4285,ICLR,2020,ASGen: Answer-containing Sentence Generation to Pre-Train Question Generator for Scale-up Data in Question Answering,Akhil Kedia;Sai Chetan Chinthakindi;Seohyun Back;Haejun Lee;Jaegul Choo,akhil.kedia@samsung.com;sai.chetan@samsung.com;scv.back@samsung.com;haejun82.lee@samsung.com;jchoo@korea.ac.kr,6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,9,0.0,yes,9/25/19,Samsung;Samsung;Samsung;Samsung;Korea University,-1;-1;-1;-1;323,-1;-1;-1;-1;179,3
4286,4286,4286,4286,4286,4286,4286,4286,ICLR,2020,Invariance vs Robustness of Neural Networks,Sandesh Kamath;Amit Deshpande;K V Subrahmanyam,amitdesh@microsoft.com;ksandeshk@cmi.ac.in;kv@cmi.ac.in,3;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,0,0.0,yes,9/25/19,Microsoft;Chennai Mathematical Institute;Chennai Mathematical Institute,-1;-1;-1,-1;-1;-1,4;8
4287,4287,4287,4287,4287,4287,4287,4287,ICLR,2020,Modelling the influence of data structure on learning in neural networks,S. Goldt;M. Mézard;F. Krzakala;L. Zdeborová,goldt.sebastian@gmail.com;marc.mezard@gmail.com;florent.krzakala@gmail.com;lenka.zdeborova@gmail.com,1;1;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,"Institut de Physique théorique, CNRS, CEA;Ecole Normale Superieure;Ecole Normale Superieure;CEA",-1;100;100;233,-1;45;45;1027,5
4288,4288,4288,4288,4288,4288,4288,4288,ICLR,2020,A Perturbation Analysis of Input Transformations for Adversarial Attacks,Adam Dziedzic;Sanjay Krishnan,ady@uchicago.edu;skr@uchicago.edu,3;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,University of Chicago;University of Chicago,48;48,9;9,4
4289,4289,4289,4289,4289,4289,4289,4289,ICLR,2020,Understanding Isomorphism Bias in Graph Data Sets ,Ivanov Sergey;Sviridov Sergey;Evgeny Burnaev,ivanovserg990@gmail.com;sergei.sviridov@gmail.com;e.burnaev@skoltech.ru,6;1;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Skolkovo Institute of Science and Technology;;Skolkovo Institute of Science and Technology,-1;-1;-1,-1;-1;-1,10;8
4290,4290,4290,4290,4290,4290,4290,4290,ICLR,2020,Data-Efficient Image Recognition with Contrastive Predictive Coding,Olivier J Henaff;Aravind Srinivas;Jeffrey De Fauw;Ali Razavi;Carl Doersch;S. M. Ali Eslami;Aaron van den Oord,henaff@google.com;aravind@cs.berkeley.edu;defauw@google.com;alirazavi@google.com;doersch@google.com;aeslami@google.com;avdnoord@google.com,3;3;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Google;University of California Berkeley;Google;Google;Google;Google;Google,-1;5;-1;-1;-1;-1;-1,-1;13;-1;-1;-1;-1;-1,2;8
4291,4291,4291,4291,4291,4291,4291,4291,ICLR,2020,OBJECT-ORIENTED REPRESENTATION OF 3D SCENES,Chang Chen;Sungjin Ahn,chang.chen@rutgers.edu;sjn.ahn@gmail.com,3;6;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,9,0.0,yes,9/25/19,Rutgers University;Rutgers University,34;34,168;168,5;8
4292,4292,4292,4292,4292,4292,4292,4292,ICLR,2020,Barcodes as summary of objective functions' topology,Serguei Barannikov;Alexander Korotin;Dmitry Oganesyan;Daniil Emtsev;Evgeny Burnaev,serguei.barannikov@imj-prg.fr;a.korotin@skoltech.ru;d.oganesyan@skoltech.ru;demtsev@student.ethz.ch;e.burnaev@skoltech.ru,1;1;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,"CNRS, Institut Mathematiques de Jussieu, Paris Diderot University;Skolkovo Institute of Science and Technology;Skolkovo Institute of Science and Technology;Swiss Federal Institute of Technology;Skolkovo Institute of Science and Technology",481;-1;-1;10;-1,1397;-1;-1;13;-1,
4293,4293,4293,4293,4293,4293,4293,4293,ICLR,2020,Asynchronous Stochastic Subgradient Methods for General Nonsmooth Nonconvex Optimization,Vyacheslav Kungurtsev;Malcolm Egan;Bapi Chatterjee;Dan Alistarh,vyacheslav.kungurtsev@fel.cvut.cz;malcom.egan@insa-lyon.fr;bapi.chatterjee@ist.ac.at;dan.alistarh@ist.ac.at,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,Czech Technical University in Prague;INSA de Lyon;Institute of Science and Technology Austria;Institute of Science and Technology Austria,323;481;481;481,956;1397;1397;1397,
4294,4294,4294,4294,4294,4294,4294,4294,ICLR,2020,Top-down training for neural networks,Shucong Zhang;Cong-Thanh Do;Rama Doddipatla;Erfan Loweimi;Peter Bell;Steve Renals,s1603602@sms.ed.ac.uk;cong-thanh.do@crl.toshiba.co.uk;rama.doddipatla@crl.toshiba.co.uk;e.loweimi@ed.ac.uk;peter.bell@ed.ac.uk;s.renals@ed.ac.uk,3;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:N/A:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,1,5,0.0,yes,9/25/19,University of Edinburgh;Toshiba Research Europe Ltd.;Toshiba Research Europe Ltd.;University of Edinburgh;University of Edinburgh;University of Edinburgh,33;-1;-1;33;33;33,30;-1;-1;30;30;30,
4295,4295,4295,4295,4295,4295,4295,4295,ICLR,2020,A new perspective in understanding of Adam-Type algorithms and beyond,Zeyi Tao;Qi Xia;Qun Li,ztao@email.wm.edu;qxia01@email.wm.edu;liqun@cs.wm.edu,3;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,College of William and Mary;College of William and Mary;College of William and Mary,154;154;154,235;235;235,1;9;8
4296,4296,4296,4296,4296,4296,4296,4296,ICLR,2020,Subjective Reinforcement Learning for Open Complex Environments,Zhile Yang*;Haichuan Gao*;Xin Su;Shangqi Guo;Feng Chen,yzl18@mails.tsinghua.edu.cn;ghc18@mails.tsinghua.edu.cn;suxin16@mails.tsinghua.edu.cn;gsq15@mails.tsinghua.edu.cn;chenfeng@mail.tsinghua.edu.cn,3;3;1,I do not know much about this area.:N/A:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University,8;8;8;8;8,23;23;23;23;23,
4297,4297,4297,4297,4297,4297,4297,4297,ICLR,2020,Discriminative Variational Autoencoder for Continual Learning with Generative Replay,Woo-Young Kang;Cheol-Ho Han;Byoung-Tak Zhang,rkddndud50@gmail.com;chhan@bi.snu.ac.kr;btzhang@bi.snu.ac.kr,1;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,Kakao Brain;Seoul National University;Seoul National University,-1;41;41,-1;64;64,5
4298,4298,4298,4298,4298,4298,4298,4298,ICLR,2020,BOSH: An Efficient Meta Algorithm for Decision-based Attacks,Zhenxin Xiao;Puyudi Yang;Yuchen Jiang;Kai-Wei Chang;Cho-Jui Hsieh,alanshawzju@gmail.com;pydyang@ucdavis.edu;jyc@zju.edu.cn;kw@kwchang.net;chohsieh@cs.ucla.edu,3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,"Zhejiang University;University of California, Davis;Zhejiang University;University of Virginia Main Campus;University of California, Los Angeles",56;79;56;59;20,107;55;107;107;17,4;11
4299,4299,4299,4299,4299,4299,4299,4299,ICLR,2020,Behavior Regularized Offline Reinforcement Learning,Yifan Wu;George Tucker;Ofir Nachum,yw4@andrew.cmu.edu;gjt@google.com;ofirnachum@google.com,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Carnegie Mellon University;Google;Google,1;-1;-1,27;-1;-1,
4300,4300,4300,4300,4300,4300,4300,4300,ICLR,2020,HyperEmbed:  Tradeoffs Between Resources and Performance in NLP Tasks with Hyperdimensional Computing enabled embedding of n-gram statistics ,Pedro Alonso;Kumar Shridhar;Denis Kleyko;Evgeny Osipov;Marcus Liwicki,pedro.alonso@ltu.se;kumar@neuralspace.ai;denis.kleyko@ltu.se;evgeny.osipov@ltu.se;marcus.liwicki@ltu.se,3;1;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,3,0.0,yes,9/25/19,Luleå University of Technology;TU Kaiserslautern;Luleå University of Technology;Luleå University of Technology;Luleå University of Technology,481;154;481;481;481,1397;601;1397;1397;1397,3
4301,4301,4301,4301,4301,4301,4301,4301,ICLR,2020,Semi-Supervised Learning with Normalizing Flows,Pavel Izmailov;Polina Kirichenko;Marc Finzi;Andrew Wilson,izmailovpavel@gmail.com;pk1822@nyu.edu;maf820@nyu.edu;andrew@cornell.edu,6;1;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,1.0,yes,9/25/19,New York University;New York University;New York University;Cornell University,25;25;25;7,29;29;29;19,
4302,4302,4302,4302,4302,4302,4302,4302,ICLR,2020,Attacking Lifelong Learning Models with Gradient Reversion,Yunhui Guo;Mingrui Liu;Yandong Li;Liqiang Wang;Tianbao Yang;Tajana Rosing,yug185@eng.ucsd.edu;mingrui-liu@uiowa.edu;lyndon.leeseu@outlook.com;lwang@cs.ucf.edu;tianbao-yang@uiowa.edu;tajana@ucsd.edu,3;3;3,I have published one or two papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,"University of California, San Diego;University of Iowa;University of Central Florida;University of Central Florida;University of Iowa;University of California, San Diego",11;154;77;77;154;11,31;227;609;609;227;31,4
4303,4303,4303,4303,4303,4303,4303,4303,ICLR,2020,Deep amortized clustering,Juho Lee;Yoonho Lee;Yee Whye Teh,juho@aitrics.com;einet89@gmail.com;y.w.teh@stats.ox.ac.uk,3;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,AITRICS;;University of Oxford,-1;-1;50,-1;-1;1,
4304,4304,4304,4304,4304,4304,4304,4304,ICLR,2020,Laconic Image Classification: Human vs. Machine Performance,Javier Carrasco;Aidan Hogan;Jorge Pérez,jaco_1031@hotmail.com;aidhog@gmail.com;jorge.perez.rojas@gmail.com,1;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,4,0.0,yes,9/25/19,Universidad de Chile;Universidad de Chile;Universidad de Chile,323;323;323,848;848;848,
4305,4305,4305,4305,4305,4305,4305,4305,ICLR,2020,Understanding and Stabilizing GANs' Training Dynamics with Control Theory,Kun Xu;Chongxuan Li;Huanshu Wei;Jun Zhu;Bo Zhang,kunxu.thu@gmail.com;chongxuanli1991@gmail.com;weihuanshu94@hotmail.com;dcszj@mail.tsinghua.edu.cn;dcszb@mail.tsinghua.edu.cn,6;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,Tsinghua University;Tsinghua University;;Tsinghua University;Tsinghua University,8;8;-1;8;8,23;23;-1;23;23,5;4
4306,4306,4306,4306,4306,4306,4306,4306,ICLR,2020,Benefits of Overparameterization in Single-Layer Latent Variable Generative Models,Rares-Darius Buhai;Andrej Risteski;Yoni Halpern;David Sontag,rbuhai@mit.edu;aristesk@andrew.cmu.edu;yhalpern@google.com;dsontag@csail.mit.edu,3;3;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Massachusetts Institute of Technology;Carnegie Mellon University;Google;Massachusetts Institute of Technology,2;1;-1;2,5;27;-1;5,8
4307,4307,4307,4307,4307,4307,4307,4307,ICLR,2020,Detecting Noisy Training Data with Loss Curves,Geoff Pleiss;Tianyi Zhang;Ethan R. Elenberg;Kilian Q. Weinberger,geoff@cs.cornell.edu;tz58@cornell.edu;eelenberg@asapp.com;kqw4@cornell.edu,3;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Cornell University;Cornell University;ASAPP Inc;Cornell University,7;7;-1;7,19;19;-1;19,8
4308,4308,4308,4308,4308,4308,4308,4308,ICLR,2020,On Concept-Based Explanations in Deep Neural Networks,Chih-Kuan Yeh;Been Kim;Sercan Arik;Chun-Liang Li;Pradeep Ravikumar;Tomas Pfister,cjyeh@cs.cmu.edu;beenkim.mit@gmail.com;soarik@google.com;chunliang.tw@gmail.com;pradeep.ravikumar@gmail.com;tpfister@google.com,6;6;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,9,0.0,yes,9/25/19,Carnegie Mellon University;Google;Google;Google;Carnegie Mellon University;Google,1;-1;-1;-1;1;-1,27;-1;-1;-1;27;-1,
4309,4309,4309,4309,4309,4309,4309,4309,ICLR,2020,Trajectory representation learning for Multi-Task NMRDPs planning,Firas JARBOUI;Vianney PERCHET;Roman EGGER,firasjarboui@gmail.com;vianney.perchet@gmail.com;roman.egger@fh-salzburg.ac.at,6;3;3,I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,ENS Paris-Saclay;ENS Paris-Saclay;,481;481;-1,644;644;-1,
4310,4310,4310,4310,4310,4310,4310,4310,ICLR,2020,Distribution-Guided Local Explanation for Black-Box Classifiers,Weijie Fu;Meng Wang;Mengnan Du;Ninghao Liu;Shijie Hao;Xia Hu,fwj.edu@gmail.com;eric.mengwang@gmail.com;dumengnan@tamu.edu;nhliu43@tamu.edu;hfut.hsj@gmail.com;hu@cse.tamu.edu,6;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,South China University of Technology;;Texas A&M;Texas A&M;;Texas A&M,481;-1;44;44;-1;44,501;-1;177;177;-1;177,
4311,4311,4311,4311,4311,4311,4311,4311,ICLR,2020,Keyword Spotter Model for Crop Pest and Disease Monitoring from Community Radio Data,Benjamin Akera;Joyce Nakatumba-Nabende;Ali Hussein;Daniel Ssendiwala;Jonathan Mukiibi,akeraben@gmail.com;jnakatumba@cis.mak.ac.ug;ali.hussein@ronininstitute.org;ssendiwaladaniel@gmail.com;jonmuk7@gmail.com,1;1;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Makerere University;Makerere University;Ronin Institute;;,481;481;-1;-1;-1,605;605;-1;-1;-1,
4312,4312,4312,4312,4312,4312,4312,4312,ICLR,2020,Shallow VAEs with RealNVP Prior Can Perform as Well as Deep Hierarchical VAEs,Haowen Xu;Wenxiao Chen;Jinlin Lai;Zhihan Li;Youjian Zhao;Dan Pei,xhw15@mails.tsinghua.edu.cn;chen-wx17@mails.tsinghua.edu.cn;laijl16@mails.tsinghua.edu.cn;lizhihan17@mails.tsinghua.edu.cn;zhaoyoujian@tsinghua.edu.cn;peidan@tsinghua.edu.cn,6;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University,8;8;8;8;8;8,23;23;23;23;23;23,5
4313,4313,4313,4313,4313,4313,4313,4313,ICLR,2020,Adaptive network sparsification with dependent variational beta-Bernoulli dropout,Juho Lee;Saehoon Kim;Jaehong Yoon;Hae Beom Lee;Eunho Yang;Sung Ju Hwang,juho@aitrics.com;shkim@aitrics.com;jaehong.yoon@kaist.ac.kr;haebeom.lee@kaist.ac.kr;eunhoy@kaist.ac.kr;sjhwang82@kaist.ac.kr,6;3;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A,Reject,0,6,0.0,yes,9/25/19,AITRICS;AITRICS;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,-1;-1;481;481;481;481,-1;-1;110;110;110;110,
4314,4314,4314,4314,4314,4314,4314,4314,ICLR,2020,Scalable Generative Models for Graphs with Graph Attention Mechanism,Wataru Kawai;Yusuke Mukuta;Tatsuya Harada,w-kawai@mi.t.u-tokyo.ac.jp;mukuta@mi.t.u-tokyo.ac.jp;harada@mi.t.u-tokyo.ac.jp,3;1;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,9,0.0,yes,9/25/19,The University of Tokyo;The University of Tokyo;The University of Tokyo,56;56;56,36;36;36,5;10
4315,4315,4315,4315,4315,4315,4315,4315,ICLR,2020,On the Dynamics and Convergence of Weight Normalization for Training Neural Networks,Yonatan Dukler;Quanquan Gu;Guido Montufar,ydukler@math.ucla.edu;qgu@cs.ucla.edu;montufar@math.ucla.edu,3;6;3,I have published one or two papers in this area.:N/A:N/A:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,"University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles",20;20;20,17;17;17,1
4316,4316,4316,4316,4316,4316,4316,4316,ICLR,2020,Semi-Supervised Few-Shot Learning with Prototypical Random Walks,Ahmed Ayyad;Nassir Navab;Mohamed Elhoseiny;Shadi Albarqouni,a.3ayad@gmail.com;nassir.navab@tum.de;mohamed.elhoseiny@gmail.com;shadi.albarqouni@tum.de,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,4,0.0,yes,9/25/19,Technical University Munich;Technical University Munich;KAUST;Technical University Munich,53;53;128;53,43;43;1397;43,6;10
4317,4317,4317,4317,4317,4317,4317,4317,ICLR,2020,Agent as Scientist: Learning to Verify Hypotheses,Kenneth Marino;Rob Fergus;Arthur Szlam;Abhinav Gupta,kdmarino@cs.cmu.edu;fergus@cs.nyu.edu;aszlam@fb.com;abhinavg@cs.cmu.edu,3;3;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Carnegie Mellon University;New York University;Facebook;Carnegie Mellon University,1;25;-1;1,27;29;-1;27,
4318,4318,4318,4318,4318,4318,4318,4318,ICLR,2020,Generating Robust Audio Adversarial Examples using Iterative Proportional Clipping,Hongting Zhang;Qiben Yan;Pan Zhou,htzhang@hust.edu.cn;qyan@msu.edu;panzhou@hust.edu.cn,3;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,6,0.0,yes,9/25/19,Hong Kong University of Science and Technology;SUN YAT-SEN UNIVERSITY;Hong Kong University of Science and Technology,39;481;39,47;299;47,4
4319,4319,4319,4319,4319,4319,4319,4319,ICLR,2020,Lean Images for Geo-Localization,Moti Kadosh;Yael Moses;Ariel Shamir,arik@idc.ac.il;yael@idc.ac.il,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,interdisciplinary center herzliya;interdisciplinary center herzliya,-1;-1,-1;-1,2
4320,4320,4320,4320,4320,4320,4320,4320,ICLR,2020,Are Powerful Graph Neural Nets Necessary? A Dissection on Graph Classification,Ting Chen;Song Bian;Yizhou Sun,iamtingchen@gmail.com;biansonghz@gmail.com;yzsun@cs.ucla.edu,3;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,"Google;Zhejiang University;University of California, Los Angeles",-1;56;20,-1;107;17,10
4321,4321,4321,4321,4321,4321,4321,4321,ICLR,2020,DEEP GRAPH SPECTRAL EVOLUTION NETWORKS FOR GRAPH TOPOLOGICAL TRANSFORMATION,Liang Zhao;Qingzhe Li;Negar Etemadyrad;Xiaojie Guo,lzhao9@gmu.edu,6;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,George Mason University,100,282,10
4322,4322,4322,4322,4322,4322,4322,4322,ICLR,2020,Robust Federated Learning Through Representation Matching and Adaptive Hyper-parameters,Hesham Mostafa,hesham.mostafa@intel.com,3;3;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Intel,-1,-1,
4323,4323,4323,4323,4323,4323,4323,4323,ICLR,2020,Enforcing Physical Constraints in Neural Neural Networks through Differentiable PDE Layer,"Chiyu Max"" Jiang;Karthik Kashinath;Prabhat;Philip Marcus""",chiyu.jiang@berkeley.edu;kkashinath@lbl.gov;prabhat@lbl.gov;pmarcus@me.berkeley.edu,3;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,University of California Berkeley;Lawrence Berkeley National Lab;Lawrence Berkeley National Lab;University of California Berkeley,5;-1;-1;5,13;-1;-1;13,5;4
4324,4324,4324,4324,4324,4324,4324,4324,ICLR,2020,Evo-NAS: Evolutionary-Neural Hybrid Agent for Architecture Search,Krzysztof Maziarz;Mingxing Tan;Andrey Khorlin;Kuang-Yu Samuel Chang;Andrea Gesmundo,krzysztof.s.maziarz@gmail.com;tanmingxing@google.com;akhorlin@google.com;kysc@google.com;agesmundo@google.com,3;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,11,0.0,yes,9/25/19,Jagiellonian University;Google;Google;Google;Google,481;-1;-1;-1;-1,610;-1;-1;-1;-1,
4325,4325,4325,4325,4325,4325,4325,4325,ICLR,2020,HUBERT Untangles BERT to Improve Transfer across NLP Tasks,Mehrad Moradshahi;Hamid Palangi;Monica S. Lam;Paul Smolensky;Jianfeng Gao,mehrad@stanford.edu;hpalangi@microsoft.com;lam@cs.stanford.edu;paul.smolensky@gmail.com;jfgao@microsoft.com,1;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,1,4,0.0,yes,9/25/19,Stanford University;Microsoft;Stanford University;Microsoft;Microsoft,4;-1;4;-1;-1,4;-1;4;-1;-1,3
4326,4326,4326,4326,4326,4326,4326,4326,ICLR,2020,Learning De-biased Representations with Biased Representations,Hyojin Bahng;Sanghyuk Chun;Sangdoo Yun;Jaegul Choo;Seong Joon Oh,hjj552@korea.ac.kr;sanghyuk.c@navercorp.com;sangdoo.yun@navercorp.com;jchoo@korea.ac.kr;coallaoh@linecorp.com,6;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,Korea University;NAVER;NAVER;Korea University;LINE,323;-1;-1;323;-1,179;-1;-1;179;-1,
4327,4327,4327,4327,4327,4327,4327,4327,ICLR,2020,ICNN: INPUT-CONDITIONED FEATURE REPRESENTATION LEARNING FOR TRANSFORMATION-INVARIANT NEURAL NETWORK,Suraj Tripathi;Chirag Singh;Abhay Kumar,surajtripathi93@gmail.com;c.singh@samsung.com;abykumar12011@gmail.com,3;1;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,1,0.0,yes,9/25/19,;Samsung;University of Southern California,-1;-1;31,-1;-1;62,1
4328,4328,4328,4328,4328,4328,4328,4328,ICLR,2020,Learning Latent State Spaces for Planning through Reward Prediction,Aaron Havens;Yi Ouyang;Prabhat Nagarajan;Yasuhiro Fujita,ahavens2@illinois.edu;ouyangyi@preferred-america.com;prabhat@preferred.jp;fujita@preferred.jp,3;3;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,"University of Illinois, Urbana Champaign;Preferred Networks, Inc.;Preferred Networks, Inc.;Preferred Networks, Inc.",3;-1;-1;-1,48;-1;-1;-1,
4329,4329,4329,4329,4329,4329,4329,4329,ICLR,2020,Certified Robustness to Adversarial Label-Flipping Attacks via Randomized Smoothing,Elan Rosenfeld;Ezra Winston;Pradeep Ravikumar;J. Zico Kolter,ekr@andrew.cmu.edu;ewinston@andrew.cmu.edu;pradeepr@cs.cmu.edu;zkolter@cs.cmu.edu,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,8,0.0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1,27;27;27;27,4
4330,4330,4330,4330,4330,4330,4330,4330,ICLR,2020,Optimistic Adaptive Acceleration for Optimization,Jun-Kun Wang;Xiaoyun Li;Ping Li,jimwang@gatech.edu;xl374@scarletmail.rutgers.edu;liping11@baidu.com,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Georgia Institute of Technology;Rutgers University;Baidu,13;34;-1,38;168;-1,
4331,4331,4331,4331,4331,4331,4331,4331,ICLR,2020,Hierarchical Bayes Autoencoders,Shuangfei Zhai;Carlos Guestrin;Joshua M. Susskind,szhai@apple.com;guestrin@apple.com;jsusskind@apple.com,1;1;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Apple;Apple;Apple,-1;-1;-1,-1;-1;-1,5;4
4332,4332,4332,4332,4332,4332,4332,4332,ICLR,2020,Project and Forget: Solving Large Scale Metric Constrained Problems,Anna C. Gilbert;Rishi Sonthalia,annacg@umich.edu;rsonthal@umich.edu,6;3;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,8,1.0,yes,9/25/19,University of Michigan;University of Michigan,8;8,21;21,1;10
4333,4333,4333,4333,4333,4333,4333,4333,ICLR,2020,Deep Auto-Deferring Policy for Combinatorial Optimization,Sungsoo Ahn;Younggyo Seo;Jinwoo Shin,sungsoo.ahn@kaist.ac.kr;younggyo.seo@kaist.ac.kr;jinwoos@kaist.ac.kr,6;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,481;481;481,110;110;110,10
4334,4334,4334,4334,4334,4334,4334,4334,ICLR,2020,Under what circumstances do local codes emerge in feed-forward neural networks,Ella M. Gale;Nicolas Martin,ella.gale@bristol.ac.uk;nm13850@bristol.ac.uk,1;3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Reject,0,0,0.0,yes,9/25/19,University of Bristol;University of Bristol,128;128,87;87,
4335,4335,4335,4335,4335,4335,4335,4335,ICLR,2020,Reinforcement Learning with Structured Hierarchical Grammar Representations of Actions,Petros Christodoulou;Robert Lange;Ali Shafti;A. Aldo Faisal,petros.christodoulou18@imperial.ac.uk;rtl17@ic.ac.uk;a.shafti@imperial.ac.uk;a.faisal@imperial.ac.uk,3;8;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Imperial College London;Imperial College London;Imperial College London;Imperial College London,73;73;73;73,10;10;10;10,
4336,4336,4336,4336,4336,4336,4336,4336,ICLR,2020,Global Adversarial Robustness Guarantees for Neural Networks,Luca Laurenti;Andrea Patane;Matthew Wicker;Luca Bortolussi;Luca Cardelli;Marta Kwiatkowska,luca.laurenti@cs.ox.ac.uk;andrea.patane@chch.ox.ac.uk;matthew.wicker@wolfson.ox.ac.uk;luca.bortolussi@gmail.com;luca.a.cardelli@gmail.com;marta.kwiatkowska@cs.ox.ac.uk,3;1;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,University of Oxford;University of Oxford;University of Oxford;;;University of Oxford,50;50;50;-1;-1;50,1;1;1;-1;-1;1,11;4;1
4337,4337,4337,4337,4337,4337,4337,4337,ICLR,2020,Learning Calibratable Policies using Programmatic Style-Consistency,Eric Zhan;Albert Tseng;Yisong Yue;Adith Swaminathan;Matthew Hausknecht,ezhan@caltech.edu;atseng@caltech.edu;yyue@caltech.edu;adswamin@microsoft.com;mahauskn@microsoft.com,6;3;3,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,California Institute of Technology;California Institute of Technology;California Institute of Technology;Microsoft;Microsoft,143;143;143;-1;-1,2;2;2;-1;-1,
4338,4338,4338,4338,4338,4338,4338,4338,ICLR,2020,Alternating Recurrent Dialog Model with Large-Scale Pre-Trained Language Models,Qingyang Wu;Yichi Zhang;Yu Li;Zhou Yu,wilwu@ucdavis.edu;zhangyic17@mails.tsinghua.edu.cn;yooli@ucdavis.edu;joyu@ucdavis.edu,1;3;8,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,"University of California, Davis;Tsinghua University;University of California, Davis;University of California, Davis",79;8;79;79,55;23;55;55,3
4339,4339,4339,4339,4339,4339,4339,4339,ICLR,2020,Learning to Reason: Distilling Hierarchy via Self-Supervision and Reinforcement Learning,Jung-Su Ha;Young-Jin Park;Hyeok-Joo Chae;Soon-Seo Park;Han-Lim Choi,jung-su.ha@ipvs.uni-stuttgart.de;yjpark@lics.kaist.ac.kr;hjchae@lics.kaist.ac.kr;sspark@lics.kaist.ac.kr;hanlimc@kaist.ac.kr,6;1;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,10,0.0,yes,9/25/19,University of Stuttgart;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,95;481;481;481;481,292;110;110;110;110,
4340,4340,4340,4340,4340,4340,4340,4340,ICLR,2020,The Geometry of Sign Gradient Descent,Lukas Balles;Fabian Pedregosa;Nicolas Le Roux,lukas.balles@tuebingen.mpg.de;f@bianp.net;nicolas@le-roux.name,3;1;3,I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,"Max Planck Institute for Intelligent Systems, Max-Planck Institute;Google;Google",-1;-1;-1,-1;-1;-1,
4341,4341,4341,4341,4341,4341,4341,4341,ICLR,2020,Effective Use of Variational Embedding Capacity in Expressive End-to-End Speech Synthesis,Eric Battenberg;Soroosh Mariooryad;Daisy Stanton;RJ Skerry-Ryan;Matt Shannon;David Kao;Tom Bagby,ebattenberg@google.com;soroosh@google.com;daisy@google.com;rjryan@google.com;mattshannon@google.com;davidkao@google.com;tombagby@google.com,6;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,1
4342,4342,4342,4342,4342,4342,4342,4342,ICLR,2020,The problem with DDPG: understanding failures in deterministic environments with sparse rewards,Guillaume Matheron;Olivier Sigaud;Nicolas Perrin,matheron@isir.upmc.fr;olivier.sigaud@upmc.fr;perrin@isir.upmc.fr,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,4,0.0,yes,9/25/19,"Computer Science Lab  - Pierre and Marie Curie University, Paris, France;Computer Science Lab  - Pierre and Marie Curie University, Paris, France;Computer Science Lab  - Pierre and Marie Curie University, Paris, France",481;481;481,1397;1397;1397,
4343,4343,4343,4343,4343,4343,4343,4343,ICLR,2020,Enhanced Convolutional Neural Tangent Kernels,Dingli Yu;Ruosong Wang;Zhiyuan Li;Wei Hu;Ruslan Salakhutdinov;Sanjeev Arora;Simon S. Du,dingliy@cs.princeton.edu;ruosongw@andrew.cmu.edu;zhiyuanli@cs.princeton.edu;huwei@cs.princeton.edu;rsalakhu@cs.cmu.edu;arora@cs.princeton.edu;ssdu@ias.edu,3;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,"Princeton University;Carnegie Mellon University;Princeton University;Princeton University;Carnegie Mellon University;Princeton University;Institue for Advanced Study, Princeton",31;1;31;31;1;31;-1,6;27;6;6;27;6;-1,
4344,4344,4344,4344,4344,4344,4344,4344,ICLR,2020,COMBINED FLEXIBLE ACTIVATION FUNCTIONS FOR DEEP NEURAL NETWORKS,Renlong Jie;Junbin Gao;Andrey Vasnev;Minh-Ngoc Tran,renlong.jie@sydney.edu.au;junbin.gao@sydney.edu.au;andrey.vasnev@sydney.edu.au;minh-ngoc.tran@sydney.edu.au,1;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,University of Sydney;University of Sydney;University of Sydney;University of Sydney,86;86;86;86,60;60;60;60,
4345,4345,4345,4345,4345,4345,4345,4345,ICLR,2020,Meta-RCNN: Meta Learning for Few-Shot Object Detection,Xiongwei Wu;Doyen Sahoo;Steven C. H. Hoi,xwwu.2015@smu.edu.sg;dsahoo@salesforce.com;shoi@salesforce.com,6;3;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Singapore Management University;SalesForce.com;SalesForce.com,92;-1;-1,1397;-1;-1,6;2
4346,4346,4346,4346,4346,4346,4346,4346,ICLR,2020,AutoSlim: Towards One-Shot Architecture Search for Channel Numbers,Jiahui Yu;Thomas Huang,jyu79@illinois.edu;t-huang1@illinois.edu,6;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,3,0.0,yes,9/25/19,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",3;3,48;48,
4347,4347,4347,4347,4347,4347,4347,4347,ICLR,2020,Learning from Imperfect Annotations: An End-to-End Approach,Emmanouil Antonios Platanios;Maruan Al-Shedivat;Eric Xing;Tom Mitchell,e.a.platanios@cs.cmu.edu;alshedivat@cs.cmu.edu;epxing@cs.cmu.edu;tom.mitchell@cs.cmu.edu,6;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,4,1.0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1,27;27;27;27,
4348,4348,4348,4348,4348,4348,4348,4348,ICLR,2020,Feature Map Transform Coding for Energy-Efficient CNN Inference,Brian Chmiel;Chaim Baskin;Ron Banner;Evgenii Zheltonozhskii;Yevgeny Yermolin;Alex Karbachevsky;Alex M. Bronstein;Avi Mendelson,brian.chmiel@intel.com;chaimbaskin@cs.technion.ac.il;ron.banner@intel.com;evgeniizh@campus.technion.ac.il;yevgeny_ye@campus.technion.ac.il;alex.k@cs.technion.ac.il;bron@cs.technion.ac.il;avi.mendelson@cs.technion.ac.il,3;3;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:N/A:N/A,Reject,0,3,0.0,yes,9/25/19,Intel;Technion;Intel;Technion;Technion;Technion;Technion;Technion,-1;26;-1;26;26;26;26;26,-1;412;-1;412;412;412;412;412,2
4349,4349,4349,4349,4349,4349,4349,4349,ICLR,2020,Zero-Shot Out-of-Distribution Detection with Feature Correlations,Chandramouli S Sastry;Sageev Oore,chandramouli.sastry@gmail.com;osageev@gmail.com,3;8;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,11,0.0,yes,9/25/19,Dalhousie University;Dalhousie University,323;323,269;269,
4350,4350,4350,4350,4350,4350,4350,4350,ICLR,2020,Neural Video Encoding,Abel Brown;Robert DiPietro,abelb@nvidia.com;rdipietro@nvidia.com,1;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:N/A:I read the paper thoroughly.:N/A;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,NVIDIA;NVIDIA,-1;-1,-1;-1,3;2
4351,4351,4351,4351,4351,4351,4351,4351,ICLR,2020,Continuous Adaptation in Multi-agent Competitive Environments,Kuei-Tso Lee;Sheng-Jyh Wang,fuj30089@gmail.com;shengjyh@faculty.nctu.edu.tw,1;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,;National Chiao Tung University,-1;143,-1;564,
4352,4352,4352,4352,4352,4352,4352,4352,ICLR,2020,Simple and Effective Stochastic Neural Networks,Tianyuan Yu;Yongxin Yang;Da Li;Timothy Hospedales;Tao Xiang,tianyuan.yu@surrey.ac.uk;yongxin.yang@surrey.ac.uk;dali.darren@hotmail.com;t.hospedales@ed.ac.uk;t.xiang@surrey.ac.uk,6;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,University of Surrey;University of Surrey;;University of Edinburgh;University of Surrey,205;205;-1;33;205,260;260;-1;30;260,4;11;8
4353,4353,4353,4353,4353,4353,4353,4353,ICLR,2020,On The Difficulty of Warm-Starting Neural Network Training,Jordan T. Ash;Ryan P. Adams,jordanta@cs.princeton.edu;rpa@princeton.edu,3;1;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,3,0.0,yes,9/25/19,Princeton University;Princeton University,31;31,6;6,8
4354,4354,4354,4354,4354,4354,4354,4354,ICLR,2020,Thwarting finite difference adversarial attacks with output randomization,Haidar Khan;Dan Park;Azer Khan;Bülent Yener,haidark@gmail.com;parkd5@gmail.com;azerkkhan@gmail.com;byener@gmail.com,3;6;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,Amazon;;;,-1;-1;-1;-1,-1;-1;-1;-1,4;1
4355,4355,4355,4355,4355,4355,4355,4355,ICLR,2020,Benchmarking Robustness in Object Detection: Autonomous Driving when Winter is Coming,Claudio Michaelis;Benjamin Mitzkus;Robert Geirhos;Evgenia Rusak;Oliver Bringmann;Alexander S. Ecker;Matthias Bethge;Wieland Brendel,claudio.michaelis@uni-tuebingen.de;benjamin.mitzkus@uni-tuebingen.de;robert@geirhos.de;evgenia.rusak@bethgelab.org;oliver.bringmann@uni-tuebingen.de;alexander.ecker@uni-tuebingen.de;matthias@bethgelab.org;wieland.brendel@bethgelab.org,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Reject,0,6,0.0,yes,9/25/19,"University of Tuebingen;University of Tuebingen;University of Tuebingen;Centre for Integrative Neuroscience, AG Bethge;University of Tuebingen;University of Tuebingen;Centre for Integrative Neuroscience, AG Bethge;Centre for Integrative Neuroscience, AG Bethge",154;154;154;-1;154;154;-1;-1,91;91;91;-1;91;91;-1;-1,2
4356,4356,4356,4356,4356,4356,4356,4356,ICLR,2020,Negative Sampling in Variational Autoencoders,Adrián Csiszárik;Beatrix Benkő;Dániel Varga,csadrian@renyi.hu;bbeatrix1010@gmail.com;daniel@renyi.hu,3;6;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Alfréd Rényi Institute of Mathematics;Eotvos Lorand University;Alfréd Rényi Institute of Mathematics,-1;-1;-1,-1;-1;-1,5;4
4357,4357,4357,4357,4357,4357,4357,4357,ICLR,2020,An Empirical and Comparative Analysis of Data Valuation with Scalable Algorithms,Ruoxi Jia;Xuehui Sun;Jiacen Xu;Ce Zhang;Bo Li;Dawn Song,ruoxijia@berkeley.edu;zidaneandmessi@sjtu.edu.cn;coldstudy@sjtu.edu.cn;ce.zhang@inf.ethz.ch;lxbosky@gmail.com;dawnsong@gmail.com,1;1;3,I have read many papers in this area.:I did not assess the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,9,0.0,yes,9/25/19,University of California Berkeley;Shanghai Jiao Tong University;Shanghai Jiao Tong University;Swiss Federal Institute of Technology;University of California Berkeley;University of California Berkeley,5;53;53;10;5;5,13;157;157;13;13;13,
4358,4358,4358,4358,4358,4358,4358,4358,ICLR,2020,Attraction-Repulsion Actor-Critic for Continuous Control Reinforcement Learning,Thang Doan;Bogdan Mazoure;Audrey Durand;Joelle Pineau;R Devon Hjelm,thang.doan@mail.mcgill.ca;bogdan.mazoure@mail.mcgill.ca;audrey.durand@ift.ulaval.ca;jpineau@cs.mcgill.ca;devon.hjelm@microsoft.com,3;3;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,McGill University;McGill University;Laval university;McGill University;Microsoft,86;86;481;86;-1,42;42;272;42;-1,
4359,4359,4359,4359,4359,4359,4359,4359,ICLR,2020,NormLime: A New Feature Importance Metric for Explaining Deep Neural Networks,Isaac Ahern;Adam Noack;Luis Guzman-Nateras;Dejing Dou;Boyang Li;Jun Huan,isaac@biofidelic.com;anoack2@uoregon.edu;lguzmann@uoregon.edu;dou@cs.uoregon.edu;boyangli@baidu.com;huanjun@baidu.com,3;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,University of Oregon;University of Oregon;University of Oregon;University of Oregon;Baidu;Baidu,205;205;205;205;-1;-1,288;288;288;288;-1;-1,
4360,4360,4360,4360,4360,4360,4360,4360,ICLR,2020,Unsupervised Learning of Node Embeddings by Detecting Communities,Chi Thang Duong;Dung Hoang;Truong Giang Le Ba;Thanh Le Cong;Hongzhi Yin;Matthias Weidlich;Quoc Viet Hung Nguyen;Karl Aberer,thang.duong@epfl.ch;dungmin97@gmail.com;giangpna98@gmail.com;thanhcls1316@gmail.com;h.yin1@uq.edu.au;matthias.weidlich@hu-berlin.de;quocviethung1@gmail.com;karl.aberer@epfl.ch,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,4,0.0,yes,9/25/19,Swiss Federal Institute of Technology Lausanne;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;University of Queensland;Humboldt Universität Berlin;;Swiss Federal Institute of Technology Lausanne,481;39;39;39;205;266;-1;481,38;47;47;47;66;73;-1;38,10
4361,4361,4361,4361,4361,4361,4361,4361,ICLR,2020,Global graph curvature,Liudmila Prokhorenkova;Egor Samosvat;Pim van der Hoorn,ostroumova-la@yandex-team.ru;sameg@yandex-team.ru;pimvdhoorn@gmail.com,6;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0.0,yes,9/25/19,Yandex;Yandex;Eindhoven University of Technology,-1;-1;205,-1;-1;185,10
4362,4362,4362,4362,4362,4362,4362,4362,ICLR,2020,Invertible generative models for  inverse problems: mitigating representation error and dataset bias,Muhammad Asim;Ali Ahmed;Paul Hand,msee16001@itu.edu.pk;ali.ahmed@itu.edu.pk;p.hand@northeastern.edu,6;1;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I did not assess the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,4,0.0,yes,9/25/19,"ITU of Punjab Lahore, Pakistan;ITU of Punjab Lahore, Pakistan;Northeastern University",-1;-1;16,-1;-1;906,5;4
4363,4363,4363,4363,4363,4363,4363,4363,ICLR,2020,Decaying momentum helps neural network training,John Chen;Anastasios Kyrillidis,jc114@rice.edu;anastasios@rice.edu,6;3;3,I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,10,0.0,yes,9/25/19,Rice University;Rice University,84;84,105;105,
4364,4364,4364,4364,4364,4364,4364,4364,ICLR,2020,GRAPH NEIGHBORHOOD ATTENTIVE POOLING,Zekarias Tilahun Kefato;Sarunas Girdzijauskas,zekarias@kth.se;sarunasg@kth.se,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,"KTH Royal Institute of Technology, Stockholm, Sweden;KTH Royal Institute of Technology, Stockholm, Sweden",128;128,222;222,10
4365,4365,4365,4365,4365,4365,4365,4365,ICLR,2020,Quantum algorithm for finding the negative curvature direction,Kaining Zhang;Min-Hsiu Hsieh;Liu Liu;Dacheng Tao,kzha3670@uni.sydney.edu.au;min-hsiu.hsieh@uts.edu.au;liu.liu1@sydney.edu.au;dacheng.tao@sydney.edu.au,6;6;3,I have read many papers in this area.:N/A:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,University of Sydney;University of Technology Sydney;University of Sydney;University of Sydney,86;108;86;86,60;193;60;60,1;9
4366,4366,4366,4366,4366,4366,4366,4366,ICLR,2020,"Walking on the Edge: Fast, Low-Distortion Adversarial Examples",Hanwei Zhang;Teddy Furon;Yannis Avrithis;Laurent Amsaleg,hanwei.zhang@irisa.fr;teddy.furon@inria.fr;yannis@avrithis.net;laurent.amsaleg@irisa.fr,3;3;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,"IRISA, Université Bretagne Sud;INRIA;INRIA;IRISA, Université Bretagne Sud",481;-1;-1;481,1397;-1;-1;1397,4
4367,4367,4367,4367,4367,4367,4367,4367,ICLR,2020,"Deep Reasoning Networks:  Thinking Fast and Slow, for Pattern De-mixing",Di Chen;Yiwei Bai;Wenting Zhao;Sebastian Ament;John M. Gregoire;Carla P. Gomes,di@cs.cornell.edu;bywbilly@cs.cornell.edu;wzhao@cs.cornell.edu;ament@cs.cornell.edu;gregoire@caltech.edu;gomes@cs.cornell.edu,3;6;3,I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Cornell University;Cornell University;Cornell University;Cornell University;California Institute of Technology;Cornell University,7;7;7;7;143;7,19;19;19;19;2;19,
4368,4368,4368,4368,4368,4368,4368,4368,ICLR,2020,Measuring Calibration in Deep Learning,Jeremy Nixon;Mike Dusenberry;Ghassen Jerfel;Linchuan Zhang;Dustin Tran,jeremynixon@google.com;dusenberrymw@google.com;ghassen@google.com;linchzhang@google.com;trandustin@google.com,6;3;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,
4369,4369,4369,4369,4369,4369,4369,4369,ICLR,2020,Towards Understanding the Regularization of Adversarial Robustness on Neural Networks,Yuxin Wen;Shuai Li;Kui Jia,wen.yuxin@mail.scut.edu.cn;lishuai918@gmail.com;kuijia@scut.edu.cn,6;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,7,0.0,yes,9/25/19,South China University of Technology;;South China University of Technology,481;-1;481,501;-1;501,4;8
4370,4370,4370,4370,4370,4370,4370,4370,ICLR,2020,BRIDGING ADVERSARIAL SAMPLES AND ADVERSARIAL NETWORKS,Faqiang Liu;Mingkun Xu;Guoqi Li;Jing Pei;Luping Shi,lfq18@mails.tsinghua.edu.cn;xmk18@mails.tsinghua.edu.cn;liguoqi@mail.tsinghua.edu.cn;peij@mail.tsinghua.edu.cn;lpshi@mail.tsinghua.edu.cn,6;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University,8;8;8;8;8,23;23;23;23;23,5;4
4371,4371,4371,4371,4371,4371,4371,4371,ICLR,2020,Kernel and Rich Regimes in Overparametrized Models,Blake Woodworth;Suriya Gunasekar;Pedro Savarese;Edward Moroshko;Itay Golan;Jason Lee;Daniel Soudry;Nathan Srebro,blake@ttic.edu;suriya@ttic.edu;savarese@ttic.edu;edward.moroshko@gmail.com;sitaygo@campus.technion.ac.il;jasondlee88@gmail.com;daniel.soudry@gmail.com;nati@ttic.edu,6;8;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Toyota Technological Institute at Chicago;Toyota Technological Institute at Chicago;Toyota Technological Institute at Chicago;Technion;Technion;University of Southern California;Technion;Toyota Technological Institute at Chicago,-1;-1;-1;26;26;31;26;-1,-1;-1;-1;412;412;62;412;-1,8
4372,4372,4372,4372,4372,4372,4372,4372,ICLR,2020,The Intriguing Effects of Focal Loss on the Calibration of Deep Neural Networks,Jishnu Mukhoti;Viveka Kulharia;Amartya Sanyal;Stuart Golodetz;Philip Torr;Puneet Dokania,jishnumukhoti7@gmail.com;viveka@robots.ox.ac.uk;amartya.sanyal@cs.ox.ac.uk;stuart@five.ai;philip.torr@eng.ox.ac.uk;puneet@robots.ox.ac.uk,6;3;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,13,0.0,yes,9/25/19,;University of Oxford;University of Oxford;FiveAI;University of Oxford;University of Oxford,-1;50;50;-1;50;50,-1;1;1;-1;1;1,3;2
4373,4373,4373,4373,4373,4373,4373,4373,ICLR,2020,Domain-Invariant Representations: A Look on Compression and Weights,Victor Bouvier;Céline Hudelot;Clément Chastagnol;Philippe Very;Myriam Tami,vbouvier@sidetrade.com;celine.hudelot@centralesupelec.fr;cchastagnol@sidetrade.com;pveryranchet@gmail.com;myriam.tami@centralesupelec.fr,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,CentraleSupelec;CentraleSupelec;Sidetrade;;CentraleSupelec,481;481;-1;-1;481,534;534;-1;-1;534,1
4374,4374,4374,4374,4374,4374,4374,4374,ICLR,2020,Last-iterate convergence rates for min-max optimization,Jacob Abernethy;Kevin A. Lai;Andre Wibisono,prof@gatech.edu;nykal212@gmail.com;andrwbsn@gmail.com,6;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology,13;13;13,38;38;38,5;4;1;9
4375,4375,4375,4375,4375,4375,4375,4375,ICLR,2020,Implicit Generative Modeling for Efficient Exploration,Neale Ratzlaff;Qinxun Bai;Li Fuxin;Wei Xu,ratzlafn@oregonstate.edu;qinxun.bai@horizon.ai;lif@oregonstate.edu;wei.xu@horizon.ai,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:N/A:N/A,Reject,0,4,0.0,yes,9/25/19,Oregon State University;Horizon Robotics;Oregon State University;Horizon Robotics,77;-1;77;-1,373;-1;373;-1,5;11
4376,4376,4376,4376,4376,4376,4376,4376,ICLR,2020,Is There Mode Collapse? A Case Study on Face Generation and Its Black-box Calibration,Zhenyu Wu;Ye Yuan;Zhaowen Wang;Jianming Zhang;Zhangyang Wang;Hailin Jin,wuzhenyu_sjtu@tamu.edu;ye.yuan@tamu.edu;zhawang@adobe.com;jianmzha@adobe.com;atlaswang@tamu.edu;hljin@adobe.com,3;6;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Texas A&M;Texas A&M;Adobe Systems;Adobe Systems;Texas A&M;Adobe Systems,44;44;-1;-1;44;-1,177;177;-1;-1;177;-1,5;4
4377,4377,4377,4377,4377,4377,4377,4377,ICLR,2020,Storage Efficient and Dynamic Flexible Runtime Channel Pruning via Deep Reinforcement Learning,Jianda Chen;Shangyu Chen;Sinno Jialin Pan,jianda001@e.ntu.edu.sg;schen025@e.ntu.edu.sg;sinnopan@ntu.edu.sg,3;3;6,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,National Taiwan University;National Taiwan University;National Taiwan University,86;86;86,120;120;120,
4378,4378,4378,4378,4378,4378,4378,4378,ICLR,2020,Generalized Zero-shot ICD Coding,Congzheng Song;Shanghang Zhang;Najmeh Sadoughi;Pengtao Xie;Eric Xing,cs2296@cornell.edu;shanghang.zhang@petuum.com;najmeh.sadoughi@petuum.com;pengtao.xie@petuum.com;eric.xing@petuum.com,3;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Cornell University;Petuum Inc.;Petuum Inc.;Petuum Inc.;Petuum Inc.,7;-1;-1;-1;-1,19;-1;-1;-1;-1,5;4;6
4379,4379,4379,4379,4379,4379,4379,4379,ICLR,2020,"Unsupervised Disentanglement of Pose, Appearance and Background from Images and Videos",Aysegul Dundar;Kevin J Shih;Animesh Garg;Robert Pottorf;Andrew Tao;Bryan Catanzaro,aysegul.dundar89@gmail.com;kjshih2@illinois.edu;garg@cs.stanford.edu;rpottorff@gmail.com;atao@nvidia.com;bcatanzaro@nvidia.com,3;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,"NVIDIA;University of Illinois, Urbana Champaign;Stanford University;Brigham Young University;NVIDIA;NVIDIA",-1;3;4;-1;-1;-1,-1;48;4;-1;-1;-1,
4380,4380,4380,4380,4380,4380,4380,4380,ICLR,2020,A Bayes-Optimal View on Adversarial Examples,Eitan Richardson;Yair Weiss,eitan.richardson@gmail.com;yweiss@cs.huji.ac.il,1;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,12,1.0,yes,9/25/19,Hebrew University of Jerusalem;Hebrew University of Jerusalem,67;67,216;216,4
4381,4381,4381,4381,4381,4381,4381,4381,ICLR,2020,"Carpe Diem, Seize the Samples Uncertain at the Moment"" for Adaptive Batch Selection""",Hwanjun Song;Minseok Kim;Sundong Kim;Jae-Gil Lee,songhwanjun@kaist.ac.kr;minseokkim@kaist.ac.kr;sundong@ibs.re.kr;jaegil@kaist.ac.kr,3;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Institute for Basic Science;Korea Advanced Institute of Science and Technology,481;481;-1;481,110;110;-1;110,
4382,4382,4382,4382,4382,4382,4382,4382,ICLR,2020,EXACT ANALYSIS OF CURVATURE CORRECTED LEARNING DYNAMICS IN DEEP LINEAR NETWORKS,Dongsung Huh,dongsunghuh@gmail.com,6;6;1,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,,,,
4383,4383,4383,4383,4383,4383,4383,4383,ICLR,2020,TreeCaps: Tree-Structured Capsule Networks for Program Source Code Processing,Vinoj Jayasundara;Nghi Duy Quoc Bui;Lingxiao Jiang;David Lo,vinojjayasundara@gmail.com;dqnbui.2016@phdis.smu.edu.sg;lxjiang@smu.edu.sg;davidlo@smu.edu.sg,1;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,University of Moratuwa;Singapore Management University;Singapore Management University;Singapore Management University,-1;92;92;92,-1;1397;1397;1397,3;10
4384,4384,4384,4384,4384,4384,4384,4384,ICLR,2020,Pre-trained Contextual Embedding of Source Code,Aditya Kanade;Petros Maniatis;Gogul Balakrishnan;Kensen Shi,akanade@google.com;maniatis@google.com;bgogul@google.com;kshi@google.com,6;3;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,14,1.0,yes,9/25/19,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,
4385,4385,4385,4385,4385,4385,4385,4385,ICLR,2020,AdaX: Adaptive Gradient Descent with Exponential Long Term Memory,Wenjie Li;Zhaoyang Zhang;Xinjiang Wang;Ping Luo,li3549@purdue.edu;zhaoyangzhang@link.cuhk.edu.hk;swanxinjiang@gmail.com;pluo.lhi@gmail.com,3;3;1;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Purdue University;The Chinese University of Hong Kong;SenseTime Group Limited;The University of Hong Kong,27;59;-1;92,88;35;-1;35,9;3;1;2
4386,4386,4386,4386,4386,4386,4386,4386,ICLR,2020,MIST: Multiple Instance Spatial Transformer Networks,Baptiste Angles;Simon Kornblith;Shahram Izadi;Andrea Tagliasacchi;Kwang Moo Yi,baptiste.angles@gmail.com;skornblith@google.com;shahrami@google.com;taglia@google.com;kyi@uvic.ca,3;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,;Google;Google;Google;University of Victoria,-1;-1;-1;-1;172,-1;-1;-1;-1;449,
4387,4387,4387,4387,4387,4387,4387,4387,ICLR,2020,Differentiable Bayesian Neural Network Inference for Data Streams,Namuk Park;Taekyu Lee;Songkuk Kim,namuk.park@yonsei.ac.kr;taekyu.lee@yonsei.ac.kr;songkuk@yonsei.ac.kr,3;3;8,I have read many papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Yonsei University;Yonsei University;Yonsei University,481;481;481,196;196;196,11;2
4388,4388,4388,4388,4388,4388,4388,4388,ICLR,2020,CAT: Compression-Aware Training for bandwidth reduction,Chaim Baskin;Brian Chmiel;Evgenii Zheltonozhskii;Ron Banner;Alex M. Bronstein;Avi Mendelson,chaimbaskin@cs.technion.ac.il;brian.chmiel@intel.com;evgeniizh@campus.technion.ac.il;ron.banner@intel.com;bron@cs.technion.ac.il;avi.mendelson@cs.technion.ac.il,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Technion;Intel;Technion;Intel;Technion;Technion,26;-1;26;-1;26;26,412;-1;412;-1;412;412,
4389,4389,4389,4389,4389,4389,4389,4389,ICLR,2020,Using Logical Specifications of Objectives in Multi-Objective Reinforcement Learning,Kolby Nottingham;Anand Balakrishnan;Jyotirmoy Deshmukh;Connor Christopherson;David Wingate,kolbytn@byu.edu;anandbal@usc.edu;jdeshmukh@usc.edu;connormc@byu.edu;wingated@cs.byu.edu,3;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Brigham Young University;University of Southern California;University of Southern California;Brigham Young University;Brigham Young University,-1;31;31;-1;-1,-1;62;62;-1;-1,
4390,4390,4390,4390,4390,4390,4390,4390,ICLR,2020,High-Frequency guided Curriculum Learning for Class-specific Object Boundary Detection,VSR Veeravasarapu;Deepak Mittal;Abhishek Goel;Maneesh Singh,vsr.veera@gmail.com;deepak.mittal@verisk.com;abhishek.goel@verisk.com;maneesh.singh@verisk.com,1;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Verisk Analytics;Verisk Analytics;Verisk Analytics;Verisk Analytics,-1;-1;-1;-1,-1;-1;-1;-1,
4391,4391,4391,4391,4391,4391,4391,4391,ICLR,2020,Learning General and Reusable Features via Racecar-Training,You Xie;Nils Thuerey,you.xie@tum.de;nils.thuerey@tum.de,1;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,3,0.0,yes,9/25/19,Technical University Munich;Technical University Munich,53;53,43;43,
4392,4392,4392,4392,4392,4392,4392,4392,ICLR,2020,Weakly-supervised Knowledge Graph Alignment with Adversarial Learning,Meng Qu;Jian Tang;Yoshua Bengio,meng.qu@umontreal.ca;jian.tang@hec.ca;yoshua.bengio@mila.quebec,6;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,4,0.0,yes,9/25/19,University of Montreal;HEC Montreal;University of Montreal,128;128;128,85;85;85,4;1;10
4393,4393,4393,4393,4393,4393,4393,4393,ICLR,2020,Differentially Private Mixed-Type Data Generation For Unsupervised Learning,Uthaipon Tantipongpipat;Chris Waites;Digvijay Boob;Amaresh Siva;Rachel Cummings,uthaipon@gmail.com;cwaites10@gmail.com;digvijaybb40@gmail.com;ankit.siva@gatech.edu;racheladcummings@gmail.com,1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,1,0.0,yes,9/25/19,Georgia Institute of Technology;;;Georgia Institute of Technology;Georgia Tech Research Corporation,13;-1;-1;13;-1,38;-1;-1;38;-1,5
4394,4394,4394,4394,4394,4394,4394,4394,ICLR,2020,Defense against Adversarial Examples by Encoder-Assisted Search in the Latent Coding Space,Wenjing Huang;Shikui Tu;Lei Xu,huangwenjing@sjtu.edu.cn;tushikui@sjtu.edu.cn,3;3;3;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,1,7,0.0,yes,9/25/19,Shanghai Jiao Tong University;Shanghai Jiao Tong University,53;53,157;157,5;4
4395,4395,4395,4395,4395,4395,4395,4395,ICLR,2020,Matrix Multilayer Perceptron,Jalil Taghia;Maria Bånkestad;Fredrik Lindsten;Thomas Schön,jalil.taghia@ericsson.com;maria.bankestad@ri.se;fredrik.lindsten@liu.se;thomas.schon@it.uu.se,6;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Ericsson;;Linköping University;Uppsala University,-1;-1;481;154,-1;-1;407;102,
4396,4396,4396,4396,4396,4396,4396,4396,ICLR,2020,A Simple Dynamic Learning Rate Tuning Algorithm For Automated Training of DNNs,Koyel Mukherjee;Alind Khare;Yogish Sabharwal;Ashish Verma,koyelmjee@gmail.com;kharealind@gmail.com;ysabharwal@in.ibm.com;ashish.verma1@ibm.com,1;1;1,I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,;Georgia Institute of Technology;International Business Machines;International Business Machines,-1;13;-1;-1,-1;38;-1;-1,4
4397,4397,4397,4397,4397,4397,4397,4397,ICLR,2020,Structural Language Models for Any-Code Generation,Uri Alon;Roy Sadaka;Omer Levy;Eran Yahav,urialon@cs.technion.ac.il;roysadaka@gmail.com;omerlevy@gmail.com;yahave@cs.technion.ac.il,6;6;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,Technion;;Facebook;Technion,26;-1;-1;26,412;-1;-1;412,3
4398,4398,4398,4398,4398,4398,4398,4398,ICLR,2020,Hierarchical Disentangle Network for Object Representation Learning,Shishi Qiao;Ruiping Wang;Shiguang Shan;Xilin Chen,qiaoshishi14@mails.ucas.ac.cn;wangruiping@ict.ac.cn;sgshan@ict.ac.cn;xlchen@ict.ac.cn,8;1;1;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,"Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences",59;59;59;59,1397;1397;1397;1397,5;4
4399,4399,4399,4399,4399,4399,4399,4399,ICLR,2020,Improved Training Techniques for Online Neural Machine Translation,Maha Elbayad;Laurent Besacier;Jakob Verbeek,maha.elbayad@inria.fr;laurent.besacier@univ-grenoble-alpes.fr;jakob.verbeek@inria.fr,3;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,3,0.0,yes,9/25/19,INRIA;University of Grenoble-Alpes;INRIA,-1;481;-1,-1;329;-1,3
4400,4400,4400,4400,4400,4400,4400,4400,ICLR,2020,Removing the Representation Error of GAN Image Priors Using the Deep Decoder,Max Daniels;Reinhard Heckel;Paul Hand,daniels.g@husky.neu.edu;rh43@rice.edu;p.hand@northeastern.edu,1;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Northeastern University;Rice University;Northeastern University,16;84;16,906;105;906,5
4401,4401,4401,4401,4401,4401,4401,4401,ICLR,2020,Learning a Behavioral Repertoire from Demonstrations,Niels Justesen;Miguel González Duque;Daniel Cabarcas Jaramillo;Jean-Baptiste Mouret;Sebastian Risi,noju@itu.edu;migonzalez@unal.edu.co;dcarbarc@unal.edu.co;jean-baptiste.mouret@inria.fr;sebr@itu.dk,1;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,0,0.0,yes,9/25/19,"ITU of Punjab Lahore, Pakistan;Universidad Nacional de Colombia;Universidad Nacional de Colombia;INRIA;IT University",-1;481;481;-1;172,-1;1397;1397;-1;416,
4402,4402,4402,4402,4402,4402,4402,4402,ICLR,2020,One-Shot Neural Architecture Search via Compressive Sensing,Minsu Cho;Mohammadreza Soltani;Chinmay Hegde,chomd90@iastate.edu;mohammadreza.soltani@duke.edu;chinmay@iastate.edu,1;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Iowa State University;Duke University;Iowa State University,172;47;172,399;20;399,6
4403,4403,4403,4403,4403,4403,4403,4403,ICLR,2020,Deep Audio Prior,Yapeng Tian;Chenliang Xu;Dingzeyu Li,yapengtian@rochester.edu;chenliang.xu@rochester.edu;dinli@adobe.com,3;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,University of Rochester;University of Rochester;Adobe Systems,100;100;-1,173;173;-1,
4404,4404,4404,4404,4404,4404,4404,4404,ICLR,2020,XD: Cross-lingual Knowledge Distillation for Polyglot Sentence Embeddings,Maksym Del;Mark Fishel,max.del.edu@gmail.com;fishel@ut.ee,1;6;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,1,4,0.0,yes,9/25/19,;University of Tartu,-1;266,-1;319,3
4405,4405,4405,4405,4405,4405,4405,4405,ICLR,2020,Graph Neural Networks for Soft Semi-Supervised Learning on Hypergraphs,Naganand Yadati;Tingran Gao;Shahab Asoodeh;Partha Talukdar;Anand Louis,y.naganand@gmail.com;trg17@uchicago.edu;shahab@seas.harvard.edu;ppt@iisc.ac.in;anandl@iisc.ac.in,3;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Indian Institute of Science;University of Chicago;Harvard University;Indian Institute of Science;Indian Institute of Science,95;48;39;95;95,301;9;7;301;301,10
4406,4406,4406,4406,4406,4406,4406,4406,ICLR,2020,Self-Supervised State-Control through Intrinsic Mutual Information Rewards,Rui Zhao;Volker Tresp;Wei Xu,zhaorui.in.germany@gmail.com;volker.tresp@siemens.com;wei.xu@horizon.ai,6;3;3,I have published in this field for several years.:N/A:N/A:N/A;I have read many papers in this area.:I carefully checked the experiments.:N/A:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Siemens Corporate Research;Siemens Corporate Research;Horizon Robotics,-1;-1;-1,-1;-1;-1,
4407,4407,4407,4407,4407,4407,4407,4407,ICLR,2020,A Generative Model for Molecular Distance Geometry,Gregor N. C. Simm;José Miguel Hernández-Lobato,gncsimm@gmail.com;jmh233@cam.ac.uk,6;3;6,I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,1,8,0.0,yes,9/25/19,University of Cambridge;University of Cambridge,71;71,3;3,5;10
4408,4408,4408,4408,4408,4408,4408,4408,ICLR,2020,Multigrid Neural Memory,Tri Huynh;Michael Maire;Matthew R. Walter,trihuynh@uchicago.edu;mmaire@uchicago.edu;mwalter@ttic.edu,3;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,1.0,yes,9/25/19,University of Chicago;University of Chicago;Toyota Technological Institute at Chicago,48;48;-1,9;9;-1,
4409,4409,4409,4409,4409,4409,4409,4409,ICLR,2020,Neural Design of Contests and All-Pay Auctions using Multi-Agent Simulation,Thomas Anthony;Ian Gemp;Janos Kramar;Tom Eccles;Andrea Tacchetti;Yoram Bachrach,twa@google.com;imgemp@google.com;janosk@google.com;eccles@google.com;atacchet@google.com;yorambac@gmail.com,6;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,
4410,4410,4410,4410,4410,4410,4410,4410,ICLR,2020,Geometry-aware Generation of Adversarial and Cooperative Point Clouds,Yuxin Wen;Jiehong Lin;Ke Chen;Kui Jia,wen.yuxin@mail.scut.edu.cn;lin.jiehong@mail.scut.edu.cn;chenk@scut.edu.cn;kuijia@scut.edu.cn,3;3;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,South China University of Technology;South China University of Technology;South China University of Technology;South China University of Technology,481;481;481;481,501;501;501;501,4;7
4411,4411,4411,4411,4411,4411,4411,4411,ICLR,2020,Point Process Flows,Nazanin Mehrasa;Ruizhi Deng;Mohamed Osama Ahmed;Bo Chang;Jiawei He;Thibaut Durand;Marcus Brubaker;Greg Mori,nmehrasa@sfu.ca;ruizhid@sfu.ca;mohamed.o.ahmed@borealisai.com;bchang@stat.ubc.ca;jha203@sfu.ca;thibaut.p.durand@borealisai.com;marcus.brubaker@borealisai.com;mori@cs.sfu.ca,6;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,7,0.0,yes,9/25/19,Simon Fraser University;Simon Fraser University;Borealis AI;University of British Columbia;Simon Fraser University;Borealis AI;Borealis AI;Simon Fraser University,64;64;-1;35;64;-1;-1;64,272;272;-1;34;272;-1;-1;272,
4412,4412,4412,4412,4412,4412,4412,4412,ICLR,2020,Learning Effective Exploration Strategies For Contextual Bandits,Amr Sharaf;Hal Daumé III,amr@cs.umd.edu;hal@umiacs.umd.edu,1;1;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,5,0.0,yes,9/25/19,"University of Maryland, College Park;University of Maryland, College Park",12;12,91;91,6
4413,4413,4413,4413,4413,4413,4413,4413,ICLR,2020,From English to Foreign Languages: Transferring Pre-trained Language Models,Ke Tran,ketranmanh@gmail.com,3;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,2,1.0,yes,9/25/19,Amazon,-1,-1,3;6
4414,4414,4414,4414,4414,4414,4414,4414,ICLR,2020,Leveraging Adversarial Examples to Obtain Robust Second-Order Representations,Mohit Prabhushankar;Gukyeong Kwon;Dogancan Temel;Ghassan AlRegib,mohit.p@gatech.edu;gukyeong.kwon@gatech.edu;cantemel@gatech.edu;alregib@gatech.edu,3;1;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,0,0.0,yes,9/25/19,Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology,13;13;13;13,38;38;38;38,4
4415,4415,4415,4415,4415,4415,4415,4415,ICLR,2020,Multi-Agent Hierarchical Reinforcement Learning for Humanoid Navigation,Glen Berseth;Brandon haworth;Seonghyeon Moon;Mubbasir Kapadia;Petros Faloutsos,gberseth@gmail.com;m.brandon.haworth@gmail.com;sm2062@cs.rutgers.edu;mubbasir.kapadia@gmail.com;pfaloutsos@gmail.com,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,University of California Berkeley;;Rutgers University;;,5;-1;34;-1;-1,13;-1;168;-1;-1,
4416,4416,4416,4416,4416,4416,4416,4416,ICLR,2020,SemanticAdv: Generating Adversarial Examples via Attribute-Conditional Image Editing,Haonan Qiu;Chaowei Xiao;Lei Yang;Xinchen Yan;HongLak Lee;Bo Li,haonanqiu@link.cuhk.edu.cn;xiaocw@umich.edu;yl016@ie.cuhk.edu.hk;xcyan@umich.edu;honglak@eecs.umich.edu;lxbosky@gmail.com,3;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,1,8,0.0,yes,9/25/19,Tsinghua University;University of Michigan;The Chinese University of Hong Kong;University of Michigan;University of Michigan;University of California Berkeley,8;8;59;8;8;5,23;21;35;21;21;13,4;2
4417,4417,4417,4417,4417,4417,4417,4417,ICLR,2020,Semantic Pruning for Single Class Interpretability,Kamila Abdiyeva;Martin Lukac;Kanat Alimanov,kabdiyeva@nu.edu.kz;martin.lukac@nu.edu.kz;kanat.alimanov@nu.edu.kz,3;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Australian National University;Australian National University;Australian National University,108;108;108,50;50;50,2
4418,4418,4418,4418,4418,4418,4418,4418,ICLR,2020,Dual Graph Representation Learning,Huiling Zhu;Xin Luo;Hankz Hankui Zhuo,zhuhling6@mail.sysu.edu.cn;luo35@mail2.sysu.edu.cn;zhuohank@mail.sysu.edu.cn,3;3;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY,481;481;481,299;299;299,10
4419,4419,4419,4419,4419,4419,4419,4419,ICLR,2020,Semi-Implicit Back Propagation,Ren Liu;Xiaoqun Zhang,liur0810@sjtu.edu.cn;xqzhang@sjtu.edu.cn,1;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Shanghai Jiao Tong University;Shanghai Jiao Tong University,53;53,157;157,9
4420,4420,4420,4420,4420,4420,4420,4420,ICLR,2020,The Effect of Neural Net Architecture on Gradient Confusion & Training Performance,Karthik A. Sankararaman;Soham De;Zheng Xu;W. Ronny Huang;Tom Goldstein,karthikabinavs@gmail.com;sohamde@google.com;xuzh@cs.umd.edu;wrhuang@cs.umd.edu;tomg@cs.umd.edu,8;1;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,"Facebook;Google;University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park",-1;-1;12;12;12,-1;-1;91;91;91,
4421,4421,4421,4421,4421,4421,4421,4421,ICLR,2020,Frequency Analysis for Graph Convolution Network,Hoang NT;Takanori Maehara,hoang.nguyen.rh@riken.jp;takanori.maehara@riken.jp,6;1;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,RIKEN;RIKEN,-1;-1,-1;-1,4;1;10
4422,4422,4422,4422,4422,4422,4422,4422,ICLR,2020,A SPIKING SEQUENTIAL MODEL: RECURRENT LEAKY INTEGRATE-AND-FIRE,Daiheng Gao;Hongwei Wang;Hehui Zhang;Meng Wang;Zhenzhi Wu,samuel.gao023@gmail.com;hongwei.wang@lynxi.com;zhh@bupt.edu.cn;wangmeng_wm@bupt.edu.cn;zhenzhi.wu@lynxi.com,3;1;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,;Lynxi;Beijing University of Post and Telecommunication;Beijing University of Post and Telecommunication;Lynxi,-1;-1;481;481;-1,-1;-1;1397;1397;-1,
4423,4423,4423,4423,4423,4423,4423,4423,ICLR,2020,Expected Tight Bounds for Robust Deep Neural Network Training,Salman Alsubaihi;Adel Bibi;Modar Alfadly;Abdullah Hamdi;Bernard Ghanem,salman.subaihi@kaust.edu.sa;adel.bibi@kaust.edu.sa;modar.alfadly@kaust.edu.sa;abdullah.hamdi@kaust.edu.sa;bernard.ghanem@kaust.edu.sa,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,1,6,0.0,yes,9/25/19,KAUST;KAUST;KAUST;KAUST;KAUST,128;128;128;128;128,1397;1397;1397;1397;1397,4;1
4424,4424,4424,4424,4424,4424,4424,4424,ICLR,2020,Neural Operator Search,Wei Li;Shaogang Gong;Xiatian Zhu,w.li@qmul.ac.uk;s.gong@qmul.ac.uk;eddy.zhuxt@gmail.com,6;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,6,0.0,yes,9/25/19,Queen Mary University London;Queen Mary University London;Samsung,233;233;-1,110;110;-1,
4425,4425,4425,4425,4425,4425,4425,4425,ICLR,2020,Behavior-Guided Reinforcement Learning,Aldo Pacchiano;Jack Parker-Holder;Yunhao Tang;Anna Choromanska;Krzysztof Choromanski;Michael I. Jordan,pacchiano@berkeley.edu;jh3764@columbia.edu;yt2541@columbia.edu;achoroma@gmail.com;kchoro@google.com;jordan@cs.berkeley.edu,1;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,University of California Berkeley;Columbia University;Columbia University;New York University;Google;University of California Berkeley,5;15;15;25;-1;5,13;16;16;29;-1;13,
4426,4426,4426,4426,4426,4426,4426,4426,ICLR,2020,"Semi-supervised semantic segmentation needs strong, high-dimensional perturbations",Geoff French;Timo Aila;Samuli Laine;Michal Mackiewicz;Graham Finlayson,g.french@uea.ac.uk;taila@nvidia.com;slaine@nvidia.com;m.mackiewicz@uea.ac.uk;g.finlayson@uea.ac.uk,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,;NVIDIA;NVIDIA;;,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,2
4427,4427,4427,4427,4427,4427,4427,4427,ICLR,2020,Interpretations are useful: penalizing explanations to align neural networks with prior knowledge,Laura Rieger;Chandan Singh;W. James Murdoch;Bin Yu,lauri@dtu.dk;c_singh@berkeley.edu;jmurdoch@berkeley.edu;binyu@berkeley.edu,6;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,2,5,0.0,yes,9/25/19,Technical University of Denmark;University of California Berkeley;University of California Berkeley;University of California Berkeley,481;5;5;5,182;13;13;13,
4428,4428,4428,4428,4428,4428,4428,4428,ICLR,2020,Axial Attention in Multidimensional Transformers,Jonathan Ho;Nal Kalchbrenner;Dirk Weissenborn;Tim Salimans,jonathanho@google.com;nalk@google.com;diwe@google.com;salimans@google.com,1;6;1;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,8
4429,4429,4429,4429,4429,4429,4429,4429,ICLR,2020,Adversarial Robustness as a Prior for Learned Representations,Logan Engstrom;Andrew Ilyas;Shibani Santurkar;Dimitris Tsipras;Brandon Tran;Aleksander Madry,engstrom@mit.edu;ailyas@mit.edu;shibani@mit.edu;tsipras@mit.edu;btran115@mit.edu;madry@mit.edu,3;3;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,12,0.0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2;2;2,5;5;5;5;5;5,4
4430,4430,4430,4430,4430,4430,4430,4430,ICLR,2020,Equivariant neural networks and equivarification,Erkao Bao;Linqi Song,baoerkao@gmail.com;linqi.song@cityu.edu.hk,6;3;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,;City University of Hong Kong,-1;92,-1;35,
4431,4431,4431,4431,4431,4431,4431,4431,ICLR,2020,Do recent advancements in model-based deep reinforcement learning really improve data efficiency?,Kacper Piotr Kielak,k.kielak@bham.ac.uk,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,1,4,0.0,yes,9/25/19,Birmingham University,128,112,
4432,4432,4432,4432,4432,4432,4432,4432,ICLR,2020,Efficient meta reinforcement learning via meta goal generation,Haotian Fu;Hongyao Tang;Jianye Hao,haotianfu@tju.edu.cn;bluecontra@tju.edu.cn;jianye.hao@tju.edu.cn,1;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,3,0.0,yes,9/25/19,Zhejiang University;Zhejiang University;Zhejiang University,56;56;56,107;107;107,6
4433,4433,4433,4433,4433,4433,4433,4433,ICLR,2020,The Discriminative Jackknife: Quantifying Uncertainty in Deep Learning via Higher-Order Influence Functions,Ahmed M. Alaa;Mihaela van der Schaar,a7med3laa@hotmail.com;mihaelaucla@gmail.com,3;6;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,;,-1;-1,-1;-1,11
4434,4434,4434,4434,4434,4434,4434,4434,ICLR,2020,Generative Adversarial Nets for Multiple Text Corpora,Diego Klabjan;Baiyang Wang,d-klabjan@northwestern.edu;baiyang@u.northwestern.edu,3;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Northwestern University;Northwestern University,44;44,22;22,3;4;5
4435,4435,4435,4435,4435,4435,4435,4435,ICLR,2020,Learning Similarity Metrics for Numerical Simulations,Georg Kohl;Kiwon Um;Nils Thuerey,georg.kohl@tum.de;kiwon.um@tum.de;nils.thuerey@tum.de,6;3;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Technical University Munich;Technical University Munich;Technical University Munich,53;53;53,43;43;43,8
4436,4436,4436,4436,4436,4436,4436,4436,ICLR,2020,Universal Adversarial Attack Using Very Few Test Examples,Amit Deshpande;Sandesh Kamath;K V Subrahmanyam,amitdesh@microsoft.com;ksandeshk@cmi.ac.in;kv@cmi.ac.in,3;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Microsoft;Chennai Mathematical Institute;Chennai Mathematical Institute,-1;-1;-1,-1;-1;-1,4;1
4437,4437,4437,4437,4437,4437,4437,4437,ICLR,2020,Improved Generalization Bound of Permutation Invariant Deep Neural Networks,Akiyoshi Sannai;Masaaki Imaizumi,akiyoshi.sannai@riken.jp;imaizumi@ism.ac.jp,1;6;3,I have published in this field for several years.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:N/A:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0.0,yes,9/25/19,"RIKEN;The Institute of Statistical Mathematics, Japan",-1;-1,-1;-1,10;1;8
4438,4438,4438,4438,4438,4438,4438,4438,ICLR,2020,Peer Loss Functions: Learning from Noisy Labels without Knowing Noise Rates,Yang Liu;Hongyi Guo,yangliu@ucsc.edu;guohongyi@sjtu.edu.cn,3;3;8,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,11,0.0,yes,9/25/19,University of Southern California;Shanghai Jiao Tong University,31;53,62;157,
4439,4439,4439,4439,4439,4439,4439,4439,ICLR,2020,Mode Connectivity and Sparse Neural Networks,Jonathan Frankle;Gintare Karolina Dziugaite;Daniel M. Roy;Michael Carbin,jfrankle@csail.mit.edu;karolina.dziugaite@gmail.com;droy@utstat.toronto.edu;mcarbin@csail.mit.edu,3;3;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,6,0.0,yes,9/25/19,Massachusetts Institute of Technology;Element AI;University of Toronto;Massachusetts Institute of Technology,2;-1;18;2,5;-1;18;5,
4440,4440,4440,4440,4440,4440,4440,4440,ICLR,2020,Generating valid Euclidean distance matrices,Moritz Hoffmann;Frank Noe,moritz.hoffmann@fu-berlin.de;frank.noe@fu-berlin.de,8;3;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Freie Universität Berlin;Freie Universität Berlin,-1;-1,-1;-1,5
4441,4441,4441,4441,4441,4441,4441,4441,ICLR,2020,GResNet: Graph Residual Network for Reviving Deep GNNs from Suspended Animation,Jiawei Zhang;Lin Meng,jiawei@ifmlab.org;lin@ifmlab.org,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY,481;481,299;299,1;10
4442,4442,4442,4442,4442,4442,4442,4442,ICLR,2020,Deep Spike Decoder (DSD),Emrah Adamey;Tarin Ziyaee;Nishanth Alapati;Jun Ye,emrah@ctrl-labs.com;tarin@ctrl-labs.com;nishanth@ctrl-labs.com;jun@ctrl-labs.com,1;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,1,0.0,yes,9/25/19,Ctrl-labs;Ctrl-labs;Ctrl-labs;Ctrl-labs,-1;-1;-1;-1,-1;-1;-1;-1,5
4443,4443,4443,4443,4443,4443,4443,4443,ICLR,2020,Efficient High-Dimensional Data Representation Learning via Semi-Stochastic Block Coordinate Descent Methods,Bingkun Wei;Yangyang Li;Fanhua Shang;Yuanyuan Liu;Hongying Liu;Shengmei Shen,bkwei028@gmail.com;1615401247li@gmail.com;fhshang@xidian.edu.cn;yyliu@xidian.edu.cn;hyliu@xidian.edu.cn;jane.shen@pensees.ai,6;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University;,8;8;8;8;8;-1,23;23;23;23;23;-1,2
4444,4444,4444,4444,4444,4444,4444,4444,ICLR,2020,Towards Controllable and Interpretable Face Completion via  Structure-Aware and Frequency-Oriented Attentive GANs,Zeyuan Chen;Shaoliang Nie;Tianfu Wu;Christopher G. Healey,zchen23@ncsu.edu;snie@ncsu.edu;tianfu_wu@ncsu.edu;healey@ncsu.edu,6;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,North Carolina State University;North Carolina State University;North Carolina State University;North Carolina State University,86;86;86;86,310;310;310;310,5;4
4445,4445,4445,4445,4445,4445,4445,4445,ICLR,2020,Why Does Hierarchy (Sometimes) Work So Well in Reinforcement Learning?,Ofir Nachum;Haoran Tang;Xingyu Lu;Shixiang Gu;Honglak Lee;Sergey Levine,ofirnachum@google.com;hrtang.alex@berkeley.edu;xingyulu0701@berkeley.edu;shanegu@google.com;honglak@google.com;svlevine@eecs.berkeley.edu,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,4,0.0,yes,9/25/19,Google;University of California Berkeley;University of California Berkeley;Google;Google;University of California Berkeley,-1;5;5;-1;-1;5,-1;13;13;-1;-1;13,
4446,4446,4446,4446,4446,4446,4446,4446,ICLR,2020,Improving Exploration of Deep Reinforcement Learning using Planning for Policy Search,Jakob J. Hollenstein;Erwan Renaudo;Justus Piater,jakob.hollenstein@uibk.ac.at;erwan.renaudo@uibk.ac.at;justus.piater@uibk.ac.at,3;1;1,I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,3,0.0,yes,9/25/19,University of Innsbruck;University of Innsbruck;University of Innsbruck,481;481;481,415;415;415,
4447,4447,4447,4447,4447,4447,4447,4447,ICLR,2020,"Improved Training Speed, Accuracy, and Data Utilization via Loss Function Optimization",Santiago Gonzalez;Risto Miikkulainen,slgonzalez@utexas.edu;risto@cs.utexas.edu,3;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,0,0.0,yes,9/25/19,"University of Texas, Austin;University of Texas, Austin",22;22,38;38,
4448,4448,4448,4448,4448,4448,4448,4448,ICLR,2020,Knowledge Hypergraphs: Prediction Beyond Binary Relations,Bahare Fatemi;Perouz Taslakian;David Vazquez;David Poole,bfatemi@cs.ubc.ca;perouz@elementai.com;dvazquez@elementai.com;poole@cs.ubc.ca,3;6;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,University of British Columbia;Element AI;Element AI;University of British Columbia,35;-1;-1;35,34;-1;-1;34,10
4449,4449,4449,4449,4449,4449,4449,4449,ICLR,2020,FR-GAN: Fair and Robust Training,Yuji Roh;Kangwook Lee;Gyeong Jo Hwang;Steven Euijong Whang;Changho Suh,rohyj113@gmail.com;kangwook.lee@wisc.edu;hkj4276@kaist.ac.kr;swhang@kaist.ac.kr;chsuh@kaist.ac.kr,3;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,3,0.0,yes,9/25/19,Korea Advanced Institute of Science and Technology;University of Southern California;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,481;31;481;481;481,110;62;110;110;110,5;4;7
4450,4450,4450,4450,4450,4450,4450,4450,ICLR,2020,Unsupervised Intuitive Physics from Past Experiences,Sebastien Ehrhardt;Aron Monszpart;Niloy Mitra;Andrea Vedaldi,hyenal@robots.ox.ac.uk;aron@nianticlabs.com;n.mitra@cs.ucl.ac.uk;vedaldi@robots.ox.ac.uk,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,University of Oxford;Niantic Inc.;University College London;University of Oxford,50;-1;50;50,1;-1;15;1,6
4451,4451,4451,4451,4451,4451,4451,4451,ICLR,2020,Continual Learning with Delayed Feedback,THEIVENDIRAM PRANAVAN;TERENCE SIM,pranavan@u.nus.edu;tsim@comp.nus.edu.sg,1;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,National University of Singapore;National University of Singapore,16;16,25;25,
4452,4452,4452,4452,4452,4452,4452,4452,ICLR,2020,Adapting to Label Shift with Bias-Corrected Calibration,Avanti Shrikumar;Amr M. Alexandari;Anshul Kundaje,avanti.shrikumar@gmail.com;amr.alexandari@gmail.com;anshul@kundaje.net,6;1;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,3,0.0,yes,9/25/19,Stanford University;Stanford University;,4;4;-1,4;4;-1,
4453,4453,4453,4453,4453,4453,4453,4453,ICLR,2020,On the Unintended Social Bias of Training Language Generation Models with News Articles,Omar U. Florez,omar.florez@aggiemail.usu.edu,1;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,0,0.0,yes,9/25/19,SUN YAT-SEN UNIVERSITY,481,299,3;7
4454,4454,4454,4454,4454,4454,4454,4454,ICLR,2020,Attacking Graph Convolutional Networks via Rewiring,Yao Ma;Suhang Wang;Tyler Derr;Lingfei Wu;Jiliang Tang,mayao4@msu.edu;szw494@psu.edu;derrtyle@msu.edu;wuli@us.ibm.com;tangjili@msu.edu,6;3;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,SUN YAT-SEN UNIVERSITY;Pennsylvania State University;SUN YAT-SEN UNIVERSITY;International Business Machines;SUN YAT-SEN UNIVERSITY,481;41;481;-1;481,299;78;299;-1;299,4;10
4455,4455,4455,4455,4455,4455,4455,4455,ICLR,2020,Modeling Fake News in Social Networks with Deep Multi-Agent Reinforcement Learning,Christoph Aymanns;Matthias Weber;Co-Pierre Georg;Jakob Foerster,christoph.aymanns@gmail.com;matthias.weber@unisg.ch;cogeorg@gmail.com;jakobfoerster@gmail.com,3;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I did not assess the experiments.:I made a quick assessment of this paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,9,0.0,yes,9/25/19,University of St Gallen;University of St. Gallen;;Facebook,481;481;-1;-1,435;435;-1;-1,4
4456,4456,4456,4456,4456,4456,4456,4456,ICLR,2020,Group-Connected Multilayer Perceptron Networks,Mohammad Kachuee;Sajad Darabi;Shayan Fazeli;Majid Sarrafzadeh,mkachuee@ucla.edu;sajad.darabi@cs.ucla.edu;shayan@cs.ucla.edu;majid@cs.ucla.edu,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,"University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles",20;20;20;20,17;17;17;17,7;10
4457,4457,4457,4457,4457,4457,4457,4457,ICLR,2020,Neural ODEs for Image Segmentation with Level Sets,Rafael Valle;Fitsum Reda;Mohammad Shoeybi;Patrick Legresley;Andrew Tao;Bryan Catanzaro,rafaelvalle@nvidia.com;freda@nvidia.com;mshoeybi@nvidia.com;plegresley@nvidia.com;atao@nvidia.com;bcatanzaro@nvidia.com,3;1;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Reject,1,0,0.0,yes,9/25/19,NVIDIA;NVIDIA;NVIDIA;NVIDIA;NVIDIA;NVIDIA,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,2
4458,4458,4458,4458,4458,4458,4458,4458,ICLR,2020,Knowledge Transfer via Student-Teacher Collaboration,Tianxiao Gao;Ruiqin Xiong;Zhenhua Liu;Siwei ma;Feng Wu;Tiejun Huang;Wen Gao,gtx@pku.edu.cn;rqxiong@pku.edu.cn;liu-zh@pku.edu.cn;swma@pku.edu.cn;fengwu@ustc.edu.cn;tjhuang@pku.edu.cn;wgao@pku.edu.cn,6;8;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,1,4,0.0,yes,9/25/19,Peking University;Peking University;Peking University;Peking University;University of Science and Technology of China;Peking University;Peking University,22;22;22;22;481;22;22,24;24;24;24;80;24;24,
4459,4459,4459,4459,4459,4459,4459,4459,ICLR,2020,VIMPNN: A physics informed neural network for estimating potential energies of out-of-equilibrium systems,Jay Morgan;Adeline Paiement;Christian Klinke,j.p.morgan@swansea.ac.uk;adeline.paiement@univ-tln.fr;christian.klinke@uni-rostock.de,6;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,3,0.0,yes,9/25/19,Swansea University;CNRS university Toulon;Universität Rostock,481;481;390,266;1397;-1,
4460,4460,4460,4460,4460,4460,4460,4460,ICLR,2020,AutoLR: A Method for Automatic Tuning of Learning Rate,Nipun Kwatra;V Thejas;Nikhil Iyer;Ramachandran Ramjee;Muthian Sivathanu,nkwatra@microsoft.com;thejasvenkatesh97@gmail.com;t-niiyer@microsoft.com;ramjee@microsoft.com;muthian@microsoft.com,6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,12,0.0,yes,9/25/19,Microsoft;;Microsoft;Microsoft;Microsoft,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,8
4461,4461,4461,4461,4461,4461,4461,4461,ICLR,2020,Continuous Convolutional Neural Network forNonuniform Time Series,Hui Shi;Yang Zhang;Hao Wu;Shiyu Chang;Kaizhi Qian;Mark Hasegawa-Johnson;Jishen Zhao,hshi@ucsd.edu;yang.zhang2@ibm.com;haowu11@illinois.edu;kqian3@illinois.edu;jhasegaw@illinois.edu;jzhao@ucsd.edu,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,3,0.0,yes,9/25/19,"University of California, San Diego;International Business Machines;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;University of California, San Diego",11;-1;3;3;3;11,31;-1;48;48;48;31,8
4462,4462,4462,4462,4462,4462,4462,4462,ICLR,2020,Test-Time Training for Out-of-Distribution Generalization,Yu Sun;Xiaolong Wang;Zhuang Liu;John Miller;Alexei A. Efros;Moritz Hardt,yusun@berkeley.edu;dragonwxl123@gmail.com;zhuangl@berkeley.edu;miller_john@berkeley.edu;efros@eecs.berkeley.edu;hardt@berkeley.edu,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,3,0.0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5;5;5,13;13;13;13;13;13,
4463,4463,4463,4463,4463,4463,4463,4463,ICLR,2020,MODELLING   BIOLOGICAL   ASSAYS   WITH ADAPTIVE DEEP KERNEL LEARNING,Prudencio Tossou;Basile Dura;Daniel Cohen;Mario Marchand;François Laviolette;Alexandre Lacoste,tossouprudencio@gmail.com;basile@invivoai.ca;daniel@invivoai.ca;mario.marchand@ift.ulaval.ca;francois.laviolette@ift.ulaval.ca;allac@elementai.com,6;3;8;3;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,3,8,0.0,yes,9/25/19,InVivo AI;;;Laval university;Laval university;Element AI,-1;-1;-1;481;481;-1,-1;-1;-1;272;272;-1,6
4464,4464,4464,4464,4464,4464,4464,4464,ICLR,2020,Hybrid Weight Representation: A Quantization Method Represented with Ternary and Sparse-Large Weights,Jinbae Park;Sung-Ho Bae,qkrwlsqo94@gmail.com;shbae@khu.ac.kr,3;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Kyung Hee University;Kyung Hee University,481;481,319;319,
4465,4465,4465,4465,4465,4465,4465,4465,ICLR,2020,Amharic Negation Handling,Girma Neshir,girma1978@gmail.com,1;1;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:N/A:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Addis Ababa University,481,1397,
4466,4466,4466,4466,4466,4466,4466,4466,ICLR,2020,Stiffness: A New Perspective on Generalization in Neural Networks,Stanislav Fort;Paweł Krzysztof Nowak;Stanisław Jastrzebski;Srini Narayanan,stanislav.fort@gmail.com;powalnow@google.com;staszek.jastrzebski@gmail.com;srinin@google.com,3;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,11,0.0,yes,9/25/19,Google;Google;New York University;Google,-1;-1;25;-1,-1;-1;29;-1,8
4467,4467,4467,4467,4467,4467,4467,4467,ICLR,2020,POP-Norm: A Theoretically Justified and More Accelerated Normalization Approach,Hanyang Peng;Shiqi Yu,philoso_phy0922@163.com;shiqi.yu@gmai.com,3;3;1,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Shenzhen University;Gmai,-1;-1,-1;-1,9
4468,4468,4468,4468,4468,4468,4468,4468,ICLR,2020,A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models,Elman Mansimov;Alex Wang;Kyunghyun Cho,elman.mansimov@gmail.com;wangalexc@gmail.com;kyunghyun.cho@nyu.edu,3;3;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,New York University;New York University;New York University,25;25;25,29;29;29,3
4469,4469,4469,4469,4469,4469,4469,4469,ICLR,2020,Pushing the bounds of dropout,Gábor Melis;Charles Blundell;Tomáš Kočiský;Karl Moritz Hermann;Chris Dyer;Phil Blunsom,melisgl@google.com;cblundell@google.com;tkocisky@google.com;kmh@google.com;cdyer@google.com;pblunsom@google.com,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,3;1
4470,4470,4470,4470,4470,4470,4470,4470,ICLR,2020,CRNet: Image Super-Resolution Using A Convolutional Sparse Coding  Inspired Network,Menglei Zhang;Zhou Liu;Jingwei He;Lei Yu,zmlhome@whu.edu.cn;liuzhou@whu.edu.cn;jingwei_he@whu.edu.cn;ly.wd@whu.edu.cn,3;1;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Wuhan University;Wuhan University;Wuhan University;Wuhan University,266;266;266;266,354;354;354;354,2
4471,4471,4471,4471,4471,4471,4471,4471,ICLR,2020,Hierarchical Graph Matching Networks for Deep Graph Similarity Learning,Xiang Ling;Lingfei Wu;Saizhuo Wang;Tengfei Ma;Fangli Xu;Chunming Wu;Shouling Ji,lingxiang@zju.edu.cn;lwu@email.wm.edu;szwang@zju.edu.cn;tengfei.ma1@ibm.com;lili@yixue.us;wuchunming@zju.edu.cn;sji@zju.edu.cn,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Zhejiang University;College of William and Mary;Zhejiang University;International Business Machines;;Zhejiang University;Zhejiang University,56;154;56;-1;-1;56;56,107;235;107;-1;-1;107;107,10
4472,4472,4472,4472,4472,4472,4472,4472,ICLR,2020,RATE-DISTORTION OPTIMIZATION GUIDED AUTOENCODER FOR GENERATIVE APPROACH,Keizo Kato;Jing Zhou;Akira Nakagawa,kato.keizo@jp.fujitsu.com;zhoujing@cn.fujitsu.com;anaka@jp.fujitsu.com,1;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,Fujitsu Laboratories Ltd.;Fujitsu Laboratories Ltd.;Fujitsu Laboratories Ltd.,-1;-1;-1,-1;-1;-1,5
4473,4473,4473,4473,4473,4473,4473,4473,ICLR,2020,Learning to Recognize the Unseen Visual Predicates,Defa Zhu;Si Liu;Wentao Jiang;Guanbin Li;Tianyi Wu;Guodong Guo,zhudefa@iie.ac.cn;liusi@buaa.edu.cn;jiangwentao@buaa.edu.cn;liguanbin@mail.sysu.edu.cn;wutianyi01@baidu.com;guoguodong01@baidu.com,6;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,"Institute of information engineering, CAS;Beihang University;Beihang University;SUN YAT-SEN UNIVERSITY;Baidu;Baidu",-1;118;118;481;-1;-1,-1;594;594;299;-1;-1,6
4474,4474,4474,4474,4474,4474,4474,4474,ICLR,2020,Self-supervised Training of Proposal-based Segmentation via Background Prediction,Isinsu Katircioglu;Helge Rhodin;Victor Constantin;Jörg Spörri;Mathieu Salzmann;Pascal Fua,isinsu.katircioglu@epfl.ch;rhodin@cs.ubc.ca;victor.constantin@epfl.ch;joerg.spoerri@balgrist.ch;mathieu.salzmann@epfl.ch;pascal.fua@epfl.ch,6;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Swiss Federal Institute of Technology Lausanne;University of British Columbia;Swiss Federal Institute of Technology Lausanne;University of Zurich;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne,481;35;481;143;481;481,38;34;38;90;38;38,2
4475,4475,4475,4475,4475,4475,4475,4475,ICLR,2020,Constrained Markov Decision Processes via Backward Value Functions,Harsh Satija;Philip Amortila;Joelle Pineau,harsh.satija@mail.mcgill.ca;philip.amortila@mail.mcgill.ca;jpineau@cs.mcgill.ca,3;8;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,McGill University;McGill University;McGill University,86;86;86,42;42;42,
4476,4476,4476,4476,4476,4476,4476,4476,ICLR,2020,Deep Interaction Processes for Time-Evolving Graphs,xiaofu chang;jianfeng wen;xuqin liu;yanming fang;le song;yuan qi,xiaofu.cxf@antfin.com;sylvain.wjf@antfin.com;xuqin.lxq@antfin.com;yanming.fym@mybank.cn;le.song@antfin.com;yuan.qi@antfin.com,3;3;3,I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,5,0.0,yes,9/25/19,Antfin;Antfin;Peking University;;Antfin;Antfin,-1;-1;22;-1;-1;-1,-1;-1;24;-1;-1;-1,10
4477,4477,4477,4477,4477,4477,4477,4477,ICLR,2020,On summarized validation curves and generalization,Mohammad Hashir;Yoshua Bengio;Joseph Paul Cohen,mohammad.hashir.khan@umontreal.ca;yoshua.bengio@mila.quebec;joseph@josephpcohen.com,3;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:N/A:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,University of Montreal;University of Montreal;University of Montreal,128;128;128,85;85;85,1
4478,4478,4478,4478,4478,4478,4478,4478,ICLR,2020,The Visual Task Adaptation Benchmark,Xiaohua Zhai;Joan Puigcerver;Alexander Kolesnikov;Pierre Ruyssen;Carlos Riquelme;Mario Lucic;Josip Djolonga;Andre Susano Pinto;Maxim Neumann;Alexey Dosovitskiy;Lucas Beyer;Olivier Bachem;Michael Tschannen;Marcin Michalski;Olivier Bousquet;Sylvain Gelly;Neil Houlsby,xzhai@google.com;jpuigcerver@google.com;alexander.kolesnikoff@gmail.com;pierrot@google.com;rikel@googel.com;lucic@google.com;josipd@google.com;andresp@google.com;maximneumann@google.com;adosovitskiy@gmail.com;lbeyer@google.com;bachem@google.com;tschannen@google.com;michalski@google.com;obousquet@google.com;sylvaingelly@google.com;neilhoulsby@google.com,8;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Reject,0,7,0.0,yes,9/25/19,Google;Google;Google;Google;Googel;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,5
4479,4479,4479,4479,4479,4479,4479,4479,ICLR,2020,SPREAD  DIVERGENCE,Mingtian Zhang;David Barber;Thomas Bird;Peter Hayes;Raza Habib,mingtian.zhang.17@ucl.ac.uk;david.barber@ucl.ac.uk;thomas.bird.17@ucl.ac.uk;peter.hayes.15@ucl.ac.uk;r.habib@cs.ucl.ac.uk,3;1;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,3,0.0,yes,9/25/19,University College London;University College London;University College London;University College London;University College London,50;50;50;50;50,15;15;15;15;15,5
4480,4480,4480,4480,4480,4480,4480,4480,ICLR,2020,Adversarially learned anomaly detection for time series data,Alexander Geiger;Alfredo Cuesta-Infante;Kalyan Veeramachaneni,geigera@mit.edu;alfredo.cuesta@urjc.es;kalyanv@mit.edu,1;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Massachusetts Institute of Technology;Universidad Rey Juan Carlos;Massachusetts Institute of Technology,2;-1;2,5;-1;5,5;4
4481,4481,4481,4481,4481,4481,4481,4481,ICLR,2020,CEB Improves Model Robustness,Ian Fischer;Alex A. Alemi,iansf@google.com;alemi@google.com,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Google;Google,-1;-1,-1;-1,4
4482,4482,4482,4482,4482,4482,4482,4482,ICLR,2020,Mildly Overparametrized Neural Nets can Memorize Training Data Efficiently,Rong Ge;Runzhe Wang;Haoyu Zhao,rongge@cs.duke.edu;wrz16@mails.tsinghua.edu.cn;zhaohy16@mails.tsinghua.edu.cn,1;3;8,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I did not assess the experiments.:N/A:I carefully checked the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Duke University;Tsinghua University;Tsinghua University,47;8;8,20;23;23,
4483,4483,4483,4483,4483,4483,4483,4483,ICLR,2020,Data-Driven Approach to Encoding and Decoding 3-D Crystal Structures,Jordan Hoffmann;Louis Maestrati;Yoshihide Sawada;Jian Tang;Jean Michel Sellier;Yoshua Bengio,jhoffmann@g.harvard.edu;maestratilouis@gmail.com;sawada.yoshihide@jp.panasonic.com;jian.tang@hec.ca;jeanmichel.sellier@mila.quebec;yoshua.bengio@mila.quebec,8;1;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:N/A:N/A:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,Harvard University;centrale lille;Panasonic Corporation;HEC Montreal;University of Montreal;University of Montreal,39;-1;-1;128;128;128,7;-1;-1;85;85;85,5
4484,4484,4484,4484,4484,4484,4484,4484,ICLR,2020,Mean Field Models for Neural Networks in Teacher-student Setting,Lexing Ying;Yuandong Tian,lexing@stanford.edu;yuandong@fb.com,3;3;1,I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Stanford University;Facebook,4;-1,4;-1,
4485,4485,4485,4485,4485,4485,4485,4485,ICLR,2020,A novel Bayesian estimation-based word embedding model for sentiment analysis,Jingyao Tang;Yun Xue;Ziwen Wang;Haoliang Zhao,manderous@foxmail.com;995438712@qq.com;773473833@qq.com;1044012786@qq.com,6;1;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Australian National University;;;,108;-1;-1;-1,50;-1;-1;-1,3;11
4486,4486,4486,4486,4486,4486,4486,4486,ICLR,2020,Improving Gradient Estimation in Evolutionary Strategies With Past Descent Directions,Florian Meier;Asier Mujika;Marcelo Gauy;Angelika Steger,meierflo@inf.ethz.ch;asierm@inf.ethz.ch;marcelo.matheus@inf.ethz.ch;steger@inf.ethz.ch,3;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10;10,13;13;13;13,1
4487,4487,4487,4487,4487,4487,4487,4487,ICLR,2020,Few-Shot Few-Shot Learning and the role of Spatial Attention,Yann Lifchitz;Yannis Avrithis;Sylvaine Picard,yann.lifchitz@safrangroup.com;yannis@avrithis.net;sylvaine.picard@safrangroup.com,3;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,SAFRAN;INRIA;SAFRAN,-1;-1;-1,-1;-1;-1,6
4488,4488,4488,4488,4488,4488,4488,4488,ICLR,2020,Generative Cleaning Networks with Quantized Nonlinear Transform  for  Deep Neural Network Defense,Jianhe Yuan;Zhihai He,yuanjia@missouri.edu;hezhi@missouri.edu,1;8;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,10,0.0,yes,9/25/19,"University of Missouri, Columbia;University of Missouri, Columbia",390;390,424;424,5;4
4489,4489,4489,4489,4489,4489,4489,4489,ICLR,2020,Clustered Reinforcement Learning,Xiao Ma;Shen-Yi Zhao;Zhao-Heng Yin;Wu-Jun Li,max@lamda.nju.edu.cn;zhaosy@lamda.nju.edu.cn;zhaohengyin@gmail.com;liwujun@nju.edu.cn,3;6;3,I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Zhejiang University;Zhejiang University;;Zhejiang University,56;56;-1;56,107;107;-1;107,
4490,4490,4490,4490,4490,4490,4490,4490,ICLR,2020,Auto Completion of User Interface Layout Design Using Transformer-Based Tree Decoders,Yang Li;Julien Amelot;Xin Zhou;Samy Bengio;Si Si,liyang@google.com;jamelot@google.com;zhouxin@google.com;bengio@google.com;sisidaisy@google.com,3;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,10
4491,4491,4491,4491,4491,4491,4491,4491,ICLR,2020,Visual Imitation with Reinforcement Learning using Recurrent Siamese Networks,Glen Berseth;Christopher Pal,gberseth@gmail.com;christopher.pal@polymtl.ca,8;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,University of California Berkeley;Polytechnique Montreal,5;390,13;1397,
4492,4492,4492,4492,4492,4492,4492,4492,ICLR,2020,Deep Graph Translation,Xiaojie Guo;Lingfei Wu;Liang Zhao,xguo7@gmu.edu;wuli@us.ibm.com;lzhao9@gmu.edu,3;3;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,George Mason University;International Business Machines;George Mason University,100;-1;100,282;-1;282,5;4;10
4493,4493,4493,4493,4493,4493,4493,4493,ICLR,2020,GAN-based Gaussian Mixture Model Responsibility Learning,Wanming Huang;Shuai Jiang;Xuan Liang;Ian Oppermann;Richard Yi Da Xu,wanming.huang@student.uts.edu.au;shuai.jiang-1@student.uts.edu.au;xuan.liang@student.uts.edu.au;ianopper@outlook.com;yida.xu@uts.edu.au,1;3;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,University of Technology Sydney;University of Technology Sydney;University of Technology Sydney;NSW Government;University of Technology Sydney,108;108;108;-1;108,193;193;193;-1;193,5;4
4494,4494,4494,4494,4494,4494,4494,4494,ICLR,2020,CROSS-DOMAIN CASCADED DEEP TRANSLATION,Oren Katzir;Dani Lischinski;Daniel Cohen-Or,orenkatzir@mail.tau.ac.il;cohenor@gmail.com;danix3d@gmail.com,6;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Tel Aviv University;;Hebrew University of Jerusalem,35;-1;67,188;-1;216,4
4495,4495,4495,4495,4495,4495,4495,4495,ICLR,2020,Mean-field Behaviour of Neural Tangent Kernel for Deep Neural Networks,Soufiane Hayou;Arnaud Doucet;Judith Rousseau,soufiane.hayou@stats.ox.ac.uk;doucet@stats.ox.ac.uk;judith.rousseau@stats.ox.ac.uk,6;6;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,University of Oxford;University of Oxford;University of Oxford,50;50;50,1;1;1,
4496,4496,4496,4496,4496,4496,4496,4496,ICLR,2020,NORML: Nodal Optimization for Recurrent Meta-Learning,David van Niekerk,davidpetrus94@gmail.com,1;1;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,0,0.0,yes,9/25/19,University of the Witwatersrand,481,193,6
4497,4497,4497,4497,4497,4497,4497,4497,ICLR,2020,Programmable Neural Network Trojan for Pre-trained Feature Extractor,Yu Ji;Zinxin Liu;Xing Hu;Peiqi Wang;Youhui Zhang,jiy15@mails.tsinghua.edu.cn;liuzixin18@mails.tsinghua.edu.cn;xinghu@ucsb.edu;wpq14@mails.tsinghua.edu.cn;zyh02@tsinghua.edu.cn,3;1;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,3,0.0,yes,9/25/19,Tsinghua University;Tsinghua University;UC Santa Barbara;Tsinghua University;Tsinghua University,8;8;38;8;8,23;23;57;23;23,4
4498,4498,4498,4498,4498,4498,4498,4498,ICLR,2020,Contextual Inverse Reinforcement Learning,Philip Korsunsky;Stav Belogolovsky;Tom Zahavy;Chen Tessler;Shie Mannor,philip.korsunsky@gmail.com;stav.belo@gmail.com;tomzahavy@gmail.com;chen.tessler@gmail.com;shie@ee.technion.ac.il,6;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,13,0.0,yes,9/25/19,Technion;Technion;Technion;Technion;Technion,26;26;26;26;26,412;412;412;412;412,
4499,4499,4499,4499,4499,4499,4499,4499,ICLR,2020,Convolutional Bipartite Attractor Networks,Michael L. Iuzzolino;Yoram Singer;Michael C. Mozer,michael.iuzzolino@colorado.edu;yoram.singer@gmail.com;mcmozer@google.com,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,"University of Colorado, Boulder;;Google",44;-1;-1,123;-1;-1,5
4500,4500,4500,4500,4500,4500,4500,4500,ICLR,2020,DeepSimplex: Reinforcement Learning of Pivot Rules Improves the Efficiency of Simplex Algorithm in Solving Linear Programming Problems,Varun Suriyanarayana;Onur Tavaslioglu;Ankit B. Patel;Andrew J. Schaefer,vs478@cornell.edu;onur.tavaslioglu@bcm.edu;ankit.patel@bcm.edu;andrew.schaefer@rice.edu,1;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Cornell University;Baylor College of Medicine;Baylor College of Medicine;Rice University,7;-1;-1;84,19;-1;-1;105,
4501,4501,4501,4501,4501,4501,4501,4501,ICLR,2020,Temporal-difference learning for nonlinear value function approximation in the lazy training regime,Andrea Agazzi;Jianfeng Lu,agazzi@math.duke.edu;jianfeng@math.duke.edu,6;6;3;3,I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Duke University;Duke University,47;47,20;20,1
4502,4502,4502,4502,4502,4502,4502,4502,ICLR,2020,Off-policy Multi-step Q-learning,Gabriel Kalweit;Maria Huegle;Joschka Boedecker,kalweitg@cs.uni-freiburg.de;hueglem@informatik.uni-freiburg.de;jboedeck@informatik.uni-freiburg.de,1;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,14,0.0,yes,9/25/19,Universität Freiburg;Universität Freiburg;Universität Freiburg,118;118;118,85;85;85,1
4503,4503,4503,4503,4503,4503,4503,4503,ICLR,2020,Towards Interpretable Molecular Graph Representation Learning,Emmanuel Noutahi;Dominique Beani;Julien Horwood;Prudencio Tossou,emmanuel@invivoai.com;dominique@invivoai.com;julien@invivoai.com;prudencio@invivoai.com,6;1;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,5,0.0,yes,9/25/19,InVivo AI;InVivo AI;InVivo AI;InVivo AI,-1;-1;-1;-1,-1;-1;-1;-1,10
4504,4504,4504,4504,4504,4504,4504,4504,ICLR,2020,Effective Mechanism to Mitigate Injuries During NFL Plays ,Arraamuthan Arulanantham;Ahamed Arshad Ahamed Anzar;Gowshalini Rajalingam;Krusanth Ingran;Prasanna S. Haddela,anzanfas@gmail.com;arulanantham.arraamuthan@my.sliit.lk;it16113800@my.sliit.lk;krusanth7@gmail.com;prasanna@sliit.lk,1;1;1,I do not know much about this area.:N/A:I made a quick assessment of this paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Srilanka Institute of information Technology;Srilanka Institute of information Technology;Srilanka Institute of information Technology;;Srilanka Institute of information Technology,481;481;481;-1;481,1397;1397;1397;-1;1397,1
4505,4505,4505,4505,4505,4505,4505,4505,ICLR,2020,Ecological Reinforcement Learning,John D. Co-Reyes;Suvansh Sanjeev;Glen Berseth;Abhishek Gupta;Sergey Levine,jcoreyes@eecs.berkeley.edu;suvansh@berkeley.edu;gberseth@gmail.com;abhigupta@berkeley.edu;svlevine@eecs.berkeley.edu,1;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,5,0.0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5;5,13;13;13;13;13,
4506,4506,4506,4506,4506,4506,4506,4506,ICLR,2020,CP-GAN: Towards a Better Global Landscape of GANs,Ruoyu Sun;Tiantian Fang;Alex Schwing,ruoyus@illinois.edu;tf6@illinois.edu;aschwing@illinois.edu,8;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",3;3;3,48;48;48,5;1;9
4507,4507,4507,4507,4507,4507,4507,4507,ICLR,2020,Learning Semantically Meaningful Representations Through Embodiment,Viviane Clay;Peter König;Kai-Uwe Kühnberger;Gordon Pipa,vkakerbeck@uos.de;pkoenig@uos.de;kkuehnbe@uos.de;gpipa@uos.de,3;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,6,1.0,yes,9/25/19,University of Osnabrück;University of Osnabrück;University of Osnabrück;University of Osnabrück,323;323;323;323,1397;1397;1397;1397,
4508,4508,4508,4508,4508,4508,4508,4508,ICLR,2020,Graph Warp Module: an Auxiliary Module for Boosting the Power of Graph Neural Networks in Molecular Graph Analysis,Katsuhiko Ishiguro;Shin-ichi Maeda;Masanori Koyama,k.ishiguro.jp@ieee.org;ichi@preferred.jp;masomatics@preferred.jp,3;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:N/A:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:N/A:N/A,Reject,0,4,0.0,yes,9/25/19,"Preferred Networks, Inc.;Preferred Networks, Inc.;Preferred Networks, Inc.",-1;-1;-1,-1;-1;-1,8
4509,4509,4509,4509,4509,4509,4509,4509,ICLR,2020,End-To-End Input Selection for Deep Neural Networks,Stefan Oehmcke;Fabian Gieseke,stefan.oehmcke@gmail.com;fabian.gieseke@di.ku.dk,3;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,University of Copenhagen;University of Copenhagen,100;100,101;101,
4510,4510,4510,4510,4510,4510,4510,4510,ICLR,2020,Learning Curves for Deep Neural Networks: A field theory perspective,Omry Cohen;Or Malka;Zohar Ringel,omrycohen.38.talpiot@gmail.com;or.malka@mail.huji.ac.il;zohar.ringel@mail.huji.ac.il,1;3;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,;Hebrew University of Jerusalem;Hebrew University of Jerusalem,-1;67;67,-1;216;216,11
4511,4511,4511,4511,4511,4511,4511,4511,ICLR,2020,Regional based query in graph active learning,Abel Roy;Louzoun Yoram,royabel10@gmail.com;louzouy@math.biu.ac.il,1;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,2,0.0,yes,9/25/19,Bar Ilan University;Bar Ilan University,95;95,513;513,10
4512,4512,4512,4512,4512,4512,4512,4512,ICLR,2020,Analysis and Interpretation of Deep CNN Representations as Perceptual Quality Features,Taimoor Tariq;Munchurl Kim,taimoor.tariq@kaist.ac.kr;mkimee@kaist.ac.kr,3;6;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,481;481,110;110,2
4513,4513,4513,4513,4513,4513,4513,4513,ICLR,2020,Dynamical System Embedding for Efficient Intrinsically Motivated Artificial Agents,Ruihan Zhao;Stas Tiomkin;Pieter Abbeel,philipzhao@berkeley.edu;stas@berkeley.edu;pabbeel@cs.berkeley.edu,1;3;3,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,13;13;13,
4514,4514,4514,4514,4514,4514,4514,4514,ICLR,2020,Siamese Attention Networks,Hongyang Gao;Yaochen Xie;Shuiwang Ji,hongyang.gao@tamu.edu;ethanycx@tamu.edu;sji@tamu.edu,6;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,2,0.0,yes,9/25/19,Texas A&M;Texas A&M;Texas A&M,44;44;44,177;177;177,
4515,4515,4515,4515,4515,4515,4515,4515,ICLR,2020,Network Pruning for Low-Rank Binary Index,Dongsoo Lee;Se Jung Kwon;Byeongwook Kim;Parichay Kapoor;Gu-Yeon Wei,dslee3@gmail.com;mogndrewk@gmail.com;quddnr145@gmail.com;kparichay@gmail.com;gywei@g.harvard.edu,3;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A,Reject,0,0,0.0,yes,9/25/19,;Samsung;Samsung;;Harvard University,-1;-1;-1;-1;39,-1;-1;-1;-1;7,
4516,4516,4516,4516,4516,4516,4516,4516,ICLR,2020,Attention Privileged Reinforcement Learning for Domain Transfer,Sasha Salter;Dushyant Rao;Markus Wulfmeier;Raia Hadsell;Ingmar Posner,sasha@robots.ox.ac.uk;dushyantr@google.com;mwulfmeier@google.com;raia@google.com;ingmar@robots.ox.ac.uk,3;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,11,0.0,yes,9/25/19,University of Oxford;Google;Google;Google;University of Oxford,50;-1;-1;-1;50,1;-1;-1;-1;1,
4517,4517,4517,4517,4517,4517,4517,4517,ICLR,2020,Wildly Unsupervised Domain Adaptation and Its Powerful and Efficient Solution,Feng Liu;Jie Lu;Bo Han;Gang Niu;Guangquan Zhang;Masashi Sugiyama,feng.liu-2@student.uts.edu.au;jie.lu@uts.edu.au;bo.han@riken.jp;gang.niu@riken.jp;guangquan.zhang@uts.edu.au;sugi@k.u-tokyo.ac.jp,1;8;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,9,0.0,yes,9/25/19,University of Technology Sydney;University of Technology Sydney;RIKEN;RIKEN;University of Technology Sydney;The University of Tokyo,108;108;-1;-1;108;56,193;193;-1;-1;193;36,
4518,4518,4518,4518,4518,4518,4518,4518,ICLR,2020,"Deep RL for Blood Glucose Control: Lessons, Challenges, and Opportunities",Ian Fox;Joyce Lee;Rodica Busui;Jenna Wiens,ifox@umich.edu;joyclee@med.umich.edu;rpbusui@umich.edu;wiensj@umich.edu,3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,University of Michigan;University of Michigan;University of Michigan;University of Michigan,8;8;8;8,21;21;21;21,
4519,4519,4519,4519,4519,4519,4519,4519,ICLR,2020,Learning Likelihoods with Conditional Normalizing Flows ,Christina Winkler;Daniel Worrall;Emiel Hoogeboom;Max Welling,christina.winkler.94@gmail.com;d.e.worrall@uva.nl;e.hoogeboom@uva.nl;m.welling@uva.nl,3;6;6,I have read many papers in this area.:N/A:N/A:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,2,5,0.0,yes,9/25/19,University of Amsterdam;University of Amsterdam;University of Amsterdam;University of Amsterdam,172;172;172;172,62;62;62;62,5;2
4520,4520,4520,4520,4520,4520,4520,4520,ICLR,2020,Detecting Out-of-Distribution Inputs to Deep Generative Models Using Typicality,Eric Nalisnick;Akihiro Matsukawa;Yee Whye Teh;Balaji Lakshminarayanan,e.nalisnick@eng.cam.ac.uk;matsukaw@deshaw.com;ywteh@google.com;balajiln@google.com,3;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,University of Cambridge;Deshaw;Google;Google,71;-1;-1;-1,3;-1;-1;-1,5
4521,4521,4521,4521,4521,4521,4521,4521,ICLR,2020,A Mechanism of Implicit Regularization in Deep Learning,Masayoshi Kubo;Genki Sugiura;Kenta Shinzato;Momose Oyama,kubo@i.kyoto-u.ac.jp;sugiura.genki.42n@st.kyoto-u.ac.jp;shinzato.kenta.82r@st.kyoto-u.ac.jp;oyama.momose.75c@st.kyoto-u.ac.jp,3;3;1,I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:N/A:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,10,0.0,yes,9/25/19,Meiji University;Meiji University;Meiji University;Meiji University,481;481;481;481,332;332;332;332,1;8
4522,4522,4522,4522,4522,4522,4522,4522,ICLR,2020,Scaleable input gradient regularization for adversarial robustness,Chris Finlay;Adam M Oberman,christopher.finlay@mail.mcgill.ca;adam.oberman@mcgill.ca,3;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,12,0.0,yes,9/25/19,McGill University;McGill University,86;86,42;42,4
4523,4523,4523,4523,4523,4523,4523,4523,ICLR,2020,Weakly-Supervised Trajectory Segmentation for Learning Reusable Skills,Parsa Mahmoudieh;Trevor Darrell;Deepak Pathak,parsa.m@berkeley.edu;trevor@eecs.berkeley.edu;pathak@berkeley.edu,3;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,3,0.0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,13;13;13,6;2
4524,4524,4524,4524,4524,4524,4524,4524,ICLR,2020,Compositional Visual Generation with Energy Based Models,Yilun Du;Shuang Li;Igor Mordatch,yilundu@mit.edu;lishuang@mit.edu;mordatch@google.com,3;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,10,0.0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Google,2;2;-1,5;5;-1,8
4525,4525,4525,4525,4525,4525,4525,4525,ICLR,2020,Fourier networks for uncertainty estimates and out-of-distribution detection,Hartmut Maennel;Alexandru Țifrea,hartmutm@google.com;tifreaa@student.ethz.ch,3;6;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,2,3,0.0,yes,9/25/19,Google;Swiss Federal Institute of Technology,-1;10,-1;13,8
4526,4526,4526,4526,4526,4526,4526,4526,ICLR,2020,Finding Winning Tickets with Limited (or No) Supervision,Mathilde Caron;Ari Morcos;Piotr Bojanowski;Julien Mairal;Armand Joulin,mathilde@fb.com;arimorcos@gmail.com;bojanowski@fb.com;julien.mairal@inria.fr;ajoulin@fb.com,1;3;6;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,5,0.0,yes,9/25/19,Facebook;Facebook;Facebook;INRIA;Facebook,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,
4527,4527,4527,4527,4527,4527,4527,4527,ICLR,2020,DASGrad: Double Adaptive Stochastic Gradient,Kin Gutierrez;Cristian Challu;Jin Li;Artur Dubrawski,kdgutier@cs.cmu.edu;cchallu@cs.cmu.edu;jinl2@cs.cmu.edu;awd@cs.cmu.edu,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,2,0.0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1,27;27;27;27,6;9
4528,4528,4528,4528,4528,4528,4528,4528,ICLR,2020,Physics-Aware Flow Data Completion Using Neural Inpainting,Sebastien Foucher;Jingwei Tang;Vinicius da Costa de Azevedo;Byungsoo Kim;Markus Gross;Barbara Solenthaler,sfoucher@ethz.ch;jingwei.tang@inf.ethz.ch;vinicius.azevedo@inf.ethz.ch;kimby@inf.ethz.ch;grossm@inf.ethz.ch;solenthaler@inf.ethz.ch,1;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10;10;10;10,13;13;13;13;13;13,
4529,4529,4529,4529,4529,4529,4529,4529,ICLR,2020,Towards Modular Algorithm Induction,Daniel A. Abolafia;Rishabh Singh;Manzil Zaheer;Charles Sutton,danabo@google.com;rising@google.com;manzilzaheer@google.com;charlessutton@google.com,1;1;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,1,0.0,yes,9/25/19,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,
4530,4530,4530,4530,4530,4530,4530,4530,ICLR,2020,PROVABLY BENEFITS OF DEEP HIERARCHICAL RL,Zeyu Jia;Simon S. Du;Ruosong Wang;Mengdi Wang;Lin F. Yang,jiazy@pku.edu.cn;ssdu@ias.edu;ruosongw@andrew.cmu.edu;mengdiw@princeton.edu;linyang@ee.ucla.edu,1;3;3,I have published one or two papers in this area.:N/A:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:N/A:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,"Peking University;Institue for Advanced Study, Princeton;Carnegie Mellon University;Princeton University;University of California, Los Angeles",22;-1;1;31;20,24;-1;27;6;17,
4531,4531,4531,4531,4531,4531,4531,4531,ICLR,2020,Effects of Linguistic Labels on Learned Visual Representations in Convolutional Neural Networks: Labels matter!,Seoyoung Ahn;Gregory Zelinsky;Gary Lupyan,seoyoung.ahn@stonybrook.edu;gregory.zelinsky@stonybrook.edu;lupyan@wisc.edu,6;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,6,0.0,yes,9/25/19,"State University of New York, Stony Brook;State University of New York, Stony Brook;University of Southern California",41;41;31,304;304;62,
4532,4532,4532,4532,4532,4532,4532,4532,ICLR,2020,What Can Learned Intrinsic Rewards Capture?,Zeyu Zheng;Junhyuk Oh;Matteo Hessel;Zhongwen Xu;Manuel Kroiss;Hado van Hasselt;David Silver;Satinder Singh,zeyu@umich.edu;junhyuk@google.com;mtthss@google.com;zhongwen@google.com;makro@google.com;hado@google.com;davidsilver@google.com;baveja@google.com,6;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0.0,yes,9/25/19,University of Michigan;Google;Google;Google;Google;Google;Google;Google,8;-1;-1;-1;-1;-1;-1;-1,21;-1;-1;-1;-1;-1;-1;-1,1;6
4533,4533,4533,4533,4533,4533,4533,4533,ICLR,2020,QXplore: Q-Learning Exploration by Maximizing Temporal Difference Error,Riley Simmons-Edler;Ben Eisner;Daniel Yang;Anthony Bisulco;Eric Mitchell;Sebastian Seung;Daniel Lee,rileys@cs.princeton.edu;ben.a.eisner@gmail.com;daniel.yang17@gmail.com;arb426@cornell.edu;eric.anthony.mitchell95@gmail.com;sseung@princeton.edu;daniel.d.lee@samsung.com,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,5,0.0,yes,9/25/19,Princeton University;Samsung;;Cornell University;Stanford University;Princeton University;Samsung,31;-1;-1;7;4;31;-1,6;-1;-1;19;4;6;-1,4
4534,4534,4534,4534,4534,4534,4534,4534,ICLR,2020,Neural Clustering Processes,Ari Pakman;Yueqi Wang;Catalin Mitelut;JinHyung Lee;Liam Paninski,aripakman@gmail.com;yueqi.wang.pku@gmail.com;mitelutco@gmail.com;jl4303@columbia.edu;liam@stat.columbia.edu,3;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Columbia University;Columbia University;;Columbia University;Columbia University,15;15;-1;15;15,16;16;-1;16;16,5;11
4535,4535,4535,4535,4535,4535,4535,4535,ICLR,2020,Pre-training as Batch Meta Reinforcement Learning with tiMe ,Quan Vuong;Shuang Liu;Minghua Liu;Kamil Ciosek;Hao Su;Henrik Iskov Christensen,quan.hovuong@gmail.com;s3liu@eng.ucsd.edu;minghua@ucsd.edu;kamil.ciosek@microsoft.com;haosu@eng.ucsd.edu;hichristensen@ucsd.edu,3;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,10,0.0,yes,9/25/19,"University of California, San Diego;University of California, San Diego;University of California, San Diego;Microsoft;University of California, San Diego;University of California, San Diego",11;11;11;-1;11;11,31;31;31;-1;31;31,8
4536,4536,4536,4536,4536,4536,4536,4536,ICLR,2020,Neural Architecture Search in Embedding Space,chun-ting liu,jimliu741523@gmail.com,1;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,3,0.0,yes,9/25/19,,,,
4537,4537,4537,4537,4537,4537,4537,4537,ICLR,2020,Multi-Task Learning via Scale Aware Feature Pyramid Networks and Effective Joint Head,Feng Ni,nifeng@pku.edu.cn,3;3,I have published one or two papers in this area.:I did not assess the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,1,0.0,yes,9/25/19,Peking University,22,24,2
4538,4538,4538,4538,4538,4538,4538,4538,ICLR,2020,BANANAS: Bayesian Optimization with Neural Networks for Neural Architecture Search,Colin White;Willie Neiswanger;Yash Savani,crwhite@cs.cmu.edu;willie@cs.cmu.edu;yash@realityengines.ai,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:N/A:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,2,9,3.0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;,1;1;-1,27;27;-1,11
4539,4539,4539,4539,4539,4539,4539,4539,ICLR,2020,Isolating Latent Structure with Cross-population Variational Autoencoders,Joe Davison;Kristen A. Severson;Soumya Ghosh,jddavison@g.harvard.edu;kristen.severson@ibm.com;ghoshso@us.ibm.com,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Harvard University;International Business Machines;International Business Machines,39;-1;-1,7;-1;-1,5
4540,4540,4540,4540,4540,4540,4540,4540,ICLR,2020,Stabilizing Off-Policy Reinforcement Learning with Conservative Policy Gradients,Chen Tessler;Nadav Merlis;Shie Mannor,chen.tessler@gmail.com;merlis.nadav@gmail.com;shiemannor@gmail.com,3;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Technion;Technion;Technion,26;26;26,412;412;412,
4541,4541,4541,4541,4541,4541,4541,4541,ICLR,2020,When Covariate-shifted Data Augmentation Increases Test Error And How to Fix It,Sang Michael Xie*;Aditi Raghunathan*;Fanny Yang;John C. Duchi;Percy Liang,xie@cs.stanford.edu;aditir@stanford.edu;fannyang@stanford.edu;jduchi@stanford.edu;pliang@cs.stanford.edu,3;6;3,I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Stanford University;Stanford University;Stanford University;Stanford University;Stanford University,4;4;4;4;4,4;4;4;4;4,4;1
4542,4542,4542,4542,4542,4542,4542,4542,ICLR,2020,Learning robust visual representations using data augmentation invariance,Alex Hernandez-Garcia;Peter König;Tim C. Kietzmann,alexhg15@gmail.com;pkoenig@uos.de;t.kietzmann@donders.ru.nl,3;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,University of Osnabrück;University of Osnabrück;Radboud University Nijmegen,323;323;390,1397;1397;128,
4543,4543,4543,4543,4543,4543,4543,4543,ICLR,2020,Learning to Generate Grounded Visual Captions without Localization Supervision,Chih-Yao Ma;Yannis Kalantidis;Ghassan AlRegib;Peter Vajda;Marcus Rohrbach;Zsolt Kira,cyma@gatech.edu;ykalant@image.ntua.gr;vajdap@fb.com;alregib@gatech.edu;maroffm@gmail.com;zkira@gatech.edu,3;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0.0,yes,9/25/19,Georgia Institute of Technology;National Technical University of Athens;Facebook;Georgia Institute of Technology;Facebook;Georgia Institute of Technology,13;323;-1;13;-1;13,38;776;-1;38;-1;38,3
4544,4544,4544,4544,4544,4544,4544,4544,ICLR,2020,On Empirical Comparisons of Optimizers for Deep Learning,Dami Choi;Christopher J. Shallue;Zachary Nado;Jaehoon Lee;Chris J. Maddison;George E. Dahl,choidami@cs.toronto.edu;shallue@google.com;znado@google.com;jaehlee@google.com;cmaddis@google.com;gdahl@google.com,1;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,4,19,0.0,yes,9/25/19,"Department of Computer Science, University of Toronto;Google;Google;Google;Google;Google",18;-1;-1;-1;-1;-1,18;-1;-1;-1;-1;-1,
4545,4545,4545,4545,4545,4545,4545,4545,ICLR,2020,LEARNING DIFFICULT PERCEPTUAL TASKS WITH HODGKIN-HUXLEY NETWORKS,Alan Lockett;Ankit Patel;Paul Pfaffinger,alan.lockett@gmail.com;ankitp@bcm.edu;paulp@bcm.edu,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,;Baylor College of Medicine;Baylor College of Medicine,-1;-1;-1,-1;-1;-1,2
4546,4546,4546,4546,4546,4546,4546,4546,ICLR,2020,Adversarial Robustness Against the Union of Multiple Perturbation Models,Pratyush Maini;Eric Wong;Zico Kolter,pratyush.maini@gmail.com;ericwong@cs.cmu.edu;zkolter@cs.cmu.edu,3;6;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,2,9,0.0,yes,9/25/19,Indian Institute of Technology Delhi;Carnegie Mellon University;Carnegie Mellon University,118;1;1,441;27;27,4;8
4547,4547,4547,4547,4547,4547,4547,4547,ICLR,2020,LightPAFF: A Two-Stage Distillation Framework for Pre-training and Fine-tuning,Kaitao Song;Hao Sun;Xu Tan;Tao Qin;Jianfeng Lu;Hongzhi Liu;Tie-Yan Liu,kt.song@njust.edu.cn;sigmeta@pku.edu.cn;xuta@microsoft.com;taoqin@microsoft.com;lujf@njust.edu.cn;liuhz@pku.edu.cn;tyliu@microsoft.com,3;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:N/A:N/A,Reject,0,4,0.0,yes,9/25/19,Hong Kong University of Science and Technology;Peking University;Microsoft;Microsoft;Hong Kong University of Science and Technology;Peking University;Microsoft,39;22;-1;-1;39;22;-1,47;24;-1;-1;47;24;-1,3
4548,4548,4548,4548,4548,4548,4548,4548,ICLR,2020,LocalGAN: Modeling Local Distributions for Adversarial Response Generation,Zhen Xu;Baoxun Wang;Huan Zhang;Kexin Qiu;Deyuan Zhang;Chengjie Sun,xuzhenhit@gmail.com;baoxun.wang@gmail.com;zhanghuan123@pku.edu.cn;kq2131@columbia.edu;dyzhang@sau.edu.cn;cjsun@insun.hit.edu.cn,3;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0.0,yes,9/25/19,Harbin Institute of Technology;;Peking University;Columbia University;Tsinghua University;Harbin Institute of Technology,172;-1;22;15;8;172,424;-1;24;16;23;424,5;4
4549,4549,4549,4549,4549,4549,4549,4549,ICLR,2020,Partial Simulation for Imitation Learning,Nir Baram;Shie Mannor,nirb@campus.technion.ac.il;shie@ee.technion.ac.il,3;6;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,3,0.0,yes,9/25/19,Technion;Technion,26;26,412;412,
4550,4550,4550,4550,4550,4550,4550,4550,ICLR,2020,Progressive Upsampling Audio Synthesis via Effective Adversarial Training,Youngwoo Cho;Minwook Chang;Gerard Jounghyun Kim;Jaegul Choo,cyw314@gmail.com;fromme0528@gmail.com;gjkim@korea.ac.kr;jchoo@korea.ac.kr,1;6;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Korea University;;Korea University;Korea University,323;-1;323;323,179;-1;179;179,5
4551,4551,4551,4551,4551,4551,4551,4551,ICLR,2020,A Quality-Diversity Controllable GAN for Text Generation,Xingyu Lou;Kaihe Xu;Zhongliang Li;Tian Xia;Shaojun Wang;Jing Xiao,louxingyu83064256@163.com;xukaihenupt@gmail.com;zlli0520@gmail.com;summerrainet2008@gmail.com;swang.usa@gmail.com;jing.xiaoj@gmail.com,1;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Pingan (Shenzhen) Technology;;Google;;Wright State University;,-1;-1;-1;-1;481;-1,-1;-1;-1;-1;1397;-1,3;4;5
4552,4552,4552,4552,4552,4552,4552,4552,ICLR,2020,Unsupervised-Learning of time-varying features,Henrik Høeg;Matthias Brix;Oswin Krause,lvt956@alumni.ku.dk;brixmatthias@gmail.com;oswin.krause@di.ku.dk,1;1;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,4,0.0,yes,9/25/19,University of Copenhagen;University of Copenhagen;University of Copenhagen,100;100;100,101;101;101,5
4553,4553,4553,4553,4553,4553,4553,4553,ICLR,2020,Improving Visual Relation Detection using Depth Maps,Sahand Sharifzadeh;Sina Moayed Baharlou;Max Berrendorf;Rajat Koner;Volker Tresp,sharifzadeh@dbs.ifi.lmu.de;sina.baharlou@gmail.com;berrendorf@dbs.ifi.lmu.de;koner@dbs.ifi.lmu.de;volker.tresp@siemens.com,6;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,8,1.0,yes,9/25/19,Institut für Informatik;;Institut für Informatik;Institut für Informatik;Siemens Corporate Research,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,
4554,4554,4554,4554,4554,4554,4554,4554,ICLR,2020,DeepAGREL: Biologically plausible deep learning via direct reinforcement,Isabella Pozzi;Sander M. Bohte;Pieter R. Roelfsema,pozzi@cwi.nl;s.m.bohte@cwi.nl;p.roelfsema@nin.knaw.nl,6;6;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,4,0.0,yes,9/25/19,Centrum voor Wiskunde en Informatica;Centrum voor Wiskunde en Informatica;,-1;-1;-1,-1;-1;-1,
4555,4555,4555,4555,4555,4555,4555,4555,ICLR,2020,Learn Interpretable Word Embeddings Efficiently with von Mises-Fisher Distribution,Minghong Yao;Liansheng Zhuang;Houqiang Li;Jian Yang;Shafei Wang,mhyao1@mail.ustc.edu.cn;lszhuang@ustc.edu.cn;lihq@ustc.edu.cn;nanwuyaoshi@163.com;rockingsandstorm@163.com,8;1;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,University of Science and Technology of China;University of Science and Technology of China;University of Science and Technology of China;163;163,481;481;481;-1;-1,80;80;80;-1;-1,3;1
4556,4556,4556,4556,4556,4556,4556,4556,ICLR,2020,Mirror Descent View For Neural Network Quantization,Thalaiyasingam Ajanthan;Kartik Gupta;Philip H. S. Torr;Richard Hartley;Puneet K. Dokania,thalaiyasingam.ajanthan@anu.edu.au;kartik.gupta@anu.edu.au;phst@robots.ox.ac.uk;richard.hartley@anu.edu.au;puneet@robots.ox.ac.uk,3;6;3;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Australian National University;Australian National University;University of Oxford;Australian National University;University of Oxford,108;108;50;108;50,50;50;1;50;1,
4557,4557,4557,4557,4557,4557,4557,4557,ICLR,2020,Topological Autoencoders,Michael Moor;Max Horn;Bastian Rieck;Karsten Borgwardt,michael.moor@bsse.ethz.ch;max.horn@bsse.ethz.ch;bastian.rieck@bsse.ethz.ch;karsten.borgwardt@bsse.ethz.ch,6;8;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10;10,13;13;13;13,
4558,4558,4558,4558,4558,4558,4558,4558,ICLR,2020,Utilizing Edge Features in Graph Neural Networks via Variational Information Maximization,Pengfei Chen;Weiwen Liu;Chang-Yu Hsieh;Guangyong Chen;Pheng Ann Heng,chenpf.cuhk@gmail.com;wwliu@cse.cuhk.edu.hk;kimhsieh@tencent.com;gycchen@tencent.com;pheng@cse.cuhk.edu.hk,3;6;8;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,The Chinese University of Hong Kong;The Chinese University of Hong Kong;Tencent AI Lab;Tencent AI Lab;The Chinese University of Hong Kong,59;59;-1;-1;59,35;35;-1;-1;35,10
4559,4559,4559,4559,4559,4559,4559,4559,ICLR,2020,Redundancy-Free Computation Graphs for Graph Neural Networks,Zhihao Jia;Sina Lin;Rex Ying;Jiaxuan You;Jure Leskovec;Alex Aiken.,zhihao@cs.stanford.edu;silin@microsoft.com;rexying@stanford.edu;jiaxuan@stanford.edu;jure@cs.stanford.edu;aiken@cs.stanford.edu,3;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,5,0.0,yes,9/25/19,Stanford University;Microsoft;Stanford University;Stanford University;Stanford University;Stanford University,4;-1;4;4;4;4,4;-1;4;4;4;4,10
4560,4560,4560,4560,4560,4560,4560,4560,ICLR,2020,Long History Short-Term Memory for Long-Term Video Prediction,Wonmin Byeon;Jan Kautz,wonmin.byeon@gmail.com;jkautz@nvidia.com,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,17,0.0,yes,9/25/19,NVIDIA;NVIDIA,-1;-1,-1;-1,4
4561,4561,4561,4561,4561,4561,4561,4561,ICLR,2020,How the Softmax Activation Hinders the Detection of Adversarial and Out-of-Distribution Examples in Neural Networks,Jonathan Aigrain;Marcin Detyniecki,jonathan.aigrain@axa.com;marcin.detyniecki@axa.com,1;1;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,3,0.0,yes,9/25/19,AXA;AXA,-1;-1,-1;-1,4
4562,4562,4562,4562,4562,4562,4562,4562,ICLR,2020,Solving Packing Problems by Conditional Query Learning,Dongda Li;Changwei Ren;Zhaoquan Gu;Yuexuan Wang;Francis Lau,lidongda@gzhu.edu.cn;rcw@zju.edu.cn;zqgu@gzhu.edu.cn;amywang@zju.edu.cn;fcmlau@cs.hku.hk,1;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Tsinghua University;Zhejiang University;Tsinghua University;Zhejiang University;The University of Hong Kong,8;56;8;56;92,23;107;23;107;35,
4563,4563,4563,4563,4563,4563,4563,4563,ICLR,2020,Context Based Machine Translation With Recurrent Neural Network For English-Amharic Translation ,Yeabsira Asefa Ashengo;Rosa Tsegaye Aga;Surafel Lemma Abebe,yeabsira.asefa@aait.edu.et;rosatsegaye@gmail.com;surafel.lemma@aait.edu.et,1;1;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Addis Ababa Institute of Technology;;Addis Ababa Institute of Technology,-1;-1;-1,-1;-1;-1,3
4564,4564,4564,4564,4564,4564,4564,4564,ICLR,2020,"Long-term planning, short-term adjustments",Hamed Khorasgani;Chi Zhang;Chetan Gupta;Susumu Serita,hamed.khorasgani@hal.hitachi.com;chi.zhang@hal.hitachi.com;chetan.gupta@hal.hitachi.com;susumu.serita@hal.hitachi.com,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,4,0.0,yes,9/25/19,Hitachi America Ltd.;Hitachi America Ltd.;Hitachi America Ltd.;Computational Life Science Cluster,-1;-1;-1;-1,-1;-1;-1;-1,
4565,4565,4565,4565,4565,4565,4565,4565,ICLR,2020,Boosting Network: Learn by Growing Filters and Layers via SplitLBI,Zuyuan Zhong;Chen Liu;Yanwei Fu;Yuan Yao,zyzhong19@fudan.edu.cn;corwinliu9669@gmail.com;yanweifu@fudan.edu.cn;yuany@ust.hk,3;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,4,0.0,yes,9/25/19,Fudan University;Fudan University;Fudan University;The Hong Kong University of Science and Technology,79;79;79;39,109;109;109;47,2
4566,4566,4566,4566,4566,4566,4566,4566,ICLR,2020,MULTI-STAGE INFLUENCE FUNCTION,Hongge Chen;Si Si;Yang Li;Ciprian Chelba;Sanjiv Kumar;Duane Boning;Cho-Jui Hsieh,chenhg@mit.edu;sisidaisy@google.com;liyang@google.com;ciprianchelba@google.com;sanjivk@google.com;boning@mtl.mit.edu;chohsieh@cs.ucla.edu,6;3;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,"Massachusetts Institute of Technology;Google;Google;Google;Google;Massachusetts Institute of Technology;University of California, Los Angeles",2;-1;-1;-1;-1;2;20,5;-1;-1;-1;-1;5;17,3;2
4567,4567,4567,4567,4567,4567,4567,4567,ICLR,2020,$\ell_1$ Adversarial Robustness Certificates: a Randomized Smoothing Approach,Jiaye Teng;Guang-He Lee;Yang Yuan,2016110299@live.sufe.edu.cn;guanghe@csail.mit.edu;yuanyang@tsinghua.edu.cn,6;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,2,4,0.0,yes,9/25/19,Shanghai University of Finance and Economics;Massachusetts Institute of Technology;Tsinghua University,266;2;8,1397;5;23,
4568,4568,4568,4568,4568,4568,4568,4568,ICLR,2020,Scaling Up Neural Architecture Search with Big Single-Stage Models,Jiahui Yu;Pengchong Jin;Hanxiao Liu;Gabriel Bender;Pieter-Jan Kindermans;Mingxing Tan;Thomas Huang;Xiaodan Song;Quoc Le,jyu79@illinois.edu;pengchong@google.com;hanxiaol@google.com;gbender@google.com;pikinder@google.com;tanmingxing@google.com;t-huang1@illinois.edu;xiaodansong@google.com;qvl@google.com,3;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,3,0.0,yes,9/25/19,"University of Illinois, Urbana Champaign;Google;Google;Google;Google;Google;University of Illinois, Urbana Champaign;Google;Google",3;-1;-1;-1;-1;-1;3;-1;-1,48;-1;-1;-1;-1;-1;48;-1;-1,
4569,4569,4569,4569,4569,4569,4569,4569,ICLR,2020,CONTRIBUTION OF INTERNAL REFLECTION IN LANGUAGE EMERGENCE WITH AN UNDER-RESTRICTED SITUATION,Kense Todo;Masayuki Yamamura,k_todo@ali.c.titech.ac.jp;my@c.titech.ac.jp,3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,3,0.0,yes,9/25/19,Tokyo Institute of Technology;Tokyo Institute of Technology,172;172,299;299,
4570,4570,4570,4570,4570,4570,4570,4570,ICLR,2020,MIM: Mutual Information Machine,Micha Livne;Kevin Swersky;David J. Fleet,mlivne@cs.toronto.edu;kswersky@google.com;leet@cs.toronto.edu,1;1;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,2,5,0.0,yes,9/25/19,"Department of Computer Science, University of Toronto;Google;Department of Computer Science, University of Toronto",18;-1;18,18;-1;18,5
4571,4571,4571,4571,4571,4571,4571,4571,ICLR,2020,Variable Complexity in the Univariate and Multivariate Structural Causal Model,Tomer Galanti;Ofir Nabati;Lior Wolf,tomerga2@post.tau.ac.il;ofirnabati@mail.tau.ac.il;wolf@fb.com,6;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Tel Aviv University;Tel Aviv University;Facebook,35;35;-1,188;188;-1,
4572,4572,4572,4572,4572,4572,4572,4572,ICLR,2020,Copy That! Editing Sequences by Copying Spans,Sheena Panthaplackel;Miltiadis Allamanis;Marc Brockschmidt,spantha@cs.utexas.edu;miallama@microsoft.com;mabrocks@microsoft.com,6;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,"University of Texas, Austin;Microsoft;Microsoft",22;-1;-1,38;-1;-1,3
4573,4573,4573,4573,4573,4573,4573,4573,ICLR,2020,OmniNet: A unified architecture for multi-modal multi-task learning,Subhojeet Pramanik;Priyanka Agrawal;Aman Hussain,subhojeetpramanik@gmail.com;pagrawal.ml@gmail.com;email@amanhussain.com,6;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,International Business Machines;;Amanhussain,-1;-1;-1,-1;-1;-1,3;8
4574,4574,4574,4574,4574,4574,4574,4574,ICLR,2020,QGAN: Quantize Generative Adversarial Networks to Extreme low-bits,Peiqi Wang;Yu Ji;Xinfeng Xie;Yongqiang Lyu;Dongsheng Wang;Yuan Xie,wpq14@tsinghua.org.cn;jiy15@mails.tsinghua.edu.cn;xinfeng@ucsb.edu;luyq@tsinghua.edu.cn;wds@mail.tsinghua.edu.cn;yuanxie@ucsb.edu,3;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,SenseTime Group Limited;Tsinghua University;UC Santa Barbara;Tsinghua University;Tsinghua University;UC Santa Barbara,-1;8;38;8;8;38,-1;23;57;23;23;57,5;4
4575,4575,4575,4575,4575,4575,4575,4575,ICLR,2020,Imagine That! Leveraging Emergent Affordances for Tool Synthesis in Reaching Tasks,Yizhe Wu;Sudhanshu Kasewa;Oliver Groth;Sasha Salter;Li Sun;Oiwi Parker Jones;Ingmar Posner,ywu@robots.ox.ac.uk;su@robots.ox.ac.uk;ogroth@robots.ox.ac.uk;sasha@robots.ox.ac.uk;kevin@robots.ox.ac.uk;oiwi.parkerjones@jesus.ox.ac.uk;ingmar@robots.ox.ac.uk,3;1;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0.0,yes,9/25/19,University of Oxford;University of Oxford;University of Oxford;University of Oxford;University of Oxford;University of Oxford;University of Oxford,50;50;50;50;50;50;50,1;1;1;1;1;1;1,5
4576,4576,4576,4576,4576,4576,4576,4576,ICLR,2020,Enhancing Attention with Explicit Phrasal Alignments,Xuan-Phi Nguyen;Shafiq Joty;Thanh-Tung Nguyen,nxphi47@gmail.com;sjoty@salesforce.com;ng0155ng@e.ntu.edu.sg,6;3;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,National Taiwan University;SalesForce.com;National Taiwan University,86;-1;86,120;-1;120,3
4577,4577,4577,4577,4577,4577,4577,4577,ICLR,2020,A GOODNESS OF FIT MEASURE FOR GENERATIVE NETWORKS,Lorenzo Luzi;Randall Balestriero;Richard Baraniuk,lorenzo.luzi.28@gmail.com;randallbalestriero@gmail.com;richb@rice.edu,3;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,4,0.0,yes,9/25/19,Rice University;Rice University;Rice University,84;84;84,105;105;105,5;4
4578,4578,4578,4578,4578,4578,4578,4578,ICLR,2020,Learning Neural Causal Models from Unknown Interventions,Nan Rosemary Ke;Olexa Bilaniuk;Anirudh Goyal;Stephan Bauer;Hugol Larochelle;Chris Pal;Yoshua Bengio,rosemary.nan.ke@gmail.com;obilaniu@gmail.com;anirudhgoyal9119@gmail.com;stefan.a.bauer@gmail.com;hugolarochelle@google.com;chris.j.pal@gmail.com;yoshua.bengio@mila.quebec,6;8;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,13,0.0,yes,9/25/19,Polytechnique Montreal;University of Montreal;University of Montreal;;Google;Ecole Polytechnique de Montreal;University of Montreal,390;128;128;-1;-1;390;128,1397;85;85;-1;-1;1397;85,6;10
4579,4579,4579,4579,4579,4579,4579,4579,ICLR,2020,SLM Lab: A Comprehensive Benchmark and Modular Software Framework for Reproducible Deep Reinforcement Learning,Wah Loon Keng;Laura Graesser;Milan Cvitkovic,kengzwl@gmail.com;lhgraesser@gmail.com;mcvitkov@caltech.edu,3;8;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Reject,0,3,0.0,yes,9/25/19,;;California Institute of Technology,-1;-1;143,-1;-1;2,
4580,4580,4580,4580,4580,4580,4580,4580,ICLR,2020,ADA+: A GENERIC FRAMEWORK WITH MORE ADAPTIVE EXPLICIT ADJUSTMENT FOR LEARNING RATE,Yue Zhao;Xiangsheng Huang;Ludan Kou,oasis.random.time@gmail.com;xiangsheng.huang@ia.ac.cn;2015019051@mail.buct.edu.cn,3;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,"Institute of automation, Chinese academy of science, Chinese Academy of Sciences;Institute of automation, Chinese academy of science, Chinese Academy of Sciences;Tsinghua University",59;59;8,1397;1397;23,8
4581,4581,4581,4581,4581,4581,4581,4581,ICLR,2020,A⋆MCTS: SEARCH WITH THEORETICAL GUARANTEE USING POLICY AND VALUE FUNCTIONS,Xian Wu;Yuandong Tian;Lexing Ying,xwu20@stanford.edu;yuandong@fb.com;lexing@stanford.edu,1;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Stanford University;Facebook;Stanford University,4;-1;4,4;-1;4,
4582,4582,4582,4582,4582,4582,4582,4582,ICLR,2020,Faster Neural Network Training with Data Echoing,Dami Choi;Alexandre Passos;Christopher J. Shallue;George E. Dahl,choidami@cs.toronto.edu;apassos@google.com;shallue@google.com;gdahl@google.com,6;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,"Department of Computer Science, University of Toronto;Google;Google;Google",18;-1;-1;-1,18;-1;-1;-1,1
4583,4583,4583,4583,4583,4583,4583,4583,ICLR,2020,Sparse Weight Activation Training,Md Aamir Raihan;Tor M. Aamodt,araihan@ece.ubc.ca;aamodt@ece.ubc.ca,3;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,9,0.0,yes,9/25/19,University of British Columbia;University of British Columbia,35;35,34;34,10
4584,4584,4584,4584,4584,4584,4584,4584,ICLR,2020,BOOSTING ENCODER-DECODER CNN FOR INVERSE PROBLEMS,Eunju Cha;Jaeduck Jang;Junho Lee;Eunha Lee;Jong Chul Ye,eunju.cha@kaist.ac.kr;jduck.jang@samsung.com;jh0325.lee@samsung.com;eunhayo.lee@samsung.com;jong.ye@kaist.ac.kr,6;1;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Korea Advanced Institute of Science and Technology;Samsung;Samsung;Samsung;Korea Advanced Institute of Science and Technology,481;-1;-1;-1;481,110;-1;-1;-1;110,
4585,4585,4585,4585,4585,4585,4585,4585,ICLR,2020,Unsupervised Spatiotemporal Data Inpainting,Yuan Yin;Arthur Pajot;Emmanuel de Bézenac;Patrick Gallinari,yuan.yin@lip6.fr;arthur.pajot@lip6.fr;emmanuel.de-bezenac@lip6.fr;patrick.gallinari@lip6.fr,3;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,LIP6;LIP6;LIP6;LIP6,-1;-1;-1;-1,-1;-1;-1;-1,5;4
4586,4586,4586,4586,4586,4586,4586,4586,ICLR,2020,Deep geometric matrix completion:  Are we doing it right?,Amit Boyarski;Sanketh Vedula;Alex Bronstein,amitboy@cs.technion.ac.il;sanketh@cs.technion.ac.il;bron@cs.technion.ac.il,3;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,1,6,0.0,yes,9/25/19,Technion;Technion;Technion,26;26;26,412;412;412,10
4587,4587,4587,4587,4587,4587,4587,4587,ICLR,2020,Predictive Coding for Boosting Deep Reinforcement Learning with Sparse Rewards,Xingyu Lu;Pieter Abbeel;Stas Tiomkin,xingyulu0701@berkeley.edu;pabbeel@cs.berkeley.edu;stas@berkeley.edu,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,3,0.0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,13;13;13,
4588,4588,4588,4588,4588,4588,4588,4588,ICLR,2020,Improved Structural Discovery and Representation Learning of Multi-Agent Data,Jennifer Hobbs;Matthew Holbrook;Nathan Frank;Long Sha;Patrick Lucey,jennifer.hobbs@statsperform.com;matthewholbrook@statsperform.com;nathan.frank@statsperform.com;long.sha@statsperform.com;patrick.lucey@statsperform.com,1;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,Stats Perform;Stats Perform;Stats Perform;Stats Perform;Stats Perform,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,
4589,4589,4589,4589,4589,4589,4589,4589,ICLR,2020,Laplacian Denoising Autoencoder,Jianbo Jiao;Linchao Bao;Yunchao Wei;Shengfeng He;Honghui Shi;Rynson Lau;Thomas Huang,jiaojianbo.i@gmail.com;linchaobao@gmail.com;wychao1987@gmail.com;shengfenghe7@gmail.com;shihonghui3@gmail.com;rynson.lau@cityu.edu.hk;t-huang1@illinois.edu,6;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,"University of Oxford;Tencent AI Lab;University of Technology Sydney;South China University of Technology;University of Oregon;City University of Hong Kong;University of Illinois, Urbana Champaign",50;-1;108;481;205;92;3,1;-1;193;501;288;35;48,
4590,4590,4590,4590,4590,4590,4590,4590,ICLR,2020,A Boolean Task Algebra for Reinforcement Learning,Geraud Nangue Tasse;Steven James;Benjamin Rosman,nanguetasse2000s@gmail.com;steven.james@wits.ac.za;brosman@csir.co.za,8;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,10,0.0,yes,9/25/19,University of the Witwatersrand;University of the Witwatersrand;CSIR,481;481;233,193;193;-1,1
4591,4591,4591,4591,4591,4591,4591,4591,ICLR,2020,"On Iterative Neural Network Pruning, Reinitialization, and the Similarity of Masks",Michela Paganini;Jessica Forde,michela@fb.com;jzf2101@columbia.edu,1;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,4,0.0,yes,9/25/19,Facebook;Columbia University,-1;15,-1;16,
4592,4592,4592,4592,4592,4592,4592,4592,ICLR,2020,Semantics Preserving Adversarial Attacks,Ousmane Amadou Dia;Elnaz Barshan;Reza Babanezhad,ousmane@elementai.com;elnaz.barshan@elementai.com;babanezhad@gmail.com,1;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Reject,0,15,0.0,yes,9/25/19,Element AI;Element AI;Samsung,-1;-1;-1,-1;-1;-1,5;4
4593,4593,4593,4593,4593,4593,4593,4593,ICLR,2020,Unified recurrent network for many feature types,Alexander Stec;Diego Klabjan;Jean Utke,stec@u.northwestern.edu;d-klabjan@northwestern.edu;jutke@allstate.com,1;1;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,0,0.0,yes,9/25/19,Northwestern University;Northwestern University;Allstate,44;44;-1,22;22;-1,
4594,4594,4594,4594,4594,4594,4594,4594,ICLR,2020,How noise affects the Hessian spectrum in overparameterized neural networks,Mingwei Wei;David Schwab,m.wei@u.northwestern.edu;dschwab@gc.cuny.edu,6;3;6,I have published in this field for several years.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,Northwestern University;The City College of New York,44;205,22;1397,8
4595,4595,4595,4595,4595,4595,4595,4595,ICLR,2020,Symmetric-APL Activations: Training Insights and Robustness to Adversarial Attacks,Mohammadamin Tavakoli;Forest Agostinelli;Pierre Baldi,mohamadt@uci.edu;fagostin@uci.edu;pfbaldi@ics.uci.edu,6;1;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,"University of California, Irvine;University of California, Irvine;University of California, Irvine",35;35;35,96;96;96,4
4596,4596,4596,4596,4596,4596,4596,4596,ICLR,2020,Temporal Probabilistic Asymmetric Multi-task Learning,Nguyen Anh Tuan;Hyewon Jeong;Eunho Yang;Sungju Hwang,nanhtuan@kaist.ac.kr;jhw162@kaist.ac.kr;eunhoy@kaist.ac.kr;sjhwang82@kaist.ac.kr,6;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,481;481;481;481,110;110;110;110,10
4597,4597,4597,4597,4597,4597,4597,4597,ICLR,2020,Learning Deep-Latent Hierarchies by Stacking Wasserstein Autoencoders,Benoit Gaujac;Ilya Feige;David Barber,benoit.gaujac.16@ucl.ac.uk;ilya@faculty.ai;david.barber@ucl.ac.uk,1;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Reject,0,3,0.0,yes,9/25/19,University College London;;University College London,50;-1;50,15;-1;15,5
4598,4598,4598,4598,4598,4598,4598,4598,ICLR,2020,Feature Selection using Stochastic Gates,Yutaro Yamada;Ofir Lindenbaum;Sahand Negahban;Yuval Kluger,yutaro.yamada@yale.edu;ofirlin@gmail.com;sahand.negahban@yale.edu;yuval.kluger@yale.edu,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,3,0.0,yes,9/25/19,Yale University;Yale University;Yale University;Yale University,64;64;64;64,8;8;8;8,
4599,4599,4599,4599,4599,4599,4599,4599,ICLR,2020,Stablizing Adversarial Invariance Induction by Discriminator Matching,Yusuke Iwasawa;Kei Akuzawa;Yutaka Matsuo,iwasawa@weblab.t.u-tokyo.ac.jp;akuzawa-kei@weblab.t.u-tokyo.ac.jp;matsuo@weblab.t.u-tokyo.ac.jp,3;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,The University of Tokyo;The University of Tokyo;The University of Tokyo,56;56;56,36;36;36,4;1;7;8
4600,4600,4600,4600,4600,4600,4600,4600,ICLR,2020,Revisiting Gradient Episodic Memory for Continual Learning,Zhiyi Chen;Tong Lin*,chenzhiy16@mails.tsinghua.edu.cn;lintong@pku.edu.cn,1;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Tsinghua University;Peking University,8;22,23;24,
4601,4601,4601,4601,4601,4601,4601,4601,ICLR,2020,"Unifying Question Answering, Text Classification, and Regression via Span Extraction",Nitish Shirish Keskar;Bryan McCann;Caiming Xiong;Richard Socher,nkeskar@salesforce.com;bmccann@salesforce.com;cxiong@salesforce.com;rsocher@salesforce.com,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,3,0.0,yes,9/25/19,SalesForce.com;SalesForce.com;SalesForce.com;SalesForce.com,-1;-1;-1;-1,-1;-1;-1;-1,
4602,4602,4602,4602,4602,4602,4602,4602,ICLR,2020,Reinforcement Learning with Probabilistically Complete Exploration,Philippe Morere;Tom Blau;Gilad Francis;Fabio Ramos,philippe.morere@sydney.edu.au;tom.blau@sydney.edu.au;gilad.francis@sydney.edu.au;fabio.ramos@sydney.edu.au,3;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,University of Sydney;University of Sydney;University of Sydney;University of Sydney,86;86;86;86,60;60;60;60,
4603,4603,4603,4603,4603,4603,4603,4603,ICLR,2020,Global Momentum Compression for Sparse Communication in Distributed SGD,Shen-Yi Zhao;Yin-Peng Xie;Hao Gao;Wu-Jun Li,zhaosy@lamda.nju.edu.cn;xieyp@lamda.nju.edu.cn;gaoh@lamda.nju.edu.cn;liwujun@nju.edu.cn,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,0,0.0,yes,9/25/19,Zhejiang University;Zhejiang University;Zhejiang University;Zhejiang University,56;56;56;56,107;107;107;107,1;9;8
4604,4604,4604,4604,4604,4604,4604,4604,ICLR,2020,Quantifying uncertainty with GAN-based priors,Dhruv V. Patel;Assad A. Oberai,dhruvvpa@usc.edu;aoberai@usc.edu,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,9,0.0,yes,9/25/19,University of Southern California;University of Southern California,31;31,62;62,5;4;11;2
4605,4605,4605,4605,4605,4605,4605,4605,ICLR,2020,Analyzing Privacy Loss in Updates of Natural Language Models,Shruti Tople;Marc Brockschmidt;Boris Köpf;Olga Ohrimenko;Santiago Zanella-Béguelin,t-shtopl@microsoft.com;mabrocks@microsoft.com;boris.koepf@microsoft.com;oohrim@microsoft.com;santiago@microsoft.com,6;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,Microsoft;Microsoft;Microsoft;Microsoft;Microsoft,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,3
4606,4606,4606,4606,4606,4606,4606,4606,ICLR,2020,Layer Flexible Adaptive Computation Time for Recurrent Neural Networks,Lida Zhang;Diego Klabjan,lidazhang2018@u.northwestern.edu;d-klabjan@northwestern.edu,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Northwestern University;Northwestern University,44;44,22;22,3
4607,4607,4607,4607,4607,4607,4607,4607,ICLR,2020,Gumbel-Matrix Routing for Flexible Multi-task Learning,Krzysztof Maziarz;Efi Kokiopoulou;Andrea Gesmundo;Luciano Sbaiz;Gabor Bartok;Jesse Berent,krzysztof.s.maziarz@gmail.com;kokiopou@google.com;agesmundo@google.com;sbaiz@google.com;bartok@google.com;jberent@google.com,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Jagiellonian University;Google;Google;Google;Google;Google,481;-1;-1;-1;-1;-1,610;-1;-1;-1;-1;-1,
4608,4608,4608,4608,4608,4608,4608,4608,ICLR,2020,"OPTIMAL TRANSPORT, CYCLEGAN, AND PENALIZED LS FOR UNSUPERVISED LEARNING IN INVERSE PROBLEMS",Byeongsu Sim;Gyutaek Oh;Sungjun Lim;and Jong Chul Ye,byeongsu.s@kaist.ac.kr;okt0711@kaist.ac.kr;sungjunlim@gmail.com;jong.ye@kaist.ac.kr,6;6;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,9,0.0,yes,9/25/19,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;;Korea Advanced Institute of Science and Technology,481;481;-1;481,110;110;-1;110,8;4;1;2;5
4609,4609,4609,4609,4609,4609,4609,4609,ICLR,2020,Stabilizing DARTS with Amended Gradient Estimation on Architectural Parameters,Kaifeng Bi;Changping Hu;Lingxi Xie;Xin Chen;Longhui Wei;Qi Tian,bikaifeng@huawei.com;huchangping@huawei.com;198808xc@gmail.com;1410452@tongji.edu.cn;weilonghui1@huawei.com;tian.qi1@huawei.com,3;3;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,12,0.0,yes,9/25/19,Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.;Tsinghua University;Huawei Technologies Ltd.;Huawei Technologies Ltd.,-1;-1;-1;8;-1;-1,-1;-1;-1;23;-1;-1,
4610,4610,4610,4610,4610,4610,4610,4610,ICLR,2020,Why ADAM Beats SGD for Attention Models	,Jingzhao Zhang;Sai Praneeth Karimireddy;Andreas Veit;Seungyeon Kim;Sashank J Reddi;Sanjiv Kumar;Suvrit Sra,jzhzhang@mit.edu;sai.karimrieddy@epfl.ch;aveit@google.com;seungyeonk@google.com;sashank@google.com;sanjivk@google.com;suvrit@mit.edu,3;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,4,0.0,yes,9/25/19,Massachusetts Institute of Technology;Swiss Federal Institute of Technology Lausanne;Google;Google;Google;Google;Massachusetts Institute of Technology,2;481;-1;-1;-1;-1;2,5;38;-1;-1;-1;-1;5,
4611,4611,4611,4611,4611,4611,4611,4611,ICLR,2020,Multi-Sample Dropout for Accelerated Training and Better Generalization,Hiroshi Inoue,inouehrs@jp.ibm.com,1;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,3,0.0,yes,9/25/19,International Business Machines,-1,-1,8
4612,4612,4612,4612,4612,4612,4612,4612,ICLR,2020,Prototype Recalls for Continual Learning,Mengmi Zhang;Tao Wang;Joo Hwee Lim;Jiashi Feng,mengmi@u.nus.edu;twangnh@gmail.com;joohwee@i2r.a-star.edu.sg;elefjia@nus.edu.sg,3;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:N/A:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,0,0.0,yes,9/25/19,National University of Singapore;National University of Singapore;A*STAR;National University of Singapore,16;16;-1;16,25;25;-1;25,6
4613,4613,4613,4613,4613,4613,4613,4613,ICLR,2020,Learning Surrogate Losses,Josif Grabocka;Randolf Scholz;Lars Schmidt-Thieme,josif@ismll.uni-hildesheim.de;rscholz@ismll.uni-hildesheim.de;schmidt-thieme@ismll.uni-hildesheim.de,8;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,10,0.0,yes,9/25/19,University of Hildesheim;University of Hildesheim;University of Hildesheim,390;390;390,1397;1397;1397,
4614,4614,4614,4614,4614,4614,4614,4614,ICLR,2020,Recurrent Independent Mechanisms,Anirudh Goyal;Alex Lamb;Shagun Sodhani;Jordan Hoffmann;Sergey Levine;Yoshua Bengio;Bernhard Scholkopf,anirudhgoyal9119@gmail.com;alex6200@gmail.com;sshagunsodhani@gmail.com;jhoffmann@g.harvard.edu;svlevine@eecs.berkeley.edu;yoshua.bengio@mila.quebec;bs@tuebingen.mpg.de,6;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,13,0.0,yes,9/25/19,"University of Montreal;;University of Montreal;Harvard University;University of California Berkeley;University of Montreal;Max Planck Institute for Intelligent Systems, Max-Planck Institute",128;-1;128;39;5;128;-1,85;-1;85;7;13;85;-1,8
4615,4615,4615,4615,4615,4615,4615,4615,ICLR,2020,Anomaly Detection Based on Unsupervised Disentangled Representation Learning in Combination with Manifold Learning,Xiaoyan Li;Iluju Kiringa;Tet Yeap;Xiaodan Zhu;Yifeng Li,xli343@uottawa.ca;iluju.kiringa@uottawa.ca;tyeap@uottawa.ca;xiaodan.zhu@queensu.ca;yifeng.li@nrc-cnrc.gc.ca,3;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,University of Ottawa;University of Ottawa;University of Ottawa;Queens University;National Research Council Canada,266;266;266;266;-1,141;141;141;258;-1,5
4616,4616,4616,4616,4616,4616,4616,4616,ICLR,2020,Stochastic Mirror Descent on Overparameterized Nonlinear Models,Navid Azizan;Sahin Lale;Babak Hassibi,azizan@caltech.edu;alale@caltech.edu;hassibi@caltech.edu,3;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,California Institute of Technology;California Institute of Technology;California Institute of Technology,143;143;143,2;2;2,8
4617,4617,4617,4617,4617,4617,4617,4617,ICLR,2020,On the implicit minimization of alternative loss functions when training deep networks,Alexandre Lemire Paquin;Brahim Chaib-draa;Philippe Giguère,alexandre.lemire-paquin.1@ulaval.ca;brahim.chaib-draa@ift.ulaval.ca;philippe.giguere@ift.ulaval.ca,1;3;3,I have read many papers in this area.:I did not assess the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,0,0.0,yes,9/25/19,Laval university;Laval university;Laval university,481;481;481,272;272;272,8
4618,4618,4618,4618,4618,4618,4618,4618,ICLR,2020,UWGAN: UNDERWATER GAN FOR REAL-WORLD UNDERWATER COLOR RESTORATION AND DEHAZING,Nan Wang;Yabin Zhou;Fenglei Han;Lichao Wan;Haitao Zhu;Yaojing Zheng,nanwangmail@hrbeu.edu.cn;zyb0977@163.com;fenglei_han@hrbeu.edu.cn;wanlch1203@hrbeu.edu.cn;zhuhaitao_heu@163.com;yaojingzheng_heu@163.com,3;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,5,0.0,yes,9/25/19,Tsinghua University;163;Tsinghua University;Tsinghua University;163;163,8;-1;8;8;-1;-1,23;-1;23;23;-1;-1,5;4
4619,4619,4619,4619,4619,4619,4619,4619,ICLR,2020,LEARNING  TO LEARN  WITH  BETTER  CONVERGENCE,Patrick H. Chen;Sashank Reddi;Sanjiv Kumar;Cho-Jui Hsieh,patrickchen@g.ucla.edu;sashank@google.com;sanjivk@google.com;chohsieh@cs.ucla.edu,3;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,"University of California, Los Angeles;Google;Google;University of California, Los Angeles",20;-1;-1;20,17;-1;-1;17,
4620,4620,4620,4620,4620,4620,4620,4620,ICLR,2020,Minimally distorted Adversarial Examples with a Fast Adaptive Boundary Attack,Francesco Croce;Matthias Hein,francesco91.croce@gmail.com;matthias.hein@uni-tuebingen.de,6;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,University of Tuebingen;University of Tuebingen,154;154,91;91,4
4621,4621,4621,4621,4621,4621,4621,4621,ICLR,2020,Octave Graph Convolutional Network,Heng Chang;Yu Rong;Somayeh Sojoudi;Junzhou Huang;Wenwu Zhu,changh17@mails.tsinghua.edu.cn;yu.rong@hotmail.com;sojoudi@berkeley.edu;jzhuang@uta.edu;wwzhu@tsinghua.edu.cn,6;3;3,I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,"Tsinghua University;Tencent AI Lab;University of California Berkeley;University of Texas, Arlington;Tsinghua University",8;-1;5;118;8,23;-1;13;708;23,1;10
4622,4622,4622,4622,4622,4622,4622,4622,ICLR,2020,"Lift-the-flap: what, where and when for context reasoning",Mengmi Zhang;Claire Tseng;Karla Montejo;Joseph Kwon;Gabriel Kreiman,mengmi.zhang@childrens.harvard.edu;ctseng@college.harvard.edu;kmont057@fiu.edu;joseph.kwon@yale.edu;gabriel.kreiman@tch.harvard.edu,3;3;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,8,0.0,yes,9/25/19,"Harvard University;Harvard University;Indiana University, Bloomington;Yale University;Harvard University",39;39;73;64;39,7;7;134;8;7,1
4623,4623,4623,4623,4623,4623,4623,4623,ICLR,2020,The Dynamics of Signal Propagation in Gated Recurrent Neural Networks,Dar Gilboa;Bo Chang;Minmin Chen;Greg Yang;Samuel S. Schoenholz;Ed H. Chi;Jeffrey Pennington,dg2893@columbia.edu;bchang@stat.ubc.ca;minminc@google.com;gregyang@microsoft.com;schsam@google.com;edchi@google.com;jpennin@google.com,3;8;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I did not assess the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Columbia University;University of British Columbia;Google;Microsoft;Google;Google;Google,15;35;-1;-1;-1;-1;-1,16;34;-1;-1;-1;-1;-1,8
4624,4624,4624,4624,4624,4624,4624,4624,ICLR,2020,Regulatory Focus: Promotion and Prevention Inclinations in Policy Search,Lanxin Lei;Zhizhong Li;Xiaoyang Li;Cong Qiu;Dahua Lin,leilansen@gmail.com;lizz@sensetime.com;lixiaoyang@nbu.edu.cn;qiucong@sensetime.com;dhlin@ie.cuhk.edu.hk,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Tsinghua University;SenseTime Group Limited;Boston University;SenseTime Group Limited;The Chinese University of Hong Kong,8;-1;67;-1;59,23;-1;61;-1;35,
4625,4625,4625,4625,4625,4625,4625,4625,ICLR,2020,WaveFlow: A Compact Flow-based Model for Raw Audio,Wei Ping;Kainan Peng;Kexin Zhao;Zhao Song,weiping.thu@gmail.com,8;6;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,3,3,1.0,yes,9/25/19,Baidu,-1,-1,5
4626,4626,4626,4626,4626,4626,4626,4626,ICLR,2020,Identifying Weights and Architectures of Unknown ReLU Networks,David Rolnick;Konrad P. Kording,drolnick@seas.upenn.edu;koerding@gmail.com,3;1;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,1,5,0.0,yes,9/25/19,University of Pennsylvania;University of Pennsylvania,19;19,11;11,
4627,4627,4627,4627,4627,4627,4627,4627,ICLR,2020,On PAC-Bayes Bounds for Deep Neural Networks using the Loss Curvature,Konstantinos Pitas,konstantinos.pitas@epfl.ch,1;3;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,13,0.0,yes,9/25/19,Swiss Federal Institute of Technology Lausanne,481,38,1;8
4628,4628,4628,4628,4628,4628,4628,4628,ICLR,2020,RoBERTa: A Robustly Optimized BERT Pretraining Approach,Yinhan Liu;Myle Ott;Naman Goyal;Jingfei Du;Mandar Joshi;Danqi Chen;Omer Levy;Mike Lewis;Luke Zettlemoyer;Veselin Stoyanov,yinhanliu@fb.com;myleott@fb.com;namangoyal@instagram.com;jingfeidu@fb.com;mandar90@cs.washington.edu;danqic@cs.princeton.edu;omerlevy@gmail.com;mikelewis@fb.com;lsz@fb.com;ves@fb.com,6;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,4,0.0,yes,9/25/19,Facebook;Facebook;Instagram;Facebook;University of Washington;Princeton University;Facebook;Facebook;Facebook;Facebook,-1;-1;-1;-1;6;31;-1;-1;-1;-1,-1;-1;-1;-1;26;6;-1;-1;-1;-1,3
4629,4629,4629,4629,4629,4629,4629,4629,ICLR,2020,A Coordinate-Free Construction of Scalable Natural Gradient,Kevin Luk;Roger Grosse,kevin.kh.luk@gmail.com;rgrosse@cs.toronto.edu,3;3;3,I have published in this field for several years.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:N/A:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,"Borealis AI;Department of Computer Science, University of Toronto",-1;18,-1;18,
4630,4630,4630,4630,4630,4630,4630,4630,ICLR,2020,Evaluating and Calibrating Uncertainty Prediction in Regression Tasks,Dan Levi;Liran Gispan;Niv Giladi;Ethan Fetaya,danmlevi@gmail.com;liran.gispan@gm.com;giladiniv@gmail.com;ethanf@cs.toronto.edu,1;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,"General Motors;General Motors;Technion;Department of Computer Science, University of Toronto",-1;-1;26;18,-1;-1;412;18,2
4631,4631,4631,4631,4631,4631,4631,4631,ICLR,2020,Why Does the VQA Model Answer No?: Improving Reasoning through Visual and Linguistic Inference,Seungjun Jung;Junyoung Byun;Kyujin Shim;Changick Kim,seungjun45@kaist.ac.kr;bjyoung@kaist.ac.kr;kjshim1028@kaist.ac.kr;changick@kaist.ac.kr,6;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,3,0.0,yes,9/25/19,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,481;481;481;481,110;110;110;110,
4632,4632,4632,4632,4632,4632,4632,4632,ICLR,2020,Modeling Winner-Take-All Competition in Sparse Binary Projections,Wenye Li,wyli@cuhk.edu.cn,3;8;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Tsinghua University,8,23,
4633,4633,4633,4633,4633,4633,4633,4633,ICLR,2020,When Robustness Doesn’t Promote Robustness: Synthetic vs. Natural Distribution Shifts on ImageNet,Rohan Taori;Achal Dave;Vaishaal Shankar;Nicholas Carlini;Benjamin Recht;Ludwig Schmidt,rohantaori@berkeley.edu;achald@cs.cmu.edu;vaishaal@berkeley.edu;nicholas@carlini.com;brecht@berkeley.edu;ludwigschmidt2@gmail.com,3;6;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:N/A:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A,Reject,0,6,0.0,yes,9/25/19,University of California Berkeley;Carnegie Mellon University;University of California Berkeley;Carlini;University of California Berkeley;University of California Berkeley,5;1;5;-1;5;5,13;27;13;-1;13;13,4
4634,4634,4634,4634,4634,4634,4634,4634,ICLR,2020,Perturbations are not Enough: Generating Adversarial Examples with Spatial Distortions,He Zhao;Trung Le;Paul Montague;Olivier De Vel;Tamas Abraham;Dinh Phung,ethanhezhao@gmail.com;trunglm@monash.edu;paul.montague@dst.defence.gov.au;olivier.devel@dst.defence.gov.au;tamas.abraham@dst.defence.gov.au;dinh.phung@monash.edu,3;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,1,3,0.0,yes,9/25/19,Monash University;Monash University;;;;Monash University,118;118;-1;-1;-1;118,75;75;-1;-1;-1;75,4
4635,4635,4635,4635,4635,4635,4635,4635,ICLR,2020,Adaptive Data Augmentation with Deep Parallel Generative Models,Boli Fang;Miao Jiang;Abhirag Nagpure;Jerry Shen,bfang@iu.edu;miajiang@iu.edu;anagpure@iu.edu;hashen@iu.edu,1;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,"Indiana University, Bloomington;Indiana University, Bloomington;Indiana University, Bloomington;Indiana University, Bloomington",73;73;73;73,134;134;134;134,5;2
4636,4636,4636,4636,4636,4636,4636,4636,ICLR,2020,Visual Explanation for Deep Metric Learning,Sijie Zhu;Taojiannan Yang;Chen Chen,szhu3@uncc.edu;tyang30@uncc.edu;chen.chen@uncc.edu,6;3;8,I have published one or two papers in this area.:I carefully checked the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,"University of North Carolina, Charlotte;University of North Carolina, Charlotte;University of North Carolina, Charlotte",73;73;73,1397;1397;1397,
4637,4637,4637,4637,4637,4637,4637,4637,ICLR,2020,Continual Learning via Neural Pruning,Siavash Golkar;Micheal Kagan;Kyunghyun Cho,siavash.golkar@gmail.com;makagan@slac.stanford.edu;kyunghyun.cho@nyu.edu,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Flatiron Institute;Stanford University;New York University,-1;4;25,-1;4;29,
4638,4638,4638,4638,4638,4638,4638,4638,ICLR,2020,Continual Learning using the SHDL Framework with Skewed Replay Distributions,Amarjot Singh;Jay McClelland,as2436@stanford.edu;jlmcc@stanford.edu,1;1;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Stanford University;Stanford University,4;4,4;4,
4639,4639,4639,4639,4639,4639,4639,4639,ICLR,2020,Goten: GPU-Outsourcing Trusted Execution of Neural Network Training and Prediction,Lucien K.L. Ng;Sherman S.M. Chow;Anna P.Y. Woo;Donald P. H. Wong;Yongjun Zhao,nkl018@ie.cuhk.edu.hk;smchow@ie.cuhk.edu.hk;woopuiyung@gmail.com;foreverjun.zhao@gmail.com,1;6;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,The Chinese University of Hong Kong;The Chinese University of Hong Kong;The Chinese University of Hong Kong;National Taiwan University,59;59;59;86,35;35;35;120,
4640,4640,4640,4640,4640,4640,4640,4640,ICLR,2020,One-way prototypical networks,Anna Kruspe,anna.kruspe@dlr.de,8;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,German Aerospace Center (DLR),-1,-1,6
4641,4641,4641,4641,4641,4641,4641,4641,ICLR,2020,FLUID FLOW MASS TRANSPORT FOR GENERATIVE NETWORKS,Jingrong Lin;Keegan Lensink;Eldad Haber,jlin@eoas.ubc.ca;klensink@eoas.ubc.ca;ehaber@eoas.ubc.ca,1;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,University of British Columbia;University of British Columbia;University of British Columbia,35;35;35,34;34;34,5;4
4642,4642,4642,4642,4642,4642,4642,4642,ICLR,2020,Improving Model Compatibility of Generative Adversarial Networks by Boundary Calibration,Si-An Chen;Chun-Liang Li;Hsuan-Tien Lin,r05922089@csie.ntu.edu.tw;chunlial@cs.cmu.edu;htlin@csie.ntu.edu.tw,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,2,0.0,yes,9/25/19,National Taiwan University;Carnegie Mellon University;National Taiwan University,86;1;86,120;27;120,5;4
4643,4643,4643,4643,4643,4643,4643,4643,ICLR,2020,Generative Imputation and Stochastic Prediction,Mohammad Kachuee;Kimmo Kärkkäinen;Orpaz Goldstein;Sajad Darabi;Majid Sarrafzadeh,mkachuee@ucla.edu;kimmo@cs.ucla.edu;orpgol@cs.ucla.edu;sajad.darabi@cs.ucla.edu;majid@cs.ucla.edu,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,9,0.0,yes,9/25/19,"University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles",20;20;20;20;20,17;17;17;17;17,
4644,4644,4644,4644,4644,4644,4644,4644,ICLR,2020,Denoising Improves Latent Space Geometry in Text Autoencoders,Tianxiao Shen;Jonas Mueller;Regina Barzilay;Tommi Jaakkola,tianxiao@mit.edu;jonasmue@amazon.com;regina@csail.mit.edu;tommi@csail.mit.edu,6;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,10,0.0,yes,9/25/19,Massachusetts Institute of Technology;Amazon;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;-1;2;2,5;-1;5;5,3;4;1
4645,4645,4645,4645,4645,4645,4645,4645,ICLR,2020,Information-Theoretic Local Minima Characterization and Regularization,Zhiwei Jia;Hao Su,zjia@ucsd.edu;haosu@eng.ucsd.edu,1;8;3,I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,18,0.0,yes,9/25/19,"University of California, San Diego;University of California, San Diego",11;11,31;31,1;8
4646,4646,4646,4646,4646,4646,4646,4646,ICLR,2020,Wider Networks Learn Better Features,Dar Gilboa;Guy Gur-Ari,dg2893@columbia.edu;guyga@google.com,3;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,0,0.0,yes,9/25/19,Columbia University;Google,15;-1,16;-1,
4647,4647,4647,4647,4647,4647,4647,4647,ICLR,2020,GENERALIZATION GUARANTEES FOR NEURAL NETS VIA HARNESSING THE LOW-RANKNESS OF JACOBIAN,Samet Oymak;Zalan Fabian;Mingchen Li;Mahdi Soltanolkotabi,sametoymak@gmail.com;zfabian@usc.edu;mli176@ucr.edu;msoltoon@gmail.com,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,"University of California, Riverside;University of Southern California;University of California, Riverside;University of Southern California",59;31;59;31,249;62;249;62,8
4648,4648,4648,4648,4648,4648,4648,4648,ICLR,2020,Implicit competitive regularization in GANs,Florian Schaefer;Hongkai Zheng;Anima Anandkumar,florian.schaefer@caltech.edu;devzhk@sjtu.edu.cn;anima@caltech.edu,6;6;8;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,California Institute of Technology;Shanghai Jiao Tong University;California Institute of Technology,143;53;143,2;157;2,5;4
4649,4649,4649,4649,4649,4649,4649,4649,ICLR,2020,TWO-STEP UNCERTAINTY NETWORK FOR TASKDRIVEN SENSOR PLACEMENT,Yangyang Sun;Yang Zhang;Hassan Foroosh;Shuo Pang,yangyang@knights.ucf.edu;yangzhang@knights.ucf.edu;foroosh@cs.ucf.edu;pang@creol.ucf.edu,1;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,University of Central Florida;University of Central Florida;University of Central Florida;University of Central Florida,77;77;77;77,609;609;609;609,5
4650,4650,4650,4650,4650,4650,4650,4650,ICLR,2020,Efficient Content-Based Sparse Attention with Routing Transformers,Aurko Roy*;Mohammad Taghi Saffar*;David Grangier;Ashish Vaswani,aurkor@google.com;msaffar@google.com;grangier@google.com;avaswani@google.com,3;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,2,0.0,yes,9/25/19,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,3
4651,4651,4651,4651,4651,4651,4651,4651,ICLR,2020,Implicit Rugosity Regularization via Data Augmentation,Daniel LeJeune;Randall Balestriero;Hamid Javadi;Richard G. Baraniuk,dlejeune@rice.edu;randallbalestriero@gmail.com;hh35@rice.edu;richb@rice.edu,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Rice University;Rice University;Rice University;Rice University,84;84;84;84,105;105;105;105,8
4652,4652,4652,4652,4652,4652,4652,4652,ICLR,2020,Balancing Cost and Benefit with Tied-Multi Transformers,Raj Dabre;Raphael Rubino;Atsushi Fujita,raj.dabre@nict.go.jp;raphael.rubino@nict.go.jp;fujita@paraphrasing.org,1;6;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,"National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology;National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology;",-1;-1;-1,-1;-1;-1,3
4653,4653,4653,4653,4653,4653,4653,4653,ICLR,2020,Learning to Learn Kernels with Variational Random Features,Haoliang Sun;Yingjun Du;Jun Xu;Yilong Yin;Xiantong Zhen;Ling Shao,haolsun.cn@gmail.com;duyingjun@buaa.edu.cn;nankaimathxujun@gmail.com;ylyin@sdu.edu.cn;zhenxt@gmail.com;ling.shao@ieee.org,6;6;8,I have published one or two papers in this area.:I carefully checked the experiments.:N/A:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,Shandong University;Beihang University;Nankai University;Shandong University;Inception Institute of Artificial Intelligence;Inception Institute of Artificial Intelligence,154;118;481;154;-1;-1,658;594;366;658;-1;-1,6
4654,4654,4654,4654,4654,4654,4654,4654,ICLR,2020,Learning Numeral Embedding,Chengyue Jiang;Zhonglin Nian;Kaihao Guo;Shanbo Chu;Yinggong Zhao;Libin Shen;Kewei Tu,jiangchy@shanghaitech.edu.cn;nianzhl@shanghaitech.edu.cn;guokh@shanghaitech.edu.cn;chushb@leyantech.com;ygzhao@leyantech.com;libin@leyantech.com;tukw@shanghaitech.edu.cn,3;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,ShanghaiTech University;ShanghaiTech University;ShanghaiTech University;Leyantech;Leyantech;Leyantech;ShanghaiTech University,481;481;481;-1;-1;-1;481,1397;1397;1397;-1;-1;-1;1397,3
4655,4655,4655,4655,4655,4655,4655,4655,ICLR,2020,Label Cleaning with Likelihood Ratio Test,Songzhu Zheng;Pengxiang Wu;Aman Goswami;Mayank Goswami;Dimitris Metaxas;Chao Chen,zheng.songzhu@stonybrook.edu;pxiangwu@gmail.com;ag77in@gmail.com;mayank.isi@gmail.com;dnm@cs.rutgers.edu;chao.chen.1@stonybrook.edu,8;3;3,I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,"State University of New York, Stony Brook;;;;Rutgers University;State University of New York, Stony Brook",41;-1;-1;-1;34;41,304;-1;-1;-1;168;304,11;1
4656,4656,4656,4656,4656,4656,4656,4656,ICLR,2020,Fast Task Adaptation for Few-Shot Learning,Yingying Zhang;Qiaoyong Zhong;Di Xie;Shiliang Pu,zhangyingying7@hikvision.com;zhongqiaoyong@hikvision.com;xiedi@hikvision.com;pushiliang@hikvision.com,8;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,6,7,0.0,yes,9/25/19,Hikvision Research Institute;Hikvision Research Institute;Hikvision Research Institute;Hikvision Research Institute,-1;-1;-1;-1,-1;-1;-1;-1,6;8
4657,4657,4657,4657,4657,4657,4657,4657,ICLR,2020,Using Objective Bayesian Methods to Determine the Optimal Degree of Curvature within the Loss Landscape,Devon Jarvis;Richard Klein;Benjamin Rosman,devonjarvi@gmail.com;kleinric@gmail.com;benjros@gmail.com,1;6;1,I have published in this field for several years.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,University of the Witwatersrand;;University of the Witwatersrand,481;-1;481,193;-1;193,11
4658,4658,4658,4658,4658,4658,4658,4658,ICLR,2020,VIDEO AFFECTIVE IMPACT PREDICTION WITH MULTIMODAL FUSION AND LONG-SHORT TEMPORAL CONTEXT,Yin Zhao;Longjun Cai;Chaoping Tu;Jie Zhang;Wu Wei,yinzhao.zy@alibaba-inc.com;longjun.clj@alibaba-inc.com;chaoping.tcp@alibaba-inc.com;auzj_alex@mail.scut.edu.cn;weiwu@scut.edu.cn,1;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:N/A:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Alibaba Group;Alibaba Group;Alibaba Group;South China University of Technology;South China University of Technology,-1;-1;-1;481;481,-1;-1;-1;501;501,
4659,4659,4659,4659,4659,4659,4659,4659,ICLR,2020,UNITER: Learning UNiversal Image-TExt Representations,Yen-Chun Chen;Linjie Li;Licheng Yu;Ahmed El Kholy;Faisal Ahmed;Zhe Gan;Yu Cheng;Jingjing Liu,yen-chun.chen@microsoft.com;lindsey.li@microsoft.com;licheng.yu@microsoft.com;ahmed.elkholy@microsoft.com;fiahmed@microsoft.com;zhe.gan@microsoft.com;yu.cheng@microsoft.com;jingjl@microsoft.com,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,12,0.0,yes,9/25/19,Microsoft;Microsoft;Microsoft;Microsoft;Microsoft;Microsoft;Microsoft;Microsoft,-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1,3
4660,4660,4660,4660,4660,4660,4660,4660,ICLR,2020,"Fast Linear Interpolation for Piecewise-Linear Functions, GAMs, and Deep Lattice Networks",Nathan Zhang;Kevin Canini;Sean Silva;and Maya R. Gupta,nzhang32@gmail.com;canini@google.com;silvasean@google.com;mayagupta@google.com,3;3;1,I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,3,0.0,yes,9/25/19,Stanford University;Google;Google;Google,4;-1;-1;-1,4;-1;-1;-1,
4661,4661,4661,4661,4661,4661,4661,4661,ICLR,2020,Knowledge Graph Embedding: A Probabilistic Perspective and Generalization Bounds,Ondrej Kuzelka;Yuyi Wang,kuzelo1@gmail.com;yuyiwang920@gmail.com,6;1;3,I do not know much about this area.:N/A:N/A:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,11,0.0,yes,9/25/19,Czech Technical University in Prague;Swiss Federal Institute of Technology,323;10,956;13,10;1;8
4662,4662,4662,4662,4662,4662,4662,4662,ICLR,2020,BUZz: BUffer Zones for defending  adversarial examples in image classification,Phuong Ha Nguyen*;Kaleel Mahmood*;Lam M. Nguyen;Thanh Nguyen;Marten van Dijk,phuongha.ntu@gmail.com;kaleel.mahmood@uconn.edu;lamnguyen.mltd@gmail.com;thanhng@iastate.edu;marten.van_dijk@uconn.edu,1;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,5,0.0,yes,9/25/19,University of Connecticut;University of Connecticut;International Business Machines;Iowa State University;University of Connecticut,154;154;-1;172;154,393;393;-1;399;393,4
4663,4663,4663,4663,4663,4663,4663,4663,ICLR,2020,Domain Aggregation Networks for Multi-Source Domain Adaptation,Junfeng Wen;Russell Greiner;Dale Schuurmans,junfengwen@gmail.com;rgreiner@ualberta.ca;daes@ualberta.ca,6;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,University of Alberta;University of Alberta;University of Alberta,100;100;100,136;136;136,8
4664,4664,4664,4664,4664,4664,4664,4664,ICLR,2020,Privacy-preserving Representation Learning by Disentanglement,Tassilo Klein;Moin Nabi,tassilo.klein@sap.com;m.nabi@sap.com,1;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,1,0,0.0,yes,9/25/19,SAP;SAP,323;323,258;258,
4665,4665,4665,4665,4665,4665,4665,4665,ICLR,2020,Angular Visual Hardness,Beidi Chen;Weiyang Liu;Animesh Garg;Zhiding Yu;Anshumali Shrivastava;Jan Kautz;Anima Anandkumar,beidi.chen@rice.edu;wyliu@gatech.edu;garg@cs.stanford.edu;zhidingy@nvidia.com;anshumali@rice.edu;jkautz@nvidia.com;anima@caltech.edu,1;8;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,Rice University;Georgia Institute of Technology;Stanford University;NVIDIA;Rice University;NVIDIA;California Institute of Technology,84;13;4;-1;84;-1;143,105;38;4;-1;105;-1;2,
4666,4666,4666,4666,4666,4666,4666,4666,ICLR,2020,Autoencoders and Generative Adversarial Networks for Imbalanced Sequence Classification,Stephanie Ger;Diego Klabjan,stephanieger@u.northwestern.edu;d-klabjan@northwestern.edu,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Northwestern University;Northwestern University,44;44,22;22,5;4
4667,4667,4667,4667,4667,4667,4667,4667,ICLR,2020,Multiagent Reinforcement Learning in Games with an Iterated Dominance Solution,Yoram Bachrach;Tor Lattimore;Marta Garnelo;Julien Perolat;David Balduzzi;Thomas Anthony;Satinder Singh;Thore Graepel,yorambac@gmail.com;lattimore@google.com;garnelo@google.com;perolat@google.com;dbalduzzi@google.com;twa@google.com;baveja@google.com;thore@google.com,1;6;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,8,0.0,yes,9/25/19,Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1,1
4668,4668,4668,4668,4668,4668,4668,4668,ICLR,2020,Blockwise Adaptivity:  Faster Training and Better Generalization in Deep Learning,Shuai Zheng;James T. Kwok,zs910504@gmail.com;jamesk@cse.ust.hk,3;6;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Amazon;The Hong Kong University of Science and Technology,-1;39,-1;47,9;8
4669,4669,4669,4669,4669,4669,4669,4669,ICLR,2020,Self-Educated Language Agent with Hindsight Experience Replay for Instruction Following,Geoffrey Cideron;Mathieu Seurin;Florian Strub;Olivier Pietquin,geoffrey.cideron@inria.fr;mathieu.seurin@inria.fr;fstrub@google.com;pietquin@google.com,6;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,8,0.0,yes,9/25/19,INRIA;INRIA;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,8
4670,4670,4670,4670,4670,4670,4670,4670,ICLR,2020,Policy path programming,Daniel McNamee,daniel.c.mcnamee@gmail.com,3;3;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,1,0.0,yes,9/25/19,University College London,50,15,8
4671,4671,4671,4671,4671,4671,4671,4671,ICLR,2020,Extreme Triplet Learning: Effectively Optimizing Easy Positives and Hard Negatives,Hong Xuan;Robert Pless,xuanhong@gwu.edu;pless@gwu.edu,3;8;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,2,0.0,yes,9/25/19,George Washington University;George Washington University,205;205,198;198,
4672,4672,4672,4672,4672,4672,4672,4672,ICLR,2020,MONET: Debiasing Graph Embeddings via the Metadata-Orthogonal Training Unit,John Palowitch;Bryan Perozzi,johnpalowitch@gmail.com;bperozzi@acm.org,6;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,;,-1;-1,-1;-1,7;10
4673,4673,4673,4673,4673,4673,4673,4673,ICLR,2020,DYNAMIC SELF-TRAINING FRAMEWORK  FOR GRAPH CONVOLUTIONAL NETWORKS,Ziang Zhou;Shenzhong Zhang;Zengfeng Huang,15300180085@fudan.edu.cn;17210980007@fudan.edu.cn;huangzf@fudan.edu.cn,3;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Fudan University;Fudan University;Fudan University,79;79;79,109;109;109,10
4674,4674,4674,4674,4674,4674,4674,4674,ICLR,2020,TOWARDS FEATURE SPACE ADVERSARIAL ATTACK,Qiuling Xu;Guanhong Tao;Siyuan Cheng;Lin Tan;Xiangyu Zhang,xu1230@purdue.edu;taog@purdue.edu;516030910472@sjtu.edu.cn;lintan@purdue.edu;xyzhang@cs.purdue.edu,6;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,3,0.0,yes,9/25/19,Purdue University;Purdue University;Shanghai Jiao Tong University;Purdue University;Purdue University,27;27;53;27;27,88;88;157;88;88,4
4675,4675,4675,4675,4675,4675,4675,4675,ICLR,2020,DIVA: Domain Invariant Variational Autoencoder,Maximilian Ilse;Jakub M. Tomczak;Christos Louizos;Max Welling,ilse.maximilian@gmail.com;jakubmkt@gmail.com;chr.louizos@gmail.com;welling.max@gmail.com,6;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,"University of Amsterdam;VU University Toronto;Qualcomm Inc, QualComm;University of California - Irvine",172;18;-1;35,62;18;-1;96,5;8
4676,4676,4676,4676,4676,4676,4676,4676,ICLR,2020,Optimal Attacks on Reinforcement Learning Policies,Alessio Russo;Alexandre Proutiere,alessior@kth.se;alepro@kth.se,3;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,"KTH Royal Institute of Technology, Stockholm, Sweden;KTH Royal Institute of Technology, Stockholm, Sweden",128;128,222;222,4
4677,4677,4677,4677,4677,4677,4677,4677,ICLR,2020,Recurrent Event Network : Global Structure Inference Over Temporal Knowledge Graph,Woojeong Jin;He Jiang;Meng Qu;Tong Chen;Changlin Zhang;Pedro Szekely;Xiang Ren,woojeong.jin@usc.edu;jian567@usc.edu;meng.qu@umontreal.ca;tongc2@andrew.cmu.edu;changlin.zhang@usc.edu;pszekely@isi.edu;xiangren@usc.edu,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,9,0.0,yes,9/25/19,University of Southern California;University of Southern California;University of Montreal;Carnegie Mellon University;University of Southern California;USC/ISI;University of Southern California,31;31;128;1;31;-1;31,62;62;85;27;62;-1;62,10
4678,4678,4678,4678,4678,4678,4678,4678,ICLR,2020,Continual Learning with Gated Incremental Memories for Sequential Data Processing,Andrea Cossu;Antonio Carta;Davide Bacciu,cossu48@gmail.com;antonio.carta@di.unipi.it;bacciu@di.unipi.it,3;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,3,0.0,yes,9/25/19,Scuola Normale Superiore;University of Pisa;University of Pisa,481;233;233,152;366;366,
4679,4679,4679,4679,4679,4679,4679,4679,ICLR,2020,Context-Aware Object Detection With Convolutional Neural Networks,Yizhou Yan;Lei Cao;Samuel Madden;Elke Rundensteiner,yyan2@wpi.edu;lcao@csail.mit.edu;madden@csail.mit.edu;rundenst@cs.wpi.edu,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Worcester Polytechnic Institute;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Worcester Polytechnic Institute,172;2;2;172,628;5;5;628,2
4680,4680,4680,4680,4680,4680,4680,4680,ICLR,2020,Resizable Neural Networks,Yichen Zhu;Xiangyu Zhang;Tong Yang;Jian Sun,k.zhu@mail.utoronto.ca;zhangxiangyu@megvii.com;yangtong@megvii.com;sunjian@megvii.com,3;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Toronto University;Megvii Technology Inc.;Megvii Technology Inc.;Megvii Technology Inc.,18;-1;-1;-1,18;-1;-1;-1,
4681,4681,4681,4681,4681,4681,4681,4681,ICLR,2020,SPROUT: Self-Progressing Robust Training,Minhao Cheng;Pin-Yu Chen;Sijia Liu;Shiyu Chang;Cho-Jui Hsieh;Payel Das,mhcheng@ucla.edu;pin-yu.chen@ibm.com;sijia.liu@ibm.com;shiyu.chang@ibm.com;chohsieh@cs.ucla.edu;daspa@us.ibm.com,3;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,"University of California, Los Angeles;International Business Machines;International Business Machines;International Business Machines;University of California, Los Angeles;International Business Machines",20;-1;-1;-1;20;-1,17;-1;-1;-1;17;-1,4
4682,4682,4682,4682,4682,4682,4682,4682,ICLR,2020,FALCON: Fast and Lightweight Convolution for Compressing and Accelerating CNN,Chun Quan;Jun-Gi Jang;Hyun Dong Lee;U Kang,chunquan_cs@outlook.com;elnino4@snu.ac.kr;hyundonglee1015@gmail.com;ukang@snu.ac.kr,6;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Seoul National University;Seoul National University;;Seoul National University,41;41;-1;41,64;64;-1;64,
4683,4683,4683,4683,4683,4683,4683,4683,ICLR,2020,Conditional Invertible Neural Networks for Guided Image Generation,Lynton Ardizzone;Carsten Lüth;Jakob Kruse;Carsten Rother;Ullrich Köthe,lynton.ardizzone@iwr.uni-heidelberg.de;clueth@live.de;jakob.kruse@iwr.uni-heidelberg.de;carsten.rother@iwr.uni-heidelberg.de;ullrich.koethe@iwr.uni-heidelberg.de,6;6;3;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:N/A:N/A,Reject,0,4,0.0,yes,9/25/19,Heidelberg University;;Heidelberg University;Heidelberg University;Heidelberg University,205;-1;205;205;205,44;-1;44;44;44,5
4684,4684,4684,4684,4684,4684,4684,4684,ICLR,2020,Domain-Independent Dominance of Adaptive Methods,Pedro Savarese;David McAllester;Sudarshan Babu;Michael Maire,savarese@ttic.edu;mcallester@ttic.edu;sudarshan@ttic.edu;mmaire@uchicago.edu,3;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0.0,yes,9/25/19,Toyota Technological Institute at Chicago;Toyota Technological Institute at Chicago;Toyota Technological Institute at Chicago;University of Chicago,-1;-1;-1;48,-1;-1;-1;9,3;8
4685,4685,4685,4685,4685,4685,4685,4685,ICLR,2020,RISE and DISE: Two Frameworks for Learning from Time Series with Missing Data,Alberto Garcia-Duran;Robert West,alberto.duran@epfl.ch;robert.west@epfl.ch,3;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,3,0.0,yes,9/25/19,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne,481;481,38;38,
4686,4686,4686,4686,4686,4686,4686,4686,ICLR,2020,Potential Flow Generator with $L_2$ Optimal Transport Regularity for Generative Models,Liu Yang;George Em Karniadakis,liu_yang@brown.edu;george_karniadakis@brown.edu,3;3;8,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,Brown University;Brown University,67;67,53;53,5
4687,4687,4687,4687,4687,4687,4687,4687,ICLR,2020,Power up! Robust Graph Convolutional Network based on Graph Powering,Ming Jin;Heng Chang;Wenwu Zhu;Somayeh Sojoudi,jinming@berkeley.edu;changh@berkeley.edu;wwzhu@tsinghua.edu.cn;sojoudi@berkeley.edu,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,7,0.0,yes,9/25/19,University of California Berkeley;University of California Berkeley;Tsinghua University;University of California Berkeley,5;5;8;5,13;13;23;13,4;10
4688,4688,4688,4688,4688,4688,4688,4688,ICLR,2020,DIME: AN INFORMATION-THEORETIC DIFFICULTY MEASURE FOR AI DATASETS,Peiliang Zhang;Huan Wang;Nikhil Naik;Caiming Xiong;Richard Socher,pez35@pitt.edu;huan.wang@salesforce.com;nnaik@salesforce.com;cxiong@salesforce.com;rsocher@salesforce.com,3;3;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,University of Pittsburgh;SalesForce.com;SalesForce.com;SalesForce.com;SalesForce.com,79;-1;-1;-1;-1,113;-1;-1;-1;-1,1
4689,4689,4689,4689,4689,4689,4689,4689,ICLR,2020,Improved Detection of Adversarial Attacks via Penetration Distortion Maximization,Shai Rozenberg;Gal Elidan;Ran El-Yaniv,shairoz@cs.technion.ac.il;elidan@google.com;elyaniv@google.com,3;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Technion;Google;Google,26;-1;-1,412;-1;-1,4
4690,4690,4690,4690,4690,4690,4690,4690,ICLR,2020,X-Forest: Approximate Random Projection Trees for Similarity Measurement,Yikai Zhao;Peiqing Chen;Zidong Zhao;Tong Yang;Jie Jiang;Bin Cui;Gong Zhang;Steve Uhlig,zyk@pku.edu.cn;chenpeiqing@pku.edu.cn;benkerd@pku.edu.cn;yangtongemail@gmail.com;jie.jiang@pku.edu.cn;bin.cui@pku.edu.cn;nicholas.zhang@huawei.com;steve.uhlig@qmul.ac.uk,1;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,0,0.0,yes,9/25/19,Peking University;Peking University;Peking University;;Peking University;Peking University;Huawei Technologies Ltd.;Queen Mary University London,22;22;22;-1;22;22;-1;233,24;24;24;-1;24;24;-1;110,
4691,4691,4691,4691,4691,4691,4691,4691,ICLR,2020,Out-of-Distribution Image Detection Using the Normalized Compression Distance,Sehun Yu;Donga Lee;Hwanjo Yu,hunu12@postech.ac.kr;dongha0914@postech.ac.kr;hwanjoyu@postech.ac.kr,3;3;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,POSTECH;POSTECH;POSTECH,118;118;118,146;146;146,
4692,4692,4692,4692,4692,4692,4692,4692,ICLR,2020,A Deep Recurrent Neural Network via Unfolding Reweighted l1-l1 Minimization,Huynh Van Luong;Duy Hung Le;Nikos Deligiannis,hvanluon@etrovub.be;dle@etrovub.be;ndeligia@etrovub.be,3;6;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Vrije Universiteit Brussel;Vrije Universiteit Brussel;Vrije Universiteit Brussel,481;481;481,235;235;235,8
4693,4693,4693,4693,4693,4693,4693,4693,ICLR,2020,Meta-Learning for Variational Inference,Ruqi Zhang;Yingzhen Li;Chris De Sa;Sam Devlin;Cheng Zhang,rz297@cornell.edu;yingzhen.li@microsoft.com;cdesa@cs.cornell.edu;sam.devlin@microsoft.com;cheng.zhang@microsoft.com,3;3;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,Cornell University;Microsoft;Cornell University;Microsoft;Microsoft,7;-1;7;-1;-1,19;-1;19;-1;-1,11;5;6
4694,4694,4694,4694,4694,4694,4694,4694,ICLR,2020,All Simulations Are Not Equal: Simulation Reweighing for Imperfect Information Games,Qucheng Gong;Yuandong Tian,qucheng@fb.com;yuandong@fb.com,1;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,3,0.0,yes,9/25/19,Facebook;Facebook,-1;-1,-1;-1,
4695,4695,4695,4695,4695,4695,4695,4695,ICLR,2020,GRASPEL: GRAPH SPECTRAL LEARNING AT SCALE,Yongyu Wang;Zhiqiang Zhao;Zhuo Feng,yongyuw@mtu.edu;qzzhao@mtu.edu;zfeng12@stevens.edu,1;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Michigan Technological University;Michigan Technological University;Stevens Institute of Technology,323;323;154,1397;1397;605,10
4696,4696,4696,4696,4696,4696,4696,4696,ICLR,2020,Latent Variables on Spheres for Sampling and Inference,Deli Zhao;Jiapeng Zhu;Bo Zhang,zhaodeli@gmail.com;jengzhu0@gmail.com;zhangbo@xiaomi.com,6;1;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,1,5,0.0,yes,9/25/19,;;Xiaomi,-1;-1;-1,-1;-1;-1,5;1
4697,4697,4697,4697,4697,4697,4697,4697,ICLR,2020,Detecting Change in Seasonal Pattern via Autoencoder and Temporal Regularization,Raphael Fettaya;Dor Bank;Rachel Lemberg;Linoy Barel,raphaelfettaya@gmail.com;doban@microsoft.com;rlemberg@microsoft.com;t-libare@microsoft.com,1;3;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Tel Aviv University;Microsoft;Microsoft;Microsoft,35;-1;-1;-1,188;-1;-1;-1,
4698,4698,4698,4698,4698,4698,4698,4698,ICLR,2020,Ladder Polynomial Neural Networks,Li-Ping Liu;Ruiyuan Gu;Xiaozhe Hu,liping.liu@tufts.edu;ruiyuan.gu@tufts.edu;xiaozhe.hu@tufts.edu,3;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Tufts University;Tufts University;Tufts University,172;172;172,139;139;139,
4699,4699,4699,4699,4699,4699,4699,4699,ICLR,2020,Universal Safeguarded Learned Convex Optimization with Guaranteed Convergence,Howard Heaton;Xiaohan Chen;Zhangyang Wang;Wotao Yin,heaton@math.ucla.edu;chernxh@tamu.edu;atlaswang@tamu.edu;wotao.yin@alibaba-inc.com,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,"University of California, Los Angeles;Texas A&M;Texas A&M;Alibaba Group",20;44;44;-1,17;177;177;-1,9
4700,4700,4700,4700,4700,4700,4700,4700,ICLR,2020,Knockoff-Inspired Feature Selection via Generative Models,Marco F. Duarte;Siwei Feng,mduarte@ecs.umass.edu;siwei@umass.edu,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,"University of Massachusetts, Amherst;University of Massachusetts, Amherst",28;28,209;209,5
4701,4701,4701,4701,4701,4701,4701,4701,ICLR,2020,Diagnosing the Environment Bias in Vision-and-Language Navigation,Yubo Zhang;Hao Tan;Mohit Bansal,zhangyb@cs.unc.edu;airsplay@cs.unc.edu;mbansal@cs.unc.edu,6;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,8,2.0,yes,9/25/19,"University of North Carolina, Chapel Hill;University of North Carolina, Chapel Hill;University of North Carolina, Chapel Hill",73;73;73,54;54;54,10
4702,4702,4702,4702,4702,4702,4702,4702,ICLR,2020,Entropy Minimization In Emergent Languages,Eugene Kharitonov;Rahma Chaabouni;Diane Bouchacourt;Marco Baroni,eugene.kharitonov@gmail.com;rchaabouni@fb.com;dianeb@fb.com;marco.baroni@unitn.it,1;6;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Reject,0,8,0.0,yes,9/25/19,Facebook;Facebook;Facebook;University of Trento,-1;-1;-1;18,-1;-1;-1;307,4
4703,4703,4703,4703,4703,4703,4703,4703,ICLR,2020,Improving SAT Solver Heuristics with Graph Networks and Reinforcement Learning,Vitaly Kurin;Saad Godil;Shimon Whiteson;Bryan Catanzaro,vitaliykurin@gmail.com;sgodil@nvidia.com;shimon.whiteson@gmail.com;bcatanzaro@nvidia.com,8;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,5,0.0,yes,9/25/19,University of Oxford;NVIDIA;University of Oxford;NVIDIA,50;-1;50;-1,1;-1;1;-1,1;10
4704,4704,4704,4704,4704,4704,4704,4704,ICLR,2020,Toward Understanding The Effect of Loss Function on The Performance of Knowledge Graph Embedding,Mojtaba Nayyeri;Chengjin Xu;Yadollah Yaghoobzadeh;Hamed Shariat Yazdi;Jens Lehmann,nayyeri@cs.uni-bonn.de;xuc@cs.uni-bonn.de;yayaghoo@microsoft.com;shariat@cs.uni-bonn.de;jens.lehmann@cs.uni-bonn.de,6;3;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0.0,yes,9/25/19,University of Bonn;University of Bonn;Microsoft;University of Bonn;University of Bonn,128;128;-1;128;128,106;106;-1;106;106,1;10
4705,4705,4705,4705,4705,4705,4705,4705,ICLR,2020,Multi-Precision Policy Enforced Training (MuPPET) : A precision-switching strategy for quantised fixed-point training of CNNs,Aditya Rajagopal;Diederik A. Vink;Stylianos I. Venieris;Christos-Savvas Bouganis,aditya.rajagopal14@imperial.ac.uk;diederik.vink14@imperial.ac.uk;stelios.ven10@gmail.com;christos-savvas.bouganis@imperial.ac.uk,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,3,0.0,yes,9/25/19,Imperial College London;Imperial College London;Samsung;Imperial College London,73;73;-1;73,10;10;-1;10,
4706,4706,4706,4706,4706,4706,4706,4706,ICLR,2020,R2D2: Reuse & Reduce via Dynamic Weight Diffusion for Training Efficient NLP Models,Yi Tay;Aston Zhang;Shuai Zhang;Alvin Chan;Luu Anh Tuan;Siu Cheung Hui,ytay017@e.ntu.edu.sg;astonz@amazon.com;cheungshuai@outlook.com;guoweial001@e.ntu.edu.sg;tuanluu@csail.mit.edu;asschui@ntu.edu.sg,3;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,National Taiwan University;Amazon;University of New South Wales;National Taiwan University;Massachusetts Institute of Technology;National Taiwan University,86;-1;481;86;2;86,120;-1;1397;120;5;120,3
4707,4707,4707,4707,4707,4707,4707,4707,ICLR,2020,Variational Autoencoders for Opponent Modeling in Multi-Agent Systems,Georgios Papoudakis;Stefano V. Albrecht,g.papoudakis@ed.ac.uk;s.albrecht@ed.ac.uk,3;1;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,6,0.0,yes,9/25/19,University of Edinburgh;University of Edinburgh,33;33,30;30,5
4708,4708,4708,4708,4708,4708,4708,4708,ICLR,2020,SELF-KNOWLEDGE DISTILLATION ADVERSARIAL ATTACK,Ma Xiaoxiong[1];Wang Renzhi[1];Tian Cong;Dong Zeqian;Duan Zhenhua,maxrumi@163.com;shanicky4ever@gmail.com;tico_tools@163.com;zqdong@stu.xidian.edu.cn;zhenhua_duan@126.com,3;1;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,6,0.0,yes,9/25/19,Tsinghua University;Tsinghua University;163;Tsinghua University;126,8;8;-1;8;-1,23;23;-1;23;-1,4;1
4709,4709,4709,4709,4709,4709,4709,4709,ICLR,2020,Continual Deep Learning by Functional Regularisation of Memorable Past,Pingbo Pan;Alexander Immer;Siddharth Swaroop;Runa Eschenhagen;Richard E Turner;Mohammad Emtiyaz Khan,pingbo.pan@student.uts.edu.au;alexander.immer@epfl.ch;ss2163@cam.ac.uk;reschenhagen@uni-osnabrueck.de;ret26@cam.ac.uk;emtiyaz.khan@riken.jp,1;1;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,University of Technology Sydney;Swiss Federal Institute of Technology Lausanne;University of Cambridge;Universität Osnabrück;University of Cambridge;RIKEN,108;481;71;481;71;-1,193;38;3;1397;3;-1,
4710,4710,4710,4710,4710,4710,4710,4710,ICLR,2020,Model Comparison of Beer data classification using an electronic nose,Mohammed Abdi;Aminat Adebiyi;Andrea Fasoli;Alberto Mannari;Ronald Labby;Luisa Bozano,mohammed.munir.abdi@ibm.com;aminat.adebiyi@ibm.com;andrea.fasoli@ibm.com;alberto.mannari@ibm.com;rlabby@us.ibm.com;lbozano@us.ibm.com,1;1;1,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:N/A:N/A:N/A,Reject,0,0,0.0,yes,9/25/19,International Business Machines;International Business Machines;International Business Machines;International Business Machines;International Business Machines;International Business Machines,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,
4711,4711,4711,4711,4711,4711,4711,4711,ICLR,2020,Deep Multi-View Learning via Task-Optimal CCA,Heather D. Couture;Roland Kwitt;J.S. Marron;Melissa Troester;Charles M. Perou;Marc Niethammer,heather@pixelscientia.com;roland.kwitt@gmail.com;marron@unc.edu;troester@unc.edu;chuck_perou@med.unc.edu;mn@cs.unc.edu,3;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,"Pixel Scientia Labs;University of Salzburg;University of North Carolina, Chapel Hill;University of North Carolina, Chapel Hill;University of North Carolina, Chapel Hill;University of North Carolina, Chapel Hill",-1;266;73;73;73;73,-1;1397;54;54;54;54,
4712,4712,4712,4712,4712,4712,4712,4712,ICLR,2020,Unsupervised Representation Learning by Predicting Random Distances,Hu Wang;Guansong Pang;Chunhua Shen;Congbo Ma,hu.wang@adelaide.edu.au;pangguansong@gmail.com;chunhua.shen@adelaide.edu.au;201520121828@mail.scut.edu.cn,6;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,The University of Adelaide;The University of Adelaide;The University of Adelaide;South China University of Technology,128;128;128;481,120;120;120;501,
4713,4713,4713,4713,4713,4713,4713,4713,ICLR,2020,Qgraph-bounded Q-learning: Stabilizing Model-Free Off-Policy Deep Reinforcement Learning,Sabrina Hoppe;Marc Toussaint,sabrina.hoppe@de.bosch.com;marc.toussaint@informatik.uni-stuttgart.de,6;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:N/A,Reject,0,4,0.0,yes,9/25/19,Bosch;University of Stuttgart,-1;95,-1;292,1;10
4714,4714,4714,4714,4714,4714,4714,4714,ICLR,2020,Neural Maximum Common Subgraph Detection with Guided Subgraph Extraction,Yunsheng Bai;Derek Xu;Ken Gu;Xueqing Wu;Agustin Marinovic;Christopher Ro;Yizhou Sun;Wei Wang,yba@ucla.edu;derekqxu@ucla.edu;ken.qgu@gmail.com;shirley0@mail.ustc.edu.cn;amarinovic@ucla.edu;christopher.j.ro@gmail.com;yzsun@cs.ucla.edu;weiwang@cs.ucla.edu,3;3;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,9,0.0,yes,9/25/19,"University of California, Los Angeles;University of California, Los Angeles;;University of Science and Technology of China;University of California, Los Angeles;;University of California, Los Angeles;University of California, Los Angeles",20;20;-1;481;20;-1;20;20,17;17;-1;80;17;-1;17;17,10
4715,4715,4715,4715,4715,4715,4715,4715,ICLR,2020,Blockwise Self-Attention for Long Document Understanding,Jiezhong Qiu;Hao Ma;Omer Levy;Scott Wen-tau Yih;Sinong Wang;Jie Tang,qiujz16@mails.tsinghua.edu.cn;gabe.hao.ma@gmail.com;omerlevy@gmail.com;scottyih@gmail.com;sinongwang@fb.com;jietang@tsinghua.edu.cn,3;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Tsinghua University;Facebook;Facebook;Facebook AI Research;Facebook;Tsinghua University,8;-1;-1;-1;-1;8,23;-1;-1;-1;-1;23,
4716,4716,4716,4716,4716,4716,4716,4716,ICLR,2020,GroSS Decomposition: Group-Size Series Decomposition for Whole Search-Space Training,Henry Howard-Jenkins;Yiwen Li;Victor Adrian Prisacariu,henryhj@robots.ox.ac.uk;kate@robots.ox.ac.uk;victor@robots.ox.ac.uk,3;6;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,University of Oxford;University of Oxford;University of Oxford,50;50;50,1;1;1,1
4717,4717,4717,4717,4717,4717,4717,4717,ICLR,2020,Emergence of Collective Policies Inside Simulations with Biased Representations,Jooyeon Kim;Alice Oh,jooyeon.kim@kaist.ac.kr;alice.oh@kaist.edu,3;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,1,0.0,yes,9/25/19,Korea Advanced Institute of Science and Technology;KAIST,481;20,110;110,
4718,4718,4718,4718,4718,4718,4718,4718,ICLR,2020,On unsupervised-supervised risk and one-class neural networks,Christophe Cerisara,cerisara@loria.fr,3;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,University of Lorraine,481,624,3;5
4719,4719,4719,4719,4719,4719,4719,4719,ICLR,2020,Shifted Randomized Singular Value Decomposition,Ali Basirat,ali.basirat@lingfil.uu.se,1;1;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Uppsala University,154,102,
4720,4720,4720,4720,4720,4720,4720,4720,ICLR,2020,Encoding Musical Style with Transformer Autoencoders,Kristy Choi;Curtis Hawthorne;Ian Simon;Monica Dinculescu;Jesse Engel,kechoi@cs.stanford.edu;fjord@google.com;iansimon@google.com;noms@google.com;jesseengel@google.com,3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Stanford University;Google;Google;Google;Google,4;-1;-1;-1;-1,4;-1;-1;-1;-1,3
4721,4721,4721,4721,4721,4721,4721,4721,ICLR,2020,Adaptive Adversarial Imitation Learning,Yiren Lu;Jonathan Tompson;Sergey Levine,luyiren92@gmail.com;tompson@google.com;slevine@google.com,6;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,;Google;Google,-1;-1;-1,-1;-1;-1,4
4722,4722,4722,4722,4722,4722,4722,4722,ICLR,2020,Adversarial Interpolation Training: A Simple Approach for Improving Model Robustness,Haichao Zhang;Wei Xu,hczhang1@gmail.com;wei.xu@horizon.ai,6;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,1,8,0.0,yes,9/25/19,Horizon Robotics;Horizon Robotics,-1;-1,-1;-1,4
4723,4723,4723,4723,4723,4723,4723,4723,ICLR,2020,Do Image Classifiers Generalize Across Time?,Vaishaal Shankar;Achal Dave;Rebecca Roelofs;Deva Ramanan;Ben Recht;Ludwig Schmidt,vaishaal@berkeley.edu;achald@cs.cmu.edu;roelofs@cs.berkely.edu;deva@cs.cmu.edu;brecht@berkeley.edu;ludwigschmidt2@gmail.com,3;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,University of California Berkeley;Carnegie Mellon University;;Carnegie Mellon University;University of California Berkeley;University of California Berkeley,5;1;-1;1;5;5,13;27;-1;27;13;13,
4724,4724,4724,4724,4724,4724,4724,4724,ICLR,2020,Manifold Learning and Alignment with Generative Adversarial Networks,Jiseob Kim;Seungjae Jung;Hyundo Lee;Byoung-Tak Zhang,jkim@bi.snu.ac.kr;sjjung@bi.snu.ac.kr;hdlee@bi.snu.ac.kr;btzhang@bi.snu.ac.kr,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,Seoul National University;Seoul National University;Seoul National University;Seoul National University,41;41;41;41,64;64;64;64,5;4
4725,4725,4725,4725,4725,4725,4725,4725,ICLR,2020,Count-guided Weakly Supervised Localization Based on Density Map,Ming Ma;Stephan Chalup;Fayeem Aziz;Yang Liu;Defu Cheng;Zhijian Zhou,mmingabc@outlook.com;stephan.chalup@newcastle.edu.au;mdfayeembin.aziz@uon.edu.au;liu15@mails.jlu.edu.cn;chengdefu@jlu.edu.cn;zhouzhijian@jlu.edu.cn,1;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,0,0.0,yes,9/25/19,"Jilin University;University of Newcastle, Australia;University of Newcastle, Australia;Jilin University;Jilin University;Jilin University",481;390;390;481;481;481,952;311;311;952;952;952,5
4726,4726,4726,4726,4726,4726,4726,4726,ICLR,2020,Neural Phrase-to-Phrase Machine Translation,Jiangtao;Feng;Lingpeng Kong;Po-sen Huang;Chong;Wang;Da;Huang Jiayuan;Mao;Kan;Qiao;Dengyong;Zhou,lingpenk@google.com,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Google,-1,-1,3
4727,4727,4727,4727,4727,4727,4727,4727,ICLR,2020,Walking the Tightrope: An Investigation of the Convolutional Autoencoder Bottleneck,Ilja Manakov;Markus Rohm;Volker Tresp,ilja.manakov@med.uni-muenchen.de;markus.rohm@med.uni-muenchen.de;volker.tresp@siemens.com,6;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,8,0.0,yes,9/25/19,Ludwig-Maximilians-Universität München;Ludwig-Maximilians-Universität München;Siemens Corporate Research,-1;-1;-1,-1;-1;-1,6;8
4728,4728,4728,4728,4728,4728,4728,4728,ICLR,2020,LEX-GAN: Layered Explainable Rumor Detector Based on Generative Adversarial Networks,Mingxi Cheng;Yizhi Li;Shahin Nazarian;Paul Bogdan,mingxic@usc.edu;yizhi.li@bupt.edu.cn;shahin.nazarian@usc.edu;pbogdan@usc.edu,3;1;8;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0.0,yes,9/25/19,University of Southern California;Beijing University of Post and Telecommunication;University of Southern California;University of Southern California,31;481;31;31,62;1397;62;62,5;4
4729,4729,4729,4729,4729,4729,4729,4729,ICLR,2020,A New Multi-input Model with the Attention Mechanism for Text Classification,Junhao Qiu;Ronghua Shi;Fangfang Li (the corresponding author);Jinjing Shi;Wangmin Liao,qiujunhao@csu.edu.cn;shirh@csu.edu.cn;lifangfang@csu.edu.cn;shijinjing@csu.edu.cn;0909123117@csu.edu.cn,1;1;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,0,0.0,yes,9/25/19,SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY,481;481;481;481;481,299;299;299;299;299,2
4730,4730,4730,4730,4730,4730,4730,4730,ICLR,2020,Stochastic Prototype Embeddings,Tyler R. Scott;Karl Ridgeway;Michael C. Mozer,tysc7237@colorado.edu;karl.ridgeway@colorado.edu;mcmozer@google.com,3;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,"University of Colorado, Boulder;University of Colorado, Boulder;Google",44;44;-1,123;123;-1,6
4731,4731,4731,4731,4731,4731,4731,4731,ICLR,2020,GraphNVP: an Invertible Flow-based Model for Generating Molecular Graphs,Kaushalya Madhawa;Katsuhiko Ishiguro;Kosuke Nakago;Motoki Abe,kaushalya@net.c.titech.ac.jp;k.ishiguro.jp@ieee.org;nakago@preferred.jp;motoki@preferred.jp,3;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,"Tokyo Institute of Technology;Preferred Networks, Inc.;Preferred Networks, Inc.;Preferred Networks, Inc.",172;-1;-1;-1,299;-1;-1;-1,5;10
4732,4732,4732,4732,4732,4732,4732,4732,ICLR,2020,Contrastive Multiview Coding,Yonglong Tian;Dilip Krishnan;Phillip Isola,yonglong@mit.edu;dilipkay@google.com;phillipi@mit.edu,3;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,3,0.0,yes,9/25/19,Massachusetts Institute of Technology;Google;Massachusetts Institute of Technology,2;-1;2,5;-1;5,
4733,4733,4733,4733,4733,4733,4733,4733,ICLR,2020,RGTI:Response generation via templates integration for End to End dialog,Yuxin Zhang;Songyan Liu,zhangyuxin960625@gmail.com;anchor3l31@gmail.com,1;1;1;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Beijing University of Post and Telecommunication;,481;-1,1397;-1,
4734,4734,4734,4734,4734,4734,4734,4734,ICLR,2020,Improving the Gating Mechanism of Recurrent Neural Networks,Albert Gua;Caglar Gulcehre;Tom le Paine;Razvan Pascanu;Matt Hoffman,gua@google.com;caglarg@google.com;mwhoffman@google.com;razp@google.com,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,3
4735,4735,4735,4735,4735,4735,4735,4735,ICLR,2020,Analyzing the Role of Model Uncertainty for Electronic Health Records,Michael W. Dusenberry;Dustin Tran;Edward Choi;Jonas Kemp;Jeremy Nixon;Ghassen Jerfel;Katherine Heller;Andrew M. Dai,dusenberrymw@google.com;trandustin@google.com;mp2893@gmail.com;jonasbkemp@google.com;jeremynixon@google.com;ghassen@google.com;kheller@google.com;adai@google.com,3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,2,3,0.0,yes,9/25/19,Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1,11
4736,4736,4736,4736,4736,4736,4736,4736,ICLR,2020,Leveraging Entanglement Entropy for Deep Understanding of  Attention Matrix in Text Matching,Peng Zhang;XiaoLiu Mao;XinDian Ma;BenYou Wang;Jing Zhang;Jun Wang;DaWei Song,pzhang@tju.edu.cn;xiaoliumao@tju.edu.cn;xindianma@tju.edu.cn;wang@dei.unipd.it;18738996120@163.com;jun.wang@cs.ucl.ac.uk;dwsong@bit.edu.cn,1;1;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Zhejiang University;Zhejiang University;Zhejiang University;Universita' degli studi di Padova;Zhejiang University;University College London;BIT,56;56;56;-1;56;50;-1,107;107;107;-1;107;15;-1,1
4737,4737,4737,4737,4737,4737,4737,4737,ICLR,2020,Superbloom: Bloom filter meets Transformer,John Anderson;Qingqing Huang;Walid Krichene;Steffen Rendle;Li Zhang,janders@google.com;qqhuang@google.com;walidk@google.com;srendle@google.com;liqzhang@google.com,6;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,3
4738,4738,4738,4738,4738,4738,4738,4738,ICLR,2020,LEARNING TO IMPUTE: A GENERAL FRAMEWORK FOR SEMI-SUPERVISED LEARNING,Wei-Hong Li;Chuan-Sheng Foo;Hakan Bilen,w.h.li@ed.ac.uk;foo_chuan_sheng@i2r.a-star.edu.sg;hbilen@ed.ac.uk,3;3;3,I have published one or two papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,University of Edinburgh;A*STAR;University of Edinburgh,33;-1;33,30;-1;30,8
4739,4739,4739,4739,4739,4739,4739,4739,ICLR,2020,Reflection-based Word Attribute Transfer,Yoichi Ishibashi;Katsuhito Sudoh;Koichiro Yoshino;Satoshi Nakamura,ishibashi.yoichi.ir3@is.naist.jp;sudoh@is.naist.jp;koichiro@is.naist.jp;s-nakamura@is.naist.jp,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,"Nara Institute of Science and Technology, Japan;Nara Institute of Science and Technology, Japan;Nara Institute of Science and Technology, Japan;Nara Institute of Science and Technology, Japan",481;481;481;481,1397;1397;1397;1397,3;7
4740,4740,4740,4740,4740,4740,4740,4740,ICLR,2020,ExpandNets: Linear Over-parameterization to Train Compact Convolutional Networks,Shuxuan Guo;Jose M. Alvarez;Mathieu Salzmann,shuxuan.guo@epfl.ch;josea@nvidia.com;mathieu.salzmann@epfl.ch,6;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Swiss Federal Institute of Technology Lausanne;NVIDIA;Swiss Federal Institute of Technology Lausanne,481;-1;481,38;-1;38,2;8
4741,4741,4741,4741,4741,4741,4741,4741,ICLR,2020,PopSGD: Decentralized Stochastic Gradient Descent in the Population Model,Giorgi Nadiradze;Amirmojtaba Sabour;Aditya Sharma;Ilia Markov;Vitaly Aksenov;Dan Alistarh.,giorgi.nadiradze@ist.ac.at;amsabour79@gmail.com;adityasharma.2000.as@gmail.com;ilia.markov@ist.ac.at;aksenov.vitaly@gmail.com;dan.alistarh@ist.ac.at,3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,Institute of Science and Technology Austria;;Indian Institute of Technology Bombay;Institute of Science and Technology Austria;ITMO University;Institute of Science and Technology Austria,481;-1;118;481;481;481,1397;-1;480;1397;480;1397,1
4742,4742,4742,4742,4742,4742,4742,4742,ICLR,2020,Wide Neural Networks are Interpolating Kernel Methods: Impact of Initialization on Generalization,Manuel Nonnenmacher;David Reeb;Ingo Steinwart,manuel.nonnenmacher@de.bosch.com;david.reeb@de.bosch.com;ingo.steinwart@mathematik.uni-stuttgart.de,1;1;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Bosch;Bosch;University of Stuttgart,-1;-1;95,-1;-1;292,1;8
4743,4743,4743,4743,4743,4743,4743,4743,ICLR,2020,Improved Mutual Information Estimation,Youssef Mroueh*;Igor Melnyk*;Pierre Dognin*;Jerret Ross*;Tom Sercu*,mroueh@us.ibm.com;igor.melnyk@ibm.com;pdognin@us.ibm.com;rossja@us.ibm.com;tom.sercu@gmail.com,1;3;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,7,0.0,yes,9/25/19,International Business Machines;International Business Machines;International Business Machines;International Business Machines;International Business Machines,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,1
4744,4744,4744,4744,4744,4744,4744,4744,ICLR,2020,Confidence Scores Make Instance-dependent Label-noise Learning Possible,Antonin Berthon;Bo Han;Gang Niu;Tongliang Liu;Masashi Sugiyama,berthon.antonin@gmail.com;bo.han@riken.jp;gang.niu@riken.jp;tongliang.liu@sydney.edu.au;sugi@k.u-tokyo.ac.jp,8;1;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,10,0.0,yes,9/25/19,RIKEN;RIKEN;RIKEN;University of Sydney;The University of Tokyo,-1;-1;-1;86;56,-1;-1;-1;60;36,
4745,4745,4745,4745,4745,4745,4745,4745,ICLR,2020,Is Deep Reinforcement Learning Really Superhuman on Atari? Leveling the playing field,Marin Toromanoff;Emilie Wirbel;Fabien Moutarde,marin.toromanoff@mines-paristech.fr;emilie.wirbel@valeo.com;fabien.moutarde@mines-paristech.fr,6;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,8,0.0,yes,9/25/19,Mines ParisTech;Valeo.ai;Mines ParisTech,481;-1;481,1397;-1;1397,
4746,4746,4746,4746,4746,4746,4746,4746,ICLR,2020,A Bilingual Generative Transformer for Semantic Sentence Embedding,John Wieting;Graham Neubig;Taylor Berg-Kirkpatrick,jwieting@cs.cmu.edu;gneubig@cs.cmu.edu;tberg@eng.ucsd.edu,3;6;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,5,0.0,yes,9/25/19,"Carnegie Mellon University;Carnegie Mellon University;University of California, San Diego",1;1;11,27;27;31,3
4747,4747,4747,4747,4747,4747,4747,4747,ICLR,2020,TechKG: A Large-Scale Chinese Technology-Oriented Knowledge Graph,Feiliang Ren,renfeiliang@cse.neu.edu.cn,1;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Northeastern University,16,906,10
4748,4748,4748,4748,4748,4748,4748,4748,ICLR,2020,Wasserstein Adversarial Regularization (WAR) on label noise,Bharath Damodaran;Kilian Fatras;Sylvain Lobry;Rémi Flamary;Devis Tuia;Nicolas Courty,bharath-bhushan.damodaran@irisa.fr;kilian.fatras@irisa.fr;sylvain.lobry@wur.nl;remi.flamary@unice.fr;devis.tuia@wur.nl;ncourty@irisa.fr,8;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,"IRISA, Université Bretagne Sud;IRISA, Université Bretagne Sud;Wageningen University and Research;Université Côte d'Azur;Wageningen University and Research;IRISA, Université Bretagne Sud",481;481;481;-1;481;481,1397;1397;59;-1;59;1397,4
4749,4749,4749,4749,4749,4749,4749,4749,ICLR,2020,Enhancing the Transformer with explicit relational encoding for math problem solving,Imanol Schlag;Paul Smolensky;Roland Fernandez;Nebojsa Jojic;Jürgen Schmidhuber;Jianfeng Gao,imanol@idsia.ch;paul.smolensky@gmail.com;rfernand@microsoft.com;jojic@microsoft.com;juergen@idsia.ch;jfgao@microsoft.com,3;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Reject,1,4,0.0,yes,9/25/19,IDSIA;Microsoft;Microsoft;Microsoft;IDSIA;Microsoft,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,
4750,4750,4750,4750,4750,4750,4750,4750,ICLR,2020,Constant Curvature Graph Convolutional Networks,Gregor Bachmann;Gary Bécigneul;Octavian-Eugen Ganea,gregorb@student.ethz.ch;garyb@mit.edu;oct@mit.edu,1;8;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Swiss Federal Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,10;2;2,13;5;5,10;8
4751,4751,4751,4751,4751,4751,4751,4751,ICLR,2020,Plan2Vec: Unsupervised Representation Learning by Latent Plans,Ge Yang;Amy Zhang;Ari Morcos;Joelle Pineau;Pieter Abbeel;Roberto Calandra,yangge1987@gmail.com;amyzhang2011@gmail.com;arimorcos@gmail.com;jpineau@cs.mcgill.ca;pabbeel@cs.berkeley.edu;rcalandra@fb.com,3;1;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,;McGill University;Facebook;McGill University;University of California Berkeley;Facebook,-1;86;-1;86;5;-1,-1;42;-1;42;13;-1,10
4752,4752,4752,4752,4752,4752,4752,4752,ICLR,2020,Generating Dialogue Responses From A Semantic Latent Space,Wei-Jen Ko;Avik Ray;Yilin Shen;Hongxia Jin,wjko@outlook.com;avik.r@samsung.com;yilin.shen@samsung.com;hongxia.jin@samsung.com,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,"University of Texas, Austin;Samsung;Samsung;Samsung",22;-1;-1;-1,38;-1;-1;-1,
4753,4753,4753,4753,4753,4753,4753,4753,ICLR,2020,Learning Key Steps to Attack Deep Reinforcement Learning Agents,Chien-Min Yu;Hsuan-Tien Lin,r07922080@csie.ntu.edu.tw;htlin@csie.ntu.edu.tw,3;3;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,National Taiwan University;National Taiwan University,86;86,120;120,4
4754,4754,4754,4754,4754,4754,4754,4754,ICLR,2020,"If MaxEnt RL is the Answer, What is the Question?",Benjamin Eysenbach;Sergey Levine,beysenba@cs.cmu.edu;svlevine@eecs.berkeley.edu,8;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I did not assess the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,14,0.0,yes,9/25/19,Carnegie Mellon University;University of California Berkeley,1;5,27;13,
4755,4755,4755,4755,4755,4755,4755,4755,ICLR,2020,Learning to Control Latent Representations for Few-Shot Learning of Named Entities,Omar U. Florez;Erik Mueller,omar.florez@aggiemail.usu.edu;erikmueller@capitalone.com,3;1;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,0,0.0,yes,9/25/19,SUN YAT-SEN UNIVERSITY;Capital One Bank,481;-1,299;-1,3;6
4756,4756,4756,4756,4756,4756,4756,4756,ICLR,2020,Variational Hyper RNN for Sequence Modeling,Ruizhi Deng;Yanshuai Cao;Bo Chang;Leonid Sigal;Greg Mori;Marcus Brubaker,ruizhid@sfu.ca;yanshuaicao@gmail.com;bchang@stat.ubc.ca;lsigal@cs.ubc.ca;mori@cs.sfu.ca;marcus.brubaker@borealisai.com,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,9,0.0,yes,9/25/19,Simon Fraser University;;University of British Columbia;University of British Columbia;Simon Fraser University;Borealis AI,64;-1;35;35;64;-1,272;-1;34;34;272;-1,
4757,4757,4757,4757,4757,4757,4757,4757,ICLR,2020,Weighted Empirical Risk Minimization: Transfer Learning based on Importance Sampling,Robin Vogel;Mastane Achab;Charles Tillier;Stéphan Clémençon,robin.vogel@telecom-paris.fr;mastane.achab@telecom-paris.fr;charles.tillier@telecom-paris.fr;stephan.clemencon@telecom-paris.fr,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,4,0.0,yes,9/25/19,Télécom Paris;Télécom Paris;Télécom Paris;Télécom Paris,481;481;481;481,187;187;187;187,8;1;6
4758,4758,4758,4758,4758,4758,4758,4758,ICLR,2020,Provenance detection through learning transformation-resilient watermarking,Jamie Hayes;Krishnamurthy Dvijotham;Yutian Chen;Sander Dieleman;Pushmeet Kohli;Norman Casagrande,j.hayes@cs.ucl.ac.uk;dvij@google.com;yutianc@google.com;sedielem@google.com;pushmeet@google.com;ncasagrande@google.com,1;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,University College London;Google;Google;Google;Google;Google,50;-1;-1;-1;-1;-1,15;-1;-1;-1;-1;-1,5;4
4759,4759,4759,4759,4759,4759,4759,4759,ICLR,2020,Underwhelming Generalization Improvements From Controlling Feature Attribution,Joseph D Viviano;Becks Simpson;Francis Dutil;Yoshua Bengio;Joseph Paul Cohen,joseph@viviano.ca;becks.simpson@imagia.com;francis.dutil@imagia.com;yoshua.bengio@mila.quebec;joseph@josephpcohen.com,3;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A,Reject,0,3,0.0,yes,9/25/19,University of Montreal;Imagia;Imagia;University of Montreal;University of Montreal,128;-1;-1;128;128,85;-1;-1;85;85,8
4760,4760,4760,4760,4760,4760,4760,4760,ICLR,2020,Combining graph and sequence information to learn protein representations,Hassan Kané;Mohamed Coulibali;Pelkins Ajanoh;Ali Abdalla,hassanmohamed@alum.mit.edu;mohamed-konoufo.coulibali.1@ulaval.ca;pelkins@alum.mit.edu;aabdalla@alum.mit.edu,1;1;3,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,0,0.0,yes,9/25/19,Massachusetts Institute of Technology;Laval university;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;481;2;2,5;272;5;5,
4761,4761,4761,4761,4761,4761,4761,4761,ICLR,2020,Sensible adversarial learning,Jungeum Kim;Xiao Wang,kim2712@purdue.edu;wangxiao@purdue.edu,3;6;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,2,9,0.0,yes,9/25/19,Purdue University;Purdue University,27;27,88;88,4
4762,4762,4762,4762,4762,4762,4762,4762,ICLR,2020,YaoGAN: Learning Worst-case Competitive Algorithms from Self-generated Inputs,Goran Zuzic;Di Wang;Aranyak Mehta;D. Sivakumar,zuza777@gmail.com;wadi@google.com;aranyak@google.com;siva@google.com,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.,Reject,0,8,0.0,yes,9/25/19,Carnegie Mellon University;Google;Google;Google,1;-1;-1;-1,27;-1;-1;-1,5;4
4763,4763,4763,4763,4763,4763,4763,4763,ICLR,2020,Learning to Remember from a Multi-Task Teacher,Yuwen Xiong;Mengye Ren;Raquel Urtasun,yuwen@cs.toronto.edu;mren@cs.toronto.edu;urtasun@uber.com,1;3;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:N/A:N/A,Reject,0,6,0.0,yes,9/25/19,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Uber",18;18;-1,18;18;-1,6
4764,4764,4764,4764,4764,4764,4764,4764,ICLR,2020,Pretraining boosts out-of-domain robustness for pose estimation,Alexander Mathis;Mert Yüksekgönül;Byron Rogers;Matthias Bethge;Mackenzie W. Mathis,amathis@fas.harvard.edu;mertyuksekgonul@gmail.com;byron@performancegenetics.com;matthias.bethge@uni-tuebingen.de;mathis@rowland.harvard.edu,1;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,3,0.0,yes,9/25/19,Harvard University;Bogazici University;Performancegenetics;University of Tuebingen;Harvard University,39;323;-1;154;39,7;672;-1;91;7,6;2;8
4765,4765,4765,4765,4765,4765,4765,4765,ICLR,2020,Self-Induced Curriculum Learning in Neural Machine Translation,Dana Ruiter;Cristina España-Bonet;Josef van Genabith,druiter@lsv.uni-saarland.de;cristinae@dfki.de;josef.van_genabith@dfki.de,6;3;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,4,0.0,yes,9/25/19,Saarland University;German Research Center for AI;German Research Center for AI,92;-1;-1,1397;-1;-1,3
4766,4766,4766,4766,4766,4766,4766,4766,ICLR,2020,Adapt-to-Learn: Policy Transfer in Reinforcement Learning,Girish Joshi;Girish Chowdhary,girishj2@illinois.edu;girishc@illinois.edu,1;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",3;3,48;48,
4767,4767,4767,4767,4767,4767,4767,4767,ICLR,2020,The Surprising Behavior Of Graph Neural Networks,Vivek Kothari;Catherine Tong;Nicholas Lane,vivek.kothari@cs.ox.ac.uk;eu.tong@cs.ox.ac.uk;nicholas.lane@cs.ox.ac.uk,6;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,University of Oxford;University of Oxford;University of Oxford,50;50;50,1;1;1,10
4768,4768,4768,4768,4768,4768,4768,4768,ICLR,2020,On the Reflection of Sensitivity in the Generalization Error,Mahsa Forouzesh;Farnood Salehi;Patrick Thiran,mahsa.forouzesh@epfl.ch;farnood.salehi@epfl.ch;patrick.thiran@epfl.ch,3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne,481;481;481,38;38;38,8
4769,4769,4769,4769,4769,4769,4769,4769,ICLR,2020,Bio-Inspired Hashing for Unsupervised Similarity Search,Chaitanya K. Ryali;John J. Hopfield;Dmitry Krotov,rckrishn@eng.ucsd.edu;hopfield@princeton.edu;krotov@ibm.com,6;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,0,1.0,yes,9/25/19,"University of California, San Diego;Princeton University;International Business Machines",11;31;-1,31;6;-1,
4770,4770,4770,4770,4770,4770,4770,4770,ICLR,2020,Neural Program Synthesis By Self-Learning,Yifan Xu;Lu Dai;Udaikaran Singh;Kening Zhang;Zhuowen Tu,yix081@ucsd.edu;dldaisy@mail.ustc.edu.cn;u1singh@ucsd.edu;kez040@ucsd.edu;ztu@ucsd.edu,3;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:N/A:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,2.0,yes,9/25/19,"University of California, San Diego;University of Science and Technology of China;University of California, San Diego;University of California, San Diego;University of California, San Diego",11;481;11;11;11,31;80;31;31;31,
4771,4771,4771,4771,4771,4771,4771,4771,ICLR,2020,The Variational InfoMax AutoEncoder,Vinenzo Crescimanna;Bruce Graham,vincenzo.crescimanna1@stir.ac.uk;bruce.graham@stir.ac.uk,3;6;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,University of Stirling;University of Stirling,481;481,350;350,5
4772,4772,4772,4772,4772,4772,4772,4772,ICLR,2020,Neural Network Out-of-Distribution Detection for Regression Tasks,Geoff Pleiss;Amauri Souza;Joseph Kim;Boyi Li;Kilian Q. Weinberger,geoff@cs.cornell.edu;ahd64@cornell.edu;jk2569@cornell.edu;bl728@cornell.edu;kqw4@cornell.edu,3;3;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,5,0.0,yes,9/25/19,Cornell University;Cornell University;Cornell University;Cornell University;Cornell University,7;7;7;7;7,19;19;19;19;19,5
4773,4773,4773,4773,4773,4773,4773,4773,ICLR,2020,Stein Self-Repulsive Dynamics: Benefits from Past Samples,Mao Ye;Tongzheng Ren;Qiang Liu,lushleaf21@gmail.com;rtz19970824@gmail.com;lqiang@cs.utexas.edu,6;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0.0,yes,9/25/19,"University of Electronic Science and Technology of China;University of Texas, Austin;University of Texas, Austin",481;22;22,628;38;38,
4774,4774,4774,4774,4774,4774,4774,4774,ICLR,2020,Mesh-Free Unsupervised Learning-Based PDE Solver of Forward and Inverse problems,Leah Bar;Nir Sochen,barleah.libra@gmail.com;sochen@tauex.tau.ac.il,6;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,5,0.0,yes,9/25/19,Tel Aviv University;Tel Aviv University,35;35,188;188,
4775,4775,4775,4775,4775,4775,4775,4775,ICLR,2020,Gaussian Conditional Random Fields for Classification,Andrija Petrovic;Mladen Nikolic;Milos Jovanovic;Boris Delibasic,aapetrovic@mas.bg.ac.rs;nikolic@matf.bg.ac.rs;milos.jovanovic@fon.bg.ac.rs;boris.delibasic@fon.bg.ac.rs,1;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,University of Belgrade;University of Belgrade;University of Belgrade;University of Belgrade,481;481;481;481,1397;1397;1397;1397,11;10
4776,4776,4776,4776,4776,4776,4776,4776,ICLR,2020,Visual Hide and Seek,Boyuan Chen;Shuran Song;Hod Lipson;Carl Vondrick,bchen@cs.columbia.edu;shurans@cs.columbia.edu;hod.lipson@columbia.edu;vondrick@cs.columbia.edu,3;3;8,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,6,0.0,yes,9/25/19,Columbia University;Columbia University;Columbia University;Columbia University,15;15;15;15,16;16;16;16,
4777,4777,4777,4777,4777,4777,4777,4777,ICLR,2020,Supervised learning with incomplete data via sparse representations,Cesar F. Caiafa;Ziyao Wang;Jordi Solé-Casals;Qibin Zhao,ccaiafa@gmail.com;zy_wang@seu.edu.cn;jordi.sole@uvic.cat;qibin.zhao@riken.jp,6;6;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,CONICET;Southeast University;University of Victoria;RIKEN,-1;-1;172;-1,-1;-1;449;-1,
4778,4778,4778,4778,4778,4778,4778,4778,ICLR,2020,Transfer Active Learning For Graph Neural Networks,Shengding Hu;Meng Qu;Zhiyuan Liu;Jian Tang,hsd16@mails.tsinghua.edu.cn;meng.qu@umontreal.ca;liuzy@tsinghua.edu.cn;jian.tang@hec.ca,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Tsinghua University;University of Montreal;Tsinghua University;HEC Montreal,8;128;8;128,23;85;23;85,10;1;6
4779,4779,4779,4779,4779,4779,4779,4779,ICLR,2020,Equivariant Entity-Relationship Networks,Devon Graham;Siamak Ravanbakhsh,drgraham@cs.ubc.ca;siamak@cs.mcgill.ca,3;3;3;8;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,11,0.0,yes,9/25/19,University of British Columbia;McGill University,35;86,34;42,1;10
4780,4780,4780,4780,4780,4780,4780,4780,ICLR,2020,Assessing Generalization in TD methods for Deep Reinforcement Learning,Emmanuel Bengio;Doina Precup;Joelle Pineau,bengioe@gmail.com;dprecup@cs.mcgill.ca;jpineau@cs.mcgill.ca,3;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,5,0.0,yes,9/25/19,McGill University;McGill University;McGill University,86;86;86,42;42;42,8
4781,4781,4781,4781,4781,4781,4781,4781,ICLR,2020,Chordal-GCN: Exploiting sparsity in training large-scale graph convolutional networks,Xin Jiang*;Kewei Cheng*;Song Jiang*;Yizhou Sun,jiangxjames@ucla.edu;viviancheng@cs.ucla.edu;songjiang@cs.ucla.edu;yzsun@cs.ucla.edu,6;1;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,"University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles",20;20;20;20,17;17;17;17,10
4782,4782,4782,4782,4782,4782,4782,4782,ICLR,2020,Certifying Distributional Robustness using Lipschitz Regularisation,Zac Cranko;Zhan Shi;Xinhua Zhang;Simon Kornblith;Richard Nock,zac.cranko@anu.edu.au;zshi22@uic.edu;zhangx@uic.edu;skornblith@google.com;richard.nock@data61.csiro.au,6;3;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,"Australian National University;University of Illinois, Chicago;University of Illinois, Chicago;Google;, CSIRO",108;56;56;-1;233,50;254;254;-1;-1,4
4783,4783,4783,4783,4783,4783,4783,4783,ICLR,2020,"Continuous Control with Contexts, Provably",Simon Du;Mengdi Wang;Ruosong Wang;Lin F. Yang,ssdu@ias.edu;mengdiw@princeton.edu;ruosongw@andrew.cmu.edu;linyang@ee.ucla.edu,3;1;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,"Institue for Advanced Study, Princeton;Princeton University;Carnegie Mellon University;University of California, Los Angeles",-1;31;1;20,-1;6;27;17,1
4784,4784,4784,4784,4784,4784,4784,4784,ICLR,2020,Using Hindsight to Anchor Past Knowledge in Continual Learning,Arslan Chaudhry;Albert Gordo;David Lopez-Paz;Puneet K. Dokania;Philip Torr,arslan.chaudhry@eng.ox.ac.uk;agordo@fb.com;david@lopezpaz.org;puneet@robots.ox.ac.uk;philip.torr@eng.ox.ac.uk,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,2,5,1.0,yes,9/25/19,University of Oxford;Facebook;Facebook;University of Oxford;University of Oxford,50;-1;-1;50;50,1;-1;-1;1;1,6
4785,4785,4785,4785,4785,4785,4785,4785,ICLR,2020,Solving single-objective tasks by preference multi-objective reinforcement learning,Jinsheng Ren;Shangqi Guo;Feng Chen,rjs17@mails.tsinghua.edu.cn;gsq15@mails.tsinghua.edu.cn;chenfeng@mail.tsinghua.edu.cn,3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Tsinghua University;Tsinghua University;Tsinghua University,8;8;8,23;23;23,
4786,4786,4786,4786,4786,4786,4786,4786,ICLR,2020,Transfer Alignment Network for Double Blind Unsupervised Domain Adaptation,Huiwen Xu;U Kang,xuhuiwen33@gmail.com;ukang@snu.ac.kr,1;1;1,I have published one or two papers in this area.:I carefully checked the experiments.:N/A:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Seoul National University;Seoul National University,41;41,64;64,6
4787,4787,4787,4787,4787,4787,4787,4787,ICLR,2020,Policy Optimization by Local Improvement through Search,Jialin Song;Joe Wenjie Jiang;Amir Yazdanbakhsh;Ebrahim Songhori;Anna Goldie;Navdeep Jaitly;Azalia Mirhoseini,jssong@caltech.edu;wenjiej@google.com;ayazdan@google.com;esonghori@google.com;agoldie@google.com;ndjaitly@google.com;azalia@google.com,3;3;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,California Institute of Technology;Google;Google;Google;Google;Google;Google,143;-1;-1;-1;-1;-1;-1,2;-1;-1;-1;-1;-1;-1,
4788,4788,4788,4788,4788,4788,4788,4788,ICLR,2020,Regularly varying representation for sentence embedding,Hamid Jalalzai;Pierre Colombo;Chloé Clavel;Eric Gaussier;Giovanna Varni;Emmanuel Vignon;Anne Sabourin,hamid.jalalzai@telecom-paris.fr;pierre.colombo@telecom-paris.fr;chloe.clavel@telecom-paris.fr;giovanna.varni@telecom-paris.fr;emmanuel.vignon@fr.ibm.com;anne.sabourin@telecom-paris.fr,3;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Télécom Paris;Télécom Paris;Télécom Paris;Télécom Paris;International Business Machines;Télécom Paris,481;481;481;481;-1;481,187;187;187;187;-1;187,3
4789,4789,4789,4789,4789,4789,4789,4789,ICLR,2020,Statistical Verification of General Perturbations by Gaussian Smoothing,Marc Fischer;Maximilian Baader;Martin Vechev,marcfisc@student.ethz.ch;mbaader@inf.ethz.ch;martin.vechev@inf.ethz.ch,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10,13;13;13,8
4790,4790,4790,4790,4790,4790,4790,4790,ICLR,2020,Learning to Anneal and Prune Proximity Graphs for Similarity Search,Minjia Zhang;Wenhan Wang;Yuxiong He,minjiaz@microsoft.com;wenhanw@microsoft.com;yuxhe@microsoft.com,6;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Microsoft;Microsoft;Microsoft,-1;-1;-1,-1;-1;-1,10
4791,4791,4791,4791,4791,4791,4791,4791,ICLR,2020,Unsupervised Learning of Graph Hierarchical Abstractions with Differentiable Coarsening and Optimal Transport,Tengfei Ma;Jie Chen,tengfei.ma1@ibm.com;chenjie@us.ibm.com,3;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,International Business Machines;International Business Machines,-1;-1,-1;-1,10
4792,4792,4792,4792,4792,4792,4792,4792,ICLR,2020,Bayesian Variational Autoencoders for Unsupervised Out-of-Distribution Detection,Erik Daxberger;José Miguel Hernández-Lobato,ead54@cam.ac.uk;jmh233@cam.ac.uk,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,University of Cambridge;University of Cambridge,71;71,3;3,5;11
4793,4793,4793,4793,4793,4793,4793,4793,ICLR,2020,Robust Instruction-Following in a Situated Agent via Transfer-Learning from Text,Felix Hill;Sona Mokra;Nathaniel Wong;Tim Harley,felixhill@google.com;sonka@google.com;nathanielwong@google.com;tharley@google.com,3;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,3;6
4794,4794,4794,4794,4794,4794,4794,4794,ICLR,2020,Fully Polynomial-Time Randomized Approximation Schemes for Global Optimization of High-Dimensional Folded Concave Penalized Generalized Linear Models,Charles Hernandez;Hungyi Lee;Hongchen Liu,cdhernandez@ufl.edu;hungyilee@ufl.edu;hliu@ise.ufl.edu,3;3;8,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,University of Florida;University of Florida;University of Florida,128;128;128,174;174;174,9
4795,4795,4795,4795,4795,4795,4795,4795,ICLR,2020,MANIFOLD FORESTS: CLOSING THE GAP ON NEURAL NETWORKS,Ronan Perry;Tyler M. Tomita;Jesse Patsolic;Benjamin Falk;Joshua Vogelstein,rperry27@jhu.edu;ttomita2@jhmi.edu;jpatsolic@jhu.edu;falk.ben@jhu.edu;jovo@jhu.edu,3;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Johns Hopkins University;;Johns Hopkins University;Johns Hopkins University;Johns Hopkins University,73;-1;73;73;73,12;-1;12;12;12,
4796,4796,4796,4796,4796,4796,4796,4796,ICLR,2020,Granger Causal Structure Reconstruction from Heterogeneous Multivariate Time Series,Yunfei Chu;Xiaowei Wang;Chunyan Feng;Jianxin Ma;Jingren Zhou;Hongxia Yang,yfchu@bupt.edu.cn;daemon.wxw@alibaba-inc.com;cyfeng@bupt.edu.cn;jason.mjx@alibaba-inc.com;jingren.zhou@alibaba-inc.com;yang.yhx@alibaba-inc.com,3;8;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Beijing University of Post and Telecommunication;Alibaba Group;Beijing University of Post and Telecommunication;Alibaba Group;Alibaba Group;Alibaba Group,481;-1;481;-1;-1;-1,1397;-1;1397;-1;-1;-1,
4797,4797,4797,4797,4797,4797,4797,4797,ICLR,2020,Towards Disentangling Non-Robust and Robust Components in Performance Metric,Yujun Shi;Benben Liao;Guangyong Chen;Yun Liu;Ming-ming Cheng;Jiashi Feng,shiyujun1016@gmail.com;bliao@tencent.com;gycchen@tencent.com;nk12csly@mail.nankai.edu.cn;cmm@nankai.edu.cn;elefjia@nus.edu.sg,1;1;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Nankai University;Tencent AI Lab;Tencent AI Lab;Nankai University;Nankai University;National University of Singapore,481;-1;-1;481;481;16,366;-1;-1;366;366;25,4;8
4798,4798,4798,4798,4798,4798,4798,4798,ICLR,2020,Distributed Training Across the World,Ligeng Zhu;Yao Lu;Yujun Lin;Song Han,ligeng@mit.edu;luyao11175@gmail.com;yujunlin@mit.edu;songhan@mit.edu,3;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,Massachusetts Institute of Technology;;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;-1;2;2,5;-1;5;5,
4799,4799,4799,4799,4799,4799,4799,4799,ICLR,2020,Risk Averse Value Expansion for Sample Efficient and Robust Policy Learning,Bo Zhou;Fan Wang;Hongsheng Zeng;Hao Tian,zhoubo01@baidu.com;wangfan04@baidu.com;zenghongsheng@baidu.com;tianhao@baidu.com,3;3;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Baidu;Baidu;Baidu;Baidu,-1;-1;-1;-1,-1;-1;-1;-1,1
4800,4800,4800,4800,4800,4800,4800,4800,ICLR,2020,MANAS: Multi-Agent Neural Architecture Search,Fabio Maria Carlucci;Pedro M Esperança;Marco Singh;Victor Gabillon;Antoine Yang;Hang Xu;Zewei Chen;Jun Wang,fabiom.carlucci@gmail.com;pedro.esperanca@huawei.com;marco.singh@huawei.com;victor.gabillon@huawei.com;antoineyang3@gmail.com;xu.hang@huawei.com;chen.zewei@huawei.com;w.j@huawei.com,6;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,4,0.0,yes,9/25/19,Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.;ENS Paris-Saclay;Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.,-1;-1;-1;-1;481;-1;-1;-1,-1;-1;-1;-1;644;-1;-1;-1,10
4801,4801,4801,4801,4801,4801,4801,4801,ICLR,2020,Neural Architecture Search by Learning Action Space for Monte Carlo Tree Search,Linnan Wang;Saining Xie;Teng Li;Rodrigo Fonseca;Yuandong Tian,linnan_wang@brown.edu;s9xie@fb.com;yuandong@fb.com;tengli@fb.com,6;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Brown University;Facebook;Facebook;Facebook,67;-1;-1;-1,53;-1;-1;-1,11
4802,4802,4802,4802,4802,4802,4802,4802,ICLR,2020,A multi-task U-net for segmentation with lazy labels,Rihuan Ke;Aurélie Bugeau;Nicolas Papadakis;Peter Schuetz;Carola-Bibiane Schönlieb,rk621@cam.ac.uk;aurelie.bugeau@labri.fr;nicolas.papadakis@math.u-bordeaux.fr;peter.schuetz@unilever.com;cbs31@cam.ac.uk,6;6;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,University of Cambridge;LaBRI;CNRS/IMB;Unilever;University of Cambridge,71;481;-1;-1;71,3;646;-1;-1;3,2
4803,4803,4803,4803,4803,4803,4803,4803,ICLR,2020,Deep automodulators,Ari Heljakka;Yuxin Hou;Juho Kannala;Arno Solin,ari.heljakka@aalto.fi;yuxin.hou@aalto.fi;arno.solin@aalto.fi;juho.kannala@aalto.fi,3;3;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,4,0.0,yes,9/25/19,Aalto University;Aalto University;Aalto University;Aalto University,143;143;143;143,182;182;182;182,5
4804,4804,4804,4804,4804,4804,4804,4804,ICLR,2020,Towards Certified Defense for Unrestricted Adversarial Attacks,Shengjia Zhao;Yang Song;Stefano Ermon,sjzhao@stanford.edu;yangsong@cs.stanford.edu;ermon@cs.stanford.edu,1;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,1,0.0,yes,9/25/19,Stanford University;Stanford University;Stanford University,4;4;4,4;4;4,4;1
4805,4805,4805,4805,4805,4805,4805,4805,ICLR,2020,Best feature performance in codeswitched hate speech texts,Edward Ombui;Lawrence Muchemi;Peter Wagacha,eombui@anu.ac.ke;lmuchemi@uonbi.ac.ke;waiganjo@uonbi.ac.ke,3;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Africa Nazarene University;University of Nairobi;University of Nairobi,481;481;481,1397;871;871,
4806,4806,4806,4806,4806,4806,4806,4806,ICLR,2020,Evolutionary Reinforcement Learning for Sample-Efficient Multiagent Coordination,Shauharda Khadka;Somdeb Majumdar;Santiago Miret;Stephen McAleer;Kagan Tumer,shauharda.khadka@intel.com;somdeb.majumdar@intel.com;santiago.miret@intel.com;smcaleer@uci.edu;kagan.tumer@oregonstate.edu,1;6;8,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,"Intel;Intel;Intel;University of California, Irvine;Oregon State University",-1;-1;-1;35;77,-1;-1;-1;96;373,
4807,4807,4807,4807,4807,4807,4807,4807,ICLR,2020,Gaussian Process Meta-Representations Of Neural Networks,Theofanis Karaletsos;Thang Bui,theofanis.karaletsos@gmail.com;thang.buivn@gmail.com,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,12,1.0,yes,9/25/19,;Uber,-1;-1,-1;-1,11
4808,4808,4808,4808,4808,4808,4808,4808,ICLR,2020,Training Deep Neural Networks with Partially Adaptive Momentum,Jinghui Chen;Dongruo Zhou;Yiqi Tang;Ziyan Yang;Yuan Cao;Quanquan Gu,jc4zg@virginia.edu;drzhou@cs.ucla.edu;yt6ze@virginia.edu;zy3cx@virginia.edu;yuanc@princeton.edu;qgu@cs.ucla.edu,1;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,"University of Virginia;University of California, Los Angeles;University of Virginia;University of Virginia;Princeton University;University of California, Los Angeles",59;20;59;59;31;20,107;17;107;107;6;17,1;9;8
4809,4809,4809,4809,4809,4809,4809,4809,ICLR,2020,Scalable Deep Neural Networks via Low-Rank Matrix Factorization,Atsushi Yaguchi;Taiji Suzuki;Shuhei Nitta;Yukinobu Sakata;Akiyuki Tanizawa,atsushi.yaguchi@toshiba.co.jp;taiji@mist.i.u-tokyo.ac.jp;shuhei.nitta@toshiba.co.jp;yuki.sakata@toshiba.co.jp;akiyuki.tanizawa@toshiba.co.jp,3;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,1,4,0.0,yes,9/25/19,Toshiba Memory;The University of Tokyo;Toshiba Memory;Toshiba Memory;Toshiba Memory,-1;56;-1;-1;-1,-1;36;-1;-1;-1,
4810,4810,4810,4810,4810,4810,4810,4810,ICLR,2020,Antifragile and Robust Heteroscedastic Bayesian Optimisation,Ryan Rhys-Griffiths;Miguel Garcia-Ortegon;Alexander A. Aldrick;Alpha A. Lee,rrg27@cam.ac.uk;mg770@cam.ac.uk;av495@cam.ac.uk;aal44@cam.ac.uk,3;1;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,1,0.0,yes,9/25/19,University of Cambridge;University of Cambridge;University of Cambridge;University of Cambridge,71;71;71;71,3;3;3;3,11
4811,4811,4811,4811,4811,4811,4811,4811,ICLR,2020,Compressed Sensing with Deep Image Prior and Learned Regularization,Dave Van Veen;Ajil Jalal;Mahdi Soltanolkotabi;Eric Price;Sriram Vishwanath;Alexandros G. Dimakis,davemvanveen@gmail.com;ajiljalal@utexas.edu;soltanol@usc.edu;ecprice@cs.utexas.edu;sriram@austin.utexas.edu;dimakis@austin.utexas.edu,3;6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,"University of Texas, Austin;University of Texas, Austin;University of Southern California;University of Texas, Austin;University of Texas, Austin;University of Texas, Austin",22;22;31;22;22;22,38;38;62;38;38;38,5;1
4812,4812,4812,4812,4812,4812,4812,4812,ICLR,2020,Unsupervised Hierarchical Graph Representation Learning with Variational Bayes,Shashanka Ubaru;Jie Chen,shashanka.ubaru@ibm.com;chenjie@us.ibm.com,3;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,International Business Machines;International Business Machines,-1;-1,-1;-1,1;10
4813,4813,4813,4813,4813,4813,4813,4813,ICLR,2020,Learning Algorithmic Solutions to Symbolic Planning Tasks with a Neural Computer,Daniel Tanneberg;Elmar Rueckert;Jan Peters,daniel@robot-learning.de;rueckert@rob.uni-luebeck.de;mail@jan-peters.net,6;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,TU Darmstadt;University of Luebeck;TU Darmstadt,64;266;64,289;1397;289,8
4814,4814,4814,4814,4814,4814,4814,4814,ICLR,2020,Multi-Step Decentralized Domain Adaptation,Akhil Mathur;Shaoduo Gan;Anton Isopoussu;Fahim Kawsar;Nadia Berthouze;Nicholas D. Lane,akhilmathurs@gmail.com;sgan@inf.ethz.ch;anton.isopoussu@gmail.com;fahim.kawsar@gmail.com;nadia.berthouze@gmail.com;nicholasd.lane@gmail.com,6;3;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,University College London;Swiss Federal Institute of Technology;;;;University of Oxford,50;10;-1;-1;-1;50,15;13;-1;-1;-1;1,4
4815,4815,4815,4815,4815,4815,4815,4815,ICLR,2020,Robust saliency maps with distribution-preserving decoys,Yang Young Lu;Wenbo Guo;Xinyu Xing;William Stafford Noble,ylu465@uw.edu;wzg13@ist.psu.edu;xxing@ist.psu.edu;william-noble@uw.edu,3;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,"University of Washington, Seattle;Pennsylvania State University;Pennsylvania State University;University of Washington, Seattle",6;41;41;6,26;78;78;26,4
4816,4816,4816,4816,4816,4816,4816,4816,ICLR,2020,Towards Interpreting Deep Neural Networks via Understanding Layer Behaviors,Jiezhang Cao;Jincheng Li;Xiping Hu;Peilin Zhao;Mingkui Tan,secaojiezhang@mail.scut.edu.cn;sejinchengli@mail.scut.edu.cn;huxp@lzu.edu.cn;peilinzhao@hotmail.com;mingkuitan@scut.edu.cn,3;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,4,0.0,yes,9/25/19,South China University of Technology;South China University of Technology;Tsinghua University;;South China University of Technology,481;481;8;-1;481,501;501;23;-1;501,1
4817,4817,4817,4817,4817,4817,4817,4817,ICLR,2020,Robust Graph Representation Learning via Neural Sparsification,Cheng Zheng;Bo Zong;Wei Cheng;Dongjin Song;Jingchao Ni;Wenchao Yu;Haifeng Chen;Wei Wang,chengzheng@cs.ucla.edu;bzong@nec-labs.com;weicheng@nec-labs.com;dsong@nec-labs.com;jni@nec-labs.com;yuwenchao@ucla.edu;haifeng@nec-labs.com;weiwang@cs.ucla.edu,6;1;8,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,"University of California, Los Angeles;NEC-Labs;NEC-Labs;NEC-Labs;NEC-Labs;University of California, Los Angeles;NEC-Labs;University of California, Los Angeles",20;-1;-1;-1;-1;20;-1;20,17;-1;-1;-1;-1;17;-1;17,10
4818,4818,4818,4818,4818,4818,4818,4818,ICLR,2020,Good Semi-supervised VAE Requires Tighter Evidence Lower Bound,Haozhe Feng;Kezhi Kong;Tianye Zhang;Siyue Xue;Wei Chen,fenghz@zju.edu.cn;kong@cs.umd.edu;zhangtianye1026@zju.edu.cn;3160104527@zju.edu.cn;chenwei@cad.zju.edu.cn,3;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,4,0.0,yes,9/25/19,"Zhejiang University;University of Maryland, College Park;Zhejiang University;Zhejiang University;Zhejiang University",56;12;56;56;56,107;91;107;107;107,5
4819,4819,4819,4819,4819,4819,4819,4819,ICLR,2020,EDUCE: Explaining model Decision through Unsupervised Concepts Extraction,Diane Bouchacourt;Ludovic Denoyer,dianeb@fb.com;denoyer@fb.com,6;3;8,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,9,0.0,yes,9/25/19,Facebook;Facebook,-1;-1,-1;-1,
4820,4820,4820,4820,4820,4820,4820,4820,ICLR,2020,Projected Canonical Decomposition for Knowledge Base Completion,Timothée Lacroix;Guillaume Obozinski;Joan Bruna;Nicolas Usunier,timothee.lax@gmail.com;guillaume.obozinski@epfl.ch;bruna@cims.nyu.edu;usunier@fb.com,3;8;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Facebook;Swiss Federal Institute of Technology Lausanne;New York University;Facebook,-1;481;25;-1,-1;38;29;-1,
4821,4821,4821,4821,4821,4821,4821,4821,ICLR,2020,Lossless Data Compression with Transformer,Gautier Izacard;Armand Joulin;Edouard Grave,gizacard@gmail.com;ajoulin@fb.com;egrave@fb.com,3;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,3,0.0,yes,9/25/19,Ecole polytechnique;Facebook;Facebook,481;-1;-1,93;-1;-1,3
4822,4822,4822,4822,4822,4822,4822,4822,ICLR,2020,Stochastically Controlled Compositional Gradient for the Composition problem,Liu Liu;Ji Liu;Cho-Jui Hsieh;Dacheng Tao,liu.liu1@sydney.edu.au;ji.liu.uwisc@gmail.com;chohsieh@cs.ucla.edu;dacheng.tao@sydney.edu.au,6;3;3,I have read many papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,"University of Sydney;University of Rochester;University of California, Los Angeles;University of Sydney",86;100;20;86,60;173;17;60,
4823,4823,4823,4823,4823,4823,4823,4823,ICLR,2020,Domain Adaptation via Low-Rank Basis Approximation,Christoph Raab;Frank-Michael Schleif,christoph.raab@fhws.de;frank-michael.schleif@fhws.de,1;3;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,University of Applied Sciences Würzburg-Schweinfurt;University of Applied Sciences Würzburg-Schweinfurt,481;481,1397;1397,
4824,4824,4824,4824,4824,4824,4824,4824,ICLR,2020,SGD with Hardness Weighted Sampling for Distributionally Robust Deep Learning,Lucas Fidon;Sebastien Ourselin;Tom Vercauteren,lucas.fidon@kcl.ac.uk;sebastien.ourselin@kcl.ac.uk;tom.vercauteren@kcl.ac.uk,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,King's College London;King's College London;King's College London,172;172;172,36;36;36,1
4825,4825,4825,4825,4825,4825,4825,4825,ICLR,2020,Fast Training of Sparse Graph Neural Networks on Dense Hardware,Matej Balog;Bart van Merriënboer;Subhodeep Moitra;Yujia Li;Daniel Tarlow,matej.balog@gmail.com;bartvm@google.com;smoitra@google.com;yujiali@google.com;dtarlow@google.com,3;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,12,0.0,yes,9/25/19,University of Cambridge;Google;Google;Google;Google,71;-1;-1;-1;-1,3;-1;-1;-1;-1,10
4826,4826,4826,4826,4826,4826,4826,4826,ICLR,2020,CGT: Clustered Graph Transformer for Urban Spatio-temporal Prediction,Xu Geng;Lingyu Zhang;Shulin Li;Yuanbo Zhang;Lulu Zhang;Leye Wang;Qiang Yang;Hongtu Zhu;Jieping Ye,xgeng@connect.ust.hk;zhanglingyu@didiglobal.com;lishulin_i@didiglobal.com;bozhangyuanbo_i@didiglobal.com;zhanglulululu@didiglobal.com;leyewang@pku.edu.cn;qyang@cse.ust.hk;zhuhongtu@didiglobal.com;yejieping@didiglobal.com,3;3;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,"The Hong Kong University of Science and Technology;DiDi AI Labs, Didi Chuxing;DiDi AI Labs, Didi Chuxing;DiDi AI Labs, Didi Chuxing;DiDi AI Labs, Didi Chuxing;Peking University;The Hong Kong University of Science and Technology;DiDi AI Labs, Didi Chuxing;DiDi AI Labs, Didi Chuxing",39;-1;-1;-1;-1;22;39;-1;-1,47;-1;-1;-1;-1;24;47;-1;-1,10
4827,4827,4827,4827,4827,4827,4827,4827,ICLR,2020,Regularizing Black-box Models for Improved Interpretability,Gregory Plumb;Maruan Al-Shedivat;Eric Xing;Ameet Talwalkar,gdplumb@andrew.cmu.edu;alshedivat@cs.cmu.edu;epxing@cs.cmu.edu;talwalkar@cmu.edu,6;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1,27;27;27;27,
4828,4828,4828,4828,4828,4828,4828,4828,ICLR,2020,AlgoNet: $C^\infty$ Smooth Algorithmic Neural Networks,Felix Petersen;Christian Borgelt;Oliver Deussen,felix.petersen@uni.kn;christian@borgelt.net;oliver.deussen@uni.kn,1;3;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,;;,-1;-1;-1,-1;-1;-1,
4829,4829,4829,4829,4829,4829,4829,4829,ICLR,2020,CAPACITY-LIMITED REINFORCEMENT LEARNING: APPLICATIONS IN DEEP ACTOR-CRITIC METHODS FOR CONTINUOUS CONTROL,Tyler James Malloy;Matthew Riemer;Miao Liu;Tim Klinger;Gerald Tesauro;Chris R. Sims,mallot@rpi.edu;mdriemer@us.ibm.com;miao.liu1@ibm.com;tklinger@us.ibm.com;gtesauro@us.ibm.com;simsc3@rpi.edu,3;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Rensselaer Polytechnic Institute;International Business Machines;International Business Machines;International Business Machines;International Business Machines;Rensselaer Polytechnic Institute,172;-1;-1;-1;-1;172,438;-1;-1;-1;-1;438,8
4830,4830,4830,4830,4830,4830,4830,4830,ICLR,2020,AdvCodec: Towards A Unified Framework for Adversarial Text Generation,Boxin Wang;Hengzhi Pei;Han Liu;Bo Li,boxinw2@illinois.edu;hzpei16@fudan.edu.cn;hanliu@northwestern.edu;lbo@illinois.edu,3;6;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,"University of Illinois, Urbana Champaign;Fudan University;Northwestern University;University of Illinois, Urbana Champaign",3;79;44;3,48;109;22;48,3;4
4831,4831,4831,4831,4831,4831,4831,4831,ICLR,2020,Function Feature Learning of Neural Networks,Guangcong Wang;Jianhuang Lai;Guangrun Wang;Wenqi Liang,wanggc3@mail2.sysu.edu.cn;stsljh@mail.sysu.edu.cn;wanggrun@mail2.sysu.edu.cn;liangwq8@mail2.sysu.edu.cn,3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,3,1.0,yes,9/25/19,SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY,481;481;481;481,299;299;299;299,
4832,4832,4832,4832,4832,4832,4832,4832,ICLR,2020,Unsupervised domain adaptation with imputation,Matthieu Kirchmeyer;Patrick Gallinari;Alain Rakotomamonjy;Amin Mantrach,m.kirchmeyer@criteo.com;patrick.gallinari@lip6.fr;a.rakotomamonjy@criteo.com;a.mantrach@criteo.com,8;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Criteo;LIP6;Criteo;Criteo,-1;-1;-1;-1,-1;-1;-1;-1,
4833,4833,4833,4833,4833,4833,4833,4833,ICLR,2020,Path Space for Recurrent Neural Networks with ReLU Activations,Yue Wang;Qi Meng;Wei Chen;Yuting Liu;Zhi-Ming Ma;Tie-Yan Liu,11271012@bjtu.edu.cn;meq@microsoft.com;wche@microsoft.com;ytliu@bjtu.edu.cn;mazm@amt.ac.cn;tie-yan.liu@microsoft.com,3;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Beijing jiaotong univercity;Microsoft;Microsoft;Beijing jiaotong univercity;Chinese Academy of Sciences;Microsoft,481;-1;-1;481;59;-1,952;-1;-1;952;1397;-1,1;10
4834,4834,4834,4834,4834,4834,4834,4834,ICLR,2020,Fast Machine Learning with Byzantine Workers and Servers,El-Mahdi El-Mhamdi;Rachid Guerraoui;Arsany Guirguis,elmahdi.elmhamdi@epfl.ch;rachid.guerraoui@epfl.ch;arsany.guirguis@epfl.ch,3;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne,481;481;481,38;38;38,1
4835,4835,4835,4835,4835,4835,4835,4835,ICLR,2020,Topology-Aware Pooling via Graph Attention,Hongyang Gao;Shuiwang Ji,hongyang.gao@tamu.edu;sji@tamu.edu,3;6;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Texas A&M;Texas A&M,44;44,177;177,3;2;10
4836,4836,4836,4836,4836,4836,4836,4836,ICLR,2020,Stochastic Latent Actor-Critic: Deep Reinforcement Learning with a Latent Variable Model,Alex X. Lee;Anusha Nagabandi;Pieter Abbeel;Sergey Levine,alexlee_gk@cs.berkeley.edu;nagaban2@berkeley.edu;pabbeel@cs.berkeley.edu;svlevine@eecs.berkeley.edu,8;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5,13;13;13;13,5
4837,4837,4837,4837,4837,4837,4837,4837,ICLR,2020,Localised Generative Flows,Rob Cornish;Anthony Caterini;George Deligiannidis;Arnaud Doucet,rcornish@robots.ox.ac.uk;anthony.caterini@stats.ox.ac.uk;deligian@stats.ox.ac.uk;doucet@stats.ox.ac.uk,3;3;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,6,0.0,yes,9/25/19,University of Oxford;University of Oxford;University of Oxford;University of Oxford,50;50;50;50,1;1;1;1,5
4838,4838,4838,4838,4838,4838,4838,4838,ICLR,2020,Guided Adaptive Credit Assignment for Sample Efficient Policy Optimization,Hao Liu;Richard Socher;Caiming Xiong,lhao499@gmail.com;rsocher@salesforce.com;cxiong@salesforce.com,3;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,University of California Berkeley;SalesForce.com;SalesForce.com,5;-1;-1,13;-1;-1,3
4839,4839,4839,4839,4839,4839,4839,4839,ICLR,2020,Proactive Sequence Generator via Knowledge Acquisition,Qing Sun;James Cross;Dmitriy Genzel,qingsun@fb.com;jcross@fb.com;dgenzel@fb.com,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,Facebook;Facebook;Facebook,-1;-1;-1,-1;-1;-1,3
4840,4840,4840,4840,4840,4840,4840,4840,ICLR,2020,Reinforcement Learning without Ground-Truth State,Xingyu Lin;Harjatin Singh Baweja;David Held,xlin3@cs.cmu.edu;dheld@andrew.cmu.edu;harjatis@andrew.cmu.edu,3;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1,27;27;27,
4841,4841,4841,4841,4841,4841,4841,4841,ICLR,2020,Generalized Inner Loop Meta-Learning,Edward Grefenstette;Brandon Amos;Denis Yarats;Phu Mon Htut;Artem Molchanov;Franziska Meier;Douwe Kiela;Kyunghyun Cho;Soumith Chintala,egrefen@gmail.com;brandon.amos.cs@gmail.com;denisyarats@cs.nyu.edu;pmh330@nyu.edu;a.molchanov86@gmail.com;fmeier@fb.com;dkiela@fb.com;kyunghyun.cho@nyu.edu;soumith@gmail.com,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,10,0.0,yes,9/25/19,Facebook;Facebook;New York University;New York University;;Facebook;Facebook;New York University;Facebook,-1;-1;25;25;-1;-1;-1;25;-1,-1;-1;29;29;-1;-1;-1;29;-1,1;6
4842,4842,4842,4842,4842,4842,4842,4842,ICLR,2020,Channel Equilibrium Networks,Wenqi Shao;Shitao Tang;Xingang Pan;Ping Tan;Xiaogang Wang;Ping Luo,weqish@link.cuhk.edu.hk;shitaot@sfu.ca;px117@ie.cuhk.edu.hk;pingtan@sfu.ca;xgwang@ee.cuhk.edu.hk;pluo.lhi@gmail.com,3;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,The Chinese University of Hong Kong;Simon Fraser University;The Chinese University of Hong Kong;Simon Fraser University;The Chinese University of Hong Kong;The University of Hong Kong,59;64;59;64;59;92,35;272;35;272;35;35,
4843,4843,4843,4843,4843,4843,4843,4843,ICLR,2020,Zeroth Order Optimization by a Mixture of Evolution Strategies,Jun-Kun Wang;Xiaoyun Li;Ping Li,jimwang@gatech.edu;xl374@scarletmail.rutgers.edu;liping11@baidu.com,3;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Georgia Institute of Technology;Rutgers University;Baidu,13;34;-1,38;168;-1,
4844,4844,4844,4844,4844,4844,4844,4844,ICLR,2020,Limitations for Learning from Point Clouds,Christian Bueno;Alan G. Hylton,christianbueno@ucsb.edu;alan.g.hylton@nasa.gov,8;3;3,I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,4,0.0,yes,9/25/19,UC Santa Barbara;NASA,38;-1,57;-1,1
4845,4845,4845,4845,4845,4845,4845,4845,ICLR,2020,WORD SEQUENCE PREDICTION FOR AMHARIC LANGUAGE,Nuniyat Kifle;Ermias Abebe,nunukifle2@gmail.com;ermiasabebe@gmail.com,1;1;1,I have published in this field for several years.:N/A:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:N/A:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Addis Ababa University;,481;-1,1397;-1,3
4846,4846,4846,4846,4846,4846,4846,4846,ICLR,2020,Learning Underlying Physical Properties From Observations For Trajectory Prediction,Ekaterina Nikonova;Jochen Renz,ekaterina.nikonova@anu.edu.au;jochen.renz@anu.edu.au,1;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,4,0.0,yes,9/25/19,Australian National University;Australian National University,108;108,50;50,
4847,4847,4847,4847,4847,4847,4847,4847,ICLR,2020,Provable Representation Learning for Imitation Learning via Bi-level Optimization,Sanjeev Arora;Simon S. Du;Sham Kakade;Yuping Luo;Nikunj Saunshi,arora@cs.princeton.edu;ssdu@ias.edu;sham@cs.washington.edu;yupingl@cs.princeton.edu;nsaunshi@cs.princeton.edu,3;3;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,"Princeton University;Institue for Advanced Study, Princeton;University of Washington;Princeton University;Princeton University",31;-1;6;31;31,6;-1;26;6;6,1
4848,4848,4848,4848,4848,4848,4848,4848,ICLR,2020,Anchor & Transform: Learning Sparse Representations of Discrete Objects,Paul Pu Liang;Manzil Zaheer;Yuan Wang;Amr Ahmed,pliang@cs.cmu.edu;manzilzaheer@google.com;yuanwang@google.com;amra@google.com,3;3;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,5,0.0,yes,9/25/19,Carnegie Mellon University;Google;Google;Google,1;-1;-1;-1,27;-1;-1;-1,3
4849,4849,4849,4849,4849,4849,4849,4849,ICLR,2020,EgoMap: Projective mapping and structured egocentric memory for Deep RL,Edward Beeching;Christian Wolf;Jilles Dibangoye;Olivier Simonin,edward.beeching@inria.fr;christian.wolf@insa-lyon.fr;jilles.dibangoye@inria.fr;olivier.simonin@inria.fr,6;3;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,INRIA;INSA de Lyon;INRIA;INRIA,-1;481;-1;-1,-1;1397;-1;-1,8
4850,4850,4850,4850,4850,4850,4850,4850,ICLR,2020,Few-shot Learning by Focusing on Differences,Muhammad Rizki Maulana;Lee Wee Sun,maulana@comp.nus.edu.sg;leews@comp.nus.edu.sg,3;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,National University of Singapore;National University of Singapore,16;16,25;25,6
4851,4851,4851,4851,4851,4851,4851,4851,ICLR,2020,On the Parameterization of Gaussian Mean Field Posteriors in Bayesian Neural Networks,Jakub Świątkowski;Kevin Roth;Bastiaan S. Veeling;Linh Tran;Joshua V. Dillon;Jasper Snoek;Stephan Mandt;Tim Salimans;Rodolphe Jenatton;Sebastian Nowozin,kuba.swiatkowski@gmail.com;kevin.roth@inf.ethz.ch;basveeling@gmail.com;linh.tran@imperial.ac.uk;jvdillon@google.com;jaspersnoek@gmail.com;stephan.mandt@gmail.com;salimans@google.com;rjenatton@google.com;nowozin@google.com,3;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,";Swiss Federal Institute of Technology;Google;Imperial College London;Google;Google;University of California, Irvine;Google;Google;Google",-1;10;-1;73;-1;-1;35;-1;-1;-1,-1;13;-1;10;-1;-1;96;-1;-1;-1,11
4852,4852,4852,4852,4852,4852,4852,4852,ICLR,2020,Deep unsupervised feature selection,Ian Covert;Uygar Sumbul;Su-In Lee,icovert@cs.washington.edu;uygars@alleninstitute.org;suinlee@cs.washington.edu,3;1;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,University of Washington;Allen Institute;University of Washington,6;-1;6,26;-1;26,1
4853,4853,4853,4853,4853,4853,4853,4853,ICLR,2020,Efficient Bi-Directional Verification of ReLU Networks via Quadratic Programming,Aleksei Kuvshinov;Stephan Guennemann,kuvshino@in.tum.de;guennemann@in.tum.de,3;8;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Technical University Munich;Technical University Munich,53;53,43;43,4;1
4854,4854,4854,4854,4854,4854,4854,4854,ICLR,2020,AdaScale SGD: A Scale-Invariant Algorithm for Distributed Training,Tyler B. Johnson;Pulkit Agrawal;Haijie Gu;Carlos Guestrin,tbjohns@apple.com;pulkit_agrawal@apple.com;jaygu@apple.com;guestrin@apple.com,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,2,4,0.0,yes,9/25/19,Apple;Apple;Apple;Apple,-1;-1;-1;-1,-1;-1;-1;-1,3;2
4855,4855,4855,4855,4855,4855,4855,4855,ICLR,2020,Small-GAN: Speeding up GAN Training using Core-Sets,Samarth Sinha;Han Zhang;Anirudh Goyal;Yoshua Bengio;Hugo Larochelle;Augustus Odena,samarth.sinha@mail.utoronto.ca;zhanghan@google.com;anirudhgoyal9119@gmail.com;yoshua.bengio@mila.quebec;hugolarochelle@google.com;augustusodena@google.com,3;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,9,1.0,yes,9/25/19,Toronto University;Google;University of Montreal;University of Montreal;Google;Google,18;-1;128;128;-1;-1,18;-1;85;85;-1;-1,5;4
4856,4856,4856,4856,4856,4856,4856,4856,ICLR,2020,AUGMENTED POLICY GRADIENT METHODS FOR EFFICIENT REINFORCEMENT LEARNING,Kai Lagemann;Gregor Roering;Christoph Henke;Rene Vossen;Frank Hees,kai.lagemann@rwth-aachen.de;gregor.roering@rwth-aachen.de;christoph.henke@ifu.rwth-aachen.de;rene.vossen@ifu.rwth-aachen.de;hees.office@ima-ifu.rwth-aachen.de,1;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,0,0.0,yes,9/25/19,RWTH Aachen University;RWTH Aachen University;RWTH Aachen University;RWTH Aachen University;RWTH Aachen University,95;95;95;95;95,98;98;98;98;98,
4857,4857,4857,4857,4857,4857,4857,4857,ICLR,2020,Neural Approximation of an Auto-Regressive Process through Confidence Guided Sampling,YoungJoon Yoo;Sanghyuk Chun;Jaejun Yoo;Sangdoo Yun;Jung Woo Ha,youngjoon.yoo@navercorp.com;sanghyuk.c@navercorp.com;jaejun.yoo@navercorp.com;sangdoo.yun@navercorp.com;jungwoo.ha@navercorp.com,6;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,NAVER;NAVER;NAVER;NAVER;NAVER,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,
4858,4858,4858,4858,4858,4858,4858,4858,ICLR,2020,Certifiably Robust Interpretation in Deep Learning,Alexander Levine;Sahil Singla;Soheil Feizi,alevine0@cs.umd.edu;ssingla@cs.umd.edu;sfeizi@cs.umd.edu,3;1;3,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,6,1.0,yes,9/25/19,"University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park",12;12;12,91;91;91,4
4859,4859,4859,4859,4859,4859,4859,4859,ICLR,2020,Towards More Realistic Neural Network Uncertainties,Joachim Sicking;Alexander Kister;Matthias Fahrland;Stefan Eickeler;Fabian Hueger;Stefan Rueping;Peter Schlicht;Tim Wirtz,joachim.sicking@iais.fraunhofer.de;alexander.kister@iais.fraunhofer.de;matthias.fahrland@iav.de;stefan.eickeler@iais.fraunhofer.de;fabian.hueger@volkswagen.de;stefan.rueping@iais.fraunhofer.de;peter.schlicht@volkswagen.de;tim.wirtz@iais.fraunhofer.de,1;3;1,I have published one or two papers in this area.:I did not assess the experiments.:N/A:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,"Fraunhofer IIS;Fraunhofer IIS;;Fraunhofer IIS;Data Lab, Volkswagen Group;Fraunhofer IIS;Data Lab, Volkswagen Group;Fraunhofer IIS",-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1,
4860,4860,4860,4860,4860,4860,4860,4860,ICLR,2020,Adversarial Training: embedding adversarial perturbations into the parameter space of a neural network to build a robust system,Shixian Wen;Laurent Itti,shixianw@usc.edu;itti@usc.edu,1;3;3,I have published in this field for several years.:N/A:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,University of Southern California;University of Southern California,31;31,62;62,4
4861,4861,4861,4861,4861,4861,4861,4861,ICLR,2020,Neural Linear Bandits: Overcoming Catastrophic Forgetting through Likelihood Matching,Tom Zahavy;Shie Mannor,tomzahavy@gmail.com;shiemannor@gmail.com,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Technion;Technion,26;26,412;412,
4862,4862,4862,4862,4862,4862,4862,4862,ICLR,2020,Neural Non-additive Utility Aggregation,Markus Zopf,mzopf@ke.tu-darmstadt.de,3;1;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Reject,0,5,0.0,yes,9/25/19,TU Darmstadt,64,289,
4863,4863,4863,4863,4863,4863,4863,4863,ICLR,2020,Understanding the functional and structural differences across excitatory and inhibitory neurons,Sun Minni;Li Ji-An;Theodore Moskovitz;Grace Lindsay;Kenneth Miller;Mario Dipoppa;Guangyu Robert Yang,sunminni1031@gmail.com;jian.li.acad@gmail.com;thmoskovitz@gmail.com;gracewlindsay@gmail.com;kendmiller@gmail.com;mario.dipoppa@gmail.com;gyyang.neuro@gmail.com,6;6;8,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,11,0.0,yes,9/25/19,Columbia University;University of Science and Technology of China;University College London;University College London;;;Columbia University,15;481;50;50;-1;-1;15,16;80;15;15;-1;-1;16,
4864,4864,4864,4864,4864,4864,4864,4864,ICLR,2020,Skew-Explore: Learn faster in continuous spaces with sparse rewards,Xi Chen;Yuan Gao;Ali Ghadirzadeh;Marten Bjorkman;Ginevra Castellano;Patric Jensfelt,xi8@kth.se;gaoyuankidult@gmail.com;algh@kth.se;celle@csc.kth.se;ginevra.castellano@it.uu.se;patric@kth.se,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,"KTH Royal Institute of Technology, Stockholm, Sweden;Uppsala University;KTH Royal Institute of Technology, Stockholm, Sweden;KTH Royal Institute of Technology, Stockholm, Sweden;Uppsala University;KTH Royal Institute of Technology, Stockholm, Sweden",128;154;128;128;154;128,222;102;222;222;102;222,
4865,4865,4865,4865,4865,4865,4865,4865,ICLR,2020,Self-Supervised Speech Recognition via Local Prior Matching,Wei-Ning Hsu;Ann Lee;Gabriel Synnaeve;Awni Hannun,wnhsu@mit.edu;annl@fb.com;gab@fb.com;awni@fb.com,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Massachusetts Institute of Technology;Facebook;Facebook;Facebook,2;-1;-1;-1,5;-1;-1;-1,3
4866,4866,4866,4866,4866,4866,4866,4866,ICLR,2020,Learning Reusable Options for Multi-Task Reinforcement Learning,Francisco M. Garcia;Chris Nota;Philip S. Thomas,fmaxgarcia@gmail.com;cnota@cs.umass.edu;pthomas@cs.umass.edu,3;6;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,"Amazon;University of Massachusetts, Amherst;University of Massachusetts, Amherst",-1;28;28,-1;209;209,
4867,4867,4867,4867,4867,4867,4867,4867,ICLR,2020,A Unified framework for randomized smoothing based certified defenses,Tianhang Zheng;Di Wang;Baochun Li;Jinhui Xu,th.zheng@mail.utoronto.ca;dwang45@buffalo.edu;bli@ece.toronto.edu;jinhui@buffalo.edu,3;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,3,4,0.0,yes,9/25/19,"Toronto University;State University of New York, Buffalo;University of Toronto;State University of New York, Buffalo",18;84;18;84,18;263;18;263,4
4868,4868,4868,4868,4868,4868,4868,4868,ICLR,2020,Closed loop deep Bayesian inversion:  Uncertainty driven acquisition for fast MRI,Thomas Sanchez;Igor Krawczuk;Zhaodong Sun;Volkan Cevher,thomas.sanchez@epfl.ch;igor.krawczuk@epfl.ch;zhaodong.sun@epfl.ch;volkan.cevher@epfl.ch,3;3;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne,481;481;481;481,38;38;38;38,5;4;11
4869,4869,4869,4869,4869,4869,4869,4869,ICLR,2020,Scalable Differentially Private Data Generation via Private  Aggregation  of  Teacher Ensembles,Yunhui Long;Suxin Lin;Zhuolin Yang;Carl A. Gunter;Han Liu;Bo Li,ylong4@illinois.edu;linsuxin28@gmail.com;lucas110550@sjtu.edu.cn;cgunter@illinois.edu;hanliu@northwestern.edu;lbo@illinois.edu,3;1;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,4,0.0,yes,9/25/19,"University of Illinois, Urbana Champaign;University of Electronic Science and Technology of China;Shanghai Jiao Tong University;University of Illinois, Urbana Champaign;Northwestern University;University of Illinois, Urbana Champaign",3;481;53;3;44;3,48;628;157;48;22;48,5;4;1
4870,4870,4870,4870,4870,4870,4870,4870,ICLR,2020,Adaptive Generation of Unrestricted Adversarial Inputs,Isaac Dunn;Hadrien Pouget;Tom Melham;Daniel Kroening,isaac.dunn@cs.ox.ac.uk;hadrien.pouget@cs.ox.ac.uk;tom.melham@cs.ox.ac.uk;kroening@cs.ox.ac.uk,3;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,9,0.0,yes,9/25/19,University of Oxford;University of Oxford;University of Oxford;University of Oxford,50;50;50;50,1;1;1;1,5;4
4871,4871,4871,4871,4871,4871,4871,4871,ICLR,2020,GPNET: MONOCULAR 3D VEHICLE DETECTION BASED ON LIGHTWEIGHT WHEEL GROUNDING POINT DETECTION NETWORK,zizhang.wu,wuzizhang87@gmail.com,1;1;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,,,,
4872,4872,4872,4872,4872,4872,4872,4872,ICLR,2020,Training Neural Networks for and by Interpolation,Leonard Berrada;Andrew Zisserman;Pawan M. Kumar,lberrada@robots.ox.ac.uk;az@robots.ox.ac.uk;pawan@robots.ox.ac.uk,1;6;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,5,0.0,yes,9/25/19,University of Oxford;University of Oxford;University of Oxford,50;50;50,1;1;1,
4873,4873,4873,4873,4873,4873,4873,4873,ICLR,2020,On the expected running time of nonconvex optimization with early stopping,Thomas Flynn;Kwang Min Yu;Abid Malik;Shinjae Yoo;Nicholas D'Imperio,thomasflynn918@gmail.com;kyu@bnl.gov;amalik@bnl.gov;sjyoo@bnl.gov;dimperio@bnl.gov,3;6;3,I have published one or two papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I did not assess the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Brookhaven National Laboratory;Brookhaven National Laboratory;Brookhaven National Laboratory;Brookhaven National Laboratory;Brookhaven National Laboratory,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,1;9;8
4874,4874,4874,4874,4874,4874,4874,4874,ICLR,2020,Interpreting video features: a comparison of 3D convolutional networks and convolutional LSTM networks,Joonatan Mänttäri*;Sofia Broomé*;John Folkesson;Hedvig Kjellström,sbroome@kth.se;manttari@kth.se;johnf@kth.se;hedvig@kth.se,6;1;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,4,0.0,yes,9/25/19,"KTH Royal Institute of Technology, Stockholm, Sweden;KTH Royal Institute of Technology, Stockholm, Sweden;KTH Royal Institute of Technology, Stockholm, Sweden;KTH Royal Institute of Technology, Stockholm, Sweden",128;128;128;128,222;222;222;222,2
4875,4875,4875,4875,4875,4875,4875,4875,ICLR,2020,Deep Multiple Instance Learning for Taxonomic Classification of Metagenomic read sets,Andreas Georgiou;Vincent Fortuin;Harun Mustafa;Gunnar Rätsch,geandrea@ethz.ch;fortuin@inf.ethz.ch;harun.mustafa@inf.ethz.ch;raetsch@inf.ethz.ch,3;3;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10;10,13;13;13;13,
4876,4876,4876,4876,4876,4876,4876,4876,ICLR,2020,Learning with Long-term Remembering: Following the Lead of Mixed Stochastic Gradient,Yunhui Guo;Mingrui Liu;Tianbao Yang;Tajana Rosing,yug185@eng.ucsd.edu;mingrui-liu@uiowa.edu;tianbao-yang@uiowa.edu;tajana@ucsd.edu,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0.0,yes,9/25/19,"University of California, San Diego;University of Iowa;University of Iowa;University of California, San Diego",11;154;154;11,31;227;227;31,
4877,4877,4877,4877,4877,4877,4877,4877,ICLR,2020,Skew-Fit: State-Covering Self-Supervised Reinforcement Learning,Vitchyr H. Pong;Murtaza Dalal;Steven Lin;Ashvin Nair;Shikhar Bahl;Sergey Levine,vitchyr@berkeley.edu;mdalal@berkeley.edu;stevenlin598@berkeley.edu;anair17@berkeley.edu;shikharbahl@berkeley.edu;svlevine@eecs.berkeley.edu,6;3;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,9,0.0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5;5;5,13;13;13;13;13;13,
4878,4878,4878,4878,4878,4878,4878,4878,ICLR,2020,Uncertainty - sensitive learning and planning with ensembles,Piotr Miłoś;Łukasz Kuciński;Konrad Czechowski;Piotr Kozakowski;Maciej Klimek,pmilos@mimuw.edu.pl;lukasz.kucinski@gmail.com;konrad.czechowski@gmail.com;p.kozakowski@mimuw.edu.pl;maciej.klimek@gmail.com,3;6;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,3,0.0,yes,9/25/19,"University of Washington, Seattle;Institute of Mathematics Polish Academy of Sciences;University of Washington, Seattle;University of Washington, Seattle;",6;-1;6;6;-1,26;-1;26;26;-1,
4879,4879,4879,4879,4879,4879,4879,4879,ICLR,2020,Gradient-based training of Gaussian Mixture Models in High-Dimensional Spaces,Alexander Gepperth;Benedikt Pfülb,alexander.gepperth@cs.hs-fulda.de;benedikt.pfuelb@cs.hs-fulda.de,3;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,HS Fulda;HS Fulda,-1;-1,-1;-1,1
4880,4880,4880,4880,4880,4880,4880,4880,ICLR,2020,Counterfactual Regularization for Model-Based Reinforcement Learning,Lawrence Neal;Li Fuxin;Xiaoli Fern,nealla@oregonstate.edu;fuxin.li@oregonstate.edu;xiaoli.fern@oregonstate.edu,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,1,0.0,yes,9/25/19,Oregon State University;Oregon State University;Oregon State University,77;77;77,373;373;373,
4881,4881,4881,4881,4881,4881,4881,4881,ICLR,2020,Three-Head Neural Network Architecture for AlphaZero Learning,Chao Gao;Martin Mueller;Ryan Hayward;Hengshuai Yao;Shangling Jui,cgao3@ualberta.ca;mmueller@ualberta.ca;hayward@ualberta.ca;hengshuai.yao@huawei.com;jui.shangling@huawei.com,6;3;6,I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,University of Alberta;University of Alberta;University of Alberta;Huawei Technologies Ltd.;Huawei Technologies Ltd.,100;100;100;-1;-1,136;136;136;-1;-1,8
4882,4882,4882,4882,4882,4882,4882,4882,ICLR,2020,GATO: Gates Are Not the Only Option,Mark Goldstein*;Xintian Han*;Rajesh Ranganath,goldstein@nyu.edu;xh1007@nyu.edu;rajeshr@cims.nyu.edu,3;8;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,New York University;New York University;New York University,25;25;25,29;29;29,1
4883,4883,4883,4883,4883,4883,4883,4883,ICLR,2020,Collaborative Inter-agent Knowledge Distillation for Reinforcement Learning,Zhang-Wei Hong;Prabhat Nagarajan;Guilherme Maeda,williamd4112@gapp.nthu.edu.tw;prabhat@preferred.jp;gjmaeda@preferred.jp,3;8;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,"National Tsing Hua University;Preferred Networks, Inc.;Preferred Networks, Inc.",172;-1;-1,365;-1;-1,
4884,4884,4884,4884,4884,4884,4884,4884,ICLR,2020,THE EFFECT OF ADVERSARIAL TRAINING: A THEORETICAL CHARACTERIZATION,Mingyang Yi;Huishuai Zhang;Wei Chen;Zhi-Ming Ma;Tie-Yan Liu,yimingyang17@mails.ucas.edu.cn;huzhang@microsoft.com;wche@microsoft.com;mazm@amt.ac.cn;tie-yan.liu@microsoft.com,1;1;1,I have read many papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,4,0.0,yes,9/25/19,University of Chinese Academy of Sciences;Microsoft;Microsoft;Chinese Academy of Sciences;Microsoft,59;-1;-1;59;-1,1397;-1;-1;1397;-1,4;1
4885,4885,4885,4885,4885,4885,4885,4885,ICLR,2020,Flexible and Efficient Long-Range Planning Through Curious Exploration,Aidan Curtis;Minjian Xin;Kevin Feigelis;Dan Yamins,southpawac@gmail.com;xinminjian@sjtu.edu.cn;feigelis@stanford.edu;yamins@stanford.edu,3;1;6,I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,16,0.0,yes,9/25/19,Rice University;Shanghai Jiao Tong University;Stanford University;Stanford University,84;53;4;4,105;157;4;4,
4886,4886,4886,4886,4886,4886,4886,4886,ICLR,2020,Learning Latent Dynamics for Partially-Observed Chaotic Systems,Said ouala;Duong Nguyen;Lucas Drumetz;Bertrand Chapron;Ananda Pascual;Fabrice Collard;Lucile Gaultier;Ronan Fablet,said.ouala@imt-atlantique.fr;van.nguyen1@imt-atlantique.fr;lucas.drumetz@imt-atlantique.fr;bertrand.chapron@ifremer.fr;ananda.pascual@imedea.uib-csic.es;dr.fab@oceandatalab.com;lucile.gaultier@oceandatalab.com;ronan.fablet@imt-atlantique.fr,3;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0.0,yes,9/25/19,IMT Atlantique;IMT Atlantique;IMT Atlantique;;Spanish National Research Council;Oceandatalab;Oceandatalab;IMT Atlantique,481;481;481;-1;-1;-1;-1;481,393;393;393;-1;-1;-1;-1;393,1
4887,4887,4887,4887,4887,4887,4887,4887,ICLR,2020,Make Lead Bias in Your Favor: A Simple and Effective Method for News Summarization,Chenguang Zhu;Ziyi Yang;Robert Gmyr;Michael Zeng;Xuedong Huang,chezhu@microsoft.com;zy99@stanford.edu;rogmyr@microsoft.com;nzeng@microsoft.com;xdh@microsoft.com,6;1;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,6,0.0,yes,9/25/19,Microsoft;Stanford University;Microsoft;Microsoft;Microsoft,-1;4;-1;-1;-1,-1;4;-1;-1;-1,
4888,4888,4888,4888,4888,4888,4888,4888,ICLR,2020,High performance RNNs with spiking neurons,Manu V Nair;Giacomo Indiveri,mnair@ini.uzh.ch;giacomo@ini.uzh.ch,6;6;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,5,1.0,yes,9/25/19,University of Zurich;University of Zurich,143;143,90;90,
4889,4889,4889,4889,4889,4889,4889,4889,ICLR,2020,Universal Learning Approach for Adversarial Defense,Uriya Pesso;Koby Bibas;Meir Feder,uriyapes@gmail.com;kobybibas@gmail.com;meir@eng.tau.ac.il,3;1;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,3,0.0,yes,9/25/19,Tel Aviv University;Tel Aviv University;Tel Aviv University,35;35;35,188;188;188,4
4890,4890,4890,4890,4890,4890,4890,4890,ICLR,2020,Re-Examining Linear Embeddings for High-dimensional Bayesian Optimization,Benjamin Letham;Roberto Calandra;Akshara Rai;Eytan Bakshy,bletham@fb.com;rcalandra@fb.com;akshararai@fb.com;ebakshy@fb.com,3;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Facebook;Facebook;Facebook;Facebook,-1;-1;-1;-1,-1;-1;-1;-1,11
4891,4891,4891,4891,4891,4891,4891,4891,ICLR,2020,Salient Explanation for Fine-grained Classification,Kanghan Oh;Sungchan Kim;Il-Seok Oh,blastps@gmail.com;s.k@jbnu.ac.kr;iosh@jbnu.ac.kr,1;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Chonbuk National University;Chonbuk National University;Chonbuk National University,-1;-1;-1,-1;-1;-1,1
4892,4892,4892,4892,4892,4892,4892,4892,ICLR,2020,Deep Coordination Graphs,Wendelin Boehmer;Vitaly Kurin;Shimon Whiteson,wendelin.boehmer@cs.ox.ac.uk;vitaly.kurin@cs.ox.ac.uk;shimon.whiteson@cs.ox.ac.uk,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,University of Oxford;University of Oxford;University of Oxford,50;50;50,1;1;1,10;8
4893,4893,4893,4893,4893,4893,4893,4893,ICLR,2020,Adversarial Training Generalizes Data-dependent Spectral Norm Regularization,Kevin Roth;Yannic Kilcher;Thomas Hofmann,kevin.roth@inf.ethz.ch;yannic.kilcher@inf.ethz.ch;thomas.hofmann@inf.ethz.ch,1;3;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,11,0.0,yes,9/25/19,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10,13;13;13,4;1
4894,4894,4894,4894,4894,4894,4894,4894,ICLR,2020,TriMap: Large-scale Dimensionality Reduction Using Triplets,Ehsan Amid;Manfred K. Warmuth,eamid@ucsc.edu;manfred@ucsc.edu,3;1;3,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,University of Southern California;University of Southern California,31;31,62;62,
4895,4895,4895,4895,4895,4895,4895,4895,ICLR,2020,Unsupervised Temperature Scaling: Robust Post-processing Calibration for Domain Shift,Azadeh Sadat Mozafari;Hugo Siqueira Gomes;Christian Gagne,azadeh-sadat.mozafari.1@ulaval.ca;hugo.siqueira-gomes.1@ulaval.ca;christian.gagne@gel.ulaval.ca,3;1;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,10,0.0,yes,9/25/19,Laval university;Laval university;Laval university,481;481;481,272;272;272,
4896,4896,4896,4896,4896,4896,4896,4896,ICLR,2020,Uncertainty-Aware Prediction for Graph Neural Networks,Xujiang Zhao;Feng Chen;Shu Hu;jin-Hee Cho,xxz190020@utdallas.edu;feng.chen@utdallas.edu;shu2@albany.edu;jicho@vt.edu,3;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,"University of Texas, Dallas;University of Texas, Dallas;State University of New York, Albany;Virginia Tech",86;86;266;79,319;319;350;240,11;10
4897,4897,4897,4897,4897,4897,4897,4897,ICLR,2020,Hydra: Preserving Ensemble Diversity for Model Distillation,Linh Tran;Bastiaan S. Veeling;Kevin Roth;Jakub Świątkowski;Joshua V. Dillon;Jasper Snoek;Stephan Mandt;Tim Salimans;Sebastian Nowozin;Rodolphe Jenatton,linh.tran@imperial.ac.uk;basveeling@gmail.com;kevin.roth@inf.ethz.ch;kuba.swiatkowski@gmail.com;jvdillon@google.com;jaspersnoek@gmail.com;stephan.mandt@gmail.com;salimans@google.com;nowozin@google.com;rjenatton@google.com,6;3;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,5,0.0,yes,9/25/19,"Imperial College London;Google;Swiss Federal Institute of Technology;;Google;Google;University of California, Irvine;Google;Google;Google",73;-1;10;-1;-1;-1;35;-1;-1;-1,10;-1;13;-1;-1;-1;96;-1;-1;-1,
4898,4898,4898,4898,4898,4898,4898,4898,ICLR,2020,Finding Deep Local Optima Using Network Pruning,Yangzi Guo;Yiyuan She;Ying Nian Wu;Adrian Barbu,yguo@math.fsu.edu;yshe@stat.fsu.edu;ywu@stat.ucla.edu;abarbu@stat.fsu.edu,3;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,"SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;University of California, Los Angeles;SUN YAT-SEN UNIVERSITY",481;481;20;481,299;299;17;299,
4899,4899,4899,4899,4899,4899,4899,4899,ICLR,2020,Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning,Xue Bin Peng;Aviral Kumar;Grace Zhang;Sergey Levine,xbpeng@berkeley.edu;aviralkumar2907@gmail.com;grace.zhang@berkeley.edu;svlevine@eecs.berkeley.edu,6;3;6,I have published one or two papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,31,0.0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5,13;13;13;13,
4900,4900,4900,4900,4900,4900,4900,4900,ICLR,2020,Generative Teaching Networks: Accelerating Neural Architecture Search by Learning  to Generate Synthetic Training Data,Felipe Petroski Such;Aditya Rawal;Joel Lehman;Kenneth Stanley;Jeff Clune,felipe.such@uber.com;aditya.rawal@uber.com;joel.lehman@uber.com;kstanley@uber.com;jeffclune@uber.com,3;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,13,0.0,yes,9/25/19,Uber;Uber;Uber;Uber;Uber,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,5
4901,4901,4901,4901,4901,4901,4901,4901,ICLR,2020,Compression without Quantization,Gergely Flamich;Marton Havasi;José Miguel Hernández-Lobato,gf332@cam.ac.uk;mh740@cam.ac.uk;jmh233@cam.ac.uk,3;3;6;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,5,0.0,yes,9/25/19,University of Cambridge;University of Cambridge;University of Cambridge,71;71;71,3;3;3,
4902,4902,4902,4902,4902,4902,4902,4902,ICLR,2020,CLAREL: classification via retrieval loss for zero-shot learning,Boris N. Oreshkin;Negar Rostamzadeh;Pedro O. Pinheiro;Christopher Pal,boris@elementai.com;negar@elementai.com;pedro@elementai.com;christopher.pal@elementai.com,1;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,Element AI;Element AI;Element AI;Element AI,-1;-1;-1;-1,-1;-1;-1;-1,6
4903,4903,4903,4903,4903,4903,4903,4903,ICLR,2020,Generating Semantic Adversarial Examples with Differentiable Rendering,Lakshya Jain;Steven Chen;Wilson Wu;Uyeong Jang;Varun Chandrasekaran;Sanjit Seshia;Somesh Jha,lakshya.jain@berkeley.edu;scchen@berkeley.edu;wilswu@berkeley.edu;wjang@cs.wisc.edu;vchandrasek4@wisc.edu;sseshia@eecs.berkeley.edu;jha@cs.wisc.edu,3;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of Southern California;University of Southern California;University of California Berkeley;University of Southern California,5;5;5;31;31;5;31,13;13;13;62;62;13;62,4;10
4904,4904,4904,4904,4904,4904,4904,4904,ICLR,2020,Constant Time Graph Neural Networks,Ryoma Sato;Makoto Yamada;Hisashi Kashima,r.sato@ml.ist.i.kyoto-u.ac.jp;myamada@i.kyoto-u.ac.jp;kashima@i.kyoto-u.ac.jp,6;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Meiji University;Meiji University;Meiji University,481;481;481,332;332;332,10
4905,4905,4905,4905,4905,4905,4905,4905,ICLR,2020,SSE-PT: Sequential Recommendation Via Personalized Transformer,Liwei Wu;Shuqing Li;Cho-Jui Hsieh;James Sharpnack,liwu@ucdavis.edu;qshli@ucdavis.edu;chohsieh@cs.ucla.edu;jsharpna@ucdavis.deu,1;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,"University of California, Davis;University of California, Davis;University of California, Los Angeles;",79;79;20;-1,55;55;17;-1,3
4906,4906,4906,4906,4906,4906,4906,4906,ICLR,2020,Adaptive Learned Bloom Filter (Ada-BF): Efficient Utilization of the Classifier,Zhenwei Dai;Anshumali Shrivastava,zd11@rice.edu;anshumali@rice.edu,3;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Rice University;Rice University,84;84,105;105,
4907,4907,4907,4907,4907,4907,4907,4907,ICLR,2020,Classification Attention for Chinese NER,Yuchen Ge;FanYang;PeiYang,geyc2@lenovo.com;yangfan24@lenovo.com;yangpei4@lenovo.com,3;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,0,0.0,yes,9/25/19,Lenovo Research;Lenovo Research;Lenovo Research,-1;-1;-1,-1;-1;-1,3
4908,4908,4908,4908,4908,4908,4908,4908,ICLR,2020,Global-Local Network for Learning Depth with Very Sparse Supervision,Antonio Loquercio;Alexey Dosovitskiy;Davide Scaramuzza,loquercio@ifi.uzh.ch;adosovitskiy@google.com;sdavide@ifi.uzh.ch,6;1;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,University of Zurich;Google;University of Zurich,143;-1;143,90;-1;90,
4909,4909,4909,4909,4909,4909,4909,4909,ICLR,2020,Localized Generations with Deep Neural Networks for Multi-Scale Structured Datasets,Yoshihiro Nagano;Shiro Takagi;Yuki Yoshida;Masato Okada,nagano@mns.k.u-tokyo.ac.jp;takagi@mns.k.u-tokyo.ac.jp;yoshida@mns.k.u-tokyo.ac.jp;okada@edu.k.u-tokyo.ac.jp,3;3;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,The University of Tokyo;The University of Tokyo;The University of Tokyo;The University of Tokyo,56;56;56;56,36;36;36;36,5;6
4910,4910,4910,4910,4910,4910,4910,4910,ICLR,2020,CrossNorm: On Normalization for Off-Policy Reinforcement Learning,Aditya Bhatt;Max Argus;Artemij Amiranashvili;Thomas Brox,aditya@bhatts.org;argus.max@gmail.com;amiranas@cs.uni-freiburg.de;brox@cs.uni-freiburg.de,3;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,TU Berlin;Universität Freiburg;Universität Freiburg;Universität Freiburg,108;118;118;118,149;85;85;85,
4911,4911,4911,4911,4911,4911,4911,4911,ICLR,2020,Diagonal Graph Convolutional Networks with Adaptive Neighborhood Aggregation,Jie Zhang;Yuxiao Dong;Jie Tang,zhangjie.exe@gmail.com;yuxdong@microsoft.com;jietang@tsinghua.edu.cn,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,webank;Microsoft;Tsinghua University,-1;-1;8,-1;-1;23,10
4912,4912,4912,4912,4912,4912,4912,4912,ICLR,2020,Variance Reduced Local SGD with Lower Communication Complexity,Xianfeng Liang;Shuheng Shen;Jingchang Liu;Zhen Pan;Yifei Cheng;Enhong Chen,zeroxf@mail.ustc.edu.cn;vaip@mail.ustc.edu.cn;jliude@cse.ust.hk;pzhen@mail.ustc.edu.cn;chengyif@mail.ustc.edu.cn;cheneh@ustc.edu.cn,3;1;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,University of Science and Technology of China;University of Science and Technology of China;The Hong Kong University of Science and Technology;University of Science and Technology of China;University of Science and Technology of China;University of Science and Technology of China,481;481;39;481;481;481,80;80;47;80;80;80,1
4913,4913,4913,4913,4913,4913,4913,4913,ICLR,2020,White Box Network: Obtaining a right composition ordering of functions,Eun saem Lee;Hyung Ju Hwang,dmstoa2502@postech.ac.kr;hjhwang@postech.ac.kr,1;1;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,POSTECH;POSTECH,118;118,146;146,
4914,4914,4914,4914,4914,4914,4914,4914,ICLR,2020,Neural Embeddings for Nearest Neighbor Search Under Edit Distance,Xiyuan Zhang;Yang Yuan;Piotr Indyk,zhangxiyuan@zju.edu.cn;yuanyang@tsinghua.edu.cn;indyk@mit.edu,6;6;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,Zhejiang University;Tsinghua University;Massachusetts Institute of Technology,56;8;2,107;23;5,
4915,4915,4915,4915,4915,4915,4915,4915,ICLR,2020,Accelerating First-Order Optimization Algorithms,Ange Tato;Roger Nkambou,angetato@gmail.com;nkambou@gmail.com,3;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,;,-1;-1,-1;-1,1
4916,4916,4916,4916,4916,4916,4916,4916,ICLR,2020,A SIMPLE AND EFFECTIVE FRAMEWORK FOR PAIRWISE DEEP METRIC LEARNING,Qi Qi;Yan Yan;Zixuan Wu;Xiaoyu Wang;Tianbao Yang,qi-qi@uiowa.edu;yanyan.tju@gmail.com;wuzu@bc.edu;fanghuaxue@gmail.com;tianbao-yang@uiowa.edu,6;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,University of Iowa;University of Iowa;Boston College;Snap Inc.;University of Iowa,154;154;266;-1;154,227;227;323;-1;227,2
4917,4917,4917,4917,4917,4917,4917,4917,ICLR,2020,Local Label Propagation for Large-Scale Semi-Supervised Learning,Chengxu Zhuang;Chaofei Fan;Xuehao Ding;Divyanshu Murli;Daniel Yamins,chengxuz@stanford.edu;stfan@stanford.edu;xhding@stanford.edu;divymurli@gmail.com;yamins@stanford.edu,3;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,Stanford University;Stanford University;Stanford University;Kount Inc;Stanford University,4;4;4;-1;4,4;4;4;-1;4,
4918,4918,4918,4918,4918,4918,4918,4918,ICLR,2020,Sparse Skill Coding: Learning Behavioral Hierarchies with Sparse Codes,Sophia Sanborn;Michael Chang;Sergey Levine;Thomas Griffiths,sanborn@berkeley.edu;mbchang@berkeley.edu;svlevine@eecs.berkeley.edu;tomg@princeton.edu,1;6;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,1,0.0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley;Princeton University,5;5;5;31,13;13;13;6,
4919,4919,4919,4919,4919,4919,4919,4919,ICLR,2020,"JAX MD: End-to-End Differentiable, Hardware Accelerated, Molecular Dynamics in Pure Python",Samuel S. Schoenholz;Ekin D. Cubuk,schsam@google.com;cubuk@google.com,6;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I do not know much about this area.:N/A:I made a quick assessment of this paper.:N/A,Reject,0,3,0.0,yes,9/25/19,Google;Google,-1;-1,-1;-1,
4920,4920,4920,4920,4920,4920,4920,4920,ICLR,2020,ROS-HPL: Robotic Object Search with Hierarchical Policy Learning and Intrinsic-Extrinsic Modeling,Xin Ye;Shibin Zheng;Yezhou Yang,xinye1@asu.edu;szheng31@asu.edu;yz.yang@asu.edu,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,3,0.0,yes,9/25/19,Arizona State University;Arizona State University;Arizona State University,95;95;95,155;155;155,
4921,4921,4921,4921,4921,4921,4921,4921,ICLR,2020,Fractional Graph Convolutional Networks (FGCN) for Semi-Supervised Learning,Yuzhou Chen;Yulia R. Gel;Konstantin Avrachenkov,yuzhouc@smu.edu;ygl@utdallas.edu;konstentin.avratchankov@inria.fr,6;3;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,1,0,0.0,yes,9/25/19,"Singapore Management University;University of Texas, Dallas;INRIA",92;86;-1,1397;319;-1,10
4922,4922,4922,4922,4922,4922,4922,4922,ICLR,2020,Graph Neural Networks For Multi-Image Matching,Stephen Phillips;Kostas Daniilidis,stephi@seas.upenn.edu;kostas@seas.upenn.edu,3;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0.0,yes,9/25/19,University of Pennsylvania;University of Pennsylvania,19;19,11;11,2;10
4923,4923,4923,4923,4923,4923,4923,4923,ICLR,2020,Cost-Effective Testing of a Deep Learning Model through Input Reduction,Jianyi Zhou;Feng Li;Jinhao Dong;Hongyu Zhang;Dan Hao,zhoujianyi@pku.edu.cn;lifeng2014@pku.edu.cn;xdu_jhdong@163.com;hongyu.zhang@newcastle.edu.au;haod@sei.pku.edu.cn,8;6;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,"Peking University;Peking University;Xidian University;University of Newcastle, Australia;Peking University",22;22;481;390;22,24;24;919;311;24,
4924,4924,4924,4924,4924,4924,4924,4924,ICLR,2020,Decoupling Weight Regularization from Batch Size for Model Compression,Dongsoo Lee;Se Jung Kwon;Byeongwook Kim;Yongkweon Jeon;Baeseong Park;Jeongin Yun;Gu-Yeon Wei,dslee3@gmail.com;mogndrewk@gmail.com;quddnr145@gmail.com;dragwon.jeon@gmail.com;qkrqotjd91@gmail.com;yji6373@naver.com;gywei@g.harvard.edu,8;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,;Samsung;Samsung;Samsung;;Samsung;Harvard University,-1;-1;-1;-1;-1;-1;39,-1;-1;-1;-1;-1;-1;7,
4925,4925,4925,4925,4925,4925,4925,4925,ICLR,2020,Amharic Text Normalization with Sequence-to-Sequence Models,Seifedin Shifaw Mohamed;Solomon Teferra Abate (PhD),seifedin28@gmail.com;solomon_teferra_7@yahoo.com,1;1;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Addis Ababa University;,481;-1,1397;-1,3
4926,4926,4926,4926,4926,4926,4926,4926,ICLR,2020,Star-Convexity in Non-Negative Matrix Factorization,Johan Bjorck;Carla Gomes;Kilian Weinberger,njb225@cornell.edu;gomes@cs.cornell.edu;kilianweinberger@cornell.edu,3;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,Cornell University;Cornell University;Cornell University,7;7;7,19;19;19,
4927,4927,4927,4927,4927,4927,4927,4927,ICLR,2020,Learning Compact Reward for Image Captioning,Nannan Li;Zhenzhong Chen,live@whu.edu.cn;zzchen@whu.edu.cn,6;1;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Wuhan University;Wuhan University,266;266,354;354,4;9
4928,4928,4928,4928,4928,4928,4928,4928,ICLR,2020,Learning Entailment-Based Sentence Embeddings from Natural Language Inference,Rabeeh Karimi Mahabadi*;Florian Mai*;James Henderson,rkarimi@idiap.ch;florian.mai@idiap.ch;james.henderson@idiap.ch,6;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:N/A:I assessed the sensibility of the derivations and theory.,Reject,0,3,1.0,yes,9/25/19,Idiap Research Institute;Idiap Research Institute;Idiap Research Institute,-1;-1;-1,-1;-1;-1,3
4929,4929,4929,4929,4929,4929,4929,4929,ICLR,2020,Word embedding re-examined: is the symmetrical factorization optimal?,Zhichao Han;Jia Li;Xu Li;Hong Cheng,zchan@se.cuhk.edu.hk;lijia@se.cuhk.edu.hk;xuli@se.cuhk.edu.hk;hcheng@se.cuhk.edu.hk,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,The Chinese University of Hong Kong;The Chinese University of Hong Kong;The Chinese University of Hong Kong;The Chinese University of Hong Kong,59;59;59;59,35;35;35;35,
4930,4930,4930,4930,4930,4930,4930,4930,ICLR,2020,Structured consistency loss for semi-supervised semantic segmentation,JongMok Kim;Joo Young Jang;Hyunwoo Park,win98man1@gmail.com;jyjang1090@gmail.com;phw08132@gmail.com,1;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,;Hyundai Mobis;,-1;-1;-1,-1;-1;-1,2
4931,4931,4931,4931,4931,4931,4931,4931,ICLR,2020,Attention on Abstract Visual Reasoning,Lukas Hahne;Timo Lüddecke;Florentin Wörgötter;David Kappel,l.hahne@stud.uni-goettingen.de;timo.lueddecke@phys.uni-goettingen.de;worgott@gwdg.de;david.kappel@phys.uni-goettingen.de,3;3;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,2,0.0,yes,9/25/19,University of Goettingen;University of Goettingen;;University of Goettingen,323;323;-1;323,123;123;-1;123,
4932,4932,4932,4932,4932,4932,4932,4932,ICLR,2020,Event extraction from unstructured Amharic text,Ephrem Tadesse;Rosa Tsegaye;Kuulaa Qaqqabaa,ephe11ta@gmail.com;rosatsegaye@gmail.com;kuulaa@gmail.com,3;1;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,;;Addis Ababa Science and Technology University,-1;-1;-1,-1;-1;-1,
4933,4933,4933,4933,4933,4933,4933,4933,ICLR,2020,Discovering Topics With Neural Topic Models Built From PLSA Loss,sileye ba,sileye.ba@outlook.com,1;1;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,2,0,0.0,yes,9/25/19,,,,
4934,4934,4934,4934,4934,4934,4934,4934,ICLR,2020,Exploiting Excessive Invariance caused by Norm-Bounded Adversarial Robustness,Jörn-Henrik Jacobsen;Jens Behrmann;Nicholas Carlini;Florian Tramèr;Nicolas Papernot,j.jacobsen@vectorinstitute.ai;jensb@uni-bremen.de;nicholas@carlini.com;tramer@cs.stanford.edu;nicolas.papernot@utoronto.ca,3;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Vector Institute;Universität Bremen;Carlini;Stanford University;Toronto University,-1;154;-1;4;18,-1;360;-1;4;18,4
4935,4935,4935,4935,4935,4935,4935,4935,ICLR,2020,WikiMatrix: Mining 135M Parallel Sentences in 1620 Language Pairs from Wikipedia,Holger Schwenk;Vishrav Chaudhary;Shuo Sun;Hongyu Gong;Francisco Guzmán,schwenk@fb.com;vishrav@fb.com;ssun32@jhu.edu;hgong6@illinois.edu;fguzman@fb.com,3;6;8;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:N/A:N/A:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,"Facebook;Facebook;Johns Hopkins University;University of Illinois, Urbana Champaign;Facebook",-1;-1;73;3;-1,-1;-1;12;48;-1,
4936,4936,4936,4936,4936,4936,4936,4936,ICLR,2020,ProtoAttend: Attention-Based Prototypical Learning,Sercan O. Arik;Tomas Pfister,soarik@google.com;tpfister@google.com,3;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Google;Google,-1;-1,-1;-1,
4937,4937,4937,4937,4937,4937,4937,4937,ICLR,2020,Discourse-Based Evaluation of Language Understanding,Damien Sileo;Tim Van-De-Cruys;Camille Pradel;Philippe Muller,damien.sileo@irit.fr;tim.vandecruys@irit.fr;camille.pradel@synapse-fr.com;philippe.muller@irit.fr,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,5,0.0,yes,9/25/19,"IRIT, University of Toulouse;IRIT, University of Toulouse;Synapse-fr;IRIT, University of Toulouse",-1;-1;-1;-1,-1;-1;-1;-1,3
4938,4938,4938,4938,4938,4938,4938,4938,ICLR,2020,SINGLE PATH ONE-SHOT NEURAL ARCHITECTURE SEARCH WITH UNIFORM SAMPLING,Zichao Guo;Xiangyu Zhang;Haoyuan Mu;Wen Heng;Zechun Liu;Yichen Wei;Jian Sun,guozichao@megvii.com;zhangxiangyu@megvii.com;muhy17@mails.tsinghua.edu.cn;hengwen@megvii.com;zliubq@connect.ust.hk;weiyichen@megvii.com;sunjian@megvii.com,6;8;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,6,1.0,yes,9/25/19,Megvii Technology Inc.;Megvii Technology Inc.;Tsinghua University;Megvii Technology Inc.;The Hong Kong University of Science and Technology;Megvii Technology Inc.;Megvii Technology Inc.,-1;-1;8;-1;39;-1;-1,-1;-1;23;-1;47;-1;-1,
4939,4939,4939,4939,4939,4939,4939,4939,ICLR,2020,Inducing Stronger Object Representations in Deep Visual Trackers,Ross Goroshin;Jonathan Tompson;Debidatta Dwibedi,goroshin@google.com;tompson@google.com;debidatta@google.com,3;3;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,1,4,0.0,yes,9/25/19,Google;Google;Google,-1;-1;-1,-1;-1;-1,
4940,4940,4940,4940,4940,4940,4940,4940,ICLR,2020,Automatically Learning Feature Crossing from Model Interpretation for Tabular Data,Zhaocheng Liu;Qiang Liu;Haoli Zhang,zhaocheng.liu@realai.ai;qiang.liu@realai.ai;haoli.zhang@realai.ai,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,1,13,0.0,yes,9/25/19,RealAI;RealAI;RealAI,-1;-1;-1,-1;-1;-1,
4941,4941,4941,4941,4941,4941,4941,4941,ICLR,2020,Semi-Supervised Boosting via Self Labelling,Akul Goyal;Yang Liu,akulg2@illinois.edu;yangliu@ucsc.edu,1;1;3,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,"University of Illinois, Urbana Champaign;University of Southern California",3;31,48;62,8
4942,4942,4942,4942,4942,4942,4942,4942,ICLR,2020,Depth creates no more spurious local minima in linear networks,Li Zhang,liqzhang@google.com,3;6;3,I have read many papers in this area.:N/A:N/A:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Google,-1,-1,1
4943,4943,4943,4943,4943,4943,4943,4943,ICLR,2020,Learning by shaking: Computing policy gradients by physical forward-propagation,Arash Mehrjou;Ashkan Soleymani;Stefan Bauer;Bernhard Schölkopf,amehrjou@tuebingen.mpg.de;soli.ashkan98@gmail.com;stefan.bauer@tuebingen.mpg.de;bs@tuebingen.mpg.de,1;1;3;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,"Max Planck Institute for Intelligent Systems, Max-Planck Institute;Sharif University of Technology;Max Planck Institute for Intelligent Systems, Max-Planck Institute;Max Planck Institute for Intelligent Systems, Max-Planck Institute",-1;323;-1;-1,-1;564;-1;-1,
4944,4944,4944,4944,4944,4944,4944,4944,ICLR,2020,Consistent Meta-Reinforcement Learning via Model Identification and Experience Relabeling,Russell Mendonca;Xinyang Geng;Chelsea Finn;Sergey Levine,russellm@berkeley.edu;young.geng@berkeley.edu;cbfinn@cs.stanford.edu;svlevine@eecs.berkeley.edu,3;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0.0,yes,9/25/19,University of California Berkeley;University of California Berkeley;Stanford University;University of California Berkeley,5;5;4;5,13;13;4;13,
4945,4945,4945,4945,4945,4945,4945,4945,ICLR,2020,Accelerating Monte Carlo Bayesian Inference via Approximating Predictive Uncertainty over the Simplex,Yufei Cui;Wuguannan Yao;Qiao Li;Antoni Chan;Chun Jason Xue,yufeicui92@gmail.com;satie.yao@my.cityu.edu.hk;qiaoli045@gmail.com;abchan@cityu.edu.hk;jasonxue@cityu.edu.hk,3;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,6,0.0,yes,9/25/19,City University of Hong Kong;City University of Hong Kong;;City University of Hong Kong;City University of Hong Kong,92;92;-1;92;92,35;35;-1;35;35,4;11
4946,4946,4946,4946,4946,4946,4946,4946,ICLR,2020,Trajectory growth through random deep ReLU networks,Ilan Price;Jared Tanner,ilan.price@maths.ox.ac.uk;tanner@maths.ox.ac.uk,3;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,University of Oxford;University of Oxford,50;50,1;1,1
4947,4947,4947,4947,4947,4947,4947,4947,ICLR,2020,Ellipsoidal Trust Region Methods for Neural Network Training,Leonard Adolphs;Jonas Kohler;Aurelien Lucchi,ladolphs@inf.ethz.ch;jonas.kohler@inf.ethz.ch;aurelien.lucchi@inf.ethz.ch,3;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10,13;13;13,8
4948,4948,4948,4948,4948,4948,4948,4948,ICLR,2020,Defensive Tensorization: Randomized Tensor Parametrization for Robust Neural Networks,Adrian Bulat;Jean Kossaifi;Sourav Bhattacharya;Yannis Panagakis;Georgios Tzimiropoulos;Nicholas D.  Lane;Maja Pantic,adrian@adrianbulat.com;jean.kossaifi@gmail.com;bsourav@gmail.com;i.panagakis@imperial.ac.uk;georgios.t@samsung.com;nic.lane@samsung.com;maja.pantic@gmail.com,6;6;6,I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,1,5,0.0,yes,9/25/19,Samsung;Imperial College London;;Imperial College London;Samsung;Samsung;Imperial College London,-1;73;-1;73;-1;-1;73,-1;10;-1;10;-1;-1;10,4;2
4949,4949,4949,4949,4949,4949,4949,4949,ICLR,2020,The Dual Information Bottleneck,Zoe Piran;Naftali Tishby,zoe.piran@mail.huji.ac.il;tishby@cs.huji.ac.il,3;6;3,I have published one or two papers in this area.:N/A:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Hebrew University of Jerusalem;Hebrew University of Jerusalem,67;67,216;216,
4950,4950,4950,4950,4950,4950,4950,4950,ICLR,2020,ADAPTIVE GENERATION OF PROGRAMMING PUZZLES,Ashwin Kalyan;Oleksandr Polozov;Adam Tauman Kalai,ashwinkv@gatech.edu;alex.polozov@microsoft.com;adam.kalai@microsoft.com,3;8;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,4,0.0,yes,9/25/19,Georgia Institute of Technology;Microsoft;Microsoft,13;-1;-1,38;-1;-1,5
4951,4951,4951,4951,4951,4951,4951,4951,ICLR,2020,Feature-map-level Online Adversarial Knowledge Distillation,Inseop Chung;SeongUk Park;Jangho Kim;Nojun Kwak,jis3613@snu.ac.kr;swpark0703@snu.ac.kr;kjh91@snu.ac.kr;nojunk@snu.ac.kr,3;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Seoul National University;Seoul National University;Seoul National University;Seoul National University,41;41;41;41,64;64;64;64,4
4952,4952,4952,4952,4952,4952,4952,4952,ICLR,2020,Graph convolutional networks for learning with few clean and many noisy labels,Ahmet Iscen;Giorgos Tolias;Yannis Avrithis;Ondrej Chum;Cordelia Schmid,iscen@google.com;giorgos.tolias@cmp.felk.cvut.cz;yannis@avrithis.net;chum@cmp.felk.cvut.cz;cordelias@google.com,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,3,0.0,yes,9/25/19,Google;Czech Technical University in Prague;INRIA;Czech Technical University in Prague;Google,-1;323;-1;323;-1,-1;956;-1;956;-1,6;10
4953,4953,4953,4953,4953,4953,4953,4953,ICLR,2020,Visual Interpretability Alone Helps Adversarial Robustness,Akhilan Boopathy;Sijia Liu;Gaoyuan Zhang;Pin-Yu Chen;Shiyu Chang;Luca Daniel,akhilan@mit.edu;sijia.liu@ibm.com;gaoyuan.zhang@ibm.com;pin-yu.chen@ibm.com;shiyu.chang@ibm.com;dluca@mit.edu,3;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,9,0.0,yes,9/25/19,Massachusetts Institute of Technology;International Business Machines;International Business Machines;International Business Machines;International Business Machines;Massachusetts Institute of Technology,2;-1;-1;-1;-1;2,5;-1;-1;-1;-1;5,4
4954,4954,4954,4954,4954,4954,4954,4954,ICLR,2020,Linguistic Embeddings as a Common-Sense Knowledge Repository: Challenges and Opportunities,Nancy Fulda,nfulda@byu.edu,1;1;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,0,0.0,yes,9/25/19,Brigham Young University,-1,-1,3;1
4955,4955,4955,4955,4955,4955,4955,4955,ICLR,2020,Starfire: Regularization-Free Adversarially-Robust Structured Sparse Training,Noah Gamboa;Kais Kudrolli;Anand Dhoot;Ardavan Pedram,ngamboa@stanford.edu;kudrolli@stanford.edu;anandd@stanford.edu;perdavan@stanford.edu,1;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Reject,0,4,0.0,yes,9/25/19,Stanford University;Stanford University;Stanford University;Stanford University,4;4;4;4,4;4;4;4,4
4956,4956,4956,4956,4956,4956,4956,4956,ICLR,2020,Pixel Co-Occurence Based Loss Metrics for Super Resolution Texture Recovery,Ying Da Wang;Pawel Swietojanski;Ryan T Armstrong;Peyman Mostaghimi,yingda.wang@unsw.edu.au;p.swietojanski@unsw.edu.au;ryan.armstrong@unsw.edu.au;peyman@unsw.edu.au,1;1;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,1,0.0,yes,9/25/19,University of New South Wales;University of New South Wales;University of New South Wales;University of New South Wales,481;481;481;481,1397;1397;1397;1397,5;4
4957,4957,4957,4957,4957,4957,4957,4957,ICLR,2020,Cross-Iteration Batch Normalization,Zhuliang Yao;Yue Cao;Shuxin Zheng;Gao Huang;Stephen Lin;Jifeng Dai,yaozhuliang13@gmail.com;yuecao@microsoft.com;shuxin.zheng@microsoft.com;gaohuang@tsinghua.edu.cn;stevelin@microsoft.com;jifdai@microsoft.com,6;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,3,9,0.0,yes,9/25/19,Tsinghua University;Microsoft;Microsoft;Tsinghua University;Microsoft;Microsoft,8;-1;-1;8;-1;-1,23;-1;-1;23;-1;-1,2
4958,4958,4958,4958,4958,4958,4958,4958,ICLR,2020,Exploration via Flow-Based Intrinsic Rewards,Hsuan-Kung Yang;Po-Han Chiang;Min-Fong Hong;Chun-Yi Lee,hellochick@gapp.nthu.edu.tw;ymmoy999@gapp.nthu.edu.tw;romulus@gapp.nthu.edu.tw;cylee@gapp.nthu.edu.tw,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,10,0.0,yes,9/25/19,National Tsing Hua University;National Tsing Hua University;National Tsing Hua University;National Tsing Hua University,172;172;172;172,365;365;365;365,2
4959,4959,4959,4959,4959,4959,4959,4959,ICLR,2020,Embodied Multimodal Multitask Learning,Devendra Singh Chaplot;Lisa Lee;Ruslan Salakhutdinov;Devi Parikh;Dhruv Batra,chaplot@cs.cmu.edu;lslee@cs.cmu.edu;rsalakhu@cs.cmu.edu;parikh@gatech.edu;dbatra@gatech.edu,3;3;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,3,0.0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Georgia Institute of Technology;Georgia Institute of Technology,1;1;1;13;13,27;27;27;38;38,
4960,4960,4960,4960,4960,4960,4960,4960,ICLR,2020,Towards Principled Objectives for Contrastive Disentanglement,Anwesa Choudhuri;Ashok Vardhan Makkuva;Ranvir Rana;Sewoong Oh;Girish Chowdhary;Alexander Schwing,anwesac2@illinois.edu;makkuva2@illinois.edu;rbrana2@illinois.edu;sewoong@cs.washington.edu;girishc@illinois.edu;aschwing@illinois.edu,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;University of Washington;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",3;3;3;6;3;3,48;48;48;26;48;48,
4961,4961,4961,4961,4961,4961,4961,4961,ICLR,2020,Hebbian Graph Embeddings,Shalin Shah;Venkataramana Kini,shalin.shah@target.com;venkataramana.kini@target.com,1;1;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Johns Hopkins University;Target,73;-1,12;-1,10
4962,4962,4962,4962,4962,4962,4962,4962,ICLR,2020,Probabilistic modeling the hidden layers of deep neural networks,Xinjie Lan;Kenneth E. Barner,lxjbit@udel.edu;barner@udel.edu,6;6;8,I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,8,0.0,yes,9/25/19,University of Delaware;University of Delaware,233;233,295;295,11;8
4963,4963,4963,4963,4963,4963,4963,4963,ICLR,2020,Empirical confidence estimates for classification by deep neural networks,Chris Finlay;Adam M. Oberman,christopher.finlay@gmail.com;adam.oberman@mcgill.ca,1;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,McGill University;McGill University,86;86,42;42,1
4964,4964,4964,4964,4964,4964,4964,4964,ICLR,2020,TED: A Pretrained Unsupervised Summarization Model with Theme Modeling and Denoising,Ziyi Yang;Chenguang Zhu;Michael Zeng;Xuedong Huang;Eric Darve,ziyi.yang@stanford.edu;chezhu@microsoft.com;nzeng@microsoft.com;xdh@microsoft.com;darve@stanford.edu,8;8;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:N/A:N/A,Reject,0,4,0.0,yes,9/25/19,Stanford University;Microsoft;Microsoft;Microsoft;Stanford University,4;-1;-1;-1;4,4;-1;-1;-1;4,
4965,4965,4965,4965,4965,4965,4965,4965,ICLR,2020,Biologically Plausible Neural Networks via Evolutionary Dynamics and Dopaminergic Plasticity,Sruthi Gorantla;Anand Louis;Christos H. Papadimitriou;Santosh Vempala;Naganand Yadati,sruthi@comp.nus.edu.sg;anandl@iisc.ac.in;christos@columbia.edu;vempala@gatech.edu;y.naganand@gmail.com,1;3;3,I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,National University of Singapore;Indian Institute of Science;Columbia University;Georgia Institute of Technology;Indian Institute of Science,16;95;15;13;95,25;301;16;38;301,1
4966,4966,4966,4966,4966,4966,4966,4966,ICLR,2020,Meta-Learning Initializations for Image Segmentation,Sean M. Hendryx;Andrew B. Leach;Paul D. Hein;Clayton T. Morrison,seanmhendryx@gmail.com;imaleach@gmail.com;pauldhein@email.arizona.edu;claytonm@email.arizona.edu,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0.0,yes,9/25/19,;Google;University of Arizona;University of Arizona,-1;-1;172;172,-1;-1;103;103,6;2;8
4967,4967,4967,4967,4967,4967,4967,4967,ICLR,2020,Sequence-level Intrinsic Exploration Model for Partially Observable Domains,Haiyan Yin;Jianda Chen;Sinno Jialin Pan,yinhaiyan@outlook.com;jianda001@e.ntu.edu.sg;sinnopan@ntu.edu.sg,6;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Reject,0,4,0.0,yes,9/25/19,National Taiwan University;National Taiwan University;National Taiwan University,86;86;86,120;120;120,
4968,4968,4968,4968,4968,4968,4968,4968,ICLR,2020,EINS: Long Short-Term Memory with Extrapolated Input Network Simplification,Nicholas I-Hsien Kuo;Mehrtash T. Harandi;Nicolas Fourrier;Gabriela Ferraro;Christian Walder;Hanna Suominen,u6424547@anu.edu.au;mehrtash.harandi@monash.edu;nicolas.fourrier@devinci.fr;gabriela.ferraro@csiro.au;gabriela.ferraro@data61.csiro.au;christian.walder@data61.csiro.au;hanna.suominen@anu.edu.au,1;1;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,"Australian National University;Monash University;Ecole Superieur d'Ingenieurs Leonard de Vinci;CSIRO;, CSIRO;, CSIRO;Australian National University",108;118;-1;-1;233;233;108,50;75;-1;-1;-1;-1;50,3;5
4969,4969,4969,4969,4969,4969,4969,4969,ICLR,2020,Connectivity-constrained interactive annotations for panoptic segmentation,Ruobing Shen;Bo Tang;Ismail Ben Ayed;Andrea Lodi;Thomas Guthier,ruobing.shen@gmobis.com;lucastang1994@gmail.com;ismail.benayed@etsmtl.ca;andrea.lodi@polymtl.ca;thomas.guthier@gmobis.com,1;3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Reject,0,5,0.0,yes,9/25/19,Hyundai Mobis;Northeastern University;École de technologie supérieure;Polytechnique Montreal;Hyundai Mobis,-1;16;481;390;-1,-1;906;1397;1397;-1,2;10
4970,4970,4970,4970,4970,4970,4970,4970,ICLR,2020,Frustratingly easy quasi-multitask learning,Gábor Berend;Norbert Kis-Szabó,berendg@inf.u-szeged.hu;kis-szabo.norbert@stud.u-szeged.hu,1;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,2,0.0,yes,9/25/19,University of Szeged;University of Szeged,323;323,874;874,8
4971,4971,4971,4971,4971,4971,4971,4971,ICLR,2020,Towards Effective 2-bit Quantization: Pareto-optimal Bit Allocation for Deep CNNs Compression,Zhe Wang;Jie Lin;Mohamed M. Sabry Aly;Sean I Young;Vijay Chandrasekhar;Bernd Girod,mark.wangzhe@gmail.com;lin-j@i2r.a-star.edu.sg;msabry@ntu.edu.sg;sean.i.young@stanford.edu;vijay@i2r.a-star.edu.sg;bgirod@stanford.edu,8;6;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,9,0.0,yes,9/25/19,Stanford University;A*STAR;National Taiwan University;Stanford University;A*STAR;Stanford University,4;-1;86;4;-1;4,4;-1;120;4;-1;4,
4972,4972,4972,4972,4972,4972,4972,4972,ICLR,2020,Representing Unordered Data Using Multiset Automata and Complex Numbers,Justin DeBenedetto;David Chiang,jdebened@nd.edu;dchiang@nd.edu,6;3;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,University of Notre Dame;University of Notre Dame,118;118,157;157,
4973,4973,4973,4973,4973,4973,4973,4973,ICLR,2020,Domain-invariant Learning using Adaptive Filter Decomposition,Ze Wang;Xiuyuan Cheng;Guillermo Sapiro;Qiang Qiu,ze.w@duke.edu;xiuyuan.cheng@duke.edu;guillermo.sapiro@duke.edu;qiang.qiu@duke.edu,6;1;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Duke University;Duke University;Duke University;Duke University,47;47;47;47,20;20;20;20,
4974,4974,4974,4974,4974,4974,4974,4974,ICLR,2020,Gradient-free Neural Network Training by Multi-convex Alternating Optimization,Junxiang Wang;Fuxun Yu;Xiang Chen;Liang Zhao,jwang40@gmu.edu;fyu2@gmu.edu;xchen26@gmu.edu;lzhao9@gmu.edu,1;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,2,0.0,yes,9/25/19,George Mason University;George Mason University;George Mason University;George Mason University,100;100;100;100,282;282;282;282,1;9
4975,4975,4975,4975,4975,4975,4975,4975,ICLR,2020,GDP: Generalized Device Placement for Dataflow Graphs,Yanqi Zhou;Sudip Roy;Amirali Abdolrashidi;Daniel Wong;Peter C. Ma;Qiumin Xu;Ming Zhong;Hanxiao Liu;Anna Goldie;Azalia Mirhoseini;James Laudon,yanqiz@google.com;sudipr@google.com;abdolrashidi@google.com;wonglkd@google.com;pcma@google.com;qiuminxu@google.com;mingzhong@google.com;hanxiaol@google.com;agoldie@google.com;azalia@google.com;jlaudon@google.com,6;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,10
4976,4976,4976,4976,4976,4976,4976,4976,ICLR,2020,Policy Optimization In the Face of Uncertainty,Tung-Long Vuong;Han Nguyen;Hai Pham;Kenneth Tran,longvt94@vnu.edu.vn;hann1@andrew.cmu.edu;htpham@cs.cmu.edu;ktran@microsoft.com,3;3;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.,Reject,0,3,0.0,yes,9/25/19,Australian National University;Carnegie Mellon University;Carnegie Mellon University;Microsoft,108;1;1;-1,50;27;27;-1,10;8
4977,4977,4977,4977,4977,4977,4977,4977,ICLR,2020,Learning relevant features for statistical inference,Cédric Bény,cedric.beny@gmail.com,3;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Hanyang University,233,393,11
4978,4978,4978,4978,4978,4978,4978,4978,ICLR,2020,Accelerating Reinforcement Learning Through GPU Atari Emulation,Steven Dalton;Michael Garland;Iuri Frosio,sdalton@nvidia.com;mgarland@nvidia.com;ifrosio@nvidia.com,1;8;8,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,NVIDIA;NVIDIA;NVIDIA,-1;-1;-1,-1;-1;-1,
4979,4979,4979,4979,4979,4979,4979,4979,ICLR,2020,Discrete InfoMax Codes for Meta-Learning,Yoonho Lee;Wonjae Kim;Seungjin Choi,einet89@gmail.com;dandelin.kim@kakaocorp.com;seungjin.choi.mlg@gmail.com,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,;Kakao;BARO,-1;-1;-1,-1;-1;-1,8;1;6
4980,4980,4980,4980,4980,4980,4980,4980,ICLR,2020,Efficient Saliency Maps for Explainable AI,T. Nathan Mundhenk;Barry Chen;Gerald Friedland,mundhenk1@llnl.gov;chen52@llnl.gov;fractor@eecs.berkeley.edu,6;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,22,0.0,yes,9/25/19,Lawrence Livermore National Labs;Lawrence Livermore National Labs;University of California Berkeley,-1;-1;5,-1;-1;13,
4981,4981,4981,4981,4981,4981,4981,4981,ICLR,2020,Monte Carlo Deep Neural Network Arithmetic,Julian Faraone;Philip Leong,julian.faraone@sydney.edu.au;philip.leong@sydney.edu.au,3;3;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,University of Sydney;University of Sydney,86;86,60;60,
4982,4982,4982,4982,4982,4982,4982,4982,ICLR,2020,Language-independent Cross-lingual Contextual Representations,Xiao Zhang;Song Wang;Dejing Dou;Xien Liu;Thien Huu Nguyen;Ji Wu,xzhang19@mails.tsinghua.edu.cn;wangsong16@mails.tsinghua.edu.cn;dou@cs.uoregon.edu;xeliu@mail.tsinghua.edu.cn;thien@cs.uoregon.edu;wuji_ee@mail.tsinghua.edu.cn,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,1,0.0,yes,9/25/19,Tsinghua University;Tsinghua University;University of Oregon;Tsinghua University;University of Oregon;Tsinghua University,8;8;205;8;205;8,23;23;288;23;288;23,3;6
4983,4983,4983,4983,4983,4983,4983,4983,ICLR,2020,Deep Nonlinear Stochastic Optimal Control for Systems with Multiplicative Uncertainties,Marcus Pereira;Ziyi Wang;Tianrong Chen;Evangelos Theodorou,mpereira30@gatech.edu;zwang450@gatech.edu;tianrong.chen@gatech.edu;evangelos.theodorou@gatech.edu,3;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology,13;13;13;13,38;38;38;38,
4984,4984,4984,4984,4984,4984,4984,4984,ICLR,2020,Causal Induction from Visual Observations for Goal Directed Tasks,Suraj Nair;Yuke Zhu;Silvio Savarese;Li Fei-Fei,surajn@stanford.edu;yukez@cs.stanford.edu;ssilvio@stanford.edu;feifeili@cs.stanford.edu,6;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Stanford University;Stanford University;Stanford University;Stanford University,4;4;4;4,4;4;4;4,6;10
4985,4985,4985,4985,4985,4985,4985,4985,ICLR,2020,Towards Understanding the Spectral Bias of Deep Learning,Yuan Cao;Zhiying Fang;Yue Wu;Ding-Xuan Zhou;Quanquan Gu,yuancao@cs.ucla.edu;zyfang4-c@my.cityu.edu.hk;ywu@cs.ucla.edu;mazhou@cityu.edu.hk;qgu@cs.ucla.edu,3;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,"University of California, Los Angeles;City University of Hong Kong;University of California, Los Angeles;City University of Hong Kong;University of California, Los Angeles",20;92;20;92;20,17;35;17;35;17,1;9;8
4986,4986,4986,4986,4986,4986,4986,4986,ICLR,2020,Reweighted Proximal Pruning for Large-Scale Language Representation,Fu-Ming Guo;Sijia Liu;Finlay S. Mungall;Xue Lin;Yanzhi Wang,elphinkuo@gmail.com;sijia.liu@ibm.com;fmungall@gmail.com;xue.lin@northeastern.edu;yanz.wang@northeastern.edu,6;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,1.0,yes,9/25/19,Northeastern University;International Business Machines;;Northeastern University;Northeastern University,16;-1;-1;16;16,906;-1;-1;906;906,3;6
4987,4987,4987,4987,4987,4987,4987,4987,ICLR,2020,Distilled embedding: non-linear embedding factorization using knowledge distillation,Vasileios Lioutas;Ahmad Rashid;Krtin Kumar;Md Akmal Haidar;Mehdi Rezagholizadeh,vasileios.lioutas@carleton.ca;ahmad.rashid@huawei.com;krtin.kumar@huawei.com;md.akmal.haidar@huawei.com;mehdi.rezagholizadeh@huawei.com,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,Carleton University;Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.,154;-1;-1;-1;-1,535;-1;-1;-1;-1,3
4988,4988,4988,4988,4988,4988,4988,4988,ICLR,2020,Switched linear projections and inactive state sensitivity for deep neural network interpretability,Lech Szymanski;Brendan McCane;Craig Atkinson,lechszym@cs.otago.ac.nz;mccane@cs.otago.ac.nz;atkcr398@student.otago.ac.nz,1;3;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,17,0.0,yes,9/25/19,University of Otago;University of Otago;University of Otago,390;390;390,218;218;218,
4989,4989,4989,4989,4989,4989,4989,4989,ICLR,2020,SpectroBank: A filter-bank convolutional layer for CNN-based audio applications,Helena Peic Tukuljac;Benjamin Ricaud;Nicolas Aspert;Pierre Vandergheynst,helena.peictukuljac@epfl.ch;benjamin.ricaud@epfl.ch;nicolas.aspert@epfl.ch;pierre.vandergheynst@epfl.ch,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0.0,yes,9/25/19,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne,481;481;481;481,38;38;38;38,
4990,4990,4990,4990,4990,4990,4990,4990,ICLR,2020,Stochastic Latent Residual Video Prediction,Jean-Yves Franceschi;Edouard Delasalles;Mickael Chen;Sylvain Lamprier;Patrick Gallinari,jean-yves.franceschi@lip6.fr;edouard.delasalles@lip6.fr;mickael.chen@lip6.fr;sylvain.lamprier@lip6.fr;patrick.gallinari@lip6.fr,6;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,1.0,yes,9/25/19,LIP6;LIP6;LIP6;LIP6;LIP6,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,
4991,4991,4991,4991,4991,4991,4991,4991,ICLR,2020,Large-scale Pretraining for Neural Machine Translation with Tens of Billions of Sentence Pairs,Yuxian Meng;Xiangyuan Ren;Zijun Sun;Xiaoya Li;Arianna Yuan;Fei Wu;Jiwei Li,yuxian_meng@shannonai.com;xiangyuan_re@shannonai.com;zijun_sun@shannonai.com;xiaoya_li@shannonai.com;xfyuan@stanford.edu;wufei@zju.edu.cn;jiwei_li@shannonai.com,3;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,1,4,0.0,yes,9/25/19,Shannon.AI;Shannon.AI;Shannon.AI;Shannon.AI;Stanford University;Zhejiang University;Shannon.AI,-1;-1;-1;-1;4;56;-1,-1;-1;-1;-1;4;107;-1,3
4992,4992,4992,4992,4992,4992,4992,4992,ICLR,2020,Random Matrix Theory Proves that Deep Learning Representations of GAN-data Behave as Gaussian Mixtures,Mohamed El Amine Seddik;Cosme Louart;Mohamed Tamaazousti;Romain Couillet,melaseddik@gmail.com;cosme.louart@gmail.com;mohamed.tamaazousti@cea.fr;romain.couillet@gmail.com,6;3;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,CEA;CEA;CEA;,233;233;233;-1,1027;1027;1027;-1,5;4
4993,4993,4993,4993,4993,4993,4993,4993,ICLR,2020,The advantage of using Student's t-priors in variational autoencoders,Najmeh Abiri;Mattias Ohlsson,najmeh@thep.lu.se;mattias@thep.lu.se,1;1;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,0,0.0,yes,9/25/19,Lund University;Lund University,390;390,98;98,5
4994,4994,4994,4994,4994,4994,4994,4994,ICLR,2020,Neural Communication Systems with Bandwidth-limited Channel,Karen Ullrich;Fabio Viola;Danilo J. Rezende,mail.karen.ullrich@gmail.com;fviola@google.com;danilor@google.com,3;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,University of Amsterdam;Google;Google,172;-1;-1,62;-1;-1,
4995,4995,4995,4995,4995,4995,4995,4995,ICLR,2020,Unified Probabilistic Deep Continual Learning through Generative Replay and Open Set Recognition,Martin Mundt;Sagnik Majumder;Iuliia Pliushch;Visvanathan Ramesh,mmundt@em.uni-frankfurt.de;majumder@ccc.cs.uni-frankfurt.de;pliushch@em.uni-frankfurt.de;ramesh@fias.uni-frankfurt.de,6;3;3,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Goethe University;Goethe University;Goethe University;Goethe University,233;233;233;233,305;305;305;305,5;11
4996,4996,4996,4996,4996,4996,4996,4996,ICLR,2020,Optimizing Loss Landscape Connectivity via Neuron Alignment,N. Joseph Tatro;Pin-Yu Chen;Payel Das;Igor Melnyk;Prasanna Sattigeri;Rongjie Lai,tatron@rpi.edu;pin-yu.chen@ibm.com;daspa@us.ibm.com;igor.melnyk@ibm.com;psattig@us.ibm.com;lair@rpi.edu,1;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,Rensselaer Polytechnic Institute;International Business Machines;International Business Machines;International Business Machines;International Business Machines;Rensselaer Polytechnic Institute,172;-1;-1;-1;-1;172,438;-1;-1;-1;-1;438,
4997,4997,4997,4997,4997,4997,4997,4997,ICLR,2020,Adaptive Online Planning for Continual Lifelong Learning,Kevin Lu;Igor Mordatch;Pieter Abbeel,kzl@berkeley.edu;imordatch@google.com;pabbeel@cs.berkeley.edu,1;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,5,0.0,yes,9/25/19,University of California Berkeley;Google;University of California Berkeley,5;-1;5,13;-1;13,
4998,4998,4998,4998,4998,4998,4998,4998,ICLR,2020,Adversarially Robust Generalization Just Requires More Unlabeled Data,Runtian Zhai;Tianle Cai;Di He;Chen Dan;Kun He;John E. Hopcroft;Liwei Wang,zhairuntian@pku.edu.cn;caitianle1998@pku.edu.cn;dihe@microsoft.com;cdan@cs.cmu.edu;brooklet60@hust.edu.cn;jeh17@cornell.edu;wanglw@cis.pku.edu.cn,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Peking University;Peking University;Microsoft;Carnegie Mellon University;Hong Kong University of Science and Technology;Cornell University;Peking University,22;22;-1;1;39;7;22,24;24;-1;27;47;19;24,4;1;8
4999,4999,4999,4999,4999,4999,4999,4999,ICLR,2020,Filling the Soap Bubbles: Efficient Black-Box Adversarial Certification with Non-Gaussian Smoothing,Dinghuai Zhang*;Mao Ye*;Chengyue Gong*;Zhanxing Zhu;Qiang Liu,zhangdinghuai@pku.edu.cn;lushleaf21@gmail.com;cygong@cs.utexas.edu;zhanxing.zhu@pku.edu.cn;lqiang@cs.utexas.edu,1;1;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,2,5,0.0,yes,9/25/19,"Peking University;University of Electronic Science and Technology of China;University of Texas, Austin;Peking University;University of Texas, Austin",22;481;22;22;22,24;628;38;24;38,4
5000,5000,5000,5000,5000,5000,5000,5000,ICLR,2020,Zeno++: Robust Fully Asynchronous SGD,Cong Xie;Oluwasanmi Koyejo;Indranil Gupta,cx2@illinois.edu;sanmi@illinois.edu;indy@illinois.edu,3;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,9,0.0,yes,9/25/19,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",3;3;3,48;48;48,1
5001,5001,5001,5001,5001,5001,5001,5001,ICLR,2020,LARGE SCALE REPRESENTATION LEARNING FROM TRIPLET COMPARISONS,Siavash Haghiri;Leena Chennuru Vankadara;Ulrike von Luxburg,siyavash.haghiri@gmail.com;leena.chennuru-vankadara@uni-tuebingen.de;luxburg@informatik.uni-tuebingen.de,3;1;3;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,9,0.0,yes,9/25/19,University of Tuebingen;University of Tuebingen;University of Tuebingen,154;154;154,91;91;91,
5002,5002,5002,5002,5002,5002,5002,5002,ICLR,2020,A Mean-Field Theory for Kernel Alignment with Random Features in Generative Adverserial Networks,Masoud Badiei Khuzani;Liyue Shen;Shahin Shahrampour;Lei Xing,mbadieik@stanford.edu;liyues@stanford.edu;shahin@tamu.edu;lei@stanford.edu,1;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Stanford University;Stanford University;Texas A&M;Stanford University,4;4;44;4,4;4;177;4,5;4;1
5003,5003,5003,5003,5003,5003,5003,5003,ICLR,2020,Homogeneous Linear Inequality Constraints for Neural Network Activations,Thomas Frerix;Matthias Nießner;Daniel Cremers,thomas.frerix@tum.de;niessner@tum.de;cremers@tum.de,1;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,4,0.0,yes,9/25/19,Technical University Munich;Technical University Munich;Technical University Munich,53;53;53,43;43;43,5
5004,5004,5004,5004,5004,5004,5004,5004,ICLR,2020,Distributed Online Optimization with Long-Term Constraints,Deming Yuan;Alexandre Proutiere;Guodong Shi,dmyuan1012@gmail.com;alepro@kth.se;guodong.shi@anu.edu.au,3;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,";KTH Royal Institute of Technology, Stockholm, Sweden;Australian National University",-1;128;108,-1;222;50,9;10
5005,5005,5005,5005,5005,5005,5005,5005,ICLR,2020,Semi-supervised Pose Estimation with Geometric Latent Representations,Luis A. Perez Rey;Dmitri Jarnikov;Mike Holenderski,l.a.perez.rey@tue.nl;d.s.jarnikov@tue.nl;m.holenderski@tue.nl,3;3;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Eindhoven University of Technology;Eindhoven University of Technology;Eindhoven University of Technology,205;205;205,185;185;185,5;2
5006,5006,5006,5006,5006,5006,5006,5006,ICLR,2020,Parallel Scheduled Sampling,Daniel Duckworth;Arvind Neelakantan;Ben Goodrich;Lukasz Kaiser;Samy Bengio,duckworthd@google.com;aneelakantan@google.com;bgoodrich@google.com;lukaszkaiser@google.com;bengio@google.com,6;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,
5007,5007,5007,5007,5007,5007,5007,5007,ICLR,2020,ADAPTING PRETRAINED LANGUAGE MODELS FOR LONG DOCUMENT CLASSIFICATION,Matthew Lyle Olson;Lisa Zhang;Chun-Nam Yu,olsomatt@oregonstate.edu;lisa.zhang@nokia-bell-labs.com;cnyu@cs.cornell.edu,3;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Reject,0,3,0.0,yes,9/25/19,Oregon State University;Nokia Bell Labs;Cornell University,77;-1;7,373;-1;19,3
5008,5008,5008,5008,5008,5008,5008,5008,ICLR,2020,Composable Semi-parametric Modelling for Long-range Motion Generation,Jingwei Xu;Huazhe Xu;Bingbing Ni;Xiaokang Yang;Trevor Darrell,xjwxjw@sjtu.edu.cn;huazhe_xu@eecs.berkeley.edu;nibingbing@sjtu.edu.cn;xkyang@sjtu.edu.cn;trevor@eecs.berkeley.edu,3;6;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Shanghai Jiao Tong University;University of California Berkeley;Shanghai Jiao Tong University;Shanghai Jiao Tong University;University of California Berkeley,53;5;53;53;5,157;13;157;157;13,
5009,5009,5009,5009,5009,5009,5009,5009,ICLR,2020,Finding Mixed Strategy Nash Equilibrium for Continuous Games through Deep Learning,Zehao Dou;Xiang Yan;Dongge Wang;Xiaotie Deng,zehaodou@pku.edu.cn;yxghost@sjtu.edu.cn;dgwang96@pku.edu.cn;xiaotie@pku.edu.cn,6;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,9,0.0,yes,9/25/19,Peking University;Shanghai Jiao Tong University;Peking University;Peking University,22;53;22;22,24;157;24;24,5;4
5010,5010,5010,5010,5010,5010,5010,5010,ICLR,2020,Omnibus Dropout for Improving The Probabilistic Classification Outputs of ConvNets,Zhilu Zhang;Adrian V. Dalca;Mert R. Sabuncu,zz452@cornell.edu;adalca@mit.edu;msabuncu@cornell.edu,3;1;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,3,0.0,yes,9/25/19,Cornell University;Massachusetts Institute of Technology;Cornell University,7;2;7,19;5;19,11
5011,5011,5011,5011,5011,5011,5011,5011,ICLR,2020,Online Learned Continual Compression with Stacked Quantization Modules,Lucas Caccia;Eugene Belilovsky;Massimo Caccia;Joelle Pineau,lucas.page-caccia@mail.mcgill.ca;belilovsky.eugene@gmail.com;massimo.p.caccia@gmail.com;jpineau@cs.mcgill.ca,3;3;6;3;6,I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,1,4,0.0,yes,9/25/19,McGill University;University of Montreal;University of Montreal;McGill University,86;128;128;86,42;85;85;42,
5012,5012,5012,5012,5012,5012,5012,5012,ICLR,2020,Yet another but more efficient black-box adversarial attack: tiling and evolution strategies,Laurent Meunier;Jamal Atif;Olivier Teytaud,laurent.meunier1995@gmail.com;jamal.atif@dauphine.fr;oteytaud@fb.com,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Univeristé Paris-Dauphine;Univeristé Paris-Dauphine;Facebook,481;481;-1,1397;1397;-1,4
5013,5013,5013,5013,5013,5013,5013,5013,ICLR,2020,AlignNet: Self-supervised Alignment Module,Antonia Creswell;Luis Piloto;David Barrett;Kyriacos Nikiforou;David Raposo;Marta Garnelo;Peter Battaglia;Murray Shanahan,tonicreswell@google.com;piloto@google.com;peterbattaglia@google.com;knikiforou@google.com;barrettdavid@google.com;garnelo@google.com;mshanahan@google.com;draposo@google.com,6;3;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1,10
5014,5014,5014,5014,5014,5014,5014,5014,ICLR,2020,A Simple Recurrent Unit with Reduced Tensor Product Representations,Shuai Tang;Paul Smolensky;Virginia R. de Sa,shuaitang93@ucsd.edu;paul.smolensky@gmail.com;desa@ucsd.edu,6;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,1,0.0,yes,9/25/19,"University of California, San Diego;Microsoft;University of California, San Diego",11;-1;11,31;-1;31,3;1
5015,5015,5015,5015,5015,5015,5015,5015,ICLR,2020,A Hierarchy of Graph Neural Networks Based on Learnable Local Features,Michael Lingzhi Li;Meng Dong;Jiawei Zhou;Alexander M. Rush,mlli@mit.edu;mengdong@g.harvard.edu;jzhou02@g.harvard.edu;srush@cornell.edu,3;8;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,Massachusetts Institute of Technology;Harvard University;Harvard University;Cornell University,2;39;39;7,5;7;7;19,10
5016,5016,5016,5016,5016,5016,5016,5016,ICLR,2020,Discriminability Distillation in Group Representation Learning,Manyuan Zhang，Guanglu Song，Yu Liu，Hang Zhou,zhangmanyuan@sensetime.com;songguanglu@sensetime.com;yuliu@ee.cuhk.edu.hk;zhouhang@link.cuhk.edu.hk,3;1;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,SenseTime Group Limited;SenseTime Group Limited;The Chinese University of Hong Kong;The Chinese University of Hong Kong,-1;-1;59;59,-1;-1;35;35,2
5017,5017,5017,5017,5017,5017,5017,5017,ICLR,2020,Blurring Structure and Learning to Optimize and Adapt Receptive Fields,Evan Shelhamer;Dequan Wang;Trevor Darrell,shelhamer@cs.berkeley.edu;dqwang@eecs.berkeley.edu;trevor@eecs.berkeley.edu,3;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,13;13;13,2
5018,5018,5018,5018,5018,5018,5018,5018,ICLR,2020,Conditional Flow Variational Autoencoders for Structured Sequence Prediction,Apratim Bhattacharyya;Michael Hanselmann;Mario Fritz;Bernt Schiele;Christoph-Nikolas Straehle,abhattac@mpi-inf.mpg.de;michael.hanselmann@de.bosch.com;fritz@cispa.saarland;schiele@mpi-inf.mpg.de;christoph-nikolas.straehle@de.bosch.com,6;6;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,"Saarland Informatics Campus, Max-Planck Institute;Bosch;CISPA Helmholtz Center for Information Security;Saarland Informatics Campus, Max-Planck Institute;Bosch",-1;-1;143;-1;-1,-1;-1;1397;-1;-1,5
5019,5019,5019,5019,5019,5019,5019,5019,ICLR,2020,LAVAE: Disentangling Location and Appearance,Andrea Dittadi;Ole Winther,adit@dtu.dk;olwi@dtu.dk,1;1;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,2,0.0,yes,9/25/19,Technical University of Denmark;Technical University of Denmark,481;481,182;182,5
5020,5020,5020,5020,5020,5020,5020,5020,ICLR,2020,BETANAS: Balanced Training and selective drop for Neural Architecture Search,Muyuan Fang;Qiang Wang;Jian Zhang;Zhao Zhong,fangmuyuan@huawei.com;wangqiang168@huawei.com;zhangjian157@huawei.com;zorro.zhongzhao@huawei.com,3;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.,-1;-1;-1;-1,-1;-1;-1;-1,7
5021,5021,5021,5021,5021,5021,5021,5021,ICLR,2020,Unifying Graph Convolutional Neural Networks and Label Propagation,Hongwei Wang;Jure Leskovec,wanghongwei55@gmail.com;jure@cs.stanford.edu,3;6;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Stanford University;Stanford University,4;4,4;4,10
5022,5022,5022,5022,5022,5022,5022,5022,ICLR,2020,Defective Convolutional Layers Learn Robust CNNs,Tiange Luo;Tianle Cai;Xiaomeng Zhang;Siyu Chen;Di He;Liwei Wang,luotg@pku.edu.cn;caitianle1998@pku.edu.cn;zhan147@usc.edu;siyuchen@pku.edu.cn;dihe@microsoft.com;wanglw@cis.pku.edu.cn,3;1;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Peking University;Peking University;University of Southern California;Peking University;Microsoft;Peking University,22;22;31;22;-1;22,24;24;62;24;-1;24,4
5023,5023,5023,5023,5023,5023,5023,5023,ICLR,2020,Unsupervised Progressive Learning and the STAM Architecture,James Smith;Constantine Dovrolis,jamessealesmith@gatech.edu;constantine@gatech.edu,8;3;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,2.0,yes,9/25/19,Georgia Institute of Technology;Georgia Institute of Technology,13;13,38;38,
5024,5024,5024,5024,5024,5024,5024,5024,ICLR,2020,Asymptotic learning curves of kernel methods: empirical data v.s. Teacher-Student paradigm,Stefano Spigler;Mario Geiger;Matthieu Wyart,stefano.spigler@epfl.ch;mario.geiger@epfl.ch;matthieu.wyart@epfl.ch,3;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne,481;481;481,38;38;38,8
5025,5025,5025,5025,5025,5025,5025,5025,ICLR,2020,Sticking to the Facts: Confident Decoding for Faithful Data-to-Text Generation,Ran Tian;Shashi Narayan;Thibault Sellam;Ankur P. Parikh,tianran@google.com;shashinarayan@google.com;tsellam@google.com;aparikh@google.com,6;3;8;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,9,0.0,yes,9/25/19,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,
5026,5026,5026,5026,5026,5026,5026,5026,ICLR,2020,Evidence-Aware Entropy Decomposition For  Active Deep Learning,Weishi Shi;Xujiang Zhao;Feng Chen;Qi Yu,ws7586@rit.edu;xujiang.zhao@utdallas.edu;feng.chen@utdallas.edu;qi.yu@rit.edu,6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,"Rochester Institute of Technology;University of Texas, Dallas;University of Texas, Dallas;Rochester Institute of Technology",128;86;86;128,843;319;319;843,
5027,5027,5027,5027,5027,5027,5027,5027,ICLR,2020,Meta-Learning by Hallucinating Useful Examples,Yu-Xiong Wang;Yuki Uchiyama;Martial Hebert;Karteek Alahari,yuxiongw@cs.cmu.edu;braverthan2@gmail.com;hebert@ri.cmu.edu;karteek.alahari@inria.fr,3;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,5,0.0,yes,9/25/19,Carnegie Mellon University;;Carnegie Mellon University;INRIA,1;-1;1;-1,27;-1;27;-1,6
5028,5028,5028,5028,5028,5028,5028,5028,ICLR,2020,PDP: A General Neural Framework for Learning SAT Solvers,Saeed Amizadeh;Sergiy Matusevych;Markus Weimer,saamizad@microsoft.com;sergiym@microsoft.com;markus.weimer@microsoft.com,1;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,3,0.0,yes,9/25/19,Microsoft;Microsoft;Microsoft,-1;-1;-1,-1;-1;-1,10
5029,5029,5029,5029,5029,5029,5029,5029,ICLR,2020,Consistency-Based Semi-Supervised Active Learning: Towards Minimizing Labeling Budget,Mingfei Gao;Zizhao Zhang;Guo Yu;Sercan O. Arik;Larry S. Davis;Tomas Pfister,mgao@cs.umd.edu;zizhaoz@google.com;gy63@uw.edu;soarik@google.com;lsd@umiacs.umd.edu;tpfister@google.com,6;6,I have published in this field for several years.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,"University of Maryland, College Park;Google;University of Washington, Seattle;Google;University of Maryland, College Park;Google",12;-1;6;-1;12;-1,91;-1;26;-1;91;-1,
5030,5030,5030,5030,5030,5030,5030,5030,ICLR,2020,TabNet: Attentive Interpretable Tabular Learning,Sercan O. Arik;Tomas Pfister,soarik@google.com;tpfister@google.com,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Reject,1,8,0.0,yes,9/25/19,Google;Google,-1;-1,-1;-1,
5031,5031,5031,5031,5031,5031,5031,5031,ICLR,2020,Adapting Behaviour for Learning Progress,Tom Schaul;Diana Borsa;David Ding;David Szepesvari;Georg Ostrovski;Will Dabney;Simon Osindero,schaul@google.com;borsa@google.com;fding@google.com;dsz@google.com;ostrovski@google.com;wdabney@google.com;osindero@google.com,6;3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,6,0.0,yes,9/25/19,Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,
5032,5032,5032,5032,5032,5032,5032,5032,ICLR,2020,Improving Differentially Private Models with Active Learning,Zhengli Zhao;Nicolas Papernot;Sameer Singh;Neoklis Polyzotis;Augustus Odena,zhengliz@uci.edu;papernot@google.com;sameer@uci.edu;npolyzotis@google.com;augustusodena@google.com,3;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,1,5,0.0,yes,9/25/19,"University of California, Irvine;Google;University of California, Irvine;Google;Google",35;-1;35;-1;-1,96;-1;96;-1;-1,
5033,5033,5033,5033,5033,5033,5033,5033,ICLR,2020,Style-based Encoder Pre-training for Multi-modal Image Synthesis,Moustafa Meshry;Yixuan Ren;Ricardo Martin-Brualla;Larry Davis;Abhinav Shrivastava,mmeshry@cs.umd.edu;yxren@cs.umd.edu;rmbrualla@google.com;lsd@umiacs.umd.edu;abhinav@cs.umd.edu,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,"University of Maryland, College Park;University of Maryland, College Park;Google;University of Maryland, College Park;University of Maryland, College Park",12;12;-1;12;12,91;91;-1;91;91,5
5034,5034,5034,5034,5034,5034,5034,5034,ICLR,2020,GRAPH ANALYSIS AND GRAPH POOLING IN THE SPATIAL DOMAIN,Mostafa Rahmani;Ping Li,mostafarahmani@baidu.com;liping11@baidu.com,6;6;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,9,1.0,yes,9/25/19,Baidu;Baidu,-1;-1,-1;-1,10
5035,5035,5035,5035,5035,5035,5035,5035,ICLR,2020,Cover Filtration and Stable Paths in the Mapper,Dustin L. Arendt;Matthew Broussard;Bala Krishnamoorthy;Nathaniel Saul,dustin.arendt@pnnl.gov;matthew.broussard@wsu.edu;kbala@wsu.edu;nat@riverasaul.com,1;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,Pacific Northwest National Laboratory;SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;Riverasaul,-1;481;481;-1,-1;299;299;-1,8
5036,5036,5036,5036,5036,5036,5036,5036,ICLR,2020,Rigging the Lottery: Making All Tickets Winners,Utku Evci;Erich Elsen;Pablo Castro;Trevor Gale,ue225@nyu.edu;eriche@google.com;tgale@google.com;psc@google.com,3;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,13,4.0,yes,9/25/19,New York University;Google;Google;Google,25;-1;-1;-1,29;-1;-1;-1,
5037,5037,5037,5037,5037,5037,5037,5037,ICLR,2020,Disentangled GANs for Controllable Generation of High-Resolution Images,Weili Nie;Tero Karras;Animesh Garg;Shoubhik Debhath;Anjul Patney;Ankit B. Patel;Anima Anandkumar,wn8@rice.edu;tkarras@nvidia.com;garg@cs.toronto.edu;shoubhikdn@gmail.com;anjul.patney@gmail.com;abp4@rice.edu;animakumar@gmail.com,3;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,5,0.0,yes,9/25/19,"Rice University;NVIDIA;Department of Computer Science, University of Toronto;NVIDIA;Facebook;Rice University;University of California-Irvine",84;-1;18;-1;-1;84;35,105;-1;18;-1;-1;105;96,5;4;8
5038,5038,5038,5038,5038,5038,5038,5038,ICLR,2020,UNIVERSAL MODAL EMBEDDING OF DYNAMICS IN VIDEOS AND ITS APPLICATIONS,Israr Ul Haq;Yoshinobu Kawahara,israr.haq@riken.jp;kawahara@imi.kyushu-u.ac.jp,3;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,RIKEN;Meiji University,-1;481,-1;332,2
5039,5039,5039,5039,5039,5039,5039,5039,ICLR,2020,First-Order Preconditioning via Hypergradient Descent,Ted Moskovitz;Rui Wang;Janice Lan;Sanyam Kapoor;Thomas Miconi;Jason Yosinski;Aditya Rawal,thmoskovitz@gmail.com;ruiwang@uber.com;janlan@uber.com;sanyam@uber.com;tmiconi@uber.com;yosinski@uber.com;aditya.rawal@uber.com,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,University College London;Uber;Uber;Uber;Uber;Uber;Uber,50;-1;-1;-1;-1;-1;-1,15;-1;-1;-1;-1;-1;-1,
5040,5040,5040,5040,5040,5040,5040,5040,ICLR,2020,Zero-shot task adaptation by homoiconic meta-mapping,Andrew K. Lampinen;James L. McClelland,lampinen@stanford.edu;jlmcc@stanford.edu,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A,Reject,0,8,0.0,yes,9/25/19,Stanford University;Stanford University,4;4,4;4,6
5041,5041,5041,5041,5041,5041,5041,5041,ICLR,2020,MLModelScope: A Distributed Platform for ML Model Evaluation and Benchmarking at Scale,Cheng Li;Abdul Dakkak;Jinjun Xiong;Wen-mei Hwu,cli99@illinois.edu;dakkak@illinois.edu;jinjun@us.ibm.com;w-hwu@illinois.edu,1;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,13,0.0,yes,9/25/19,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;International Business Machines;University of Illinois, Urbana Champaign",3;3;-1;3,48;48;-1;48,
5042,5042,5042,5042,5042,5042,5042,5042,ICLR,2020,Efficient Multivariate Bandit Algorithm with Path Planning,Keyu Nie;Zezhong Zhang;Ted Tao Yuan;Rong Song;Pauline Berry Burke,keyunie@google.com;zezzhang@ebay.com;teyuan@ebay.com;rsong@ebay.com;pmburke10@gmail.com,3;1;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,2,0.0,yes,9/25/19,Google;eBay;eBay;eBay;,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,10
5043,5043,5043,5043,5043,5043,5043,5043,ICLR,2020,Statistical Adaptive Stochastic Optimization,Pengchuan Zhang;Hunter Lang;Qiang Liu;Lin Xiao,penzhan@microsoft.com;hjl@mit.edu;lqiang@cs.utexas.edu;lin.xiao@microsoft.com,6;8;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,"Microsoft;Massachusetts Institute of Technology;University of Texas, Austin;Microsoft",-1;2;22;-1,-1;5;38;-1,
5044,5044,5044,5044,5044,5044,5044,5044,ICLR,2020,The Benefits of Over-parameterization at Initialization in Deep ReLU Networks,Devansh Arpit;Yoshua Bengio,devansharpit@gmail.com;yoshua.bengio@mila.quebec,1;3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,SalesForce.com;University of Montreal,-1;128,-1;85,1
5045,5045,5045,5045,5045,5045,5045,5045,ICLR,2020,Multichannel Generative Language Models,Harris Chan;Jamie Kiros;William Chan,hchan@cs.toronto.edu;kiros@google.com;williamchan@google.com,3;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,"Department of Computer Science, University of Toronto;Google;Google",18;-1;-1,18;-1;-1,3;5
5046,5046,5046,5046,5046,5046,5046,5046,ICLR,2020,Set Functions for Time Series,Max Horn;Michael Moor;Christian Bock;Bastian Rieck;Karsten Borgwardt,max.horn@bsse.ethz.ch;michael.moor@bsse.ethz.ch;christian.bock@bsse.ethz.ch;bastian.rieck@bsse.ethz.ch;karsten.borgwardt@bsse.ethz.ch,6;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10;10;10,13;13;13;13;13,
5047,5047,5047,5047,5047,5047,5047,5047,ICLR,2020,HIPPOCAMPAL NEURONAL REPRESENTATIONS IN CONTINUAL LEARNING,Samia Mohinta;Rui Ponte Costa;Stephane Ciocchi,dc18393@bristol.ac.uk;rui.costa@bristol.ac.uk,1;1;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,University of Bristol;University of Bristol,128;128,87;87,
5048,5048,5048,5048,5048,5048,5048,5048,ICLR,2020,Augmenting Transformers with KNN-Based Composite Memory,Angela Fan;Claire Gardent;Chloe Braud;Antoine Bordes,angelafan@fb.com;claire.gardent@loria.fr;chloe.braud@loria.fr;abordes@fb.com,6;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Facebook;University of Lorraine;University of Lorraine;Facebook,-1;481;481;-1,-1;624;624;-1,5
5049,5049,5049,5049,5049,5049,5049,5049,ICLR,2020,Winning Privately: The Differentially Private Lottery Ticket Mechanism,Lovedeep Gondara;Ke Wang;Ricardo Silva Carvalho,lgondara@sfu.ca;wang@sfu.ca;ricardo_silva_carvalho@sfu.ca,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,3,0.0,yes,9/25/19,Simon Fraser University;Simon Fraser University;Simon Fraser University,64;64;64,272;272;272,
5050,5050,5050,5050,5050,5050,5050,5050,ICLR,2020,Localizing and Amortizing: Efficient Inference for Gaussian Processes,Linfeng Liu;Liping Liu,linfeng.liu@tufts.edu;liping.liu@tufts.edu,1;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,1,5,0.0,yes,9/25/19,Tufts University;Tufts University,172;172,139;139,
5051,5051,5051,5051,5051,5051,5051,5051,ICLR,2020,BAIL: Best-Action Imitation Learning for Batch Deep Reinforcement Learning,Xinyue Chen;Zijian Zhou;Zheng Wang;Che Wang;Yanqiu Wu;Qing Deng;Keith Ross,xc1305@nyu.edu;zz1435@nyu.edu;zw1454@nyu.edu;cw1681@nyu.edu;yanqiu.wu@nyu.edu;qd319@nyu.edu;keithwross@nyu.edu,3;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,New York University;New York University;New York University;New York University;New York University;New York University;New York University,25;25;25;25;25;25;25,29;29;29;29;29;29;29,
5052,5052,5052,5052,5052,5052,5052,5052,ICLR,2020,Entropy Penalty: Towards Generalization Beyond the IID Assumption,Devansh Arpit;Caiming Xiong;Richard Socher,devansharpit@gmail.com;cxiong@salesforce.com;rsocher@salesforce.com,1;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,SalesForce.com;SalesForce.com;SalesForce.com,-1;-1;-1,-1;-1;-1,8
5053,5053,5053,5053,5053,5053,5053,5053,ICLR,2020,A Causal View on Robustness  of Neural Networks,Cheng Zhang;Yingzhen Li,cheng.zhang@microsoft.com;yingzhen.li@microsoft.com,3;8;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,2,0.0,yes,9/25/19,Microsoft;Microsoft,-1;-1,-1;-1,
5054,5054,5054,5054,5054,5054,5054,5054,ICLR,2020,Improved Image Augmentation for Convolutional Neural Networks by Copyout and CopyPairing,Philip May,eniak.info@gmail.com,1;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,0,0.0,yes,9/25/19,T-Systems on site services GmbH,-1,-1,
5055,5055,5055,5055,5055,5055,5055,5055,ICLR,2020,Model-free Learning Control of Nonlinear Stochastic Systems with Stability Guarantee,Minghao Han;Yuan Tian;Lixian Zhang;Jun Wang;Wei Pan,mhhan@hit.edu.cn;yuantian013@163.com;lixianzhang@hit.edu.cn;jun.wang@cs.ucl.ac.uk;wei.pan@tudelft.nl,1;6;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,Harbin Institute of Technology;Swiss Federal Institute of Technology;Harbin Institute of Technology;University College London;Delft University of Technology,172;10;172;50;89,424;13;424;15;67,
5056,5056,5056,5056,5056,5056,5056,5056,ICLR,2020,A Theoretical Analysis of  Deep Q-Learning,Zhuoran Yang;Yuchen Xie;Zhaoran Wang,zy6@princeton.edu;yuchenxie2020@u.northwestern.edu;zhaoranwang@gmail.com,8;3;3,I have read many papers in this area.:N/A:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,Princeton University;Northwestern University;Northwestern University,31;44;44,6;22;22,
5057,5057,5057,5057,5057,5057,5057,5057,ICLR,2020,On the Pareto Efficiency of Quantized CNN,Ting-Wu Chin;Pierce I-Jen Chuang;Vikas Chandra;Diana Marculescu,tingwuc@cmu.edu;pichuang@fb.com;vchandra@fb.com;dianam@cmu.edu,3;6;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Carnegie Mellon University;Facebook;Facebook;Carnegie Mellon University,1;-1;-1;1,27;-1;-1;27,2
5058,5058,5058,5058,5058,5058,5058,5058,ICLR,2020,Robust Reinforcement Learning via Adversarial Training with  Langevin Dynamics,Huang Yu-Ting;Parameswaran Kamalaruban;Paul Rolland;Ya-Ping Hsieh;Volkan Cevher,yu.huang@epfl.ch;kamalaruban.parameswaran@epfl.ch;paul.rolland@epfl.ch;ya-ping.hsieh@epfl.ch;volkan.cevher@epfl.ch,3;3;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne,481;481;481;481;481,38;38;38;38;38,8
5059,5059,5059,5059,5059,5059,5059,5059,ICLR,2020,Robust Reinforcement Learning with Wasserstein Constraint,Linfang Hou;Liang Pang;Xin Hong;Yanyan Lan;Zhiming Ma;Dawei Yin,houlinfang09@gmail.com;pangliang@ict.ac.cn;hongxin19b@ict.ac.cn;lanyanyan@ict.ac.cn;mazm@amt.ac.cn;yindawei@acm.org,3;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,"JD AI Research;Institute of Computing Technology, Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences;Chinese Academy of Sciences;",-1;59;59;59;59;-1,-1;1397;1397;1397;1397;-1,
5060,5060,5060,5060,5060,5060,5060,5060,ICLR,2020,Representation Learning for Remote Sensing: An Unsupervised Sensor Fusion Approach,Aidan M. Swope;Xander H. Rudelis;Kyle T. Story,aidanswope@gmail.com;xander@descarteslabs.com;kyle@descarteslabs.com,3;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,"California Institute of Technology;Descartes Labs, Inc;Descartes Labs, Inc",143;-1;-1,2;-1;-1,
5061,5061,5061,5061,5061,5061,5061,5061,ICLR,2020,Learning to Combat Compounding-Error in Model-Based Reinforcement Learning,Chenjun Xiao;Yifan Wu;Chen Ma;Dale Schuurmans;Martin Müller,chenjun@ualberta.ca;yw4@andrew.cmu.edu;chenchloem@gmail.com;daes@ualberta.ca;mmueller@ualberta.ca,1;6;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,University of Alberta;Carnegie Mellon University;;University of Alberta;University of Alberta,100;1;-1;100;100,136;27;-1;136;136,
5062,5062,5062,5062,5062,5062,5062,5062,ICLR,2020,Recurrent Neural Networks are Universal Filters,Wenjie Xu;Xiuqiong Chen;Stephen S.-T. Yau,1155118056@link.cuhk.edu.hk;cxq0828@tsinghua.edu.cn;yau@uic.edu,3;3;6,I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,4,0.0,yes,9/25/19,"The Chinese University of Hong Kong;Tsinghua University;University of Illinois, Chicago",59;8;56,35;23;254,11;1
5063,5063,5063,5063,5063,5063,5063,5063,ICLR,2020,Soft Token Matching for Interpretable Low-Resource Classification,Federico Errica;Fabrizio Silvestri;Bora Edizel;Sebastian Riedel;Ludovic Denoyer;Vassilis Plachouras,federico.errica@phd.unipi.it;fabrizio.silvestri@gmail.com;b.edizel@gmail.com;sebastian.riedel@gmail.com;denoyer@fb.com;vplachouras@fb.com,3;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,University of Pisa;;Facebook;University College London;Facebook;Facebook,233;-1;-1;50;-1;-1,366;-1;-1;15;-1;-1,
5064,5064,5064,5064,5064,5064,5064,5064,ICLR,2020,A Mention-Pair Model of Annotation with Nonparametric User Communities,Silviu Paun;Juntao Yu;Jon Chamberlain;Udo Kruschwitz;Massimo Poesio,s.paun@qmul.ac.uk;juntao.yu@qmul.ac.uk;jchamb@essex.ac.uk;udo@essex.ac.uk;m.poesio@qmul.ac.uk,3;8;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Queen Mary University London;Queen Mary University London;University of Sussex;University of Sussex;Queen Mary University London,233;233;233;233;233,110;110;146;146;110,3
5065,5065,5065,5065,5065,5065,5065,5065,ICLR,2020,Towards Finding Longer Proofs,Zsolt Zombori;Adrián Csiszárik;Henryk Michalewski;Cezary Kaliszyk;Josef Urban,zombori@renyi.hu;csadrian@renyi.hu;henrykmichalewski@gmail.com;cezary.kaliszyk@uibk.ac.at;josef.urban@gmail.com,6;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,4,0.0,yes,9/25/19,Alfréd Rényi Institute of Mathematics;Alfréd Rényi Institute of Mathematics;;University of Innsbruck;Czech Technical University in Prague,-1;-1;-1;481;323,-1;-1;-1;415;956,1
5066,5066,5066,5066,5066,5066,5066,5066,ICLR,2020,iWGAN: an Autoencoder WGAN for Inference,Yao Chen;Qingyi Gao;Xiao Wang,chen2037@purdue.edu;gao424@purdue.edu;wangxiao@purdue.edu,8;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Purdue University;Purdue University;Purdue University,27;27;27,88;88;88,5;4;1;8
5067,5067,5067,5067,5067,5067,5067,5067,ICLR,2020,Teacher-Student Compression with Generative Adversarial Networks,Ruishan Liu;Nicolo Fusi;Lester Mackey,ruishan@stanford.edu;lmackey@stanford.edu;fusi@microsoft.com,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Stanford University;Stanford University;Microsoft,4;4;-1,4;4;-1,5;4
5068,5068,5068,5068,5068,5068,5068,5068,ICLR,2020,Characterize and Transfer Attention in Graph Neural Networks,Mufei Li;Hao Zhang;Xingjian Shi;Minjie Wang;Yixing Guan;Zheng Zhang,limufe@amazon.com;sufeidechabei@gmail.com;xshiab@connect.ust.hk;wmjlyjemaine@gmail.com;guayixin@amazon.com;zz@nyu.edu,3;6;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,7,0.0,yes,9/25/19,Amazon;;The Hong Kong University of Science and Technology;;Amazon;New York University,-1;-1;39;-1;-1;25,-1;-1;47;-1;-1;29,6;10
5069,5069,5069,5069,5069,5069,5069,5069,ICLR,2020,Benchmarking Model-Based Reinforcement Learning,Tingwu Wang;Xuchan Bao;Ignasi Clavera;Jerrick Hoang;Yeming Wen;Eric Langlois;Shunshi Zhang;Guodong Zhang;Pieter Abbeel;Jimmy Ba,tingwuwang@cs.toronto.edu;xuchan.bao@mail.utoronto.ca;iclavera@berkeley.edu;jhoang@cs.toronto.edu;ywen@cs.toronto.edu;edl@cs.toronto.edu;matthew.zhang@mail.utoronto.ca;gdzhang@cs.toronto.edu;pabbeel@cs.berkeley.edu;jba@cs.toronto.edu,1;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:N/A:N/A,Reject,0,5,0.0,yes,9/25/19,"Department of Computer Science, University of Toronto;Toronto University;University of California Berkeley;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Toronto University;Department of Computer Science, University of Toronto;University of California Berkeley;Department of Computer Science, University of Toronto",18;18;5;18;18;18;18;18;5;18,18;18;13;18;18;18;18;18;13;18,
5070,5070,5070,5070,5070,5070,5070,5070,ICLR,2020,Gradient Perturbation is Underrated for Differentially Private Convex Optimization,Da Yu;Huishuai Zhang;Wei Chen;Tie-yan Liu;Jian Yin,yuda3@mail2.sysu.edu.cn;huishuai.zhang@microsoft.com;wche@microsoft.com;tie-yan.liu@microsoft.com;issjyin@mail.sysu.edu.cn,6;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,SUN YAT-SEN UNIVERSITY;Microsoft;Microsoft;Microsoft;SUN YAT-SEN UNIVERSITY,481;-1;-1;-1;481,299;-1;-1;-1;299,9
5071,5071,5071,5071,5071,5071,5071,5071,ICLR,2020,BasisVAE: Orthogonal Latent Space for Deep Disentangled Representation,Jin-Young  Kim;Sung-Bae Cho,seago0828@yonsei.ac.kr;sbcho@yonsei.ac.kr,1;1;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,1,20,0.0,yes,9/25/19,Yonsei University;Yonsei University,481;481,196;196,5
5072,5072,5072,5072,5072,5072,5072,5072,ICLR,2020,How many weights are enough : can tensor factorization learn efficient policies ?,Pierre H. Richemond;Arinbjorn Kolbeinsson;Yike Guo,phr17@ic.ac.uk;ak711@imperial.ac.uk;y.guo@imperial.ac.uk,3;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Imperial College London;Imperial College London;Imperial College London,73;73;73,10;10;10,
5073,5073,5073,5073,5073,5073,5073,5073,ICLR,2020,Superseding Model Scaling by Penalizing Dead Units and Points with Separation Constraints,Carles Riera;Camilo Rey-Torres;Eloi Puertas;Oriol Pujol,blauigris@gmail.com;camilorey@gmail.com;epuertas@ub.edu;oriol_pujol@ub.edu,3;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,13,1.0,yes,9/25/19,Universitat de Barcelona;Universidad Sergio Arboleda;Universitat de Barcelona;Universitat de Barcelona,481;481;481;481,213;1397;213;213,
5074,5074,5074,5074,5074,5074,5074,5074,ICLR,2020,AHash: A Load-Balanced One Permutation Hash,Chenxingyu Zhao;Jie Gui;Yixiao Guo;Jie Jiang;Tong Yang;Bin Cui;Gong Zhang,dkzcxy@pku.edu.cn;guisj2017@pku.edu.cn;1700016637@pku.edu.cn;jie.jiang@pku.edu.cn;yangtongemail@gmail.com;bin.cui@pku.edu.cn;nicholas.zhang@huawei.com,1;6;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Peking University;Peking University;Peking University;Peking University;;Peking University;Huawei Technologies Ltd.,22;22;22;22;-1;22;-1,24;24;24;24;-1;24;-1,
5075,5075,5075,5075,5075,5075,5075,5075,ICLR,2020,Representation Quality Explain Adversarial Attacks,Danilo Vasconcellos Vargas;Shashank Kotyan;Moe Matsuki,vargas@inf.kyushu-u.ac.jp;shashankkotyan@gmail.com;matsuki.sousisu@gmail.com,1;3;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Meiji University;Meiji University;,481;481;-1,332;332;-1,4;6
5076,5076,5076,5076,5076,5076,5076,5076,ICLR,2020,Explaining A Black-box By Using A Deep Variational Information Bottleneck Approach,Seojin Bang;Pengtao Xie;Heewook Lee;Wei Wu;Eric Xing,seojinb@cs.cmu.edu,6;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,12,0.0,yes,9/25/19,Carnegie Mellon University,1,27,
5077,5077,5077,5077,5077,5077,5077,5077,ICLR,2020,Neural networks are a priori biased towards Boolean functions with low entropy,Chris Mingard;Joar Skalse;Guillermo Valle-Pérez;David Martínez-Rubio;Vladimir Mikulik;Ard A. Louis,christopher.mingard@hertford.ox.ac.uk;joar.skalse@hertford.ox.ac.uk;guillermo.valle@dtc.ox.ac.uk;david.martinez@cs.ox.ac.uk;vladimir.mikulik@hertford.ox.ac.uk;ard.louis@physics.ox.ac.uk,3;6;6,I do not know much about this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,4,0.0,yes,9/25/19,University of Oxford;University of Oxford;University of Oxford;University of Oxford;University of Oxford;University of Oxford,50;50;50;50;50;50,1;1;1;1;1;1,1
5078,5078,5078,5078,5078,5078,5078,5078,ICLR,2020,Learning Structured Communication for Multi-agent Reinforcement Learning,Junjie Sheng;Xiangfeng Wang;Bo Jin;Junchi Yan;Wenhao Li;Tsung-Hui Chang;Jun Wang;Hongyuan Zha,52194501003@stu.ecnu.edu.cn;xfwang@sei.ecnu.edu.cn;bjin@cs.ecnu.edu.cn;yanjunchi@sjtu.edu.cn;52194501026@stu.ecnu.edu.cn;changtsunghui@cuhk.edu.cn;jwang@sei.ecnu.edu.cn;zha@sei.ecnu.edu.cn,6;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Australian National University;Australian National University;Australian National University;Shanghai Jiao Tong University;Australian National University;Tsinghua University;Australian National University;Australian National University,108;108;108;53;108;8;108;108,50;50;50;157;50;23;50;50,10
5079,5079,5079,5079,5079,5079,5079,5079,ICLR,2020,Lipschitz Lifelong Reinforcement Learning,Erwan Lecarpentier;David Abel;Kavosh Asadi;Yuu Jinnai;Emmanuel Rachelson;Michael L. Littman,erwan.lecarpentier@isae-supaero.fr;david_abel@brown.edu;k8@brown.edu;yuu_jinnai@brown.edu;emmanuel.rachelson@isae-supaero.fr;michael_littman@brown.edu,6;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Institut Supérieur de l'Aéronautique et de l'Espace;Brown University;Brown University;Brown University;Institut Supérieur de l'Aéronautique et de l'Espace;Brown University,481;67;67;67;481;67,1397;53;53;53;1397;53,
5080,5080,5080,5080,5080,5080,5080,5080,ICLR,2020,Graph Neural Networks for Reasoning 2-Quantified Boolean Formulas,Fei Wang;Zhanfu Yang;Ziliang Chen;Guannan Wei;Tiark Rompf,wang603@purdue.edu;yang1676@purdue.edu;c.ziliang@yahoo.com;wei220@purdue.edu;tiark@purdue.edu,6;8;3,I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Purdue University;Purdue University;SUN YAT-SEN UNIVERSITY;Purdue University;Purdue University,27;27;481;27;27,88;88;299;88;88,10
5081,5081,5081,5081,5081,5081,5081,5081,ICLR,2020,Semi-Supervised Few-Shot Learning with a Controlled Degree of Task-Adaptive Conditioning,Sung Whan Yoon;Jun Seo;Jaekyun Moon,shyoon8@kaist.ac.kr;tjwns0630@kaist.ac.kr;jmoon@kaist.edu,3;6;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;KAIST,481;481;20,110;110;110,6
5082,5082,5082,5082,5082,5082,5082,5082,ICLR,2020,Deep Lifetime Clustering,S Chandra Mouli;Leonardo Teixeira;Jennifer Neville;Bruno Ribeiro,chandr@purdue.edu;lteixeir@purdue.edu;ribeiro@cs.purdue.edu;neville@cs.purdue.edu,6;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,Purdue University;Purdue University;Purdue University;Purdue University,27;27;27;27,88;88;88;88,1
5083,5083,5083,5083,5083,5083,5083,5083,ICLR,2020,PassNet: Learning pass probability surfaces from single-location labels. An architecture for visually-interpretable soccer analytics,Javier Fernández;Luke Bornn,javier.fernandezr@fcbarcelona.cat;lbornn@kings.com,3;8;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Universitat Politècnica de Catalunya;Kings,266;-1,612;-1,
5084,5084,5084,5084,5084,5084,5084,5084,ICLR,2020,Few-Shot One-Class Classification via Meta-Learning,Ahmed Frikha;Denis Krompaß;Hans-Georg Koepken;Volker Tresp,ahmed.frikha@siemens.com;denis.krompass@siemens.com;hans-georg.koepken@siemens.com;volker.tresp@siemens.com,1;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,13,0.0,yes,9/25/19,Siemens Corporate Research;Siemens Corporate Research;Siemens Corporate Research;Siemens Corporate Research,-1;-1;-1;-1,-1;-1;-1;-1,6
5085,5085,5085,5085,5085,5085,5085,5085,ICLR,2020,Simple is Better: Training an End-to-end Contract Bridge Bidding Agent without Human Knowledge,Qucheng Gong;Yu Jiang;Yuandong Tian,qucheng@fb.com;tinayujiang@fb.com;yuandong@fb.com,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,3,0.0,yes,9/25/19,Facebook;Facebook;Facebook,-1;-1;-1,-1;-1;-1,
5086,5086,5086,5086,5086,5086,5086,5086,ICLR,2020,Rethinking deep active learning: Using unlabeled data at model training,Oriane Siméoni;Mateusz Budnik;Yannis Avrithis;Guillaume Gravier,oriane.simeoni@inria.fr;mateusz.budnik@inria.fr;yannis@avrithis.net;guig@irisa.fr,3;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,7,0.0,yes,9/25/19,"INRIA;INRIA;INRIA;IRISA, Université Bretagne Sud",-1;-1;-1;481,-1;-1;-1;1397,
5087,5087,5087,5087,5087,5087,5087,5087,ICLR,2020,Why Convolutional Networks Learn Oriented Bandpass Filters: A Hypothesis,Richard P. Wildes,wildes@cse.yorku.ca,3;3;1;3,I have published in this field for several years.:N/A:N/A:N/A;I do not know much about this area.:N/A:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published in this field for several years.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,York University,172,416,
5088,5088,5088,5088,5088,5088,5088,5088,ICLR,2020,Deep Expectation-Maximization in Hidden Markov Models via Simultaneous Perturbation Stochastic Approximation,Chong Li;Dan Shen;C.J. Richard Shi;Hongxia Yang,chongli@uw.edu;dshen@alibaba-inc.com;cjshi@uw.edu;yang.yhx@alibaba-inc.com,3;3,I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,2,0.0,yes,9/25/19,"University of Washington, Seattle;Alibaba Group;University of Washington, Seattle;Alibaba Group",6;-1;6;-1,26;-1;26;-1,1
5089,5089,5089,5089,5089,5089,5089,5089,ICLR,2020,Learning to Contextually Aggregate Multi-Source Supervision for Sequence Labeling,Ouyu Lan*;Xiao Huang*;Bill Yuchen Lin;He Jiang;Xiang Ren,olan@usc.edu;huan183@usc.edu;yuchen.lin@usc.edu;jian567@usc.edu;xiangren@usc.edu,6;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,University of Southern California;University of Southern California;University of Southern California;University of Southern California;University of Southern California,31;31;31;31;31,62;62;62;62;62,3;6
5090,5090,5090,5090,5090,5090,5090,5090,ICLR,2020,Disentangled Cumulants Help Successor Representations Transfer to New Tasks,Chris Grimm;Irina Higgins;Andre Barreto;Denis Teplyashin;Markus Wulfmeier;Tim Hertweck;Raia Hadsell;Satinder Singh,crgrimm@umich.edu;irinah@google.com;andrebarreto@google.com;teplyashin@google.com;mwulfmeier@google.com;thertweck@google.com;raia@google.com;baveja@google.com,6;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,University of Michigan;Google;Google;Google;Google;Google;Google;Google,8;-1;-1;-1;-1;-1;-1;-1,21;-1;-1;-1;-1;-1;-1;-1,
5091,5091,5091,5091,5091,5091,5091,5091,ICLR,2020,Adversarial training with perturbation generator networks,Hyeungill Lee;Sungyeob Han;Jungwoo Lee,hyungil0113@snu.ac.kr;yubise7en@snu.ac.kr;junglee@snu.ac.kr,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,9,0.0,yes,9/25/19,Seoul National University;Seoul National University;Seoul National University,41;41;41,64;64;64,4
5092,5092,5092,5092,5092,5092,5092,5092,ICLR,2020,Batch Normalization has Multiple Benefits: An Empirical Study on Residual Networks,Soham De;Samuel L Smith,sohamde@google.com;slsmith@google.com,3;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,4,0.0,yes,9/25/19,Google;Google,-1;-1,-1;-1,
5093,5093,5093,5093,5093,5093,5093,5093,ICLR,2020,A TWO-STAGE FRAMEWORK FOR MATHEMATICAL EXPRESSION RECOGNITION,Jin Zhang;Weipeng Ming;Pengfei Liu,zhangjin9@100tal.com;mingweipeng@100tal.com;liupengfei1@100tal.com,1;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,100tal;Chinese Academy of Sciences;100tal,-1;59;-1,-1;1397;-1,2;8
5094,5094,5094,5094,5094,5094,5094,5094,ICLR,2020,The Sooner The Better: Investigating Structure of Early Winning Lottery Tickets,Shihui Yin;Kyu-Hyoun Kim;Jinwook Oh;Naigang Wang;Mauricio Serrano;Jae-Sun Seo;Jungwook Choi,shihui.yin@ibm.com;kimk@us.ibm.com;ohj@us.ibm.com;nwang@us.ibm.com;mserrano@us.ibm.com;jaesun.seo@asu.edu;choij@hanyang.ac.kr,6;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,6,0.0,yes,9/25/19,International Business Machines;International Business Machines;International Business Machines;International Business Machines;International Business Machines;Arizona State University;Hanyang University,-1;-1;-1;-1;-1;95;233,-1;-1;-1;-1;-1;155;393,
5095,5095,5095,5095,5095,5095,5095,5095,ICLR,2020,How Well Do WGANs Estimate the Wasserstein Metric?,Anton Mallasto;Guido Montúfar;Augusto Gerolin,anton.mallasto@gmail.com;montufar@math.ucla.edu;augustogerolin@gmail.com,1;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,3,0.0,yes,9/25/19,"University of Copenhagen;University of California, Los Angeles;",100;20;-1,101;17;-1,5;4
5096,5096,5096,5096,5096,5096,5096,5096,ICLR,2020,A Group-Theoretic Framework for Knowledge Graph Embedding,Tong Yang;Long Sha;Pengyu Hong,yangto@bc.edu;longsha@brandeis.edu;hongpeng@brandeis.edu,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,4,4,0.0,yes,9/25/19,Boston College;Brandeis University;Brandeis University,266;323;323,323;244;244,10
5097,5097,5097,5097,5097,5097,5097,5097,ICLR,2020,Accelerated Information Gradient flow,Yifei Wang;Wuchen Li,zackwang24@pku.edu.cn;wcli@math.ucla.edu,3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,"Peking University;University of California, Los Angeles",22;20,24;17,11;1
5098,5098,5098,5098,5098,5098,5098,5098,ICLR,2020,Time2Vec: Learning a Vector Representation of Time,Seyed Mehran Kazemi;Rishab Goel;Sepehr Eghbali;Janahan Ramanan;Jaspreet Sahota;Sanjay Thakur;Stella Wu;Cathal Smyth;Pascal Poupart;Marcus Brubaker,mehran.kazemi@borealisai.com;rishab.goel@borealisai.com;sepehr.eghbali@rbc.com;janahan.ramanan@borealisai.com;jaspreet.sahota@borealisai.com;sttsanjay@gmail.com;stella.wu@borealisai.com;cathal.smyth@rbc.com;pascal.poupart@borealisai.com;marcus.brubaker@borealisai.com,6;1;3;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Borealis AI;Borealis AI;Borealis AI;Borealis AI;Borealis AI;;Borealis AI;Borealis AI;Borealis AI;Borealis AI,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,
5099,5099,5099,5099,5099,5099,5099,5099,ICLR,2020,MGP-AttTCN: An Interpretable Machine Learning Model for the Prediction of Sepsis,Margherita Rosnati;Vincent Fortuin,mrosnati@ethz.ch;fortuin@inf.ethz.ch,3;1;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10,13;13,
5100,5100,5100,5100,5100,5100,5100,5100,ICLR,2020,NESTED LEARNING FOR MULTI-GRANULAR TASKS,Raphaël Achddou;J. Matias Di Martino;Guillermo Sapiro,raphael.achddou@telecom-paristech.fr;matiasdm@fing.edu.uy;guillermo.sapiro@duke.edu,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Télécom ParisTech;Facultad de Ingeniería;Duke University,481;-1;47,187;-1;20,
5101,5101,5101,5101,5101,5101,5101,5101,ICLR,2020,Attack-Resistant Federated Learning with Residual-based Reweighting,Shuhao Fu;Chulin Xie;Bo Li;Qifeng Chen,sfuab@connect.ust.hk;chulinxie@zju.edu.cn;lbo@illinois.edu;chenqifeng22@gmail.com,3;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,1,0.0,yes,9/25/19,"The Hong Kong University of Science and Technology;Zhejiang University;University of Illinois, Urbana Champaign;Hong Kong University of Science and Technology",39;56;3;39,47;107;48;47,4
5102,5102,5102,5102,5102,5102,5102,5102,ICLR,2020,Self-Supervised Policy Adaptation,Christopher Mutschler;Sebastian Pokutta,christopher.mutschler@iis.fraunhofer.de;pokutta@zib.de,1;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,1,0.0,yes,9/25/19,Fraunhofer IIS;,-1;-1,-1;-1,
5103,5103,5103,5103,5103,5103,5103,5103,ICLR,2020,Efficient generation of structured objects with Constrained Adversarial Networks,Jacopo Gobbi;Luca Di Liello;Pierfrancesco Ardino;Paolo Morettin;Stefano Teso;Andrea Passerini,jacopo.gobbi@studenti.unitn.it;luca.diliello@studenti.unitn.it;pierfrancesco.ardino@unitn.it;paolo.morettin@unitn.it;stefano.teso@gmail.com;andrea.passerini@unitn.it,3;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,University of Trento;University of Trento;University of Trento;University of Trento;;University of Trento,18;18;18;18;-1;18,307;307;307;307;-1;307,5;4;10
5104,5104,5104,5104,5104,5104,5104,5104,ICLR,2020,Subgraph Attention for Node Classification and Hierarchical Graph Pooling,Sambaran Bandyopadhyay;Manasvi Aggarwal;M. N. Murty,sambaran.ban89@gmail.com;manasvia@iisc.ac.in;mnm@iisc.ac.in,3;6;1,I do not know much about this area.:N/A:N/A:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,International Business Machines;Indian Institute of Science;Indian Institute of Science,-1;95;95,-1;301;301,10
5105,5105,5105,5105,5105,5105,5105,5105,ICLR,2020,Learning from Partially-Observed Multimodal Data with Variational Autoencoders,Yu Gong;Hossein Hajimirsadeghi;Jiawei He;Megha Nawhal;Thibaut Durand;Greg Mori,yu_gong@sfu.ca;hossein.hajimirsadeghi@gmail.com;jha203@sfu.ca;mnawhal@sfu.ca;thibaut.p.durand@borealisai.com;mori@cs.sfu.ca,3;3;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,Simon Fraser University;Borealis AI;Simon Fraser University;Simon Fraser University;Borealis AI;Simon Fraser University,64;-1;64;64;-1;64,272;-1;272;272;-1;272,5
5106,5106,5106,5106,5106,5106,5106,5106,ICLR,2020,A Simple Approach to the Noisy Label Problem Through the Gambler's Loss,Liu Ziyin;Ru Wang;Paul Pu Liang;Ruslan Salakhutdinov;Louis-Philippe Morency;Masahito Ueda,zliu@cat.phys.s.u-tokyo.ac.jp;wangru1994305@gmail.com;pliang@cs.cmu.edu;rsalakhu@cs.cmu.edu;morency@cs.cmu.edu;ueda@phys.s.u-tokyo.ac.jp,6;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,The University of Tokyo;;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;The University of Tokyo,56;-1;1;1;1;56,36;-1;27;27;27;36,8
5107,5107,5107,5107,5107,5107,5107,5107,ICLR,2020,Contextual Temperature for Language Modeling,Pei-Hsin Wang;Sheng-Iou Hsieh;Shieh-Chieh Chang;Jia-Yu Pan;Yu-Ting Chen;Wei Wei;Da-Cheng Juan,peihsin@gapp.nthu.edu.tw;steins1111@gapp.nthu.edu.tw;scchang@cs.nthu.edu.tw;jypan@google.com;yutingchen@google.com;wewei@google.com;dacheng@google.com,3;6;3,I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,5,0.0,yes,9/25/19,National Tsing Hua University;National Tsing Hua University;National Tsing Hua University;Google;Google;Google;Google,172;172;172;-1;-1;-1;-1,365;365;365;-1;-1;-1;-1,3
5108,5108,5108,5108,5108,5108,5108,5108,ICLR,2020,MULTI-LABEL METRIC LEARNING WITH BIDIRECTIONAL REPRESENTATION DEEP NEURAL NETWORKS,Tao Zheng;Ivor Tsang;Xin Yao,tao.zheng@student.uts.edu.au;ivor.tsang@uts.edu.au;xiny@sustech.edu.cn,1;3;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,University of Technology Sydney;University of Technology Sydney;Tsinghua University,108;108;8,193;193;23,
5109,5109,5109,5109,5109,5109,5109,5109,ICLR,2020,Unsupervised Data Augmentation for Consistency Training,Qizhe Xie;Zihang Dai;Eduard Hovy;Minh-Thang Luong;Quoc V. Le,qizhex@cs.cmu.edu;dzihang@cs.cmu.edu;hovy@cs.cmu.edu;thangluong@google.com;qvl@google.com,8;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,11,1.0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Google;Google,1;1;1;-1;-1,27;27;27;-1;-1,6
5110,5110,5110,5110,5110,5110,5110,5110,ICLR,2020,Representation Learning Through Latent Canonicalizations,Or Litany;Ari Morcos;Srinath Sridhar;Leonidas Guibas;Judy Hoffman,orlitany@gmail.com;arimorcos@gmail.com;ssrinath@cs.stanford.edu;guibas@cs.stanford.edu;judy@gatech.edu,3;8;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Stanford University;Facebook;Stanford University;Stanford University;Georgia Institute of Technology,4;-1;4;4;13,4;-1;4;4;38,8
5111,5111,5111,5111,5111,5111,5111,5111,ICLR,2020,Molecular Graph Enhanced Transformer for Retrosynthesis Prediction,Kelong Mao;Peilin Zhao;Tingyang Xu;Yu Rong;Xi Xiao;Junzhou Huang,mkl18@mails.tsinghua.edu.cn;masonzhao@tencent.com;tingyangxu@tencent.com;yu.rong@hotmail.com;xiaox@sz.tsinghua.edu.cn;joehhuang@tencent.com,3;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Tsinghua University;Tencent AI Lab;Tencent AI Lab;Tencent AI Lab;Tsinghua University;Tencent AI Lab,8;-1;-1;-1;8;-1,23;-1;-1;-1;23;-1,3;10
5112,5112,5112,5112,5112,5112,5112,5112,ICLR,2020,Frequency Pooling: Shift-Equivalent and Anti-Aliasing Down Sampling,Zhendong Zhang;Cheolkon Jung,zhd.zhang.ai@gmail.com;zhengzk@xidian.edu.cn,1;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,Tsinghua University;Tsinghua University,8;8,23;23,
5113,5113,5113,5113,5113,5113,5113,5113,ICLR,2020,Where is the Information in a Deep Network?,Alessandro Achille;Stefano Soatto,achille@cs.ucla.edu;soatto@cs.ucla.edu,6;8;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,8,1.0,yes,9/25/19,"University of California, Los Angeles;University of California, Los Angeles",20;20,17;17,1;8
5114,5114,5114,5114,5114,5114,5114,5114,ICLR,2020,FleXOR: Trainable Fractional Quantization,Dongsoo Lee;Se Jung Kwon;Byeongwook Kim;Yongkweon Jeon;Baeseong Park;Jeongin Yun;Gu-Yeon Wei,dslee3@gmail.com;mogndrewk@gmail.com;quddnr145@gmail.com;dragwon.jeon@gmail.com;qkrqotjd91@gmail.com;yji6373@naver.com;gywei@g.harvard.edu,3;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,;Samsung;Samsung;Samsung;;Samsung;Harvard University,-1;-1;-1;-1;-1;-1;39,-1;-1;-1;-1;-1;-1;7,
5115,5115,5115,5115,5115,5115,5115,5115,ICLR,2020,Mixture Distributions for Scalable Bayesian Inference,Pranav Poduval;Hrushikesh Loya;Rajat Patel;Sumit Jain,pranav97.poduval@gmail.com;loyahrushikesh@gmail.com;prajat5232@iitb.ac.in;sumitjain3033@gmail.com,1;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,13,0.0,yes,9/25/19,Indian Institute of Technology Bombay;Indian Institute of Technology Bombay;Indian Institute of Technology Bombay;,118;118;118;-1,480;480;480;-1,4;11
5116,5116,5116,5116,5116,5116,5116,5116,ICLR,2020,On the Invertibility of Invertible Neural Networks,Jens Behrmann;Paul Vicol;Kuan-Chieh Wang;Roger B. Grosse;Jörn-Henrik Jacobsen,jensb@uni-bremen.de;pvicol@cs.toronto.edu;wangkua1@cs.toronto.edu;rgrosse@cs.toronto.edu;j.jacobsen@vectorinstitute.ai,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,"Universität Bremen;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Vector Institute",154;18;18;18;-1,360;18;18;18;-1,5
5117,5117,5117,5117,5117,5117,5117,5117,ICLR,2020,Meta Learning via Learned Loss,Sarah Bechtle;Artem Molchanov;Yevgen Chebotar;Edward Grefenstette;Ludovic Righetti;Gaurav Sukhatme;Franziska Meier,sbechtle@tuebingen.mpg.de;molchano@usc.edu;ychebota@usc.edu;egrefen@gmail.com;ludovic.righetti@nyu.edu;gaurav@usc.edu;fmeier@fb.com,3;3;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,"Max Planck Institute for Intelligent Systems, Max-Planck Institute;University of Southern California;University of Southern California;Facebook;New York University;University of Southern California;Facebook",-1;31;31;-1;25;31;-1,-1;62;62;-1;29;62;-1,6
5118,5118,5118,5118,5118,5118,5118,5118,ICLR,2020,Demonstration Actor Critic,Guoqing Liu;Li Zhao;Pushi Zhang;Jiang Bian;Tao Qin;Nenghai Yu;Tie-Yan Liu,lgq1001@mail.ustc.edu.cn;lizo@microsoft.com;zpschang@gmail.com;jiang.bian@microsoft.com;taoqin@microsoft.com;ynh@ustc.edu.cn;tyliu@microsoft.com,6;6;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,9,0.0,yes,9/25/19,University of Science and Technology of China;Microsoft;Tsinghua University;Microsoft;Microsoft;University of Science and Technology of China;Microsoft,481;-1;8;-1;-1;481;-1,80;-1;23;-1;-1;80;-1,1
5119,5119,5119,5119,5119,5119,5119,5119,ICLR,2020,Moniqua: Modulo Quantized Communication in Decentralized SGD,Yucheng Lu;Christopher De Sa,yl2967@cornell.edu;cdesa@cs.cornell.edu,3;3;3;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,10,0.0,yes,9/25/19,Cornell University;Cornell University,7;7,19;19,1;10
5120,5120,5120,5120,5120,5120,5120,5120,ICLR,2020,Testing For Typicality with Respect to an Ensemble of Learned Distributions,Forrest Laine;Claire Tomlin,forrest.laine@berkeley.edu;tomlin@eecs.berkeley.edu,3;1;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,University of California Berkeley;University of California Berkeley,5;5,13;13,
5121,5121,5121,5121,5121,5121,5121,5121,ICLR,2020,Min-Max Optimization without Gradients: Convergence and Applications to Adversarial ML,Sijia Liu;Songtao Lu;Xiangyi Chen;Yao Feng;Kaidi Xu;Abdullah Al-Dujaili;Minyi Hong;Una-May Obelilly,sijia.liu@ibm.com;songtao@ibm.com;chen5719@umn.edu;feng-y16@mails.tsinghua.edu.cn;xu.kaid@husky.neu.edu;aldujail@mit.edu;mhong@umn.edu;unamay@csail.mit.edu,3;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,"International Business Machines;International Business Machines;University of Minnesota, Minneapolis;Tsinghua University;Northeastern University;Massachusetts Institute of Technology;University of Minnesota, Minneapolis;Massachusetts Institute of Technology",-1;-1;59;8;16;2;59;2,-1;-1;79;23;906;5;79;5,4;9
5122,5122,5122,5122,5122,5122,5122,5122,ICLR,2020,Wyner VAE: A Variational Autoencoder with Succinct Common Representation Learning,J. Jon Ryu;Yoojin Choi;Young-Han Kim;Mostafa El-Khamy;Jungwon Lee,jongha.ryu@gmail.com;yoojin.c@samsung.com;yhk@ucsd.edu;mostafa.e@samsung.com;jungwon2.lee@samsung.com,6;6;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,15,0.0,yes,9/25/19,"University of California, San Diego;Samsung;University of California, San Diego;Samsung;Samsung",11;-1;11;-1;-1,31;-1;31;-1;-1,5
5123,5123,5123,5123,5123,5123,5123,5123,ICLR,2020,Improving Confident-Classifiers For Out-of-distribution Detection,Sachin Vernekar;Ashish Gaurav;Vahdat Abdelzad;Taylor Denouden;Rick Salay;Krzysztof Czarnecki,sverneka@uwaterloo.ca;a5gaurav@uwaterloo.ca;vabdelza@gsd.uwaterloo.ca;tadenoud@uwaterloo.ca;rsalay@gsd.uwaterloo.ca;kczarnec@gsd.uwaterloo.ca,3;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,University of Waterloo;University of Waterloo;University of Waterloo;University of Waterloo;University of Waterloo;University of Waterloo,28;28;28;28;28;28,235;235;235;235;235;235,
5124,5124,5124,5124,5124,5124,5124,5124,ICLR,2020,FINBERT:  FINANCIAL SENTIMENT ANALYSIS   WITH PRE-TRAINED LANGUAGE MODELS,Dogu Araci;Zulkuf Genc,dogu.araci@naspers.com;zulkuf.genc@naspers.com,3;3;1;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Naspers;Prosus,-1;-1,-1;-1,3;6
5125,5125,5125,5125,5125,5125,5125,5125,ICLR,2020,Rethinking Neural Network Quantization,Qing Jin;Linjie Yang;Zhenyu Liao,jinqingking@gmail.com;yljatthu@gmail.com;liaozhenyu2004@gmail.com,3;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,Bytedance;Bytedance;Kuaishou,-1;-1;-1,-1;-1;-1,
5126,5126,5126,5126,5126,5126,5126,5126,ICLR,2020,A Simple and Scalable Shape Representation for 3D Reconstruction,Mateusz Michalkiewicz;Eugene Belilovsky;Mahsa Baktashmotagh;Anders Eriksson,78lhar@gmail.com;belilovsky.eugene@gmail.com;m.baktashmotlagh@uq.edu.au;a.eriksson@uq.edu.au,6;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,University of Queensland;University of Montreal;University of Queensland;University of Queensland,205;128;205;205,66;85;66;66,
5127,5127,5127,5127,5127,5127,5127,5127,ICLR,2020,Learning Cross-Context Entity Representations from Text,Jeffrey Ling;Nicholas FitzGerald;Zifei Shan;Livio Baldini Soares;Thibault Févry;David Weiss;Tom Kwiatkowski,jeffreyling@google.com;nfitz@google.com;zifeis@google.com;liviobs@google.com;tfevry@google.com;djweiss@google.com;tomkwiat@google.com,3;3;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:N/A:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,3;6
5128,5128,5128,5128,5128,5128,5128,5128,ICLR,2020,Unsupervised Few Shot Learning via Self-supervised Training,Zilong Ji;Xiaolong Zou;Tiejun Huang;Si Wu,jizilong@mail.bnu.edu.cn;xiaolz@pku.edu.cn;tjhuang@pku.edu.cn;siwu@pku.edu.cn,1;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:N/A:N/A:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Australian National University;Peking University;Peking University;Peking University,108;22;22;22,50;24;24;24,6
5129,5129,5129,5129,5129,5129,5129,5129,ICLR,2020,Confidence-Calibrated Adversarial Training: Towards Robust Models Generalizing Beyond the Attack Used During Training,David Stutz;Matthias Hein;Bernt Schiele,david.stutz@mpi-inf.mpg.de;schiele@mpi-inf.mpg.de;matthias.hein@uni-tuebingen.de,3;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,9,0.0,yes,9/25/19,"Saarland Informatics Campus, Max-Planck Institute;Saarland Informatics Campus, Max-Planck Institute;University of Tuebingen",-1;-1;154,-1;-1;91,4
5130,5130,5130,5130,5130,5130,5130,5130,ICLR,2020,Optimal Unsupervised Domain Translation,Emmanuel de Bézenac;Ibrahim Ayed;Patrick Gallinari,emmanuel.de-bezenac@lip6.fr;ayedibrahim@gmail.com;patrick.gallinari@lip6.fr,3;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,LIP6;LIP6;LIP6,-1;-1;-1,-1;-1;-1,
5131,5131,5131,5131,5131,5131,5131,5131,ICLR,2020,A Uniform Generalization Error Bound for Generative Adversarial Networks,Hao Chen;Zhanfeng Mo;Qingyi Gao;Zhouwang Yang;Xiao Wang,ch330822@mail.ustc.edu.cn;oscarmzf@mail.ustc.edu.cn;gao424@purdue.edu;yangzw@ustc.edu.cn;wangxiao@purdue.edu,1;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,University of Science and Technology of China;University of Science and Technology of China;Purdue University;University of Science and Technology of China;Purdue University,481;481;27;481;27,80;80;88;80;88,5;4;1;8
5132,5132,5132,5132,5132,5132,5132,5132,ICLR,2020,Efficient Wrapper Feature Selection using Autoencoder and Model Based Elimination,Sharan Ramjee;Aly El Gamal,sramjee@purdue.edu;elgamala@purdue.edu,1;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,1,5,0.0,yes,9/25/19,Purdue University;Purdue University,27;27,88;88,
5133,5133,5133,5133,5133,5133,5133,5133,ICLR,2020,Training Data Distribution Search with Ensemble Active Learning,Kashyap Chitta;Jose M. Alvarez;Elmar Haussmann;Clement Farabet,kashyap.chitta@tue.mpg.de;josea@nvidia.com;ehaussmann@nvidia.com;cfarabet@nvidia.com,6;6;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,3,0.0,yes,9/25/19,"Max Planck Institute for Intelligent Systems, Max-Planck Institute;NVIDIA;NVIDIA;NVIDIA",-1;-1;-1;-1,-1;-1;-1;-1,
5134,5134,5134,5134,5134,5134,5134,5134,ICLR,2020,Coresets for Accelerating Incremental Gradient Methods,Baharan Mirzasoleiman;Jeff Bilmes;Jure Leskovec,baharanm@cs.stanford.edu;bilmes@uw.edu;jure@cs.stanford.edu,8;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,"Stanford University;University of Washington, Seattle;Stanford University",4;6;4,4;26;4,1
5135,5135,5135,5135,5135,5135,5135,5135,ICLR,2020,Gaussian MRF Covariance Modeling for Efficient Black-Box Adversarial Attacks,Anit Kumar Sahu;J. Zico Kolter;Satya Narayan Shukla,anit.sahu@gmail.com;zkolter@cs.cmu.edu;snshukla@cs.umass.edu,8;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,";Carnegie Mellon University;University of Massachusetts, Amherst",-1;1;28,-1;27;209,4;11
5136,5136,5136,5136,5136,5136,5136,5136,ICLR,2020,How can we generalise learning distributed representations of graphs?,Paul M Scherer;Pietro Lio,pms69@cam.ac.uk;pl219@cam.ac.uk,3;6;1,I have published one or two papers in this area.:I carefully checked the experiments.:N/A:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,8,0.0,yes,9/25/19,University of Cambridge;University of Cambridge,71;71,3;3,10
5137,5137,5137,5137,5137,5137,5137,5137,ICLR,2020,Graph Residual Flow for Molecular Graph Generation,Shion Honda;Hirotaka Akita;Katsuhiko Ishiguro;Toshiki Nakanishi;Kenta Oono,26x.orc.ed5.1hs@gmail.com;akita714@preferred.jp;k.ishiguro.jp@ieee.org;nakanishi@preferred.jp;oono@preferred.jp,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,"The University of Tokyo;Preferred Networks, Inc.;Preferred Networks, Inc.;Preferred Networks, Inc.;Preferred Networks, Inc.",56;-1;-1;-1;-1,36;-1;-1;-1;-1,5;10
5138,5138,5138,5138,5138,5138,5138,5138,ICLR,2020,Regularizing Deep Multi-Task Networks using Orthogonal Gradients,Mihai Suteu;Yi-ke Guo,m.suteu16@imperial.ac.uk;y.guo@imperial.ac.uk,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,3,0.0,yes,9/25/19,Imperial College London;Imperial College London,73;73,10;10,
5139,5139,5139,5139,5139,5139,5139,5139,ICLR,2020,Blending Diverse Physical Priors with Neural Networks,Yunhao Ba;Guangyuan Zhao;Achuta Kadambi,yhba@ucla.edu;zhaoguangyuan@ucla.edu;achuta@ee.ucla.edu,6;3;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,6,0.0,yes,9/25/19,"University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles",20;20;20,17;17;17,8
5140,5140,5140,5140,5140,5140,5140,5140,ICLR,2020,The Detection of Distributional Discrepancy for Text Generation,Xingyuan Chen;Ping Cai;Peng Jin;Haokun Du;Hongjun Wang;Xinyu Dai;Jiajun Chen,1045258214@qq.com;1061185275@qq.com;jandp@pku.edu.cn;626913553@qq.com;wanghongjun@swjtu.edu.cn;daixinyu@nju.edu.cn;chenjj@nju.edu.cn,1;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,;Tsinghua University;Peking University;;Tsinghua University;Zhejiang University;Zhejiang University,-1;8;22;-1;8;56;56,-1;23;24;-1;23;107;107,3;4;5
5141,5141,5141,5141,5141,5141,5141,5141,ICLR,2020,DG-GAN: the GAN with the duality gap,Cheng Peng;Hao Wang;Xiao Wang;Zhouwang Yang,pch0051@mail.ustc.edu.cn;wh001@mail.ustc.edu.cn;wangxiao@purdue.edu;yangzw@ustc.edu.cn,1;1;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,University of Science and Technology of China;University of Science and Technology of China;Purdue University;University of Science and Technology of China,481;481;27;481,80;80;88;80,5;4;1;8
5142,5142,5142,5142,5142,5142,5142,5142,ICLR,2020,Samples Are Useful? Not Always: denoising policy gradient updates using variance explained,Yannis Flet-Berliac;Philippe Preux,yannis.flet-berliac@inria.fr;philippe.preux@inria.fr,3;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,INRIA;INRIA,-1;-1,-1;-1,
5143,5143,5143,5143,5143,5143,5143,5143,ICLR,2020,Continual Learning via Principal Components Projection,Gyuhak Kim;Bing Liu,gkim87@uic.edu;liub@uic.edu,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,"University of Illinois, Chicago;University of Illinois, Chicago",56;56,254;254,
5144,5144,5144,5144,5144,5144,5144,5144,ICLR,2020,Iterative Deep Graph Learning for Graph Neural Networks,Yu Chen;Lingfei Wu;Mohammed J. Zaki,cheny39@rpi.edu;lwu@email.wm.edu;zaki@cs.rpi.edu,3;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Rensselaer Polytechnic Institute;College of William and Mary;Rensselaer Polytechnic Institute,172;154;172,438;235;438,10
5145,5145,5145,5145,5145,5145,5145,5145,ICLR,2020,Training Deep Neural Networks by optimizing over nonlocal paths in hyperparameter space,Vlad Pushkarov;Yonathan Efroni;Mykola Maksymenko;Maciej Koch-Janusz,vladpush@icloud.com;jonathan.efroni@gmail.com;mmaks@softserveinc.com;maciejk@ethz.ch,6;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Icloud;Technion;SoftServe Inc.;Swiss Federal Institute of Technology,-1;26;-1;10,-1;412;-1;13,
5146,5146,5146,5146,5146,5146,5146,5146,ICLR,2020,An Optimization Principle Of Deep Learning?,Cheng Chen;Junjie Yang;Yi Zhou,u0952128@utah.edu;yang.4972@buckeyemail.osu.edu;yi.zhou@utah.edu,3;1;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,University of Utah;Ohio State University;University of Utah,233;77;233,366;373;366,9
5147,5147,5147,5147,5147,5147,5147,5147,ICLR,2020,Amata: An Annealing Mechanism for Adversarial Training Acceleration,Nanyang Ye;Qianxiao Li;Zhanxing Zhu,yn272@cam.ac.uk;qianxiao@nus.edu.sg;zhanxing.zhu@pku.edu.cn,6;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,University of Cambridge;National University of Singapore;Peking University,71;16;22,3;25;24,4;1
5148,5148,5148,5148,5148,5148,5148,5148,ICLR,2020,Sparsity Meets Robustness: Channel Pruning for the Feynman-Kac Formalism Principled Robust Deep Neural Nets,Thu Dinh*;Bao Wang*;Andrea L. Bertozzi;Stanley J. Osher;Jack Xin,thud2@uci.edu;wangbaonj@gmail.com;bertozzi@math.ucla.edu;sjo@math.ucla.edu;jxin@math.uci.edu,3;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,"University of California, Irvine;University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles;University of California, Irvine",35;20;20;20;35,96;17;17;17;96,4
5149,5149,5149,5149,5149,5149,5149,5149,ICLR,2020,Perceptual Generative Autoencoders,Zijun Zhang;Ruixiang Zhang;Zongpeng Li;Yoshua Bengio;Liam Paull,zijun.zhang@ucalgary.ca;sodabeta7@gmail.com;zongpeng@whu.edu.cn;yoshua.bengio@mila.quebec;paulll@iro.umontreal.ca,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,University of Calgary;MILA;Wuhan University;University of Montreal;University of Montreal,172;-1;266;128;128,210;-1;354;85;85,5
5150,5150,5150,5150,5150,5150,5150,5150,ICLR,2020,Deep symbolic regression,Brenden K. Petersen,petersen33@llnl.gov,3;6;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Lawrence Livermore National Labs,-1,-1,
5151,5151,5151,5151,5151,5151,5151,5151,ICLR,2020,Task-Relevant Adversarial Imitation Learning,Konrad Zolna;Scott Reed;Alexander Novikov;Ziyu Wang;Sergio Gómez;David Budden;Serkan Cabi;Misha Denil;Nando de Freitas,konrad.zolna@gmail.com;reedscot@google.com;anovikov@google.com;ziyu@google.com;sergomez@google.com;budden@google.com;cabi@google.com;mdenil@google.com;nandodefreitas@google.com,8;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,1,5,0.0,yes,9/25/19,Jagiellonian University;Google;Google;Google;Google;Google;Google;Google;Google,481;-1;-1;-1;-1;-1;-1;-1;-1,610;-1;-1;-1;-1;-1;-1;-1;-1,5;4
5152,5152,5152,5152,5152,5152,5152,5152,ICLR,2020,"Imitation Learning of Robot Policies using Language, Vision and Motion",Simon Stepputtis;Joseph Campbell;Mariano Phielipp;Chitta Baral;Heni Ben Amor,sstepput@asu.edu;jacampb1@asu.edu;mariano.j.phielipp@intel.com;chitta@asu.edu;hbenamor@asu.edu,6;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,6,0.0,yes,9/25/19,Arizona State University;Arizona State University;Intel;Arizona State University;Arizona State University,95;95;-1;95;95,155;155;-1;155;155,3;8
5153,5153,5153,5153,5153,5153,5153,5153,ICLR,2020,Why Learning of Large-Scale Neural Networks Behaves Like Convex Optimization,Hui Jiang,hj@cse.yorku.ca,1;1;1;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,York University,172,416,9
5154,5154,5154,5154,5154,5154,5154,5154,ICLR,2020,Information Theoretic Model Predictive Q-Learning,Mohak Bhardwaj;Ankur Handa;Dieter Fox;Byron Boots,mohakb@cs.washington.edu;ahanda@nvidia.com;fox@cs.washington.edu;bboots@cs.washington.edu,3;6;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,9,0.0,yes,9/25/19,University of Washington;NVIDIA;University of Washington;University of Washington,6;-1;6;6,26;-1;26;26,
5155,5155,5155,5155,5155,5155,5155,5155,ICLR,2020,Learning to Optimize via Dual space Preconditioning,Sélim Chraibi;Adil Salim;Samuel Horváth;Filip Hanzely;Peter Richtárik,selimsepthuit@gmail.com;adil.salim@kaust.edu.sa;samuel.horvath@kaust.edu.sa;filip.hanzely@kaust.edu.sa;richtarik@gmail.com,3;3;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,CNRS;KAUST;KAUST;KAUST;KAUST,-1;128;128;128;128,-1;1397;1397;1397;1397,
5156,5156,5156,5156,5156,5156,5156,5156,ICLR,2020,HaarPooling: Graph Pooling with Compressive Haar Basis,Yu Guang Wang;Ming Li;Zheng Ma;Guido Montufar;Xiaosheng Zhuang;Yanan Fan,yuguang.wang@unsw.edu.au;ming.li.ltu@gmail.com;mzheng@princeton.edu;montufar@math.ucla.edu;xzhuang7@cityu.edu.hk;y.fan@unsw.edu.au,3;3;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,9,0.0,yes,9/25/19,"University of New South Wales;Australian National University;Princeton University;University of California, Los Angeles;City University of Hong Kong;University of New South Wales",481;108;31;20;92;481,1397;50;6;17;35;1397,10
5157,5157,5157,5157,5157,5157,5157,5157,ICLR,2020,Neural-Guided Symbolic Regression with Asymptotic Constraints,Li Li;Minjie Fan;Rishabh Singh;Patrick Riley,leeley@google.com;mjfan@google.com;rising@google.com;pfr@google.com,3;3;8,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:N/A:N/A,Reject,0,12,0.0,yes,9/25/19,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,
5158,5158,5158,5158,5158,5158,5158,5158,ICLR,2020,Handwritten Amharic Character Recognition System Using Convolutional Neural Networks,Fetulhak Abdurahman,afetulhak@yahoo.com,1;1;1,I have published one or two papers in this area.:I did not assess the experiments.:N/A:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,1,1,0.0,yes,9/25/19,Zhejiang University,56,107,
5159,5159,5159,5159,5159,5159,5159,5159,ICLR,2020,Deep Relational Factorization Machines,Hongchang Gao;Gang Wu;Ryan Rossi;Viswanathan Swaminathan;Heng Huang,hongchanggao@gmail.com;gawu@adobe.com;ryrossi@adobe.com;vishy@adobe.com;henghuanghh@gmail.com,1;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,1,0.0,yes,9/25/19,University of Pittsburgh;Adobe Systems;Adobe Systems;Adobe Systems;University of Pittsburgh,79;-1;-1;-1;79,113;-1;-1;-1;113,10
5160,5160,5160,5160,5160,5160,5160,5160,ICLR,2020,Dirichlet Wrapper to Quantify Classification Uncertainty in Black-Box Systems,José Mena Roldán;Oriol Pujol Vila;Jordi Vitrià Marca,jmenarol7@alumnes.ub.edu;oriol_pujol@ub.edu;jordi.vitria@ub.edu,6;1;1;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Universitat de Barcelona;Universitat de Barcelona;Universitat de Barcelona,481;481;481,213;213;213,3;2
5161,5161,5161,5161,5161,5161,5161,5161,ICLR,2020,DO-AutoEncoder: Learning and Intervening Bivariate Causal Mechanisms in Images,Tianshuo Cong;Dan Peng;Furui Liu;Zhitang Chen,cts17@mails.tsinghua.edu.cn;lepangdan@outlook.com;liufurui2@huawei.com;chenzhitang2@huawei.com,1;1;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,3,0.0,yes,9/25/19,Tsinghua University;Harbin Institute of Technology;Huawei Technologies Ltd.;Huawei Technologies Ltd.,8;172;-1;-1,23;424;-1;-1,4;10
5162,5162,5162,5162,5162,5162,5162,5162,ICLR,2020,Forecasting Deep Learning Dynamics with Applications to Hyperparameter Tuning,Piotr Kozakowski;Łukasz Kaiser;Afroz Mohiuddin,p.kozakowski@mimuw.edu.pl;lukaszkaiser@google.com;afrozm@google.com,3;6;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,"University of Washington, Seattle;Google;Google",6;-1;-1,26;-1;-1,
5163,5163,5163,5163,5163,5163,5163,5163,ICLR,2020,Disentangling Trainability and Generalization in Deep Learning,Lechao Xiao;Jeffrey Pennington;Sam Schoenholz,xlc@google.com;jpennin@google.com;schsam@google.com,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Google;Google;Google,-1;-1;-1,-1;-1;-1,8
5164,5164,5164,5164,5164,5164,5164,5164,ICLR,2020,Versatile Anomaly Detection with Outlier Preserving Distribution Mapping Autoencoders,Walter Gerych;Elke Rundensteiner;Emmanuel Agu,wgerych@wpi.edu;rundenst@wpi.edu;emmanuel@wpi.edu,6;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Worcester Polytechnic Institute;Worcester Polytechnic Institute;Worcester Polytechnic Institute,172;172;172,628;628;628,
5165,5165,5165,5165,5165,5165,5165,5165,ICLR,2020,DeepPCM: Predicting Protein-Ligand Binding using Unsupervised Learned Representations,Paul Kim;Robin Winter;Djork-Arné Clevert,paul.kim@bayer.com;robin.winter@bayer.com;djork-arne.clevert@bayer.com,3;1;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,2,0,0.0,yes,9/25/19,Bayer Ag;Bayer Ag;Bayer Ag,-1;-1;-1,-1;-1;-1,
5166,5166,5166,5166,5166,5166,5166,5166,ICLR,2020,Learning transitional skills with intrinsic motivation,Qiangxing Tian;Jinxin Liu;Donglin Wang,11821087@zju.edu.cn;liujinxin@westlake.edu.cn;wangdonglin@westlake.edu.cn,3;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Zhejiang University;Westlake University;Westlake University,56;-1;-1,107;-1;-1,
5167,5167,5167,5167,5167,5167,5167,5167,ICLR,2020,Preventing Imitation Learning with Adversarial Policy Ensembles,Albert Zhan;Pieter Abbeel;Stas Tiomkin,albertzhan@berkeley.edu;pabbeel@cs.berkeley.edu;stasti@gmail.com,3;1;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,13;13;13,4
5168,5168,5168,5168,5168,5168,5168,5168,ICLR,2020,Noisy Machines: Understanding noisy neural networks and enhancing robustness to analog hardware errors using distillation,Chuteng Zhou;Prad Kadambi;Matthew Mattina;Paul N. Whatmough,chu.zhou@arm.com;pkadambi@asu.edu;matthew.mattina@arm.com;paul.whatmough@arm.com,3;6;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,5,0.0,yes,9/25/19,arm;Arizona State University;arm;arm,-1;95;-1;-1,-1;155;-1;-1,
5169,5169,5169,5169,5169,5169,5169,5169,ICLR,2020,Mutual Information Maximization for Robust Plannable Representations,Yiming Ding;Ignasi Clavera;Pieter Abbeel,dingyiming0427@berkeley.edu;iclavera@berkeley.edu;pabbeel@cs.berkeley.edu,3;1;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,13;13;13,
5170,5170,5170,5170,5170,5170,5170,5170,ICLR,2020,Removing input features via a generative model to explain their attributions to classifier's decisions,Chirag Agarwal;Dan Schonfeld;Anh Nguyen,chiragagarwall12@gmail.com;dans@uic.edu;anh.ng8@gmail.com,1;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,8,0.0,yes,9/25/19,"University of Illinois, Chicago;University of Illinois, Chicago;Auburn University",56;56;390,254;254;651,5
5171,5171,5171,5171,5171,5171,5171,5171,ICLR,2020,Gauge Equivariant Spherical CNNs,Berkay Kicanaoglu;Pim de Haan;Taco Cohen,b.kicanaoglu@uva.nl;pimdehaan@gmail.com;taco.cohen@gmail.com,8;8;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,University of Amsterdam;University of Amsterdam;University of Amsterdam,172;172;172,62;62;62,2;10
5172,5172,5172,5172,5172,5172,5172,5172,ICLR,2020,Relation-based Generalized Zero-shot Classification with the Domain Discriminator on the shared representation,Masahiro Suzuki;Yutaka Matsuo,masa@weblab.t.u-tokyo.ac.jp;matsuo@weblab.t.u-tokyo.ac.jp,6;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,The University of Tokyo;The University of Tokyo,56;56,36;36,5;6
5173,5173,5173,5173,5173,5173,5173,5173,ICLR,2020,Machine Truth Serum,Tianyi Luo;Yang Liu,tluo6@ucsc.edu;yangliu@ucsc.edu,6;1;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,University of Southern California;University of Southern California,31;31,62;62,11
5174,5174,5174,5174,5174,5174,5174,5174,ICLR,2020,A Gradient-Based Approach to Neural Networks Structure Learning,Amir Ali Moinfar;Amirkeivan Mohtashami;Mahdieh Soleymani;Ali Sharifi-Zarchi,moinfar@ce.sharif.edu;mohtashami@ce.sharif.edu;soleymani@sharif.edu;sharifi@sharif.edu,6;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Sharif University of Technology;Sharif University of Technology;Sharif University of Technology;Sharif University of Technology,323;323;323;323,564;564;564;564,
5175,5175,5175,5175,5175,5175,5175,5175,ICLR,2020,Learning Generative Models using Denoising Density Estimators,Siavash Bigdeli;Geng Lin;Tiziano Portenier;Andrea Dunbar;Matthias Zwicker,siavash.bigdeli@csem.ch;geng@cs.umd.edu;tiziano.portenier@vision.ee.ethz.ch;andrea.dunbar@csem.ch;zwicker@cs.umd.edu,1;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0.0,yes,9/25/19,"Swiss Center for Electronics and Micro Electronics;University of Maryland, College Park;Swiss Federal Institute of Technology;Swiss Center for Electronics and Micro Electronics;University of Maryland, College Park",-1;12;10;-1;12,-1;91;13;-1;91,5;4;1
5176,5176,5176,5176,5176,5176,5176,5176,ICLR,2020,NPTC-net: Narrow-Band Parallel Transport Convolutional Neural Network on Point Clouds,Pengfei Jin;Tianhao Lai;Rongjie Lai;Bin Dong,jinpf@pku.edu.cn;howeverlth@pku.edu.cn;lair@rpi.edu;dongbin@math.pku.edu.cn,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,3,0.0,yes,9/25/19,Peking University;Peking University;Rensselaer Polytechnic Institute;Peking University,22;22;172;22,24;24;438;24,2;8
5177,5177,5177,5177,5177,5177,5177,5177,ICLR,2020,Attributed Graph Learning with 2-D Graph Convolution,Qimai Li;Xiaotong Zhang;Han Liu;Xiao-Ming Wu,csqmli@comp.polyu.edu.hk;zxt.dut@hotmail.com;liu.han.dut@gmail.com;xiao-ming.wu@polyu.edu.hk,3;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,4,0.0,yes,9/25/19,The Hong Kong Polytechnic University;The Hong Kong Polytechnic University;The Hong Kong Polytechnic University;The Hong Kong Polytechnic University,172;172;172;172,171;171;171;171,10
5178,5178,5178,5178,5178,5178,5178,5178,ICLR,2020,Pareto Optimality in No-Harm Fairness,Natalia Martinez;Martin Bertran;Guillermo Sapiro,natalia.martinez@duke.edu;martin.bertran@duke.edu;guillermo.sapiro@duke.edu,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Duke University;Duke University;Duke University,47;47;47,20;20;20,1;7
5179,5179,5179,5179,5179,5179,5179,5179,ICLR,2020,Conditional generation of molecules from disentangled representations,Amina Mollaysa;Brooks Paige;Alexandros  Kalousis,amina.mollaysa@gmail.com;tbpaige@gmail.com;alexandros.kalousis@hesge.ch,3;1;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,"University of Geneva, Switzerland;University College London;Geneva School of Business Administration, HES-SO University of Applied Sciences of Western Switzerland;",481;50;481;-1,144;15;1397;-1,
5180,5180,5180,5180,5180,5180,5180,5180,ICLR,2020,Overparameterized Neural Networks Can Implement Associative Memory,Adityanarayanan Radhakrishnan;Mikhail Belkin;Caroline Uhler,aradha@mit.edu;mbelkin@cse.ohio-state.edu;cuhler@mit.edu,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,9,0.0,yes,9/25/19,Massachusetts Institute of Technology;Ohio State University;Massachusetts Institute of Technology,2;77;2,5;373;5,
5181,5181,5181,5181,5181,5181,5181,5181,ICLR,2020,TinyBERT: Distilling BERT for Natural Language Understanding,Xiaoqi Jiao;Yichun Yin;Lifeng Shang;Xin Jiang;Xiao Chen;Linlin Li;Fang Wang;Qun Liu,jiaoxiaoqi@hust.edu.cn;yinyichun@huawei.com;shang.lifeng@huawei.com;jiang.xin@huawei.com;chen.xiao2@huawei.com;lynn.lilinlin@huawei.com;wangfang@hust.edu.cn;qun.liu@huawei.com,8;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,1,9,0.0,yes,9/25/19,Hong Kong University of Science and Technology;Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.;Hong Kong University of Science and Technology;Huawei Technologies Ltd.,39;-1;-1;-1;-1;-1;39;-1,47;-1;-1;-1;-1;-1;47;-1,3
5182,5182,5182,5182,5182,5182,5182,5182,ICLR,2020,Cross Domain Imitation Learning,Kun Ho Kim;Yihong Gu;Jiaming Song;Shengjia Zhao;Stefano Ermon,khkim@cs.stanford.edu;gyh15@mails.tsinghua.edu.cn;jiaming.tsong@gmail.com;sjzhao@stanford.edu;ermon@cs.stanford.edu,3;6;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,5,0.0,yes,9/25/19,Stanford University;Tsinghua University;Stanford University;Stanford University;Stanford University,4;8;4;4;4,4;23;4;4;4,5;4;6
5183,5183,5183,5183,5183,5183,5183,5183,ICLR,2020,Multi-source Multi-view Transfer Learning in Neural Topic Modeling with Pretrained Topic and Word Embeddings,Pankaj Gupta;Yatin Chaudhary;Hinrich Schütze,pankaj_gupta96@yahoo.com;yatinchaudhary91@gmail.com;hinrich@hotmail.com,6;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,Siemens Corporate Research;Siemens Corporate Research;Institut für Informatik,-1;-1;-1,-1;-1;-1,3;6;8
5184,5184,5184,5184,5184,5184,5184,5184,ICLR,2020,Natural Image Manipulation for Autoregressive Models Using Fisher Scores,Wilson Yan;Jonathan Ho;Pieter Abbeel,wilson1.yan@berkeley.edu;jonathanho@berkeley.edu;pabbeel@cs.berkeley.edu,3;8;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,4,0.0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,13;13;13,5
5185,5185,5185,5185,5185,5185,5185,5185,ICLR,2020,SPECTRA: Sparse Entity-centric Transitions,Rim Assouel;Yoshua Bengio,rim.assouel@hotmail.fr;yoshua.bengio@mila.quebec,3;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,1,0.0,yes,9/25/19,University of Montreal;University of Montreal,128;128,85;85,
5186,5186,5186,5186,5186,5186,5186,5186,ICLR,2020,Attentive Sequential Neural Processes,Jaesik Yoon;Gautam Singh;Sungjin Ahn,jaesik817@gmail.com;singh.gautam.iitg@gmail.com;sjn.ahn@gmail.com,6;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,6,0.0,yes,9/25/19,;Rutgers University;Rutgers University,-1;34;34,-1;168;168,
5187,5187,5187,5187,5187,5187,5187,5187,ICLR,2020,Generative Latent Flow,Zhisheng Xiao;Qing Yan;Yali Amit,zxiao@uchicago.edu;yanq@uchicago.edu;amit@marx.uchicago.edu,3;3;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,University of Chicago;University of Chicago;University of Chicago,48;48;48,9;9;9,5
5188,5188,5188,5188,5188,5188,5188,5188,ICLR,2020,"Farkas layers: don't shift the data, fix the geometry",Aram-Alexandre Pooladian;Chris Finlay;Adam M Oberman,aram-alexandre.pooladian@mail.mcgill.ca;christopher.finlay@gmail.com;adam.oberman@mcgill.ca,1;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,McGill University;McGill University;McGill University,86;86;86,42;42;42,
5189,5189,5189,5189,5189,5189,5189,5189,ICLR,2020,STABILITY AND CONVERGENCE THEORY FOR LEARNING RESNET: A FULL CHARACTERIZATION,Huishuai Zhang;Da Yu;Mingyang Yi;Wei Chen;Tie-yan Liu,huishuai.zhang@microsoft.com;yuda3@mail2.sysu.edu.cn;v-minyi@microsoft.com;wche@microsoft.com;tie-yan.liu@microsoft.com,3;1;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,1,4,0.0,yes,9/25/19,Microsoft;SUN YAT-SEN UNIVERSITY;Microsoft;Microsoft;Microsoft,-1;481;-1;-1;-1,-1;299;-1;-1;-1,9
5190,5190,5190,5190,5190,5190,5190,5190,ICLR,2020,Universal approximations of permutation invariant/equivariant functions by deep neural networks,Akiyoshi Sannai;Yuuki Takai;Matthieu Cordonnier,akiyoshi.sannai@riken.jp;yuuki.takai@riken.jp;matthieu.cordonnier@ens-paris-saclay.fr,3;3;3,I do not know much about this area.:N/A:N/A:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:N/A:N/A:I did not assess the derivations or theory.;I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,3,0.0,yes,9/25/19,RIKEN;RIKEN;Ecole Normale Superieure,-1;-1;100,-1;-1;45,
5191,5191,5191,5191,5191,5191,5191,5191,ICLR,2020,Context-aware Attention Model for Coreference Resolution,Yufei Li;Xiangyu Zhou;Jie Ma;Yu Long;Xuan Wang;Chen Li,vermouthtarot@gmail.com;zxy951005@stu.xjtu.edu.cn;majack@stu.xjtu.edu.cn;longyu95@stu.xjtu.edu.cn;wangxuan8888@stu.xjtu.edu.cn;cli@xjtu.edu.cn,1;1;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,;Xi'an Jiaotong University;Xi'an Jiaotong University;Xi'an Jiaotong University;Xi'an Jiaotong University;Xi'an Jiaotong University,-1;481;481;481;481;481,-1;555;555;555;555;555,
5192,5192,5192,5192,5192,5192,5192,5192,ICLR,2020,A Random Matrix Perspective on Mixtures of Nonlinearities in High Dimensions,Ben Adlam;Jake Levinson;Jeffrey Pennington,adlam@google.com;jpennin@google.com;jlev@google.com,8;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Google;Google;Google,-1;-1;-1,-1;-1;-1,
5193,5193,5193,5193,5193,5193,5193,5193,ICLR,2020,CaptainGAN: Navigate Through Embedding Space For Better Text Generation,Chun-Hsing Lin;Alvin Chiang;Chi-Liang Liu;Chien-Fu Lin;Po-Hsien Chu;Siang-Ruei Wu;Yi-En Tsai;Chung-Yang (Ric) Huang,jsaon92@gmail.com;alvin.chiang.180@gmail.com;liangtaiwan1230@gmail.com;gblin75468@gmail.com;cph@yoctol.com;raywu0@gmail.com;ypiheyn.imm02g@g2.nctu.edu.tw;cyhuang@ntu.edu.tw,6;6;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,12,0.0,yes,9/25/19,;;National Taiwan University;;Yoctol;;National Chiao Tung University;National Taiwan University,-1;-1;86;-1;-1;-1;143;86,-1;-1;120;-1;-1;-1;564;120,3;5
5194,5194,5194,5194,5194,5194,5194,5194,ICLR,2020,Policy Optimization with Stochastic Mirror Descent,Long Yang;Gang Zheng;Zavier Zhang;Yu Zhang;Qian Zheng;Jun Wen;Gang Pana sample efficient policy gradient method with stochastic mirror descent.,yanglong@zju.edu.cn;gang_zheng@zju.edu.cn;21721269@zju.edu.cn;hzzhangyu@zju.edu.cn;csqianzheng@gmail.com;junwen@zju.edu.cn;gpan@zju.edu.cn,3;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Zhejiang University;Zhejiang University;Zhejiang University;Zhejiang University;;Zhejiang University;Zhejiang University,56;56;56;56;-1;56;56,107;107;107;107;-1;107;107,9
5195,5195,5195,5195,5195,5195,5195,5195,ICLR,2020,A Novel Analysis Framework of Lower Complexity Bounds for Finite-Sum Optimization,Guangzeng Xie;Luo Luo;Zhihua Zhang,smsxgz@pku.edu.cn;rickyluoluo@gmail.com;zhzhang@math.pku.edu.cn,3;8;6,I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,4,0.0,yes,9/25/19,Peking University;Shanghai Jiao Tong University;Peking University,22;53;22,24;157;24,1
5196,5196,5196,5196,5196,5196,5196,5196,ICLR,2020,Reducing Computation in Recurrent Networks by Selectively Updating State Neurons,Thomas Hartvigsen;Cansu Sen;Xiangnan Kong;Elke Rundensteiner,twhartvigsen@wpi.edu;csen@wpi.edu;xkong@wpi.edu;rundenst@wpi.edu,6;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Worcester Polytechnic Institute;Worcester Polytechnic Institute;Worcester Polytechnic Institute;Worcester Polytechnic Institute,172;172;172;172,628;628;628;628,
5197,5197,5197,5197,5197,5197,5197,5197,ICLR,2020,Selective Brain Damage: Measuring the Disparate Impact of Model Pruning,Sara Hooker;Yann Dauphin;Aaron Courville;Andrea Frome,shooker@google.com;ynd@google.com;aaron.courville@gmail.com;onepinkfairyarmadillo@gmail.com,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,4,0.0,yes,9/25/19,Google;Google;University of Montreal;,-1;-1;128;-1,-1;-1;85;-1,
5198,5198,5198,5198,5198,5198,5198,5198,ICLR,2020,Deep exploration by novelty-pursuit with maximum state entropy,Zi-Niu Li;Xiong-Hui Chen;Yang Yu,liziniu1997@gmail.com;chenxh@lamda.nju.edu.cn;yuy@lamda.nju.edu.cn,1;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,Xi'an Jiaotong University;Zhejiang University;Zhejiang University,481;56;56,555;107;107,
5199,5199,5199,5199,5199,5199,5199,5199,ICLR,2020,Smart Ternary Quantization,Gregoire Morin;Ryan Razani;Vahid Partovi Nia;Eyyub Sari,gregoire.morin@huawei.com;ryan.razani@huawei.com;vahid.partovinia@huawei.com;eyyub.sari@huawei.com,3;6;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.,Reject,0,3,0.0,yes,9/25/19,Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.,-1;-1;-1;-1,-1;-1;-1;-1,
5200,5200,5200,5200,5200,5200,5200,5200,ICLR,2020,Alleviating Privacy Attacks via Causal Learning,Shruti Tople;Amit Sharma;Aditya Nori,t-shtopl@microsoft.com;amshar@microsoft.com;adityan@microsoft.com,3;6;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,14,0.0,yes,9/25/19,Microsoft;Microsoft;Microsoft,-1;-1;-1,-1;-1;-1,11;4;1
5201,5201,5201,5201,5201,5201,5201,5201,ICLR,2020,Generalized Domain Adaptation with Covariate and Label Shift CO-ALignment,Shuhan Tan;Xingchao Peng;Kate Saenko,tanshh@mail2.sysu.edu.cn;xpeng@bu.edu;saenko@bu.edu,6;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,1,5,0.0,yes,9/25/19,SUN YAT-SEN UNIVERSITY;Boston University;Boston University,481;67;67,299;61;61,
5202,5202,5202,5202,5202,5202,5202,5202,ICLR,2020,Deeper Insights into Weight Sharing in Neural Architecture Search,Yuge Zhang;Quanlu Zhang;Junyang Jiang;Zejun Lin;Yujing Wang,scottyugochang@gmail.com;quanlu.zhang@microsoft.com;jyjiang97@gmail.com;gdzejlin@gmail.com;yujing.wang@microsoft.com,3;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Microsoft;Microsoft;Fudan University;Columbia University;Microsoft,-1;-1;79;15;-1,-1;-1;109;16;-1,
5203,5203,5203,5203,5203,5203,5203,5203,ICLR,2020,Combining MixMatch and Active Learning for Better Accuracy with Fewer Labels,Shuang Song;David Berthelot;Afshin Rostamizadeh,shuangsong@google.com;dberth@google.com;rostami@google.com,3;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Google;Google;Google,-1;-1;-1,-1;-1;-1,
5204,5204,5204,5204,5204,5204,5204,5204,ICLR,2020,Perceptual Regularization: Visualizing and Learning Generalizable Representations,Hongzhou Lin;Joshua Robinson;Stefanie Jegelka,hongzhou@mit.edu;joshrob@mit.edu;stefje@csail.mit.edu,6;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,6,0.0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2,5;5;5,4
5205,5205,5205,5205,5205,5205,5205,5205,ICLR,2020,Attention Forcing for Sequence-to-sequence Model Training,Qingyun Dou;Yiting Lu;Joshua Efiong;Mark J.F. Gales,qd212@cam.ac.uk;ytl28@cam.ac.uk;je369@cam.ac.uk;mjfg@cam.ac.uk,1;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,2,4,0.0,yes,9/25/19,University of Cambridge;University of Cambridge;University of Cambridge;University of Cambridge,71;71;71;71,3;3;3;3,3
5206,5206,5206,5206,5206,5206,5206,5206,ICLR,2020,Transferable Recognition-Aware Image Processing,Zhuang Liu;Tinghui Zhou;Zhiqiang Shen;Bingyi Kang;Trevor Darrell,zhuangl@berkeley.edu;tinghuiz@eecs.berkeley.edu;zhiqians@andrew.cmu.edu;kang@u.nus.edu;trevor@eecs.berkeley.edu,8;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0.0,yes,9/25/19,University of California Berkeley;University of California Berkeley;Carnegie Mellon University;National University of Singapore;University of California Berkeley,5;5;1;16;5,13;13;27;25;13,
5207,5207,5207,5207,5207,5207,5207,5207,ICLR,2020,NeuralUCB: Contextual Bandits with Neural Network-Based Exploration,Dongruo Zhou;Lihong Li;Quanquan Gu,drzhou@cs.ucla.edu;lihongli.cs@gmail.com;qgu@cs.ucla.edu,3;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,10,0.0,yes,9/25/19,"University of California, Los Angeles;Google;University of California, Los Angeles",20;-1;20,17;-1;17,1
5208,5208,5208,5208,5208,5208,5208,5208,ICLR,2020,Quantized Reinforcement Learning (QuaRL),Srivatsan Krishnan;Sharad Chitlangia;Maximilian Lam;Zishen Wan;Aleksandra Faust;Vijay Janapa Reddi,srivatsan@seas.harvard.edu;f20170472@goa.bits-pilani.ac.in;maxlam@g.harvard.edu;zishenwan@g.harvard.edu;sandrafaust@google.com;vj@eecs.harvard.edu,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Reject,0,3,0.0,yes,9/25/19,"Harvard University;BITS Pilani, BITS Pilani;Harvard University;Harvard University;Google;Harvard University",39;480;39;39;-1;39,7;1397;7;7;-1;7,
5209,5209,5209,5209,5209,5209,5209,5209,ICLR,2020,CNAS: Channel-Level Neural Architecture Search,Heechul Lim;Min-Soo Kim;Jinjun Xiong,skyde1021@dgist.ac.kr;mskim@dgist.ac.kr;jinjun@us.ibm.com,3;1;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Gwangju Institute of Science and Technology;Gwangju Institute of Science and Technology;International Business Machines,-1;-1;-1,-1;-1;-1,
5210,5210,5210,5210,5210,5210,5210,5210,ICLR,2020,Measuring Numerical Common Sense: Is A Word Embedding Approach Effective?,Hiroaki Yamane;Chin-Yew Lin;Tatsuya Harada,hiroaki.yamane@riken.jp;cyl@microsoft.com;harada@mi.t.u-tokyo.ac.jp,1;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,0,0.0,yes,9/25/19,RIKEN;Microsoft;The University of Tokyo,-1;-1;56,-1;-1;36,3
5211,5211,5211,5211,5211,5211,5211,5211,ICLR,2020,Semi-supervised Semantic Segmentation using Auxiliary Network,Wei-Hsu Chen;Hsueh-Ming Hang,qoososola520.ee06g@nctu.edu.tw;hmhang@nctu.edu.tw,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,National Chiao Tung University;National Chiao Tung University,143;143,564;564,4;2
5212,5212,5212,5212,5212,5212,5212,5212,ICLR,2020,DeepSFM: Structure From Motion Via Deep Bundle Adjustment,Xingkui Wei;Yinda Zhang;Zhuwen Li;Yanwei Fu;Xiangyang Xue,xkwei19@fudan.edu.cn;yindaz@cs.princeton.edu;lzhuwen@gmail.com;yanweifu@fudan.edu.cn;xyxue@fudan.edu.cn,6;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,"Fudan University;Princeton University;Nuro, Inc;Fudan University;Fudan University",79;31;-1;79;79,109;6;-1;109;109,2
5213,5213,5213,5213,5213,5213,5213,5213,ICLR,2020,A Base Model Selection Methodology for Efficient Fine-Tuning,Yosuke Ueno;Masaaki Kondo,ueno@hal.ipc.i.u-tokyo.ac.jp;kondo@hal.ipc.i.u-tokyo.ac.jp,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0.0,yes,9/25/19,The University of Tokyo;The University of Tokyo,56;56,36;36,6
5214,5214,5214,5214,5214,5214,5214,5214,ICLR,2020,Localized Meta-Learning: A PAC-Bayes Analysis for Meta-Leanring Beyond Global Prior,Chenghao Liu;Tao Lu;Doyen Sahoo;Yuan Fang;Steven C.H. Hoi.,chliu@smu.edu.sg;lutaott@zju.edu.cn;doyensahoo@gmail.com;yfang@smu.edu.sg;shoi@salesforce.com,3;3;8;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,9,0.0,yes,9/25/19,Singapore Management University;Zhejiang University;;Singapore Management University;SalesForce.com,92;56;-1;92;-1,1397;107;-1;1397;-1,6
5215,5215,5215,5215,5215,5215,5215,5215,ICLR,2020,Searching to Exploit Memorization Effect in Learning from Corrupted Labels,Hansi Yang;Quanming Yao;Bo Han;Gang Niu,yhs17@mails.tsinghua.edu.cn;qyaoaa@connect.ust.hk;bo.han@riken.jp;gang.niu@riken.jp,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,Tsinghua University;The Hong Kong University of Science and Technology;RIKEN;RIKEN,8;39;-1;-1,23;47;-1;-1,
5216,5216,5216,5216,5216,5216,5216,5216,ICLR,2020,Attention over Phrases,Wanyun Cui,cui.wanyun@sufe.edu.cn,3;1;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,2,0.0,yes,9/25/19,Shanghai University of Finance and Economics,266,1397,3
5217,5217,5217,5217,5217,5217,5217,5217,ICLR,2020,Retrospection: Leveraging the Past for Efficient Training of Deep Neural Networks,Ayush Chopra;Surgan Jandial;Mausoom Sarkar;Balaji Krishnamurthy;Vineeth Balasubramanian,ayuchopr@adobe.com;cs17btech11038@iith.ac.in;msarkar@adobe.com;kbalaji@adobe.com;vineethnb@iith.ac.in,8;3,I have read many papers in this area.:I carefully checked the experiments.:N/A:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,Adobe Systems;Indian Institute of Technology Hyderabad;Adobe Systems;Adobe Systems;Indian Institute of Technology Hyderabad,-1;205;-1;-1;205,-1;713;-1;-1;713,
5218,5218,5218,5218,5218,5218,5218,5218,ICLR,2020,Improving Dirichlet Prior Network for Out-of-Distribution Example Detection,Jay Nandy,a0123886@u.nus.edu,3;3;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,National University of Singapore,16,25,
5219,5219,5219,5219,5219,5219,5219,5219,ICLR,2020,Continual Density Ratio Estimation (CDRE): A new method for evaluating generative models in continual learning,Yu Chen;Song Liu;Tom Diethe;Peter Flach,yc14600@bristol.ac.uk;song.liu@bristol.ac.uk;tdiethe@amazon.com;peter.flach@bristol.ac.uk,1;1;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0.0,yes,9/25/19,University of Bristol;University of Bristol;Amazon;University of Bristol,128;128;-1;128,87;87;-1;87,5
5220,5220,5220,5220,5220,5220,5220,5220,ICLR,2020,Prune or quantize? Strategy for Pareto-optimally low-cost and accurate CNN,Kengo Nakata;Daisuke Miyashita;Asuka Maki;Fumihiko Tachibana;Shinichi Sasaki;Jun Deguchi,kengo1.nakata@toshiba.co.jp;daisuke1.miyashita@toshiba.co.jp;asuka.maki@toshiba.co.jp;fumihiko.tachibana@toshiba.co.jp;shinichi8.sasaki@toshiba.co.jp;jun.deguchi@toshiba.co.jp,3;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,11,0.0,yes,9/25/19,Toshiba Memory;Toshiba Memory;Toshiba Memory;Toshiba Memory;Toshiba Memory;Toshiba Memory,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,
5221,5221,5221,5221,5221,5221,5221,5221,ICLR,2020,Concise Multi-head Attention Models,Srinadh Bhojanapalli;Chulhee Yun;Ankit Singh Rawat;Sashank Reddi;Sanjiv Kumar,bsrinadh@google.com;chulheey@mit.edu;ankitsrawat@google.com;sashank@google.com;sanjivk@google.com,1;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0.0,yes,9/25/19,Google;Massachusetts Institute of Technology;Google;Google;Google,-1;2;-1;-1;-1,-1;5;-1;-1;-1,3
5222,5222,5222,5222,5222,5222,5222,5222,ICLR,2020,A Finite-Time Analysis of  Q-Learning with Neural Network Function Approximation,Pan Xu;Quanquan Gu,panxu@cs.ucla.edu;qgu@cs.ucla.edu,3;6;6,I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,4,6,0.0,yes,9/25/19,"University of California, Los Angeles;University of California, Los Angeles",20;20,17;17,1;9
5223,5223,5223,5223,5223,5223,5223,5223,ICLR,2020,SCELMo: Source Code Embeddings from Language Models,Rafael - Michael Karampatsis;Charles Sutton,mpatsis13@gmail.com;charlessutton@google.com,3;3;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,4,0.0,yes,9/25/19,University of Edinburgh;Google,33;-1,30;-1,3
5224,5224,5224,5224,5224,5224,5224,5224,ICLR,2020,GraphMix: Regularized Training of Graph Neural Networks for Semi-Supervised Learning,Vikas Verma;Meng Qu;Alex Lamb;Yoshua Bengio;Juho Kannala;Jian Tang,vikasverma.iitm@gmail.com;meng.qu@umontreal.ca;lambalex@iro.umontreal.ca;yoshua.bengio@mila.quebec;juho.kannala@aalto.fi;jian.tang@hec.ca,3;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,5,0.0,yes,9/25/19,;University of Montreal;University of Montreal;University of Montreal;Aalto University;HEC Montreal,-1;128;128;128;143;128,-1;85;85;85;182;85,10
5225,5225,5225,5225,5225,5225,5225,5225,ICLR,2020,Neural networks with motivation,Sergey A. Shuvaev;Ngoc B. Tran;Marcus Stephenson-Jones;Bo Li;Alexei A. Koulakov,sshuvaev@cshl.edu;ntran@cshl.edu;mstephen@cshl.edu;bli@cshl.edu;akula@cshl.edu,1;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,11,0.0,yes,9/25/19,Cold Spring Harbor Laboratory;Cold Spring Harbor Laboratory;Cold Spring Harbor Laboratory;Cold Spring Harbor Laboratory;Cold Spring Harbor Laboratory,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,5
5226,5226,5226,5226,5226,5226,5226,5226,ICLR,2020,Measuring causal influence with back-to-back regression: the linear case,Jean-Remi King;Francois Charton;Maxime Oquab;David Lopez-Paz,jeanremi@fb.com;fcharton@fb.com;qas@fb.com;dlp@fb.com,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0.0,yes,9/25/19,Facebook;Facebook;Facebook;Facebook,-1;-1;-1;-1,-1;-1;-1;-1,
5227,5227,5227,5227,5227,5227,5227,5227,ICLR,2020,Longitudinal Enrichment of Imaging Biomarker Representations for Improved Alzheimer's Disease Diagnosis,Saad Elbeleidy;Lyujian Lu;L. Zoe Baker;Hua Wang;Feiping Nie,selbeleidy@mymail.mines.edu;lyujianlu@mines.edu;laurenzoebaker@mymail.mines.edu;huawangcs@gmail.com;feipingnie@gmail.com,1;3;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0.0,yes,9/25/19,Colorado School of Mines;Colorado School of Mines;Colorado School of Mines;Colorado School of Mines;,172;172;172;172;-1,343;343;343;343;-1,
5228,5228,5228,5228,5228,5228,5228,5228,ICLR,2020,RefNet: Automatic Essay Scoring by Pairwise Comparison,Jiaxin Li;Jinan Zhou,jiaxin.li@link.cuhk.edu.hk;jinan.zhou@link.cuhk.edu.hk,1;3;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,The Chinese University of Hong Kong;The Chinese University of Hong Kong,59;59,35;35,
5229,5229,5229,5229,5229,5229,5229,5229,ICLR,2020,Topology of deep neural networks,Gregory Naitzat;Andrey Zhitnikov;Lek-Heng Lim,gregn@uchicago.edu;andreyz@technion.ac.il;lekheng@galton.uchicago.edu,1;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,University of Chicago;Technion;University of Chicago,48;26;48,9;412;9,8
5230,5230,5230,5230,5230,5230,5230,5230,ICLR,2020,Provable Convergence and Global Optimality  of Generative Adversarial Network,Qi Cai;Zhuoran Yang;Jason D. Lee;Shaolei S. Du;Zhaoran Wang,qicai2022@u.northwestern.edu;zy6@princeton.edu;jasonlee@princeton.edu;ssdu@ias.edu;zhaoranwang@gmail.com,3;3;3,I do not know much about this area.:N/A:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:N/A:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,"Northwestern University;Princeton University;Princeton University;Institue for Advanced Study, Princeton;Northwestern University",44;31;31;-1;44,22;6;6;-1;22,5;4;1;9
5231,5231,5231,5231,5231,5231,5231,5231,ICLR,2020,Improving Irregularly Sampled Time Series Learning with Dense Descriptors of Time,Rafael Teixeira Sousa;Lucas Araújo Pereira;Anderson da Silva Soares,rafaelts777@gmail.com;apereiral@outlook.com;engsoares@gmail.com,1;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Universidade Federal de Goiàs;;,-1;-1;-1,-1;-1;-1,
5232,5232,5232,5232,5232,5232,5232,5232,ICLR,2020,Understanding and Training Deep Diagonal Circulant Neural Networks,Alexandre Araujo;Benjamin Negrevergne;Yann Chevaleyre;Jamal Atif,alexandre.araujo@dauphine.eu;benjamin.negrevergne@dauphine.psl.eu;yann.chevaleyre@lamsade.dauphine.fr;jamal.atif@lamsade.dauphine.fr,1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,2,,yes,9/25/19,Univeristé Paris-Dauphine;Univeristé Paris-Dauphine;Univeristé Paris-Dauphine;Univeristé Paris-Dauphine,481;481;481;481,1397;1397;1397;1397,
5233,5233,5233,5233,5233,5233,5233,5233,ICLR,2020,DP-LSSGD: An Optimization Method to Lift the Utility in Privacy-Preserving ERM,Bao Wang;Quanquan Gu;March Boedihardjo;Farzin Barekat;Stanley J. Osher,wangbaonj@gmail.com;qgu@cs.ucla.edu;march@math.ucla.edu;fbarekat@math.ucla.edu;sjo@math.ucla.edu,3;6;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,"University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles",20;20;20;20;20,17;17;17;17;17,
5234,5234,5234,5234,5234,5234,5234,5234,ICLR,2020,On Global Feature Pooling for Fine-grained Visual Categorization,Pei Guo;Connor Anderson;Ryan Farrell,peiguo@cs.byu.edu;thecatalystak@gmail.com;farrell@cs.byu.edu,6;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,2,0,,yes,9/25/19,Brigham Young University;Brigham Young University;Brigham Young University,-1;-1;-1,-1;-1;-1,8
5235,5235,5235,5235,5235,5235,5235,5235,ICLR,2020,Quantifying Exposure Bias for Neural Language Generation,Tianxing He;Jingzhao Zhang;Zhiming Zhou;James Glass,cloudygoose@csail.mit.edu;jzhzhang@mit.edu;heyohai@apex.sjtu.edu.cn;glass@mit.edu,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,6,,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Shanghai Jiao Tong University;Massachusetts Institute of Technology,2;2;53;2,5;5;157;5,3
5236,5236,5236,5236,5236,5236,5236,5236,ICLR,2020,Mix-review: Alleviate Forgetting in the Pretrain-Finetune Framework for Neural Language Generation Models,Tianxing He;Jun Liu;Kyunghyun Cho;Myle Ott;Bing Liu;James Glass;Fuchun Peng,tianxing@mit.edu;junliu@fb.com;kyunghyuncho@fb.com;myleott@fb.com;bingl@fb.com;glass@mit.edu;fuchunpeng@fb.com,3;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,Massachusetts Institute of Technology;Facebook;Facebook;Facebook;Facebook;Massachusetts Institute of Technology;Facebook,2;-1;-1;-1;-1;2;-1,5;-1;-1;-1;-1;5;-1,
5237,5237,5237,5237,5237,5237,5237,5237,ICLR,2020,INVOCMAP: MAPPING METHOD NAMES TO METHOD INVOCATIONS VIA MACHINE LEARNING,Hung Phan;Ali Jannesari,hungphd@iastate.edu;jannesar@iastate.edu,1;1;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,4,,yes,9/25/19,Iowa State University;Iowa State University,172;172,399;399,3
5238,5238,5238,5238,5238,5238,5238,5238,ICLR,2020,Fix-Net: pure fixed-point representation of deep neural networks,Lukas Enderich;Fabian Timm;Lars Rosenbaum;Wolfram Burgard,lukas.enderich@de.bosch.com;fabian.timm@de.bosch.com;lars.rosenbaum@de.bosch.com;burgard@informatik.uni-freiburg.de,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,0,,yes,9/25/19,Bosch;Bosch;Bosch;Universität Freiburg,-1;-1;-1;118,-1;-1;-1;85,
5239,5239,5239,5239,5239,5239,5239,5239,ICLR,2020,Random Partition Relaxation for Training Binary and Ternary Weight Neural Network,Lukas Cavigelli;Luca Benini,cavigelli@iis.ee.ethz.ch;benini@iis.ee.ethz.ch,1;3;1;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10,13;13,
5240,5240,5240,5240,5240,5240,5240,5240,ICLR,2020,Emergent Communication in Networked Multi-Agent Reinforcement Learning,Shubham Gupta;Rishi Hazra;Amebdkar Dukkipati,shubhamg@iisc.ac.in;rishihazra@iisc.ac.in;ambedkar@iisc.ac.in,3;1;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,Indian Institute of Science;Indian Institute of Science;Indian Institute of Science,95;95;95,301;301;301,
5241,5241,5241,5241,5241,5241,5241,5241,ICLR,2020,Recognizing Plans by Learning Embeddings from Observed Action Distributions,Yantian Zha;Yikang Li;Sriram Gopalakrishnan;Hankz Hankui Zhuo;Baoxin Li;Subbarao Kambhampati,yantian.zha@asu.edu;yikangli@asu.edu;sgopal28@asu.edu;zhuohank@mail.sysu.edu.cn;baoxin.li@asu.edu;rao@asu.edu,1;3;1;3,I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,5,,yes,9/25/19,Arizona State University;Arizona State University;Arizona State University;SUN YAT-SEN UNIVERSITY;Arizona State University;Arizona State University,95;95;95;481;95;95,155;155;155;299;155;155,
5242,5242,5242,5242,5242,5242,5242,5242,ICLR,2020,Cancer homogeneity in single cell revealed by Bi-state model and Binary matrix factorization,Changlin Wan;Wennan Chang;Sha Cao;Xiao Wang;Chi Zhang,wan82@purdue.edu;chang534@purdue.edu;shacao@iu.edu;wangxiao@purdue.edu;czhang87@iu.edu,1;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Withdrawn,0,0,,yes,9/25/19,"Purdue University;Purdue University;Indiana University, Bloomington;Purdue University;Indiana University, Bloomington",27;27;73;27;73,88;88;134;88;134,
5243,5243,5243,5243,5243,5243,5243,5243,ICLR,2020,Residual EBMs: Does Real vs. Fake Text Discrimination Generalize?,Anton Bakhtin;Sam Gross;Myle Ott;Yuntian Deng;Marc'Aurelio Ranzato;Arthur Szlam,yolo@fb.com;sgross@fb.com;myleott@fb.com;dengyuntian@g.harvard.edu;ranzato@fb.com;aszlam@fb.com,1;3;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Facebook;Facebook;Facebook;Harvard University;Facebook;Facebook,-1;-1;-1;39;-1;-1,-1;-1;-1;7;-1;-1,3;8
5244,5244,5244,5244,5244,5244,5244,5244,ICLR,2020,EnsembleNet: A novel architecture for Incremental Learning,Suri Bhasker Sri Harsha;Y Kalidas,cs18s506@iittp.ac.in;ykalidas@iittp.ac.in,1;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,Indian Institute of Technology Tirupati;Indian Institute of Technology Tirupati,-1;-1,-1;-1,
5245,5245,5245,5245,5245,5245,5245,5245,ICLR,2020,Dual Sequential Monte Carlo: Tunneling Filtering and Planning in Continuous POMDPs,Yunbo Wang;Bo Liu;Jiajun Wu;Yuke Zhu;Simon Shaolei Du;Li Fei-Fei;Joshua B. Tenenbaum,yunbo.thu@gmail.com;bliu@cs.utexas.edu;jiajunw@stanford.edu;yukez@cs.stanford.edu;ssdu@ias.edu;feifeili@cs.stanford.edu;jbt@mit.edu,1;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,4,,yes,9/25/19,"Tsinghua University;University of Texas, Austin;Stanford University;Stanford University;Institue for Advanced Study, Princeton;Stanford University;Massachusetts Institute of Technology",8;22;4;4;-1;4;2,23;38;4;4;-1;4;5,4
5246,5246,5246,5246,5246,5246,5246,5246,ICLR,2020,Fast Bilinear Matrix Normalization via  Rank-1 Update,Tan Yu;Yunfeng Cai;Ping Li,tanyu01@baidu.com;caiyunfeng@baidu.com;liping11@baidu.com,6;3;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,7,,yes,9/25/19,Baidu;Baidu;Baidu,-1;-1;-1,-1;-1;-1,2
5247,5247,5247,5247,5247,5247,5247,5247,ICLR,2020,Task-Mediated Representation Learning,Sergei Bugrov;Ron Sun,bugros@rpi.edu;rsun@rpi.edu,1;1;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,0,,yes,9/25/19,Rensselaer Polytechnic Institute;Rensselaer Polytechnic Institute,172;172,438;438,5
5248,5248,5248,5248,5248,5248,5248,5248,ICLR,2020,Universality Theorems for Generative Models,Valentin Khrulkov;Ivan Oseledets,khrulkov.v@gmail.com;i.oseledets@skoltech.ru,3;1;3,I have read many papers in this area.:N/A:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:N/A:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:N/A:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Skolkovo Institute of Science and Technology;Skolkovo Institute of Science and Technology,-1;-1,-1;-1,5;1
5249,5249,5249,5249,5249,5249,5249,5249,ICLR,2020,Embodied Language Grounding with Implicit 3D Visual Feature Representations,Mihir Prabhudesai;Hsiao-Yu Fish Tung;Syed Ashar Javed;Maximilian Sieb;Adam W. Harley;Katerina Fragkiadaki,mprabhud@cs.cmu.edu;htung@cs.cmu.edu;sajaved@andrew.cmu.edu;aharley@cs.cmu.edu;katef@cs.cmu.edu,3;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,0,,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1;1,27;27;27;27;27,3;2
5250,5250,5250,5250,5250,5250,5250,5250,ICLR,2020,Geometry-Aware Visual Predictive Models of Intuitive Physics,Hsiao-Yu Fish Tung;Zhou Xian;Mihir Prabhudesai;Katerina Fragkiadaki,htung@cs.cmu.edu;zhouxian@cmu.edu;mprabhud@cs.cmu.edu;katef@cs.cmu.edu,3;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1,27;27;27;27,2
5251,5251,5251,5251,5251,5251,5251,5251,ICLR,2020,PNEN: Pyramid Non-Local Enhanced Networks,Feida Zhu;Chaowei Fang;Kai-Kuang Ma,feida.zhu@ntu.edu.sg;chwfang@connect.hku.hk;ekkma@ntu.edu.sg,1;1;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,National Taiwan University;The University of Hong Kong;National Taiwan University,86;92;86,120;35;120,
5252,5252,5252,5252,5252,5252,5252,5252,ICLR,2020,"Being Bayesian, Even Just a Bit, Fixes Overconfidence in ReLU Networks",Agustinus Kristiadi;Matthias Hein;Philipp Hennig,agustinus.kristiadi@uni-tuebingen.de;matthias.hein@uni-tuebingen.de;philipp.hennig@uni-tuebingen.de,1;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,University of Tuebingen;University of Tuebingen;University of Tuebingen,154;154;154,91;91;91,11
5253,5253,5253,5253,5253,5253,5253,5253,ICLR,2020,Auto Network Compression with Cross-Validation Gradient,Nannan Tian;Yong Liu,tiannannan@iie.ac.cn;liuyong@iie.ac.cn,1;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,"Institute of information engineering, CAS;Institute of information engineering, CAS",-1;-1,-1;-1,1
5254,5254,5254,5254,5254,5254,5254,5254,ICLR,2020,Feature-based Augmentation for Semi-Supervised Learning,Min-Hye Oh;Byung-Gook Park,listogato3@gmail.com;bgpark@snu.ac.kr,3;1;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,;Seoul National University,-1;41,-1;64,8
5255,5255,5255,5255,5255,5255,5255,5255,ICLR,2020,Capsule Networks without Routing Procedures,Zhenhua Chen;Xiwen Li;Chuhua Wang;David Crandall,chen478@iu.edu;xiwenli@wustl.edu;cw234@iu.edu;djcran@indiana.edu,1;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,6,3,,yes,9/25/19,"Indiana University, Bloomington;Washington University, St. Louis;Indiana University, Bloomington;University of Arizona",73;100;73;172,134;52;134;103,4
5256,5256,5256,5256,5256,5256,5256,5256,ICLR,2020,Noisy $\ell^{0}$-Sparse Subspace Clustering on Dimensionality Reduced Data,Yingzhen Yang,superyyzg@gmail.com,3;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Withdrawn,0,0,,yes,9/25/19,Independent Researcher,-1,-1,
5257,5257,5257,5257,5257,5257,5257,5257,ICLR,2020,Side-Tuning: Network Adaptation via Additive Side Networks,Alexander Sax;Jeffrey Zhang;Amir Zamir;Silvio Savarese;Jitendra Malik,sax@berkeley.edu;jozhang@berkeley.edu;zamir@cs.stanford.edu;ssilvio@cs.stanford.edu;malik@eecs.berkeley.edu,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,5,,yes,9/25/19,University of California Berkeley;University of California Berkeley;Stanford University;Stanford University;University of California Berkeley,5;5;4;4;5,13;13;4;4;13,3;6
5258,5258,5258,5258,5258,5258,5258,5258,ICLR,2020,Deep Multivariate Mixture of Gaussians for Object Detection under Occlusion,Yihui He;Jianren Wang,he2@andrew.cmu.edu;jianrenw@andrew.cmu.edu,1;1;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,0,,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University,1;1,27;27,
5259,5259,5259,5259,5259,5259,5259,5259,ICLR,2020,Dynamic Graph Message Passing Networks,Li Zhang;Dan Xu;Anurag Arnab;Philip H.S. Torr,lz@robots.ox.ac.uk;danxu@robots.ox.ac.uk;anurag.arnab@gmail.com;phst@robots.ox.ac.uk,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,3,,yes,9/25/19,University of Oxford;University of Oxford;Google;University of Oxford,50;50;-1;50,1;1;-1;1,2;10
5260,5260,5260,5260,5260,5260,5260,5260,ICLR,2020,POLYNOMIAL ACTIVATION FUNCTIONS,Vikas Gottemukkula,vikas11187@gmail.com,3;1;1,I have published one or two papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Zoloz,-1,-1,
5261,5261,5261,5261,5261,5261,5261,5261,ICLR,2020,CopyCAT: Taking Control of Neural Policies with Constant Attacks,Léonard Hussenot;Matthieu Geist;Olivier Pietquin,hussenot@google.com;mfgeist@google.com;pietquin@google.com,1;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,4,,yes,9/25/19,Google;Google;Google,-1;-1;-1,-1;-1;-1,4
5262,5262,5262,5262,5262,5262,5262,5262,ICLR,2020,Influence-aware Memory for Deep Reinforcement Learning,Miguel Suau;Elena Congeduti;Rolf A.N. Starre;Aleksander Czechowski;Frans A. Oliehoek,m.suaudecastro@tudelft.nl;e.congeduti@tudelft.nl;a.t.czechowski@tudelft.nl;r.a.n.starre@tudelft.nl;f.a.oliehoek@tudelft.nl,1;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,1,4,,yes,9/25/19,Delft University of Technology;Delft University of Technology;Delft University of Technology;Delft University of Technology;Delft University of Technology,89;89;89;89;89,67;67;67;67;67,
5263,5263,5263,5263,5263,5263,5263,5263,ICLR,2020,CurricularFace: Adaptive Curriculum Learning Loss for Deep Face Recognition,Yuge Huang;Yuhan Wang;Ying Tai;Xiaoming Liu;Pengcheng Shen;Shaoxin Li;Jilin Li;Feiyue Huang,huangyg@zju.edu.cn;wang_yuhan@zju.edu.cn;yingtai@tencent.com;liuxm@cse.msu.edu;quantshen@tencent.com;darwinli@tencent.com;jerolinli@tencent.com;garyhuang@tencent.com,3;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Withdrawn,0,3,,yes,9/25/19,Zhejiang University;Zhejiang University;Tencent AI Lab;SUN YAT-SEN UNIVERSITY;Tencent AI Lab;Tencent AI Lab;Tencent AI Lab;Tencent AI Lab,56;56;-1;481;-1;-1;-1;-1,107;107;-1;299;-1;-1;-1;-1,2
5264,5264,5264,5264,5264,5264,5264,5264,ICLR,2020,Compressive Hyperspherical Energy Minimization,Rongmei Lin;Weiyang Liu;Zhen Liu;Chen Feng;Zhiding Yu;James M. Rehg;Li Xiong;Le Song,rongmei.lin@emory.edu;wyliu@gatech.edu;zhen.liu.2@umontreal.ca;cfeng@nyu.edu;zhidingy@nvidia.com;rehg@gatech.edu;lxiong@emory.edu;lsong@cc.gatech.edu,6;3;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,9,,yes,9/25/19,Emory University;Georgia Institute of Technology;University of Montreal;New York University;NVIDIA;Georgia Institute of Technology;Emory University;Georgia Institute of Technology,172;13;128;25;-1;13;172;13,416;38;85;29;-1;38;416;38,4;8
5265,5265,5265,5265,5265,5265,5265,5265,ICLR,2020,Testing Robustness Against Unforeseen Adversaries,Daniel Kang*;Yi Sun*;Dan Hendrycks;Tom Brown;Jacob Steinhardt,ddkang@stanford.edu;yisun@math.columbia.edu;hendrycks@berkeley.edu;tom@openai.com;jsteinhardt@berkeley.edu,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,1,4,,yes,9/25/19,Stanford University;Columbia University;University of California Berkeley;OpenAI;University of California Berkeley,4;15;5;-1;5,4;16;13;-1;13,4
5266,5266,5266,5266,5266,5266,5266,5266,ICLR,2020,Reasoning-Aware Graph Convolutional Network for Visual Question Answering,Yangyang Cheng;Chun Yuan,chengyang317@gmail.com;yuanc@sz.tsinghua.edu.cn,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Tsinghua University;Tsinghua University,8;8,23;23,10
5267,5267,5267,5267,5267,5267,5267,5267,ICLR,2020,Text Embedding Bank Module for Detailed Image Paragraph Caption,Zengming Shen;Arjun Gupta;Thomas S. Huang,zshen5@illinois.edu;arjung2@illinois.edu;t-huang1@illinois.edu,1;1;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",3;3;3,48;48;48,3
5268,5268,5268,5268,5268,5268,5268,5268,ICLR,2020,FNNP: Fast Neural Network Pruning Using Adaptive Batch Normalization,Bailin Li;Bowen Wu;Jiang Su;Guangrun Wang,bl-zorro@163.com;wubw6@mail2.sysu.edu.cn;sujiang@dm-ai.cn;wangguangrun@dm-ai.cn,3;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Withdrawn,1,5,,yes,9/25/19,SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;DMAI Inc.;DMAI Inc.,481;481;-1;-1,299;299;-1;-1,
5269,5269,5269,5269,5269,5269,5269,5269,ICLR,2020,Boosting Ticket: Towards Practical Pruning for Adversarial Training with Lottery Ticket Hypothesis,Bai Li;Shiqi Wang;Yunhan Jia;Yantao Lu;Zhenyu Zhong;Lawrence Carin;Suman Jana,bai.li@duke.edu;tcwangshiqi@cs.columbia.edu;jack0082010@gmail.com;ylu25@syr.edu;edwardzhong@baidu.com;lcarin@duke.edu;suman@cs.columbia.edu,3;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Duke University;Columbia University;University of Michigan;Syracuse University;Baidu;Duke University;Columbia University,47;15;8;233;-1;47;15,20;16;21;292;-1;20;16,4;9
5270,5270,5270,5270,5270,5270,5270,5270,ICLR,2020,Stabilizing Neural ODE Networks with Stochasticity,Xuanqing Liu;Tesi Xiao;Si Si;Qin Cao;Sanjiv Kumar;Cho-Jui Hsieh,xqliu@cs.ucla.edu;texiao@ucdavis.edu;sisidaisy@google.com;qincao@google.com;sanjivk@google.com;chohsieh@cs.ucla.edu,6;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,"University of California, Los Angeles;University of California, Davis;Google;Google;Google;University of California, Los Angeles",20;79;-1;-1;-1;20,17;55;-1;-1;-1;17,4;8
5271,5271,5271,5271,5271,5271,5271,5271,ICLR,2020,Robustness and/or Redundancy Emerge in Overparametrized Deep Neural Networks,Stephen Casper;Xavier Boix;Vanessa D'Amario;Christopher Rodriguez;Ling Guo;Kasper Vinken;Gabriel Kreiman,scasper@college.harvard.edu;xboix@mit.edu;vanedamario@gmail.com;chrizrodz@gmail.com;kasper.vinken@kuleuven.be;gabriel.kreiman@childrens.harvard.edu,8;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,0,,yes,9/25/19,Harvard University;Massachusetts Institute of Technology;Università degli Studi di Genova;;KU Leuven;Harvard University,39;2;-1;-1;118;39,7;5;-1;-1;45;7,
5272,5272,5272,5272,5272,5272,5272,5272,ICLR,2020,Interpretable Deep Neural Network Models: Hybrid of Image Kernels and Neural Networks,Mr. Jay Hoon Jung;and Prof. YoungMin Kwon,jay.jung@stonybrook.edu;youngmin.kwon@sunykorea.ac.kr,1;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,"State University of New York, Stony Brook;Korea University",41;323,304;179,
5273,5273,5273,5273,5273,5273,5273,5273,ICLR,2020,EnsembleNet: End-to-End Optimization of Multi-headed Models,Hanhan Li;Joe Ng;Apostol (Paul) Natsev,mirror.haha@gmail.com;yhng@google.com;natsev@google.com,3;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,6,,yes,9/25/19,Google;Google;Google,-1;-1;-1,-1;-1;-1,
5274,5274,5274,5274,5274,5274,5274,5274,ICLR,2020,PatchVAE: Learning Local Latent Codes for Recognition,Kamal Gupta;Saurabh Singh;Abhinav Shrivastava,kamalgupta308@gmail.com;saurabhsingh@google.com;abhinav@cs.umd.edu,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,0,,yes,9/25/19,"University of Maryland, College Park;Google;University of Maryland, College Park",12;-1;12,91;-1;91,5
5275,5275,5275,5275,5275,5275,5275,5275,ICLR,2020,Improving One-Shot NAS By Suppressing The Posterior Fading,Xiang Li*;Chen Lin*;Chuming Li;Ming Sun;Wei Wu;Junjie Yan;Wanli Ouyang,xiang_li_1@brown.edu;linchen@sensetime.com;lichuming@sensetime.com;sunming1@sensetime.com;wuwei@sensetime.com;yanjunjie@sensetime.com;wanli.ouyang@sydney.edu.au,3;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,Brown University;SenseTime Group Limited;SenseTime Group Limited;SenseTime Group Limited;SenseTime Group Limited;SenseTime Group Limited;University of Sydney,67;-1;-1;-1;-1;-1;86,53;-1;-1;-1;-1;-1;60,11;2
5276,5276,5276,5276,5276,5276,5276,5276,ICLR,2020,Min-max Entropy for Weakly Supervised Pointwise Localization,Belharbi Soufiane;Rony Jérôme;Dolz Jose;Ben Ayed Ismail;McCaffrey Luke;Granger Eric,soufiane.belharbi.1@etsmtl.net;jerome.rony.1@etsmtl.net;jose.dolz@etsmtl.ca;ismail.benayed@etsmtl.ca;luke.mccaffrey@mcgill.ca;eric.granger@etsmtl.ca,3;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,École de technologie supérieure;École de technologie supérieure;École de technologie supérieure;École de technologie supérieure;McGill University;École de technologie supérieure,481;481;481;481;86;481,1397;1397;1397;1397;42;1397,
5277,5277,5277,5277,5277,5277,5277,5277,ICLR,2020,Context-Gated Convolution,Xudong Lin;Lin Ma;Wei Liu;Shih-Fu Chang,xudong.lin@columbia.edu;forest.linma@gmail.com;wl2223@columbia.edu;shih.fu.chang@columbia.edu,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,3,,yes,9/25/19,Columbia University;Tencent AI Lab;Columbia University;Columbia University,15;-1;15;15,16;-1;16;16,3
5278,5278,5278,5278,5278,5278,5278,5278,ICLR,2020,FAKE CAN BE REAL IN GANS,Song Tao;Jia Wang,taosong@sjtu.edu.cn;jiawang@sjtu.edu.cn,1;3;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Withdrawn,0,4,,yes,9/25/19,Shanghai Jiao Tong University;Shanghai Jiao Tong University,53;53,157;157,5;4;1;8
5279,5279,5279,5279,5279,5279,5279,5279,ICLR,2020,WHAT ILLNESS OF LANDSCAPE CAN OVER-PARAMETERIZATION ALONE CURE?,Dawei Li;Tian Ding;Ruoyu Sun,dawei2@illinois.edu;dt016@ie.cuhk.edu.hk;ruoyus@illinois.edu,3;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,9,,yes,9/25/19,"University of Illinois, Urbana Champaign;The Chinese University of Hong Kong;University of Illinois, Urbana Champaign",3;59;3,48;35;48,1
5280,5280,5280,5280,5280,5280,5280,5280,ICLR,2020,Representational Disentanglement for Multi-Domain Image Completion,Liyue Shen;Wentao Zhu;Xiaosong Wang;Lei Xing;John Pauly;Baris Turkbey;Stephanie Harmon;Thomas Sanford;Sherif Mehralivand;Peter L. Choyke;Bradford J. Wood;Daguang Xu,liyues@stanford.edu;wentaoz@nvidia.com;xiaosongw@nvidia.com;lei@stanford.edu;pauly@stanford.edu;ismail.turkbey@nih.gov;stephanie.harmon@nih.gov;thomas.sanford@nih.gov;sherif.mehralivand@nih.gov;pchoyke@mail.nih.gov;bwood@nih.gov;daguangx@nvidia.com,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,Stanford University;NVIDIA;NVIDIA;Stanford University;Stanford University;National Institutes of Health;National Institutes of Health;National Institutes of Health;National Institutes of Health;National Institutes of Health;National Institutes of Health;NVIDIA,4;-1;-1;4;4;-1;-1;-1;-1;-1;-1;-1,4;-1;-1;4;4;-1;-1;-1;-1;-1;-1;-1,5;4;2
5281,5281,5281,5281,5281,5281,5281,5281,ICLR,2020,SCL: Towards Accurate Domain Adaptive Object Detection via Gradient Detach Based Stacked Complementary Losses,Zhiqiang Shen;Harsh Maheshwari;Weichen Yao;Marios Savvides,zhiqians@andrew.cmu.edu;harshmaheshwari135@gmail.com;wyao2@andrew.cmu.edu;marioss@andrew.cmu.edu,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,1,5,,yes,9/25/19,Carnegie Mellon University;Indian Institute of Technology Kharagpur;Carnegie Mellon University;Carnegie Mellon University,1;266;1;1,27;476;27;27,2
5282,5282,5282,5282,5282,5282,5282,5282,ICLR,2020,Relevant-features based Auxiliary Cells for Robust and Energy Efficient Deep Learning,Aparna Aketi;Priyadarshini Panda;Kaushik Roy,saketi@purdue.edu;priya.panda@yale.edu;kaushik@purdue.edu,1;3;6,I have published in this field for several years.:I carefully checked the experiments.:I made a quick assessment of this paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,5,,yes,9/25/19,Purdue University;Yale University;Purdue University,27;64;27,88;8;88,
5283,5283,5283,5283,5283,5283,5283,5283,ICLR,2020,Learning to Transfer Learn,Linchao Zhu;Sercan O. Arik;Yi Yang;Tomas Pfister,zhulinchao7@gmail.com;soarik@google.com;yi.yang@uts.edu.au;tpfister@google.com,3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,4,,yes,9/25/19,University of Technology Sydney;Google;University of Technology Sydney;Google,108;-1;108;-1,193;-1;193;-1,6
5284,5284,5284,5284,5284,5284,5284,5284,ICLR,2020,Variational lower bounds on mutual information based on nonextensive statistical mechanics,Valeriu Balaban;Yang Zikun;Paul Bogdan,vbalaban@usc.edu;yangzikun@buaa.edu.cn;pbogdan@usc.edu,3;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,University of Southern California;Beihang University;University of Southern California,31;118;31,62;594;62,1;8
5285,5285,5285,5285,5285,5285,5285,5285,ICLR,2020,Instance adaptive adversarial training: Improved accuracy tradeoffs in neural nets,Yogesh Balaji;Tom Goldstein;Judy Hoffman,yogesh@cs.umd.edu;tomg@cs.umd.edu;judy@gatech.edu,3;6;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,5,,yes,9/25/19,"University of Maryland, College Park;University of Maryland, College Park;Georgia Institute of Technology",12;12;13,91;91;38,4;8
5286,5286,5286,5286,5286,5286,5286,5286,ICLR,2020,Robust Few-Shot Learning with Adversarially Queried Meta-Learners,Micah Goldblum;Liam Fowl;Tom Goldstein,goldblumcello@gmail.com;lhfowl@gmail.com;tomg@cs.umd.edu,3;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,4,,yes,9/25/19,"University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park",12;12;12,91;91;91,4;6
5287,5287,5287,5287,5287,5287,5287,5287,ICLR,2020,Mixture Density Networks Find Viewpoint the Dominant Factor for Accurate Spatial Offset Regression,Ali Varamesh;Tinne Tuytelaars,ali.varamesh@kuleuven.be;tinne.tuytelaars@esat.kuleuven.be,3;1;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,KU Leuven;KU Leuven,118;118,45;45,2
5288,5288,5288,5288,5288,5288,5288,5288,ICLR,2020,Learning Adversarial Grammars for Future Prediction,AJ Piergiovanni;Alexander Toshev;Anelia Angelova;Michael Ryoo,ajpiergi@indiana.edu;toshev@google.com;anelia@google.com;mryoo@google.com,1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,2,,yes,9/25/19,University of Arizona;Google;Google;Google,172;-1;-1;-1,103;-1;-1;-1,4
5289,5289,5289,5289,5289,5289,5289,5289,ICLR,2020,Dual-Component Deep Domain Adaptation: A New Approach for Cross Project Software Vulnerability Detection,Van Nguyen;Trung Le;Olivier de Vel;Paul Montague;John C Grundy;Dinh Phung,van.nk@monash.edu;trunglm@monash.edu;olivier.devel@dst.defence.gov.au;paul.montague@dst.defence.gov.au;john.grundy@monash.edu;dinh.phung@monash.edu,1;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Monash University;Monash University;;;Monash University;Monash University,118;118;-1;-1;118;118,75;75;-1;-1;75;75,5;4;6
5290,5290,5290,5290,5290,5290,5290,5290,ICLR,2020,FAN: Focused Attention Networks,Chu Wang;Babak Samari;Vladimir Kim;Siddhartha Chaudhuri;Kaleem Siddiqi,chuwang@cim.mcgill.ca;babak@cim.mcgill.ca;vokim@adobe.com;sidch@adobe.com;siddiqi@cim.mcgill.ca,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,8,,yes,9/25/19,McGill University;McGill University;Adobe Systems;Adobe Systems;McGill University,86;86;-1;-1;86,42;42;-1;-1;42,
5291,5291,5291,5291,5291,5291,5291,5291,ICLR,2020,State2vec: Off-Policy Successor Feature Approximators,Sephora Madjiheurem;Laura Toni,sephora.madjiheurem.17@ucl.ac.uk;l.toni@ucl.ac.uk,1;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:N/A:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,University College London;University College London,50;50,15;15,10
5292,5292,5292,5292,5292,5292,5292,5292,ICLR,2020,Semi-supervised Autoencoding Projective Dependency Parsing,Xiao Zhang;Dan Goldwasser,zhang923@purdue.edu;dgoldwas@purdue.edu,1;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Purdue University;Purdue University,27;27,88;88,5;10
5293,5293,5293,5293,5293,5293,5293,5293,ICLR,2020,Progressive Knowledge Distillation For Generative Modeling,Yu-Xiong Wang;Adrien Bardes;Ruslan Salakhutdinov;Martial Hebert,yuxiongw@cs.cmu.edu;adrien.bardes@dbmail.com;rsalakhu@cs.cmu.edu;hebert@ri.cmu.edu,3;3;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,0,,yes,9/25/19,Carnegie Mellon University;Ecole Normale Superieure;Carnegie Mellon University;Carnegie Mellon University,1;100;1;1,27;45;27;27,5;6
5294,5294,5294,5294,5294,5294,5294,5294,ICLR,2020,Lyceum: An efficient and scalable ecosystem for robot learning,Colin X. Summers;Kendall Lowrey;Aravind Rajeswaran;Emanuel Todorov;Siddhartha Srinivasa,colinxs@cs.washington.edu;klowrey@cs.washington.edu;todorov@cs.washington.edu;siddh@cs.washington.edu;aravraj@cs.washington.edu,1;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,1,,yes,9/25/19,University of Washington;University of Washington;University of Washington;University of Washington;University of Washington,6;6;6;6;6,26;26;26;26;26,
5295,5295,5295,5295,5295,5295,5295,5295,ICLR,2020,Adversarial Attribute Learning by  Exploiting negative correlated attributes,Satoshi Tsutsui;Yanwei Fu;David Crandall,stsutsui@indiana.edu;yanweifu@fudan.edu.cn;djcran@indiana.edu,1;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,University of Arizona;Fudan University;University of Arizona,172;79;172,103;109;103,4
5296,5296,5296,5296,5296,5296,5296,5296,ICLR,2020,VISUALIZING POINT CLOUD CLASSIFIERS BY MORPHING POINT CLOUDS INTO POTATOES,Ziwen Chen;Wenxuan Wu;Zhongang Qi;Fuxin Li,chenziwe@grinnell.edu;wuwen@oregonstate.edu;qiz@oregonstate.edu;lif@oregonstate.edu,3;3;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,4,,yes,9/25/19,Grinnell College;Oregon State University;Oregon State University;Oregon State University,-1;77;77;77,-1;373;373;373,
5297,5297,5297,5297,5297,5297,5297,5297,ICLR,2020,Learning Multi-Agent Communication Through Structured Attentive Reasoning,Murtaza Rangwala;Ryan Williams,murtazar@vt.edu;rywilli1@vt.edu,3;1;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,Virginia Tech;Virginia Tech,79;79,240;240,
5298,5298,5298,5298,5298,5298,5298,5298,ICLR,2020,The Power of  Semantic Similarity based Soft-Labeling for Generalized Zero-Shot Learning,Shabnam Daghaghi;Tharun Medini;Anshumali Shrivastava,shabnam.daghaghi@rice.edu;tharun.medini@rice.edu;anshumali@rice.edu,6;1;3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,5,,yes,9/25/19,Rice University;Rice University;Rice University,84;84;84,105;105;105,6
5299,5299,5299,5299,5299,5299,5299,5299,ICLR,2020,Spline Templated Based Handwriting Generation,Daniel Clothiaux;Ravi Starzl,dclothia@andrew.cmu.edu;rstarzl@cs.cmu.edu,6;3;1,I do not know much about this area.:I carefully checked the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University,1;1,27;27,
5300,5300,5300,5300,5300,5300,5300,5300,ICLR,2020,Improving the robustness of ImageNet classifiers using elements of human visual cognition,Emin Orhan;Brenden Lake,aeminorhan@gmail.com;brenden@nyu.edu,3;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,New York University;New York University,25;25,29;29,4
5301,5301,5301,5301,5301,5301,5301,5301,ICLR,2020,Going Deeper with Lean Point Networks,Eric-Tuan Le;Iasonas Kokkinos;Niloy J. Mitra,eric-tuan.le.18@ucl.ac.uk;i.kokkinos@cs.ucl.ac.uk;n.mitra@cs.ucl.ac.uk,1;6;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,University College London;University College London;University College London,50;50;50,15;15;15,2
5302,5302,5302,5302,5302,5302,5302,5302,ICLR,2020,Newton Residual Learning,Grigorios Chrysos;Jiankang Deng;Yannis Panagakis;Stefanos Zafeiriou,g.chrysos@imperial.ac.uk;j.deng16@imperial.ac.uk;i.panagakis@imperial.ac.uk;s.zafeiriou@imperial.ac.uk,8;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,1,1,,yes,9/25/19,Imperial College London;Imperial College London;Imperial College London;Imperial College London,73;73;73;73,10;10;10;10,2
5303,5303,5303,5303,5303,5303,5303,5303,ICLR,2020,Training-Free Uncertainty Estimation for Neural Networks,Lu Mi;Hao Wang;Yonglong Tian;Nir Shavit,lumi@mit.edu;hoguewang@gmail.com;yonglong@mit.edu;shanir@csail.mit.edu,1;6;6;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,5,,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2,5;5;5;5,11;2
5304,5304,5304,5304,5304,5304,5304,5304,ICLR,2020,AMUSED: A Multi-Stream Vector Representation Method for Use In Natural Dialogue,Gaurav Kumar;Rishabh Joshi;Jaspreet Singh;Promod Yenigalla,gaurav.k1@samsung.com;rjoshi2@andrew.cmu.edu;jaspreet.ahluwalia@stonybrook.edu;promod.y@samsung.com,6;1;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A,Withdrawn,0,3,,yes,9/25/19,"Samsung;Carnegie Mellon University;State University of New York, Stony Brook;Samsung",-1;1;41;-1,-1;27;304;-1,6;10
5305,5305,5305,5305,5305,5305,5305,5305,ICLR,2020,PAD-Nets: Learning Dynamic Receptive Fields via Pixel-Wise Adaptive Dilation,Dongdong Wang;Hao Hu;Jie Yao;Zihang Zou;Liqiang Wang,daniel.wang@knights.ucf.edu;hhu@fxpal.com;17112098@bjtu.edu.cn;zzz@knights.ucf.edu;lwang@cs.ucf.edu,1;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,"University of Central Florida;FX Palo Alto Laboratory, Inc.;Beijing jiaotong univercity;University of Central Florida;University of Central Florida",77;-1;481;77;77,609;-1;952;609;609,
5306,5306,5306,5306,5306,5306,5306,5306,ICLR,2020,Posterior Sampling: Make Reinforcement Learning Sample Efficient Again,Calvin Seward;Urs Bergmann;Roland Vollgraf;Sepp Hochreiter,seward@bioinf.jku.at;urs.bergmann@zalando.de;roland.vollgraf@zalando.de;hochreit@bioinf.jku.at,3;3;6;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,4,,yes,9/25/19,Johannes Kepler University Linz;Zalando Research;Zalando Research;Johannes Kepler University Linz,481;-1;-1;481,620;-1;-1;620,11
5307,5307,5307,5307,5307,5307,5307,5307,ICLR,2020,Unifying Part Detection And Association For Multi-person Pose Estimation,Rania Briq;Andreas Doering;Juergen Gall,briq@iai.uni-bonn.de;doering@iai.uni-bonn.de;gall@iai.uni-bonn.de,3;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,0,,yes,9/25/19,University of Bonn;University of Bonn;University of Bonn,128;128;128,106;106;106,2
5308,5308,5308,5308,5308,5308,5308,5308,ICLR,2020,Slow Thinking Enables Task-Uncertain Lifelong and Sequential Few-Shot Learning,Rosalie Dolor;Hsin-Chi Chu;Shan-Hung Wu,rosalie@ghtinc.com;hcchu@datalab.cs.nthu.edu.tw;shwu@cs.nthu.edu.tw,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Ghtinc;National Tsing Hua University;National Tsing Hua University,-1;172;172,-1;365;365,6
5309,5309,5309,5309,5309,5309,5309,5309,ICLR,2020,How Aggressive Can Adversarial Attacks Be: Learning Ordered Top-k Attacks,Zekun Zhang;Tianfu Wu,zzhang56@ncsu.edu;tianfu_wu@ncsu.edu,3;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,North Carolina State University;North Carolina State University,86;86,310;310,4
5310,5310,5310,5310,5310,5310,5310,5310,ICLR,2020,PolyGAN: High-Order Polynomial Generators,Grigorios Chrysos;Stylianos Moschoglou;Yannis Panagakis;Stefanos Zafeiriou,g.chrysos@imperial.ac.uk;s.moschoglou@imperial.ac.uk;i.panagakis@imperial.ac.uk;s.zafeiriou@imperial.ac.uk,1;3;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,Imperial College London;Imperial College London;Imperial College London;Imperial College London,73;73;73;73,10;10;10;10,5;4
5311,5311,5311,5311,5311,5311,5311,5311,ICLR,2020,Generative Multi Source Domain Adaptation,Subhankar Roy;Aliaksandr Siarohin;Enver Sangineto;Moin Nabi;Tassilo Klein;Nicu Sebe;Elisa Ricci,subhankar.roy@unitn.it;aliaksandr.siarohin@unitn.it;enver.sangineto@unitn.it;m.nabi@sap.com;tassilo.klein@sap.com;niculae.sebe@unitn.it;eliricci@fbk.eu,3;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,5,,yes,9/25/19,University of Trento;University of Trento;University of Trento;SAP;SAP;University of Trento;Fondazione Bruno Kessler,18;18;18;323;323;18;-1,307;307;307;258;258;307;-1,5;4
5312,5312,5312,5312,5312,5312,5312,5312,ICLR,2020,Semi-Supervised Named Entity Recognition with CRF-VAEs,Thomas Effland;Michael Collins,teffland@cs.columbia.edu;mcollins@cs.columbia.edu;mc3354@columbia.edu,3;3;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,1,3,,yes,9/25/19,Columbia University;Columbia University;Columbia University,15;15;15,16;16;16,5
5313,5313,5313,5313,5313,5313,5313,5313,ICLR,2020,Understanding the (Un)interpretability of Natural Image Distributions Using Generative Models,Ryen Krusinga;Sohil Shah;Matthias Zwicker;Tom Goldstein;David Jacobs,krusinga@cs.umd.edu;sohilas@umd.edu;zwicker@inf.unibe.ch;tom@cs.umd.edu;djacobs@umiacs.umd.edu,1;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,1,0,,yes,9/25/19,"University of Maryland, College Park;University of Maryland, College Park;University of Bern;University of Maryland, College Park;University of Maryland, College Park",12;12;390;12;12,91;91;113;91;91,5
5314,5314,5314,5314,5314,5314,5314,5314,ICLR,2020,Depth-Recurrent Residual Connections for Super-Resolution of Real-Time Renderings,Erik Franz;Mengyu Chu;Rüdiger Westermann;Nils Thuerey,franzer@in.tum.de;mengyu.chu@tum.de;westermann@tum.de;nils.thuerey@tum.de,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Technical University Munich;Technical University Munich;Technical University Munich;Technical University Munich,53;53;53;53,43;43;43;43,5;4
5315,5315,5315,5315,5315,5315,5315,5315,ICLR,2020,The Blessing of Dimensionality: An Empirical Study of Generalization,W. Ronny Huang;Zeyad Emam;Micah Goldblum;Liam Fowl;Justin K. Terry;Furong Huang;Tom Goldstein,wrhuang@umd.edu;zeyad@math.umd.edu;goldblum@math.umd.edu;lfowl@math.umd.edu;jkterry@cs.umd.edu;furongh@cs.umd.edu;tomg@cs.umd.edu,3;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,"University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park",12;12;12;12;12;12;12,91;91;91;91;91;91;91,8
5316,5316,5316,5316,5316,5316,5316,5316,ICLR,2020,Strong Baseline Defenses Against Clean-Label Poisoning Attacks,Neal Gupta;W. Ronny Huang;Liam Fowl;Chen Zhu;Soheil Feizi;Tom Goldstein;John Dickerson,ngupta@cs.umd.edu;wronnyhuang@gmail.com;lhfowl@gmail.com;chenzhu@cs.umd.edu;sfeizi@cs.umd.edu;tomg@cs.umd.edu;john@cs.umd.edu,3;3;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,"University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park",12;12;12;12;12;12;12,91;91;91;91;91;91;91,4
5317,5317,5317,5317,5317,5317,5317,5317,ICLR,2020,MetaPoison:   Learning to craft adversarial poisoning examples via meta-learning,W. Ronny Huang;Jonas Geiping;Liam Fowl;Gavin Taylor;Tom Goldstein,wronnyhuang@gmail.com;jonas.geiping@uni-siegen.de;lfowl@math.umd.edu;taylor@usna.edu;tomg@cs.umd.edu,3;3;1,I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,"University of Maryland, College Park;University of Siegen;University of Maryland, College Park;University of Arizona;University of Maryland, College Park",12;323;12;172;12,91;570;91;103;91,4;6
5318,5318,5318,5318,5318,5318,5318,5318,ICLR,2020,Domain-Relevant Embeddings for Question Similarity,Clara McCreery;Namit Katariya;Anitha Kannan;Manish Chablani;Xavier Amatriain,mccreery@stanford.edu;namit@curai.com;anitha@curai.com;manish@curai.com;xavier@curai.com,3;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,0,,yes,9/25/19,Stanford University;Curai;Curai;Curai;Curai,4;233;233;233;233,4;-1;-1;-1;-1,
5319,5319,5319,5319,5319,5319,5319,5319,ICLR,2020,Divide-and-Conquer Adversarial Learning for High-Resolution Image Enhancement,Zhiwu Huang;Danda Pani Paudel;Guanju Li;Jiqing Wu;Radu Timofte;Luc Van Gool,zhiwu.huang@vision.ee.ethz.ch;paudel@vision.ee.ethz.ch;ligua@student.ethz.ch;jwu@vision.ee.ethz.ch;radu.timofte@vision.ee.ethz.ch;vangool@vision.ee.ethz.ch,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:N/A:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10;10;10;10,13;13;13;13;13;13,5;4
5320,5320,5320,5320,5320,5320,5320,5320,ICLR,2020,Learning Out-of-distribution Detection without Out-of-distribution Data,Yen-Chang Hsu;Yilin Shen;Hongxia Jin;Zsolt Kira,yenchang.hsu@gatech.edu;yilin.shen@samsung.com;hongxia.jin@samsung.com;zkira@gatech.edu,3;3;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,3,1,,yes,9/25/19,Georgia Institute of Technology;Samsung;Samsung;Georgia Institute of Technology,13;-1;-1;13,38;-1;-1;38,
5321,5321,5321,5321,5321,5321,5321,5321,ICLR,2020,Domain Adaptation Through Label Propagation: Learning Clustered and Aligned Features,Changhwa Park;Jaeyoon Yoo;Youngjun Hong;Sungroh Yoon,omega6464@snu.ac.kr;yjy765@snu.ac.kr;youngjun.hong@enerzai.com;sryoon@snu.ac.kr,3;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,1,10,,yes,9/25/19,Seoul National University;Seoul National University;Enerzai;Seoul National University,41;41;-1;41,64;64;-1;64,1
5322,5322,5322,5322,5322,5322,5322,5322,ICLR,2020,ManiGAN: Text-Guided Image Manipulation,Bowen Li;Xiaojuan Qi;Thomas Lukasiewicz;Philip H. S. Torr,bowen.li@cs.ox.ac.uk;xiaojuan.qi@eng.ox.ac.uk;thomas.lukasiewicz@cs.ox.ac.uk;philip.torr@eng.ox.ac.uk,1;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,University of Oxford;University of Oxford;University of Oxford;University of Oxford,50;50;50;50,1;1;1;1,3;4;5
5323,5323,5323,5323,5323,5323,5323,5323,ICLR,2020,Empirical observations pertaining to learned priors for deep latent variable models,Rogan Morrow;Wei-Chen Chiu,rogan.o.morrow@gmail.com;walon@cs.nctu.edu.tw,3;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,4,,yes,9/25/19,National Chiao Tung University;National Chiao Tung University,143;143,564;564,5;4;2
5324,5324,5324,5324,5324,5324,5324,5324,ICLR,2020,Variational inference of latent hierarchical dynamical systems in neuroscience: an application to calcium imaging data,Luke Y. Prince;Blake A. Richards,luke.prince@utoronto.ca;blake.richards@mcgill.ca,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Toronto University;McGill University,18;86,18;42,5
5325,5325,5325,5325,5325,5325,5325,5325,ICLR,2020,Characterizing convolutional neural networks with one-pixel signature,Shanjiaoyang Huang;Weiqi Peng;Zhuowen Tu,shh236@ucsd.edu;wep012@ucsd.edu;ztu@ucsd.edu,3;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,"University of California, San Diego;University of California, San Diego;University of California, San Diego",11;11;11,31;31;31,4
5326,5326,5326,5326,5326,5326,5326,5326,ICLR,2020,Increasing batch size through instance repetition improves generalization,Elad Hoffer;Tal Ben-Nun;Itay Hubara;Niv Giladi;Torsten Hoefler;Daniel Soudry,elad.hoffer@gmail.com;talbn@inf.ethz.ch;itayhubara@gmail.com;giladiniv@campus.technion.ac.il;htor@inf.ethz.ch;daniel.soudry@gmail.com,3;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,2,3,,yes,9/25/19,Technion;Swiss Federal Institute of Technology;;Technion;Swiss Federal Institute of Technology;Technion,26;10;-1;26;10;26,412;13;-1;412;13;412,8
5327,5327,5327,5327,5327,5327,5327,5327,ICLR,2020,"Mix & Match: training convnets with mixed image sizes for improved accuracy, speed and scale resiliency",Elad Hoffer;Berry Weinstein;Itay Hubara;Tal Ben-Nun;Torsten Hoefler;Daniel Soudry,elad.hoffer@gmail.com;bweinstein@habana.ai;itayhubara@gmail.com;talbn@inf.ethz.ch;htor@inf.ethz.ch;daniel.soudry@gmail.com,6;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,3,,yes,9/25/19,Technion;interdisciplinary center herzliya;;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Technion,26;-1;-1;10;10;26,412;-1;-1;13;13;412,
5328,5328,5328,5328,5328,5328,5328,5328,ICLR,2020,Revisit Knowledge Distillation: a Teacher-free Framework,Li Yuan;Francis EH Tay;Guilin Li;Tao Wang;Jiashi Feng,ylustcnus@gmail.com;mpetayeh@nus.edu.sg;liguilin2@huawei.com;twangnh@gmail.com;elefjia@nus.edu.sg,6;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,5,,yes,9/25/19,National University of Singapore;National University of Singapore;Huawei Technologies Ltd.;National University of Singapore;National University of Singapore,16;16;-1;16;16,25;25;-1;25;25,1
5329,5329,5329,5329,5329,5329,5329,5329,ICLR,2020,On learning visual odometry errors,Andrea De Maio;Simon Lacroix,andrea.de-maio@laas.fr;simon.lacroix@laas.fr,1;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,4,,yes,9/25/19,LAAS / CNRS;LAAS / CNRS,-1;-1,-1;-1,
5330,5330,5330,5330,5330,5330,5330,5330,ICLR,2020,Mitigating Posterior Collapse in Strongly Conditioned Variational Autoencoders,Mohammad Sadegh Aliakbarian;Fatemeh Sadat Saleh;Mathieu Salzmann;Lars Petersson;Stephen Gould,sadegh.aliakbarian@anu.edu.au;fatemehsadat.saleh@anu.edu.au;mathieu.salzmann@epfl.ch;lars.petersson@data61.csiro.au;stephen.gould@anu.edu.au,3;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,4,,yes,9/25/19,"Australian National University;Australian National University;Swiss Federal Institute of Technology Lausanne;, CSIRO;Australian National University",108;108;481;233;108,50;50;38;-1;50,5
5331,5331,5331,5331,5331,5331,5331,5331,ICLR,2020,Unsupervised  Video-to-Video Translation via Self-Supervised Learning,Kangning Liu;Shuhang Gu;Radu Timofte,kl3141@nyu.edu;shuhang.gu@vision.ee.ethz.ch;radu.timofte@vision.ee.ethz.ch,3;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,4,,yes,9/25/19,New York University;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,25;10;10,29;13;13,
5332,5332,5332,5332,5332,5332,5332,5332,ICLR,2020,ROBUST SINGLE-STEP ADVERSARIAL TRAINING,B.S. Vivek;R. Venkatesh Babu,svivek@iisc.ac.in;venky@iisc.ac.in,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,3,,yes,9/25/19,Indian Institute of Science;Indian Institute of Science,95;95,301;301,4;2
5333,5333,5333,5333,5333,5333,5333,5333,ICLR,2020,Attentive Weights Generation for Few Shot Learning via Information Maximization,Yiluan Guo;Ngai-Man Cheung,guoyl1990@outlook.com;ngaiman_cheung@sutd.edu.sg,1;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,5,,yes,9/25/19,Singapore University of Technology and Design;Singapore University of Technology and Design,481;481,1397;1397,1;6
5334,5334,5334,5334,5334,5334,5334,5334,ICLR,2020,DropGrad: Gradient Dropout Regularization for Meta-Learning,Hung-Yu Tseng;Yi-Wen Chen;Yi-Hsuan Tsai;Sifei Liu;Yen-Yu Lin;Ming-Hsuan Yang,htseng6@ucmerced.edu;ychen319@ucmerced.edu;wasidennis@gmail.com;sifeil@nvidia.com;lin@cs.nctu.edu.tw;mhyang@ucmerced.edu,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,0,,yes,9/25/19,University of California at Merced;University of California at Merced;NEC-Labs;NVIDIA;National Chiao Tung University;University of California at Merced,481;481;-1;-1;143;481,354;354;-1;-1;564;354,6;8
5335,5335,5335,5335,5335,5335,5335,5335,ICLR,2020,Hierarchical Image-to-image Translation with Nested Distributions Modeling,Shishi Qiao;Ruiping Wang;Shiguang Shan;Xilin Chen,qiaoshishi14@mails.ucas.ac.cn;wangruiping@ict.ac.cn;sgshan@ict.ac.cn;xlchen@ict.ac.cn,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,"Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences",59;59;59;59,1397;1397;1397;1397,
5336,5336,5336,5336,5336,5336,5336,5336,ICLR,2020,Deep Neural Forests: An Architecture for Tabular Data,Ami Abutbul;Gal Elidan;Liran Katzir;Ran El-Yaniv,amramabutbul@cs.technion.ac.il;elidan@google.com;lirank@google.com;elyaniv@google.com,3;3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,4,,yes,9/25/19,Technion;Google;Google;Google,26;-1;-1;-1,412;-1;-1;-1,
5337,5337,5337,5337,5337,5337,5337,5337,ICLR,2020,Is my Deep Learning Model Learning more than I want it to?,Naveen Panwar;Tarun Tater;Anush Sankaran;Senthil Mani,naveen.panwar@in.ibm.com;anussank@in.ibm.com;taruntater3@gmail.com;sentmani@in.ibm.com,3;1;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,International Business Machines;International Business Machines;International Business Machines;International Business Machines,-1;-1;-1;-1,-1;-1;-1;-1,8
5338,5338,5338,5338,5338,5338,5338,5338,ICLR,2020,Mixing Up Real Samples and Adversarial Samples for Semi-Supervised Learning,Yun Ma;Xudong Mao;Yangbin Chen;Qing Li,mayun371@gmail.com;xudong.xdmao@gmail.com;robinchen2-c@my.cityu.edu.hk;qing-prof.li@polyu.edu.hk,1;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:N/A:N/A,Withdrawn,0,1,,yes,9/25/19,The Hong Kong Polytechnic University;The Hong Kong Polytechnic University;City University of Hong Kong;The Hong Kong Polytechnic University,172;172;92;172,171;171;35;171,4
5339,5339,5339,5339,5339,5339,5339,5339,ICLR,2020,ADASAMPLE: ADAPTIVE SAMPLING OF HARD POSITIVES FOR DESCRIPTOR LEARNING,Xin-Yu Zhang;Jia-Wang Bian;Le Zhang;Zao-Yi Zheng;Yun Liu;Ming-Ming Cheng;Ian Reid,xinyuzhang@mail.nankai.edu.cn;jiawang.bian@gmail.com;zhangleuestc@gmail.com;roymarssss@gmail.com;nk12csly@mail.nankai.edu.cn;cmm@nankai.edu.cn;ian.reid@adelaide.edu.au,3;6;3,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,5,,yes,9/25/19,Nankai University;The University of Adelaide;A*STAR;;Nankai University;Nankai University;The University of Adelaide,481;128;-1;-1;481;481;128,366;120;-1;-1;366;366;120,
5340,5340,5340,5340,5340,5340,5340,5340,ICLR,2020,Better Optimization for Neural Architecture Search with Mixed-Level Reformulation,Chaoyang He;Haishan Ye;Tong Zhang,chaoyang.he@usc.edu;yhs12354123@163.com;tongzhang@tongzhang-ml.org,3;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,University of Southern California;163;The Hong Kong University of Science and Technology,31;-1;39,62;-1;47,
5341,5341,5341,5341,5341,5341,5341,5341,ICLR,2020,Amharic Light Stemmer,Girma Neshir;Andeas Rauber;and Solomon Atnafu,girma1978@gmail.com;rauber@ifs.tuwien.ac.at;solomon.atnafu@aau.edu.et,1;1;1,I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Withdrawn,0,0,,yes,9/25/19,Addis Ababa University;TU Wien Vienna University of Technology;Addis Ababa University,481;100;481,1397;360;1397,
5342,5342,5342,5342,5342,5342,5342,5342,ICLR,2020,Neural Reverse Engineering of Stripped Binaries,Yaniv David;Uri Alon;Eran Yahav,yanivd@cs.technion.ac.il;urialon@cs.technion.ac.il;yahave@cs.technion.ac.il,6;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:N/A:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,7,,yes,9/25/19,Technion;Technion;Technion,26;26;26,412;412;412,10
5343,5343,5343,5343,5343,5343,5343,5343,ICLR,2020,Cost-Effective Interactive Neural Attention Learning,Jay Heo;Junhyeon Park;Hyewon Jeong;Wuhyun Shin;Kwang Joon Kim;juho Lee;Eunho Yang;Sung Ju Hwang,jayheo@kaist.ac.kr;pjh2941@kaist.ac.kr;jhw162@kaist.ac.kr;wuhyun.shin@kaist.ac.kr;preppie@yuhs.ac.kr;juho@aitrics.com;eunhoy@kaist.ac.kr;sjhwang82@kaist.ac.kr,3;3;3;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,4,,yes,9/25/19,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Kyung Hee;AITRICS;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,481;481;481;481;481;-1;481;481,110;110;110;110;319;-1;110;110,
5344,5344,5344,5344,5344,5344,5344,5344,ICLR,2020,Gated Channel Transformation for Visual Recognition,Zongxin Yang;Linchao Zhu;Yu Wu;Yi Yang,zongxin.yang@student.uts.edu.au;zhulinchao7@gmail.com;yu.wu-3@student.uts.edu.au;yi.yang@uts.edu.au,3;1;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,University of Technology Sydney;University of Technology Sydney;University of Technology Sydney;University of Technology Sydney,108;108;108;108,193;193;193;193,2
5345,5345,5345,5345,5345,5345,5345,5345,ICLR,2020,Masked Translation Model,Arne Nix;Yunsu Kim;Jan Rosendahl;Shahram Khadivi;Hermann Ney,nix@i6.informatik.rwth-aachen.de;kim@i6.informatik.rwth-aachen.de;rosendahl@i6.informatik.rwth-aachen.de;skhadivi@ebay.com;ney@i6.informatik.rwth-aachen.de,3;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,1,1,,yes,9/25/19,RWTH Aachen University;RWTH Aachen University;RWTH Aachen University;eBay;RWTH Aachen University,95;95;95;-1;95,98;98;98;-1;98,3
5346,5346,5346,5346,5346,5346,5346,5346,ICLR,2020,Target-directed Atomic Importance Estimation via Reverse Self-attention,Gyoung S. Na;Hyun Woo Kim,ngs0726@gmail.com;ahwk@krict.re.kr,1;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,POSTECH;,118;-1,146;-1,10
5347,5347,5347,5347,5347,5347,5347,5347,ICLR,2020,ProxNet: End-to-End Learning of  Structured Representation by Proximal Mapping,Mao Li;Yingyi Ma;Xinhua Zhang,mli206@uic.edu;yma36@uic.edu;zhangx@uic.edu,3;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,2,,yes,9/25/19,"University of Illinois, Chicago;University of Illinois, Chicago;University of Illinois, Chicago",56;56;56,254;254;254,4
5348,5348,5348,5348,5348,5348,5348,5348,ICLR,2020,Measure by Measure: Automatic Music Composition with Traditional Western Music Notation,Yujia Yan;Zhiyao Duan,yujia.yan.w@gmail.com;zhiyao.duan@rochester.edu,1;1;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,1,,yes,9/25/19,University of Rochester;University of Rochester,100;100,173;173,
5349,5349,5349,5349,5349,5349,5349,5349,ICLR,2020,When Do Variational Autoencoders Know  What They Don't Know?,Bin Dai;David Wipf,daib13@mails.tsinghua.edu.cn;davidwipf@gmail.com,8;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,Tsinghua University;Microsoft,8;-1,23;-1,5
5350,5350,5350,5350,5350,5350,5350,5350,ICLR,2020,Irrationality can help reward inference,Lawrence Chan;Andrew Critch;Anca Dragan,chanlaw@berkeley.edu;critch@berkeley.edu;anca@berkeley.edu,3;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,13;13;13,
5351,5351,5351,5351,5351,5351,5351,5351,ICLR,2020,Teaching GAN to generate per-pixel annotation,Danil Galeev;Konstantin Sofiyuk;Danila Rukhovich;Anton Konushin;Mikhail Romanov,denemmy@gmail.com;ksofiyuk@gmail.com;danrukh@gmail.com;a.konushin@samsung.com;m.romanov@samsung.com,3;1;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Samsung;Samsung;Lomonosov Moscow State University;Samsung;Samsung,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,5;2
5352,5352,5352,5352,5352,5352,5352,5352,ICLR,2020,One Generation Knowledge Distillation by Utilizing Peer Samples,Xingjian Li;Haozhe An;Haoyi Xiong;Jun Huan;Dejing Dou;Chengzhong Xu,1762778193@qq.com;haozhe.an@yahoo.com,1;3;3,I have published one or two papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,;Baidu,-1;-1,-1;-1,
5353,5353,5353,5353,5353,5353,5353,5353,ICLR,2020,Guided variational autoencoder for disentanglement learning,Zheng Ding;Yifan Xu;Weijian Xu;Yang Yang;Max Welling;Zhuowen Tu,dingz16@mails.tsinghua.edu.cn;yix081@ucsd.edu;wex041@eng.ucsd.edu;yyangy@qti.qualcomm.com;welling.max@gmail.com;ztu@ucsd.edu,1;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,"Tsinghua University;University of California, San Diego;University of California, San Diego;Qualcomm Inc, QualComm;University of California - Irvine;University of California, San Diego",8;11;11;-1;35;11,23;31;31;-1;96;31,5;4
5354,5354,5354,5354,5354,5354,5354,5354,ICLR,2020,Learning Sparsity and Quantization Jointly and Automatically for Neural Network Compression via Constrained Optimization,Haichuan Yang;Shupeng Gui;Yuhao Zhu;Ji Liu,h.yang@rochester.edu;sgui2@ur.rochester.edu;yzhu@rochester.edu;ji.liu.uwisc@gmail.com,3;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,University of Rochester;University of Rochester;University of Rochester;University of Rochester,100;100;100;100,173;173;173;173,
5355,5355,5355,5355,5355,5355,5355,5355,ICLR,2020,FairFace: A Novel Face Attribute Dataset for Bias Measurement and Mitigation,Kimmo Kärkkäinen;Jungseock Joo,kimmo@cs.ucla.edu;jjoo@comm.ucla.edu,3;1;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,3,,yes,9/25/19,"University of California, Los Angeles;University of California, Los Angeles",20;20,17;17,7;2;8
5356,5356,5356,5356,5356,5356,5356,5356,ICLR,2020,Pruning Depthwise Separable Convolutions for Extra Efficiency Gain of Lightweight Models,Cheng-Hao Tu;Jia-Hong Lee;Yi-Ming Chan;Chu-Song Chen,andytu28@iis.sinica.edu.tw;honghenry.lee@iis.sinica.edu.tw;yiming@iis.sinica.edu.tw;song@iis.sinica.edu.tw,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,1,0,,yes,9/25/19,Academia Sinica;Academia Sinica;Academia Sinica;Academia Sinica,-1;-1;-1;-1,-1;-1;-1;-1,
5357,5357,5357,5357,5357,5357,5357,5357,ICLR,2020,Benchmarking Adversarial Robustness,Yinpeng Dong;Qi-An Fu;Xiao Yang;Tianyu Pang;Hang Su;Jun Zhu,dyp17@mails.tsinghua.edu.cn;fqa19@mails.tsinghua.edu.cn;yangxiao19@mails.tsinghua.edu.cn;pty17@mails.tsinghua.edu.cn;suhangss@mail.tsinghua.edu.cn;dcszj@mail.tsinghua.edu.cn,1;3;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,0,,yes,9/25/19,Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University,8;8;8;8;8;8,23;23;23;23;23;23,4
5358,5358,5358,5358,5358,5358,5358,5358,ICLR,2020,Unsupervised Learning from Video with Deep Neural Embeddings,Chengxu Zhuang;Tianwei She;Alex Andonian;Daniel Yamins,chengxuz@stanford.edu;shetw@stanford.edu;aandonia@mit.edu;yamins@stanford.edu,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A,Withdrawn,0,0,,yes,9/25/19,Stanford University;Stanford University;Massachusetts Institute of Technology;Stanford University,4;4;2;4,4;4;5;4,
5359,5359,5359,5359,5359,5359,5359,5359,ICLR,2020,Towards Unifying Neural Architecture Space Exploration and Generalization,Kartikeya Bhardwaj;Radu Marculescu,bhardwajkartikeya@gmail.com;radum@cmu.edu,3;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,arm;Carnegie Mellon University,-1;1,-1;27,8
5360,5360,5360,5360,5360,5360,5360,5360,ICLR,2020,IEG: Robust neural net training with severe label noises,Zizhao Zhang;Han Zhang;Sercan Arik;Honglak Lee;Tomas Pfister,zizhaoz@google.com;zhanghan@google.com;soarik@google.com;honglak@google.com;tpfister@google.com,1;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,6,,yes,9/25/19,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,
5361,5361,5361,5361,5361,5361,5361,5361,ICLR,2020,Stochastic Geodesic Optimization for Neural Networks,Zana Rashidi;Aijun An;Xiaogang Wang,zrashidi@eecs.yorku.ca;aan@cse.yorku.ca;stevenw@mathstat.yorku.ca,1;3;3,I have read many papers in this area.:I did not assess the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,York University;York University;York University,172;172;172,416;416;416,
5362,5362,5362,5362,5362,5362,5362,5362,ICLR,2020,WEEGNET: an wavelet based Convnet for Brain-computer interfaces,Mouad Riyad;Mohammed Khalil;Abdellah Adib,riyadmouad1@gmail.com;medkhalil87@gmail.com;adib@fstm.ma,1;3;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:N/A:N/A,Withdrawn,0,0,,yes,9/25/19,Faculty of Sciences and Technologies -Mohammedia;;,-1;-1;-1,-1;-1;-1,
5363,5363,5363,5363,5363,5363,5363,5363,ICLR,2020,Classification as Decoder: Trading Flexibility for Control in Multi Domain Dialogue,Sam Shleifer;Manish Chablani;Namit Katariya;Anitha Kannan;Xavier Amatriain,sshleifer@gmail.com;manish@curai.com;namit@curai.com;anitha@curai.com;xavier@curai.com,1;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,3,,yes,9/25/19,Stanford University;Curai;Curai;Curai;Curai,4;233;233;233;233,4;-1;-1;-1;-1,5
5364,5364,5364,5364,5364,5364,5364,5364,ICLR,2020,Anomaly Detection and Localization in Images using Guided Attention,Shashanka Venkataramanan;Rajat Vikram Singh;Kuan-Chuan Peng,shashankv@knights.ucf.edu;singh.rajat@siemens.com;kp388@cornell.edu,6;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,5,,yes,9/25/19,University of Central Florida;Siemens Corporate Research;Cornell University,77;-1;7,609;-1;19,5;4;2
5365,5365,5365,5365,5365,5365,5365,5365,ICLR,2020,In-training Matrix Factorization for Parameter-frugal Neural Machine Translation,Zachary Kaden;Teven Le Scao;Raphael Olivier,kadenzack@gmail.com;tlescao@andrew.cmu.edu;rolivier@cs.cmu.edu,1;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,0,,yes,9/25/19,;Carnegie Mellon University;Carnegie Mellon University,-1;1;1,-1;27;27,3
5366,5366,5366,5366,5366,5366,5366,5366,ICLR,2020,Dataset Distillation,Tongzhou Wang;Jun-Yan Zhu;Antonio Torralba;Alexei A. Efros,tongzhou.wang.1994@gmail.com;junyanz@mit.edu;torralba@mit.edu;efros@eecs.berkeley.edu,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,2,11,,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;University of California Berkeley,2;2;2;5,5;5;5;13,
5367,5367,5367,5367,5367,5367,5367,5367,ICLR,2020,Spatial Information is Overrated for Image Classification,Yue Fan;Yongqin Xian;Max Maria Losch;Bernt Schiele,yfan@mpi-inf.mpg.de;yxian@mpi-inf.mpg.de;mlosch@mpi-inf.mpg.de;schiele@mpi-inf.mpg.de,6;1;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,"Saarland Informatics Campus, Max-Planck Institute;Saarland Informatics Campus, Max-Planck Institute;Saarland Informatics Campus, Max-Planck Institute;Saarland Informatics Campus, Max-Planck Institute",-1;-1;-1;-1,-1;-1;-1;-1,
5368,5368,5368,5368,5368,5368,5368,5368,ICLR,2020,GMM-UNIT: Unsupervised Multi-Domain and Multi-Modal Image-to-Image Translation via Attribute Gaussian Mixture Modelling,Yahui Liu;Marco De Nadai;Jian Yao;Nicu Sebe;Bruno Lepri;Xavier Alameda-Pineda,yahui.liu@unitn.it;denadai@fbk.eu;jian.yao@whu.edu.cn;niculae.sebe@unitn.it;lepri@fbk.eu;xavier.alameda-pineda@inria.fr,6;3;1,I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,University of Trento;Fondazione Bruno Kessler;Wuhan University;University of Trento;Fondazione Bruno Kessler;INRIA,18;-1;266;18;-1;-1,307;-1;354;307;-1;-1,
5369,5369,5369,5369,5369,5369,5369,5369,ICLR,2020,Information lies in the eye of the beholder: The effect of representations on observed mutual information,Julian Zilly;Lorenz Hetzel;Andrea Censi;Emilio Frazzoli,jzilly@ethz.ch;hetzell@ethz.ch;acensi@ethz.ch;emilio.frazzoli@idsc.mavt.ethz.ch,3;1;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,2,,yes,9/25/19,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10;10,13;13;13;13,
5370,5370,5370,5370,5370,5370,5370,5370,ICLR,2020,Quantitatively Disentangling and Understanding Part Information in CNNs,Quanshi Zhang;Yu Yang;Haotian Ma;Ying Nian Wu,zqs1022@sjtu.edu.cn;yy19970901@ucla.edu;11612807@mail.sustc.edu.cn;ywu@stat.ucla.edu,6;3;3,I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,"Shanghai Jiao Tong University;University of California, Los Angeles;University of Science and Technology of China;University of California, Los Angeles",53;20;481;20,157;17;80;17,
5371,5371,5371,5371,5371,5371,5371,5371,ICLR,2020,Graph-based motion planning networks,Tai Hoang;Ngo Anh Vien,thobotics@gmail.com;v.ngo@qub.ac.uk,1;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,0,,yes,9/25/19,VNG Corporation;Queen's University Belfast,-1;266,-1;204,10;8
5372,5372,5372,5372,5372,5372,5372,5372,ICLR,2020,Bridging ELBO objective and MMD,Talip Ucar,pilatracu@gmail.com,1;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,University College London,50,15,5
5373,5373,5373,5373,5373,5373,5373,5373,ICLR,2020,Deep 3D-Zoom Net: Unsupervised Learning of Photo-Realistic 3D-Zoom,Juan Luis Gonzalez Bello;Munchurl Kim,juanluisgb@kaist.ac.kr;mkimee@kaist.ac.kr,3;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,481;481,110;110,6
5374,5374,5374,5374,5374,5374,5374,5374,ICLR,2020,Through the Lens of Neural Network: Analyzing Neural QA Models via Quantized Latent Representation,Tsung-Han Wu;Chun-Cheng Hsieh;Yen-Hao Chen;Hung-yi Lee,ynnekuw@gmail.com;syasyunjyo@gmail.com;r07921112@ntu.edu.tw;hungyilee@ntu.edu.tw,3;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,0,,yes,9/25/19,National Taiwan University;;National Taiwan University;National Taiwan University,86;-1;86;86,120;-1;120;120,
5375,5375,5375,5375,5375,5375,5375,5375,ICLR,2020,Hierarchical Complement Objective Training,Hao-Yun Chen;Li-Huang Tsai;Shih-Chieh Chang;Jia-Yu Pan;Yu-Ting Chen;Wei Wei;Da-Cheng Juan,haoyunchen@gapp.nthu.edu.tw;lihuangtsai@gapp.nthu.edu.tw;scchang@cs.nthu.edu.tw;jypan@google.com;yutingchen@google.com;wewei@google.com;dacheng@google.com,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,National Tsing Hua University;National Tsing Hua University;National Tsing Hua University;Google;Google;Google;Google,172;172;172;-1;-1;-1;-1,365;365;365;-1;-1;-1;-1,2
5376,5376,5376,5376,5376,5376,5376,5376,ICLR,2020,Anomalous Pattern Detection in Activations and Reconstruction Error of Autoencoders,Celia Cintas;Skyler Speakman;Victor Akinwande;Srihari Sridharan;William Ogallo;Edward McFowland III,celia.cintas@ibm.com;skyler@ke.ibm.com;victor.akinwande1@ibm.com;sriharis.sridharan@ke.ibm.com;william.ogallo@ibm.com;mcfowland@umn.edu,3;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.,Withdrawn,0,3,,yes,9/25/19,"International Business Machines;International Business Machines;International Business Machines;International Business Machines;International Business Machines;University of Minnesota, Minneapolis",-1;-1;-1;-1;-1;59,-1;-1;-1;-1;-1;79,4
5377,5377,5377,5377,5377,5377,5377,5377,ICLR,2020,Dynamically Balanced Value Estimates for Actor-Critic Methods,Nicolai Dorka;Joschka Boedecker;Wolfram Burgard,dorka@cs.uni-freiburg.de;jboedeck@cs.uni-freiburg.de;burgard@cs.uni-freiburg.de,3;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,0,,yes,9/25/19,Universität Freiburg;Universität Freiburg;Universität Freiburg,118;118;118,85;85;85,
5378,5378,5378,5378,5378,5378,5378,5378,ICLR,2020,Incorporating Perceptual Prior to Improve Model's Adversarial Robustness,B.S. Vivek;Arya Baburaj;Ashutosh B Sathe;R. Venkatesh Babu,svivek@iisc.ac.in;aryababuraj@iisc.ac.in;satheab16.mech@coep.ac.in;venky@iisc.ac.in,1;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,5,,yes,9/25/19,"Indian Institute of Science;Indian Institute of Science;College of Engineering, Pune;Indian Institute of Science",95;95;-1;95,301;301;-1;301,4;2
5379,5379,5379,5379,5379,5379,5379,5379,ICLR,2020,MultiGrain: a unified image embedding for classes and instances,Maxim Berman;Hervé Jégou;Andrea Vedaldi;Iasonas Kokkinos;Matthijs Douze,maxim.berman@esat.kuleuven.be;rvj@fb.com;vedaldi@fb.com;iasonas.kokkinos@gmail.com;matthijs@fb.com,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Withdrawn,0,4,,yes,9/25/19,KU Leuven;Facebook;Facebook;Ariel AI;Facebook,118;-1;-1;-1;-1,45;-1;-1;-1;-1,
5380,5380,5380,5380,5380,5380,5380,5380,ICLR,2020,A Simple Geometric Proof for the Benefit of Depth in ReLU Networks,Asaf Amrami;Yoav Goldberg,asaf.amrami@gmail.com;yoav.goldberg@gmail.com,3;3;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,;Bar-Ilan University,-1;95,-1;513,1
5381,5381,5381,5381,5381,5381,5381,5381,ICLR,2020,How does Lipschitz Regularization Influence GAN Training?,Yipeng Qin;Niloy Mitra;Peter Wonka,qinyipeng1991@gmail.com;niloym@gmail.com;pwonka@gmail.com,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,Cardiff University;University College London;KAUST,172;50;128,196;15;1397,5
5382,5382,5382,5382,5382,5382,5382,5382,ICLR,2020,SIMULTANEOUS ATTRIBUTED NETWORK EMBEDDING AND CLUSTERING,Lazhar labiod;Mohamed Nadif,lazhar.labiod@parisdescartes.fr;mohamed.nadif@parisdescartes.fr,1;1;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,University Paris Descartes;University Paris Descartes,-1;-1,-1;-1,
5383,5383,5383,5383,5383,5383,5383,5383,ICLR,2020,Tree-structured Attention Module for Image Classification,Gyungin Shin;Sung-Ho Bae;and Yong-Jae Moon,gishin@khu.ac.kr;shbae@khu.ac.kr;moonyj@khu.ac.kr,3;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Withdrawn,0,3,,yes,9/25/19,Kyung Hee University;Kyung Hee University;Kyung Hee University,481;481;481,319;319;319,2
5384,5384,5384,5384,5384,5384,5384,5384,ICLR,2020,Task Level Data Augmentation for Meta-Learning,Jialin Liu;Fei Chao;Chih-Min Lin,31520171153232@stu.xmu.edu.cn;fchao@xmu.edu.cn;cml@saturn.yzu.edu.tw,3;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,1,,yes,9/25/19,"Xiamen University;Xiamen University;Department of Computer Science and Engineering, Yuan Ze University",64;64;-1,8;8;-1,1;6
5385,5385,5385,5385,5385,5385,5385,5385,ICLR,2020,Learning Semantic Correspondences from Noisy Data-text Pairs by Local-to-Global Alignments,Feng Nie;Jinpeng Wang;Rong Pan;Chin-Yew Lin,fengniesysu@gmail.com;jinpwa@microsoft.com;panr@sysu.edu.cn;cyl@microsoft.com,3;8,I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,SUN YAT-SEN UNIVERSITY;Microsoft;SUN YAT-SEN UNIVERSITY;Microsoft,481;-1;481;-1,299;-1;299;-1,3
5386,5386,5386,5386,5386,5386,5386,5386,ICLR,2020,VideoEpitoma: Efficient Recognition of Long-range Actions,Noureldien Hussein;Babak Ehteshami Bejnordi;Mihir Jain,nhussein@uva.nl;behtesha@qti.qualcomm.com;mijain@qti.qualcomm.com,1;3;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I carefully checked the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,"University of Amsterdam;Qualcomm Inc, QualComm;Qualcomm Inc, QualComm",172;-1;-1,62;-1;-1,
5387,5387,5387,5387,5387,5387,5387,5387,ICLR,2020,StacNAS: Towards Stable and Consistent  Optimization for Differentiable  Neural Architecture Search,Li Guilin;Zhang Xing;Wang Zitong;Li Zhenguo;Zhang Tong,hiliguilin@gmail.com;zhang.xing1@huawei.com;ztwang@math.cuhk.edu.hk;li.zhenguo@huawei.com;tongzhang@tongzhang-ml.org,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Withdrawn,1,3,,yes,9/25/19,;Huawei Technologies Ltd.;The Chinese University of Hong Kong;Huawei Technologies Ltd.;The Hong Kong University of Science and Technology,-1;-1;59;-1;39,-1;-1;35;-1;47,
5388,5388,5388,5388,5388,5388,5388,5388,ICLR,2020,Construction of Macro Actions for Deep Reinforcement Learning,Yi-Hsiang Chang;Kuan-Yu	Chang;Henry Kuo;Chun-Yi Lee,shawn420@gapp.nthu.edu.tw;kychang@elsa.cs.nthu.edu.tw;hkuo@college.harvard.edu;cylee@gapp.nthu.edu.tw,1;1;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,1,,yes,9/25/19,National Tsing Hua University;National Tsing Hua University;Harvard University;National Tsing Hua University,172;172;39;172,365;365;7;365,
5389,5389,5389,5389,5389,5389,5389,5389,ICLR,2020,Towards Understanding Generalization in Gradient-Based Meta-Learning,Simon Guiroy;Vikas Verma;Christopher J. Pal,simon.guiroy@umontreal.ca;vikasverma.iitm@gmail.com;christopher.pal@polymtl.ca,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,University of Montreal;;Polytechnique Montreal,128;-1;390,85;-1;1397,6;8
5390,5390,5390,5390,5390,5390,5390,5390,ICLR,2020,Structural Multi-agent Learning,Kaiqian Han;Liangliang Ren;Jiwen Lu;Jie Zhou,hkg16@mails.tsinghua.edu.cn;renll16@mails.tsinghua.edu.cn;lujiwen@tsinghua.edu.cn;jzhou@tsinghua.edu.cn,1;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,4,,yes,9/25/19,Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University,8;8;8;8,23;23;23;23,10
5391,5391,5391,5391,5391,5391,5391,5391,ICLR,2020,AdamT: A Stochastic Optimization with Trend Correction Scheme,Bingxin Zhou;Xuebin Zheng;Junbin Gao,bzho3923@uni.sydney.edu.au;xzhe2914@uni.sydney.edu.au;junbin.gao@sydney.edu.au,3;3;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,University of Sydney;University of Sydney;University of Sydney,86;86;86,60;60;60,
5392,5392,5392,5392,5392,5392,5392,5392,ICLR,2020,Elastic-InfoGAN: Unsupervised Disentangled Representation Learning in Imbalanced Data,Utkarsh Ojha;Krishna Kumar Singh;Cho-Jui Hsieh;Yong Jae Lee,uojha@ucdavis.edu;krsingh@ucdavis.edu;chohsieh@cs.ucla.edu;yongjaelee@ucdavis.edu,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,"University of California, Davis;University of California, Davis;University of California, Los Angeles;University of California, Davis",79;79;20;79,55;55;17;55,5
5393,5393,5393,5393,5393,5393,5393,5393,ICLR,2020,Generalizing Deep Multi-task Learning with Heterogeneous Structured Networks,Ming Hou;Xinqi Chen;Shifeng Huang;Shengli Xie;Guoxu Zhou;Qibin Zhao,ming.hou@riken.jp;xinqicham@gmail.com;sfengmmin@163.com;shlxie@gdut.edu.cn;gx.zhou@gdut.edu.cn;qibin.zhao@riken.jp,3;1;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,0,,yes,9/25/19,RIKEN;South China University of Technology;163;South China University of Technology;South China University of Technology;RIKEN,-1;481;-1;481;481;-1,-1;501;-1;501;501;-1,
5394,5394,5394,5394,5394,5394,5394,5394,ICLR,2020,Rethinking Data Augmentation: Self-Supervision and Self-Distillation,Hankook Lee;Sung Ju Hwang;Jinwoo Shin,hankook.lee@kaist.ac.kr;sjhwang82@kaist.ac.kr;jinwoos@kaist.ac.kr,1;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,2,,yes,9/25/19,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,481;481;481,110;110;110,6;8
5395,5395,5395,5395,5395,5395,5395,5395,ICLR,2020,Dynamical Clustering of Time Series Data Using Multi-Decoder RNN Autoencoder,Daisuke Kaji;Kazuho Watanabe;Masahiro Kobayashi,daisuke.kaji.j3a@jp.denso.com;wkazuho@cs.tut.ac.jp;kobayashi@lisl.cs.tut.ac.jp,3;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,"Jp.denso;Toyohashi University of Technology,;Toyohashi University of Technology,",-1;-1;-1,-1;-1;-1,
5396,5396,5396,5396,5396,5396,5396,5396,ICLR,2020,Regularizing Predictions via Class-wise Self-knowledge Distillation,Sukmin Yun;Jongjin Park;Kimin Lee;Jinwoo Shin,sukmin.yun@kaist.ac.kr;jongjin.park@kaist.ac.kr;kiminlee@kaist.ac.kr;jinwoos@kaist.ac.kr,3;1;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Withdrawn,0,1,,yes,9/25/19,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,481;481;481;481,110;110;110;110,8
5397,5397,5397,5397,5397,5397,5397,5397,ICLR,2020,Imbalanced Classification via Adversarial Minority Over-sampling,Jaehyung Kim;Jongheon Jeong;Jinwoo Shin,jaehyungkim@kaist.ac.kr;jongheonj@kaist.ac.kr;jinwoos@kaist.ac.kr,3;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,481;481;481,110;110;110,3;4;8
5398,5398,5398,5398,5398,5398,5398,5398,ICLR,2020,Neuron ranking - an informed way to compress convolutional neural networks,Kamil Adamczewski;Mijung Park,kamil.m.adamczewski@gmail.com;mijung.park@tuebingen.mpg.de,1;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,"Max Planck Institute for Intelligent Systems, Max-Planck Institute;Max Planck Institute for Intelligent Systems, Max-Planck Institute",-1;-1,-1;-1,
5399,5399,5399,5399,5399,5399,5399,5399,ICLR,2020,Compressing Deep Neural Networks With Learnable Regularization,Yoojin Choi;Mostafa El-Khamy;Jungwon Lee,yoojin.c@samsung.com;mostafa.e@samsung.com;jungwon2.lee@samsung.com,3;6;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,Samsung;Samsung;Samsung,-1;-1;-1,-1;-1;-1,
5400,5400,5400,5400,5400,5400,5400,5400,ICLR,2020,Image Classification Through Top-Down Image Pyramid Traversal,Athanasios Papadopoulos;Pawel Korus;Nasir Memon,ap4094@nyu.edu;pkorus@nyu.edu;memon@nyu.edu,3;1;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,4,,yes,9/25/19,New York University;New York University;New York University,25;25;25,29;29;29,
5401,5401,5401,5401,5401,5401,5401,5401,ICLR,2020,PAC-Bayes Few-shot Meta-learning with Implicit Learning of Model Prior Distribution,Cuong Nguyen;Thanh-Toan Do;Gustavo Carneiro,cuong.nguyen@adelaide.edu.au;thanh-toan.do@liverpool.ac.uk;gustavo.carneiro@adelaide.edu.au,6;1;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,The University of Adelaide;University of Liverpool;The University of Adelaide,128;-1;128,120;-1;120,11;5;6
5402,5402,5402,5402,5402,5402,5402,5402,ICLR,2020,CRAP: Semi-supervised Learning via Conditional Rotation Angle Prediction,Hai-Ming Xu;Lingqiao Liu,hai-ming.xu@adelaide.edu.au;lingqiao.liu@adelaide.edu.au,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Withdrawn,0,2,,yes,9/25/19,The University of Adelaide;The University of Adelaide,128;128,120;120,
5403,5403,5403,5403,5403,5403,5403,5403,ICLR,2020,A novel text representation which enables image classifiers to perform text classification,Stephen M. Petrie;T'Mir D. Julius,spetrie@swin.edu.au;tdjempire@gmail.com,1;1;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,0,,yes,9/25/19,Swinburne University of Technology;,-1;-1,-1;-1,3
5404,5404,5404,5404,5404,5404,5404,5404,ICLR,2020,FACE SUPER-RESOLUTION GUIDED BY 3D FACIAL PRIORS,xiaobin hu;wenqi ren;jiaolong yang;xiaochun cao;Xiaoming Li;John LaMaster;Bjoern Menze;wei liu,xiaobin.hu@tum.de;rwq.renwenqi@gmail.com;jiaoyan@microsoft.com;caoxiaochun@iie.ac.cn;hit.xmshr@gmail.com;jlamaste@gmail.com;bjoern.menze@tum.de;wl2223@columbia.edu,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,"Technical University Munich;Institute of information engineering, CAS;Microsoft;Institute of information engineering, CAS;Harbin Institute of Technology;Technical University Munich;Technical University Munich;Columbia University",53;-1;-1;-1;172;53;53;15,43;-1;-1;-1;424;43;43;16,
5405,5405,5405,5405,5405,5405,5405,5405,ICLR,2020,Doubly Normalized Attention,Nan Ding;Xinjie Fan;Zhenzhong Lan;Dale Schuurmans;Radu Soricut,dingnan@google.com;fan.xinjiebuaa@gmail.com;lanzhzh@google.com;schuurmans@google.com;rsoricut@google.com,3;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,"Google;University of Texas, Austin;Google;Google;Google",-1;22;-1;-1;-1,-1;38;-1;-1;-1,
5406,5406,5406,5406,5406,5406,5406,5406,ICLR,2020,Correctness Verification of Neural Network,Yichen Yang;Martin Rinard,yicheny@csail.mit.edu;rinard@csail.mit.edu,1;3;1,I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2,5;5,1
5407,5407,5407,5407,5407,5407,5407,5407,ICLR,2020,Diversely Stale Parameters for Efficient Training of Deep Convolutional Networks,An Xu;Zhouyuan Huo;Heng Huang,an.xu@pitt.edu;zhouyuan.huo@pitt.edu;heng.huang@pitt.edu,3;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Withdrawn,0,0,,yes,9/25/19,University of Pittsburgh;University of Pittsburgh;University of Pittsburgh,79;79;79,113;113;113,1;8
5408,5408,5408,5408,5408,5408,5408,5408,ICLR,2020,WHAT DATA IS USEFUL FOR MY DATA: TRANSFER LEARNING WITH A MIXTURE OF SELF-SUPERVISED EXPERTS,Xi Yan;David Acuna;Sanja Fidler,xi.yan@mail.utoronto.ca;davidj@cs.toronto.edu;fidler@cs.toronto.edu,1;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,"Toronto University;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",18;18;18,18;18;18,6;2
5409,5409,5409,5409,5409,5409,5409,5409,ICLR,2020,Classification Logit Two-sample Testing by Neural Networks,Xiuyuan Cheng;Alexander Cloninger,xiuyuan.cheng@duke.edu;acloninger@ucsd.edu,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,"Duke University;University of California, San Diego",47;11,20;31,5;4;1
5410,5410,5410,5410,5410,5410,5410,5410,ICLR,2020,Frontal low-rank random tensors for high-order feature representation,Yan Zhang;Krikamol Muandet;Qianli Ma;Heiko Neumann;Siyu Tang,yan.zhang@tuebingen.mpg.de;krikamol@tuebingen.mpg.de;qianli.ma@tue.mpg.de;heiko.neumann@uni-ulm.de;stang@tuebingen.mpg.de,6;3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,"Max Planck Institute for Intelligent Systems, Max-Planck Institute;Max Planck Institute for Intelligent Systems, Max-Planck Institute;Max Planck Institute for Intelligent Systems, Max-Planck Institute;Ulm University;Max Planck Institute for Intelligent Systems, Max-Planck Institute",-1;-1;-1;172;-1,-1;-1;-1;416;-1,1;2
5411,5411,5411,5411,5411,5411,5411,5411,ICLR,2020,Interpretability Evaluation Framework for Deep Neural Networks,Junxiang Wang;Liang Zhao;Yanfang Ye;Houman Homayoun,jwang40@gmu.edu;lzhao9@gmu.edu;yanfang.ye@mail.wvu.edu;hhomayou@gmu.edu,1;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,George Mason University;George Mason University;West Virginia University;George Mason University,100;100;-1;100,282;282;-1;282,1
5412,5412,5412,5412,5412,5412,5412,5412,ICLR,2020,CWAE-IRL: Formulating a supervised approach to Inverse Reinforcement Learning problem,Arpan Kusari,arpan.kusari@gmail.com,3;1;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,,,,5
5413,5413,5413,5413,5413,5413,5413,5413,ICLR,2020,"Study of a Simple, Expressive and Consistent Graph Feature Representation",Pineau Edouard,pineau.edouard@gmail.com,1;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,Télécom ParisTech,481,187,10
5414,5414,5414,5414,5414,5414,5414,5414,ICLR,2020,Input Alignment along Chaotic directions increases Stability in Recurrent Neural Networks,Priyadarshini Panda;Kaushik Roy,priya.panda@yale.edu;kaushik@purdue.edu,6;1;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Yale University;Purdue University,64;27,8;88,
5415,5415,5415,5415,5415,5415,5415,5415,ICLR,2020,All Neural Networks are Created Equal,Guy Hacohen;Leshem Choshen;Daphna Weinshall,guy.hacohen@mail.huji.ac.il;leshem.choshen@mail.huji.ac.il;daphna@cs.huji.ac.il,3;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Withdrawn,0,1,,yes,9/25/19,Hebrew University of Jerusalem;Hebrew University of Jerusalem;Hebrew University of Jerusalem,67;67;67,216;216;216,10;8
5416,5416,5416,5416,5416,5416,5416,5416,ICLR,2020,Hyperbolic Image Embeddings,Valentin Khrulkov;Leyla Mirvakhabova;Evgeniya Ustinova;Ivan Oseledets;Victor Lempitsky,khrulkov.v@gmail.com;leyla.mirvakhabova@skoltech.ru;evgeniya.ustinova@skoltech.ru;i.oseledets@skoltech.ru;v.lempitsky@samsung.com,1;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,Skolkovo Institute of Science and Technology;Skolkovo Institute of Science and Technology;Skolkovo Institute of Science and Technology;Skolkovo Institute of Science and Technology;Samsung,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,6;2
5417,5417,5417,5417,5417,5417,5417,5417,ICLR,2020,Instant Quantization of Neural Networks using Monte Carlo Methods,Gonçalo Mordido;Matthijs Van Keirsbilck;Alexander Keller,goncalo.mordido@hpi.de;matthijsv@nvidia.com;akeller@nvidia.com,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Withdrawn,0,0,,yes,9/25/19,Hasso Plattner Institute;NVIDIA;NVIDIA,266;-1;-1,1397;-1;-1,
5418,5418,5418,5418,5418,5418,5418,5418,ICLR,2020,Gating Revisited: Deep Multi-layer RNNs That Can Be Trained,Mehmet Ozgur Turkoglu;Stefano D'Aronco;Jan Dirk Wegner;Konrad Schindler,ozgur.turkoglu@geod.baug.ethz.ch;stefano.daronco@geod.baug.ethz.ch;jan.wegner@geod.baug.ethz.ch;schindler@geod.baug.ethz.ch,1;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10;10,13;13;13;13,
5419,5419,5419,5419,5419,5419,5419,5419,ICLR,2020,ILS-SUMM: Iterated Local Search for Unsupervised Video Summarization,Yair Shemer;Daniel Rotman;Nahum Shimkin,sy@campus.technion.ac.il;danieln@il.ibm.com;shimkin@ee.technion.ac.il,1;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,Technion;International Business Machines;Technion,26;-1;26,412;-1;412,
5420,5420,5420,5420,5420,5420,5420,5420,ICLR,2020,Quadratic GCN for graph classification,Omer Nagar;Yoram Louzoun,ovednagar@hotmail.com;louzouy@math.biu.ac.il,1;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,;Bar Ilan University,-1;95,-1;513,10
5421,5421,5421,5421,5421,5421,5421,5421,ICLR,2020,Topological based classification using graph convolutional networks,Roy Abel;Idan Benami;Yoram Louzoun,royabel10@gmail.com;louzouy@math.biu.ac.il,3;1;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,0,,yes,9/25/19,Bar Ilan University;Bar Ilan University,95;95,513;513,10
5422,5422,5422,5422,5422,5422,5422,5422,ICLR,2020,Quantifying Layerwise Information Discarding of Neural Networks and Beyond,Haotian Ma;Yinqing Zhang;Fan Zhou;Quanshi Zhang,11612807@mail.sustc.edu.cn;zhangyinqing@sjtu.edu.cn;zhoufan98@sjtu.edu.cn;zqs1022@sjtu.edu.cn,3;3;3,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Withdrawn,0,3,,yes,9/25/19,University of Science and Technology of China;Shanghai Jiao Tong University;Shanghai Jiao Tong University;Shanghai Jiao Tong University,481;53;53;53,80;157;157;157,7
5423,5423,5423,5423,5423,5423,5423,5423,ICLR,2020,Real or Fake: An Empirical Study and Improved Model for Fake Face Detection,Zhengzhe Liu;Xiaojuan Qi;Jiaya Jia;Philip H. S. Torr,liuzhengzhelzz@gmail.com;xiaojuan.qi@eng.ox.ac.uk;leojia@cse.cuhk.edu.hk;philip.torr@eng.ox.ac.uk,3;3;3,I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,;University of Oxford;The Chinese University of Hong Kong;University of Oxford,-1;50;59;50,-1;1;35;1,5
5424,5424,5424,5424,5424,5424,5424,5424,ICLR,2020,Minimizing Change in Classifier Likelihood to Mitigate Catastrophic Forgetting,Ashish Gaurav;Sachin Vernekar;Sean Sedwards;Jaeyoung Lee;Vahdat Abdelzad;Krzysztof Czarnecki,ashish.gaurav@uwaterloo.ca;sachin.vernekar@uwaterloo.ca;sean.sedwards@uwaterloo.ca;jaeyoung.lee@uwaterloo.ca;vabdelza@gsd.uwaterloo.ca;kczarnec@gsd.uwaterloo.ca,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,1,1,,yes,9/25/19,University of Waterloo;University of Waterloo;University of Waterloo;University of Waterloo;University of Waterloo;University of Waterloo,28;28;28;28;28;28,235;235;235;235;235;235,
5425,5425,5425,5425,5425,5425,5425,5425,ICLR,2020,A Memory-augmented Neural Network by Resembling Human Cognitive Process of Memorization,Dongjing Shan;Xiongwei Zhang;Chao Zhang;Limin Wang,shandongjing@pku.edu.cn;xwzhang9898@163.com;chzhang@cis.pku.edu.cn;07wanglimin@gmail.com,3;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Peking University;163;Peking University;Zhejiang University,22;-1;22;56,24;-1;24;107,
5426,5426,5426,5426,5426,5426,5426,5426,ICLR,2020,Molecule Property Prediction and Classification with Graph Hypernetworks,Eliya Nachmani;Lior Wolf,enk100@gmail.com;wolf@fb.com,1;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Facebook;Facebook,-1;-1,-1;-1,10
5427,5427,5427,5427,5427,5427,5427,5427,ICLR,2020, Sparsity Learning in Deep Neural Networks,Amirsina Torfi;Rouzbeh A. Shirvani;Sobhan Soleymani;Nasser M. Nasrabadi,atorfi@vt.edu;rouzbeh.asghari@gmail.com;ssoleyma@mix.wvu.edu;nasser.nasrabadi@mail.wvu.edu,3;1;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,0,,yes,9/25/19,Virginia Tech;;West Virginia University;West Virginia University,79;-1;-1;-1,240;-1;-1;-1,
5428,5428,5428,5428,5428,5428,5428,5428,ICLR,2020,A Harmonic Structure-Based Neural Network Model for Musical Pitch Detection,Xian Wang;Lingqiao Liu;Qinfeng Shi,xian.wang01@adelaide.edu.au;lingqiao.liu@adelaide.edu.au;javen.shi@adelaide.edu.au,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:N/A:N/A,Withdrawn,0,0,,yes,9/25/19,The University of Adelaide;The University of Adelaide;The University of Adelaide,128;128;128,120;120;120,
5429,5429,5429,5429,5429,5429,5429,5429,ICLR,2020,SEERL : Sample Efficient Ensemble Reinforcement Learning,Rohan Saphal;Balaraman Ravindran;Dheevatsa Mudigere;Sasikanth Avancha;Bharat Kaul,rohansaphal@gmail.com;ravi@cse.iitm.ac.in;dheevatsa@fb.com;sasikanth.avancha@intel.com;bharat.kaul@intel.com,1;1;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,0,,yes,9/25/19,University of Oxford;Indian Institute of Technology Madras;Facebook;Intel;Intel,50;154;-1;-1;-1,1;641;-1;-1;-1,1
5430,5430,5430,5430,5430,5430,5430,5430,ICLR,2020,Exploring by Exploiting Bad Models in Model-Based Reinforcement Learning,Yixin Lin;Sarah Bechtle;Ludovic Righetti;Akshara Rai;Franziska Meier,yixinlin@fb.com;sbechtle@tuebingen.mpg.de;ludovic.righetti@nyu.edu;akshararai@fb.com;fmeier@fb.com,3;1;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,0,,yes,9/25/19,"Facebook;Max Planck Institute for Intelligent Systems, Max-Planck Institute;New York University;Facebook;Facebook",-1;-1;25;-1;-1,-1;-1;29;-1;-1,
5431,5431,5431,5431,5431,5431,5431,5431,ICLR,2020,Defensive Quantization Layer For Convolutional Network Against Adversarial Attack,Sirui Song;Qinglong Wang;Derek Yang;Yan Song;Xue Liu;Tong Zhang,siruisong97@gmail.com;qinglong.wang@mail.mcgill.ca;dyang1206@gmail.com;songyan@chuangxin.com;xueliu@cs.mcgill.ca;tongzhang0@gmail.com,3;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,"Tsinghua University;McGill University;University of California, San Diego;Sinovation Ventures;McGill University;The Hong Kong University of Science and Technology",8;86;11;-1;86;39,23;42;31;-1;42;47,4
5432,5432,5432,5432,5432,5432,5432,5432,ICLR,2020,Learnable Higher-order Representation for Action Recognition,Kai Hu;Bhiksha Raj,kaihu@cmu.edu;bhiksha@cs.cmu.edu,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,1,,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University,1;1,27;27,1
5433,5433,5433,5433,5433,5433,5433,5433,ICLR,2020,Meta Module Network for Compositional Visual Reasoning,Wenhu Chen;Zhe Gan;Linjie Li;Yu Cheng;William Wang;Jingjing Liu,wenhuchen@ucsb.edu;zhe.gan@microsoft.com;lindsey.li@microsoft.com;yu.cheng@microsoft.com;william@cs.ucsb.edu;jingjl@microsoft.com,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,UC Santa Barbara;Microsoft;Microsoft;Microsoft;UC Santa Barbara;Microsoft,38;-1;-1;-1;38;-1,57;-1;-1;-1;57;-1,10;8
5434,5434,5434,5434,5434,5434,5434,5434,ICLR,2020,Multi-Task Adapters for On-Device Audio Inference,M. Tagliasacchi;F. de Chaumont Quitry;D. Roblek,mtagliasacchi@google.com;fcq@google.com;droblek@google.com,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,0,,yes,9/25/19,Google;Google;Google,-1;-1;-1,-1;-1;-1,
5435,5435,5435,5435,5435,5435,5435,5435,ICLR,2020,Learning audio representations with self-supervision,M. Tagliasacchi;B. Gfeller;F. de Chaumont Quitry;D. Roblek,mtagliasacchi@google.com;beatg@google.com;fcq@google.com;droblek@google.com,3;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,3
5436,5436,5436,5436,5436,5436,5436,5436,ICLR,2020,Scholastic-Actor-Critic For Multi Agent Reinforcement Learning,Weiying Chen，Ruize Hou,dissolution@126.com;fsszns@163.com,1;1;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,4,,yes,9/25/19,Netaji Subhas Institute of Technology;Peking University,-1;22,-1;24,
5437,5437,5437,5437,5437,5437,5437,5437,ICLR,2020,Non-Gaussian processes and neural networks at finite widths,Sho Yaida,shoyaida@fb.com,3;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,Facebook,-1,-1,11
5438,5438,5438,5438,5438,5438,5438,5438,ICLR,2020,Uncertainty-aware Variational-Recurrent Imputation Network for Clinical Time Series,Ahmad Wisnu Mulyadi;Eunji Jun;Heung-Il Suk,wisnumulyadi@korea.ac.kr;ejjun92@korea.ac.kr;hisuk@korea.ac.kr,1;3;6;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Korea University;Korea University;Korea University,323;323;323,179;179;179,5
5439,5439,5439,5439,5439,5439,5439,5439,ICLR,2020,An Information Theoretic Perspective on Disentangled Representation Learning,Xiaojiang Yang;Wendong Bi;Yu Cheng;Junchi Yan,yangxiaojiang@sjtu.edu.cn;biwendong1997@gmail.com;yu.cheng@microsoft.com;yanjunchi@sjtu.edu.cn,1;1;3,I have published one or two papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Withdrawn,0,4,,yes,9/25/19,Shanghai Jiao Tong University;Microsoft;Microsoft;Shanghai Jiao Tong University,53;-1;-1;53,157;-1;-1;157,5;1
5440,5440,5440,5440,5440,5440,5440,5440,ICLR,2020,Towards a Unified Evaluation of Explanation Methods without Ground Truth,Hao Zhang;Jiayi Chen;Haotian Xue;Quanshi Zhang,1603023-zh@sjtu.edu.cn;miracle3310@sjtu.edu.cn;xavihart@sjtu.edu.cn;zqs1022@sjtu.edu.cn,3;1;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,Shanghai Jiao Tong University;Shanghai Jiao Tong University;Shanghai Jiao Tong University;Shanghai Jiao Tong University,53;53;53;53,157;157;157;157,
5441,5441,5441,5441,5441,5441,5441,5441,ICLR,2020,Fast Sparse ConvNets,Erich Elsen;Marat Dukhan;Trevor Gale;Karen Simonyan,eriche@google.com;maratek@google.com;tgale@google.com;simonyan@google.com,3;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A,Withdrawn,0,3,,yes,9/25/19,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,
5442,5442,5442,5442,5442,5442,5442,5442,ICLR,2020,Locally adaptive activation functions with slope recovery term for deep and physics-informed neural networks,Ameya D. Jagtap;Kenji Kawaguchi;George Em Karniadakis,ameya_jagtap@brown.edu;kawaguch@mit.edu;george_karniadakis@brown.edu,1;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Withdrawn,0,0,,yes,9/25/19,Brown University;Massachusetts Institute of Technology;Brown University,67;2;67,53;5;53,1
5443,5443,5443,5443,5443,5443,5443,5443,ICLR,2020,GumbelClip: Off-Policy Actor-Critic Using Experience Replay,Norman Tasfi;Miriam Capretz,ntasfi@gmail.com,1;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,University of Western Ontario,-1,-1,
5444,5444,5444,5444,5444,5444,5444,5444,ICLR,2020,Adversarial Neural Pruning,Divyam Madaan;Jinwoo Shin;Sung Ju Hwang,dmadaan@kaist.ac.kr;jinwoos@kaist.ac.kr;sjhwang82@kaist.ac.kr,1;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,481;481;481,110;110;110,4;11
5445,5445,5445,5445,5445,5445,5445,5445,ICLR,2020,Auto-Encoding Explanatory Examples,César Ojeda;David Biesner;Ramses Sanchez;Kostadin Cvejoski;Jannis Schuecker;Christian Bauckhage;Bodgan Georgiev,cesarali07@gmail.com;david.biesner@iais.fraunhofer.de;sanchez@bit.uni-bonn.de;kostadin.cvejoski@iais.fraunhofer.de;jannis.schuecker@iais.fraunhofer.de;christian.bauckhage@iais.fraunhofer.de;bogdan.georgiev@iais.fraunhofer.de,3;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,Fraunhofer IIS;Fraunhofer IIS;University of Bonn;Fraunhofer IIS;Fraunhofer IIS;Fraunhofer IIS;Fraunhofer IIS,-1;-1;128;-1;-1;-1;-1,-1;-1;106;-1;-1;-1;-1,
5446,5446,5446,5446,5446,5446,5446,5446,ICLR,2020,Affine Self Convolution,Nichita Diaconu;Daniel E. Worrall,diacon995@gmail.com;d.e.worrall@uva.nl,1;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,University of Amsterdam;University of Amsterdam,172;172,62;62,
5447,5447,5447,5447,5447,5447,5447,5447,ICLR,2020,Universal Source-Free Domain Adaptation,Jogendra Nath Kundu;Naveen Venkat;Rahul M V;R. Venkatesh Babu,jogendrak@iisc.ac.in;nav.naveenvenkat@gmail.com;rmvenkat@andrew.cmu.edu;venky@iisc.ac.in,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Withdrawn,0,4,,yes,9/25/19,Indian Institute of Science;Indian Institute of Science;Carnegie Mellon University;Indian Institute of Science,95;95;1;95,301;301;27;301,5;4
5448,5448,5448,5448,5448,5448,5448,5448,ICLR,2020,Learning an off-policy predictive state representation for deep reinforcement learning for vision-based steering in autonomous driving,Daniel Graves,dgraves@ualberta.ca,1;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,4,,yes,9/25/19,University of Alberta,100,136,8
5449,5449,5449,5449,5449,5449,5449,5449,ICLR,2020,$\textrm{D}^2$GAN: A Few-Shot Learning Approach with Diverse and Discriminative Feature Synthesis,Kai Li;Yulun Zhang;Kunpeng Li;Yun Fu,li.kai.gml@gmail.com;yulun100@gmail.com;kunpengli@ece.neu.edu;yunfu@ece.neu.edu,1;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Northeastern University;Northeastern University;Northeastern University;Northeastern University,16;16;16;16,906;906;906;906,5;4;6
5450,5450,5450,5450,5450,5450,5450,5450,ICLR,2020,Hardware-aware One-Shot Neural Architecture Search in Coordinate Ascent Framework,Li Lyna Zhang;Yuqing Yang;Yuhang Jiang;Wenwu Zhu;Yunxin Liu,lzhani@microsoft.com;yuqing.yang@microsoft.com;jyh17@mails.tsinghua.edu.cn;wwzhu@tsinghua.edu.cn;yunxin.liu@microsoft.com,3;1;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Microsoft;Microsoft;Tsinghua University;Tsinghua University;Microsoft,-1;-1;8;8;-1,-1;-1;23;23;-1,
5451,5451,5451,5451,5451,5451,5451,5451,ICLR,2020,Multi-task Network Embedding with Adaptive Loss Weighting,Fatemeh Salehi Rizi;Michael Granitzer,fatemeh.salehirizi@uni-passau.de;michael.granitzer@uni-passau.de,3;1;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,University of Passau;University of Passau,233;233,146;146,
5452,5452,5452,5452,5452,5452,5452,5452,ICLR,2020,Perception-Driven Curiosity with Bayesian Surprise,Bernadette Bucher;Anton Arapin;Ramanan Sekar;Marc Badger;Feifei Duan;Oleh Rybkin;Kostas Daniilidis,bucherb@seas.upenn.edu;aarapin@fandm.edu;ramanans@seas.upenn.edu;mbadger@seas.upenn.edu;feifeid@seas.upenn.edu;oleh@seas.upenn.edu;kostas@seas.upenn.edu,1;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,University of Pennsylvania;Franklin & Marshall College;University of Pennsylvania;University of Pennsylvania;University of Pennsylvania;University of Pennsylvania;University of Pennsylvania,19;-1;19;19;19;19;19,11;-1;11;11;11;11;11,
5453,5453,5453,5453,5453,5453,5453,5453,ICLR,2020,Global reasoning network for image super-resolution,Jiahui Zhang;Bin Zhou;Qingchang Tao;Deqiang Wang,jhzhang988@gmail.com;binzhou@sdu.edu.cn;taoqingchang@mail.tsinghua.edu.cn;wdq_sdu@sdu.edu.cn,1;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,1,2,,yes,9/25/19,Shandong University;Shandong University;Tsinghua University;Shandong University,154;154;8;154,658;658;23;658,10
5454,5454,5454,5454,5454,5454,5454,5454,ICLR,2020,MUSE: Multi-Scale Attention Model for Sequence to Sequence Learning,Guangxiang Zhao;Xu Sun;Jingjing Xu;Zhiyuan Zhang;Liangchen Luo,1701214310@pku.edu.cn;xusun@pku.edu.cn;jingjingxu@pku.edu.cn;zzy1210@pku.edu.cn;luolc@pku.edu.cn,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:N/A:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,6,,yes,9/25/19,Peking University;Peking University;Peking University;Peking University;Peking University,22;22;22;22;22,24;24;24;24;24,3
5455,5455,5455,5455,5455,5455,5455,5455,ICLR,2020,Utility Analysis of Network Architectures for 3D Point Cloud Processing,Shikun Huang;Binbin Zhang;Wen Shen;Zhihua Wei;Quanshi Zhang,hsk@tongji.edu.cn;0206zbb@tongji.edu.cn;1810068@tongji.edu.cn;zhihua_wei@tongji.edu.cn;zqs1022@sjtu.edu.cn,6;3;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University;Shanghai Jiao Tong University,8;8;8;8;53,23;23;23;23;157,4;1
5456,5456,5456,5456,5456,5456,5456,5456,ICLR,2020,Reducing Sentiment Bias in Language Models via Counterfactual Evaluation,Po-Sen Huang;Huan Zhang;Ray Jiang;Robert Stanforth;Johannes Welbl;Jack Rae;Vishal Maini;Dani Yogatama;Pushmeet Kohli,posenhuang@google.com;huan@huan-zhang.com;rayjiang@google.com;stanforth@google.com;j.welbl@cs.ucl.ac.uk;jwrae@google.com;vmaini@google.com;dyogatama@google.com;pushmeet@google.com,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Withdrawn,0,6,,yes,9/25/19,"Google;University of California, Los Angeles;Google;Google;University College London;Google;Google;Google;Google",-1;20;-1;-1;50;-1;-1;-1;-1,-1;17;-1;-1;15;-1;-1;-1;-1,3;7
5457,5457,5457,5457,5457,5457,5457,5457,ICLR,2020,BERT for Sequence-to-Sequence Multi-Label Text Classification,Ramil Yarullin;Pavel Serdyukov,ramly@ya.ru;pavel.serdyukov@gmail.com,3;3;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,4,,yes,9/25/19,Yandex;,-1;-1,-1;-1,
5458,5458,5458,5458,5458,5458,5458,5458,ICLR,2020,Boosting Generative Models by Leveraging Cascaded Meta-Models,Fan Bao;Hang Su;Jun Zhu,bf19@mails.tsinghua.edu.cn;suhangss@mail.tsinghua.edu.cn;dcszj@mail.tsinghua.edu.cn,1;6;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Tsinghua University;Tsinghua University;Tsinghua University,8;8;8,23;23;23,5;1
5459,5459,5459,5459,5459,5459,5459,5459,ICLR,2020,RAT-SQL: Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers,Bailin Wang*;Richard Shin*;Xiaodong Liu;Oleksandr Polozov;Matthew Richardson,bailin.wang@ed.ac.uk;ricshin@cs.berkeley.edu;xiaodl@microsoft.com;polozov@microsoft.com;mattri@microsoft.com,6;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,4,,yes,9/25/19,University of Edinburgh;University of California Berkeley;Microsoft;Microsoft;Microsoft,33;5;-1;-1;-1,30;13;-1;-1;-1,3;8
5460,5460,5460,5460,5460,5460,5460,5460,ICLR,2020,Shape Features Improve General Model Robustness,Chaowei Xiao;Mingjie Sun;Haonan Qiu;Han Liu;Mingyan Liu;Bo Li,xiaocw@umich.edu;mingjies@andrew.cmu.edu;haonanqiu@link.cuhk.edu.cn;hanliu@northwestern.edu;mingyan@umich.ed;lxbosky@gmail.com,1;1;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,University of Michigan;Carnegie Mellon University;Tsinghua University;Northwestern University;;University of California Berkeley,8;1;8;44;-1;5,21;27;23;22;-1;13,5;4
5461,5461,5461,5461,5461,5461,5461,5461,ICLR,2020,The Secret Revealer: Generative Model Inversion Attacks Against Deep Neural Networks,Yuheng Zhang;Ruoxi Jia;Hengzhi Pei;Wenxiao Wang;Bo Li;Dawn Song,16307130075@fudan.edu.cn;ruoxijia@berkeley.edu;hzpei16@fudan.edu.cn;wangwx16@mails.tsinghua.edu.cn;lxbosky@gmail.com,6;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Fudan University;University of California Berkeley;Fudan University;Tsinghua University;University of California Berkeley,79;5;79;8;5,109;13;109;23;13,5;4;1;2
5462,5462,5462,5462,5462,5462,5462,5462,ICLR,2020,Common sense and Semantic-Guided Navigation via Language in Embodied Environments,Dian Yu;Chandra Khatri;Alexandros Papangelis;Mahdi Namazifar;Andrea Madotto;Huaixiu Zheng;Gokhan Tur,dian.yu@uber.com;chandrak@uber.com;apapangelis@uber.com;mahdin@uber.com;amadotto@connect.ust.hk;huaixiu.zheng@uber.com;gokhan@uber.com,1;1;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,0,,yes,9/25/19,Uber;Uber;Uber;Uber;The Hong Kong University of Science and Technology;Uber;Uber,-1;-1;-1;-1;39;-1;-1,-1;-1;-1;-1;47;-1;-1,
5463,5463,5463,5463,5463,5463,5463,5463,ICLR,2020,TransINT: Embedding Implication Rules in Knowledge Graphs with Isomorphic Intersections of Linear Subspaces,So Yeon Min;Preethi Raghavan;Peter Szolovits,symin95@mit.edu;praghav@us.ibm.com;psz@mit.edu,3;3;1,I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,2,3,,yes,9/25/19,Massachusetts Institute of Technology;International Business Machines;Massachusetts Institute of Technology,2;-1;2,5;-1;5,10
5464,5464,5464,5464,5464,5464,5464,5464,ICLR,2020,Meta Label Correction for Learning with Weak Supervision,Guoqing Zheng;Ahmed Hassan Awadallah;Susan Dumais,zheng@microsoft.com;hassanam@microsoft.com;sdumais@microsoft.com,3;3;8;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,1,,yes,9/25/19,Microsoft;Microsoft;Microsoft,-1;-1;-1,-1;-1;-1,6
5465,5465,5465,5465,5465,5465,5465,5465,ICLR,2020,Towards Effective and Efficient Zero-shot Learning by Fine-tuning with  Task Descriptions,Tian Jin*;Zhun Liu*;Shengjia Yan;Alexandre Eichenberger;Louis-Philippe Morency,tian.jin1@ibm.com;zhunl@andrew.cmu.edu;sjyan@nyu.edu;alexe@us.ibm.com;morency@cs.cmu.edu,3;1;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,International Business Machines;Carnegie Mellon University;New York University;International Business Machines;Carnegie Mellon University,-1;1;25;-1;1,-1;27;29;-1;27,3;6
5466,5466,5466,5466,5466,5466,5466,5466,ICLR,2020,Multi-hop Question Answering via Reasoning Chains,Jifan Chen;Shih-ting Lin;Greg Durrett,jfchen@cs.utexas.edu;j0717lin@gmail.com;gdurrett@cs.utexas.edu,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,4,,yes,9/25/19,"University of Texas, Austin;;University of Texas, Austin",22;-1;22,38;-1;38,
5467,5467,5467,5467,5467,5467,5467,5467,ICLR,2020,Factorized Multimodal Transformer for Multimodal Sequential Learning,Amir Zadeh;Chengfeng Mao;Jiaxin Shi;Yiwei Zhang;Paul Pu Liang;Soujanya Poria;Louis-Philippe Morency,abagherz@andrew.cmu.edu;chengfem@andrew.cmu.edu;jiaxins1@andrew.cmu.edu;yiweizh2@andrew.cmu.edu;pliang@cs.cmu.edu;sporia@ntu.edu.sg;morency@cs.cmu.edu,1;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;National Taiwan University;Carnegie Mellon University,1;1;1;1;1;86;1,27;27;27;27;27;120;27,8
5468,5468,5468,5468,5468,5468,5468,5468,ICLR,2020,Faster and Just As Accurate: A Simple Decomposition for Transformer Models,Qingqing Cao;Harsh Trivedi;Aruna Balasubramanian;Niranjan Balasubramanian,qicao@cs.stonybrook.edu;hjtrivedi@cs.stonybrook.edu;arunab@cs.stonybrook.edu;niranjan@cs.stonybrook.edu,6;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Withdrawn,1,3,,yes,9/25/19,"State University of New York, Stony Brook;State University of New York, Stony Brook;State University of New York, Stony Brook;State University of New York, Stony Brook",41;41;41;41,304;304;304;304,3
5469,5469,5469,5469,5469,5469,5469,5469,ICLR,2020,Learning Function-Specific Word Representations,Daniela Gerz;Ivan Vulić;Marek Rei;Roi Reichart;Anna Korhonen,dsg40@cam.ac.uk;iv250@cam.ac.uk;marek.rei@cl.cam.ac.uk;roiri@technion.ac.il;alk23@cam.ac.uk,3;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,University of Cambridge;University of Cambridge;University of Cambridge;Technion;University of Cambridge,71;71;71;26;71,3;3;3;412;3,
5470,5470,5470,5470,5470,5470,5470,5470,ICLR,2020,Attention over Parameters for Dialogue Systems,Andrea Madotto;Zhaojiang Lin;Chien-Sheng Wu;Jamin Shin;Pascale Fung,amadotto@connect.ust.hk;zlinao@connect.ust.hk;wu.jason@salesforce.com;jay.shin@connect.ust.hk;pascale@ece.ust.hk,1;3;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,The Hong Kong University of Science and Technology;The Hong Kong University of Science and Technology;SalesForce.com;The Hong Kong University of Science and Technology;The Hong Kong University of Science and Technology,39;39;-1;39;39,47;47;-1;47;47,
5471,5471,5471,5471,5471,5471,5471,5471,ICLR,2020,"RL-ST: Reinforcing Style, Fluency and Content Preservation for Unsupervised Text Style Transfer",Bhargav Upadhyay;Akhilesh Sudhakar;Arjun Maheswaran,bhargav@agaralabs.com;akhilesh@agaralabs.com;arjun@agaralabs.com,3;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:N/A:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Agara;Agara;Agara,-1;-1;-1,-1;-1;-1,3
5472,5472,5472,5472,5472,5472,5472,5472,ICLR,2020,Extreme Language Model Compression with Optimal Subwords and Shared Projections,Sanqiang Zhao;Raghav Gupta;Yang Song;Denny Zhou,sanqiang.zhao@pitt.edu;raghavgupta@google.com;yangso@google.com;dennyzhou@google.com,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,3,,yes,9/25/19,University of Pittsburgh;Google;Google;Google,79;-1;-1;-1,113;-1;-1;-1,3
5473,5473,5473,5473,5473,5473,5473,5473,ICLR,2020,Distilling the Knowledge of BERT for Text Generation,Yen-Chun Chen;Zhe Gan;Yu Cheng;Jingzhou Liu;Jingjing Liu,yen-chun.chen@microsoft.com;zhe.gan@microsoft.com;yu.cheng@microsoft.com;jingzhol@andrew.cmu.edu;jingjl@microsoft.com,1;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Microsoft;Microsoft;Microsoft;Carnegie Mellon University;Microsoft,-1;-1;-1;1;-1,-1;-1;-1;27;-1,3
5474,5474,5474,5474,5474,5474,5474,5474,ICLR,2020,DCTD: Deep Conditional Target Densities for Accurate Regression,Fredrik K. Gustafsson;Martin Danelljan;Goutam Bhat;Thomas B. Schön,fredrik.gustafsson@it.uu.se;martin.danelljan@vision.ee.ethz.ch;goutam.bhat@vision.ee.ethz.ch;thomas.schon@it.uu.se,3;6;1,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Uppsala University;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Uppsala University,154;10;10;154,102;13;13;102,2
5475,5475,5475,5475,5475,5475,5475,5475,ICLR,2020,A Gradient-based Architecture HyperParameter Optimization Approach,Zechun Liu;Xiangyu Zhang;Zhe Li;Yichen Wei;Kwang-Ting Cheng;Jian Sun,zliubq@connect.ust.hk;zhangxiangyu@megvii.com;lizhe@megvii.com;weiyichen@megvii.com;timcheng@ust.hk;sunjian@megvii.com,3;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Withdrawn,0,0,,yes,9/25/19,The Hong Kong University of Science and Technology;Megvii Technology Inc.;Megvii Technology Inc.;Megvii Technology Inc.;The Hong Kong University of Science and Technology;Megvii Technology Inc.,39;-1;-1;-1;39;-1,47;-1;-1;-1;47;-1,
5476,5476,5476,5476,5476,5476,5476,5476,ICLR,2020,Generalizing Natural Language Analysis through Span-relation Representations,Zhengbao Jiang;Wei Xu;Jun Araki;Graham Neubig,zhengbaj@cs.cmu.edu;weixu@cse.ohio-state.edu;jun.araki@us.bosch.com;gneubig@cs.cmu.edu,3;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,1,,yes,9/25/19,Carnegie Mellon University;;Bosch;Carnegie Mellon University,1;-1;-1;1,27;-1;-1;27,3
5477,5477,5477,5477,5477,5477,5477,5477,ICLR,2020,On the Distribution of Penultimate Activations of Classification Networks,Minkyo Seo;Yoonho Lee;Suha Kwak,mkseo@postech.ac.kr;einet89@gmail.com;suha.kwak@postech.ac.kr,1;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,POSTECH;;POSTECH,118;-1;118,146;-1;146,5
5478,5478,5478,5478,5478,5478,5478,5478,ICLR,2020,Learning to Sit: Synthesizing Human-Chair Interactions via Hierarchical Control,Yu-Wei Chao;Jimei Yang;Weifeng Chen;Jia Deng,ychao@nvidia.com;jimyang@adobe.com;wfchen@umich.edu;jiadeng@princeton.edu,3;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,NVIDIA;Adobe Systems;University of Michigan;Princeton University,-1;-1;8;31,-1;-1;21;6,
5479,5479,5479,5479,5479,5479,5479,5479,ICLR,2020,"Unsupervised Few-shot Object Recognition by Integrating Adversarial, Self-supervision, and Deep Metric Learning of Latent Parts",Khoi Nguyen;Sinisa Todorovic,nguyenkh@oregonstate.edu;sinisa@oregonstate.edu,1;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,0,,yes,9/25/19,Oregon State University;Oregon State University,77;77,373;373,5;4;6
5480,5480,5480,5480,5480,5480,5480,5480,ICLR,2020,BERT Wears GloVes: Distilling Static Embeddings from Pretrained Contextual Representations,Rishi Bommasani;Kelly Davis;Claire Cardie,rb724@cornell.edu;kdavis@mozilla.com;cardie@cs.cornell.edu,3;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Cornell University;Mozilla;Cornell University,7;-1;7,19;-1;19,3
5481,5481,5481,5481,5481,5481,5481,5481,ICLR,2020,Mixed Setting Training Methods for Incremental Slot-Filling Tasks,Daniel C. Michelin;Jonathan K. Kummerfeld;Kevin Leach;Stefan Larson;Yunqi Zhang;Joeseph J. Peper,daniel@clinc.com;jkk@clinc.com;kevin.leach@clinc.com;slars@clinc.com;yunqi@clinc.com;joe@clinc.com,1;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Clinc;Clinc;Clinc;Clinc;Clinc;Clinc,233;233;233;233;233;233,-1;-1;-1;-1;-1;-1,
5482,5482,5482,5482,5482,5482,5482,5482,ICLR,2020,PLEX: PLanner and EXecutor for Embodied Learning in Navigation,Gil Avraham;Yan Zuo;Tom Drummond,gil.avraham@monash.edu;yan.zuo@monash.edu;tom.drummond@monash.edu,3;1;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Monash University;Monash University;Monash University,118;118;118,75;75;75,
5483,5483,5483,5483,5483,5483,5483,5483,ICLR,2020,UniLoss: Unified Surrogate Loss by Adaptive Interpolation,Lanlan Liu;Mingzhe Wang;Jia Deng,llanlan@umich.edu;mingzhew@cs.princeton.edu;jiadeng@princeton.edu,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,University of Michigan;Princeton University;Princeton University,8;31;31,21;6;6,
5484,5484,5484,5484,5484,5484,5484,5484,ICLR,2020,MobileBERT: Task-Agnostic Compression of BERT by Progressive Knowledge Transfer,Zhiqing Sun;Hongkun Yu;Xiaodan Song;Renjie Liu;Yiming Yang;Denny Zhou,zhiqings@andrew.cmu.edu;hongkuny@google.com;xiaodansong@google.com;renjieliu@google.com;yiming@cs.cmu.edu;dennyzhou@google.com,6;3;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,8,,yes,9/25/19,Carnegie Mellon University;Google;Google;Google;Carnegie Mellon University;Google,1;-1;-1;-1;1;-1,27;-1;-1;-1;27;-1,3
5485,5485,5485,5485,5485,5485,5485,5485,ICLR,2020,Interactive Classification by Asking Informative Questions,Lili Yu;Howard Chen;Sida I. Wang;Yoav Artzi;Tao Lei,liliyu@asapp.com;hchen@asapp.com;sidaw@cs.princeton.edu;yoav@cs.cornell.edu;tao@asapp.com,6;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,ASAPP Inc;ASAPP Inc;Princeton University;Cornell University;ASAPP Inc,-1;-1;31;7;-1,-1;-1;6;19;-1,3
5486,5486,5486,5486,5486,5486,5486,5486,ICLR,2020,Cross-Lingual Vision-Language Navigation,An Yan;Xin Wang;Jiangtao Feng;Lei Li;William Wang,ayan@ucsd.edu;xwang@cs.ucsb.edu;fengjiangtao@bytedance.com;lileilab@bytedance.com;william@cs.ucsb.edu,6;3;1,I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,"University of California, San Diego;UC Santa Barbara;Bytedance;Bytedance;UC Santa Barbara",11;38;-1;-1;38,31;57;-1;-1;57,3;4;6
5487,5487,5487,5487,5487,5487,5487,5487,ICLR,2020,Single Deep Counterfactual Regret Minimization,Eric Steinberger,ericsteinberger.est@gmail.com,3;6;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,Massachusetts Institute of Technology,2,5,
5488,5488,5488,5488,5488,5488,5488,5488,ICLR,2020,I love your chain mail! Making knights smile in a fantasy game world,Shrimai Prabhumoye;Margaret Li;Jack Urbanek;Emily Dinan;Douwe Kiela;Jason Weston;Arthur Szlam,sprabhum@cs.cmu.edu;margaretli@fb.com;jju@fb.com;edinan@fb.com;dkiela@fb.com;jase@fb.com;aszlam@fb.com,3;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,0,,yes,9/25/19,Carnegie Mellon University;Facebook;Facebook;Facebook;Facebook;Facebook;Facebook,1;-1;-1;-1;-1;-1;-1,27;-1;-1;-1;-1;-1;-1,
5489,5489,5489,5489,5489,5489,5489,5489,ICLR,2020,DOUBLE-HARD DEBIASING: TAILORING WORD EMBEDDINGS FOR GENDER BIAS MITIGATION,Tianlu Wang;Xi Victoria Lin;Nazneen Fatema Rajani;Vicente Ordonez;Caimng Xiong,tianlu@virginia.edu;xilin@salesforce.com;nazneen.rajani@salesforce.com;vicente@virginia.edu;cxiong@salesforce.com,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Withdrawn,0,0,,yes,9/25/19,University of Virginia;SalesForce.com;SalesForce.com;University of Virginia;SalesForce.com,59;-1;-1;59;-1,107;-1;-1;107;-1,3;7
5490,5490,5490,5490,5490,5490,5490,5490,ICLR,2020,Joint text classification on multiple levels with multiple labels,Miruna Pîslar;Marek Rei,miruna.pislar@gmail.com;marek.rei@cl.cam.ac.uk,1;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,University of Cambridge;University of Cambridge,71;71,3;3,3;6
5491,5491,5491,5491,5491,5491,5491,5491,ICLR,2020,Discrete Transformer,Jambay Kinley;Yuntian Deng;Alexander M. Rush,j_kinley@college.harvard.edu;dengyuntian@seas.harvard.edu;arush@cornell.edu,3;3;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,Harvard University;Harvard University;Cornell University,39;39;7,7;7;19,3
5492,5492,5492,5492,5492,5492,5492,5492,ICLR,2020,Pragmatic Evaluation of Adversarial Examples in Natural Language,John Morris;Eli Lifland;Ji Gao;Jack Lanchantin;Yangfeng Ji;Yanjun Qi,jm8wx@virginia.edu;edl9cy@virginia.edu;jg6yd@virginia.edu;jjl5sw@virginia.edu;yj3fs@virginia.edu;yq2h@virginia.edu,6;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Withdrawn,1,3,,yes,9/25/19,University of Virginia;University of Virginia;University of Virginia;University of Virginia;University of Virginia;University of Virginia,59;59;59;59;59;59,107;107;107;107;107;107,3;4
5493,5493,5493,5493,5493,5493,5493,5493,ICLR,2020,Question Generation from Paragraphs: A Tale of Two Hierarchical Models,Vishwajeet Kumar;Raktim Chaki;Sai Teja Talluri;Ganesh Ramakrishnan;Yuan-Fang Li;Gholamreza Haffari,vishwajeet@cse.iitb.ac.in;raktimchaki@cse.iitb.ac.in;saiteja.talluri@gmail.com;ganesh@cse.iitb.ac.in;yuanfang.li@monash.edu;gholamreza.haffari@monash.edu,1;1;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Indian Institute of Technology Bombay;Indian Institute of Technology Bombay;Indian Institute of Technology Bombay;Indian Institute of Technology Bombay;Monash University;Monash University,118;118;118;118;118;118,480;480;480;480;75;75,
5494,5494,5494,5494,5494,5494,5494,5494,ICLR,2020,Should All Cross-Lingual Embeddings Speak English?,Antonios Anastasopoulos;Graham Neubig,aanastas@andrew.cmu.edu;gneubig@cs.cmu.edu,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University,1;1,27;27,3
5495,5495,5495,5495,5495,5495,5495,5495,ICLR,2020,Couple-VAE: Mitigating the Encoder-Decoder Incompatibility in Variational Text Modeling with Coupled Deterministic Networks,Chen Wu;Prince Zizhuang Wang;William Yang Wang,wu-c16@mails.tsinghua.edu.cn;zizhuang_wang@ucsb.edu;william@cs.ucsb.edu,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,Tsinghua University;UC Santa Barbara;UC Santa Barbara,8;38;38,23;57;57,5
5496,5496,5496,5496,5496,5496,5496,5496,ICLR,2020,Task-agnostic Continual Learning via Growing Long-Term Memory Networks,Germán Kruszewski;Ionut Teodor Sorodoc;Tomas Mikolov,germank@gmail.com;ionutteodor.sorodoc@upf.edu;tmikolov@fb.com,6;6;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:N/A:N/A,Withdrawn,0,0,,yes,9/25/19,;Universitat Pompeu Fabra;Facebook,-1;481;-1,-1;141;-1,3;2
5497,5497,5497,5497,5497,5497,5497,5497,ICLR,2020,Exploring the Pareto-Optimality between Quality and Diversity in Text Generation,Jianing Li;Yanyan Lan;Jiafeng Guo;Xueqi Cheng,lijianing@ict.ac.cn;lanyanyan@ict.ac.cn;guojiafeng@ict.ac.cn;cxq@ict.ac.cn,1;3;3,I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,"Institute of Computing Technology, Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences",59;59;59;59,1397;1397;1397;1397,1
5498,5498,5498,5498,5498,5498,5498,5498,ICLR,2020,An Empirical Study of Encoders and Decoders in Graph-Based Dependency Parsing,Ge Wang;Ziyuan Hu;Zechuan Hu;Kewei Tu,wangge@shanghaitech.edu.cn;huzy@shanghaitech.edu.cn;huzch@shanghaitech.edu.cn;tukw@shanghaitech.edu.cn,3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,0,,yes,9/25/19,ShanghaiTech University;ShanghaiTech University;ShanghaiTech University;ShanghaiTech University,481;481;481;481,1397;1397;1397;1397,
5499,5499,5499,5499,5499,5499,5499,5499,ICLR,2020,Hierarchical Summary-to-Article Generation,Wangchunshu Zhou;Tao Ge;Ke Xu;Furu Wei;Ming Zhou,v-waz@microsoft.com;tage@microsoft.com;kexu@nlsde.buaa.edu.cn;fuwei@microsoft.com;mingzhou@microsoft.com,3;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,4,,yes,9/25/19,Microsoft;Microsoft;Beihang University;Microsoft;Microsoft,-1;-1;118;-1;-1,-1;-1;594;-1;-1,3
5500,5500,5500,5500,5500,5500,5500,5500,ICLR,2020,Anomaly Detection by Deep Direct Density Ratio Estimation,Masahiro Abe;Masashi Sugiyama,masahiro.abe@d2c.co.jp;sugi@k.u-tokyo.ac.jp,3;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,;The University of Tokyo,-1;56,-1;36,
5501,5501,5501,5501,5501,5501,5501,5501,ICLR,2020,Generating Biased Datasets for Neural Natural Language Processing,Alvin Chan;Yi Tay;Yew Soon Ong;Aston Zhang,guoweial001@e.ntu.edu.sg;ytay017@e.ntu.edu.sg;asysong@ntu.edu.sg;astonz@amazon.com,3;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,National Taiwan University;National Taiwan University;National Taiwan University;Amazon,86;86;86;-1,120;120;120;-1,3;5;7
5502,5502,5502,5502,5502,5502,5502,5502,ICLR,2020,3D-SIC: 3D Semantic Instance Completion for RGB-D Scans,Ji Hou;Angela Dai;Matthias Niessner,ji.hou@tum.de;angela.dai@tum.de;niessner@tum.de,6;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,0,,yes,9/25/19,Technical University Munich;Technical University Munich;Technical University Munich,53;53;53,43;43;43,
5503,5503,5503,5503,5503,5503,5503,5503,ICLR,2020,Towards Holistic and Automatic Evaluation of Open-Domain Dialogue Generation,Bo Pang;Erik Nijkamp;Wenjuan Han;Alex Zhou,bopang@g.ucla.edu;erik.nijkamp@gmail.com;hanwj0309@gmail.com;alexzhou907@gmail.com,1;1;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,"University of California, Los Angeles;;;",20;-1;-1;-1,17;-1;-1;-1,3
5504,5504,5504,5504,5504,5504,5504,5504,ICLR,2020,Semi-Supervised Semantic Dependency Parsing Using CRF Autoencoders,Zixia Jia;Youmi Ma;Jiong Cai;Kewei Tu,jiazx@shanghaitech.edu.cn;maym@shanghaitech.edu.cn;caijiong@shanghaitech.edu.cn;tukw@shanghaitech.edu.cn,3;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,ShanghaiTech University;ShanghaiTech University;ShanghaiTech University;ShanghaiTech University,481;481;481;481,1397;1397;1397;1397,5;10
5505,5505,5505,5505,5505,5505,5505,5505,ICLR,2020,A Syntax-Aware Approach for Unsupervised Text Style Transfer,Yun Ma;Yangbin Chen;Xudong Mao;Qing Li,mayun371@gmail.com;robinchen2-c@my.cityu.edu.hk;xudong.xdmao@gmail.com;qing-prof.li@polyu.edu.hk,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,The Hong Kong Polytechnic University;City University of Hong Kong;The Hong Kong Polytechnic University;The Hong Kong Polytechnic University,172;92;172;172,171;35;171;171,
5506,5506,5506,5506,5506,5506,5506,5506,ICLR,2020,Contextualized Sparse Representation with Rectified N-Gram Attention for Open-Domain Question Answering,Jinhyuk Lee;Minjoon Seo;Hannaneh Hajishirzi;Jaewoo Kang,jinhyuk_lee@korea.ac.kr;minjoon@cs.washington.edu;hannaneh@washington.edu;kangj@korea.ac.kr,6;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,4,,yes,9/25/19,Korea University;University of Washington;University of Washington;Korea University,323;6;6;323,179;26;26;179,
5507,5507,5507,5507,5507,5507,5507,5507,ICLR,2020,Recurrent Layer Attention Network,Eunseok Kim;Inwook Shim,eunseok1117@gmail.com;inugi00@gmail.com,1;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Withdrawn,2,1,,yes,9/25/19,Agency for Defense Development;,-1;-1,-1;-1,2
5508,5508,5508,5508,5508,5508,5508,5508,ICLR,2020,Super-AND: A Holistic Approach to Unsupervised Embedding Learning,Sungwon Han;Yizhan Xu;Sungwon Park;Meeyoung Cha;Cheng-Te Li,lion4151@kaist.ac.kr;re6071020@gs.ncku.edu.tw;psw0416@kaist.ac.kr;mcha@ibs.re.kr;chengte@mail.ncku.edu.tw,1;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,1,,yes,9/25/19,Korea Advanced Institute of Science and Technology;Peking University;Korea Advanced Institute of Science and Technology;Institute for Basic Science;Peking University,481;22;481;-1;22,110;24;110;-1;24,
5509,5509,5509,5509,5509,5509,5509,5509,ICLR,2020,Natural Language State Representation for Reinforcement Learning,Erez Schwartz;Guy Tennenholtz;Chen Tessler;Shie Mannor,erezschwartz@campus.technion.ac.il;sguyt@campus.technion.ac.il;chen.tessler@gmail.com;shiemannor@gmail.com,1;1;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,1,,yes,9/25/19,Technion;Technion;Technion;Technion,26;26;26;26,412;412;412;412,3
5510,5510,5510,5510,5510,5510,5510,5510,ICLR,2020,BEAN: Interpretable Representation Learning with Biologically-Enhanced Artificial Neuronal Assembly Regularization,Yuyang Gao;Giorgio Ascoli;Liang Zhao,ygao13@gmu.edu;ascoli@gmu.edu;lzhao9@gmu.edu,3;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,George Mason University;George Mason University;George Mason University,100;100;100,282;282;282,
5511,5511,5511,5511,5511,5511,5511,5511,ICLR,2020,Revisiting Fine-tuning for Few-shot Learning,Akihiro Nakamura;Tatsuya Harada,nakamura@mi.t.u-tokyo.ac.jp;harada@mi.t.u-tokyo.ac.jp,1;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,The University of Tokyo;The University of Tokyo,56;56,36;36,6
5512,5512,5512,5512,5512,5512,5512,5512,ICLR,2020,Unrestricted Adversarial Attacks For Semantic Segmentation,Guangyu Shen;Chengzhi Mao;Junfeng Yang;Baishakhi Ray,shen447@purdue.edu;cm3797@columbia.edu;junfeng@cs.columbia.edu;rayb@cs.columbia.edu,6;6;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Purdue University;Columbia University;Columbia University;Columbia University,27;15;15;15,88;16;16;16,5;4;2
5513,5513,5513,5513,5513,5513,5513,5513,ICLR,2020,End-to-End Multi-Domain Task-Oriented Dialogue Systems with Multi-level Neural Belief Tracker,Hung Le;Doyen Sahoo;Chenghao Liu;Nancy F. Chen;Steven C.H. Hoi,l.hung1610@gmail.com;dsahoo@salesforce.com;chliu@smu.edu.sg;nfychen@i2r.a-star.edu.sg;shoi@salesforce.com,3;3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,0,,yes,9/25/19,Singapore Management University;SalesForce.com;Singapore Management University;A*STAR;SalesForce.com,92;-1;92;-1;-1,1397;-1;1397;-1;-1,
5514,5514,5514,5514,5514,5514,5514,5514,ICLR,2020,Accelerate DNN Inference  By Inter-Operator Parallelization,Yaoyao Ding;Ligeng Zhu;Zhihao Jia;Song Han,yyding@mit.edu;ligeng@mit.edu;zhihao@cs.stanford.edu;songhan@mit.edu,3;1;3,I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,0,,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Stanford University;Massachusetts Institute of Technology,2;2;4;2,5;5;4;5,
5515,5515,5515,5515,5515,5515,5515,5515,ICLR,2020,Posterior Control of Blackbox Generation ,Xiang Lisa Li;Alexander M. Rush,xli150@jhu.edu;srush@seas.harvard.edu,3;3;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Johns Hopkins University;Harvard University,73;39,12;7,3
5516,5516,5516,5516,5516,5516,5516,5516,ICLR,2020,Learning to Generate 3D Training Data through Hybrid Gradient,Dawei Yang;Jia Deng,ydawei@umich.edu;jiadeng@princeton.edu,8;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,University of Michigan;Princeton University,8;31,21;6,10
5517,5517,5517,5517,5517,5517,5517,5517,ICLR,2020,Learning to Learn via Gradient Component Corrections,Christian Simon;Piotr Koniusz;Richard Nock;Mehrtash Harandi,christian.simon@anu.edu.au;peter.koniusz@data61.csiro.au;richard.nock@data61.csiro.au;mehrtash.harandi@monash.edu,1;3;3,I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,"Australian National University;, CSIRO;, CSIRO;Monash University",108;233;233;118,50;-1;-1;75,6
5518,5518,5518,5518,5518,5518,5518,5518,ICLR,2020,Counting the Paths in Deep Neural Networks as a Performance Predictor,Michele Sasdelli;Ian Reid;Gustavo Carneiro,michele.sasdelli@adelaide.edu.au;ian.reid@adelaide.edu.au;gustavo.carneiro@adelaide.edu.au,3;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,The University of Adelaide;The University of Adelaide;The University of Adelaide,128;128;128,120;120;120,10
5519,5519,5519,5519,5519,5519,5519,5519,ICLR,2020,VUSFA:Variational Universal Successor Features Approximator ,Shamane Siriwardhana;Rivindu Weerasakera;Denys J.C. Matthies;Suranga Nanayakkara,shamane@ahlab.org;rivindu@ahlab.org;denys@ahlab.org;suranga@ahlab.org,1;1;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,3,,yes,9/25/19,University of Auckland;;;,266;-1;-1;-1,177;-1;-1;-1,6
5520,5520,5520,5520,5520,5520,5520,5520,ICLR,2020,Data Annealing Transfer learning Procedure for Informal Language Understanding Tasks,Jing Gu;Yu Zhou,jkgu@ucdavis.edu;joyu@ucdavis.edu,3;3;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,"University of California, Davis;University of California, Davis",79;79,55;55,3;6
5521,5521,5521,5521,5521,5521,5521,5521,ICLR,2020,Building Hierarchical Interpretations in Natural Language via Feature Interaction Detection,Hanjie Chen;Guangtao Zheng;Yangfeng Ji,hc9mx@virginia.edu;gz5hp@virginia.edu;yangfeng@virginia.edu,3;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,University of Virginia;University of Virginia;University of Virginia,59;59;59,107;107;107,3
5522,5522,5522,5522,5522,5522,5522,5522,ICLR,2020,Mem2Mem: Learning to Summarize Long Texts with Memory-to-Memory Transfer,Jaehong Park;Jonathan Pilault;Christopher Pal,jaehong.park@elementai.com;jonathan.pilault@elementai.com;christopher.pal@elementai.com,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,Element AI;Element AI;Element AI,-1;-1;-1,-1;-1;-1,
5523,5523,5523,5523,5523,5523,5523,5523,ICLR,2020,Improving Neural Abstractive Summarization Using Transfer Learning and Factuality-Based Evaluation: Towards Automating Science Journalism,Rumen Dangovski*;Michelle Shen*;Dawson Byrd*;Li Jing*;Preslav Nakov;Marin Soljacic,rumenrd@mit.edu;mcshen99@mit.edu;dbyrd@exeter.edu;ljing@mit.edu;pnakov@qf.org.qa;soljacic@mit.edu,1;1;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Phillips Exeter Academy;Massachusetts Institute of Technology;QCRI;Massachusetts Institute of Technology,2;2;-1;2;-1;2,5;5;-1;5;-1;5,6
5524,5524,5524,5524,5524,5524,5524,5524,ICLR,2020,Higher-order Weighted Graph Convolutional Networks,Songtao Liu;Lingwei Chen;Hanze Dong;Zihao Wang;Dinghao Wu;Zengfeng Huang,stliu15@fudan.edu.cn;lvc5613@psu.edu;hdongaj@ust.hk;zzw166@psu.edu;duw12@psu.edu;huangzf@fudan.edu.cn,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,Fudan University;Pennsylvania State University;The Hong Kong University of Science and Technology;Pennsylvania State University;Pennsylvania State University;Fudan University,79;41;39;41;41;79,109;78;47;78;78;109,10
5525,5525,5525,5525,5525,5525,5525,5525,ICLR,2020,Distilling Neural Networks for Faster and Greener Dependency Parsing,Mark Anderson;Carlos Gómez-Rodríguez,mark.anderson.nlp@gmail.com;carlos.gomez@udc.es,3;6;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,5,,yes,9/25/19,Universidade da Coruña;Universidade da Coruña,-1;-1,-1;-1,3
5526,5526,5526,5526,5526,5526,5526,5526,ICLR,2020,From Here to There: Video Inbetweening Using Direct 3D Convolutions,Yunpeng Li;Dominik Roblek;Marco Tagliasacchi,yunpeng@google.com;droblek@google.com;mtagliasacchi@google.com,3;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,Google;Google;Google,-1;-1;-1,-1;-1;-1,4
5527,5527,5527,5527,5527,5527,5527,5527,ICLR,2020,ON SOLVING COOPERATIVE DECENTRALIZED MARL PROBLEMS WITH SPARSE REINFORCEMENTS,Rajiv Ranjan Kumar;Pradeep Varakantham,rajivrk.2017@phdis.smu.edu.sg;pradeepv@smu.edu.sg,1;6;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Withdrawn,0,0,,yes,9/25/19,Singapore Management University;Singapore Management University,92;92,1397;1397,
5528,5528,5528,5528,5528,5528,5528,5528,ICLR,2020,On the Anomalous Generalization of GANs,Jinchen Xuan;Yunchang Yang;Ze Yang;Di He;Liwei Wang,1600012865@pku.edu.cn;1500010650@pku.edu.cn;yangze@pku.edu.cn;dihe@microsoft.com;wanglw@cis.pku.edu.cn,1;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Withdrawn,0,0,,yes,9/25/19,Peking University;Peking University;Peking University;Microsoft;Peking University,22;22;22;-1;22,24;24;24;-1;24,5;4;8
5529,5529,5529,5529,5529,5529,5529,5529,ICLR,2020,Bridging the domain gap in cross-lingual document classification,Guokun Lai;Barlas Oguz;Yiming Yang;Veselin Stoyanov,guokun@cs.cmu.edu;barlaso@fb.com;yiming@cs.cmu.edu;ves@fb.com,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:N/A:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Carnegie Mellon University;Facebook;Carnegie Mellon University;Facebook,1;-1;1;-1,27;-1;27;-1,3
5530,5530,5530,5530,5530,5530,5530,5530,ICLR,2020,NAMSG: An Efficient Method for Training Neural Networks,Yushu Chen;Hao Jing;Wenlai Zhao;Zhiqiang Liu;Ouyi Li;Liang Qiao;Haohuan Fu;Wei Xue;Guangwen Yang,yschen11@126.com;jinghao0320@gmail.com;cryinlaugh@gmail.com;gt_liuzq@163.com;18801087946@163.com;qiaoliang6363@163.com;haohuan@tsinghua.edu.cn;xuewei@mail.tsinghua.edu.cn;ygw@mail.tsinghua.edu.cn,3;3;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Withdrawn,2,4,,yes,9/25/19,126;;;163;163;163;Tsinghua University;Tsinghua University;Tsinghua University,-1;-1;-1;-1;-1;-1;8;8;8,-1;-1;-1;-1;-1;-1;23;23;23,1
5531,5531,5531,5531,5531,5531,5531,5531,ICLR,2020,INTERPRETING CNN  PREDICTION THROUGH  LAYER - WISE SELECTED DISCERNIBLE NEURONS,Md Tauhid Bin Iqbal;Abdul Muqeet;Sung-Ho Bae,tauhidiq@khu.ac.kr;amuqeet@khu.ac.kr;shbae@khu.ac.kr,1;3;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Kyung Hee University;Kyung Hee University;Kyung Hee University,481;481;481,319;319;319,
5532,5532,5532,5532,5532,5532,5532,5532,ICLR,2020,Toward Controllable Text Content Manipulation,Shuai Lin;Wentao Wang;Zhiting Hu;Zichao Yang;Xiaodan Liang;Haoran Shi;Frank Xu;Eric Xing,shuailin97@gmail.com;wwt.cpp@gmail.com;zhitinghu@gmail.com;yangtze2301@gmail.com;xdliang328@gmail.com;haoranshi97@gmail.com;eric.xing@petuum.com,3;1;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,0,,yes,9/25/19,SUN YAT-SEN UNIVERSITY;Peking University;Carnegie Mellon University;;SUN YAT-SEN UNIVERSITY;Carnegie Mellon University;Petuum Inc.,481;22;1;-1;481;1;-1,299;24;27;-1;299;27;-1,
5533,5533,5533,5533,5533,5533,5533,5533,ICLR,2020,Learning Low-rank Deep Neural Networks via Singular Vector Orthogonality Regularization and Singular Value Sparsification,Huanrui Yang;Minxue Tang;Wei Wen;Feng Yan;Daniel Hu;Ang Li;Hai Li,huanrui.yang@duke.edu;tangmx16@mails.tsinghua.edu.cn;wei.wen@duke.edu;fyan@unr.edu;danielhu2003@gmail.com;ang.li630@duke.edu;hai.li@duke.edu,3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,"Duke University;Tsinghua University;Duke University;University of Nevada, Reno;Newport High School;Duke University;Duke University",47;8;47;266;-1;47;47,20;23;20;1397;-1;20;20,
5534,5534,5534,5534,5534,5534,5534,5534,ICLR,2020,FAST LEARNING VIA EPISODIC MEMORY: A PERSPECTIVE FROM ANIMAL DECISION-MAKING,Xiaohan Zhang;Lu Liu;Guodong Long;jing jiang;Shenquan Liu,xh1315255662@gmail.com;lu.liu.cs@icloud.com;guodong.long@uts.edu.au;jing.jiang@uts.edu.au;mashqliu@scut.edu.cn,1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,0,,yes,9/25/19,South China University of Technology;University of Technology Sydney;University of Technology Sydney;University of Technology Sydney;South China University of Technology,481;108;108;108;481,501;193;193;193;501,5
5535,5535,5535,5535,5535,5535,5535,5535,ICLR,2020,Improving and Stabilizing Deep Energy-Based Learning,Lifu Tu;Richard Yuanzhe Pang;Kevin Gimpel,lifu@ttic.edu;yzpang@nyu.edu;kgimpel@ttic.edu,6;1;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Toyota Technological Institute at Chicago;New York University;Toyota Technological Institute at Chicago,-1;25;-1,-1;29;-1,
5536,5536,5536,5536,5536,5536,5536,5536,ICLR,2020,INTERPRETING CNN COMPRESSION USING INFORMATION BOTTLENECK,Hui Xiang;Feifei Shi;Peng Wang;Qigang Wang;Zhongchao Shi,xianghui1@lenovo.com;shiff3@lenovo.com;wangpeng31@lenovo.com;wangqg1@lenovo.com;shizc2@lenovo.com,1;1;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Lenovo Research;Lenovo Research;Lenovo Research;Lenovo Research;Lenovo Research,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,1;8
5537,5537,5537,5537,5537,5537,5537,5537,ICLR,2020,Conversation Generation with Concept Flow,Houyu Zhang;Zhenghao Liu;Chenyan Xiong;Zhiyuan Liu,houyu_zhang@brown.edu;liu-zh16@mails.tsinghua.edu.cn;chenyan.xiong@microsoft.com;liuzy@tsinghua.edu.cn,6;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,0,,yes,9/25/19,Brown University;Tsinghua University;Microsoft;Tsinghua University,67;8;-1;8,53;23;-1;23,10
5538,5538,5538,5538,5538,5538,5538,5538,ICLR,2020,iSOM-GSN: An Integrative Approach for Transforming Multi-omic Data into Gene Similarity Networks via Self-organizing Maps,Nazia Fatima;Johan Fernandes;Luis Rueda,fatiman@uwindsor.ca;ferna11i@uwindsor.ca;lrueda@uwindsor.ca,3;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,University of Windsor;University of Windsor;University of Windsor,-1;-1;-1,-1;-1;-1,10
5539,5539,5539,5539,5539,5539,5539,5539,ICLR,2020,Task-Agnostic Robust Encodings for Combating Adversarial Typos,Erik Jones;Robin Jia;Aditi Raghunathan;Percy Liang,erik.jones313@gmail.com;robinjia@stanford.edu;aditir@stanford.edu;pliang@cs.stanford.edu,3;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,4,,yes,9/25/19,Stanford University;Stanford University;Stanford University;Stanford University,4;4;4;4,4;4;4;4,3;4
5540,5540,5540,5540,5540,5540,5540,5540,ICLR,2020,Natural Language Adversarial Attack and Defense in Word Level,Xiaosen Wang;Hao Jin;Kun He,xiaosen@hust.edu.cn;mailtojinhao@hust.edu.cn;brooklet60@hust.edu.cn,6;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,2,,yes,9/25/19,Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology,39;39;39,47;47;47,3;4;2
5541,5541,5541,5541,5541,5541,5541,5541,ICLR,2020,Randomness in Deconvolutional Networks for Visual Representation,Kun He;Jingbo Wang;Haochuan Li;Yao Shu;Liwei Wang;John E. Hopcroft,brooklet60@hust.edu.cn;jingbow@usc.edu;lhchuan@pku.edu.cn;shuyao95@gmail.com;wanglw@cis.pku.edu.cn;jeh@cs.cornell.edu,1;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Hong Kong University of Science and Technology;University of Southern California;Peking University;National University of Singapore;Peking University;Cornell University,39;31;22;16;22;7,47;62;24;25;24;19,
5542,5542,5542,5542,5542,5542,5542,5542,ICLR,2020,An Inter-Layer Weight Prediction and Quantization for Deep Neural Networks based on Smoothly Varying Weight Hypothesis,Kang-Ho Lee;JoonHyun Jung;Sung-Ho Bae,ho7719@khu.ac.kr;doublejtoh@khu.ac.kr;shbae@khu.ac.kr,3;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Kyung Hee University;Kyung Hee University;Kyung Hee University,481;481;481,319;319;319,
5543,5543,5543,5543,5543,5543,5543,5543,ICLR,2020,Generalized Transformation-based Gradient,Anbang Wu,wab@zju.edu.cn,3;1;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,Zhejiang University,56,107,
5544,5544,5544,5544,5544,5544,5544,5544,ICLR,2020,Learning Multi-facet Embeddings of Phrases and Sentences using Sparse Coding for Unsupervised Semantic Applications,Haw-Shiuan Chang;Amol Agrawal;Andrew McCallum,hschang@cs.umass.edu;amolagrawal@cs.umass.edu;mccallum@cs.umass.edu,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,1,,yes,9/25/19,"University of Massachusetts, Amherst;University of Massachusetts, Amherst;University of Massachusetts, Amherst",28;28;28,209;209;209,3
5545,5545,5545,5545,5545,5545,5545,5545,ICLR,2020,Making DenseNet Interpretable: A Case Study in Clinical Radiology,Kwun Ho Ngan;Artur d'Avila Garcez;Karen M. Knapp;Andy Appelboam;Constantino Carlos Reyes-Aldasoro,kwun-ho.ngan@city.ac.uk;a.garcez@city.ac.uk;k.m.knapp@exeter.ac.uk;andy.appelboam@nhs.net;reyes@city.ac.uk,1;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,"City, University of London;City, University of London;University of Exeter;;City, University of London",266;266;390;-1;266,422;422;146;-1;422,2
5546,5546,5546,5546,5546,5546,5546,5546,ICLR,2020,Learning Classifier Synthesis for Generalized Few-Shot Learning,Han-Jia Ye;Hexiang Hu;De-Chuan Zhan;Fei Sha,yehj@lamda.nju.edu.cn;hexiang.frank.hu@gmail.com;zhandc@nju.edu.cn;feisha@usc.edu,3;3;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,1,,yes,9/25/19,Zhejiang University;University of Southern California;Zhejiang University;University of Southern California,56;31;56;31,107;62;107;62,6
5547,5547,5547,5547,5547,5547,5547,5547,ICLR,2020,Restoration of Video Frames from a Single Blurred Image with Motion Understanding,Dawit Mureja Argaw;Junsik Kim;Francois Rameau;Chaoning Zhang;In so Kweon,dawitmureja@kaist.ac.kr;mibastro@gmail.com;rameau.fr@gmail.com;chaoningzhang1990@gmail.com;iskweon77@kaist.ac.kr,3;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,481;481;481;481;481,110;110;110;110;110,
5548,5548,5548,5548,5548,5548,5548,5548,ICLR,2020,Open-Set Domain Adaptation with Category-Agnostic Clusters,Yingwei Pan;Ting Yao;Yehao Li;Chong-Wah Ngo;Tao Mei,panyw.ustc@gmail.com;tingyao.ustc@gmail.com;yehaoli.sysu@gmail.com;cscwngo@cityu.edu.hk;tmei@live.com,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,JD AI Research;JD AI Research;SUN YAT-SEN UNIVERSITY;City University of Hong Kong;JD AI Research,-1;-1;481;92;-1,-1;-1;299;35;-1,8
5549,5549,5549,5549,5549,5549,5549,5549,ICLR,2020,Understanding Distributional Ambiguity via Non-robust Chance Constraint,Shumin MA;LEUNG Cheuk Hang;Qi WU;Wei Liu,shuminma@cityu.edu.hk;chleung87@cityu.edu.hk;qiwu55@cityu.edu.hk;wl2223@columbia.edu,3;1;3,I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Withdrawn,0,0,,yes,9/25/19,City University of Hong Kong;City University of Hong Kong;City University of Hong Kong;Columbia University,92;92;92;15,35;35;35;16,
5550,5550,5550,5550,5550,5550,5550,5550,ICLR,2020,Simple but effective techniques to reduce dataset biases,Rabeeh Karimi Mahabadi;James Henderson,rkarimi@idiap.ch;james.henderson@idiap.ch,3;6;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,Idiap Research Institute;Idiap Research Institute,-1;-1,-1;-1,3
5551,5551,5551,5551,5551,5551,5551,5551,ICLR,2020,EfferenceNets for latent space planning,Hlynur Davíð Hlynsson;Merlin Schüler;Robin Schiewer;Laurenz Wiskott,hlynurd@gmail.com;merlin.schueler@ini.rub.de;robin.schiewer@ini.rub.de;laurenz.wiskott@ini.rub.de,1;1;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A,Withdrawn,0,0,,yes,9/25/19,;Ruhr-Universtät Bochum;Ruhr-Universtät Bochum;Ruhr-Universtät Bochum,-1;-1;-1;-1,-1;-1;-1;-1,10
5552,5552,5552,5552,5552,5552,5552,5552,ICLR,2020,Mining GANs for knowledge transfer to small domains,Yaxing Wang;Abel Gonzalez-Garcia;David Berga;Luis Herranz;Fahad Shahbaz Khan;Joost van de Weijer,yaxing@cvc.uab.es;agonzalez@cvc.uab.es;dberga@cvc.uab.es;lherranz@cvc.uab.es;fahad.khan@liu.se;joost@cvc.uab.es,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:N/A:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,"Computer Vision Center, Universitat Autònoma de Barcelona;Computer Vision Center, Universitat Autònoma de Barcelona;Computer Vision Center, Universitat Autònoma de Barcelona;Computer Vision Center, Universitat Autònoma de Barcelona;Linköping University;Computer Vision Center, Universitat Autònoma de Barcelona",-1;-1;-1;-1;481;-1,-1;-1;-1;-1;407;-1,5
5553,5553,5553,5553,5553,5553,5553,5553,ICLR,2020,"EMS: End-to-End Model Search for Network Architecture, Pruning and Quantization",Tianzhe Wang;Kuan Wang;Han Cai;Ji Lin;Yujun Lin;Zhijian Liu;Song Han,usedtobe@mit.edu;wangkuan15@mails.tsinghua.edu.cn;hancai@mit.edu;jilin@mit.edu;yujunlin@mit.edu;zhijian@mit.edu;songhan@mit.edu,3;1;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:N/A:I assessed the sensibility of the derivations and theory.,Withdrawn,1,0,,yes,9/25/19,Massachusetts Institute of Technology;Tsinghua University;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;8;2;2;2;2;2,5;23;5;5;5;5;5,
5554,5554,5554,5554,5554,5554,5554,5554,ICLR,2020,Deflecting Adversarial Attacks,Yao Qin;Nicholas Frosst;Colin Raffel;Garrison Cottrell;Geoffrey Hinton,yaq007@eng.ucsd.edu;frosst@google.com;craffel@google.com;gary@eng.ucsd.edu;geoffhinton@google.com,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,2,,yes,9/25/19,"University of California, San Diego;Google;Google;University of California, San Diego;Google",11;-1;-1;11;-1,31;-1;-1;31;-1,4
5555,5555,5555,5555,5555,5555,5555,5555,ICLR,2020,One Demonstration Imitation Learning,Bradly C. Stadie;Siyan Zhao;Qiqi Xu;Bonnie Li;Lunjun Zhang,bstadie@berkeley.edu;siyan.zhao@mail.utoronto.ca;frances.xu@mail.utoronto.ca;bonnieli20010901@gmail.com;lunjun.zhang@mail.utoronto.ca,1;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,University of California Berkeley;Toronto University;Toronto University;McGill University;Toronto University,5;18;18;86;18,13;18;18;42;18,
5556,5556,5556,5556,5556,5556,5556,5556,ICLR,2020,SDNet: Contextualized Attention-based Deep Network for Conversational Question Answering,Chenguang Zhu;Michael Zeng;Xuedong Huang,chezhu@microsoft.com;nzeng@microsoft.com;xdh@microsoft.com,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,0,,yes,9/25/19,Microsoft;Microsoft;Microsoft,-1;-1;-1,-1;-1;-1,
5557,5557,5557,5557,5557,5557,5557,5557,ICLR,2020,Differentially Private Survival Function Estimation,Lovedeep Gondara;Ke Wang,lgondara@sfu.ca;wang@sfu.ca,1;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,Simon Fraser University;Simon Fraser University,64;64,272;272,
5558,5558,5558,5558,5558,5558,5558,5558,ICLR,2020,GPU Memory Management for Deep Neural Networks Using Deep Q-Network,Shicheng Chen,coder.chen.shi.cheng@gmail.com,3;3;1,I do not know much about this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,0,,yes,9/25/19,,,,
5559,5559,5559,5559,5559,5559,5559,5559,ICLR,2020,Meta Decision Trees for Explainable Recommendation Systems,Eyal Shulman;Lior Wolf,shulmaneyal@gmail.com;wolf@fb.com,3;8;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,Tel Aviv University;Facebook,35;-1,188;-1,
5560,5560,5560,5560,5560,5560,5560,5560,ICLR,2020,Parameterized Action Reinforcement Learning for Inverted Index Match Plan Generation,Linfeng Zhao;Lifei Zhu;Qi Chen;Hui Xue;Haidong Wang;Chuanjie Liu;Yuan Liu;Lawson Wong;Lintao Zhang,zhao.linf@husky.neu.edu;v-lifzh@microsoft.com;cheqi@microsoft.com;xuehui@microsoft.com;haidwa@microsoft.com;chuanli@microsoft.com;yuanliu@neu.edu.cn;lawsonlsw@northeastern.edu;lintaoz@microsoft.com,3;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.,Withdrawn,0,5,,yes,9/25/19,Northeastern University;Microsoft;Microsoft;Microsoft;Microsoft;Microsoft;Northeastern University;Northeastern University;Microsoft,16;-1;-1;-1;-1;-1;16;16;-1,906;-1;-1;-1;-1;-1;906;906;-1,
5561,5561,5561,5561,5561,5561,5561,5561,ICLR,2020,Fault Tolerant Reinforcement Learning via A Markov Game of Control and Stopping,David Mguni,davidmguni@hotmail.com,3;1;3,I have read many papers in this area.:N/A:N/A:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Prowler.io,-1,-1,4;1
5562,5562,5562,5562,5562,5562,5562,5562,ICLR,2020,Fuzzing-Based Hard-Label Black-Box Attacks Against Machine Learning Models,Yi Qin;Chuan Yue,yiqin@mines.edu;chuanyue@mines.edu,3;1;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,5,,yes,9/25/19,Colorado School of Mines;Colorado School of Mines,172;172,343;343,4
5563,5563,5563,5563,5563,5563,5563,5563,ICLR,2020,"Read, Highlight and Summarize: A Hierarchical Neural Semantic Encoder-based Approach",Rajeev Bhatt Ambati;Saptarashmi Bandyopadhyay;Prasenjit Mitra,rajeev24811@gmail.com;sbandyo20@gmail.com;pum10@psu.edu,3;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Pennsylvania State University;Pennsylvania State University;Pennsylvania State University,41;41;41,78;78;78,3;8
5564,5564,5564,5564,5564,5564,5564,5564,ICLR,2020,Generalization Puzzles in Deep Networks,Qianli Liao;Brando Miranda;Lorenzo Rosasco;Andrzej Banburski;Robert Liang;Jack Hidary;Tomaso Poggio,lql@mit.edu;miranda9@illinois.edu;lrosasco@mit.edu;kappa666@mit.edu;bobliang345@gmail.com;hidary@google.com;tp@csail.mit.edu,1;1;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,"Massachusetts Institute of Technology;University of Illinois, Urbana Champaign;Massachusetts Institute of Technology;Massachusetts Institute of Technology;;Google;Massachusetts Institute of Technology",2;3;2;2;-1;-1;2,5;48;5;5;-1;-1;5,8
5565,5565,5565,5565,5565,5565,5565,5565,ICLR,2020,Recurrent Chunking Mechanisms for Conversational Machine Reading Comprehension,Hongyu Gong;Yelong Shen;Dian Yu;Jianshu Chen;Dong Yu,hgong6@illinois.edu;yelongshen@tencent.com;yudian@tencent.com;jianshuchen@tencent.com;dyu@tencent.com,6;3;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,8,,yes,9/25/19,"University of Illinois, Urbana Champaign;Tencent AI Lab;Tencent AI Lab;Tencent AI Lab;Tencent AI Lab",3;-1;-1;-1;-1,48;-1;-1;-1;-1,
5566,5566,5566,5566,5566,5566,5566,5566,ICLR,2020,Rethinking Generalized Matrix Factorization for Recommendation: The Importance of Multi-hot Encoding,Lei Feng;Hongxin Wei;Qingyu Guo;Zhuoyi Lin;Bo An,feng0093@e.ntu.edu.sg;owenwei@ntu.edu.sg;qguo005@e.ntu.edu.sg;zhuoyi001@e.ntu.edu.sg;boan@ntu.edu.sg,3;3;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,National Taiwan University;National Taiwan University;National Taiwan University;National Taiwan University;National Taiwan University,86;86;86;86;86,120;120;120;120;120,
5567,5567,5567,5567,5567,5567,5567,5567,ICLR,2020,Extractor-Attention Network: A New Attention Network with Hybrid Encoders for Chinese Text Classification,Junhao Qiu;Ronghua Shi;Fangfang Li (the corresponding author);Jinjing Shi;Wangmin Liao,qiujunhao@csu.edu.cn;shirh@csu.edu.cn;lifangfang@csu.edu.cn;shijinjing@csu.edu.cn;0909123117@csu.edu.cn,1;6;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY,481;481;481;481;481,299;299;299;299;299,
5568,5568,5568,5568,5568,5568,5568,5568,ICLR,2020,Fooling Pre-trained Language Models: An Evolutionary Approach to Generate Wrong Sentences with High Acceptability Score,Marco Di Giovanni;Marco Brambilla,marco.digiovanni@polimi.it;marco.brambilla@polimi.it,3;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Withdrawn,0,6,,yes,9/25/19,Politecnico di Milano;Politecnico di Milano,128;128,347;347,3;4;8
5569,5569,5569,5569,5569,5569,5569,5569,ICLR,2020,The Convex Information Bottleneck Lagrangian,Borja Rodríguez Gálvez;Ragnar Thobaben;Mikael Skoglund,borjarg@kth.se;ragnart@kth.se;skoglund@kth.se,3;3;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,5,,yes,9/25/19,"KTH Royal Institute of Technology, Stockholm, Sweden;KTH Royal Institute of Technology, Stockholm, Sweden;KTH Royal Institute of Technology, Stockholm, Sweden",128;128;128,222;222;222,1
5570,5570,5570,5570,5570,5570,5570,5570,ICLR,2020,Deep Learning-Based Average Consensus,Masako Kishida;Masaki Ogura;Yuichi Yoshida;Tadashi Wadayama,kishida@nii.ac.jp;oguram@is.naist.jp;yyoshida@nii.ac.jp;wadayama@nitech.ac.jp,1;1;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,4,,yes,9/25/19,"Meiji University;Nara Institute of Science and Technology, Japan;Meiji University;Meiji University",481;481;481;481,332;1397;332;332,10
5571,5571,5571,5571,5571,5571,5571,5571,ICLR,2020,STYLE EXAMPLE-GUIDED TEXT GENERATION USING GENERATIVE ADVERSARIAL TRANSFORMERS,Kuo-Hao Zeng;Mohammad Shoeybi;Ming-Yu Liu,khzeng@cs.washington.edu;mshoeybi@nvidia.com;sean.mingyu.liu@gmail.com,3;3;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,University of Washington;NVIDIA;NVIDIA,6;-1;-1,26;-1;-1,5
5572,5572,5572,5572,5572,5572,5572,5572,ICLR,2020,GraphFlow: Exploiting Conversation Flow with Graph Neural Networks for Conversational Machine Comprehension,Yu Chen;Lingfei Wu;Mohammed J. Zaki,cheny39@rpi.edu;lwu@email.wm.edu;zaki@cs.rpi.edu,1;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Rensselaer Polytechnic Institute;College of William and Mary;Rensselaer Polytechnic Institute,172;154;172,438;235;438,10
5573,5573,5573,5573,5573,5573,5573,5573,ICLR,2020,Revisiting the Information Plane,Martin Schiemer;Juan Ye,martin.schiemer@web.de;juan.ye@st-andrews.ac.uk,3;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,0,,yes,9/25/19,University of St Andrews;University of St Andrews,-1;-1,-1;-1,8
5574,5574,5574,5574,5574,5574,5574,5574,ICLR,2020,Fully Quantized Transformer for Improved Translation,Gabriele Prato;Ella Charlaix;Mehdi Rezagholizadeh,prato.gab@gmail.com;ella.charlaix@huawei.com;mehdi.rezagholizadeh@huawei.com,3;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,3,,yes,9/25/19,University of Montreal;Huawei Technologies Ltd.;Huawei Technologies Ltd.,128;-1;-1,85;-1;-1,3
