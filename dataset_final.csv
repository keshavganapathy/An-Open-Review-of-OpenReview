,conference,year,paper,authors,emails,ratings,confidences,decisions,cmt_before_review,cmt_between,cmt_after_decision,double_blinded,submission_date,institution,csranking,ranking,categories,earliest_appearance_date,citations,cite_background,cite_methods,cite_results,twitter_mentions,highly_influenced_papers,authors_citations,authors_publications,authors_h-index,authors_highly_influenced_papers,genders
0,ICLR,2017,Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy,Dougal J. Sutherland;Hsiao-Yu Tung;Heiko Strathmann;Soumyajit De;Aaditya Ramdas;Alex Smola;Arthur Gretton,dougal@gmail.com;htung@cs.cmu.edu;heiko.strathmann@gmail.com;soumyajitde.cse@gmail.com;aramdas@berkeley.edu;alex@smola.org;arthur.gretton@gmail.com,6;8;7,3;3;4,Accept (Poster),2,3,1,no,11/4/16,University College London;Carnegie Mellon University;University College London;;University of California Berkeley;Carnegie-Mellon University;University College London,45;1;45;-1;5;1;45,15;23;15;-1;10;23;15,5;4,11/4/16,109,44,52,1,0,12,734;445;739;104;1268;64889;12995,36;25;25;2;104;389;216,14;8;13;1;19;99;48,100;44;139;12;135;9073;2223,-1;-1
1,ICLR,2017,A Simple but Tough-to-Beat Baseline for Sentence Embeddings,Sanjeev Arora;Yingyu Liang;Tengyu Ma,arora@cs.princeton.edu;yingyul@cs.princeton.edu;tengyu@cs.princeton.edu,7;7;8,4;4;3,Accept (Poster),8,4,0,no,11/4/16,Princeton University;Princeton University;Princeton University,32;32;32,7;7;7,3;5;6,11/4/16,582,241,295,21,0,112,16100;2835;3902,349;81;87,61;25;32,1798;422;503,-1;-1
2,ICLR,2017,A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks,Dan Hendrycks;Kevin Gimpel,dan@ttic.edu;kgimpel@ttic.edu,6;6;6,3;3;3,Accept (Poster),2,4,0,no,11/4/16,Toyota Technological Institute at Chicago;Toyota Technological Institute at Chicago,-1;-1,-1;-1,3;2,10/7/16,387,218,194,14,0,98,1493;5710,27;99,14;31,280;821,-1;-1
3,ICLR,2017,MS MARCO: A Human-Generated MAchine Reading COmprehension Dataset,Tri Nguyen;Mir Rosenberg;Xia Song;Jianfeng Gao;Saurabh Tiwary;Rangan Majumder;Li Deng,trnguye@microsoft.com;miriamr@microsoft.com;xiaso@microsoft.com;jfgao@microsoft.com;satiwary@microsoft.com;ranganm@microsoft.com;deng@microsoft.com,6;6;6,3;3,Reject,3,2,0,no,11/4/16,Microsoft;Microsoft;Microsoft;Microsoft;Microsoft;Microsoft;Microsoft,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,,11/4/16,438,232,163,10,0,92,656;436;595;18692;504;431;21150,67;4;41;353;14;2;409,10;3;6;61;5;2;63,105;90;101;2662;97;90;1883,-1;-1
4,ICLR,2017,Dynamic Steerable Frame Networks,Jörn-Henrik Jacobsen;Bert De Brabandere;Arnold W.M. Smeulders,j.jacobsen@uva.nl;bert.debrabandere@esat.kuleuven.be;a.w.m.smeulders@uva.nl,5;4;7,4;3;3,Reject,1,4,0,no,11/4/16,University of Amsterdam;KU Leuven;University of Amsterdam,161;105;161,63;40;63,,11/4/16,22,0,0,0,0,0,408;812;22187,19;15;469,9;11;50,37;112;1926,-1;-1
5,ICLR,2017,FILTER SHAPING FOR CONVOLUTIONAL NEURAL NETWORKS,Xingyi Li;Fuxin Li;Xiaoli Fern;Raviv Raich,lixin@eecs.oregonstate.edu;lif@eecs.oregonstate.edu;xfern@eecs.oregonstate.edu;raich@eecs.oregonstate.edu,6;7;7,4;3;4,Accept (Poster),2,5,0,no,11/5/16,Oregon State University;Oregon State University;Oregon State University;Oregon State University,75;75;75;75,316;316;316;316,,11/5/16,10,8,3,0,0,2,70;2378;2772;2258,24;83;125;157,4;24;26;24,2;317;300;186,-1;-1
6,ICLR,2017, A Theoretical Framework for Robustness of (Deep) Classifiers against Adversarial Samples,Beilun Wang;Ji Gao;Yanjun Qi,bw4mw@virginia.edu;jg6yd@virginia.edu;yanjun@virginia.edu,5;3;5,2;4;4,Invite to Workshop Track,8,21,0,no,11/4/16,University of Virginia;University of Virginia;University of Virginia,67;67;67,123;123;123,4;1,11/4/16,28,20,3,0,0,3,159;810;3128,22;179;114,5;12;26,11;67;265,-1;-1
7,ICLR,2017,Efficient Vector Representation for Documents through Corruption,Minmin Chen,m.chen@criteo.com,6;7;7,4;3;4,Accept (Poster),2,8,3,no,11/5/16,Criteo,-1,-1,3,11/5/16,68,32,31,5,11,17,1873,44,18,292,-1
8,ICLR,2017,Adversarial Training Methods for Semi-Supervised Text Classification,Takeru Miyato;Andrew M. Dai;Ian Goodfellow,takeru.miyato@gmail.com;adai@google.com;ian@openai.com,6;7;7,4;3;5,Accept (Poster),1,4,0,no,11/3/16,"Preferred Networks, Inc.;Google;OpenAI",-1;-1;-1,-1;-1;-1,3;4,5/25/16,289,145,123,15,0,53,2795;3771;56046,17;50;90,10;19;56,583;468;9400,-1;-1
9,ICLR,2017,Neural Architecture Search with Reinforcement Learning,Barret Zoph;Quoc Le,barretzoph@google.com;qvl@google.com,9;9;9,4;4;5,Accept (Oral),8,8,4,no,11/4/16,Google;Google,-1;-1,-1;-1,3,11/4/16,1668,807,704,26,0,243,7052;48299,30;193,21;81,1304;6043,-1;-1
10,ICLR,2017,Decomposing Motion and Content for Natural Video Sequence Prediction,Ruben Villegas;Jimei Yang;Seunghoon Hong;Xunyu Lin;Honglak Lee,rubville@umich.edu;jimyang@adobe.com;maga33@postech.ac.kr;timelin@buaa.edu.cn;honglak@umich.edu,7;7;6,5;4;4,Accept (Poster),5,4,4,no,11/4/16,University of Michigan;Adobe Systems;POSTECH;Beihang University;University of Michigan,8;-1;113;129;8,21;-1;104;981;21,,11/4/16,235,130,111,14,8,57,957;5443;2680;406;24193,21;74;25;5;166,10;35;15;3;61,132;850;263;76;2826,-1;-1
11,ICLR,2017,Maximum Entropy Flow Networks,Gabriel Loaiza-Ganem *;Yuanjun Gao *;John P. Cunningham,gl2480@columbia.edu;yg2312@columbia.edu;jpc2181@columbia.edu,6;6;9,4;4;5,Accept (Poster),2,4,1,no,11/4/16,Columbia University;Columbia University;Columbia University,14;14;14,16;16;16,2,11/4/16,20,9,5,0,0,0,33;604;4953,5;14;99,3;6;29,0;48;425,-1;-1
12,ICLR,2017,beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework,Irina Higgins;Loic Matthey;Arka Pal;Christopher Burgess;Xavier Glorot;Matthew Botvinick;Shakir Mohamed;Alexander Lerchner,irinah@google.com;lmatthey@google.com;arkap@google.com;cpburgess@google.com;glorotx@google.com;botvinick@google.com;shakir@google.com;lerchner@google.com,5;7;6,4;4;4,Accept (Poster),2,4,0,no,11/4/16,Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1,5,11/4/16,1160,723,438,44,0,214,2002;1818;1787;1500;17982;13623;6973;2115,25;13;11;64;25;146;51;23,11;12;10;9;16;45;27;15,298;274;274;240;1310;1414;1010;321,-1;-1
13,ICLR,2017,Soft Weight-Sharing for Neural Network Compression,Karen Ullrich;Edward Meeds;Max Welling,karen.ullrich@uva.nl;tmeeds@gmail.com;welling.max@gmail.com,7;7;7,3;4;3,Accept (Poster),3,2,1,no,11/4/16,University of Amsterdam;VU University Toronto;University of California - Irvine,161;15;36,63;22;99,,11/4/16,190,87,73,6,38,26,528;587;26809,10;21;269,6;10;59,73;80;5116,-1;-1
14,ICLR,2017,Support Regularized Sparse Coding and Its Fast Encoder,Yingzhen Yang;Jiahui Yu;Pushmeet Kohli;Jianchao Yang;Thomas S. Huang,superyyzg@gmail.com;jyu79@illinois.edu;pkohli@microsoft.com;jianchao.yang@snapchat.com;t-huang1@illinois.edu,6;7;7,3;4;4,Accept (Poster),1,4,0,no,11/4/16,"University of Illinois, Urbana-Champaign;University of Illinois, Urbana Champaign;Microsoft;Snap Inc.;University of Illinois, Urbana Champaign",4;4;-1;-1;4,36;36;-1;-1;36,,11/4/16,1,1,1,0,0,1,1825;1641;22310;14279;68514,83;40;313;122;1501,19;13;69;44;119,171;251;2770;2084;6370,-1;-1
15,ICLR,2017,Stochastic Neural Networks for Hierarchical Reinforcement Learning,Carlos Florensa;Yan Duan;Pieter Abbeel,florensa@berkeley.edu;rocky@openai.com;pieter@openai.com,7;7;8,4;4;4,Accept (Poster),0,8,1,no,11/5/16,University of California Berkeley;OpenAI;OpenAI,5;-1;-1,10;-1;-1,,11/5/16,152,103,38,2,20,17,485;5544;36935,10;48;433,6;19;94,46;643;4451,-1;-1
16,ICLR,2017,Combining policy gradient and Q-learning,Brendan O'Donoghue;Remi Munos;Koray Kavukcuoglu;Volodymyr Mnih,bodonoghue@google.com;munos@google.com;korayk@google.com;vmnih@google.com,9;7;7,5;3;4,Accept (Poster),15,4,0,no,11/4/16,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,,11/4/16,141,57,49,4,22,14,2400;1747;53013;21076,30;30;90;38,16;13;58;27,297;261;6854;3370,-1;-1
17,ICLR,2017,Recurrent Environment Simulators,Silvia Chiappa;Sébastien Racaniere;Daan Wierstra;Shakir Mohamed,csilvia@google.com;sracaniere@google.com;wierstra@google.com;shakir@google.com,7;5;8,5;4;4,Accept (Poster),3,5,0,no,11/4/16,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,,11/4/16,100,64,30,3,145,7,594;787;27419;6973,45;32;64;51,12;12;40;27,42;77;4775;1010,-1;-1
18,ICLR,2017,Multi-Agent Cooperation and the Emergence of (Natural) Language,Angeliki Lazaridou;Alexander Peysakhovich;Marco Baroni,angeliki.lazaridou@unitn.it;alexpeys@fb.com;marco.baroni@unitn.it,7;7;7,3;3;3,Accept (Oral),0,3,7,no,11/4/16,University of Trento;Facebook;University of Trento,15;-1;15,236;-1;236,3,11/4/16,172,123,23,9,0,10,1733;1709;10216,76;46;190,21;15;43,193;126;1310,-1;-1
19,ICLR,2017,Stick-Breaking Variational Autoencoders,Eric Nalisnick;Padhraic Smyth,enalisni@uci.edu;smyth@ics.uci.edu,4;8;8,4;4;5,Accept (Poster),6,7,0,no,11/4/16,"University of California, Irvine;University of California, Irvine",36;36,99;99,5;11,5/20/16,78,31,33,1,10,14,645;25999,33;410,13;65,96;2302,-1;-1
20,ICLR,2017,Sigma Delta Quantized Networks,Peter O'Connor;Max Welling,peter.ed.oconnor@gmail.com;max.welling@uva.nl,8;6;8,4;3;4,Accept (Poster),3,2,0,no,11/4/16,University of Amsterdam;University of Amsterdam,161;161,63;63,,11/4/16,13,5,5,0,0,0,1787;26809,81;269,17;59,133;5116,-1;-1
21,ICLR,2017,Semi-supervised deep learning by metric embedding,Elad Hoffer;Nir Ailon,ehoffer@tx.technion.ac.il;nailon@cs.technion.ac.il,4;6,4;4,Invite to Workshop Track,1,3,0,no,11/4/16,Technion;Technion,24;24,301;301,,11/4/16,20,11,10,0,2,2,1479;3402,27;92,12;24,175;413,-1;-1
22,ICLR,2017,Adversarial Feature Learning,Jeff Donahue;Philipp Krähenbühl;Trevor Darrell,jdonahue@cs.berkeley.edu;philkr@utexas.edu;trevor@eecs.berkeley.edu,7;7;7,3;3;4,Accept (Poster),2,1,1,no,11/4/16,"University of California Berkeley;University of Texas, Austin;University of California Berkeley",5;20;5,10;50;10,5;4,5/31/16,792,439,266,36,0,137,35234;8830;89488,56;43;559,25;26;112,4713;1548;11436,-1;-1
23,ICLR,2017,Learning Visual Servoing with Deep Features and Fitted Q-Iteration,Alex X. Lee;Sergey Levine;Pieter Abbeel,alexlee_gk@cs.berkeley.edu;svlevine@cs.berkeley.edu;pabbeel@cs.berkeley.edu,8;7;7,3;4;3,Accept (Poster),3,0,0,no,11/5/16,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,10;10;10,8,11/5/16,39,12,14,0,9,0,1340;24658;36938,18;310;433,15;73;94,151;3204;4451,-1;-1
24,ICLR,2017,Learning End-to-End Goal-Oriented Dialog,Antoine Bordes;Y-Lan Boureau;Jason Weston,abordes@fb.com;ylan@fb.com;jase@fb.com,7;8;8,4;5;4,Accept (Oral),3,5,0,no,11/4/16,Facebook;Facebook;Facebook,-1;-1;-1,-1;-1;-1,,5/24/16,452,285,110,16,630,71,16045;4903;45098,77;24;243,32;14;78,2359;334;5838,-1;-1
25,ICLR,2017,Learning Graphical State Transitions,Daniel D. Johnson,ddjohnson@hmc.edu,9;7;9,3;2;3,Accept (Oral),6,6,0,no,10/29/16,Harvey Mudd College,462,981,10,10/29/16,62,23,19,0,0,4,238,55,8,16,-1
26,ICLR,2017,Revisiting Classifier Two-Sample Tests,David Lopez-Paz;Maxime Oquab,dlp@fb.com;qas@fb.com,7;8;7,3;5;4,Accept (Poster),6,6,0,no,11/4/16,Facebook;Facebook,-1;-1,-1;-1,5;4,10/20/16,101,43,45,2,8,12,2441;2924,46;20,19;7,418;222,-1;-1
27,ICLR,2017,Mollifying Networks,Caglar Gulcehre;Marcin Moczulski;Francesco Visin;Yoshua Bengio,gulcehrc@iro.umontreal.ca;marcin-m@post.pl;fvisin@gmail.com;yoshua.umontreal@gmail.com,7;6;6,5;4;4,Accept (Poster),3,3,0,no,11/4/16,University of Montreal;University of Oxford;Politecnico di Milano;University of Montreal,113;50;140;113,103;1;981;103,4;9,8/17/16,25,15,4,0,0,4,19554;437;2757;205027,36;19;11;807,26;7;9;147,2990;40;222;24136,-1;-1
28,ICLR,2017,Introspection:Accelerating Neural Network Training By Learning Weight Evolution,Abhishek Sinha;Aahitagni Mukherjee;Mausoom Sarkar;Balaji Krishnamurthy,abhishek.sinha94@gmail.com;ahitagnimukherjeeam@gmail.com;msarkar@adobe.com;kbalaji@adobe.com,8;7;9,5;4;5,Accept (Poster),3,10,0,no,11/4/16,Adobe Systems;IIT Kanpur;Adobe Systems;Adobe Systems,-1;140;-1;-1,-1;498;-1;-1,,11/4/16,9,5,2,1,0,1,48;10;18;272,19;3;13;68,4;2;3;8,3;1;2;20,-1;-1
29,ICLR,2017,Neural Program Lattices,Chengtao Li;Daniel Tarlow;Alexander L. Gaunt;Marc Brockschmidt;Nate Kushman,ctli@mit.edu;dtarlow@microsoft.com;algaunt@microsoft.com;mabrocks@microsoft.com;nkushman@microsoft.com,4;7;7,4;4;5,Accept (Poster),3,5,0,no,11/4/16,Massachusetts Institute of Technology;Microsoft;Microsoft;Microsoft;Microsoft,2;-1;-1;-1;-1,5;-1;-1;-1;-1,,11/4/16,20,15,3,0,0,3,1120;2537;1188;2508;1504,102;68;38;61;29,20;23;14;22;17,122;306;88;329;202,-1;-1
30,ICLR,2017,Highway and Residual Networks learn Unrolled Iterative Estimation,Klaus Greff;Rupesh K. Srivastava;Jürgen Schmidhuber,klaus@idsia.ch;rupesh@idsia.ch;juergen@idsia.ch,7;8;6,4;4;5,Accept (Poster),0,3,0,no,11/5/16,IDSIA;IDSIA;IDSIA,-1;-1;-1,-1;-1;-1,,11/5/16,111,63,16,6,45,9,4213;4936;65496,24;34;347,13;12;75,393;470;8323,-1;-1
31,ICLR,2017,Hadamard Product for Low-rank Bilinear Pooling,Jin-Hwa Kim;Kyoung-Woon On;Woosang Lim;Jeonghee Kim;Jung-Woo Ha;Byoung-Tak Zhang,jnhwkim@snu.ac.kr;kwon@bi.snu.ac.kr;quasar17@kaist.ac.kr;jeonghee.kim@navercorp.com;jungwoo.ha@navercorp.com;btzhang@bi.snu.ac.kr,7;7;6,3;5;3,Accept (Poster),2,13,0,no,10/17/16,Seoul National University;Seoul National University;Korea Advanced Institute of Science and Technology;NAVER;NAVER;Seoul National University,50;50;462;-1;-1;50,72;72;88;-1;-1;72,2,10/17/16,-1,-1,-1,-1,-1,-1,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,0;0;0;0;0;0,-1;-1
32,ICLR,2017,Machine Comprehension Using Match-LSTM and Answer Pointer,Shuohang Wang;Jing Jiang,shwang.2014@phdis.smu.edu.sg;jingjiang@smu.edu.sg,7;6;6,3;4;3,Accept (Poster),4,5,0,no,11/4/16,Singapore Management University;Singapore Management University,87;87,981;981,3,8/29/16,373,165,141,7,0,68,1104;7075,21;202,8;32,212;794,-1;-1
33,ICLR,2017,Unsupervised Cross-Domain Image Generation,Yaniv Taigman;Adam Polyak;Lior Wolf,yaniv@fb.com;adampolyak@fb.com;wolf@fb.com,7;6;7,3;3;4,Accept (Poster),18,8,1,no,11/4/16,Facebook;Facebook;Facebook,-1;-1;-1,-1;-1;-1,5,11/4/16,495,268,129,2,18,35,5820;757;13974,26;12;199,16;7;45,563;58;1634,-1;-1
34,ICLR,2017,Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling,Hakan Inan;Khashayar Khosravi;Richard Socher,inanh@stanford.edu;khosravi@stanford.edu;rsocher@salesforce.com,6;7;8,4;4;4,Accept (Poster),6,4,0,no,11/4/16,Stanford University;Stanford University;SalesForce.com,3;3;-1,3;3;-1,3,11/4/16,245,99,116,9,93,26,352;417;52991,11;14;180,7;5;49,31;52;8889,-1;-1
35,ICLR,2017,Making Neural Programming Architectures Generalize via Recursion,Jonathon Cai;Richard Shin;Dawn Song,jonathon@cs.berkeley.edu;ricshin@cs.berkeley.edu;dawnsong@cs.berkeley.edu,8;8;9,4;3;5,Accept (Oral),1,5,1,no,11/5/16,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,10;10;10,1,11/5/16,87,59,27,1,20,8,91;279;43319,6;29;396,2;9;100,8;27;4323,-1;-1
36,ICLR,2017,Generative Multi-Adversarial Networks,Ishan Durugkar;Ian Gemp;Sridhar Mahadevan,idurugkar@cs.umass.edu;imgemp@cs.umass.edu;mahadeva@cs.umass.edu,7;6;7,4;4;3,Accept (Poster),7,6,0,no,11/4/16,"University of Massachusetts, Amherst;University of Massachusetts, Amherst;University of Massachusetts, Amherst",25;25;25,166;166;166,5;4,11/4/16,155,78,40,4,25,14,335;256;6007,15;25;204,5;4;37,47;26;548,-1;-1
37,ICLR,2017,Variational Recurrent Adversarial Deep Domain Adaptation,Sanjay Purushotham;Wilka Carvalho;Tanachat Nilanon;Yan Liu,spurusho@usc.edu;wcarvalh@usc.edu;nilanon@usc.edu;yanliu.cs@usc.edu,6;5;6,4;4;4,Accept (Poster),3,4,0,no,11/4/16,University of Southern California;University of Southern California;University of Southern California;University of Southern California,31;31;31;31,60;60;60;60,4,11/4/16,27,14,7,0,0,3,1094;110;78;6219,39;4;7;584,13;3;4;35,116;17;6;503,-1;-1
38,ICLR,2017,Data Noising as Smoothing in Neural Network Language Models,Ziang Xie;Sida I. Wang;Jiwei Li;Daniel Lévy;Aiming Nie;Dan Jurafsky;Andrew Y. Ng,zxie@cs.stanford.edu;sidaw@cs.stanford.edu;jiweil@stanford.edu;danilevy@cs.stanford.edu;anie@cs.stanford.edu;jurafsky@stanford.edu;ang@cs.stanford.edu,6;6;8,4;4,Accept (Poster),2,3,0,no,11/4/16,Stanford University;Stanford University;Stanford University;Stanford University;Stanford University;Stanford University;Stanford University,3;3;3;3;3;3;3,3;3;3;3;3;3;3,3,11/4/16,62,28,26,3,0,8,541;1848;5787;208;138;29825;102605,29;17;100;11;15;309;289,12;12;30;4;4;76;114,55;186;835;19;19;3495;14220,-1;-1
39,ICLR,2017,Deep Probabilistic Programming,Dustin Tran;Matthew D. Hoffman;Rif A. Saurous;Eugene Brevdo;Kevin Murphy;David M. Blei,dustin@cs.columbia.edu;mathoffm@adobe.com;rif@google.com;ebrevdo@google.com;kpmurphy@google.com;david.blei@columbia.edu,5;8;7,4;4;4,Accept (Poster),2,4,0,no,11/4/16,Columbia University;Adobe Systems;Google;Google;Google;Columbia University,14;-1;-1;-1;-1;14,16;-1;-1;-1;-1;16,5;4,11/4/16,125,56,51,1,0,13,1842;8751;2474;8188;16491;55384,50;94;31;23;83;306,20;29;15;11;41;77,194;1228;420;929;2323;9343,-1;-1
40,ICLR,2017,Hierarchical Multiscale Recurrent Neural Networks,Junyoung Chung;Sungjin Ahn;Yoshua Bengio,junyoung.chung@umontreal.ca;sungjin.ahn@umontreal.ca;yoshua.bengio@umontreal.ca,8;7;8,4;3;4,Accept (Poster),2,3,0,no,10/29/16,University of Montreal;University of Montreal;University of Montreal,113;113;113,103;103;103,3,9/6/16,309,145,107,12,129,49,5932;1364;205027,21;41;807,9;12;147,938;162;24136,-1;-1
41,ICLR,2017,Programming With a Differentiable Forth Interpreter,Matko Bošnjak;Tim Rocktäschel;Jason Naradowsky;Sebastian Riedel,m.bosnjak@cs.ucl.ac.uk;t.rocktaschel@cs.ucl.ac.uk;j.narad@cs.ucl.ac.uk;s.riedel@cs.ucl.ac.uk,7;5;6,2;2;4,Invite to Workshop Track,1,4,1,no,11/4/16,University College London;University College London;University College London;University College London,45;45;45;45,15;15;15;15,10,5/21/16,86,46,15,1,0,10,3517;2272;835;5852,24;48;27;230,11;22;12;35,477;282;126;891,-1;-1
42,ICLR,2017,Learning to Play in a Day: Faster Deep Reinforcement Learning by Optimality Tightening,Frank S.He;Yang Liu;Alexander G. Schwing;Jian Peng,frankheshibi@gmail.com;liu301@illinois.edu;aschwing@illinois.edu;jianpeng@illinois.edu,9;4;9,3;4;4,Accept (Poster),10,9,0,no,11/4/16,"Zhejiang University;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",129;4;4;4,981;36;36;36,,11/4/16,46,18,19,5,26,7,55;284;3755;2121,6;52;117;120,2;8;31;23,8;29;348;207,-1;-1
43,ICLR,2017,Energy-based Generative Adversarial Networks,Junbo Zhao;Michael Mathieu;Yann LeCun,jakezhao@cs.nyu.edu;mathieu@cs.nyu.edu;yann@cs.nyu.edu,8;7;7,3;5;3,Accept (Poster),4,5,0,no,11/4/16,New York University;New York University;New York University,25;25;25,32;32;32,5;4,11/4/16,168,21,18,1,0,13,3383;8016;92622,14;18;345,9;16;107,451;767;10396,-1;-1
44,ICLR,2017,Normalizing the Normalizers: Comparing and Extending Network Normalization Schemes,Mengye Ren;Renjie Liao;Raquel Urtasun;Fabian H. Sinz;Richard S. Zemel,mren@cs.toronto.edu;rjliao@cs.toronto.edu;urtasun@cs.toronto.edu;fabian.sinz@epagoge.de;zemel@cs.toronto.edu,7;5;9,4;4;5,Accept (Poster),0,7,1,no,11/5/16,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Baylor College of Medicine;Department of Computer Science, University of Toronto",15;15;15;-1;15,22;22;22;-1;22,3,11/5/16,43,15,12,0,0,2,1502;1615;24598;1727;21678,18;63;245;44;208,12;22;73;16;52,213;208;3464;258;2504,-1;-1
45,ICLR,2017,Learning through Dialogue Interactions by Asking Questions,Jiwei Li;Alexander H. Miller;Sumit Chopra;Marc'Aurelio Ranzato;Jason Weston,jiwel@fb.com;ahm@fb.com;spchopra@fb.com;ranzato@fb.com;jase@fb.com,7;8;7,3;5;3,Accept (Poster),4,9,0,no,11/4/16,Facebook;Facebook;Facebook;Facebook;Facebook,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,,11/4/16,43,24,6,1,22,5,5787;975;11361;21223;45098,100;16;56;91;243,30;10;22;43;78,835;116;1149;2114;5838,-1;-1
46,ICLR,2017,Third Person Imitation Learning,Bradly C Stadie;Pieter Abbeel;Ilya Sutskever,bstadie@openai.com;pieter@openai.com;ilyasu@openai.com,6;5;6,4;3;4,Accept (Poster),2,4,0,no,11/4/16,OpenAI;OpenAI;OpenAI,-1;-1;-1,-1;-1;-1,,11/4/16,108,57,28,0,6,4,773;36938;132389,16;433;90,7;94;53,50;4451;17007,-1;-1
47,ICLR,2017,The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables,Chris J. Maddison;Andriy Mnih;Yee Whye Teh,cmaddis@stats.ox.ac.uk;amnih@google.com;y.w.teh@stats.ox.ac.uk,7;9;8,3;5;4,Accept (Poster),4,1,0,no,11/4/16,University of Oxford;Google;University of Oxford,50;-1;50,1;-1;1,10,11/2/16,837,251,522,12,301,120,8342;9769;23411,30;37;249,15;21;52,525;1554;3226,-1;-1
48,ICLR,2017,Pruning Convolutional Neural Networks for Resource Efficient Inference,Pavlo Molchanov;Stephen Tyree;Tero Karras;Timo Aila;Jan Kautz,pmolchanov@nvidia.com;styree@nvidia.com;tkarras@nvidia.com;taila@nvidia.com;jkautz@nvidia.com,7;6;9,4;4;4,Accept (Poster),5,6,0,no,11/4/16,NVIDIA;NVIDIA;NVIDIA;NVIDIA;NVIDIA,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,6;8,11/4/16,425,169,180,14,0,62,1772;2424;4718;6174;13942,46;42;42;68;301,14;17;21;32;57,196;269;848;993;1885,-1;-1
49,ICLR,2017,Adaptive Feature Abstraction for Translating Video to Language,Yunchen Pu;Martin Renqiang Min;Zhe Gan;Lawrence Carin,yunchen.pu@duke.edu;renqiang@nec-labs.com;zhe.gan@duke.edu;lcarin@duke.edu,4;4;7,5;4;4,Invite to Workshop Track,6,7,0,no,11/4/16,Duke University;NEC-Labs;Duke University;Duke University,42;-1;42;42,18;-1;18;18,,11/4/16,26,13,5,0,4,1,1153;887;2310;19351,44;58;85;819,14;14;25;66,119;103;320;1985,-1;-1
50,ICLR,2017,Learning Recurrent Representations for Hierarchical Behavior Modeling,Eyrun Eyjolfsdottir;Kristin Branson;Yisong Yue;Pietro Perona,eeyjolfs@caltech.edu;bransonk@janelia.hhmi.org;yyue@caltech.edu;perona@caltech.edu,7;6;7,3;4;4,Accept (Poster),3,7,0,no,11/3/16,California Institute of Technology;HHMI Janelia Research Campus;California Institute of Technology;California Institute of Technology,129;-1;129;129,2;-1;2;2,5;7,11/1/16,20,11,8,0,13,1,252;1706;3233;63330,9;41;121;444,6;19;29;91,27;140;391;8572,-1;-1
51,ICLR,2017,Generalizing Skills with Semi-Supervised Reinforcement Learning,Chelsea Finn;Tianhe Yu;Justin Fu;Pieter Abbeel;Sergey Levine,cbfinn@eecs.berkeley.edu;tianhe.yu@berkeley.edu;justinfu@eecs.berkeley.edu;pabbeel@eecs.berkeley.edu;svlevine@eecs.berkeley.edu,6;7;8,5;3;4,Accept (Poster),2,0,0,no,11/4/16,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5;5,10;10;10;10;10,8,11/4/16,32,21,4,0,0,1,7805;583;551;36938;24658,98;17;20;433;310,33;8;10;94;73,1047;68;81;4451;3204,-1;-1
52,ICLR,2017,Entropy-SGD: Biasing Gradient Descent Into Wide Valleys,Pratik Chaudhari;Anna Choromanska;Stefano Soatto;Yann LeCun;Carlo Baldassi;Christian Borgs;Jennifer Chayes;Levent Sagun;Riccardo Zecchina,pratikac@ucla.edu;achoroma@cims.nyu.edu;soatto@cs.ucla.edu;yann@cs.nyu.edu;carlo.baldassi@polito.it;borgs@microsoft.com;jchayes@microsoft.com;sagun@cims.nyu.edu;riccardo.zecchina@polito.it,7;8;9,4;4;3,Accept (Poster),4,13,0,no,11/4/16,"University of California, Los Angeles;New York University;University of California, Los Angeles;New York University;Politecnico di Torino;Microsoft;Microsoft;New York University;Politecnico di Torino",20;25;20;25;462;-1;-1;25;462,14;32;14;32;384;-1;-1;32;384,8,11/4/16,286,169,67,11,48,40,639;2479;15626;92622;910;5823;5449;960;5466,32;89;458;345;57;191;144;22;180,9;19;61;107;13;39;38;12;37,76;231;1438;10396;82;613;574;129;459,-1;-1
53,ICLR,2017,Tree-structured decoding with doubly-recurrent neural networks,David Alvarez-Melis;Tommi S. Jaakkola,dalvmel@mit.edu;tommi@csail.mit.edu,7;6;6,4;4;4,Accept (Poster),3,10,0,no,11/4/16,Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2,5;5,,11/4/16,63,32,20,0,0,6,580;22163,26;292,11;69,57;2321,-1;-1
54,ICLR,2017,A Compositional Object-Based Approach to Learning Physical Dynamics,Michael Chang;Tomer Ullman;Antonio Torralba;Joshua Tenenbaum,mbchang@mit.edu;tomeru@mit.edu;torralba@mit.edu;jbt@mit.edu,9;6;7,4;4;4,Accept (Poster),6,6,0,no,11/4/16,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2,5;5;5;5,,11/4/16,215,132,34,4,66,15,1155;559;48619;30839,42;33;281;590,16;8;88;83,84;32;6342;2683,-1;-1
55,ICLR,2017,"Attend, Adapt and Transfer: Attentive Deep Architecture for Adaptive Transfer from multiple sources in the same domain",Janarthanan Rajendran;Aravind Lakshminarayanan;Mitesh M. Khapra;Prasanna P;Balaraman Ravindran,rjana@umich.edu;aravindsrinivas@gmail.com;miteshk@cse.iitm.ac.in;prasanna.p@cs.mcgill.ca;ravi@cse.iitm.ac.in,7;7;7,4;3;4,Accept (Poster),1,5,0,no,11/4/16,University of Michigan;University of Montreal;Indian Institute of Technology Madras;McGill University;Indian Institute of Technology Madras,8;113;140;80;140,21;103;472;42;472,,10/10/15,18,12,2,2,0,1,104;167;1337;36;2503,14;13;91;8;234,6;7;18;3;28,10;9;153;4;202,-1;-1
56,ICLR,2017,Multilayer Recurrent Network Models of Primate Retinal Ganglion Cell Responses,Eleanor Batty;Josh Merel;Nora Brackbill;Alexander Heitman;Alexander Sher;Alan Litke;E.J. Chichilnisky;Liam Paninski,erb2180@columbia.edu;jsmerel@gmail.com;nbrack@stanford.edu;alexkenheitmen@gmail.com;sashake3@uscs.edu;Alan.Litke@cern.ch;ej@stanford.edu;liam@stat.columbia.edu,8;7;4,5;4;4,Accept (Poster),2,5,0,no,11/5/16,Columbia University;;Stanford University;;;CERN;Stanford University;Columbia University,14;-1;3;-1;-1;-1;3;14,16;-1;3;-1;-1;-1;3;16,5,11/5/16,29,11,8,0,0,1,94;1658;277;84;6185;5651;455;10827,6;29;12;5;161;78;24;251,4;16;6;3;32;34;11;50,8;127;11;3;487;476;27;983,-1;-1
57,ICLR,2017,LR-GAN: Layered Recursive Generative Adversarial Networks for Image Generation,Jianwei Yang;Anitha Kannan;Dhruv Batra;Devi Parikh,jw2yang@vt.edu;akannan@fb.com;dbatra@gatech.edu;parikh@gatech.edu,6;6;7,3;4,Accept (Poster),5,5,0,no,11/4/16,Virginia Tech;Facebook;Georgia Institute of Technology;Georgia Institute of Technology,80;-1;12;12,286;-1;33;33,5;4,11/4/16,109,50,28,6,24,11,2007;1723;8468;15162,79;66;147;185,13;20;41;55,252;186;1150;2387,-1;-1
58,ICLR,2017,Learning to Optimize,Ke Li;Jitendra Malik,ke.li@eecs.berkeley.edu;malik@eecs.berkeley.edu,7;6;7,4;4;4,Accept (Poster),5,5,1,no,11/4/16,University of California Berkeley;University of California Berkeley,5;5,10;10,,6/6/16,136,64,34,1,0,3,7826;75103,782;429,43;116,307;7769,-1;-1
59,ICLR,2017,Variational Lossy Autoencoder,Xi Chen;Diederik P. Kingma;Tim Salimans;Yan Duan;Prafulla Dhariwal;John Schulman;Ilya Sutskever;Pieter Abbeel,peter@openai.com;dpkingma@openai.com;tim@openai.com;rocky@openai.com;prafulla@mit.edu;joschu@openai.com;ilyasu@openai.com;pieter@openai.com,7;7;6,4;4;4,Accept (Poster),8,8,0,no,11/4/16,OpenAI;OpenAI;OpenAI;OpenAI;Massachusetts Institute of Technology;OpenAI;OpenAI;OpenAI,-1;-1;-1;-1;2;-1;-1;-1,-1;-1;-1;-1;5;-1;-1;-1,5,11/4/16,326,210,109,11,82,69,13601;56020;6679;5544;3327;15023;132389;36938,444;26;35;48;12;55;90;433,42;19;13;19;6;31;53;94,1548;10288;1037;643;906;2492;17007;4451,-1;-1
60,ICLR,2017,Improving Generative Adversarial Networks with Denoising Feature Matching,David Warde-Farley;Yoshua Bengio,d.warde.farley@gmail.com;yoshua.umontreal@gmail.com,7;6;7,4;2;5,Accept (Poster),5,4,0,no,11/5/16,Google;University of Montreal,-1;113,-1;103,5;4,11/5/16,128,38,37,2,0,20,24367;205027,27;807,19;147,3870;24136,-1;-1
61,ICLR,2017,Emergence of foveal image sampling from learning to attend in visual scenes,Brian Cheung;Eric Weiss;Bruno Olshausen,bcheung@berkeley.edu;eaweiss@berkeley.edu;baolshausen@berkeley.edu,6;5;6,4;4;5,Accept (Poster),3,3,0,no,11/5/16,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,10;10;10,,11/5/16,15,8,2,0,10,0,1512;425;15088,46;40;133,12;11;38,147;26;1160,-1;-1
62,ICLR,2017,Words or Characters? Fine-grained Gating for Reading Comprehension,Zhilin Yang;Bhuwan Dhingra;Ye Yuan;Junjie Hu;William W. Cohen;Ruslan Salakhutdinov,zhiliny@cs.cmu.edu;bdhingra@andrew.cmu.edu;yey1@andrew.cmu.edu;junjieh@cmu.edu;wcohen@cs.cmu.edu;rsalakhu@cs.cmu.edu,7;6;7,4;4;3,Accept (Poster),2,0,2,no,11/4/16,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1;1;1,23;23;23;23;23;23,,11/4/16,79,32,17,2,27,5,4991;1208;1404;510;22626;67952,90;37;115;21;424;254,26;15;17;11;68;82,798;169;123;34;2627;7814,-1;-1
63,ICLR,2017,Transfer Learning for Sequence Tagging with Hierarchical Recurrent Networks,Zhilin Yang;Ruslan Salakhutdinov;William W. Cohen,zhiliny@cs.cmu.edu;rsalakhu@cs.cmu.edu;wcohen@cs.cmu.edu,7;5;8,4;4;4,Accept (Poster),2,5,0,no,11/4/16,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1,23;23;23,6,11/4/16,182,100,58,7,41,29,4991;67952;22626,90;254;424,26;82;68,798;7814;2627,-1;-1
64,ICLR,2017,Bidirectional Attention Flow for Machine Comprehension,Minjoon Seo;Aniruddha Kembhavi;Ali Farhadi;Hannaneh Hajishirzi,minjoon@cs.washington.edu;anik@allenai.org;alif@allenai.org;hannaneh@cs.washington.edu,7;8;8,5;4;5,Accept (Poster),10,7,2,no,11/4/16,University of Washington;Allen Institute for Artificial Intelligence;Allen Institute for Artificial Intelligence;University of Washington,6;-1;-1;6,25;-1;-1;25,,11/4/16,1067,537,646,36,0,295,1735;2640;16428;2890,20;32;119;97,14;14;43;25,412;457;2809;602,-1;-1
65,ICLR,2017,Query-Reduction Networks for Question Answering,Minjoon Seo;Sewon Min;Ali Farhadi;Hannaneh Hajishirzi,minjoon@cs.washington.edu;shmsw25@snu.ac.kr;ali@cs.washington.edu;hannaneh@cs.washington.edu,7;7;7,3;4;4,Accept (Poster),7,2,1,no,11/4/16,University of Washington;Seoul National University;University of Washington;University of Washington,6;50;6;6,25;72;25;25,,6/14/16,85,56,34,6,0,22,1735;392;16428;2890,20;12;119;97,14;10;43;25,412;82;2809;602,-1;-1
66,ICLR,2017,Training Compressed Fully-Connected Networks with a Density-Diversity Penalty,Shengjie Wang;Haoran Cai;Jeff Bilmes;William Noble,wangsj@cs.washington.edu;haoran@uw.edu;bilmes@uw.edu;william-noble@u.washington.edu,9;6;6,4;4;2,Accept (Poster),5,10,0,no,11/4/16,"University of Washington;University of Washington, Seattle;University of Washington, Seattle;University of Washington",6;6;6;6,25;25;25;25,,11/4/16,5,2,1,0,0,0,944;135;13766;34535,63;23;350;402,15;5;55;75,53;4;1279;3105,-1;-1
67,ICLR,2017,Do Deep Convolutional Nets Really Need to be Deep and Convolutional?,Gregor Urban;Krzysztof J. Geras;Samira Ebrahimi Kahou;Ozlem Aslan;Shengjie Wang;Abdelrahman Mohamed;Matthai Philipose;Matt Richardson;Rich Caruana,gurban@uci.edu;k.j.geras@sms.ed.ac.uk;samira.ebrahimi-kahou@polymtl.ca;ozlem@cs.ualberta.ca;wangsj@cs.washington.edu;asamir@microsoft.com;matthaip@microsoft.com;mattri@microsoft.com;rcaruana@microsoft.com,7;7,3;4,Accept (Poster),2,4,0,no,11/4/16,"University of California, Irvine;University of Edinburgh;Polytechnique Montreal;University of Alberta;University of Washington;Microsoft;Microsoft;Microsoft;Microsoft",36;33;351;94;6;-1;-1;-1;-1,99;27;981;106;25;-1;-1;-1;-1,,3/17/16,98,46,18,4,0,7,811;540;3852;145;944;20068;5643;10332;16973,24;35;70;21;63;68;117;109;148,12;12;14;8;15;27;34;30;48,37;48;380;4;53;1235;413;1279;1358,-1;-1
68,ICLR,2017,Recurrent Hidden Semi-Markov Model,Hanjun Dai;Bo Dai;Yan-Ming Zhang;Shuang Li;Le Song,hanjundai@gatech.edu;bodai@gatech.edu;ymzhang@nlpr.ia.ac.cn;sli370@gatech.edu;lsong@cc.gatech.edu,7;7;7,4;3;4,Accept (Poster),2,3,0,no,11/4/16,"Georgia Institute of Technology;Georgia Institute of Technology;Institute of automation, Chinese academy of science, Chinese Academy of Sciences;Georgia Institute of Technology;Georgia Institute of Technology",12;12;67;12;12,33;33;981;33;33,5;2,11/4/16,22,15,11,2,0,1,1894;3872;563;391;9670,58;397;56;75;329,17;30;11;9;54,284;280;38;38;1120,-1;-1
69,ICLR,2017,Lie-Access Neural Turing Machines,Greg Yang;Alexander Rush,gyang@college.harvard.edu;srush@seas.harvard.edu,6;7;6;8,4;4;3;4,Accept (Poster),4,6,0,no,11/4/16,Harvard University;Harvard University,36;36,6;6,8,11/4/16,2,0,1,0,0,0,505;7010,24;87,10;32,53;940,-1;-1
70,ICLR,2017,What does it take to generate natural textures?,Ivan Ustyuzhaninov *;Wieland Brendel *;Leon Gatys;Matthias Bethge,ivan.ustyuzhaninov@bethgelab.org;wieland.brendel@bethgelab.org;leon.gatys@bethgelab.org;matthias.bethge@bethgelab.org,7;8;8,4;3;5,Accept (Poster),2,3,0,no,11/5/16,"Centre for Integrative Neuroscience, AG Bethge;Centre for Integrative Neuroscience, AG Bethge;Centre for Integrative Neuroscience, AG Bethge;Centre for Integrative Neuroscience, AG Bethge",-1;-1;-1;-1,-1;-1;-1;-1,,11/5/16,17,6,8,2,0,4,81;1768;4095;11475,13;40;35;416,5;17;13;47,9;217;566;1264,-1;-1
71,ICLR,2017,Tracking the World State with Recurrent Entity Networks,Mikael Henaff;Jason Weston;Arthur Szlam;Antoine Bordes;Yann LeCun,mbh305@nyu.edu;jase@fb.com;azslam@fb.com;abordes@fb.com;yann@fb.com,7;7;7,3;5;4,Accept (Poster),6,4,0,no,11/4/16,New York University;Facebook;Facebook;Facebook;Facebook,25;-1;-1;-1;-1,32;-1;-1;-1;-1,,11/4/16,131,70,54,8,669,30,2377;45098;8769;16045;92622,23;243;86;77;345,13;78;32;32;107,201;5838;923;2359;10396,-1;-1
72,ICLR,2017,Training Agent for First-Person Shooter Game with Actor-Critic Curriculum Learning,Yuxin Wu;Yuandong Tian,ppwwyyxx@gmail.com;yuandong@fb.com,6;4;7,4;5;4,Accept (Poster),1,4,3,no,11/4/16,Carnegie Mellon University;Facebook,1;-1,23;-1,,11/4/16,105,56,28,1,0,8,2102;2473,13;85,10;25,329;283,-1;-1
73,ICLR,2017,Deep Biaffine Attention for Neural Dependency Parsing,Timothy Dozat;Christopher D. Manning,tdozat@stanford.edu;manning@stanford.edu,5;5;6,4;4;4,Accept (Poster),9,1,2,no,11/4/16,Stanford University;Stanford University,3;3,3;3,10,11/4/16,363,120,142,12,2,55,1776;89690,18;481,11;115,226;11683,-1;-1
74,ICLR,2017,Learning Curve Prediction with Bayesian Neural Networks,Aaron Klein;Stefan Falkner;Jost Tobias Springenberg;Frank Hutter,kleinaa@cs.uni-freiburg.de;sfalkner@cs.uni-freiburg.de;springj@cs.uni-freiburg.de;fh@cs.uni-freiburg.de,7;7;7,4;4;5,Accept (Poster),3,3,0,no,11/4/16,Universität Freiburg;Universität Freiburg;Universität Freiburg;Universität Freiburg,113;113;113;113,95;95;95;95,11,11/4/16,101,49,34,6,0,13,1733;785;7361;12786,48;32;54;233,15;11;28;50,222;79;787;1536,-1;-1
75,ICLR,2017,Deep Information Propagation,Samuel S. Schoenholz;Justin Gilmer;Surya Ganguli;Jascha Sohl-Dickstein,schsam@google.com;gilmer@google.com;sganguli@stanford.edu;jaschasd@google.com,8;9;8,2;4;3,Accept (Poster),0,3,0,no,11/4/16,Google;Google;Stanford University;Google,-1;-1;3;-1,-1;-1;3;-1,,11/4/16,133,73,29,16,0,32,3086;3389;6029;5018,70;45;129;101,21;18;39;33,383;464;603;703,-1;-1
76,ICLR,2017,Pruning Filters for Efficient ConvNets,Hao Li;Asim Kadav;Igor Durdanovic;Hanan Samet;Hans Peter Graf,haoli@cs.umd.edu;asim@nec-labs.com;igord@nec-labs.com;hjs@cs.umd.edu;hpg@nec-labs.com,6;7;7;7,5;4;5;4,Accept (Poster),0,6,0,no,11/5/16,"University of Maryland, College Park;NEC-Labs;NEC-Labs;University of Maryland, College Park;NEC-Labs",12;-1;-1;12;-1,67;-1;-1;67;-1,,8/31/16,990,532,491,69,17,220,7681;1770;1727;17408;3569,540;40;19;440;113,40;16;10;61;26,694;295;306;1264;423,-1;-1
77,ICLR,2017,SGDR: Stochastic Gradient Descent with Warm Restarts,Ilya Loshchilov;Frank Hutter,ilya@cs.uni-freiburg.de;fh@cs.uni-freiburg.de,7;7;7,3;4;5,Accept (Poster),2,2,0,no,11/4/16,Universität Freiburg;Universität Freiburg,113;113,95;95,9,8/13/16,1128,173,500,12,196,154,2594;12786,38;233,21;50,358;1536,-1;-1
78,ICLR,2017,Program Synthesis for Character Level Language Modeling,Pavol Bielik;Veselin Raychev;Martin Vechev,pavol.bielik@inf.ethz.ch;veselin.raychev@inf.ethz.ch;martin.vechev@inf.ethz.ch,5;8;8,3;2;4,Accept (Poster),2,4,0,no,11/4/16,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,9;9;9,9;9;9,3,11/4/16,6,3,1,0,0,0,513;1415;4221,26;29;153,11;15;36,56;159;466,-1;-1
79,ICLR,2017,Learning Invariant Feature Spaces to Transfer Skills with Reinforcement Learning,Abhishek Gupta;Coline Devin;YuXuan Liu;Pieter Abbeel;Sergey Levine,abhigupta@berkeley.edu;coline@berkeley.edu;yuxuanliu@berkeley.edu;pabbeel@cs.berkeley.edu;svlevine@eecs.berkeley.edu,7;6;6,3;5;4,Accept (Poster),3,6,0,no,11/5/16,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5;5,10;10;10;10;10,6,11/5/16,98,60,16,1,7,3,1683;598;544;36938;24658,204;18;62;433;310,18;10;10;94;73,169;27;33;4451;3204,-1;-1
80,ICLR,2017,On the Quantitative Analysis of Decoder-Based Generative Models,Yuhuai Wu;Yuri Burda;Ruslan Salakhutdinov;Roger Grosse,ywu@cs.toronto.edu;yburda@openai.com;rsalakhu@cs.cmu.edu;rgrosse@cs.toronto.edu,7;7;6,4;4;5,Accept (Poster),3,9,0,no,11/4/16,"Department of Computer Science, University of Toronto;OpenAI;Carnegie Mellon University;Department of Computer Science, University of Toronto",15;-1;1;15,22;-1;23;22,5;4,11/4/16,148,60,53,6,0,20,1180;1331;67952;5727,28;21;254;48,13;8;82;28,184;281;7814;813,-1;-1
81,ICLR,2017,Tighter bounds lead to improved classifiers,Nicolas Le Roux,nicolas@le-roux.name,6;8;4,4;5;4,Accept (Poster),0,3,1,no,11/2/16,Criteo,-1,-1,1,6/29/16,4,3,1,0,16,0,4950,172,24,536,-1
82,ICLR,2017,Topology and Geometry of Half-Rectified Network Optimization,C. Daniel Freeman;Joan Bruna,daniel.freeman@berkeley.edu;bruna@cims.nyu.edu,2;7;8,5;3;3,Accept (Poster),5,5,0,no,11/4/16,University of California Berkeley;New York University,5;25,10;32,1,11/4/16,113,61,8,7,15,8,163;11495,13;90,6;29,11;1285,-1;-1
83,ICLR,2017,Quasi-Recurrent Neural Networks,James Bradbury;Stephen Merity;Caiming Xiong;Richard Socher,james.bradbury@salesforce.com;smerity@salesforce.com;cxiong@salesforce.com;rsocher@salesforce.com,6;7;7,4;4;4,Accept (Poster),2,5,0,no,11/4/16,SalesForce.com;SalesForce.com;SalesForce.com;SalesForce.com,-1;-1;-1;-1,-1;-1;-1;-1,3,11/4/16,195,89,77,5,0,40,1311;1852;6299;52991,14;18;156;180,7;8;31;49,232;363;1059;8889,-1;-1
84,ICLR,2017,Pointer Sentinel Mixture Models,Stephen Merity;Caiming Xiong;James Bradbury;Richard Socher,smerity@salesforce.com;cxiong@salesforce.com;james.bradbury@salesforce.com;rsocher@salesforce.com,7;8;8,4;4;4,Accept (Poster),2,2,0,no,11/3/16,SalesForce.com;SalesForce.com;SalesForce.com;SalesForce.com,-1;-1;-1;-1,-1;-1;-1;-1,3,9/26/16,486,162,268,9,265,105,1863;6299;1311;52991,18;156;14;180,8;31;7;49,364;1059;232;8889,-1;-1
85,ICLR,2017,Learning to Generate Samples from Noise through Infusion Training,Florian Bordes;Sina Honari;Pascal Vincent,florian.bordes@umontreal.ca;sina.honari@umontreal.ca;pascal.vincent@umontreal.ca,8;7;6,5;5;4,Accept (Poster),2,6,0,no,11/5/16,University of Montreal;University of Montreal;University of Montreal,113;113;113,103;103;103,5;4,11/5/16,23,6,8,0,11,1,30;1973;15646,5;24;118,2;8;33,1;157;1337,-1;-1
86,ICLR,2017,EPOpt: Learning Robust Neural Network Policies Using Model Ensembles,Aravind Rajeswaran;Sarvjeet Ghotra;Balaraman Ravindran;Sergey Levine,aravraj@cs.washington.edu;sarvjeet.13it236@nitk.edu.in;ravi@cse.iitm.ac.in;svlevine@eecs.berkeley.edu,8;7;7,4;4,Accept (Poster),2,1,0,no,11/4/16,University of Washington;National Institute of Technology Karnataka;Indian Institute of Technology Madras;University of California Berkeley,6;462;140;5,25;981;472;10,4;11,10/5/16,148,93,53,4,0,20,891;152;2503;959,24;3;234;34,13;2;28;9,85;22;202;103,-1;-1
87,ICLR,2017,Steerable CNNs,Taco S. Cohen;Max Welling,taco.cohen@gmail.com;m.welling@uva.nl,6;7;8,3;4;3,Accept (Poster),6,4,0,no,11/4/16,University of Amsterdam;University of Amsterdam,161;161,63;63,,11/4/16,129,70,35,2,0,15,1716;27393,32;271,17;60,251;5190,-1;-1
88,ICLR,2017,Learning to Perform Physics Experiments via Deep Reinforcement Learning,Misha Denil;Pulkit Agrawal;Tejas D Kulkarni;Tom Erez;Peter Battaglia;Nando de Freitas,mdenil@google.com;pulkitag@berkeley.edu;tkulkarni@google.com;etom@google.com;peterbattaglia@google.com;nandodefreitas@google.com,7;7;6;7,4;3;3;3,Accept (Poster),3,6,0,no,11/4/16,Google;University of California Berkeley;Google;Google;Google;Google,-1;5;-1;-1;-1;-1,-1;10;-1;-1;-1;-1,3,11/4/16,50,30,6,0,0,0,3331;2996;1952;6519;4546;19160,38;52;23;48;88;184,20;16;13;19;29;55,285;246;152;1095;423;1851,-1;-1
89,ICLR,2017,The Neural Noisy Channel,Lei Yu;Phil Blunsom;Chris Dyer;Edward Grefenstette;Tomas Kocisky,lei.yu@cs.ox.ac.uk;pblunsom@google.com;cdyer@google.com;etg@google.com;tkocisky@google.com,7;7;6,4;4;4,Accept (Poster),3,4,0,no,11/4/16,University of Oxford;Google;Google;Google;Google,50;-1;-1;-1;-1,1;-1;-1;-1;-1,3,11/4/16,37,19,12,3,33,2,2228;11526;21262;7065;2750,281;144;232;57;16,24;47;60;25;10,138;1333;3162;842;412,-1;-1
90,ICLR,2017,Learning to Compose Words into Sentences with Reinforcement Learning,Dani Yogatama;Phil Blunsom;Chris Dyer;Edward Grefenstette;Wang Ling,dyogatama@google.com;pblunsom@google.com;cdyer@google.com;etg@google.com;lingwang@google.com,6;7;8,3;5;4,Accept (Poster),15,1,0,no,11/4/16,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,3,11/4/16,89,63,31,3,0,17,3557;11526;21262;7065;2596,41;144;232;57;757,21;47;60;25;21,403;1333;3162;842;268,-1;-1
91,ICLR,2017,Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data,Nicolas Papernot;Martín Abadi;Úlfar Erlingsson;Ian Goodfellow;Kunal Talwar,ngp5056@cse.psu.edu;abadi@google.com;ulfar@google.com;ian@openai.com;kunal@google.com,9;7;9,4;3;4,Accept (Oral),6,9,0,no,11/2/16,Pennsylvania State University;Google;Google;OpenAI;Google,39;-1;-1;-1;-1,68;-1;-1;-1;-1,5,10/18/16,302,164,82,5,0,48,9339;28596;5234;56065;16459,66;277;65;90;144,27;70;29;56;42,1086;3379;790;9399;2158,-1;-1
92,ICLR,2017,Diet Networks: Thin Parameters for Fat Genomics,Adriana Romero;Pierre Luc Carrier;Akram Erraqabi;Tristan Sylvain;Alex Auvolat;Etienne Dejoie;Marc-André Legault;Marie-Pierre Dubé;Julie G. Hussin;Yoshua Bengio,adriana.romero.soriano@umontreal.ca;pierre-luc.carrier@umontreal.ca;akram.er-raqabi@umontreal.ca;Tristan.sylvain@umontreal.ca;alexis211@gmail.com;etiennedejoie@gmail.com;marc-andre.legault.1@umontreal.ca;julieh@well.ox.ac.uk;yoshua.umontreal@gmail.com,6;7;8,4;3;3,Accept (Poster),2,12,0,no,11/4/16,University of Montreal;University of Montreal;University of Montreal;University of Montreal;;;University of Montreal;University of Oxford;University of Montreal,113;113;113;113;-1;-1;113;50;113,103;103;103;103;-1;-1;103;1;103,,11/4/16,37,9,10,0,46,2,3868;2363;65;84;212;32;141;2760;319;205027,52;12;10;14;12;2;15;144;11;807,14;7;5;3;5;2;7;28;9;147,641;224;7;4;30;1;4;177;23;24136,-1;-1
93,ICLR,2017,Autoencoding Variational Inference For Topic Models,Akash Srivastava;Charles Sutton,akash.srivastava@ed.ac.uk;csutton@inf.ed.ac.uk,6;7;6,4;3;5,Accept (Poster),9,8,0,no,11/4/16,University of Edinburgh;University of Edinburgh,33;33,27;27,,11/4/16,132,58,66,8,0,46,478;4993,49;91,7;23,99;564,-1;-1
94,ICLR,2017,Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data,Maximilian Karl;Maximilian Soelch;Justin Bayer;Patrick van der Smagt,karlma@in.tum.de;m.soelch@tum.de;bayer.justin@googlemail.com;smagt@brml.org,6;7;6,4;3;3,Accept (Poster),2,5,0,no,11/4/16,"Technical University Munich;Technical University Munich;Data Lab, Volkswagen Group;TU Munich",54;54;-1;54,46;46;-1;30,,5/20/16,129,65,45,6,45,19,281;198;2298;3939,18;7;44;122,5;3;11;26,30;26;179;417,-1;-1
95,ICLR,2017,Why Deep Neural Networks for Function Approximation?,Shiyu Liang;R. Srikant,sliang26@illinois.edu;rsrikant@illinois.edu,7;7;7,4;4;3,Accept (Poster),2,3,0,no,11/1/16,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",4;4,36;36,,10/13/16,147,75,15,4,120,11,600;17005,16;428,8;69,96;1494,-1;-1
96,ICLR,2017,Reinforcement Learning with Unsupervised Auxiliary Tasks,Max Jaderberg;Volodymyr Mnih;Wojciech Marian Czarnecki;Tom Schaul;Joel Z Leibo;David Silver;Koray Kavukcuoglu,jaderberg@google.com;vmnih@google.com;lejlot@google.com;schaul@google.com;jzl@google.com;davidsilver@google.com;korayk@google.com,7;8;8,4;4;4,Accept (Oral),12,4,0,no,11/4/16,Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,,11/4/16,564,332,140,15,0,53,7475;21076;2316;8477;3502;42682;53136,31;38;44;83;75;159;90,21;27;20;30;29;56;58,928;3370;246;1128;352;5911;6857,-1;-1
97,ICLR,2017,Improving Policy Gradient by Exploring Under-appreciated Rewards,Ofir Nachum;Mohammad Norouzi;Dale Schuurmans,ofirnachum@google.com;mnorouzi@google.com;schuurmans@google.com,7;8;7,4;4;4,Accept (Poster),5,4,3,no,11/4/16,Google;Google;Google,-1;-1;-1,-1;-1;-1,,11/4/16,19,11,3,1,18,1,1047;7953;6115,41;125;247,15;31;41,155;1008;639,-1;-1
98,ICLR,2017,Mode Regularized Generative Adversarial Networks,Tong Che;Yanran Li;Athul Jacob;Yoshua Bengio;Wenjie Li,tong.che@umontreal.ca;csyli@comp.polyu.edu.hk;ap.jacob@umontreal.ca;yoshua.bengio@umontreal.ca;cswjli@comp.polyu.edu.hk,4;7;7,4;4;4,Accept (Poster),9,11,0,no,11/5/16,University of Montreal;The Hong Kong Polytechnic University;University of Montreal;University of Montreal;The Hong Kong Polytechnic University,113;194;113;113;194,103;192;103;103;192,5;4,11/5/16,258,142,81,6,17,40,768;833;362;205027;4181,24;35;6;807;340,8;9;5;147;32,103;127;55;24136;441,-1;-1
99,ICLR,2017,An Actor-Critic Algorithm for Sequence Prediction,Dzmitry Bahdanau;Philemon Brakel;Kelvin Xu;Anirudh Goyal;Ryan Lowe;Joelle Pineau;Aaron Courville;Yoshua Bengio,dimabgv@gmail.com;pbpop3@gmail.com;iamkelvinxu@gmail.com;anirudhgoyal9119@gmail.com;lowe.ryan.t@gmail.com;jpineau@cs.mcgill.ca;aaron.courville@gmail.com;yoshua.bengio@gmail.com,4;8;8,4;5;4,Accept (Poster),5,9,0,no,11/2/16,University of Montreal;University of Montreal;Google;University of Montreal;McGill University;McGill University;University of Montreal;,113;113;-1;113;80;80;113;-1,103;103;-1;103;42;42;103;-1,3,7/24/16,313,154,151,8,85,50,26989;1723;7412;1125;3322;11249;61043;205027,31;21;19;46;53;266;203;807,18;16;11;12;17;45;65;147,4179;161;757;127;516;1227;7900;24136,-1;-1
100,ICLR,2017,Trusting SVM for Piecewise Linear CNNs,Leonard Berrada;Andrew Zisserman;M. Pawan Kumar,lberrada@robots.ox.ac.uk;az@robots.ox.ac.uk;pawan@robots.ox.ac.uk,5;4;6,4;4;4,Accept (Poster),1,4,0,no,11/4/16,University of Oxford;University of Oxford;University of Oxford,50;50;50,1;1;1,,11/4/16,8,2,3,0,0,0,52;139335;2640,5;797;83,4;140;24,6;20052;247,-1;-1
101,ICLR,2017,End-to-end Optimized Image Compression,Johannes Ballé;Valero Laparra;Eero P. Simoncelli,johannes.balle@nyu.edu;valero.laparra@uv.es;eero.simoncelli@nyu.edu,8;9;8;8,4;4;4;3,Accept (Oral),3,3,0,no,11/5/16,New York University;Universitat de València;New York University,25;-1;25,32;-1;32,5,11/5/16,326,167,156,19,0,91,1165;1232;28284,40;77;298,14;18;72,223;157;3139,-1;-1
102,ICLR,2017,"Snapshot Ensembles: Train 1, Get M for Free",Gao Huang;Yixuan Li;Geoff Pleiss;Zhuang Liu;John E. Hopcroft;Kilian Q. Weinberger,gh349@cornell.edu;yl2363@cornell.edu;geoff@cs.cornell.edu;liuzhuangthu@gmail.com;jeh@cs.cornell.edu;kqw4@cornell.edu,9;7;8,4;5;3,Accept (Poster),8,17,0,no,11/4/16,Cornell University;Cornell University;Cornell University;Tsinghua University;Cornell University;Cornell University,7;7;7;11;7;7,19;19;19;35;19;19,,11/4/16,254,100,105,7,85,43,12433;2322;1523;35438;28680;23933,60;162;17;341;303;165,22;20;11;85;60;54,2077;267;242;2366;2732;3840,-1;-1
103,ICLR,2017,Learning Features of Music From Scratch,John Thickstun;Zaid Harchaoui;Sham Kakade,thickstn@cs.washington.edu;sham@cs.washington.edu;zaid@uw.edu,8;6;6,4;4;4,Accept (Poster),3,3,0,no,11/4/16,"University of Washington;University of Washington;University of Washington, Seattle",6;6;6,25;25;25,,11/4/16,73,32,30,0,0,13,133;4856;13591,16;57;197,3;35;58,26;681;1976,-1;-1
104,ICLR,2017,DeepCoder: Learning to Write Programs,Matej Balog;Alexander L. Gaunt;Marc Brockschmidt;Sebastian Nowozin;Daniel Tarlow,matej.balog@gmail.com;t-algaun@microsoft.com;mabrocks@microsoft.com;Sebastian.Nowozin@microsoft.com;dtarlow@microsoft.com,6;6;7,4;4;2,Accept (Poster),7,7,0,no,11/4/16,University of Cambridge;Microsoft;Microsoft;Microsoft;Microsoft,67;-1;-1;-1;-1,4;-1;-1;-1;-1,4,11/4/16,219,121,82,3,0,33,270;1194;2508;6959;2537,8;38;61;134;68,4;15;22;39;23,40;88;329;931;306,-1;-1
105,ICLR,2017,A Compare-Aggregate Model for Matching Text Sequences,Shuohang Wang;Jing Jiang,shwang.2014@phdis.smu.edu.sg;jingjiang@smu.edu.sg,6;7;8,4;5;5,Accept (Poster),4,4,0,no,11/4/16,Singapore Management University;Singapore Management University,87;87,981;981,3,11/4/16,174,69,63,7,4,27,1104;7075,21;202,8;32,212;794,-1;-1
106,ICLR,2017,Automatic Rule Extraction from Long Short Term Memory Networks,W. James Murdoch;Arthur Szlam,jmurdoch@berkeley.edu;aszlam@fb.com,7;7;7,4;3;3,Accept (Poster),1,5,2,no,11/4/16,University of California Berkeley;Facebook,5;-1,10;-1,3,11/4/16,58,22,19,0,45,5,352;8769,10;86,6;32,29;923,-1;-1
107,ICLR,2017,Capacity and Trainability in Recurrent Neural Networks,Jasmine Collins;Jascha Sohl-Dickstein;David Sussillo,jlcollins@google.com;jaschasd@google.com;sussillo@google.com,8;7;8,4;4;4,Accept (Poster),1,5,0,no,11/4/16,Google;Google;Google,-1;-1;-1,-1;-1;-1,1,11/4/16,102,60,25,3,68,11,268;5018;3281,15;101;45,5;33;25,19;703;267,-1;-1
108,ICLR,2017,Learning to Repeat: Fine Grained Action Repetition for Deep Reinforcement Learning,Sahil Sharma;Aravind S. Lakshminarayanan;Balaraman Ravindran,ssahil08@gmail.com;aravindsrinivas@gmail.com;ravi@cse.iitm.ac.in,8;7;8,5;3;4,Accept (Poster),4,6,1,no,11/4/16,Indian Institute of Technology Madras;University of Montreal;Indian Institute of Technology Madras,140;113;140,472;103;472,,11/4/16,33,16,7,0,0,3,235;167;2503,43;13;234,10;7;28,13;9;202,-1;-1
109,ICLR,2017,Multi-view Recurrent Neural Acoustic Word Embeddings,Wanjia He;Weiran Wang;Karen Livescu,wanjia@ttic.edu;weiranwang@ttic.edu;klivescu@ttic.edu,5;6;6,4;4;3,Accept (Poster),2,1,0,no,11/4/16,Toyota Technological Institute at Chicago;Toyota Technological Institute at Chicago;Toyota Technological Institute at Chicago,-1;-1;-1,-1;-1;-1,3,11/4/16,49,29,16,3,3,5,49;1479;5361,1;63;128,1;18;35,5;173;685,-1;-1
110,ICLR,2017,Neuro-Symbolic Program Synthesis,Emilio Parisotto;Abdel-rahman Mohamed;Rishabh Singh;Lihong Li;Dengyong Zhou;Pushmeet Kohli,eparisot@andrew.cmu.edu;asamir@microsoft.com;risin@microsoft.com;lihongli@microsoft.com;denzho@microsoft.com;pkohli@microsoft.com,5;7;8,4;3;4,Accept (Poster),2,4,0,no,11/4/16,Carnegie Mellon University;Microsoft;Microsoft;Microsoft;Microsoft;Microsoft,1;-1;-1;-1;-1;-1,23;-1;-1;-1;-1;-1,,11/4/16,141,73,53,1,0,16,882;19117;2166;10762;8962;22310,17;56;80;242;78;313,10;27;24;47;34;69,76;1201;184;1192;1344;2770,-1;-1
111,ICLR,2017,Learning to Remember Rare Events,Lukasz Kaiser;Ofir Nachum;Aurko Roy;Samy Bengio,lukaszkaiser@google.com;ofirnachum@google.com;aurko@gatech.edu;bengio@google.com,6;8;7,5;3;4,Accept (Poster),4,3,0,no,11/4/16,Google;Google;Georgia Institute of Technology;Google,-1;-1;12;-1,-1;-1;33;-1,3,11/4/16,172,95,53,6,0,30,22730;1047;1052;26525,86;41;21;332,24;15;10;67,3893;155;116;3482,-1;-1
112,ICLR,2017,Discrete Variational Autoencoders,Jason Tyler Rolfe,jrolfe@dwavesys.com,8;9;8,2;4;4,Accept (Poster),9,8,0,no,11/4/16,D-Wave Systems,-1,-1,5,9/7/16,85,39,25,1,61,1,242,7,3,17,-1
113,ICLR,2017,Understanding Trainable Sparse Coding with Matrix Factorization,Thomas Moreau;Joan Bruna,thomas.moreau@cmla.ens-cachan.fr;joan.bruna@berkeley.edu,6;8;5,3;4;2,Accept (Poster),1,3,0,no,11/2/16,ENS Paris-Saclay;University of California Berkeley,462;5,475;10,1,9/1/16,30,11,12,2,4,6,149;11503,41;90,7;29,11;1285,-1;-1
114,ICLR,2017,"Learning to Query, Reason, and Answer Questions On Ambiguous Texts",Xiaoxiao Guo;Tim Klinger;Clemens Rosenbaum;Joseph P. Bigus;Murray Campbell;Ban Kawas;Kartik Talamadupula;Gerry Tesauro;Satinder   Singh,tklinger@us.ibm.com;guoxiao@umich.edu;cgbr@cs.umass.edu;jbigus@us.ibm.com;mcam@us.ibm.com;bkawas@us.ibm.com;krtalamad@us.ibm.com;gtesauro@us.ibm.com;baveja@umich.edu,6;7;7,3;4;3,Accept (Poster),5,2,0,no,11/4/16,"International Business Machines;University of Michigan;University of Massachusetts, Amherst;International Business Machines;International Business Machines;International Business Machines;International Business Machines;International Business Machines;University of Michigan",-1;8;25;-1;-1;-1;-1;-1;8,-1;21;166;-1;-1;-1;-1;-1;21,,11/4/16,13,7,2,0,0,0,1508;638;501;1028;1574;91;782;7589;630,42;34;70;27;57;17;71;124;71,13;12;11;12;19;6;15;45;12,214;115;35;70;130;15;53;716;57,-1;-1
115,ICLR,2017,Variable Computation in Recurrent Neural Networks,Yacine Jernite;Edouard Grave;Armand Joulin;Tomas Mikolov,yacine.jernite@nyu.edu;egrave@fb.com;ajoulin@fb.com;tmikolov@fb.com,7;7;4,4;4;5,Accept (Poster),2,6,0,no,11/4/16,New York University;Facebook;Facebook;Facebook,25;-1;-1;-1,32;-1;-1;-1,,11/4/16,29,19,6,0,0,5,1366;7642;10538;-1,20;56;74;-1,11;23;32;-1,158;1135;1531;0,-1;-1
116,ICLR,2017,Learning and Policy Search in Stochastic Dynamical Systems with Bayesian Neural Networks,Stefan Depeweg;José Miguel Hernández-Lobato;Finale Doshi-Velez;Steffen Udluft,stefan.depeweg@siemens.com;jmh233@cam.ac.uk;finale@seas.harvard.edu;steffen.udluft@siemens.com,7;6;7,3;3;3,Accept (Poster),2,4,0,no,11/4/16,Siemens Corporate Research;University of Cambridge;Harvard University;Siemens Corporate Research,-1;67;36;-1,-1;4;6;-1,11,5/23/16,86,39,39,4,14,3,199;3790;2520;705,10;114;109;86,5;28;30;16,13;422;237;33,-1;-1
117,ICLR,2017,Reinforcement Learning through Asynchronous Advantage Actor-Critic on a GPU,Mohammad Babaeizadeh;Iuri Frosio;Stephen Tyree;Jason Clemons;Jan Kautz,mb2@uiuc.edu;ifrosio@nvidia.com;styree@nvidia.com;jclemons@nvidia.com;jkautz@nvidia.com,5;7;8,5;5;3,Accept (Poster),7,2,0,no,11/4/16,"University of Illinois, Urbana-Champaign;NVIDIA;NVIDIA;NVIDIA;NVIDIA",4;-1;-1;-1;-1,36;-1;-1;-1;-1,,11/4/16,80,27,37,1,29,16,620;1101;2424;460;13993,19;41;42;25;301,8;13;17;11;58,92;97;269;56;1888,-1;-1
118,ICLR,2017,Reasoning with Memory Augmented Neural Networks for Language Comprehension,Tsendsuren Munkhdalai;Hong Yu,tsendsuren.munkhdalai@umassmed.edu;hong.yu@umassmed.edu,7;6;7,2;3;4,Accept (Poster),1,6,0,no,11/3/16,UMass;UMass,25;25,166;166,,10/20/16,23,8,3,2,33,2,706;1360,30;50,12;11,41;88,-1;-1
119,ICLR,2017,Learning a Natural Language Interface with Neural Programmer,Arvind Neelakantan;Quoc V. Le;Martin Abadi;Andrew McCallum;Dario Amodei,arvind@cs.umass.edu;qvl@google.com;abadi@google.com;mccallum@cs.umass.edu;damodei@openai.com,7;6;6,3;3;4,Accept (Poster),3,6,1,no,11/4/16,"University of Massachusetts, Amherst;Google;Google;University of Massachusetts, Amherst;OpenAI",25;-1;-1;25;-1,166;-1;-1;166;-1,3,11/4/16,96,58,35,5,0,8,1483;48299;28596;44752;4991,24;193;277;434;30,14;81;70;96;21,162;6043;3379;4843;613,-1;-1
120,ICLR,2017,Loss-aware Binarization of Deep Networks,Lu Hou;Quanming Yao;James T. Kwok,lhouab@cse.ust.hk;qyaoaa@cse.ust.hk;jamesk@cse.ust.hk,7;7;7,3;4;3,Accept (Poster),1,14,0,no,11/4/16,The Hong Kong University of Science and Technology;The Hong Kong University of Science and Technology;The Hong Kong University of Science and Technology,39;39;39,48;48;48,,11/4/16,100,52,51,8,5,10,450;927;9607,31;58;197,9;15;51,30;105;1053,-1;-1
121,ICLR,2017,Hyperband: Bandit-Based Configuration Evaluation for Hyperparameter Optimization,Lisha Li;Kevin Jamieson;Giulia DeSalvo;Afshin Rostamizadeh;Ameet Talwalkar,lishal@cs.ucla.edu;kjamieson@berkeley.edu;desalvo@cims.nyu.edu;rostami@google.com;ameet@cs.ucla.edu,8;7;7,4;5;4,Accept (Poster),3,4,1,no,11/4/16,"University of California, Los Angeles;University of California Berkeley;New York University;Google;University of California, Los Angeles",20;5;25;-1;20,14;10;32;-1;14,11,11/4/16,80,43,38,3,0,21,213;1613;724;4296;6399,150;47;22;61;78,6;17;8;23;34,36;241;112;608;766,-1;-1
122,ICLR,2017,Nonparametric Neural Networks,George Philipp;Jaime G. Carbonell,george.philipp@email.de;jgc@cs.cmu.edu,7;7;5,3;4;4,Accept (Poster),2,9,0,no,11/5/16,Carnegie Mellon University;Carnegie Mellon University,1;1,23;23,1,11/5/16,12,6,5,0,18,0,53;15872,13;507,5;55,8;1633,-1;-1
123,ICLR,2017,Improving Neural Language Models with a Continuous Cache,Edouard Grave;Armand Joulin;Nicolas Usunier,egrave@fb.com;ajoulin@fb.com;usunier@fb.com,7;5;9,5;4;5,Accept (Poster),6,1,0,no,11/4/16,Facebook;Facebook;Facebook,-1;-1;-1,-1;-1;-1,3,11/4/16,159,86,59,10,0,33,7642;10538;6183,56;74;109,23;32;30,1135;1531;1175,-1;-1
124,ICLR,2017,Episodic Exploration for Deep Deterministic Policies for StarCraft Micromanagement,Nicolas Usunier;Gabriel Synnaeve;Zeming Lin;Soumith Chintala,usunier@fb.com;gab@fb.com;zlin@fb.com;soumith@fb.com,7;7;8,4;4;4,Accept (Poster),2,0,0,no,11/4/16,Facebook;Facebook;Facebook;Facebook,-1;-1;-1;-1,-1;-1;-1;-1,,9/10/16,101,42,25,6,0,13,6183;1501;6040;18902,109;62;19;31,30;19;9;18,1175;154;744;2820,-1;-1
125,ICLR,2017,Lossy Image Compression with Compressive Autoencoders,Lucas Theis;Wenzhe Shi;Andrew Cunningham;Ferenc Huszár,ltheis@twitter.com;wshi@twitter.com;acunningham@twitter.com;fhuszar@twitter.com,8;7;5,5;3;4,Accept (Poster),7,12,2,no,11/4/16,Twitter;Twitter;Twitter;Twitter,-1;-1;-1;-1,-1;-1;-1;-1,,11/4/16,314,155,130,11,0,55,5534;3503;623;6129,45;81;43;21,18;22;9;15,661;337;62;817,-1;-1
126,ICLR,2017,Temporal Ensembling for Semi-Supervised Learning,Samuli Laine;Timo Aila,slaine@nvidia.com;taila@nvidia.com,7;9;8,4;4;5,Accept (Poster),3,3,1,no,11/4/16,NVIDIA;NVIDIA,-1;-1,-1;-1,,10/7/16,408,207,202,18,23,97,5455;6174,72;68,26;32,911;993,-1;-1
127,ICLR,2017,Optimization as a Model for Few-Shot Learning,Sachin Ravi;Hugo Larochelle,sachinr@twitter.com;hugo@twitter.com;sachinr@princeton.edu,8;9;6,4;5;4,Accept (Oral),4,5,0,no,11/4/16,Twitter;Twitter;Princeton University,-1;-1;32,-1;-1;7,6,11/4/16,967,459,371,21,0,147,1340;24882,22;123,10;44,197;2864,-1;-1
128,ICLR,2017,Towards a Neural Statistician,Harrison Edwards;Amos Storkey,h.l.edwards@sms.ed.ac.uk;amos.storkey@ed.ac.uk,8;8;6,2;4;4,Accept (Poster),2,3,0,no,11/4/16,University of Edinburgh;University of Edinburgh,33;33,27;27,5,6/7/16,184,75,75,5,12,18,1169;3868,18;198,8;32,191;441,-1;-1
129,ICLR,2017,Faster CNNs with Direct Sparse Convolutions and Guided Pruning,Jongsoo Park;Sheng Li;Wei Wen;Ping Tak Peter Tang;Hai Li;Yiran Chen;Pradeep Dubey,jongsoo.park@intel.com;sheng.r.li@intel.com;peter.tang@intel.com;weiwen.web@gmail.com;HAL66@pitt.edu;yic52@pitt.edu;pradeep.dubey@intel.com,6;7;6,3;3;3,Accept (Poster),0,3,0,no,11/4/16,Intel;Intel;Intel;University of Pittsburgh;University of Pittsburgh;University of Pittsburgh;Intel,-1;-1;-1;78;78;78;-1,-1;-1;-1;80;80;80;-1,,8/4/16,86,56,27,0,0,15,581;130;2362;2234;162;617;8767,58;5;98;72;26;74;250,12;3;19;20;6;12;46,54;23;262;209;17;48;929,-1;-1
130,ICLR,2017,Boosted Generative Models,Aditya Grover;Stefano Ermon,adityag@cs.stanford.edu;ermon@cs.stanford.edu,5;6;5,3;3;3,Reject,1,4,0,no,11/5/16,Stanford University;Stanford University,3;3,3;3,5,11/5/16,23,8,9,1,39,5,3541;4911,54;203,16;31,798;661,-1;-1
131,ICLR,2017,Deep Predictive Coding Networks for Video Prediction and Unsupervised Learning,William Lotter;Gabriel Kreiman;David Cox,lotter@fas.harvard.edu;gabriel.kreiman@tch.harvard.edu;davidcox@fas.harvard.edu,6;8;8,3;4;5,Accept (Poster),6,0,0,no,11/4/16,Harvard University;Harvard University;Harvard University,36;36;36,6;6;6,,5/25/16,407,216,139,12,0,75,582;11225;1294,11;184;70,5;33;15,90;901;167,-1;-1
132,ICLR,2017, Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer,Noam Shazeer;*Azalia Mirhoseini;*Krzysztof Maziarz;Andy Davis;Quoc Le;Geoffrey Hinton;Jeff Dean,noam@google.com;azalia@google.com;krzysztof.maziarz@student.uj.edu.pl;andydavis@google.com;qvl@google.com;geoffhinton@google.com;jeff@google.com,7;6;7,4;4;4,Accept (Poster),4,10,0,no,11/4/16,Google;Google;Jagiellonian University;Google;Google;Google;Google,-1;-1;462;-1;-1;-1;-1,-1;-1;625;-1;-1;-1;-1,3,11/4/16,440,228,124,11,0,33,12762;1074;445;15186;48310;214640;1958,44;56;8;21;193;415;27,19;14;2;10;81;127;9,2769;76;33;1839;6042;21599;288,-1;-1
133,ICLR,2017,Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic,Shixiang Gu;Timothy Lillicrap;Zoubin Ghahramani;Richard E. Turner;Sergey Levine,sg717@cam.ac.uk;countzero@google.com;zoubin@eng.cam.ac.uk;ret26@cam.ac.uk;svlevine@eecs.berkeley.edu,7;7;7;8,4;5;4;3,Accept (Oral),1,11,9,no,11/4/16,University of Cambridge;Google;University of Cambridge;University of Cambridge;University of California Berkeley,67;-1;67;67;5,4;-1;4;4;10,,11/4/16,197,117,82,11,0,15,3813;23840;43086;3005;24658,39;74;463;175;310,21;39;92;30;73,474;2911;5089;314;3204,-1;-1
134,ICLR,2017,Recurrent Batch Normalization,Tim Cooijmans;Nicolas Ballas;César Laurent;Çağlar Gülçehre;Aaron Courville,tim.cooijmans@umontreal.ca;nicolas.ballas@umontreal.ca;cesar.laurent@umontreal.ca;caglar.gulcehre@umontreal.ca;aaron.courville@umontreal.ca,7;8;7,4;4;4,Accept (Poster),3,4,0,no,11/4/16,University of Montreal;University of Montreal;University of Montreal;University of Montreal;University of Montreal,113;113;113;113;113,103;103;103;103;103,3;8,3/30/16,271,100,101,16,0,34,1962;5005;2258;202;61116,8;54;10;11;203,5;20;6;3;65,170;589;180;14;7900,-1;-1
135,ICLR,2017,Inductive Bias of Deep Convolutional Networks through Pooling Geometry,Nadav Cohen;Amnon Shashua,cohennadav@cs.huji.ac.il;shashua@cs.huji.ac.il,7;7;6,5;3;3,Accept (Poster),1,3,0,no,11/3/16,Hebrew University of Jerusalem;Hebrew University of Jerusalem,57;57,186;186,2,5/22/16,44,27,8,4,0,1,1063;8307,63;173,15;48,117;727,-1;-1
136,ICLR,2017,Incremental Network Quantization: Towards Lossless CNNs with Low-precision Weights,Aojun Zhou;Anbang Yao;Yiwen Guo;Lin Xu;Yurong Chen,aojun.zhou@intel.com;anbang.yao@intel.com;yiwen.guo@intel.com;lin.x.xu@intel.com;yurong.chen@intel.com,7;8;7,3;4;4,Accept (Poster),2,2,0,no,11/4/16,Intel;Intel;Intel;Intel;Intel,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,,11/4/16,454,202,176,7,22,55,549;1853;1030;3307;3035,11;33;25;241;102,5;15;7;26;23,63;239;156;276;337,-1;-1
137,ICLR,2017,,,,4;7;7,4;3;3,Reject,0,0,1,no,11/4/16,,,,,11/4/16,-1,-1,-1,-1,-1,-1,-1,-1,-1,0,-1
138,ICLR,2017,DSD: Dense-Sparse-Dense Training for Deep Neural Networks,Song Han;Jeff Pool;Sharan Narang;Huizi Mao;Enhao Gong;Shijian Tang;Erich Elsen;Peter Vajda;Manohar Paluri;John Tran;Bryan Catanzaro;William J. Dally,songhan@stanford.edu;jpool@nvidia.com;sharan@baidu.com;huizi@stanford.edu;enhaog@stanford.edu;sjtang@stanford.edu;eriche@google.com;vajdap@fb.com;mano@fb.com;johntran@nvidia.com;bcatanzaro@nvidia.com;dally@stanford.edu,8;8;5,3;3;4,Accept (Poster),3,3,0,no,11/4/16,Stanford University;NVIDIA;Baidu;Stanford University;Stanford University;Stanford University;Google;Facebook;Facebook;NVIDIA;NVIDIA;Stanford University,3;-1;-1;3;3;3;-1;-1;-1;-1;-1;3,3;-1;-1;3;3;3;-1;-1;-1;-1;-1;3,,7/15/16,79,25,25,3,50,9,15090;2695;2567;5884;643;395;4626;670;5525;3661;9287;30398,374;38;15;22;40;19;53;47;38;49;73;268,36;11;10;14;12;9;21;10;20;13;28;67,1942;369;275;752;40;34;488;85;1083;445;1071;3737,-1;-1
139,ICLR,2017,TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency,Adji B. Dieng;Chong Wang;Jianfeng Gao;John Paisley,abd2141@columbia.edu;chowang@microsoft.com;jfgao@microsoft.com;jpaisley@columbia.edu,6;7;8,3;4;4,Accept (Poster),2,7,2,no,11/4/16,Columbia University;Microsoft;Microsoft;Columbia University,14;-1;-1;14,16;-1;-1;16,3,11/4/16,123,54,48,1,59,23,468;18533;18692;4414,14;1045;353;97,7;56;61;27,60;1645;2662;682,-1;-1
140,ICLR,2017,Efficient Representation of Low-Dimensional Manifolds using Deep Networks,Ronen Basri;David W. Jacobs,ronen.basri@weizmann.ac.il;djacobs@cs.umd.edu,6;5;7,3;3;5,Accept (Poster),1,3,0,no,11/3/16,"Weizmann Institute;University of Maryland, College Park",105;12,981;67,,2/15/16,26,15,4,0,6,1,10682;11869,174;215,45;49,1270;1611,-1;-1
141,ICLR,2017,Training deep neural-networks using a noise adaptation layer,Jacob Goldberger;Ehud Ben-Reuven,jacob.goldberger@biu.ac.il;udi.benreuven@gmail.com,5;7;5,4;5;5,Accept (Poster),2,3,0,no,11/4/16,Bar Ilan University;,87;-1,489;-1,,11/4/16,160,71,75,4,0,18,5219;168,180;10,35;3,548;19,-1;-1
142,ICLR,2017,SampleRNN: An Unconditional End-to-End Neural Audio Generation Model,Soroush Mehri;Kundan Kumar;Ishaan Gulrajani;Rithesh Kumar;Shubham Jain;Jose Sotelo;Aaron Courville;Yoshua Bengio,soroush.mehri@umontreal.ca;kundankumar2510@gmail.com;igul222@gmail.com;ritheshkumar.95@gmail.com;shubhamjain1310@gmail.com;rdz.sotelo@gmail.com;aaron.courville@umontreal.ca;yoshua.bengio@umontreal.ca,9;8;8,4;4;3,Accept (Poster),4,5,0,no,11/4/16,University of Montreal;University of Montreal;University of Montreal;Sri Sivasubramaniya Nadar College Of Engineering;;;University of Montreal;University of Montreal,113;113;113;-1;-1;-1;113;113,103;103;103;-1;-1;-1;103;103,,11/4/16,274,118,125,4,0,58,735;1329;4256;355;663;871;61116;205096,8;106;10;9;44;25;203;807,6;14;7;4;11;9;65;147,109;131;911;73;95;107;7900;24136,-1;-1
143,ICLR,2017,FractalNet: Ultra-Deep Neural Networks without Residuals,Gustav Larsson;Michael Maire;Gregory Shakhnarovich,larsson@cs.uchicago.edu;mmaire@ttic.edu;greg@ttic.edu,6;6;5,5;4;5,Accept (Poster),2,2,0,no,11/4/16,University of Chicago;Toyota Technological Institute at Chicago;Toyota Technological Institute at Chicago,45;-1;-1,10;-1;-1,,5/24/16,389,157,122,11,0,58,897;15886;6256,7;62;98,5;24;34,115;2798;604,-1;-1
144,ICLR,2017,Towards the Limit of Network Quantization,Yoojin Choi;Mostafa El-Khamy;Jungwon Lee,yoojin.c@samsung.com;mostafa.e@samsung.com;jungwon2.lee@samsung.com,7;7;7,3;3;4,Accept (Poster),3,5,0,no,11/4/16,Samsung;Samsung;Samsung,-1;-1;-1,-1;-1;-1,,11/4/16,71,36,36,5,5,13,459;1094;2287,35;102;166,9;17;25,48;110;236,f;m
145,ICLR,2017,Understanding deep learning requires rethinking generalization,Chiyuan Zhang;Samy Bengio;Moritz Hardt;Benjamin Recht;Oriol Vinyals,chiyuan@mit.edu;bengio@google.com;mrtz@google.com;brecht@berkeley.edu;vinyals@google.com,10;9;10,4;3;4,Accept (Oral),2,33,9,no,11/4/16,Massachusetts Institute of Technology;Google;Google;University of California Berkeley;Google,2;-1;-1;5;-1,5;-1;-1;10;-1,8,11/4/16,1863,1308,249,72,0,193,5550;26525;7775;20413;52829,74;332;89;141;121,27;67;33;55;55,631;3482;962;2665;6560,-1;-1
146,ICLR,2017,Optimal Binary Autoencoding with Pairwise Correlations,Akshay Balsubramani,abalsubr@stanford.edu,7;7;6,3;2;4,Accept (Poster),0,2,0,no,11/4/16,Stanford University,3,3,9,11/4/16,1,0,0,0,0,0,284,25,7,33,-1
147,ICLR,2017,Metacontrol for Adaptive Imagination-Based Optimization,Jessica B. Hamrick;Andrew J. Ballard;Razvan Pascanu;Oriol Vinyals;Nicolas Heess;Peter W. Battaglia,jhamrick@berkeley.edu;aybd@google.com;razp@google.com;vinyals@google.com;heess@google.com;peterbattaglia@google.com,8;8;7;8,3;3;3;3,Accept (Poster),0,4,0,no,11/4/16,University of California Berkeley;Google;Google;Google;Google;Google,5;-1;-1;-1;-1;-1,10;-1;-1;-1;-1;-1,,11/4/16,49,24,15,0,11,0,2273;807;16936;52829;11482;4546,51;42;101;121;104;88,15;9;46;55;37;29,166;86;1688;6560;1630;423,-1;-1
148,ICLR,2017,Sparsely-Connected Neural Networks: Towards Efficient VLSI Implementation of Deep Neural Networks,Arash Ardakani;Carlo Condo;Warren J. Gross,arash.ardakani@mail.mcgill.ca;carlo.condo@mail.mcgill.ca;warren.gross@mcgill.ca,6;7;6,4;3;3,Accept (Poster),2,6,0,no,11/3/16,McGill University;McGill University;McGill University,80;80;80,42;42;42,,11/3/16,21,10,9,1,7,0,164;663;4317,19;71;253,8;15;31,13;47;351,-1;-1
149,ICLR,2017,Calibrating Energy-based Generative Adversarial Networks,Zihang Dai;Amjad Almahairi;Philip Bachman;Eduard Hovy;Aaron Courville,zander.dai@gmail.com;amjadmahayri@gmail.com;phil.bachman@gmail.com;hovy@cmu.edu;aaron.courville@gmail.com,8;7;8,4;5;4,Accept (Poster),0,7,0,no,11/4/16,Carnegie Mellon University;University of Montreal;Maluuba;Carnegie Mellon University;University of Montreal,1;113;-1;1;113,23;103;-1;23;103,5;4;1,11/4/16,71,27,21,0,0,7,2410;2014;1764;24388;61116,27;11;31;611;203,14;7;14;76;65,436;171;223;2531;7900,-1;-1
150,ICLR,2017,Designing Neural Network Architectures using Reinforcement Learning,Bowen Baker;Otkrist Gupta;Nikhil Naik;Ramesh Raskar,bowen@mit.edu;otkrist@mit.edu;naik@mit.edu;raskar@mit.edu,6;6;6,4;3;4,Accept (Poster),4,6,0,no,11/4/16,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2,5;5;5;5,,11/4/16,548,233,247,7,0,66,1246;1437;1123;16801,13;31;25;556,7;13;11;64,145;156;127;1126,-1;-1
151,ICLR,2017,Categorical Reparameterization with Gumbel-Softmax,Eric Jang;Shixiang Gu;Ben Poole,ejang@google.com;sg717@cam.ac.uk;poole@cs.stanford.edu,7;6;6,5;4;3,Accept (Poster),4,3,0,no,11/4/16,Google;University of Cambridge;Stanford University,-1;67;3,-1;4;3,5,11/3/16,1212,364,839,16,0,238,1950;3813;4239,15;39;41,11;21;19,292;474;695,-1;-1
152,ICLR,2017,PixelVAE: A Latent Variable Model for Natural Images,Ishaan Gulrajani;Kundan Kumar;Faruk Ahmed;Adrien Ali Taiga;Francesco Visin;David Vazquez;Aaron Courville,igul222@gmail.com;kundankumar2510@gmail.com;faruk.ahmed.91@gmail.com;adrien.alitaiga@gmail.com;francesco.visin@polimi.it;dvazquez@cvc.uab.es;aaron.courville@gmail.com,7;6;7,4;3;3,Accept (Poster),3,3,0,no,11/4/16,"University of Montreal;University of Montreal;University of Montreal;University of Montreal;Politecnico di Milano;Computer Vision Center, Universitat Autònoma de Barcelona;University of Montreal",113;113;113;113;140;462;113,103;103;103;103;981;164;103,5,11/4/16,186,101,69,2,8,31,4256;1329;3635;244;2765;2813;61116,10;106;62;8;11;286;203,7;14;12;5;9;27;65,911;131;778;34;222;135;7900,-1;-1
153,ICLR,2017,Transfer of View-manifold Learning to Similarity Perception of Novel Objects,Xingyu Lin;Hao Wang;Zhihao Li;Yimeng Zhang;Alan Yuille;Tai Sing Lee,sean.linxingyu@pku.edu.cn;hao.wang@pku.edu.cn;zhihaol@andrew.cmu.edu;yimengzh@andrew.cmu.edu;alan.yuille@jhu.edu;tai@cnbc.cmu.edu,5;6;7,5;4;3,Accept (Poster),2,4,0,no,11/5/16,Peking University;Peking University;Carnegie Mellon University;Carnegie Mellon University;Johns Hopkins University;Carnegie Mellon University,25;25;1;1;67;1,29;29;23;23;17;23,,11/5/16,9,7,2,0,8,0,269;-1;2073;609;33029;4751,32;-1;95;40;494;161,9;-1;22;10;80;25,6;0;105;56;3745;361,-1;-1
154,ICLR,2017,Sample Efficient Actor-Critic with  Experience Replay,Ziyu Wang;Victor Bapst;Nicolas Heess;Volodymyr Mnih;Remi Munos;Koray Kavukcuoglu;Nando de Freitas,ziyu@google.com;vbapst@google.com;heess@google.com;vmnih@google.com;Munos@google.com;korayk@google.com;nandodefreitas@google.com,7;6;6,3;3;4,Accept (Poster),3,8,1,no,11/4/16,Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,,11/3/16,309,129,129,4,0,39,4267;1590;11482;21077;9402;53136;19160,51;32;104;38;190;90;184,21;15;37;27;53;58;55,489;166;1630;3370;1323;6857;1851,-1;-1
155,ICLR,2017,Distributed Second-Order Optimization using Kronecker-Factored Approximations,Jimmy Ba;Roger Grosse;James Martens,jimmy@psi.toronto.edu;rgrosse@cs.toronto.edu;jmartens@cs.toronto.edu,7;6,4;3,Accept (Poster),6,3,0,no,11/5/16,"University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",15;15;15,22;22;22,,11/5/16,50,11,31,3,0,11,52425;5727;5662,52;48;39,22;28;18,8551;813;651,-1;-1
156,ICLR,2017,Learning to Navigate in Complex Environments,Piotr Mirowski;Razvan Pascanu;Fabio Viola;Hubert Soyer;Andy Ballard;Andrea Banino;Misha Denil;Ross Goroshin;Laurent Sifre;Koray Kavukcuoglu;Dharshan Kumaran;Raia Hadsell,piotrmirowski@google.com;razp@google.com;fviola@google.com;soyer@google.com;aybd@google.com;abanino@google.com;mdenil@google.com;goroshin@google.com;sifre@google.com;korayk@google.com;dkumaran@google.com;raia@google.com,7;5;7,5;4;3,Accept (Poster),6,9,0,no,11/4/16,Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,,11/4/16,414,242,90,7,0,26,1895;16936;1803;2101;501;674;3332;1649;11609;53136;17046;8230,49;101;49;22;10;9;38;14;14;90;65;63,18;46;16;11;4;4;20;9;12;58;35;26,119;1688;211;258;31;42;285;156;580;6857;2291;796,-1;-1
157,ICLR,2017,Learning to superoptimize programs,Rudy Bunel;Alban Desmaison;M. Pawan Kumar;Philip H.S. Torr;Pushmeet Kohli,rudy@robots.ox.ac.uk;alban@robots.ox.ac.uk;pawan@robots.ox.ac.uk;philip.torr@eng.ox.ac.uk;pkohli@microsoft.com,6;8;7,5;4;4,Accept (Poster),2,7,0,no,11/4/16,University of Oxford;University of Oxford;University of Oxford;University of Oxford;Microsoft,50;50;50;50;-1,1;1;1;1;-1,3,11/4/16,17,9,3,0,83,0,428;7028;2643;29494;22310,20;19;83;386;313,10;9;24;84;69,53;858;247;3908;2770,-1;-1
158,ICLR,2017,Trained Ternary Quantization,Chenzhuo Zhu;Song Han;Huizi Mao;William J. Dally,zhucz13@mails.tsinghua.edu.cn;songhan@stanford.edu;huizi@stanford.edu;dally@stanford.edu,3;7;7;8,3;5;3;5,Accept (Poster),0,10,0,no,11/4/16,Tsinghua University;Stanford University;Stanford University;Stanford University,11;3;3;3,35;3;3;3,,11/4/16,490,242,167,14,6,60,546;15090;5884;30398,4;374;22;268,3;36;14;67,63;1942;752;3737,-1;-1
159,ICLR,2017,Online Bayesian Transfer Learning for Sequential Data Modeling,Priyank Jaini;Zhitang Chen;Pablo Carbajal;Edith Law;Laura Middleton;Kayla Regan;Mike Schaekermann;George Trimponias;James Tung;Pascal Poupart,pjaini@uwaterloo.ca;chenzhitang2@huawei.com;pablo@veedata.io;edith.law@uwaterloo.ca;lmiddlet@uwaterloo.ca;kregan@uwaterloo.ca;mschaekermann@uwaterloo.ca;g.trimponias@huawei.com;james.tung@uwaterloo.ca;ppoupart@uwaterloo.ca,7;6;6,3;3;3,Accept (Poster),2,7,0,no,11/4/16,University of Waterloo;Huawei Technologies Ltd.;;University of Waterloo;University of Waterloo;University of Waterloo;University of Waterloo;Huawei Technologies Ltd.;University of Waterloo;University of Waterloo,25;-1;-1;25;25;25;25;-1;25;25,174;-1;-1;174;174;174;174;-1;174;174,11;6,11/4/16,8,1,4,0,0,0,150;211;5;1240;22;17;83;88;489;4243,20;39;2;71;6;5;21;20;45;170,7;8;1;17;2;3;6;5;7;35,11;16;0;75;0;1;3;6;26;458,-1;-1
160,ICLR,2017,Deep Variational Information Bottleneck,Alexander A. Alemi;Ian Fischer;Joshua V. Dillon;Kevin Murphy,alemi@google.com;iansf@google.com;jvdillon@google.com;kpmurphy@google.com,7;6;6,4;4;3,Accept (Poster),2,8,0,no,11/4/16,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,4;1;8,11/4/16,358,197,145,8,0,88,1290;2685;991;16161,53;16;28;83,14;12;13;41,186;358;166;2301,-1;-1
161,ICLR,2017,Exploring Sparsity in Recurrent Neural Networks,Sharan Narang;Greg Diamos;Shubho Sengupta;Erich Elsen,sharan@baidu.com;gdiamos@baidu.com;ssengupta@baidu.com;eriche@google.com,7;6,3;4,Accept (Poster),6,6,0,no,11/4/16,Baidu;Baidu;Baidu;Google,-1;-1;-1;-1,-1;-1;-1;-1,,11/4/16,121,63,57,3,81,13,2567;2589;2843;4626,15;16;13;53,10;8;7;21,275;249;281;488,-1;-1
162,ICLR,2017,Neural Photo Editing with Introspective Adversarial Networks,Andrew Brock;Theodore Lim;J.M. Ritchie;Nick Weston,ajb5@hw.ac.uk;t.lim@hw.ac.uk;j.m.ritchie@hw.ac.uk;Nick.Weston@renishaw.com,5;6;6,3;4;4,Accept (Poster),4,5,0,no,10/29/16,Heriot-Watt University;Heriot-Watt University;Heriot-Watt University;Renishaw,275;275;275;-1,429;429;429;-1,5;4;8,9/22/16,227,130,56,3,0,21,2102;2227;1208;824,88;133;47;17,15;20;16;6,244;190;124;96,-1;-1
163,ICLR,2017,Semi-Supervised Classification with Graph Convolutional Networks,Thomas N. Kipf;Max Welling,T.N.Kipf@uva.nl;M.Welling@uva.nl,7;7;7,3;4;4,Accept (Poster),1,1,1,no,11/3/16,University of Amsterdam;University of Amsterdam,161;161,63;63,10,9/9/16,3750,1927,1930,73,0,1134,5371;26852,26;269,12;59,1444;5126,-1;-1
164,ICLR,2017,Deep Learning with Dynamic Computation Graphs,Moshe Looks;Marcello Herreshoff;DeLesley Hutchins;Peter Norvig,madscience@google.com;marcelloh@google.com;delesley@google.com;pnorvig@google.com,8;7;8,3;5;3,Accept (Poster),1,5,1,no,11/4/16,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,3;10,11/4/16,86,37,30,0,340,21,420;111;187;21004,33;5;13;140,11;3;5;27,40;23;25;2062,-1;-1
165,ICLR,2017,A recurrent neural network without chaos,Thomas Laurent;James von Brecht,tlaurent@lmu.edu;james.vonbrecht@csulb.edu,8;7;7,3;4;4,Accept (Poster),6,3,0,no,11/4/16,"Loyola Marymount University;California State University, Long Beach",462;462,981;981,3;1,11/4/16,36,14,6,0,38,3,1462;108,56;4,21;4,131;12,-1;-1
166,ICLR,2017,Adversarially Learned Inference,Vincent Dumoulin;Ishmael Belghazi;Ben Poole;Alex Lamb;Martin Arjovsky;Olivier Mastropietro;Aaron Courville,vincent.dumoulin@umontreal.ca;ishmael.belghazi@gmail.com;poole@cs.stanford.edu;alex6200@gmail.com;martinarjovsky@gmail.com;oli.mastro@gmail.com;aaron.courville@gmail.com,7;7;8,4;3;4,Accept (Poster),4,6,0,no,11/4/16,University of Montreal;University of Montreal;Stanford University;University of Montreal;New York University;University of Montreal;University of Montreal,113;113;3;113;25;113;113,103;103;3;103;32;103;103,5;4,6/2/16,707,362,227,30,84,132,8210;873;4239;1168;7964;2268;61116,22;2;41;21;15;4;203,17;2;19;9;9;3;65,1225;166;695;181;1784;257;7900,-1;-1
167,ICLR,2017,Amortised MAP Inference for Image Super-resolution,Casper Kaae Sønderby;Jose Caballero;Lucas Theis;Wenzhe Shi;Ferenc Huszár,casperkaae@gmail.com;jcaballero@twitter.com;ltheis@twitter.com;wshi@twitter.com;fhuszar@twitter.com,8;9;7,5;3;2,Accept (Oral),2,5,0,no,11/1/16,University of Copenhagen;Twitter;Twitter;Twitter;Twitter,105;-1;-1;-1;-1,120;-1;-1;-1;-1,5;4,10/14/16,263,119,75,7,128,22,2106;361;5534;3503;6129,29;5;45;81;21,16;2;18;22;15,205;27;661;337;817,-1;-1
168,ICLR,2017,A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING,Zhouhan Lin;Minwei Feng;Cicero Nogueira dos Santos;Mo Yu;Bing Xiang;Bowen Zhou;Yoshua Bengio,lin.zhouhan@gmail.com;mfeng@us.ibm.com;cicerons@us.ibm.com;yum@us.ibm.com;bingxia@us.ibm.com;zhou@us.ibm.com;yoshua.bengio@umontreal.ca,6;5;8,5;4;4,Accept (Poster),4,5,0,no,11/4/16,University of Montreal;International Business Machines;International Business Machines;International Business Machines;International Business Machines;International Business Machines;University of Montreal,113;-1;-1;-1;-1;-1;113,103;-1;-1;-1;-1;-1;103,,11/4/16,833,386,351,9,0,95,4286;1196;4518;3544;5545;6452;205096,28;21;60;71;105;187;807,13;8;19;26;30;31;147,403;136;503;454;717;885;24136,-1;-1
169,ICLR,2017,Geometry of Polysemy,Jiaqi Mu;Suma Bhat;Pramod Viswanath,jiaqimu2@illinois.edu;spbhat2@illinois.edu;pramodv@illinois.edu,7;7;7,3;4;4,Accept (Poster),4,6,0,no,11/4/16,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",4;4;4,36;36;36,3,10/24/16,10,4,4,0,9,1,118;331;16622,15;64;179,4;9;39,14;29;2161,-1;-1
170,ICLR,2017,On Detecting Adversarial Perturbations,Jan Hendrik Metzen;Tim Genewein;Volker Fischer;Bastian Bischoff,JanHendrik.Metzen@de.bosch.com;Tim.Genewein@de.bosch.com;Volker.Fischer@de.bosch.com;Bastian.Bischoff@de.bosch.com,7;7;5,4;4;3,Accept (Poster),3,3,0,no,11/4/16,Bosch;Bosch;Bosch;Bosch,-1;-1;-1;-1,433;433;433;433,4,11/4/16,406,243,113,3,25,37,1826;843;671;567,64;27;17;18,15;10;6;9,175;65;51;50,-1;-1
171,ICLR,2017,Frustratingly Short Attention Spans in Neural Language Modeling,Michał Daniluk;Tim Rocktäschel;Johannes Welbl;Sebastian Riedel,michal.daniluk.15@ucl.ac.uk;t.rocktaschel@cs.ucl.ac.uk;j.welbl@cs.ucl.ac.uk;s.riedel@cs.ucl.ac.uk,7;7;7,4;4;4,Accept (Poster),3,3,1,no,11/4/16,University College London;University College London;University College London;University College London,45;45;45;45,15;15;15;15,3,11/4/16,58,35,18,5,0,14,108;2272;1010;5852,11;48;19;230,3;22;9;35,24;282;270;891,-1;-1
172,ICLR,2017,Learning Invariant Representations Of Planar Curves ,Gautam Pai;Aaron Wetzler;Ron Kimmel,paigautam@cs.technion.ac.il;twerd@cs.technion.ac.il;ron@cs.technion.ac.il,5;6;8,2;5;3,Accept (Poster),3,4,0,no,11/4/16,Technion;Technion;Technion,24;24;24,301;301;301,,11/4/16,3,1,0,0,4,0,23;290;16209,13;66;259,3;8;56,1;23;1389,-1;-1
173,ICLR,2017,Deep Multi-task Representation Learning: A Tensor Factorisation Approach,Yongxin Yang;Timothy M. Hospedales,yongxin.yang@qmul.ac.uk;t.hospedales@qmul.ac.uk,5;7;8,3;4;4,Accept (Poster),1,5,0,no,11/4/16,Queen Mary University London;Queen Mary University London,275;275,113;113,,5/20/16,122,57,45,3,5,21,2079;5116,64;157,21;36,330;663,-1;-1
174,ICLR,2017,Unrolled Generative Adversarial Networks,Luke Metz;Ben Poole;David Pfau;Jascha Sohl-Dickstein,lmetz@google.com;poole@cs.stanford.edu;pfau@google.com;jaschasd@google.com,7;7;9,5;5;5,Accept (Poster),3,8,0,no,11/4/16,Google;Stanford University;Google;Google,-1;3;-1;-1,-1;3;-1;-1,5;4,11/4/16,443,221,138,11,127,65,7313;4239;2026;5020,26;41;29;101,10;19;12;33,1213;695;212;703,-1;-1
175,ICLR,2017,Structured Attention Networks,Yoon Kim;Carl Denton;Luong Hoang;Alexander M. Rush,yoonkim@seas.harvard.edu;carldenton@college.harvard.edu;lhoang@g.harvard.edu;srush@seas.harvard.edu,8;8;8,4;5;3,Accept (Poster),3,2,0,no,11/4/16,Harvard University;Harvard University;Harvard University;Harvard University,36;36;36;36,6;6;6;6,3;2;10,11/4/16,195,99,63,6,0,21,8145;190;334;7010,18;1;13;87,15;1;5;32,1320;21;31;940,-1;-1
176,ICLR,2017,HyperNetworks,David Ha;Andrew M. Dai;Quoc V. Le,hadavid@google.com;adai@google.com;qvl@google.com,8;7;6,4;4;5,Accept (Poster),2,5,0,no,10/27/16,Google;Google;Google,-1;-1;-1,-1;-1;-1,3,9/27/16,229,85,73,2,0,31,348;3846;49563,28;51;193,4;19;82,54;478;6128,-1;-1
177,ICLR,2017,Dropout with Expectation-linear Regularization,Xuezhe Ma;Yingkai Gao;Zhiting Hu;Yaoliang Yu;Yuntian Deng;Eduard Hovy,xuezhem@cs.cmu.edu;yingkaig@cs.cmu.edu;zhitinghu@cs.cmu.edu;yaoliang@cs.cmu.edu;dengyuntian@gmail.com;hovy@cmu.edu,8;7;8,3;4;3,Accept (Poster),1,4,0,no,10/28/16,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;;Carnegie Mellon University,1;1;1;1;-1;1,23;23;23;23;-1;23,1,9/26/16,24,12,8,0,18,2,1941;69;3229;1632;1467;23709,37;3;64;93;29;582,14;2;29;20;13;76,245;8;366;133;232;2484,-1;-1
178,ICLR,2017,Dynamic Coattention Networks For Question Answering,Caiming Xiong;Victor Zhong;Richard Socher,cxiong@salesforce.com;vzhong@salesforce.com;rsocher@salesforce.com,8;8;8,3;4;4,Accept (Poster),12,9,2,no,11/4/16,SalesForce.com;SalesForce.com;SalesForce.com,-1;-1;-1,-1;-1;-1,,11/4/16,435,249,167,7,0,78,6299;1871;52991,156;18;180,31;11;49,1059;346;8889,-1;-1
179,ICLR,2017,Visualizing Deep Neural Network Decisions: Prediction Difference Analysis,Luisa M Zintgraf;Taco S Cohen;Tameem Adel;Max Welling,lmzintgraf@gmail.com;t.s.cohen@uva.nl;tameem.hesham@gmail.com;m.welling@uva.nl,6;9;6,4;4;5,Accept (Poster),1,6,0,no,11/4/16,Vrije Universiteit Brussel;University of Amsterdam;University of Cambridge;University of Amsterdam,-1;161;67;161,-1;63;4;63,,11/4/16,267,127,93,9,21,33,471;1688;436;26852,16;32;25;269,8;17;9;59,51;251;50;5126,-1;-1
180,ICLR,2017,Density estimation using Real NVP,Laurent Dinh;Jascha Sohl-Dickstein;Samy Bengio,dinh.laurent@gmail.com;jaschasd@google.com;bengio@google.com,8;8;7,4;4;4,Accept (Poster),4,2,0,no,11/4/16,Google;Google;Google,-1;-1;-1,-1;-1;-1,,5/27/16,801,419,374,9,0,203,4689;5020;26525,24;101;332,12;33;67,622;703;3482,-1;-1
181,ICLR,2017,Dialogue Learning With Human-in-the-Loop,Jiwei Li;Alexander H. Miller;Sumit Chopra;Marc'Aurelio Ranzato;Jason Weston,jiwel@fb.com;ahm@fb.com;spchopra@fb.com;ranzato@fb.com;jase@fb.com,5;6;7,4;3;4,Accept (Poster),4,5,0,no,11/4/16,Facebook;Facebook;Facebook;Facebook;Facebook,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,,11/4/16,46,26,11,2,24,4,5794;975;11361;20519;45098,100;16;56;75;243,30;10;22;43;78,836;116;1149;2079;5838,-1;-1
182,ICLR,2017,Learning to Act by Predicting the Future,Alexey Dosovitskiy;Vladlen Koltun,adosovitskiy@gmail.com;vkoltun@gmail.com,7;8;8,4;4;4,Accept (Oral),4,1,0,no,11/4/16,Universität Freiburg;Intel,113;-1,95;-1,,11/4/16,179,103,49,6,0,15,9585;17726,57;191,31;63,1266;2515,-1;-1
183,ICLR,2017,"Offline bilingual word vectors, orthogonal transformations and the inverted softmax",Samuel L. Smith;David H. P. Turban;Steven Hamblin;Nils Y. Hammerla,samuel.smith@babylonhealth.com;dt382@cam.ac.uk;steven.hamblin@babylonhealth.com;nils.hammerla@babylonhealth.com,7;8;6,5;5;3,Accept (Poster),2,6,1,no,11/4/16,babylon health;University of Cambridge;babylon health;babylon health,-1;67;-1;-1,-1;4;-1;-1,1,11/4/16,283,140,142,16,12,60,1214;361;713;2320,26;12;31;43,11;5;9;19,110;60;72;216,-1;-1
184,ICLR,2017,Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer,Sergey Zagoruyko;Nikos Komodakis,sergey.zagoruyko@enpc.fr;nikos.komodakis@enpc.fr,6;6;6,4;4;4,Accept (Poster),4,7,0,no,11/5/16,ENPC;ENPC,462;462,373;373,3;2,11/5/16,375,205,145,7,0,72,3512;9295,20;121,10;39,678;1313,-1;-1
185,ICLR,2017,Recurrent Mixture Density Network for Spatiotemporal Visual Attention,Loris Bazzani;Hugo Larochelle;Lorenzo Torresani,loris.bazzani@gmail.com;hugo.larochelle@usherbrooke.ca;lt@dartmouth.edu,7;6;6,4;4;4,Accept (Poster),4,7,0,no,11/3/16,Amazon;Université de Sherbrooke;Dartmouth College,-1;351;140,-1;567;82,2,3/27/16,66,37,17,2,7,4,3408;24882;8298,40;123;103,20;44;36,514;2864;1370,-1;-1
186,ICLR,2017,Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations,David Krueger;Tegan Maharaj;Janos Kramar;Mohammad Pezeshki;Nicolas Ballas;Nan Rosemary Ke;Anirudh  Goyal;Yoshua Bengio;Aaron Courville;Christopher Pal,davidscottkrueger@gmail.com;tegan.jrm@gmail.com;ballas.n@gmail.com,7;8;8,4;4;5,Accept (Poster),3,3,0,no,11/4/16,University of Montreal;Polytechnique Montreal;University of Montreal,113;351;113,103;981;103,3;8,6/3/16,187,56,62,4,117,22,1506;719;405;1884;5005;760;1125;205096;61116;777,48;18;14;11;54;32;46;807;203;58,12;7;7;6;20;13;12;147;65;10,221;77;36;148;589;76;127;24136;7900;71,-1;-1
187,ICLR,2017,Adversarial Machine Learning at Scale,Alexey Kurakin;Ian J. Goodfellow;Samy Bengio,kurakin@google.com;ian@openai.com;bengio@google.com,7;6;6,3;4;4,Accept (Poster),3,3,0,no,11/3/16,Google;OpenAI;Google,-1;-1;-1,-1;-1;-1,4,11/3/16,928,522,372,23,0,231,4552;56065;26525,14;90;332,9;56;67,830;9399;3482,-1;-1
188,ICLR,2017,A Learned Representation For Artistic Style,Vincent Dumoulin;Jonathon Shlens;Manjunath Kudlur,vi.dumoulin@gmail.com;shlens@google.com;keveman@google.com,7;8;8,3;5;5,Accept (Poster),6,10,0,no,10/26/16,University of Montreal;Google;Google,113;-1;-1,103;-1;-1,,10/24/16,442,188,190,8,0,56,8210;23691;15934,22;80;33,17;37;20,1225;4021;1917,-1;-1
189,ICLR,2017,Batch Policy Gradient  Methods for  Improving Neural Conversation Models,Kirthevasan Kandasamy;Yoram Bachrach;Ryota Tomioka;Daniel Tarlow;David Carter,kandasamy@cmu.edu;yorambac@gmail.com;ryoto@microsoft.com;dtarlow@microsoft.com;dacart@microsoft.com,6;7;8,3;3;3,Accept (Poster),2,5,0,no,11/4/16,Carnegie Mellon University;Microsoft;Microsoft;Microsoft;Microsoft,1;-1;-1;-1;-1,23;-1;-1;-1;-1,3,11/4/16,20,9,2,0,9,1,874;2739;4837;2537;968,43;136;86;68;278,15;29;29;23;17,99;196;658;306;80,-1;-1
190,ICLR,2017,Incorporating long-range consistency in CNN-based texture generation,Guillaume Berger;Roland Memisevic,guillaume.berger@umontreal.ca;memisevr@iro.umontreal.ca,5;7;7,5;5;5,Accept (Poster),2,1,0,no,11/4/16,University of Montreal;University of Montreal,113;113,103;103,,6/3/16,27,14,14,2,0,7,58;5437,10;105,3;31,20;497,-1;-1
191,ICLR,2017,Paleo: A Performance Model for Deep Neural Networks,Hang Qi;Evan R. Sparks;Ameet Talwalkar,hangqi@cs.ucla.edu;sparks@cs.berkeley.edu;ameet@cs.ucla.edu,6;7;6,4;4;4,Accept (Poster),2,3,0,no,11/4/16,"University of California, Los Angeles;University of California Berkeley;University of California, Los Angeles",20;5;20,14;10;14,,11/4/16,61,28,23,0,0,9,239;1716;6399,12;21;78,6;12;34,35;185;766,-1;-1
192,ICLR,2017,On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima,Nitish Shirish Keskar;Dheevatsa Mudigere;Jorge Nocedal;Mikhail Smelyanskiy;Ping Tak Peter Tang,keskar.nitish@u.northwestern.edu;dheevatsa.mudigere@intel.com;j-nocedal@northwestern.edu;mikhail.smelyanskiy@intel.com;peter.tang@intel.com,8;10;6,3;3;4,Accept (Oral),2,5,2,no,11/3/16,Northwestern University;Intel;Northwestern University;Intel;Intel,45;-1;45;-1;-1,20;-1;20;-1;-1,8,9/15/16,880,543,106,53,0,95,2177;1417;26906;3325;2234,28;40;174;86;72,13;15;46;24;20,333;144;3016;279;209,-1;-1
193,ICLR,2017,Predicting Medications from Diagnostic Codes with Recurrent Neural Networks,Jacek M. Bajor;Thomas A. Lasko,jacek.m.bajor@vanderbilt.edu;tom.lasko@vanderbilt.edu,6;8;7,3;4;5,Accept (Poster),3,3,0,no,11/3/16,Vanderbilt University;Vanderbilt University,230;230,108;108,,11/3/16,20,7,6,1,0,1,25;1251,2;57,2;13,2;56,-1;-1
194,ICLR,2017,An Information-Theoretic Framework for Fast and Robust Unsupervised Learning via Neural Population Infomax,Wentao Huang;Kechen Zhang,whuang21@jhmi.edu;kzhang4@jhmi.edu,8;5;7,2;2;3,Accept (Poster),1,6,0,no,11/5/16,;Johns Hopkins University,-1;67,-1;17,,11/5/16,5,0,1,0,22,0,1119;624,158;37,16;9,40;60,-1;-1
195,ICLR,2017,Regularizing CNNs with Locally Constrained Decorrelations,Pau Rodríguez;Jordi Gonzàlez;Guillem Cucurull;Josep M. Gonfaus;Xavier Roca,pau.rodriguez@cvc.uab.es;poal@cvc.uab.es;pep.gonfaus@visual-tagging.com;xavier.roca@visual-tagging.com,7;7;7,3;4;4,Accept (Poster),4,4,0,no,11/4/16,"Computer Vision Center, Universitat Autònoma de Barcelona;Computer Vision Center, Universitat Autònoma de Barcelona;Visual-tagging;Visual-tagging",462;462;-1;-1,164;164;-1;-1,,11/4/16,62,32,9,3,5,5,264;10791;1526;429;703,40;1458;12;16;90,6;47;8;6;14,24;807;385;46;41,-1;-1
196,ICLR,2017,Generating Long and Diverse Responses with Neural Conversation Models,Louis Shao;Stephan Gouws;Denny Britz;Anna Goldie;Brian Strope;Ray Kurzweil,overmind@google.com;sgouws@google.com;dennybritz@google.com;agoldie@google.com;bps@google.com;raykurzweil@google.com,5;7;7,3;4;3,Reject,4,5,0,no,11/5/16,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,3,11/5/16,38,14,8,1,16,4,47;3641;558;498;1534;981,2;23;11;18;52;52,2;12;9;6;17;13,5;335;59;44;188;138,-1;-1
197,ICLR,2017,DeepDSL: A Compilation-based Domain-Specific Language for Deep Learning,Tian Zhao;Xiao Bing Huang;Yu Cao,tzhao@uwm.edu;xiaobing@uwm.edu;ycao@cs.uml.edu,6;7;8,4;4;3,Accept (Poster),3,2,0,no,11/4/16,"College of William and Mary;College of William and Mary;University of Massachusetts, Lowell",161;161;-1,280;280;-1,,11/4/16,3,1,1,0,0,0,901;1659;17615,116;160;1215,18;21;59,63;23;1243,-1;-1
198,ICLR,2017,Delving into Transferable Adversarial Examples and Black-box Attacks,Yanpei Liu;Xinyun Chen;Chang Liu;Dawn Song,resodo.liu@gmail.com;jungyhuk@gmail.com;liuchang@eecs.berkeley.edu;dawnsong@cs.berkeley.edu,6;7;5,3;3;3,Accept (Poster),7,5,0,no,11/4/16,Shanghai Jiao Tong University;Shanghai Jiao Tong University;University of California Berkeley;University of California Berkeley,61;61;5;5,214;214;10;10,4,11/4/16,615,378,129,25,11,57,1646;1670;6509;43319,196;51;697;396,17;14;38;100,129;143;302;4323,-1;-1
199,ICLR,2017,Central Moment Discrepancy (CMD) for Domain-Invariant Representation Learning,Werner Zellinger;Thomas Grubinger;Edwin Lughofer;Thomas Natschläger;Susanne Saminger-Platz,werner.zellinger@jku.at;thomas.grubinger@scch.at;edwin.lughofer@jku.at;thomas.natschlaeger@scch.at;susanne.saminger-platz@jku.at,6;7;9,4;4;5,Accept (Poster),3,3,0,no,11/4/16,Johannes Kepler University Linz;Software Competence Center Hagenberg GmbH;Johannes Kepler University Linz;Software Competence Center Hagenberg GmbH;Johannes Kepler University Linz,462;-1;462;-1;462,498;-1;498;-1;498,4;1,11/4/16,123,61,57,0,6,25,185;394;3936;3922;233,17;22;235;60;25,6;9;35;17;6,29;43;141;374;38,-1;-1
200,ICLR,2017,Towards Deep Interpretability (MUS-ROVER II): Learning Hierarchical Representations of Tonal Music,Haizi Yu;Lav R. Varshney,haiziyu7@illinois.edu;varshney@illinois.edu,6;8;6,3;4;3,Accept (Poster),5,1,0,no,11/4/16,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",4;4,36;36,3,11/4/16,5,3,4,0,0,0,19;3432,15;226,3;22,0;219,-1;-1
201,ICLR,2017,PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications,Tim Salimans;Andrej Karpathy;Xi Chen;Diederik P. Kingma,tim@openai.com;karpathy@openai.com;peter@openai.com;dpkingma@openai.com,6;7;7,3;4;5,Accept (Poster),5,5,0,no,11/5/16,OpenAI;OpenAI;OpenAI;OpenAI,-1;-1;-1;-1,-1;-1;-1;-1,5,11/5/16,307,111,155,9,32,57,6682;24626;13677;56187,35;23;444;26,13;15;42;19,1037;3353;1552;10320,-1;-1
202,ICLR,2017,Latent Sequence Decompositions,William Chan;Yu Zhang;Quoc Le;Navdeep Jaitly,williamchan@cmu.edu;yzhang87@mit.edu;qvl@google.com;ndjaitly@google.com,7;8;7,4;4;5,Accept (Poster),2,3,0,no,11/4/16,Carnegie Mellon University;Massachusetts Institute of Technology;Google;Google,1;2;-1;-1,23;5;-1;-1,,10/10/16,46,24,14,1,22,4,2112;18019;48310;17843,21;1400;193;94,15;55;81;39,229;1177;6042;1601,-1;-1
203,ICLR,2017,HolStep: A Machine Learning Dataset for Higher-order Logic Theorem Proving,Cezary Kaliszyk;François Chollet;Christian Szegedy,cezary.kaliszyk@uibk.ac.at;fchollet@google.com;szegedy@google.com,6;8;7,3;3;3,Accept (Poster),2,2,0,no,11/2/16,University of Innsbruck;Google;Google,462;-1;-1,307;-1;-1,1,11/2/16,44,9,19,1,53,3,1711;8528;66351,123;98;37,23;37;17,95;856;9204,-1;-1
204,ICLR,2017,Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks,Yossi Adi;Einat Kermany;Yonatan Belinkov;Ofer Lavi;Yoav Goldberg,yossiadidrum@gmail.com;einatke@il.ibm.com;belinkov@mit.edu;oferl@il.ibm.com;yoav.goldberg@gmail.com,8;8;8,5;4;4,Accept (Poster),5,3,0,no,11/3/16,Bar Ilan University;International Business Machines;Massachusetts Institute of Technology;International Business Machines;Bar-Ilan University,87;-1;2;-1;87,489;-1;5;-1;489,,8/15/16,211,122,72,10,0,28,602;359;1568;405;9577,27;11;63;16;116,9;6;21;7;40,74;37;164;39;1021,-1;-1
205,ICLR,2017,Learning similarity preserving representations with neural similarity and context encoders,Franziska Horn;Klaus-Robert Müller,franziska.horn@campus.tu-berlin.de;klaus-robert.mueller@tu-berlin.de,3;2;3,4;5;4,Reject,3,1,0,no,11/3/16,TU Berlin;TU Berlin,105;105,82;82,3,11/3/16,1,0,1,0,0,0,418;38520,21;522,6;83,41;3874,-1;-1
206,ICLR,2017,Memory-augmented Attention Modelling for Videos,Rasool Fakoor;Abdel-rahman Mohamed;Margaret Mitchell;Sing Bing Kang;Pushmeet Kohli,rasool.fakoor@mavs.uta.edu;asamir@microsoft.com;margarmitchell@gmail.com;SingBing.Kang@microsoft.com;pkohli@microsoft.com,4;4;4,4;5,Reject,9,3,0,no,11/4/16,"University of Texas, Arlington;Microsoft;;Microsoft;Microsoft",113;-1;-1;-1;-1,570;-1;-1;-1;-1,,11/4/16,13,2,3,0,4,0,149;19117;4312;12042;22310,16;56;118;244;313,4;27;28;60;69,8;1201;408;913;2770,-1;-1
207,ICLR,2017,Neural Causal Regularization under the Independence of Mechanisms Assumption,Mohammad Taha Bahadori;Krzysztof Chalupka;Edward Choi;Robert Chen;Walter F. Stewart;Jimeng Sun,bahadori@gatech.edu;kjchalup@caltech.edu;mp2893@gatech.edu;rchen87@gatech.edu;StewarWF@sutterhealth.org;jsun@cc.gatech.edu,5;4;6,4;4;5,Reject,7,5,0,no,11/4/16,Georgia Institute of Technology;California Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;;Georgia Institute of Technology,12;129;12;12;-1;12,33;2;33;33;-1;33,,11/4/16,1,0,0,0,0,0,1673;195;1003;10224;24040;10478,34;17;36;377;390;240,16;6;11;56;77;55,191;19;82;897;1200;787,-1;-1
208,ICLR,2017,New Learning Approach By Genetic Algorithm In A Convolutional Neural Network For Pattern Recognition,Mohammad Ali Mehrolhassani;Majid Mohammadi,Alimehrolhassani@yahoo.com;Mohammadi@uk.ac.ir,3;3;2,5;5;5,Reject,6,7,0,no,11/1/16,;University of Tehran,-1;462,-1;747,,11/1/16,1,0,0,0,0,0,1;41,1;29,1;4,0;1,-1;-1
209,ICLR,2017,Bit-Pragmatic Deep Neural Network Computing,Jorge Albericio;Patrick Judd;Alberto Delmas;Sayeh Sharify;Andreas Moshovos,jorge.albericio@gmail.com;judd@ece.utoronto.ca;delmas1@ece.utoronto.ca;sayeh@ece.utoronto.ca;moshovos@ece.utoronto.ca,6;7;5,3;2;2,Invite to Workshop Track,1,6,0,no,11/4/16,University of Toronto;Toronto University;Toronto University;Toronto University;Toronto University,15;15;15;15;15,22;22;22;22;22,10,10/20/16,90,55,23,2,13,10,970;887;168;226;792,30;29;16;21;17,13;12;7;9;8,116;104;13;20;96,-1;-1
210,ICLR,2017,Learning Identity Mappings with Residual Gates,Pedro H. P. Savarese;Leonardo O. Mazza;Daniel R. Figueiredo,savarese@land.ufrj.br;leonardomazza@poli.ufrj.br;daniel@land.ufrj.br,5;5;6,5;5;4,Reject,3,9,0,no,11/4/16,Federal University of Rio de Janeiro - UFRJ;Federal University of Rio de Janeiro - UFRJ;Federal University of Rio de Janeiro - UFRJ,461;461;461,693;693;693,,11/4/16,2,0,0,0,13,0,43;-1;1509,4;-1;74,2;-1;16,6;0;116,-1;-1
211,ICLR,2017,Investigating Different Context Types and Representations for Learning Word Embeddings,Bofang Li;Tao Liu;Zhe Zhao;Buzhou Tang;Xiaoyong Du,libofang@ruc.edu.cn;tliu@ruc.edu.cn;helloworld@ruc.edu.cn;tangbuzhou@gmail.com;duyong@ruc.edu.cn,4;6;4,5;4;3,Reject,4,5,0,no,11/4/16,"University of Illinois, Urbana-Champaign;University of Illinois, Urbana-Champaign;University of Illinois, Urbana-Champaign;;University of Illinois, Urbana-Champaign",4;4;4;-1;4,36;36;36;-1;36,3,11/4/16,0,0,0,0,0,0,148;893;911;1464;1989,22;158;134;104;322,7;13;14;21;21,11;26;83;139;126,-1;-1
212,ICLR,2017,Neural Combinatorial Optimization with Reinforcement Learning,Irwan Bello*;Hieu Pham*;Quoc V. Le;Mohammad Norouzi;Samy Bengio,ibello@google.com;hyhieu@google.com;qvl@google.com;mnorouzi@google.com;bengio@google.com,6;6;6,4;4;4,Reject,16,8,0,no,11/4/16,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,10,11/4/16,279,150,132,13,72,62,532;5071;48310;7957;26525,9;18;193;125;332,5;11;81;31;67,99;773;6042;1010;3482,-1;-1
213,ICLR,2017,A Differentiable Physics Engine for Deep Learning in Robotics,Jonas Degrave;Michiel Hermans;Joni Dambre;Francis wyffels,Jonas.Degrave@UGent.be;x@UGent.be;Joni.Dambre@UGent.be;Francis.wyffels@UGent.be,5;5;6,2;4;4,Invite to Workshop Track,3,4,0,no,11/3/16,;;;,-1;-1;-1;-1,-1;-1;-1;-1,,11/3/16,43,24,9,0,29,3,694;724;2991;473,32;30;206;71,12;12;22;12,47;38;156;24,-1;-1
214,ICLR,2017,Two Methods for Wild Variational Inference,Qiang Liu;Yihao Feng,qiang.liu@dartmouth.edu;yihao.feng.gr@dartmouth.edu,3;3;3,4;4;4,Reject,1,1,0,no,11/4/16,Dartmouth College;Dartmouth College,140;140,82;82,,11/4/16,19,5,5,0,0,1,4626;143,553;12,32;6,345;14,-1;-1
215,ICLR,2017,Sample Importance in Training Deep Neural Networks,Tianxiang Gao;Vladimir Jojic,tgao@cs.unc.edu;vjojic@cs.unc.edu,2;7;3,4;4;4,Reject,6,4,0,no,11/4/16,"University of North Carolina, Chapel Hill;University of North Carolina, Chapel Hill",73;73,56;56,,11/4/16,3,2,0,0,0,0,116;2301,27;59,7;20,5;132,-1;-1
216,ICLR,2017,Neural Code Completion,Chang Liu;Xin Wang;Richard Shin;Joseph E. Gonzalez;Dawn Song,xinw@eecs.berkeley.edu;liuchang@eecs.berkeley.edu;ricshin@berkeley.edu;jegonzal@berkeley.edu;dawnsong@cs.berkeley.edu,5;5;4,4;4;4,Reject,3,7,0,no,11/4/16,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5;5,10;10;10;10;10,,11/4/16,12,7,6,1,0,5,6509;289;288;1700;43319,697;85;29;57;396,38;10;9;8;100,302;25;28;259;4323,-1;-1
217,ICLR,2017,Demystifying ResNet,Sihan Li;Jiantao Jiao;Yanjun Han;Tsachy Weissman,lisihan13@mails.tsinghua.edu.cn;jiantao@stanford.edu;yjhan@stanford.edu;tsachy@stanford.edu,4;5;4,4;3;5,Reject,7,3,0,no,11/3/16,Tsinghua University;Stanford University;Stanford University;Stanford University,11;3;3;3,35;3;3;3,9,11/3/16,15,9,3,0,11,1,508;1112;936;3776,25;60;76;288,10;17;17;29,39;151;79;362,-1;-1
218,ICLR,2017,Adjusting for Dropout Variance in Batch Normalization and Weight Initialization,Dan Hendrycks;Kevin Gimpel,dan@ttic.edu;kgimpel@ttic.edu,5;7;6,4;4;4,Reject,2,5,0,no,11/4/16,Toyota Technological Institute at Chicago;Toyota Technological Institute at Chicago,-1;-1,-1;-1,,11/4/16,8,3,3,0,0,0,1504;5717,27;99,14;31,286;825,-1;-1
219,ICLR,2017,Tartan: Accelerating Fully-Connected and Convolutional Layers in Deep Learning Networks by Exploiting Numerical Precision Variability,Alberto Delmás Lascorz;Sayeh Sharify;Patrick Judd;Andreas Moshovos,delmasl1@ece.utoronto.ca;sayeh@ece.utoronto.ca;moshovos@ece.utoronto.ca,5;5;6;4;4,5;5;2;1;3,Reject,2,8,0,no,11/4/16,Toronto University;Toronto University;Toronto University,15;15;15,22;22;22,,11/4/16,12,3,2,0,4,0,168;226;887;3476,16;21;29;141,7;9;12;29,13;20;104;349,-1;-1
220,ICLR,2017,Cooperative Training of Descriptor and Generator Networks,Jianwen Xie;Yang Lu;Ruiqi Gao;Song-Chun Zhu;Ying Nian Wu,jianwen@ucla.edu;yanglv@ucla.edu;ruiqigao@ucla.edu;sczhu@stat.ucla.edu;ywu@stat.ucla.edu,4;3;6,3;4;4,Reject,2,2,0,no,11/4/16,"University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles",20;20;20;20;20,14;14;14;14;14,,9/29/16,46,15,17,1,26,4,678;15334;227;13977;5786,58;1766;30;449;276,15;52;9;62;38,49;723;17;986;594,-1;-1
221,ICLR,2017,Regularizing Neural Networks by Penalizing Confident Output Distributions,Gabriel Pereyra;George Tucker;Jan Chorowski;Lukasz Kaiser;Geoffrey Hinton,pereyra@google.com;gjt@google.com;chorowski@google.com;lukaszkaiser@google.com;geoffhinton@google.com,6;5;5,4;4;4,Reject,4,3,0,no,11/4/16,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,3,11/4/16,342,169,124,9,18,54,578;2652;5692;22646;214640,12;74;42;75;415,3;21;18;24;127,76;298;463;3882;21599,-1;-1
222,ICLR,2017,Song From PI: A Musically Plausible Network for Pop Music Generation,Hang Chu;Raquel Urtasun;Sanja Fidler,chuhang1122@cs.toronto.edu;urtasun@cs.toronto.edu;fidler@cs.toronto.edu,4;6;7,3;4;3,Invite to Workshop Track,3,1,0,no,11/4/16,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",15;15;15,22;22;22,,11/4/16,60,37,14,3,0,6,376;24603;10638,25;245;160,11;73;48,27;3465;1369,-1;-1
223,ICLR,2017,Dataset Augmentation in Feature Space,Terrance DeVries;Graham W. Taylor,terrance@uoguelph.ca;gwtaylor@uoguelph.ca,4;7;6,5;4;5,Invite to Workshop Track,2,4,0,no,11/4/16,University of Guelph;University of Guelph,275;275,398;398,,11/4/16,108,56,41,2,26,7,837;5883,10;143,7;31,141;498,-1;-1
224,ICLR,2017,Recurrent Neural Networks for Multivariate Time Series with Missing Values,Zhengping Che;Sanjay Purushotham;Kyunghyun Cho;David Sontag;Yan Liu,zche@usc.edu;spurusho@usc.edu;kyunghyun.cho@nyu.edu;dsontag@cs.nyu.edu;yanliu.cs@usc.edu,6;5;6,4;3;3,Reject,4,4,0,no,11/4/16,University of Southern California;University of Southern California;New York University;New York University;University of Southern California,31;31;25;25;31,60;60;32;32;60,,6/6/16,458,187,144,10,82,55,1084;1094;45940;5666;6246,22;39;274;100;584,11;13;52;32;36,110;116;6588;783;504,-1;-1
225,ICLR,2017,Counterpoint by Convolution,Cheng-Zhi Anna Huang;Tim Cooijmans;Adam Roberts;Aaron Courville;Douglas Eck,chengzhiannahuang@gmail.com;tim.cooijmans@umontreal.ca;adarob@google.com;aaron.courville@umontreal.ca;deck@google.com,6;5;6,5;4;3,Reject,2,4,0,no,11/4/16,Harvard University;University of Montreal;Google;University of Montreal;Google,36;113;-1;113;-1,6;103;-1;103;-1,5,11/4/16,54,27,21,3,5,6,258;1962;10511;61116;2559,16;8;36;203;84,7;5;19;65;27,32;170;1456;7900;274,-1;-1
226,ICLR,2017,Fast Chirplet Transform to Enhance CNN Machine Listening - Validation on Animal calls and Speech,Herve Glotin;Julien Ricard;Randall Balestriero,glotin@univ-tln.fr;julien.ricard@gmail.com;randallbalestriero@gmail.com,6;6;4,3;5;4,Invite to Workshop Track,1,8,0,no,11/4/16,CNRS university Toulon;;Rice University,462;-1;80,981;-1;87,6,11/4/16,1,1,0,0,2,0,1971;242;114,280;36;38,25;8;5,109;8;2,-1;-1
227,ICLR,2017,Finding a Jack-of-All-Trades: An Examination of Semi-supervised Learning in Reading Comprehension,Rudolf Kadlec;Ondřej Bajgar;Peter Hrincar;Jan Kleindienst,rudolf_kadlec@cz.ibm.com;obajgar@cz.ibm.com;phrincar@cz.ibm.com;jankle@cz.ibm.com,6;4;3,4;4;4,Reject,5,5,0,no,11/4/16,International Business Machines;International Business Machines;International Business Machines;International Business Machines,-1;-1;-1;-1,-1;-1;-1;-1,3;6,11/4/16,1,0,1,0,0,0,686;375;1;658,40;10;2;76,11;5;1;9,94;63;0;89,-1;-1
228,ICLR,2017,Efficient Summarization with Read-Again and Copy Mechanism,Wenyuan Zeng;Wenjie Luo;Sanja Fidler;Raquel Urtasun,cengwy13@mails.tsinghua.edu.cn;wenjie@cs.toronto.edu;fidler@cs.toronto.edu;urtasun@cs.toronto.edu,5;6;5,4;4;5,Reject,4,4,0,no,11/4/16,"Tsinghua University;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",11;15;15;15,35;22;22;22,,11/4/16,55,33,15,2,4,3,412;1775;10638;24603,12;31;160;245,7;15;48;73,54;190;1369;3465,-1;-1
229,ICLR,2017,The Predictron: End-To-End Learning and Planning,David Silver;Hado van Hasselt;Matteo Hessel;Tom Schaul;Arthur Guez;Tim Harley;Gabriel Dulac-Arnold;David Reichert;Neil Rabinowitz;Andre Barreto;Thomas Degris,davidsilver@google.com;hado@google.com;mtthss@google.com;schaul@google.com;aguez@google.com;tharley@google.com;dulacarnold@google.com;reichert@google.com;ncr@google.com;andrebarreto@google.com;degris@google.com,4;6;9,4;5;2,Reject,2,10,0,no,11/4/16,Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,,11/4/16,134,69,42,2,159,8,42730;5578;2292;8477;13230;4173;881;955;3015;402;2337,159;51;27;83;32;14;22;18;37;12;24,56;21;13;30;17;7;12;11;17;6;13,5910;921;368;1128;938;816;112;86;395;37;331,-1;-1
230,ICLR,2017,Training Group Orthogonal Neural Networks with Privileged Information,Yunpeng Chen;Xiaojie Jin;Jiashi Feng;Shuicheng Yan,chenyunpeng@u.nus.edu;xiaojie.jin@u.nus.edu;elefjia@nus.edu.sg;yanshuicheng@360.cn,6;5;6,4;4;4,Reject,2,4,0,no,11/3/16,National University of Singapore;National University of Singapore;National University of Singapore;Qihoo 360 Technology Co. Ltd,15;15;15;-1,24;24;24;-1,2;8,11/3/16,21,11,8,0,10,0,1314;1041;9415;42352,45;52;328;782,14;15;51;96,177;122;1222;5192,-1;-1
231,ICLR,2017,Neural Data Filter for Bootstrapping Stochastic Gradient Descent,Yang Fan;Fei Tian;Tao Qin;Tie-Yan Liu,v-yanfa@microsoft.com;fetia@microsoft.com;taoqin@microsoft.com;tie-yan.liu@microsoft.com,4;6;7,5;4,Invite to Workshop Track,2,8,0,no,11/4/16,Microsoft;Microsoft;Microsoft;Microsoft,-1;-1;-1;-1,-1;-1;-1;-1,,11/4/16,3,1,0,0,0,0,714;5217;4959;13439,558;370;289;366,15;32;33;51,43;269;587;1721,-1;-1
232,ICLR,2017,Surprise-Based Intrinsic Motivation for Deep Reinforcement Learning,Joshua Achiam;Shankar Sastry,jachiam@berkeley.edu;sastry@coe.berkeley.edu,6;6;6,3;3;4,Invite to Workshop Track,1,3,0,no,11/4/16,University of California Berkeley;University of California Berkeley,5;5,10;10,,11/4/16,79,53,17,1,3,9,709;33758,9;460,6;78,126;3463,-1;-1
233,ICLR,2017,Eigenvalues of the Hessian in Deep Learning: Singularity and Beyond,Levent Sagun;Leon Bottou;Yann LeCun,leventsagun@gmail.com;leon@bottou.org;yann@cs.nyu.edu,3;4;4,4;5;5,Reject,0,8,0,no,11/4/16,New York University;Facebook;New York University,25;-1;25,32;-1;32,,11/4/16,91,53,14,5,53,7,960;48407;92693,22;167;345,12;58;107,129;7106;10399,-1;-1
234,ICLR,2017,Extensions and Limitations of the Neural GPU,Eric Price;Wojciech Zaremba;Ilya Sutskever,ecprice@cs.utexas.edu;woj@openai.com;ilyasu@openai.com,5;4;5,3;4;4,Reject,0,2,0,no,11/4/16,"University of Texas, Austin;OpenAI;OpenAI",20;-1;-1,50;-1;-1,4;8,11/2/16,11,9,4,0,6,2,2734;18028;132921,63;43;90,21;27;53,396;2338;17040,-1;-1
235,ICLR,2017,Options Discovery with Budgeted Reinforcement Learning,Aurelia Léon;Ludovic Denoyer,aurelia.leon@lip6.fr;ludovic.denoyer@lip6.fr,5;4;5;4,4;5;5;4,Reject,2,3,0,no,11/4/16,LIP6;LIP6,-1;-1,-1;-1,,11/4/16,2,1,0,0,17,0,10;3033,18;129,2;22,0;526,-1;-1
236,ICLR,2017,On orthogonality and learning recurrent networks with long term dependencies,Eugene Vorontsov;Chiheb Trabelsi;Samuel Kadoury;Chris Pal,eugene.vorontsov@gmail.com;chiheb.trabelsi@polymtl.ca;samuel.kadoury@polymtl.ca;christopher.pal@polymtl.ca,7;5;5,4;4;5,Reject,5,4,0,no,11/5/16,Polytechnique Montreal;Polytechnique Montreal;Polytechnique Montreal;Polytechnique Montreal,351;351;351;351,981;981;981;981,1,11/5/16,93,51,30,3,13,11,921;284;2380;8308,20;11;151;120,11;4;25;33,55;42;124;761,-1;-1
237,ICLR,2017,Learning Continuous Semantic Representations of Symbolic Expressions,Miltiadis Allamanis;Pankajan Chanthirasegaran;Pushmeet Kohli;Charles Sutton,m.allamanis@ed.ac.uk;pankajan.chanthirasegaran@ed.ac.uk;pkohli@microsoft.com;csutton@ed.ac.uk,7;5;6,3;3;4,Invite to Workshop Track,2,5,0,no,11/4/16,University of Edinburgh;University of Edinburgh;Microsoft;University of Edinburgh,33;33;-1;33,27;27;-1;27,,11/4/16,45,31,13,3,6,7,1939;77;22310;4993,42;4;313;91,18;3;69;23,202;9;2770;564,-1;-1
238,ICLR,2017,Fuzzy paraphrases in learning word representations with a lexicon,Yuanzhi Ke;Masafumi Hagiwara,enshika8811.a6@keio.jp;hagiwara@keio.jp,5;3;6,3;4;4,Reject,1,9,0,no,11/3/16,Keio University;Keio University,351;351,603;603,,11/2/16,1,1,0,0,0,0,33;720,10;193,3;13,3;60,-1;-1
239,ICLR,2017,Unsupervised Deep Learning of State Representation Using Robotic Priors ,Timothee LESORT;David FILLIAT,timothee.lesort@ensta-paristech.fr;david.filliat@ensta-paristech.fr,3;3;3,5;4;4,Reject,2,2,0,no,11/4/16,ENSTA ParisTech;ENSTA ParisTech,-1;-1,-1;-1,,11/4/16,2,1,0,0,0,0,242;2358,18;135,8;23,16;160,-1;-1
240,ICLR,2017,LSTM-Based System-Call Language Modeling and Ensemble Method for Host-Based Intrusion Detection,Gyuwan Kim;Hayoon Yi;Jangho Lee;Yunheung Paek;Sungroh Yoon,kgwmath@snu.ac.kr;hyyi@snu.ac.kr;ubuntu@snu.ac.kr;ypaek@snu.ac.kr;sryoon@snu.ac.kr,5;5;8,3;4;3,Reject,1,4,0,no,11/4/16,Seoul National University;Seoul National University;Seoul National University;Seoul National University;Seoul National University,50;50;50;50;50,72;72;72;72;72,3,11/4/16,43,12,9,0,33,1,84;207;370;1769;2842,14;14;57;184;237,3;7;11;23;28,9;10;27;152;176,-1;-1
241,ICLR,2017,Epitomic Variational Autoencoders,Serena Yeung;Anitha Kannan;Yann Dauphin;Li Fei-Fei,serena@cs.stanford.edu;akannan@fb.com;ynd@fb.com;feifeili@cs.stanford.edu,5;4;8,5;5;5,Reject,1,4,0,no,11/4/16,Stanford University;Facebook;Facebook;Stanford University,3;-1;-1;3,3;-1;-1;3,5,11/4/16,3,2,2,0,0,0,1091;1723;8635;79610,39;66;43;449,16;20;27;95,123;186;1029;11687,-1;-1
242,ICLR,2017,Layer Recurrent Neural Networks,Weidi Xie;Alison Noble;Andrew Zisserman,weidi.xie@eng.ox.ac.uk;alison.noble@eng.ox.ac.uk;az@robots.ox.ac.uk,7;6;5,4;4;5,Reject,4,5,0,no,11/4/16,University of Oxford;University of Oxford;University of Oxford,50;50;50,1;1;1,2,11/4/16,2,2,1,0,0,0,1098;22;139335,30;13;797,12;2;140,208;2;20052,-1;-1
243,ICLR,2017,Communicating Hierarchical Neural Controllers for Learning Zero-shot Task Generalization,Junhyuk Oh;Satinder Singh;Honglak Lee;Pushmeet Kohli,junhyuk@umich.edu;baveja@umich.edu;honglak@umich.edu;pkohli@microsoft.com,4;7;5;3,3;5;4,Reject,2,6,0,no,11/4/16,University of Michigan;University of Michigan;University of Michigan;Microsoft,8;8;8;-1,21;21;21;-1,3;8,11/4/16,2,1,0,0,0,0,1377;19518;24193;22310,24;247;166;313,13;55;61;69,135;1940;2826;2770,-1;-1
244,ICLR,2017,NEWSQA: A MACHINE COMPREHENSION DATASET,Adam Trischler;Tong Wang;Xingdi Yuan;Justin Harris;Alessandro Sordoni;Philip Bachman;Kaheer Suleman,adam.trischler@maluuba.com;tong.wang@maluuba.com;eric.yuan@maluuba.com;justin.harris@maluuba.com;alessandro.sordoni@maluuba.com;phil.bachman@maluuba.com;k.suleman@maluuba.com,6;6;6,4;4;3,Reject,3,5,0,no,11/4/16,Maluuba;Maluuba;Maluuba;Maluuba;Maluuba;Maluuba;Maluuba,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,3,11/4/16,288,169,97,6,0,66,1570;4458;815;1310;3858;1764;692,47;479;27;37;54;31;18,17;32;10;11;19;14;10,289;278;133;147;546;223;110,-1;-1
245,ICLR,2017,Coarse Pruning of Convolutional Neural Networks with Random Masks,Sajid Anwar;Wonyong Sung,sajid@dsp.snu.ac.kr;wysung@snu.ac.kr,6;4;5,3;4;4,Reject,0,0,0,no,11/4/16,Seoul National University;Seoul National University,50;50,72;72,4,11/4/16,8,4,4,0,0,0,90;3880,26;195,6;27,3;390,-1;-1
246,ICLR,2017,Universality in halting time,Levent Sagun;Thomas Trogdon;Yann LeCun,leventsagun@gmail.com;tom.trogdon@gmail.com;yann@cs.nyu.edu,5;5;2,4;3;4,Reject,3,4,0,no,11/4/16,New York University;;New York University,25;-1;25,32;-1;32,,11/19/15,4,1,1,0,0,0,960;176;92693,22;33;345,12;7;107,129;7;10399,-1;-1
247,ICLR,2017,Generating Interpretable Images with Controllable Structure,Scott Reed;Aäron van den Oord;Nal Kalchbrenner;Victor Bapst;Matt Botvinick;Nando de Freitas,reedscot@google.com;avdnoord@google.com;nalk@google.com;vbapst@google.com;botvinick@google.com;nandodefreitas@google.com,7;5;6,3;3;3,Invite to Workshop Track,2,1,0,no,11/4/16,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,2,11/4/16,42,18,14,1,0,3,11140;8683;1590;16206;13625;19160,28;40;32;27;146;184,16;24;15;18;45;55,2061;1104;166;1416;1414;1851,-1;-1
248,ICLR,2017,Representing inferential uncertainty in deep neural networks through sampling,Patrick McClure;Nikolaus Kriegeskorte,Patrick.McClure@mrc-cbu.cam.ac.uk;Nikolaus.Kriegeskorte@mrc-cbu.cam.ac.uk,4;4;5,4;4;4,Reject,1,5,0,no,11/4/16,University of Cambridge;University of Cambridge,67;67,4;4,11,11/4/16,22,7,10,0,9,2,336;13169,37;176,10;44,19;1209,-1;-1
249,ICLR,2017,Online Structure Learning for Sum-Product Networks with Gaussian Leaves,Wilson Hsu;Agastya Kalra;Pascal Poupart,wwhsu@uwaterloo.ca;a6kalra@uwaterloo.ca;ppoupart@uwaterloo.ca,6;4;4,3;2;1,Invite to Workshop Track,2,11,0,no,11/4/16,University of Waterloo;University of Waterloo;University of Waterloo,25;25;25,174;174;174,10,11/4/16,18,8,5,0,0,1,25;29;4243,3;8;171,2;2;35,1;2;458,-1;-1
250,ICLR,2017,A Convolutional Encoder Model for Neural Machine Translation,Jonas Gehring;Michael Auli;David Grangier;Yann N. Dauphin,jgehring@fb.com;michaelauli@fb.com;grangier@fb.com;ynd@fb.com,7;6;6,3;5;4,Reject,4,5,0,no,11/4/16,Facebook;Facebook;Facebook;Facebook,-1;-1;-1;-1,-1;-1;-1;-1,3,11/4/16,209,86,85,3,34,25,2002;6880;5119;8635,17;71;64;43,9;31;25;27,259;1025;757;1029,-1;-1
251,ICLR,2017,Reference-Aware Language Models,Zichao Yang;Phil Blunsom;Chris Dyer;Wang Ling,zichaoy@cs.cmu.edu;pblunsom@google.com;cdyer@google.com;lingwang@google.com,6;5;5,4;4;4,Reject,4,3,0,no,11/4/16,Carnegie Mellon University;Google;Google;Google,1;-1;-1;-1,23;-1;-1;-1,3,11/4/16,52,35,27,2,5,10,4323;11526;21291;2596,40;144;232;757,18;47;60;21,600;1333;3165;268,-1;-1
252,ICLR,2017,Deep Learning with Sets and Point Clouds,Siamak Ravanbakhsh;Jeff Schneider;Barnabas Poczos,mravanba@cs.cmu.edu;bapoczos@cs.cmu.edu;jeff.schneider@cs.cmu.edu,5;7;5,4;1;4,Invite to Workshop Track,2,7,0,no,11/4/16,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1,23;23;23,,11/4/16,89,47,24,1,73,4,1045;5291;5788,39;204;243,12;41;40,106;497;709,-1;-1
253,ICLR,2017,Exponential Machines,Alexander Novikov;Mikhail Trofimov;Ivan Oseledets,novikov@bayesgroup.ru;mikhail.trofimov@phystech.edu;i.oseledets@skoltech.ru,5;6;7;6,4;4;3;4,Invite to Workshop Track,4,8,0,no,11/4/16,Higher School of Economics;Moscow Institute of Physics and Technology;Skolkovo Institute of Science and Technology,462;462;-1,456;313;-1,,11/4/16,37,11,16,0,0,3,1585;51;4373,153;7;197,19;3;30,153;3;402,-1;-1
254,ICLR,2017,Adversarial examples in the physical world,Alexey Kurakin;Ian J. Goodfellow;Samy Bengio,kurakin@google.com;ian@openai.com;bengio@google.com,5;6;6,4;3;3,Invite to Workshop Track,2,3,0,no,11/2/16,Google;OpenAI;Google,-1;-1;-1,-1;-1;-1,4,7/8/16,1689,971,681,30,0,323,4552;56065;26525,14;90;332,9;56;67,830;9399;3482,-1;-1
255,ICLR,2017,Gated Multimodal Units for Information Fusion,John Arevalo;Thamar Solorio;Manuel Montes-y-Gómez;Fabio A. González,jearevaloo@unal.edu.co;solorio@cs.uh.edu;smmontesg@inaoep.mx;fagonzalezo@unal.edu.co,6;7;4,3;5;4,Invite to Workshop Track,4,5,0,no,11/5/16,Universidad Nacional de Colombia;University of Houston;;Universidad Nacional de Colombia,462;161;-1;462,981;365;-1;981,,11/5/16,63,38,30,2,0,14,642;1985;634;3440,16;142;107;182,9;23;13;28,33;237;57;249,-1;-1
256,ICLR,2017,Warped Convolutions: Efficient Invariance to Spatial Transformations,Joao F. Henriques;Andrea Vedaldi,joao@robots.ox.ac.uk;vedaldi@robots.ox.ac.uk,6;7;6,5;4;4,Reject,5,6,0,no,11/4/16,University of Oxford;University of Oxford,50;50,1;1,,9/14/16,69,34,17,0,21,5,7286;34143,44;202,20;63,1801;4663,-1;-1
257,ICLR,2017,Human perception in computer vision,Ron Dekel,ron.dekel@weizmann.ac.il,7;6;6,3;4;3,Reject,1,4,0,no,11/4/16,Weizmann Institute,105,981,2;8,11/4/16,3,1,0,0,12,0,22,15,3,1,-1
258,ICLR,2017,Binary Paragraph Vectors,Karol Grzegorczyk;Marcin Kurdziel,kgr@agh.edu.pl;kurdziel@agh.edu.pl,6;5;6,2;5;3,Reject,1,5,0,no,11/4/16,"AGH University of Science and Technology, Krakow, Poland;AGH University of Science and Technology, Krakow, Poland",462;462,708;708,6,11/3/16,5,2,0,0,7,1,24;168,8;30,4;7,1;9,-1;-1
259,ICLR,2017,Modularized Morphing of Neural Networks,Tao Wei;Changhu Wang;Chang Wen Chen,taowei@buffalo.edu;chw@microsoft.com;chencw@buffalo.edu,7;6;7;5,4;5;5;4,Invite to Workshop Track,1,5,0,no,11/4/16,"State University of New York, Buffalo;Microsoft;State University of New York, Buffalo",94;-1;94,263;-1;263,1;10,11/4/16,11,6,1,1,15,2,2302;2425;6254,106;98;576,20;24;35,230;201;508,-1;-1
260,ICLR,2017,NEUROGENESIS-INSPIRED DICTIONARY LEARNING: ONLINE MODEL ADAPTION IN A CHANGING WORLD,Sahil Garg;Irina Rish;Guillermo Cecchi;Aurelie Lozano,sahilgar@usc.edu;rish@us.ibm.com;gcecchi@us.ibm.com;aclozano@us.ibm.com,7;5;5,4;3;4,Reject,1,4,0,no,11/4/16,University of Southern California;International Business Machines;International Business Machines;International Business Machines,31;-1;-1;-1,60;-1;-1;-1,,11/4/16,3,1,2,0,8,0,711;3784;3586;100,101;141;198;24,17;26;30;5,15;321;213;4,-1;-1
261,ICLR,2017,Low-rank passthrough neural networks,Antonio Valerio Miceli Barone,amiceli@inf.ed.ac.uk,4;5;6,4;4,Reject,3,10,0,no,11/4/16,University of Edinburgh,33,27,,3/10/16,8,4,1,0,16,0,671,21,10,88,-1
262,ICLR,2017,Towards Understanding the Invertibility of Convolutional Neural Networks,Anna C. Gilbert;Yi Zhang;Kibok Lee;Yuting Zhang;Honglak Lee,annacg@umich.edu;yeezhang@umich.edu;kibok@umich.edu;yutingzh@umich.edu;honglak@umich.edu,5;7;4,4;3;4,Reject,2,3,1,no,11/4/16,University of Michigan;University of Michigan;University of Michigan;University of Michigan;University of Michigan,8;8;8;8;8,21;21;21;21;21,,11/4/16,35,19,3,1,0,1,12525;2988;736;-1;24193,151;196;35;-1;166,35;28;12;-1;61,1521;197;111;0;2826,-1;-1
263,ICLR,2017,Recurrent Normalization Propagation,César Laurent;Nicolas Ballas;Pascal Vincent,cesar.laurent@umontreal.ca;nicolas.ballas@umontreal.ca;pascal.vincent@umontreal.ca,4;6;6,4;4;3,Invite to Workshop Track,3,3,0,no,11/4/16,University of Montreal;University of Montreal;University of Montreal,113;113;113,103;103;103,3;5,11/4/16,0,0,0,0,0,0,2258;5005;15655,10;54;118,6;20;33,180;589;1337,-1;-1
264,ICLR,2017,Learning in Implicit Generative Models,Shakir Mohamed;Balaji Lakshminarayanan,shakir@google.com;balajiln@google.com,6;8;7,4;4;3,Invite to Workshop Track,0,5,1,no,11/4/16,Google;Google,-1;-1,-1;-1,5;4;11,10/11/16,201,111,43,5,189,6,6973;2939,51;43,27;22,1010;382,-1;-1
265,ICLR,2017,Intelligible Language Modeling with Input Switched Affine Networks,Jakob Foerster;Justin Gilmer;Jan Chorowski;Jascha Sohl-dickstein;David Sussillo,jakob.foerster@cs.ox.ac.uk;gilmer@google.com;jan.chorowski@cs.uni.wroc.pl;jaschasd@google.com;sussillo@google.com,6;7;6,4;3;4,Reject,1,1,0,no,11/5/16,University of Oxford;Google;University of Wroclaw;Google;Google,50;-1;230;-1;-1,1;-1;981;-1;-1,3,11/5/16,6,4,1,0,22,0,2072;3404;5692;5020;3281,58;45;42;101;45,19;18;18;33;25,335;464;463;703;267,-1;-1
266,ICLR,2017,Lifelong Perceptual Programming By Example,Alexander L. Gaunt;Marc Brockschmidt;Nate Kushman;Daniel Tarlow,t-algaun@microsoft.com;mabrocks@microsoft.com;nkushman@microsoft.com;dtarlow@microsoft.com,2;8;4,5;4;4,Invite to Workshop Track,1,8,0,no,11/4/16,Microsoft;Microsoft;Microsoft;Microsoft,-1;-1;-1;-1,-1;-1;-1;-1,,11/4/16,7,3,0,0,0,0,1194;2508;1511;2537,38;61;29;68,15;22;17;23,88;329;202;306,-1;-1
267,ICLR,2017,Alternating Direction Method of Multipliers for Sparse Convolutional Neural Networks,Farkhondeh Kiaee;Christian Gagné;and Mahdieh Abbasi,farkhondeh.kiaee.1@ulaval.ca;christian.gagne@gel.ulaval.ca;mahdieh.abbasi.1@ulaval.ca,7;7;6;5,3;5;4;3,Reject,1,7,1,no,11/4/16,Laval university;Laval university;Laval university,462;462;462,265;265;265,8,11/4/16,12,6,5,1,5,1,37;1937;72,7;102;10,3;19;4,3;152;5,-1;-1
268,ICLR,2017,Sentence Ordering using Recurrent Neural Networks,Lajanugen Logeswaran;Honglak Lee;Dragomir Radev,llajan@umich.edu;honglak@eecs.umich.edu;radev@umich.edu,6;6;7,4;4;3,Reject,3,1,0,no,11/3/16,University of Michigan;University of Michigan;University of Michigan,8;8;8,21;21;21,3,11/3/16,14,7,2,2,0,2,1622;24193;13144,12;166;280,6;61;53,164;2826;1439,-1;-1
269,ICLR,2017,Towards an automatic Turing test: Learning to evaluate dialogue responses,Ryan Lowe;Michael Noseworthy;Iulian V. Serban;Nicolas Angelard-Gontier;Yoshua Bengio;Joelle Pineau,rlowe1@cs.mcgill.ca;michael.noseworthy@mail.mcgill.ca;julianserban@gmail.com;nicolas.angelard-gontier@mail.mcgill.ca;yoshua.umontreal@gmail.com;jpineau@cs.mcgill.ca,5;4;7,4;4;4,Invite to Workshop Track,4,5,0,no,11/5/16,McGill University;McGill University;University College London;McGill University;University of Montreal;McGill University,80;80;45;80;113;80,42;42;15;42;103;42,,11/5/16,155,92,29,9,61,24,3322;720;2004;165;205096;11249,53;10;13;3;807;266,17;3;6;2;147;45,516;76;163;25;24136;1227,-1;-1
270,ICLR,2017,Inference and Introspection in Deep Generative Models of Sparse Data,Rahul G. Krishnan;Matthew Hoffman,rahul@cs.nyu.edu;matthoffm@adobe.com,6;7;5;5,4;3;3;4,Reject,1,5,0,no,11/4/16,New York University;Adobe Systems,25;-1,32;-1,5,11/4/16,3,2,0,0,0,0,571;8511,14;94,7;29,113;1217,-1;-1
271,ICLR,2017,TreNet: Hybrid Neural Networks for Learning the Local Trend in Time Series,Tao Lin;Tian Guo;Karl Aberer,tao.lin@epfl.ch;tian.guo@epfl.ch;karl.aberer@epfl.ch,6;5;4,4;5;4,Reject,4,5,0,no,11/5/16,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne,25;25;25,30;30;30,,11/5/16,1,0,0,0,0,0,439;3;12743,111;8;631,11;1;53,24;0;994,-1;-1
272,ICLR,2017,Compositional Kernel Machines,Robert Gens;Pedro Domingos,rcg@cs.washington.edu;pedrod@cs.washington.edu,5;5;6;5,3;4;4;4,Invite to Workshop Track,1,4,0,no,11/3/16,University of Washington;University of Washington,6;6,25;25,2,11/3/16,4,1,2,0,0,1,583;28993,19;229,6;72,85;3260,-1;-1
273,ICLR,2017,Introducing Active Learning for CNN under the light of Variational Inference,Melanie Ducoffe;Frederic Precioso,ducoffe@i3s.unice.fr;precioso@i3s.unice.fr,6;6;6,1;2;2,Reject,6,4,0,no,11/5/16,Université Côte d'Azur;Université Côte d'Azur,-1;-1,-1;-1,,11/5/16,0,0,0,0,0,0,1700;714,12;81,4;14,130;41,-1;-1
274,ICLR,2017,L-SR1: A Second Order Optimization Method for Deep Learning,Vivek Ramamurthy;Nigel Duffy,vivek.ramamurthy@sentient.ai;nigel.duffy@sentient.ai,4;4;5,4;3;3,Reject,3,6,0,no,11/4/16,"Sentient Technologies, Inc.;Sentient Technologies, Inc.",-1;-1,-1;-1,,11/4/16,3,1,2,0,0,1,22;3390,14;23,2;9,1;328,-1;-1
275,ICLR,2017,Adversarial examples for generative models,Jernej Kos;Ian Fischer;Dawn Song,jernej@kos.mx;iansf@google.com;dawnsong.travel@gmail.com,5;6;5,4;3;4,Reject,3,3,0,no,11/5/16,National University of Singapore;Google;University of California Berkeley,15;-1;5,24;-1;10,5;4,11/5/16,134,85,21,1,14,9,589;370;43319,23;25;396,8;8;100,52;23;4323,-1;-1
276,ICLR,2017,Hierarchical compositional feature learning,Miguel Lazaro-Gredilla;Yi Liu;D. Scott Phoenix;Dileep George,miguel@vicarious.com;yi@vicarious.com;scott@vicarious.com;dileep@vicarious.com,5;5;4,4;4;4,Reject,1,3,0,no,11/3/16,Vicarious Inc.;Vicarious Inc.;Vicarious Inc.;Vicarious Inc.,-1;-1;-1;-1,-1;-1;-1;-1,5,11/3/16,8,4,3,0,14,0,1833;9412;292;699,59;734;10;26,19;41;6;9,202;630;26;67,-1;-1
277,ICLR,2017,Unsupervised Perceptual Rewards for Imitation Learning,Pierre Sermanet;Kelvin Xu;Sergey Levine,sermanet@google.com;kelvinxx@google.com;slevine@google.com,4;6;6,4;4;5,Invite to Workshop Track,2,3,0,no,11/4/16,Google;Google;Google,-1;-1;-1,-1;-1;-1,,11/4/16,64,41,12,0,40,5,23970;7412;24658,42;19;310,18;11;73,2964;757;3204,-1;-1
278,ICLR,2017,Submodular Sum-product Networks for Scene Understanding,Abram L. Friesen;Pedro Domingos,afriesen@cs.washington.edu;pedrod@cs.washington.edu,5;4;4,4;3;3,Reject,1,4,0,no,11/4/16,University of Washington;University of Washington,6;6,25;25,10,11/4/16,1,0,1,0,0,0,233;28993,20;229,7;72,17;3260,-1;-1
279,ICLR,2017,Semi-Supervised Detection of Extreme Weather Events in Large Climate Datasets,Evan Racah;Christopher Beckham;Tegan Maharaj;Prabhat;Christopher Pal,eracah@lbl.gov;christopher.beckham@polymtl.ca;tegan.maharaj@polymtl.ca;prabhat@lbl.gov;christopher.pal@polymtl.ca,6;4;6,3;4;4,Reject,2,3,0,no,11/4/16,Lawrence Berkeley National Lab;Polytechnique Montreal;Polytechnique Montreal;Lawrence Berkeley National Lab;Polytechnique Montreal,-1;351;351;-1;351,-1;981;981;-1;981,,11/4/16,11,4,5,0,2,0,407;319;719;3113;8308,24;39;18;151;120,11;9;7;27;33,21;44;77;188;761,-1;-1
280,ICLR,2017,Progressive Attention Networks for Visual Attribute Prediction,Paul Hongsuck Seo;Zhe Lin;Scott Cohen;Xiaohui Shen;Bohyung Han,hsseo@postech.ac.kr;zlin@adobe.com;scohen@adobe.com;xshen@adobe.com;bhhan@postech.ac.kr,7;4;6,4;5;3,Reject,2,5,0,no,11/3/16,POSTECH;Adobe Systems;Adobe Systems;Adobe Systems;POSTECH,113;-1;-1;-1;113,104;-1;-1;-1;104,,6/8/16,27,7,13,0,9,4,456;10142;3636;6377;7015,13;132;732;159;120,8;48;26;40;38,48;1553;428;904;861,-1;-1
281,ICLR,2017,Fast Adaptation in Generative Models with Generative Matching Networks,Sergey Bartunov;Dmitry P. Vetrov,sbos.net@gmail.com;vetrovd@yandex.ru,4;5;7,4;3;4,Reject,2,4,0,no,11/4/16,Google;Higher School of Economics,-1;462,-1;456,5;6;8,11/4/16,29,17,8,2,0,3,1131;2097,13;124,8;16,122;285,-1;-1
282,ICLR,2017,Classless Association using Neural Networks,Federico Raue;Sebastian Palacio;Andreas Dengel;Marcus Liwicki,federico.raue@dfki.de;sebastian.palacio@dfki.de;andreas.dengel@dfki.de;liwicki@cs.uni-kl.de,5;5;6,4;3;3,Reject,1,4,0,no,11/4/16,German Research Center for AI;German Research Center for AI;German Research Center for AI;TU Kaiserslautern,-1;-1;-1;140,-1;-1;-1;398,,11/4/16,3,0,1,0,0,0,255;35;4777;4619,21;16;442;260,3;3;33;30,22;2;314;372,-1;-1
283,ICLR,2017,Recurrent Coevolutionary Feature Embedding Processes for Recommendation,Hanjun Dai*;Yichen Wang*;Rakshit Trivedi;Le Song,hanjundai@gatech.edu;yichen.wang@gatech.edu;rstrivedi@gatech.edu;lsong@cc.gatech.edu,6;6;6,4;4;3,Reject,4,4,0,no,11/4/16,Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology,12;12;12;12,33;33;33;33,,11/4/16,7,4,0,0,0,0,1868;2197;416;9393,57;179;30;329,17;25;12;53,279;124;38;1105,-1;-1
284,ICLR,2017,Unsupervised Learning of State Representations for Multiple Tasks,Antonin Raffin;Sebastian Höfer;Rico Jonschkowski;Oliver Brock;Freek Stulp,antonin.raffin@ensta-paristech.fr;sebastian.hoefer@tu-berlin.de;rico.jonschkowski@tu-berlin.de;oliver.brock@tu-berlin.de;freek.stulp@dlr.de,6;5;6,4;4;3,Reject,2,1,0,no,11/4/16,ENSTA ParisTech;TU Berlin;TU Berlin;TU Berlin;German Aerospace Center (DLR),-1;105;105;105;-1,-1;82;82;82;-1,,11/4/16,7,4,0,0,0,0,49;80;484;6856;1691,6;26;30;281;97,4;5;11;44;22,0;4;49;390;82,-1;-1
285,ICLR,2017,A Neural Stochastic Volatility Model,Rui Luo;Xiaojun Xu;Weinan Zhang;Jun Wang,r.luo@cs.ucl.ac.uk;xuxj@apex.sjtu.edu.cn;wnzhang@apex.sjtu.edu.cn;j.wang@cs.ucl.ac.uk,6;5;5,4;4;3,Reject,3,1,0,no,11/4/16,University College London;Shanghai Jiao Tong University;Shanghai Jiao Tong University;University College London,45;61;61;45,15;214;214;15,5,11/4/16,13,7,1,0,17,0,1051;1156;4935;5733,82;98;207;106,14;15;31;27,116;101;700;939,-1;-1
286,ICLR,2017,Playing SNES in the Retro Learning Environment,Nadav Bhonker;Shai Rozenberg;Itay Hubara,nadavbh@tx.technion.ac.il;shairoz@tx.technion.ac.il;itayhubara@gmail.com,5;4;7,4;4;4,Reject,3,3,0,no,11/4/16,Technion;Technion;,24;24;-1,301;301;-1,,11/4/16,11,6,5,0,0,2,11;116;3038,3;9;21,1;6;11,2;17;435,-1;-1
287,ICLR,2017,Linear Time Complexity Deep Fourier Scattering Network and Extension to Nonlinear Invariants,Randall Balestriero;Herve Glotin,randallbalestriero@gmail.com;glotin@univ-tln.fr,4;5;4,3;3;5,Reject,1,4,0,no,11/4/16,Rice University;CNRS university Toulon,80;462,87;981,,11/4/16,3,2,1,0,8,0,114;1971,38;280,5;25,2;109,-1;-1
288,ICLR,2017,Boosting Image Captioning with Attributes,Ting Yao;Yingwei Pan;Yehao Li;Zhaofan Qiu;Tao Mei,tiyao@microsoft.com;v-yipan@microsoft.com;v-yehl@microsoft.com;v-zhqiu@microsoft.com;tmei@microsoft.com,4;5;6,5;4;5,Reject,3,3,0,no,11/4/16,Microsoft;Microsoft;Microsoft;Microsoft;Microsoft,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,3;2,11/4/16,293,131,98,11,14,36,2877;1523;668;1218;12210,87;41;21;25;561,26;16;10;14;57,287;170;85;140;1097,-1;-1
289,ICLR,2017,Recursive Regression with Neural Networks: Approximating the HJI PDE Solution,Vicenç Rubies Royo;Claire Tomlin,vrubies@berkeley.edu;tomlin@berkeley.edu,7;3;5,3;5;1,Invite to Workshop Track,0,4,0,no,11/4/16,University of California Berkeley;University of California Berkeley,5;5,10;10,,11/4/16,5,1,3,0,3,0,31;15242,2;535,2;58,1;722,-1;-1
290,ICLR,2017,Deep Unsupervised Clustering with Gaussian Mixture Variational Autoencoders,Nat Dilokthanakul;Pedro A. M. Mediano;Marta Garnelo;Matthew C.H. Lee;Hugh Salimbeni;Kai Arulkumaran;Murray Shanahan,n.dilokthanakul14@imperial.ac.uk;pmediano@imperial.ac.uk;m.garnelo-abellanas13@imperial.ac.uk;matthew.lee13@imperial.ac.uk;h.salimbeni15@imperial.ac.uk;kailash.arulkumaran13@imperial.ac.uk;m.shanahan@imperial.ac.uk,8;4;4,4;4;4,Reject,6,5,0,no,11/3/16,Imperial College London;Imperial College London;Imperial College London;Imperial College London;Imperial College London;Imperial College London;Imperial College London,75;75;75;75;75;75;75,8;8;8;8;8;8;8,5,11/3/16,212,89,85,4,14,23,258;341;941;978;409;1395;4866,8;20;23;18;8;20;166,4;8;11;12;6;9;38,25;29;113;100;70;103;461,-1;-1
291,ICLR,2017,Taming the waves: sine as activation function in deep neural networks,Giambattista Parascandolo;Heikki Huttunen;Tuomas Virtanen,giambattista.parascandolo@tut.fi;heikki.huttunen@tut.fi;tuomas.virtanen@tut.fi,4;4;4,4;4;4,Reject,1,5,0,no,11/4/16,Tampere University of Technology;Tampere University of Technology;Tampere University of Technology,462;462;462,552;552;552,,11/4/16,17,11,1,1,0,2,926;1514;7117,15;129;219,12;16;44,95;117;607,-1;-1
292,ICLR,2017,Significance of Softmax-Based Features over Metric Learning-Based Features,Shota Horiguchi;Daiki Ikami;Kiyoharu Aizawa,horiguchi@hal.t.u-tokyo.ac.jp;ikami@hal.t.u-tokyo.ac.jp;aizawa@hal.t.u-tokyo.ac.jp,5;7;4,4;4;5,Reject,3,10,0,no,10/31/16,The University of Tokyo;The University of Tokyo;The University of Tokyo,50;50;50,39;39;39,2,10/31/16,25,13,2,0,24,0,147;171;4729,20;9;666,8;4;34,5;26;318,-1;-1
293,ICLR,2017,Deep Generalized Canonical Correlation Analysis,Adrian Benton;Huda Khayrallah;Biman Gujral;Drew Reisinger;Sheng Zhang;Raman Arora,adrian@cs.jhu.edu;huda@jhu.edu;bgujral1@jhu.edu;reisinger@cogsci.jhu.edu;zsheng2@jhu.edu;arora@cs.jhu.edu,6;5;7,4;5;3,Reject,0,4,0,no,11/4/16,Johns Hopkins University;Johns Hopkins University;Johns Hopkins University;Johns Hopkins University;Johns Hopkins University;Johns Hopkins University,67;67;67;67;67;67,17;17;17;17;17;17,,11/4/16,44,20,16,0,24,8,593;310;61;158;401;2691,24;24;4;10;21;85,10;10;4;3;11;22,44;30;10;20;64;443,-1;-1
294,ICLR,2017,Learning Recurrent Span Representations for Extractive Question Answering,Kenton Lee;Tom Kwiatkowksi;Ankur Parikh;Dipanjan Das,kentonl@cs.washington.edu;tomkwiat@google.com;aparikh@google.com;dipanjand@google.com,7;6;6,4;5;3,Reject,1,1,0,no,11/4/16,University of Washington;Google;Google;Google,6;-1;-1;-1,25;-1;-1;-1,3,11/4/16,99,48,23,1,9,15,12305;2371;1596;5306,36;40;54;68,20;18;17;30,3508;212;199;726,-1;-1
295,ICLR,2017,GRAM: Graph-based Attention Model for Healthcare Representation Learning,Edward Choi;Mohammad Taha Bahadori;Le Song;Walter F. Stewart;Jimeng Sun,mp2893@gatech.edu;bahadori@gatech.edu;lsong@cc.gatech.edu;stewarwf@sutterhealth.org;jsun@cc.gatech.edu,6;6;6,4;4;3,Reject,2,6,0,no,11/4/16,Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;;Georgia Institute of Technology,12;12;12;-1;12,33;33;33;-1;33,10,11/4/16,194,89,64,4,49,22,1003;1673;9393;24040;10478,36;34;329;390;240,11;16;53;77;55,82;191;1105;1200;787,-1;-1
296,ICLR,2017,Improving Sampling from Generative Autoencoders with Markov Chains,Antonia Creswell;Kai Arulkumaran;Anil Anthony Bharath,ac2211@imperial.ac.uk;ka709@imperial.ac.uk;aab01@imperial.ac.uk,3;3;3,5;4;4,Reject,5,4,0,no,10/31/16,Imperial College London;Imperial College London;Imperial College London,75;75;75,8;8;8,5;4,10/28/16,7,4,2,0,42,0,464;1395;2883,15;20;128,7;9;20,34;103;171,-1;-1
297,ICLR,2017,Towards Principled Methods for Training Generative Adversarial Networks,Martin Arjovsky;Leon Bottou,martinarjovsky@gmail.com;leonb@fb.com,8;10;7,3;5;4,Accept (Oral),4,16,0,no,11/4/16,New York University;Facebook,25;-1,32;-1,5;4,11/4/16,841,504,138,17,75,89,7964;48407,15;167,9;58,1784;7106,-1;-1
298,ICLR,2017,Learning Disentangled Representations in Deep Generative Models,N. Siddharth;Brooks Paige;Alban Desmaison;Jan-Willem van de Meent;Frank Wood;Noah D. Goodman;Pushmeet Kohli;Philip H.S. Torr,nsid@robots.ox.ac.uk;brooks@robots.ox.ac.uk;alban@robots.ox.ac.uk;j.vandemeent@northeastern.edu;fwood@robots.ox.ac.uk;ngoodman@stanford.edu;pkohli@microsoft.com;philip.torr@eng.ox.ac.uk,6;6;5,5;4;3,Reject,2,4,0,no,11/4/16,University of Oxford;University of Oxford;University of Oxford;Northeastern University;University of Oxford;Stanford University;Microsoft;University of Oxford,50;50;50;18;50;3;-1;50,1;1;1;778;1;3;-1;1,5;10,11/4/16,14,8,4,0,0,0,212;832;7028;757;2676;8530;22310;1960,21;39;19;56;77;240;313;30,9;13;9;14;27;48;69;9,15;87;858;61;210;582;2770;459,-1;-1
299,ICLR,2017,Local minima in training of deep networks,Grzegorz Swirszcz;Wojciech Marian Czarnecki;Razvan Pascanu,swirszcz@google.com;lejlot@google.com;razp@google.com,5;5;3,3;4;5,Reject,3,4,0,no,11/4/16,Google;Google;Google,-1;-1;-1,-1;-1;-1,1,11/4/16,60,33,3,2,32,4,643;2766;16936,50;71;101,15;20;46,52;285;1688,-1;-1
300,ICLR,2017,Compact Embedding of Binary-coded Inputs and Outputs using Bloom Filters,Joan Serrà;Alexandros Karatzoglou,joan.serra@telefonica.com;alexandros.karatzoglou@telefonica.com,3;6;6,3;4;4,Reject,2,6,0,no,11/3/16,Telefonica Research;Telefonica Research,-1;-1,-1;-1,10,11/3/16,1,0,0,0,0,0,2502;5008,152;91,23;28,276;620,-1;-1
301,ICLR,2017,RenderGAN: Generating Realistic Labeled Data,Leon Sixt;Benjamin Wild;Tim Landgraf,leon.sixt@fu-berlin.de;benjamin.wild@fu-berlin.de;tim.landgraf@fu-berlin.de,5;6;6,3;4;4,Invite to Workshop Track,3,6,0,no,11/4/16,Freie Universität Berlin;Freie Universität Berlin;Freie Universität Berlin,-1;-1;-1,-1;-1;-1,5;4;2,11/4/16,86,32,37,0,10,2,111;148;505,6;8;44,3;6;13,5;7;21,-1;-1
302,ICLR,2017,BIOACOUSTIC SEGMENTATION BY HIERARCHICAL DIRICHLET PROCESS HIDDEN MARKOV MODEL,Vincent Roger;Marius Bartcus;Faicel Chamroukhi;Hervé Glotin,vincent-roger@etud.univ-tln.fr;marius.bartcus@gmail.com;faicel.chamroukhi@unicaen.fr;glotin@univ-tln.fr,5;4;5,3;3;5,Reject,2,0,0,no,11/4/16,CNRS university Toulon;;University of Caen Normandie;CNRS university Toulon,462;-1;-1;462,981;-1;-1;981,11;2,11/4/16,0,0,0,0,0,0,21;17;763;1971,40;10;66;280,2;3;11;25,1;0;48;109,-1;-1
303,ICLR,2017,Tuning Recurrent Neural Networks with Reinforcement Learning,Natasha Jaques;Shixiang Gu;Richard E. Turner;Douglas Eck,jaquesn@mit.edu;sg717@cam.ac.uk;ret26@cam.ack.uk;deck@google.com,5;6;5,5;3;5,Invite to Workshop Track,7,5,1,no,11/4/16,Massachusetts Institute of Technology;University of Cambridge;University of Cambridge;Google,2;67;67;-1,5;4;4;-1,,11/4/16,41,12,16,1,0,3,911;3813;3005;2559,43;39;175;84,19;21;30;27,74;474;314;274,-1;-1
304,ICLR,2017,An Analysis of Feature Regularization for Low-shot Learning,Zhuoyuan Chen;Han Zhao;Xiao Liu;Wei Xu,chenzhuoyuan@baidu.com;liuxiao12@baidu.com;wei.xu@baidu.com;han.zhao@cs.cmu.edu,5;6;6,4;3;3,Reject,3,4,0,no,11/4/16,Baidu;Baidu;Baidu;Carnegie Mellon University,-1;-1;-1;1,-1;-1;-1;23,,11/4/16,2,2,0,0,0,0,1799;2582;3921;275,93;275;287;62,21;25;31;9,103;154;202;11,-1;-1
305,ICLR,2017,Gradients of Counterfactuals,Mukund Sundararajan;Ankur Taly;Qiqi Yan,mukunds@google.com;ataly@google.com;qiqiyan@google.com,3;3;5,4;4;4,Reject,0,4,0,no,11/4/16,Google;Google;Google,-1;-1;-1,-1;-1;-1,3,11/4/16,28,9,7,0,8,5,2327;1767;1636,78;42;68,21;17;16,309;231;251,-1;-1
306,ICLR,2017,Encoding and Decoding Representations with Sum- and Max-Product Networks,Antonio Vergari;Robert Peharz;Nicola Di Mauro;Floriana Esposito,antonio.vergari@uniba.it;robert.peharz@medunigraz.at;nicola.dimauro@uniba.it;floriana.esposito@uniba.it,6;3;6,4;3;4,Reject,3,8,0,no,11/4/16,University of Bari;Medical University of Graz;University of Bari;University of Bari,194;462;194;194,110;452;110;110,5;1,11/4/16,0,0,0,0,0,0,274;684;732;2891,31;46;142;409,9;13;15;26,21;53;31;184,-1;-1
307,ICLR,2017,Skip-graph: Learning graph embeddings with an encoder-decoder model,John Boaz Lee;Xiangnan Kong,jtlee@wpi.edu;xkong@wpi.edu,5;6;7,4;1;3,Reject,3,5,0,no,11/4/16,Worcester Polytechnic Institute;Worcester Polytechnic Institute,194;194,981;981,3;10,11/4/16,6,3,4,0,0,0,403;2337,32;120,11;27,35;191,-1;-1
308,ICLR,2017,Information Dropout: learning optimal representations through noise,Alessandro Achille;Stefano Soatto,achille@cs.ucla.edu;soatto@cs.ucla.edu,4;6;6,4;4;4,Reject,2,3,0,no,11/4/16,"University of California, Los Angeles;University of California, Los Angeles",20;20,14;14,5;8,11/4/16,20,5,8,1,0,1,643;15626,31;458,11;61,77;1438,-1;-1
309,ICLR,2017,Deep unsupervised learning through spatial contrasting,Elad Hoffer;Itay Hubara;Nir Ailon,ehoffer@tx.technion.ac.il;itayh@tx.technion.ac.il;nailon@cs.technion.ac.il,5;6;7,4;4;4,Reject,3,3,0,no,10/19/16,Technion;Technion;Technion,24;24;24,301;301;301,,10/2/16,20,9,8,0,7,0,1480;3038;3403,27;21;92,12;11;24,176;435;413,-1;-1
310,ICLR,2017,Dynamic Partition Models,Marc Goessling;Yali Amit,goessling@uchicago.edu,3;6;3,4;3;4,Reject,2,4,0,no,11/2/16,University of Chicago,45,10,,11/2/16,6,0,0,0,6,0,19;3448,11;92,2;25,2;236,-1;-1
311,ICLR,2017,End-to-End Answer Chunk Extraction and Ranking for Reading Comprehension,Yang Yu;Wei Zhang;Bowen Zhou;Kazi Hasan;Mo Yu;Bing Xiang,yu@us.ibm.com;zhangwei@us.ibm.com;zhou@us.ibm.com;kshasan@us.ibm.com;yum@us.ibm.com;bingxia@us.ibm.com,4;5;6,3;3;3,Reject,3,1,0,no,11/4/16,International Business Machines;International Business Machines;International Business Machines;International Business Machines;International Business Machines;International Business Machines,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,,10/31/16,48,22,6,1,4,6,3970;296;6452;262;3544;5545,649;11;187;37;71;105,27;8;31;11;26;30,101;52;885;13;454;717,-1;-1
312,ICLR,2017,#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning,Haoran Tang;Rein Houthooft;Davis Foote;Adam Stooke;Xi Chen;Yan Duan;John Schulman;Filip De Turck;Pieter Abbeel,hrtang.alex@berkeley.edu;rein.houthooft@ugent.be;djfoote@berkeley.edu;adam.stooke@berkeley.edu;peter@openai.com;rocky@openai.com;joschu@openai.com;filip.deturck@ugent.be;pieter@openai.com,6;4;7,4;3;4,Reject,4,4,0,no,11/5/16,University of California Berkeley;Ghent University;University of California Berkeley;University of California Berkeley;OpenAI;OpenAI;OpenAI;Ghent University;OpenAI,5;462;5;5;-1;-1;-1;462;-1,10;118;10;10;-1;-1;-1;118;-1,8,11/5/16,254,125,88,6,0,25,616;3652;312;302;13677;5544;15023;7416;36938,17;22;7;8;444;48;55;680;434,3;11;2;3;42;19;31;36;94,90;420;32;31;1552;643;2492;496;4451,-1;-1
313,ICLR,2017,Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models,Ashwin K Vijayakumar;Michael Cogswell;Ramprasaath R. Selvaraju;Qing Sun;Stefan Lee;David Crandall;Dhruv Batra,ashwinkv@vt.edu;cogswell@vt.edu;ram21@vt.edu;sunqing@vt.edu;steflee@vt.edu;djcran@indiana.edu;dbatra@vt.edu,6;6;4,4;4;4,Reject,2,5,0,no,11/4/16,Virginia Tech;Virginia Tech;Virginia Tech;Virginia Tech;Virginia Tech;University of Arizona;Virginia Tech,80;80;80;80;80;161;80,286;286;286;286;286;156;286,3,10/7/16,142,82,42,6,0,16,223;2475;2498;219;1794;5409;1619,6;12;14;12;31;176;26,4;9;8;4;20;31;15,27;343;352;28;262;463;278,-1;-1
314,ICLR,2017,Extrapolation and learning equations,Georg Martius;Christoph H. Lampert,gmartius@ist.ac.at;chl@ist.ac.at,7;3;6,4;4;3,Invite to Workshop Track,3,4,0,no,11/2/16,Institute of Science and Technology Austria;Institute of Science and Technology Austria,94;94,981;981,,10/10/16,22,7,9,0,0,2,554;7577,77;153,14;34,29;1147,-1;-1
315,ICLR,2017,Learning Python Code Suggestion with a Sparse Pointer Network,Avishkar Bhoopchand;Tim Rocktäschel;Earl Barr;Sebastian Riedel,avishkar.bhoopchand.15@ucl.ac.uk;t.rocktaschel@cs.ucl.ac.uk;e.barr@cs.ucl.ac.uk;s.riedel@cs.ucl.ac.uk,6;6;5,4;4;4,Reject,4,5,0,no,11/3/16,University College London;University College London;University College London;University College London,45;45;45;45,15;15;15;15,3,11/3/16,35,16,16,0,58,5,48;2272;3069;5852,3;48;78;230,2;22;24;35,5;282;288;891,-1;-1
316,ICLR,2017,Evaluation of Defensive Methods for DNNs against Multiple Adversarial Evasion Models,Xinyun Chen;Bo Li;Yevgeniy Vorobeychik,jungyhuk@gmail.com;bbbli@umich.edu;yevgeniy.vorobeychik@vanderbilt.edu,5;5;4,4;3;3,Reject,4,4,0,no,11/4/16,Shanghai Jiao Tong University;University of Michigan;Vanderbilt University,61;8;230,214;21;108,4;8,11/4/16,3,3,0,0,0,0,1670;3946;2434,51;358;226,14;29;27,143;248;173,-1;-1
317,ICLR,2017,Representation Stability as a Regularizer for Improved Text Analytics Transfer Learning,Matthew Riemer;Elham Khabiri;Richard Goodwin,mdriemer@us.ibm.com;ekhabiri@us.ibm.com;rgoodwin@us.ibm.com,5;7;6,4;4;3,Reject,2,3,0,no,11/4/16,International Business Machines;International Business Machines;International Business Machines,-1;-1;-1,-1;-1;-1,6;8,11/4/16,12,8,4,0,3,1,281;248;2072,27;23;105,9;7;25,25;23;131,-1;-1
318,ICLR,2017,Exploring LOTS in Deep Neural Networks,Andras Rozsa;Manuel Gunther;Terrance E. Boult,andras.rozsa@yahoo.com;siebenkopf@googlemail.com;tboult@vast.uccs.edu,6;6;6,4;4;4,Reject,3,7,2,no,11/4/16,"University of Colorado, Colorado Springs;University of Colorado, Colorado Springs;University of Colorado, Colorado Springs",462;462;462,981;981;981,4,11/4/16,2,0,0,0,2,0,403;829;5706,15;38;286,9;17;38,38;81;496,-1;-1
319,ICLR,2017,Development of JavaScript-based deep learning platform and application to distributed training,Masatoshi Hidaka;Ken Miura;Tatsuya Harada,hidaka@mi.t.u-tokyo.ac.jp;miura@mi.t.u-tokyo.ac.jp;harada@mi.t.u-tokyo.ac.jp,6;4;7,4;2;3,Invite to Workshop Track,2,3,2,no,11/4/16,The University of Tokyo;The University of Tokyo;The University of Tokyo,50;50;50,39;39;39,,11/4/16,72,1,0,0,71,0,20;259;2567,5;70;213,3;9;26,3;17;324,-1;-1
320,ICLR,2017,Annealing Gaussian into ReLU: a New Sampling Strategy for Leaky-ReLU RBM,Chun-Liang Li;Siamak Ravanbakhsh;Barnabas Poczos,chunlial@cs.cmu.edu;mravanba@cs.cmu.edu;bapoczos@cs.cmu.edu,5;5;6,5;4;4,Reject,3,5,0,no,11/4/16,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1,23;23;23,5;10,11/4/16,2,1,0,0,3,0,1238;1045;5788,89;39;244,17;12;40,115;106;709,-1;-1
321,ICLR,2017,Out-of-class novelty generation: an experimental foundation,Mehdi Cherti;Balázs Kégl;Akın Kazakçı,mehdicherti@gmail.com;balazskegl@gmail.com;akin.kazakci@mines-paristech.fr,7;6;4;5,4;3;4;3,Reject,3,4,0,no,11/5/16,Linear accelerator Laboratory;;Mines ParisTech,-1;-1;462,-1;-1;265,5,11/5/16,7,3,0,1,0,1,26;5460;277,9;177;36,3;32;9,2;505;13,-1;-1
322,ICLR,2017,End-to-End Learnable Histogram Filters,Rico Jonschkowski;Oliver Brock,rico.jonschkowski@tu-berlin.de;oliver.brock@tu-berlin.de,4;4;3,3;3;3,Reject,3,4,0,no,11/5/16,TU Berlin;TU Berlin,105;105,82;82,,11/5/16,20,10,12,0,0,3,484;6856,30;281,11;44,49;390,-1;-1
323,ICLR,2017,Character-aware Attention Residual Network for Sentence Representation,Xin Zheng;Zhenzhou Wu,xzheng008@e.ntu.edu.sg;zhenzhou.wu@sap.com,4;4;4,4;4;5,Reject,3,3,0,no,11/5/16,National Taiwan University;SAP,87;351,195;257,,11/5/16,0,0,0,0,0,0,3952;708,369;52,32;12,215;46,-1;-1
324,ICLR,2017,ReasoNet: Learning to Stop Reading in Machine Comprehension,Yelong Shen;Po-Sen Huang;Jianfeng Gao;Weizhu Chen,yeshen@microsoft.com;pshuang@microsoft.com;jfgao@microsoft.com;wzchen@microsoft.com,5;5;6,3;3;5,Reject,4,2,0,no,11/4/16,Microsoft;Microsoft;Microsoft;Microsoft,-1;-1;-1;-1,-1;-1;-1;-1,10,9/17/16,230,104,68,2,0,19,2154;1751;18692;1802,50;59;351;62,16;17;61;19,217;246;2662;239,-1;-1
325,ICLR,2017,Implicit ReasoNet: Modeling Large-Scale Structured Relationships with Shared Memory,Yelong Shen*;Po-Sen Huang*;Ming-Wei Chang;Jianfeng Gao,yeshen@microsoft.com;pshuang@microsoft.com;minchang@microsoft.com;jfgao@microsoft.com,6;6;6,4;4;4,Reject,6,4,0,no,11/4/16,Microsoft;Microsoft;Microsoft;Microsoft,-1;-1;-1;-1,-1;-1;-1;-1,,11/4/16,14,4,3,0,24,2,2154;1751;10933;18692,50;59;90;351,16;17;30;61,217;246;2880;2662,-1;-1
326,ICLR,2017,Efficient iterative policy optimization,Nicolas Le Roux,nicolas@le-roux.name,3;7;5,2;3;4,Reject,1,3,0,no,11/4/16,Criteo,-1,-1,,11/4/16,4,1,4,0,15,0,4950,172,24,536,-1
327,ICLR,2017,A hybrid network: Scattering and Convnet,Edouard Oyallon,edouard.oyallon@ens.fr,7;5;7,4;4;3,Reject,5,4,0,no,11/4/16,Ecole Normale Superieure,94,66,,11/4/16,5,3,2,1,0,0,603,25,10,53,-1
328,ICLR,2017,Cortical-Inspired Open-Bigram Representation for Handwritten Word Recognition,Théodore Bluche;Christopher Kermorvant;Claude Touzet;Hervé Glotin,tb@a2ia.com;kermorvant@teklia.com;claude.touzet@univ-amu.fr;glotin@univ-tln.fr,5;4;7,5;4;5,Reject,2,3,0,no,11/4/16,A2iA SAS;TEKLIA;Aix Marseille Univ;CNRS university Toulon,-1;-1;462;462,-1;-1;313;981,,11/4/16,2,2,0,0,0,0,592;1114;325;1971,27;66;39;280,14;16;8;25,63;86;25;109,-1;-1
329,ICLR,2017,Learning Locomotion Skills Using DeepRL: Does the Choice of Action Space Matter?,Xue Bin Peng;Michiel van de Panne,xbpeng@cs.ubc.ca;van@cs.ubc.ca,6;6;6,4;4;3,Reject,2,3,0,no,11/2/16,University of British Columbia;University of British Columbia,34;34,36;36,,11/2/16,47,13,6,0,4,1,1100;5192,23;137,11;42,84;319,-1;-1
330,ICLR,2017,Learning to Discover Sparse Graphical Models,Eugene Belilovsky;Kyle Kastner;Gael Varoquaux;Matthew B. Blaschko,eugene.belilovsky@inria.fr;kyle.kastner@umontreal.ca;gael.varoquaux@inria.fr;matthew.blaschko@esat.kuleuven.be,6;7;5,3;3;2,Invite to Workshop Track,0,4,0,no,11/4/16,INRIA;University of Montreal;INRIA;KU Leuven,-1;113;-1;105,-1;103;-1;40,10,5/20/16,10,5,3,0,5,2,304;1125;30971;4192,30;21;161;148,10;10;34;28,32;152;2188;446,-1;-1
331,ICLR,2017,Efficient Communications in Training Large Scale Neural Networks,Linnan Wang;Wei Wu;George Bosilca;Richard Vuduc;Zenglin Xu,linnan.wang@gatech.edu;wwu12@vols.utk.edu;bosilca@icl.utk.edu;richie@cc.gatech.edu;zlxu@uestc.edu.cn,5;5,3;5,Reject,5,6,0,no,11/2/16,"Georgia Institute of Technology;University of Tennessee, Knoxville;University of Tennessee, Knoxville;Georgia Institute of Technology;University of Electronic Science and Technology of China",12;161;161;12;462,33;286;286;33;933,,11/2/16,12,5,3,0,3,0,231;3466;4099;5186;2070,21;464;163;160;138,8;27;28;33;24,20;131;481;428;143,-1;-1
332,ICLR,2017,Semantic Noise Modeling for Better Representation Learning,Hyo-Eun Kim;Sangheum Hwang;Kyunghyun Cho,hekim@lunit.io;shwang@lunit.io;kyunghyun.cho@nyu.edu,4;3;2,4;4;4,Reject,1,7,0,no,11/2/16,Lunit Inc.;Lunit Inc.;New York University,-1;-1;25,-1;-1;32,8,11/2/16,2,0,0,0,2,0,315;269;45940,56;21;273,11;8;52,23;19;6588,-1;-1
333,ICLR,2017,Attentive Recurrent Comparators,Pranav Shyam;Ambedkar Dukkipati,pranavm.cs13@rvce.edu.in;ad@csa.iisc.ernet.in,4;5;3,5;2;5,Reject,0,0,0,no,11/5/16,"R V College of Engineering;Indian Institute of Science Bangalore., Indian institute of science, Bangalore",462;461,981;247,11;8,11/5/16,55,28,12,2,61,7,137;474,6;118,4;12,23;40,-1;-1
334,ICLR,2017,Combating Deep Reinforcement Learning's Sisyphean Curse with Intrinsic Fear,Zachary C. Lipton;Jianfeng Gao;Lihong Li;Jianshu Chen;Li Deng,zlipton@cs.ucsd.edu;jfgao@microsoft.com;lihongli.cs@gmail.com;jianshuc@microsoft.com;deng@microsoft.com,5;4;4,2;3;4,Reject,2,5,0,no,11/3/16,"University of California, San Diego;Microsoft;Microsoft;Microsoft;Microsoft",9;-1;-1;-1;-1,41;-1;-1;-1;-1,,11/3/16,66,25,3,0,57,2,4822;18692;11396;2867;22151,97;351;269;107;437,28;61;47;26;63,435;2662;1213;229;1915,-1;-1
335,ICLR,2017,Shift Aggregate Extract Networks,Francesco Orsini;Daniele Baracchi;Paolo Frasconi,francesco.orsini@kuleuven.be;daniele.baracchi@unifi.it;paolo.frasconi@unifi.it,5;5;3,3;3;2,Invite to Workshop Track,1,3,0,no,11/4/16,KU Leuven;University of Florence;University of Florence,105;462;462,40;439;439,10,11/4/16,11,5,4,1,4,3,1370;11;9165,56;5;232,12;1;38,155;3;621,-1;-1
336,ICLR,2017,Unsupervised Pretraining for Sequence to Sequence Learning,Prajit Ramachandran;Peter J. Liu;Quoc V. Le,prajitram@gmail.com;peterjliu@google.com;qvl@google.com,6;7;5,4;5;5,Reject,3,4,0,no,11/4/16,"University of Illinois, Urbana Champaign;Google;Google",4;-1;-1,36;-1;-1,3;8,11/4/16,168,69,65,4,19,10,1072;2568;48310,14;24;193,8;12;81,137;500;6042,-1;-1
337,ICLR,2017,Revisiting Batch Normalization For Practical Domain Adaptation,Yanghao Li;Naiyan Wang;Jianping Shi;Jiaying Liu;Xiaodi Hou,lyttonhao@pku.edu.cn;winsty@gmail.com;shijianping5000@gmail.com;liujiaying@pku.edu.cn;xiaodi.hou@gmail.com,4;6;5,4;3;4,Reject,1,7,1,no,11/4/16,Peking University;;;Peking University;TUSIMPLE LLC,25;-1;-1;25;-1,29;-1;-1;29;-1,2;8,3/15/16,215,99,91,8,13,35,1065;6149;7348;2514;5023,24;44;73;194;17,9;23;30;24;13,132;860;1436;337;846,-1;-1
338,ICLR,2017,A Neural Knowledge Language Model,Sungjin Ahn;Heeyoul Choi;Tanel Parnamaa;Yoshua Bengio,sjn.ahn@gmail.com;heeyoul@gmail.com;tanel.parnamaa@gmail.com;yoshua.bengio@umontreal.ca,6;6;6,4;4;3,Reject,2,5,0,no,11/3/16,University of Montreal;Handong Global University;;University of Montreal,113;462;-1;113,103;981;-1;103,3;10,8/1/16,80,52,25,1,0,8,1368;429;692;205096,41;35;4;807,12;8;4;147,162;34;34;24136,-1;-1
339,ICLR,2017,Dynamic Neural Turing Machine with Continuous and Discrete Addressing Schemes,Caglar Gulcehre;Sarath Chandar;Kyunghyun Cho;Yoshua Bengio,gulcehrc@iro.umontreal.ca;apsarathchandar@gmail.com;kyunghyun.cho@nyu.edu;yoshua.umontreal@gmail.com,6;4;7,4;4;4,Reject,3,6,0,no,11/4/16,University of Montreal;University of Montreal;New York University;University of Montreal,113;113;25;113,103;103;32;103,,11/4/16,18,8,5,0,2,0,19558;1153;205096;45940,36;31;807;273,26;14;147;52,2991;114;24136;6588,-1;-1
340,ICLR,2017,Gated-Attention Readers for Text Comprehension,Bhuwan Dhingra;Hanxiao Liu;Zhilin Yang;William W. Cohen;Ruslan Salakhutdinov,bdhingra@cs.cmu.edu;hanxiaol@cs.cmu.edu;zhiliny@cs.cmu.edu;wcohen@cs.cmu.edu;rsalakhu@cs.cmu.edu,6;6;7,3;3,Reject,15,5,0,no,11/4/16,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1;1,23;23;23;23;23,,6/5/16,257,141,114,10,0,51,1208;1993;4995;22656;67997,36;35;90;423;254,15;12;26;68;82,169;518;798;2630;7815,-1;-1
341,ICLR,2017,Deep Character-Level Neural Machine Translation By Learning Morphology,Shenjian Zhao;Zhihua Zhang,sword.york@gmail.com;zhzhang@math.pku.edu.cn,5;6;7,5;4;4,Reject,2,9,0,no,11/4/16,Shanghai Jiao Tong University;Peking University,61;25,214;29,3,11/4/16,3,1,0,0,0,0,22;159,7;45,3;6,4;22,-1;-1
342,ICLR,2017,Automated Generation of Multilingual Clusters for the Evaluation of Distributed Representations,Philip Blair;Yuval Merhav;Joel Barry,pblair@basistech.com;yuval@basistech.com;joelb@basistech.com,5;8;6,3;4;3,Invite to Workshop Track,3,2,0,no,11/4/16,Northeastern University;Basistech;Basistech,18;-1;-1,778;-1;-1,3;10,11/4/16,11,3,2,0,10,0,79;76;21,7;12;8,3;5;3,2;6;2,-1;-1
343,ICLR,2017,Adding Gradient Noise Improves Learning for Very Deep Networks,Arvind Neelakantan;Luke Vilnis;Quoc V. Le;Lukasz Kaiser;Karol Kurach;Ilya Sutskever;James Martens,arvind@cs.umass.edu;luke@cs.umass.edu;qvl@google.com;lukaszkaiser@google.com;kkurach@google.com;ilyasu@openai.com;jmartens@cs.toronto.edu,4;4;7,5;4;5,Reject,1,4,0,no,11/4/16,"University of Massachusetts, Amherst;University of Massachusetts, Amherst;Google;Google;Google;OpenAI;Department of Computer Science, University of Toronto",25;25;-1;-1;-1;-1;15,166;166;-1;-1;-1;-1;22,,11/21/15,248,124,58,12,42,22,1483;1754;48310;22646;1181;132921;5662,24;25;193;75;28;90;39,14;10;81;24;11;53;18,162;294;6042;3882;135;17040;651,-1;-1
344,ICLR,2017,Knowledge Adaptation: Teaching to Adapt,Sebastian Ruder;Parsa Ghaffari;John G. Breslin,sebastian.ruder@insight-centre.org;parsa@aylien.com;john.breslin@insight-centre.org,6;7;5,4;3;4,Reject,2,6,0,no,11/3/16,Insight Centre for Data Analytics;Aylien;Insight Centre for Data Analytics,-1;-1;-1,-1;-1;-1,4,11/3/16,15,6,7,1,15,2,4732;283;3887,45;11;248,23;7;31,483;29;289,-1;-1
345,ICLR,2017,Neural Taylor Approximations: Convergence and Exploration in Rectifier Networks,David Balduzzi;Brian McWilliams;Tony Butler-Yeoman,david.balduzzi@vuw.ac.nz;brian@disneyresearch.com;butlertony@ecs.vuw.ac.nz,3;7;7,4;3;2,Invite to Workshop Track,1,3,0,no,11/4/16,"Victoria University Wellington;Disney Research, Disney;Victoria University Wellington",275;-1;275,362;-1;362,1;9,11/4/16,16,11,3,0,24,3,2487;692;40,61;9;5,21;6;3,356;233;4,-1;-1
346,ICLR,2017,On the Expressive Power of Deep Neural Networks,Maithra Raghu;Ben Poole;Jon Kleinberg;Surya Ganguli;Jascha Sohl-Dickstein,maithrar@gmail.com;benmpoole@gmail.com;kleinber@cs.cornell.edu;sganguli@stanford.edu;jaschasd@google.com,3;6;5,3;5;3,Reject,4,5,0,no,11/4/16,Cornell University;Stanford University;Cornell University;Stanford University;Google,7;3;7;3;-1,19;3;19;3;-1,,6/16/16,294,180,32,22,152,28,1104;4239;46889;6029;571,27;41;442;129;8,10;19;96;39;8,146;695;5165;603;40,-1;-1
347,ICLR,2017,The loss surface of residual networks: Ensembles and the role of batch normalization,Etai Littwin;Lior Wolf,etai.littwin@gmail.com;liorwolf@gmail.com,7;3;7,3;5;3,Reject,3,6,0,no,11/4/16,Tel Aviv University;Tel Aviv University,36;36,216;216,,11/4/16,9,6,0,0,0,0,38;13985,11;199,4;45,4;1634,-1;-1
348,ICLR,2017,Understanding trained CNNs by indexing neuron selectivity,Ivet Rafegas;Maria Vanrell;Luís A. Alexandre,ivet.rafegas@uab.cat;maria.vanrell@uab.cat;lfbaa@ubi.pt,7;3;7,4;5;3,Reject,3,4,0,no,11/4/16,Universitat Autonoma de Barcelona;Universitat Autonoma de Barcelona;Universidade da Beira Interior,-1;-1;-1,-1;-1;-1,,11/4/16,10,4,5,1,73,1,26;1300;2488,6;85;96,3;16;22,1;99;208,-1;-1
349,ICLR,2017,Riemannian Optimization for Skip-Gram Negative Sampling,Alexander Fonarev;Alexey Grinchuk;Gleb Gusev;Pavel Serdyukov;Ivan Oseledets,newo@newo.su;oleksii.hrinchuk@skolkovotech.ru;gleb57@yandex-team.ru;pavser@yandex-team.ru;ioseledets@skoltech.ru,4;5;6,4;3;3,Reject,0,4,0,no,11/4/16,Skolkovo Institute of Science and Technology;Skolkovo Institute of Science and Technology;Yandex;Yandex;Skolkovo Institute of Science and Technology,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,3,11/4/16,6,1,2,0,2,0,13;0;728;2897;4373,6;3;70;152;197,2;0;13;28;30,2;0;71;237;402,-1;-1
350,ICLR,2017,Exploring the Application of Deep Learning for Supervised Learning Problems,Jose Rozanec;Gilad Katz;Eui Chul Richard Shin;Dawn Song,jmrozanec@gmail.com;giladk@berkeley.edu;ricshin@berkeley.edu;dawnsong@eecs.berkeley.edu,4;3;5,3;5;4,Reject,2,2,0,no,11/5/16,;University of California Berkeley;University of California Berkeley;University of California Berkeley,-1;5;5;5,-1;10;10;10,6,11/5/16,1,0,0,0,0,0,18;304;933;43319,34;35;21;396,2;8;11;100,0;20;111;4323,-1;-1
351,ICLR,2017,A Way out of the Odyssey: Analyzing and Combining Recent Insights for LSTMs,Shayne Longpre;Sabeek Pradhan;Caiming Xiong;Richard Socher,slongpre@cs.stanford.edu;sabeekp@cs.stanford.edu;cxiong@salesforce.com;rsocher@salesforce.com,5;5;5,4;4;4,Reject,2,0,0,no,11/4/16,Stanford University;Stanford University;SalesForce.com;SalesForce.com,3;3;-1;-1,3;3;-1;-1,3,11/4/16,7,3,1,1,47,1,18;14;6299;53141,8;4;156;179,2;2;31;49,4;1;1059;8913,-1;-1
352,ICLR,2017,LipNet: End-to-End Sentence-level Lipreading,Yannis M. Assael;Brendan Shillingford;Shimon Whiteson;Nando de Freitas,yannis.assael@cs.ox.ac.uk;brendan.shillingford@cs.ox.ac.uk;shimon.whiteson@cs.ox.ac.uk;nando.de.freitas@cs.ox.ac.uk,4;6;4,4;3;4,Reject,6,14,1,no,11/4/16,University of Oxford;University of Oxford;University of Oxford;University of Oxford,50;50;50;50,1;1;1;1,,11/4/16,113,44,32,3,69,15,875;295;5373;19160,14;13;203;184,9;5;38;55,94;34;579;1851,-1;-1
353,ICLR,2017,Distributed Transfer Learning for Deep Convolutional Neural Networks by Basic Probability Assignment,Arash Shahriari,arash.shahriari@csiro.au,3;3;4,3;4;4,Reject,2,3,0,no,11/4/16,CSIRO,-1,-1,6,11/4/16,0,0,0,0,0,0,8,11,2,1,-1
354,ICLR,2017,DeepRebirth: A General Approach for Accelerating Deep Neural Network Execution on Mobile Devices,Dawei Li;Xiaolong Wang;Deguang Kong;Mooi Choo Chuah,dal312@lehigh.edu;visionxiaolong@gmail.com;doogkong@gmail.com,4;4;4,4;4;4,Reject,0,6,0,no,11/4/16,Lehigh University;Samsung;Yahoo,230;-1;-1,441;-1;-1,,11/4/16,2,1,1,0,0,0,454;121;987;1957,145;48;49;157,11;6;15;23,35;5;81;150,-1;-1
355,ICLR,2017,Modelling Relational Time Series using Gaussian Embeddings,Ludovic Dos Santos;Ali Ziat;Ludovic Denoyer;Benjamin Piwowarski;Patrick Gallinari,ludovic.dossantos@lip6.fr;ali.ziat@vedecom.fr;ludovic.denoyer@lip6.fr;benjamin.piwowarski@lip6.fr;patrick.gallinari@lip6.fr,4;4;4,3;5;4,Reject,2,0,0,no,11/3/16,LIP6;;LIP6;LIP6;LIP6,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,,11/3/16,0,0,0,0,0,0,35;33;161;1723;4824,20;6;21;105;450,3;3;7;20;33,2;4;8;186;377,-1;-1
356,ICLR,2017,Higher Order Recurrent Neural Networks,Rohollah Soltani;Hui Jiang,rsoltani@cse.yorku.ca;hj@cse.yorku.ca,4;6;3,4;4;4,Reject,3,0,0,no,11/5/16,York University;York University,161;161,316;316,3,4/30/16,33,11,14,3,43,9,37;835,6;112,2;14,9;41,-1;-1
357,ICLR,2017,Multi-task learning with deep model based reinforcement learning,Asier Mujika,asierm@student.ethz.ch,2;4;4,5;4;4,Reject,1,3,0,no,11/4/16,Swiss Federal Institute of Technology,9,9,,11/4/16,8,2,1,0,2,0,85,10,4,9,-1
358,ICLR,2017,Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units,Dan Hendrycks;Kevin Gimpel,dan@ttic.edu;kgimpel@ttic.edu,4;5;5,4;4;4,Reject,3,1,0,no,11/4/16,Toyota Technological Institute at Chicago;Toyota Technological Institute at Chicago,-1;-1,-1;-1,,6/27/16,146,30,42,0,36,14,1504;5717,27;99,14;31,286;825,-1;-1
359,ICLR,2017,Iterative Refinement for Machine Translation,Roman Novak;Michael Auli;David Grangier,roman.novak@polytechnique.edu;michaelauli@fb.com;grangier@fb.com,5;5;7;4,3;5;3;4,Reject,0,0,0,no,11/2/16,Ecole polytechnique;Facebook;Facebook,462;-1;-1,116;-1;-1,3,10/20/16,13,5,6,1,7,1,734;6880;5119,12;71;64,9;31;25,110;1025;757,-1;-1
360,ICLR,2017,Machine Solver for Physics Word Problems,Megan Leszczynski;Jose Moreira,mel255@cornell.edu;jmoreira@us.ibm.com,4;4;5,4;4;4,Reject,1,4,0,no,11/4/16,Cornell University;International Business Machines,7;-1,19;-1,,11/4/16,2,2,0,0,0,0,65;3758,7;204,2;32,4;251,-1;-1
361,ICLR,2017,b-GAN: Unified Framework of Generative Adversarial Networks,Masatosi Uehara;Issei Sato;Masahiro Suzuki;Kotaro Nakayama;Yutaka Matsuo,uehara-masatoshi136@g.ecc.u-tokyo.ac.jp;sato@k.u-tokyo.ac.jp;masa@weblab.t.u-tokyo.ac.jp;nakayama@weblab.t.u-tokyo.ac.jp;matsuo@weblab.t.u-tokyo.ac.jp,5;6;4,3;4;3,Reject,0,4,0,no,11/5/16,The University of Tokyo;The University of Tokyo;The University of Tokyo;The University of Tokyo;The University of Tokyo,50;50;50;50;50,39;39;39;39;39,5;4,11/5/16,3,1,0,0,0,0,16;1075;776;681;3573,4;113;316;71;492,2;17;12;13;28,0;107;78;89;194,-1;-1
362,ICLR,2017,Structured Sequence Modeling with Graph Convolutional Recurrent Networks,Youngjoo Seo;Michaël Defferrard;Pierre Vandergheynst;Xavier Bresson,youngjoo.seo@epfl.ch;michael.defferrard@epfl.ch;pierre.vandergheynst@epfl.ch;xavier.bresson@gmail.com,4;4;4,4;4;4,Reject,3,3,0,no,11/4/16,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;National Taiwan University,25;25;25;87,30;30;30;195,3;10;8,11/4/16,90,35,35,0,27,9,113;1927;16934;6435,15;16;409;105,2;4;54;34,10;312;1900;826,-1;-1
363,ICLR,2017,Revisiting Denoising Auto-Encoders,Luis Gonzalo Sanchez Giraldo,lgsanchez@cs.miami.edu,4;4;5,5;4;4,Reject,4,0,0,no,11/5/16,University of Miami,351,183,,11/5/16,1,1,0,0,0,0,163,29,7,4,-1
364,ICLR,2017,Revisiting Distributed Synchronous SGD,Jianmin Chen*;Xinghao Pan*;Rajat Monga;Samy Bengio;Rafal Jozefowicz,jmchen@google.com;xinghao@google.com;rajatmonga@google.com;bengio@google.com;rafal@openai.com,6;6;5,4;4;3,Reject,9,3,0,no,11/1/16,Google;Google;Google;Google;OpenAI,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,,4/4/16,392,211,123,17,48,61,8036;-1;19333;26525;11210,37;-1;18;332;30,12;-1;12;67;13,937;0;2191;3482;1351,-1;-1
365,ICLR,2017,Here's My Point: Argumentation Mining with Pointer Networks,Peter Potash;Alexey Romanov;Anna Rumshisky,ppotash@cs.uml.edu;aromanov@cs.uml.edu;arum@cs.uml.edu,4;5;5,3;4;4,Reject,2,5,0,no,11/4/16,"University of Massachusetts, Lowell;University of Massachusetts, Lowell;University of Massachusetts, Lowell",-1;-1;-1,-1;-1;-1,,11/4/16,25,16,11,4,8,4,154;302;1511,21;27;91,6;10;19,16;39;124,-1;-1
366,ICLR,2017,DyVEDeep: Dynamic Variable Effort Deep Neural Networks,Sanjay Ganapathy;Swagath Venkataramani;Balaraman Ravindran;Anand Raghunathan,sanjaygana@gmail.com;venkata0@purdue.edu;ravi@cse.iitm.ac.in;raghunathan@purdue.edu,6;7;6,3;3;4,Reject,1,3,0,no,11/4/16,;Purdue University;Indian Institute of Technology Madras;Purdue University,-1;25;140;25,-1;70;472;70,,11/4/16,3,1,1,0,12,0,13;1797;2503;10760,6;76;234;354,3;20;28;53,3;163;202;749,-1;-1
367,ICLR,2017,Improved Architectures for Computer Go,Tristan Cazenave,Tristan.Cazenave@dauphine.fr,3;7;4;3,4;5;4;4,Reject,2,0,0,no,11/3/16,Univeristé Paris-Dauphine,462,981,,11/3/16,0,0,0,0,0,0,1506,182,19,102,-1
368,ICLR,2017,Deep Variational Canonical Correlation Analysis,Weiran Wang;Xinchen Yan;Honglak Lee;Karen Livescu,weiranwang@ttic.edu;xcyan@umich.edu;honglak@umich.edu;klivescu@ttic.edu,5;5;7,4;4;4,Reject,3,4,0,no,11/4/16,Toyota Technological Institute at Chicago;University of Michigan;University of Michigan;Toyota Technological Institute at Chicago,-1;8;8;-1,-1;21;21;-1,5;1,10/11/16,65,31,34,0,3,13,1493;3156;24193;5361,63;17;166;128,18;10;61;35,174;467;2826;685,-1;-1
369,ICLR,2017,Charged Point Normalization: An Efficient Solution to the Saddle Point Problem,Armen Aghajanyan,armen.ag@live.com,5;4;4,4;4;3,Invite to Workshop Track,6,3,0,no,10/18/16,Dimensional Mechanics,-1,-1,9,9/29/16,5,0,0,0,5,0,42,7,3,2,-1
370,ICLR,2017,Multi-label learning with semantic embeddings,Liping Jing;MiaoMiao Cheng;Liu Yang;Alex Gittens;Michael W. Mahoney,lpjing@bjtu.edu.cn;15112085@bjtu.edu.cn;11112191@bjtu.edu.cn;gittens@icsi.berkeley.edu;mmahoney@stat.berkeley.edu,4;4;5,4;4;4,Reject,3,0,0,no,10/31/16,Beijing jiaotong univercity;Beijing jiaotong univercity;Beijing jiaotong univercity;University of California Berkeley;University of California Berkeley,462;462;462;5;5,981;981;981;10;10,,10/31/16,0,0,0,0,0,0,57;64;775;834;47,21;20;385;35;17,1;5;14;15;3,10;1;51;101;3,-1;-1
371,ICLR,2017,Emergent Predication Structure in Vector Representations of Neural Readers,Hai Wang;Takeshi Onishi;Kevin Gimpel;David McAllester,haiwang@ttic.edu;tonishi@ttic.edu;kgimpel@ttic.edu;mcallester@ttic.edu,6;6;5,5;4;3,Reject,3,2,0,no,11/4/16,Toyota Technological Institute at Chicago;Toyota Technological Institute at Chicago;Toyota Technological Institute at Chicago;Toyota Technological Institute at Chicago,-1;-1;-1;-1,-1;-1;-1;-1,,11/4/16,7,4,2,1,8,2,2179;124;5717;21076,156;23;99;173,21;4;31;54,169;29;825;3226,-1;-1
372,ICLR,2017,An Actor-critic Algorithm for Learning Rate Learning,Chang Xu;Tao Qin;Gang Wang;Tie-Yan Liu,changxu@nbjl.nankai.edu.cn;taoqin@microsoft.com;wgzwp@nbjl.nankai.edu.cn;tie-yan.liu@microsoft.com,3;5;4,5;4;4,Reject,3,0,0,no,11/3/16,Nankai University;Microsoft;Nankai University;Microsoft,462;-1;462;-1,905;-1;905;-1,,11/3/16,1,0,0,0,0,0,158;344;3563;13439,52;101;311;366,7;10;30;51,4;22;164;1721,-1;-1
373,ICLR,2017,Joint Multimodal Learning with Deep Generative Models,Masahiro Suzuki;Kotaro Nakayama;Yutaka Matsuo,masa@weblab.t.u-tokyo.ac.jp;k-nakayama@weblab.t.u-tokyo.ac.jp;matsuo@weblab.t.u-tokyo.ac.jp,5;3;5,5;4;3,Reject,4,0,0,no,11/4/16,The University of Tokyo;The University of Tokyo;The University of Tokyo,50;50;50,39;39;39,5,11/4/16,67,40,31,5,5,23,776;681;7578,316;71;382,12;13;34,78;89;512,-1;-1
374,ICLR,2017,Energy-Based Spherical Sparse Coding,Bailey Kong;Charless C. Fowlkes,bhkong@ics.uci.edu;fowlkes@ics.uci.edu,5;5;6,4;4;4,Reject,3,3,0,no,11/5/16,"University of California, Irvine;University of California, Irvine",36;36,99;99,,11/5/16,1,0,1,0,5,0,33;13682,9;163,2;44,2;1774,-1;-1
375,ICLR,2017,Improving Invariance and Equivariance Properties of Convolutional Neural Networks,Christopher Tensmeyer;Tony Martinez,tensmeyer@byu.edu;martinez@cs.byu.edu,4;5;4,4;3;5,Reject,3,1,0,no,11/5/16,Brigham Young University;Brigham Young University,-1;-1,-1;-1,,11/5/16,2,0,1,0,0,0,5;3829,7;199,2;25,0;340,-1;-1
376,ICLR,2017,Efficient Softmax Approximation for GPUs,Édouard Grave;Armand Joulin;Moustapha Cissé;David Grangier;Hervé Jégou,egrave@fb.com;ajoulin@fb.com;moustaphacisse@fb.com;grangier@fb.com;rvj@fb.com,7;6;7,3;5;4,Invite to Workshop Track,1,5,0,no,11/4/16,Facebook;Facebook;Facebook;Facebook;Facebook,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,3;10,9/14/16,143,59,83,3,0,23,7642;10538;2992;5119;13813,57;74;48;64;165,23;32;19;25;40,1135;1531;407;757;2378,-1;-1
377,ICLR,2017,The Preimage of Rectifier Network Activities,Stefan Carlsson;Hossein Azizpour;Ali Razavian,stefanc@kth.se;azizpour@kth.se;razavian@kth.se,4;4;4,4;5,Reject,2,1,0,no,11/4/16,"KTH Royal Institute of Technology, Stockholm, Sweden;KTH Royal Institute of Technology, Stockholm, Sweden;KTH Royal Institute of Technology, Stockholm, Sweden",129;129;129,160;160;160,,11/4/16,4,3,1,0,0,0,7990;4225;4039,307;30;10,39;11;7,508;295;292,-1;-1
378,ICLR,2017,Deep Convolutional Neural Network Design Patterns,Leslie N. Smith;Nicholay Topin,leslie.smith@nrl.navy.mil;ntopin@umbc.edu,3;3;4,4;4;3,Reject,2,1,0,no,11/4/16,US Naval Research Laboratory;Boston College,-1;275,-1;212,,11/2/16,32,17,7,0,104,2,1111;207,26;13,10;6,88;17,-1;-1
379,ICLR,2017,OMG: Orthogonal Method of Grouping With Application of K-Shot Learning,Haoqi Fan;Yu Zhang;Kris M. Kitani,haoqif@andrew.cmu.edu;kkitani@cs.cmu.edu,4;4;4,4;4;5,Reject,2,0,0,no,11/4/16,Carnegie Mellon University;Carnegie Mellon University,1;1,23;23,,11/4/16,0,0,0,0,0,0,0;381;903,3;103;50,0;9;15,0;26;74,-1;-1
380,ICLR,2017,Spatio-Temporal Abstractions in Reinforcement Learning Through Neural Encoding,Nir Baram;Tom Zahavy;Shie Mannor,nirb@campus.technion.ac.il;tomzahavy@campus.technion.ac.il;shie@ee.technion.ac.il,4;4;4,5;5;4,Reject,2,1,0,no,11/4/16,Technion;Technion;Technion,24;24;24,301;301;301,,11/4/16,3,2,1,0,0,1,320;4;11552,24;8;418,9;1;50,28;2;1224,-1;-1
381,ICLR,2017,Making Stochastic Neural Networks from Deterministic Ones,Kimin Lee;Jaehyung Kim;Song Chong;Jinwoo Shin,kiminlee@kaist.ac.kr;jaehyungkim@kaist.ac.kr;songchong@kaist.edu;jinwoos@kaist.ac.kr,5;6,5;4,Reject,7,3,0,no,11/4/16,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;KAIST;Korea Advanced Institute of Science and Technology,462;462;23;462,88;88;88;88,,11/4/16,0,0,0,0,0,0,491;65;5109;1712,26;44;200;185,8;4;30;18,105;4;483;223,-1;-1
382,ICLR,2017,Chess Game Concepts Emerge under Weak Supervision: A Case Study of Tic-tac-toe,Hao Zhao;Ming Lu;Anbang Yao;Yurong Chen;Li Zhang,zhao-h13@mails.tsinghua.edu.cn;lu-m13@mails.tsinghua.edu.cn;anbang.yao@intel.com;yurong.chen@intel.com;chinazhangli@mail.tsinghua.edu.cn,3;3;3,5;3;2,Reject,3,3,0,no,11/4/16,Tsinghua University;Tsinghua University;Intel;Intel;Tsinghua University,11;11;-1;-1;11,35;35;-1;-1;35,,11/4/16,0,0,0,0,0,0,28;2615;1853;3035;71,19;216;33;102;24,3;23;15;23;5,3;234;239;337;5,-1;-1
383,ICLR,2017,Generative Adversarial Parallelization,Daniel Jiwoong Im;He Ma;Chris Dongjoo Kim;Graham Taylor,daniel.im@aifounded.com;hma02@uoguelph.ca;ckim07@uoguelph.ca;gwtaylor@uoguelph.ca,4;4;4,3;4;4,Reject,2,4,0,no,11/5/16,Aifounded;University of Guelph;University of Guelph;University of Guelph,-1;275;275;275,-1;398;398;398,5;4;1,11/5/16,19,8,1,0,27,1,396;555;192;5883,23;79;7;143,9;12;5;31,39;30;19;498,-1;-1
384,ICLR,2017,Investigating Recurrence and Eligibility Traces in Deep Q-Networks,Jean Harb;Doina Precup,jharb@cs.mcgill.ca;dprecup@cs.mcgill.ca,4;4;3,4;5;5,Reject,3,0,0,no,11/5/16,McGill University;McGill University,80;80,42;42,,11/5/16,12,3,4,0,7,0,1085;10155,14;325,6;38,212;1107,-1;-1
385,ICLR,2017,Discovering objects and their relations from entangled scene representations,David Raposo;Adam Santoro;David Barrett;Razvan Pascanu;Timothy Lillicrap;Peter Battaglia,draposo@google.com;adamsantoro@google.com;barrettdavid@google.com;razp@google.com;countzero@google.com;peterbattaglia@google.com,7;3;7,4;5;4,Invite to Workshop Track,0,5,0,no,11/4/16,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,5,11/4/16,65,41,13,0,0,1,1550;3022;1159;16936;23840;4546,15;35;24;101;74;88,8;20;11;46;39;29,212;342;157;1688;2911;423,-1;-1
386,ICLR,2017,Generalizable Features From Unsupervised Learning,Mehdi Mirza;Aaron Courville;Yoshua Bengio,memirzamo@gmail.com;aaron.courville@gmail.com;yoshua.umontreal@gmail.com,5;5;3,4;4;4,Invite to Workshop Track,2,0,0,no,11/4/16,Google;University of Montreal;University of Montreal,-1;113;113,-1;103;103,8,11/4/16,13,8,2,0,10,1,28441;61116;205096,118;203;807,26;65;147,5088;7900;24136,-1;-1
387,ICLR,2017,RL^2: Fast Reinforcement Learning via Slow Reinforcement Learning,Yan Duan;John Schulman;Xi Chen;Peter L. Bartlett;Ilya Sutskever;Pieter Abbeel,rocky@openai.com;joschu@openai.com;peter@openai.com;peter@berkeley.edu;ilyasu@openai.com;pieter@openai.com,4;3;3,3;4;4,Reject,2,3,4,no,11/4/16,OpenAI;OpenAI;OpenAI;University of California Berkeley;OpenAI;OpenAI,-1;-1;-1;5;-1;-1,-1;-1;-1;10;-1;-1,,11/4/16,318,165,109,16,0,37,5544;15023;13677;17814;132921;36938,48;55;444;306;90;434,19;31;42;57;53;94,643;2492;1552;2359;17040;4451,-1;-1
388,ICLR,2017,Opening the vocabulary of  neural language models with character-level word representations,Matthieu Labeau;Alexandre Allauzen,labeau@limsi.fr;allauzen@limsi.fr,3;2;4,4;5;4,Reject,8,0,0,no,11/4/16,LIMSI-CNRS / Université Paris-Sud;LIMSI-CNRS / Université Paris-Sud,462;462,180;180,3,11/4/16,0,0,0,0,0,0,89;1149,17;106,4;17,9;69,-1;-1
389,ICLR,2017,Multi-modal Variational Encoder-Decoders,Iulian V. Serban;Alexander G. Ororbia II;Joelle Pineau;Aaron Courville,julianserban@gmail.com;ago109@psu.edu;jpineau@cs.mcgill.ca;aaron.courville@umontreal.ca,3;4;4,4;5;4,Reject,14,5,0,no,11/4/16,University College London;Pennsylvania State University;McGill University;University of Montreal,45;39;80;113,15;68;42;103,3;5;10,11/4/16,14,7,8,2,0,2,3385;609;11249;61116,29;59;267;203,14;15;45;65,471;41;1227;7900,-1;-1
390,ICLR,2017,Transformational Sparse Coding,Dimitrios C. Gklezakos;Rajesh P. N. Rao,gklezd@cs.washington.edu;rao@cs.washington.edu,5;4;4,4;4;4,Reject,7,3,0,no,11/4/16,University of Washington;University of Washington,6;6,25;25,,11/4/16,4,0,0,0,4,0,1;7141,4;245,1;43,0;430,-1;-1
391,ICLR,2017,Convolutional Neural Networks Generalization Utilizing the Data Graph Structure,Yotam Hechtlinger;Purvasha Chakravarti;Jining Qin,yhechtli@andrew.cmu.edu;pchakrav@andrew.cmu.edu;jiningq@andrew.cmu.edu,6;3;3,3;3,Reject,2,3,0,no,11/4/16,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1,23;23;23,10;8,11/4/16,2,1,1,0,0,0,96;34;36,8;6;3,4;2;2,7;3;4,-1;-1
392,ICLR,2017,Learning Approximate Distribution-Sensitive Data Structures,Zenna Tavares;Armando Solar-Lezama,zenna@mit.edu;asolar@csail.mit.edu,4;4;3,3;3;4,Reject,1,0,0,no,11/4/16,Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2,5;5,,11/4/16,0,0,0,0,0,0,9;3933,8;130,2;29,1;396,-1;-1
393,ICLR,2017,Sampling Generative Networks,Tom White,tom.white@vuw.ac.nz,5;5;6,4;3;3,Reject,6,3,0,no,11/2/16,Victoria University Wellington,275,362,5;4,9/14/16,98,39,34,0,696,9,3998,48,14,698,-1
394,ICLR,2017,Collaborative Deep Embedding via Dual Networks,Yilei Xiong;Dahua Lin;Haoying Niu;JIefeng Cheng;Zhenguo Li,xy014@ie.cuhk.edu.hk;dhlin@ie.cuhk.edu.hk;niu.haoying@huawei.com;cheng.jiefeng@huawei.com;li.zhenguo@huawei.com,5;5;4,3;4;4,Reject,2,3,0,no,11/4/16,The Chinese University of Hong Kong;The Chinese University of Hong Kong;Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.,61;61;-1;-1;-1,43;43;-1;-1;-1,,11/4/16,0,0,0,0,0,0,15;6119;0;897;86,4;145;5;40;23,1;37;0;15;5,5;1072;0;70;6,-1;-1
395,ICLR,2017,An Empirical Analysis of Deep Network Loss Surfaces,Daniel Jiwoong Im;Michael Tao;Kristin Branson,daniel.im@aifounded.com;mtao@dgp.toronto.edu;bransonk@janelia.hhmi.org,6;4;4,4;4;4,Reject,3,0,0,no,11/5/16,Aifounded;University of Toronto;HHMI Janelia Research Campus,-1;15;-1,-1;22;-1,,11/5/16,28,18,4,1,0,3,396;106;1706,23;14;41,9;6;19,39;8;140,-1;-1
396,ICLR,2017,Parametric Exponential Linear Unit for Deep Convolutional Neural Networks,Ludovic Trottier;Philippe Giguère;Brahim Chaib-draa,ludovic.trottier.1@ulaval.ca;philippe.giguere@ift.ulaval.ca;brahim.chaib-draa@ift.ulaval.ca,5;7;4;6,4;5;4;4,Reject,2,1,0,no,11/4/16,Laval university;Laval university;Laval university,462;462;462,265;265;265,,5/30/16,84,39,21,5,23,13,115;1611;2811,12;111;207,4;22;27,13;94;262,-1;-1
397,ICLR,2017,CAN AI GENERATE LOVE ADVICE?: TOWARD NEURAL ANSWER GENERATION FOR NON-FACTOID QUESTIONS,Makoto Nakatsuji;Hisashi Ito;Naruhiro Ikeda;Shota Sagara;Akihisa Fujita,nakatuji@nttr.co.jp;h-ito@nttr.co.jp;nikeda@nttr.co.jp;s-sagara@nttr.co.jp;akihisa@nttr.co.jp,4;4;4,4;4,Reject,1,0,0,no,11/5/16,;;;;,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,3,11/5/16,0,0,0,0,0,0,405;-1;-1;-1;-1,36;-1;-1;-1;-1,12;-1;-1;-1;-1,37;0;0;0;0,-1;-1
398,ICLR,2017,Multiagent System for Layer Free Network,Hiroki Kurotaki;Kotaro Nakayama;Yutaka Matsuo,kurotaki@weblab.t.u-tokyo.ac.jp;nakayama@weblab.t.u-tokyo.ac.jp;matsuo@weblab.t.u-tokyo.ac.jp,3;1;2,3;5;5,Reject,2,0,0,no,11/4/16,The University of Tokyo;The University of Tokyo;The University of Tokyo,50;50;50,39;39;39,,11/4/16,0,0,0,0,0,0,0;681;3573,1;71;492,0;13;28,0;89;194,-1;-1
399,ICLR,2017,Boosted Residual Networks,Alan Mosca;George D. Magoulas,a.mosca@dcs.bbk.ac.uk;gmagoulas@dcs.bbk.ac.uk,3;4;3,5;5;5,Reject,2,0,0,no,11/4/16,Birkbeck;Birkbeck,230;230,251;251,,11/4/16,1,0,1,0,0,0,42;3060,10;219,4;26,3;161,-1;-1
400,ICLR,2017,Multi-view Generative Adversarial Networks,Mickaël Chen;Ludovic Denoyer,mickael.chen@lip6.fr;ludovic.denoyer@lip6.fr,3;5;6,3;3;4,Reject,2,0,0,no,11/4/16,LIP6;LIP6,-1;-1,-1;-1,,11/4/16,22,11,6,0,4,3,46;3033,4;129,3;22,6;526,-1;-1
401,ICLR,2017,Modular Multitask Reinforcement Learning with Policy Sketches,Jacob Andreas;Dan Klein;Sergey Levine,jda@cs.berkeley.edu;klein@cs.berkeley.edu;svlevine@eecs.berkeley.edu,4;5;3,5;5;4,Invite to Workshop Track,2,5,0,no,11/4/16,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,10;10;10,,11/4/16,158,88,45,5,0,23,2498;19745;24658,42;233;310,24;65;73,333;2212;3204,-1;-1
402,ICLR,2017,Prototypical Networks for Few-shot Learning,Jake Snell;Kevin Swersky;Richard Zemel,jsnell@cs.toronto.edu;kswersky@twitter.com;zemel@cs.toronto.edu,5;6;4,3;4;5,Reject,1,3,0,no,11/5/16,"Department of Computer Science, University of Toronto;Twitter;Department of Computer Science, University of Toronto",15;-1;15,22;-1;22,6,11/5/16,1235,796,587,65,0,349,1511;5714;21678,12;52;208,6;23;52,383;873;2504,-1;-1
403,ICLR,2017,Towards Information-Seeking Agents,Philip Bachman;Alessandro Sordoni;Adam Trischler,phil.bachman@maluuba.com;alessandro.sordoni@maluuba.com;adam.trischler@maluuba.com,6;4;4,4;4;4,Reject,1,1,0,no,11/5/16,Maluuba;Maluuba;Maluuba,-1;-1;-1,-1;-1;-1,,11/5/16,11,9,1,0,34,1,1764;3858;1570,31;54;47,14;19;17,223;546;289,-1;-1
404,ICLR,2017,Beyond Bilingual: Multi-sense Word Embeddings using Multilingual Context,Shyam Upadhyay;Kai-Wei Chang;James Zou;Matt Taddy;Adam Kalai,upadhya3@illinois.edu;kwchang@virginia.edu;jamesz@stanford.edu;taddy@microsoft.com;adum@microsoft.com,4;4;5,3;4;3,Reject,2,3,0,no,11/4/16,"University of Illinois, Urbana Champaign;University of Virginia;Stanford University;Microsoft;Microsoft",4;67;3;-1;-1,36;123;3;-1;-1,3;11,11/4/16,18,10,3,0,7,1,486;10100;7117;1264;5113,29;84;40;53;126,12;25;18;20;33,52;1393;591;116;695,-1;-1
405,ICLR,2017,A Context-aware Attention Network for Interactive Question Answering,Huayu Li;Martin Renqiang Min;Yong Ge;Asim Kadav,hli38@uncc.edu;renqiang@nec-labs.com;yongge@email.arizona.edu;asim@nec-labs.com,5;4;4,3;4;4,Reject,3,3,0,no,11/4/16,"University of North Carolina, Charlotte;NEC-Labs;University of Arizona;NEC-Labs",73;-1;161;-1,981;-1;156;-1,,11/4/16,35,8,6,0,12,0,362;887;4013;1770,41;58;218;40,10;14;31;16,27;103;212;295,-1;-1
406,ICLR,2017,Sequence to Sequence Transduction with Hard Monotonic Attention,Roee Aharoni;Yoav Goldberg,roee.aharoni@gmail.com;yoav.goldberg@gmail.com,5;5;4,4;3;5,Reject,5,0,0,no,11/4/16,Bar Ilan University;Bar-Ilan University,87;87,489;489,,11/4/16,14,4,6,0,0,1,341;9577,11;116,8;40,42;1021,-1;-1
407,ICLR,2017,SoftTarget Regularization: An Effective Technique to Reduce Over-Fitting in Neural Networks,Armen Aghajanyan,armen.ag@live.com,3;4;4,5;5;5,Reject,7,2,0,no,10/18/16,Dimensional Mechanics,-1,-1,,9/21/16,8,3,4,0,41,0,42,7,3,2,-1
408,ICLR,2017,Parallel Stochastic Gradient Descent with Sound Combiners,Saeed Maleki;Madanlal Musuvathi;Todd Mytkowicz;Yufei Ding,saemal@microsoft.com;madanm@microsoft.com;toddm@microsoft.com;yding8@ncsu.edu,4;6;4,5;4;5,Reject,3,3,0,no,11/4/16,Microsoft;Microsoft;Microsoft;North Carolina State University,-1;-1;-1;87,-1;-1;-1;249,9,11/4/16,8,4,3,0,7,1,322;4156;1526;-1,47;99;75;-1,8;32;18;-1,37;487;111;0,-1;-1
409,ICLR,2017,Inefficiency of stochastic gradient descent with larger mini-batches (and more learners),Onkar Bhardwaj;Guojing Cong,onkar.bhardwaj@gmail.com;gcong@us.ibm.com,6;4;5,4;4;3,Reject,4,3,0,no,11/4/16,International Business Machines;International Business Machines,-1;-1,-1;-1,,11/4/16,0,0,0,0,0,0,147;880,19;82,5;14,24;63,-1;-1
410,ICLR,2017,The Power of Sparsity in Convolutional Neural Networks,Soravit Changpinyo;Mark Sandler;Andrey Zhmoginov,schangpi@usc.edu;sandler@google.com;azhmogin@google.com,7;5;4,3;4;4,Reject,6,3,0,no,11/4/16,University of Southern California;Google;Google,31;-1;-1,60;-1;-1,,11/4/16,67,29,19,2,0,2,778;3175;2652,13;67;46,7;15;10,139;758;670,-1;-1
411,ICLR,2017,The Variational Walkback Algorithm,Anirudh Goyal;Nan Rosemary Ke;Alex Lamb;Yoshua Bengio,anirudhgoyal9119@gmail.com;rosemary.nan.ke@gmail.com;lambalex@iro.umontreal.ca;yoshua.umontreal@gmail.com,4;5;4,4;5;5,Reject,2,1,0,no,11/4/16,University of Montreal;Polytechnique Montreal;University of Montreal;University of Montreal,113;351;113;113,103;981;103;103,5;1;10,11/4/16,3,1,1,0,0,0,1125;760;1168;205096,46;32;21;807,12;13;9;147,127;76;181;24136,-1;-1
412,ICLR,2017,Incremental Sequence Learning,Edwin D. de Jong,edwin.webmail@gmail.com,5;3;5,3;4;4,Reject,6,4,0,no,11/4/16,Utrecht University,275,86,5;6;8,11/4/16,4,1,3,1,67,2,2074,135,25,158,-1
413,ICLR,2017,Learning to Draw Samples: With Application to Amortized MLE for Generative Adversarial Learning,Dilin Wang;Qiang Liu,dilin.wang.gr@dartmouth.edu;qiang.liu@dartmouth.edu,4;4;4,3;4;3,Invite to Workshop Track,2,1,0,no,11/4/16,Dartmouth College;Dartmouth College,140;140,82;82,4,11/4/16,78,16,28,2,3,8,622;2904,25;190,11;28,115;323,-1;-1
414,ICLR,2017,Gaussian Attention Model and Its Application to Knowledge Base Embedding and Question Answering,Liwen Zhang;John Winn;Ryota Tomioka,liwenz@cs.uchicago.edu;jwinn@microsoft.com;ryoto@microsoft.com,5;4;4,4;4;3,Reject,2,1,0,no,11/4/16,University of Chicago;Microsoft;Microsoft,45;-1;-1,10;-1;-1,,11/4/16,5,1,2,0,10,0,2219;16856;4837,152;82;86,25;29;29,124;2142;658,-1;-1
415,ICLR,2017,Recurrent Inference Machines for Solving Inverse Problems,Patrick Putzky;Max Welling,patrick.putzky@gmail.com;welling.max@gmail.com,5;4;7,4;4;3,Reject,2,3,0,no,11/4/16,University of Amsterdam;University of California - Irvine,161;36,63;99,,11/4/16,49,20,19,0,28,1,108;26852,9;270,7;59,2;5126,-1;-1
416,ICLR,2017,Learning a Static Analyzer: A Case Study on a Toy Language,Manzil Zaheer;Jean-Baptiste Tristan;Michael L. Wick;Guy L. Steele Jr.,manzil.zaheer@cmu.edu;jean.baptiste.tristan@oracle.com;michael.wick@oracle.com;guy.steele@oracle.com,4;3;3,4;4;5,Reject,5,0,0,no,11/3/16,Carnegie Mellon University;Oracle;Oracle;Oracle,1;-1;-1;-1,23;-1;-1;-1,3,11/3/16,2,1,0,0,0,0,1583;462;820;9953,63;37;39;201,17;10;15;41,261;45;77;1372,-1;-1
417,ICLR,2017,The Incredible Shrinking Neural Network: New Perspectives on Learning Representations Through The Lens of Pruning,Nikolas Wolfe;Aditya Sharma;Lukas Drude;Bhiksha Raj,nwolfe@cs.cmu.edu;adityasharma@cmu.edu;drude@nt.upb.de;bhiksha@cs.cmu.edu,3;3;3,4;4;4,Reject,7,1,0,no,11/4/16,Carnegie Mellon University;Carnegie Mellon University;;Carnegie Mellon University,1;1;-1;1,23;23;-1;23,8,11/4/16,14,4,8,1,14,0,54;-1;795;8081,13;-1;41;315,3;-1;14;44,3;0;87;786,-1;-1
418,ICLR,2017,Neural Machine Translation with Latent Semantic of Image and Text,Joji Toyama;Masanori Misono;Masahiro Suzuki;Kotaro Nakayama;Yutaka Matsuo,toyama@weblab.t.u-tokyo.ac.jp;misono@weblab.t.u-tokyo.ac.jp;masa@weblab.t.u-tokyo.ac.jp;k-nakayama@weblab.t.u-tokyo.ac.jp;matsuo@weblab.t.u-tokyo.ac.jp,3;4;3,4;5;5,Reject,7,0,0,no,11/4/16,The University of Tokyo;The University of Tokyo;The University of Tokyo;The University of Tokyo;The University of Tokyo,50;50;50;50;50,39;39;39;39;39,3,11/4/16,8,5,5,0,4,3,21;8;776;681;7578,10;5;316;71;382,3;1;12;13;34,5;3;78;89;512,-1;-1
419,ICLR,2017,CONTENT2VEC: SPECIALIZING JOINT REPRESENTATIONS OF PRODUCT IMAGES AND TEXT FOR THE TASK OF PRODUCT RECOMMENDATION,Thomas Nedelec;Elena Smirnova;Flavian Vasile,t.nedelec@criteo.com;e.smirnova@criteo.com;f.vasile@criteo.com,3;3;5,3;3;3,Reject,1,1,0,no,11/5/16,Criteo;Criteo;Criteo,-1;-1;-1,-1;-1;-1,,11/5/16,6,0,3,0,0,0,96;258;302,14;80;28,5;9;7,3;7;24,-1;-1
420,ICLR,2017,Group Sparse CNNs for Question Sentence Classification with Answer Sets,Mingbo Ma;Liang Huang;Bing Xiang;Bowen Zhou,mam@oregonstate.edu;liang.huang@oregonstate.edu;bingxia@us.ibm.com;zhou@us.ibm.com,4;5;6,4;4;4,Reject,1,0,0,no,11/4/16,Oregon State University;Oregon State University;International Business Machines;International Business Machines,75;75;-1;-1,316;316;-1;-1,,11/4/16,7,4,1,0,7,0,362;144;5545;6452,36;38;105;187,11;6;30;31,43;16;717;885,-1;-1
421,ICLR,2017,Multi-label learning with the RNNs for Fashion Search,Taewan Kim,taey.16@navercorp.com,4;3;3,4;4;3,Reject,3,0,0,no,11/5/16,NAVER,-1,-1,2,11/5/16,1,0,1,0,0,0,6545,948,37,334,-1
422,ICLR,2017,A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks,Kazuma Hashimoto;Caiming Xiong;Yoshimasa Tsuruoka;Richard Socher,hassy@logos.t.u-tokyo.ac.jp;cxiong@salesforce.com;tsuruoka@logos.t.u-tokyo.ac.jp;rsocher@salesforce.com,6;3;5,4;4;4,Reject,3,3,0,no,11/4/16,The University of Tokyo;SalesForce.com;The University of Tokyo;SalesForce.com,50;-1;50;-1,39;-1;39;-1,3,11/4/16,288,148,96,6,0,32,847;6299;3789;53141,49;156;129;180,11;31;32;49,95;1059;334;8913,-1;-1
423,ICLR,2017,An Analysis of Deep Neural Network Models for Practical Applications,Alfredo Canziani;Adam Paszke;Eugenio Culurciello,canziani@purdue.edu;a.paszke@students.mimuw.edu.pl;euge@purdue.edu,4;4;5,3;3;4,Reject,3,5,0,no,11/4/16,"Purdue University;University of Washington, Seattle;Purdue University",25;6;25,70;25;70,1;2,5/24/16,493,242,102,11,0,29,551;7958;4719,11;7;163,5;5;33,33;1035;429,-1;-1
424,ICLR,2017,Improving Stochastic Gradient Descent with Feedback,Jayanth Koushik;Hiroaki Hayashi,jkoushik@cs.cmu.edu;hiroakih@cs.cmu.edu,5;6;5,4;4;4,Reject,5,2,0,no,11/4/16,Carnegie Mellon University;Carnegie Mellon University,1;1,23;23,3,11/4/16,23,4,10,0,239,3,148;208,11;56,5;7,11;9,-1;-1
425,ICLR,2017,Understanding intermediate layers using linear classifier probes,Guillaume Alain;Yoshua Bengio,guillaume.alain.umontreal@gmail.com;yoshua.bengio@gmail.com,5;4;4,3;4;4,Reject,2,2,0,no,11/5/16,University of Montreal;,113;-1,103;-1,,10/5/16,167,84,47,3,133,12,3012;205096,21;807,12;147,246;24136,-1;-1
426,ICLR,2017,Classify or Select: Neural Architectures for Extractive Document Summarization,Ramesh Nallapati;Bowen Zhou;Mingbo Ma,nallapati@us.ibm.com;zhou@us.ibm.com;mam@oregonstate.edu,6;4;4,4;4;4,Reject,7,3,0,no,11/4/16,International Business Machines;International Business Machines;Oregon State University,-1;-1;75,-1;-1;316,,11/4/16,45,18,17,2,4,9,4863;6452;362,69;187;36,20;31;11,734;885;43,-1;-1
427,ICLR,2017,Joint Training of Ratings and Reviews with Recurrent Recommender Networks,Chao-Yuan Wu;Amr Ahmed;Alex Beutel;Alexander J. Smola,cywu@cs.utexas.edu;amra@google.com;alexbeutel@google.com;alex@smola.org,6;6;6,4;3;4,Reject,4,6,0,no,11/4/16,"University of Texas, Austin;Google;Google;Carnegie-Mellon University",20;-1;-1;1,50;-1;-1;23,3,11/4/16,13,6,3,0,0,1,585;4890;2430;64953,18;148;59;389,9;28;26;99,64;526;262;9074,-1;-1
428,ICLR,2017,Divide and Conquer with Neural Networks,Alex Nowak;Joan Bruna,anv273@nyu.edu;bruna@cims.nyu.edu,4;4;3,2;4;2,Reject,4,1,0,no,11/4/16,New York University;New York University,25;25,32;32,10;8,11/4/16,1,0,1,0,5,0,67;11503,10;90,4;29,3;1285,-1;-1
429,ICLR,2017,Is a picture worth a thousand words? A Deep Multi-Modal Fusion Architecture for Product Classification in e-commerce,Tom Zahavy;Alessandro Magnani;Abhinandan Krishnan;Shie Mannor,tomzahavy@tx.technion.ac.il;AMagnani@walmartlabs.com;AKrishnan@walmartlabs.com;shie@ee.technion.ac.il,5;5;4,4;4;4,Reject,2,2,0,no,11/4/16,Technion;Walmartlabs;Walmartlabs;Technion,24;-1;-1;24,301;-1;-1;301,,11/4/16,21,12,10,2,8,6,491;1060;28;11552,31;80;5;418,10;16;2;50,30;67;6;1224,-1;-1
430,ICLR,2017,Deep Symbolic Representation Learning for Heterogeneous Time-series Classification,Shengdong Zhang;Soheil Bahrampour;Naveen Ramakrishnan;Mohak Shah,zhangshengdongofgz@gmail.com;Soheil.Bahrampour@us.bosch.com;Naveen.Ramakrishnan@us.bosch.com;mohak@mohakshah.com,3;5;4,4;4;3,Reject,4,2,0,no,11/4/16,Simon Fraser University;Bosch;Bosch;Mohakshah,-1;-1;-1;-1,-1;433;433;-1,,11/4/16,13,0,0,0,13,0,114;413;327;1112,41;27;30;59,6;8;6;11,8;29;28;94,-1;-1
431,ICLR,2017,Enforcing constraints on outputs with unconstrained inference,Jay Yoon Lee;Michael L. Wick;Jean-Baptiste Tristan,lee.jayyoon@gmail.com;michael.wick@oracle.com;jean.baptiste.tristan@oracle.com,4;3;3,5;4;4,Reject,2,0,0,no,11/3/16,Carnegie Mellon University;Oracle;Oracle,1;-1;-1,23;-1;-1,3,11/3/16,3,1,0,0,2,0,145;295;462,13;27;37,5;7;10,11;21;45,-1;-1
432,ICLR,2017,Sequence generation with a physiologically plausible model of handwriting and Recurrent Mixture Density Networks,Daniel Berio;Memo Akten;Frederic Fol Leymarie;Mick Grierson;Réjean Plamondon,d.berio@gold.ac.uk;m.akten@ac.uk;ffl@gold.ac.uk;m.grierson@gold.ac.uk;rejean.plamondon@polymtl.ca,3;3;3,3;5;3,Reject,3,1,0,no,11/5/16,;;;;Polytechnique Montreal,-1;-1;-1;-1;351,-1;-1;-1;-1;981,,11/5/16,2,1,0,0,0,0,57;26;1479;262;6950,13;9;103;62;156,4;2;15;8;36,0;1;59;20;385,-1;-1
433,ICLR,2017,Deep Neural Networks and the Tree of Life,Yan Wang;Kun He;John E. Hopcroft;Yu Sun,yanwang@hust.edu.cn;brooklet60@hust.edu.cn;jeh@cs.cornell.edu;ys646@cornell.edu,3;4;4,5;4;4,Reject,0,3,0,no,11/5/16,Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Cornell University;Cornell University,39;39;7;7,48;48;19;19,2,11/5/16,0,0,0,0,0,0,915;662;28685;5343,170;39;303;507,14;16;60;33,52;61;2732;335,-1;-1
434,ICLR,2017,Hierarchical Memory Networks,Sarath Chandar;Sungjin Ahn;Hugo Larochelle;Pascal Vincent;Gerald Tesauro;Yoshua Bengio,apsarathchandar@gmail.com;sjn.ahn@gmail.com;hugo@twitter.com;vincentp@iro.umontreal.ca;gtesauro@us.ibm.com;yoshua.bengio@umontreal.ca,5;5;4,3;5;4,Reject,3,3,0,no,11/4/16,University of Montreal;University of Montreal;Twitter;University of Montreal;International Business Machines;University of Montreal,113;113;-1;113;-1;113,103;103;-1;103;-1;103,,5/24/16,48,27,17,0,35,2,1153;1368;24882;15655;7589;205096,31;41;123;118;124;807,14;12;44;33;45;147,114;162;2864;1337;716;24136,-1;-1
435,ICLR,2017,Near-Data Processing for Machine Learning,Hyeokjun Choe;Seil Lee;Hyunha Nam;Seongsik Park;Seijoon Kim;Eui-Young Chung;Sungroh Yoon,genesis1104@snu.ac.kr;lees231@dsl.snu.ac.kr;godqhr825@snu.ac.kr;pss015@snu.ac.kr;hokiespa@snu.ac.kr;eychung@yonsei.ac.kr;sryoon@snu.ac.kr,4;6;5,4;2;2,Reject,2,3,0,no,11/5/16,Seoul National University;Seoul National University;Seoul National University;Seoul National University;Seoul National University;Yonsei University;Seoul National University,50;50;50;50;50;462;50,72;72;72;72;72;293;72,,11/5/16,9,3,2,0,18,2,26;49;3;83;32;1019;2842,6;21;2;26;6;100;238,3;4;1;6;3;16;28,5;3;0;9;4;63;176,-1;-1
436,ICLR,2017,Rotation Plane Doubly Orthogonal Recurrent Neural Networks,Zoe McCarthy;Andrew Bai;Xi Chen;Pieter Abbeel,zmccarthy@berkeley.edu;xiaoyang.bai@berkeley.edu;c.xi@berkeley.edu;pabbeel@berkeley.edu,4;4;5,4;4;3,Reject,0,0,0,no,11/5/16,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5,10;10;10;10,,11/5/16,0,0,0,0,0,0,468;4;55866;36938,20;6;2967;434,11;1;95;94,28;0;3875;4451,-1;-1
437,ICLR,2017,FastText.zip: Compressing text classification models,Armand Joulin;Edouard Grave;Piotr Bojanowski;Matthijs Douze;Herve Jegou;Tomas Mikolov,ajoulin@fb.com;egrave@fb.com;bojanowski@fb.com;matthijs@fb.com;rvj@fb.com;tmikolov@fb.com,5;6;6,4;4;3,Reject,2,2,0,no,11/4/16,Facebook;Facebook;Facebook;Facebook;Facebook;Facebook,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,3,11/4/16,266,54,141,4,0,33,10538;7642;7672;10392;13813;-1,74;57;41;99;165;-1,32;23;20;31;40;-1,1531;1135;1123;1802;2378;0,-1;-1
438,ICLR,2017,Symmetry-Breaking Convergence Analysis of Certain Two-layered Neural Networks with ReLU nonlinearity,Yuandong Tian,yuandong@fb.com,4;8;4,3;4;4,Invite to Workshop Track,1,3,0,no,11/4/16,Facebook,-1,-1,1;9,11/4/16,28,15,4,1,0,3,2473,85,25,283,-1
439,ICLR,2017,Training Long Short-Term Memory With Sparsified Stochastic Gradient Descent,Maohua Zhu;Minsoo Rhu;Jason Clemons;Stephen W. Keckler;Yuan Xie,maohuazhu@ece.ucsb.edu;mrhu@nvidia.com;jclemons@nvidia.com;skeckler@nvidia.com;yuanxie@ece.ucsb.edu,4;5;4,4;3;4,Reject,2,0,0,no,11/4/16,UC Santa Barbara;NVIDIA;NVIDIA;NVIDIA;UC Santa Barbara,39;-1;-1;-1;39,48;-1;-1;-1;48,,11/4/16,2,1,1,0,0,0,107;1050;460;10134;1005,14;31;25;221;62,7;13;11;48;16,9;140;56;1185;89,-1;-1
440,ICLR,2017,Unsupervised Learning Using Generative Adversarial Training And Clustering,Vittal Premachandran;Alan L. Yuille,vittalp@jhu.edu;ayuille1@jhu.edu,3;3;3,4;5;4,Reject,0,0,0,no,11/5/16,Johns Hopkins University;Johns Hopkins University,67;67,17;17,5;4,11/5/16,14,5,3,1,0,0,220;33029,22;495,9;80,21;3745,-1;-1
441,ICLR,2017,Inverse Problems in Computer Vision using  Adversarial  Imagination Priors,Hsiao-Yu Fish Tung;Katerina Fragkiadaki,htung@cs.cmu.edu;katef@cs.cmu.edu,3;5;6,4;3;3,Reject,0,0,0,no,11/4/16,Carnegie Mellon University;Carnegie Mellon University,1;1,23;23,4;2;10,11/4/16,3,0,1,0,0,0,445;1934,25;55,8;15,44;202,-1;-1
442,ICLR,2017,Generative Adversarial Networks as Variational Training of Energy Based Models,Shuangfei Zhai;Yu Cheng;Rogerio Feris;Zhongfei Zhang,szhai2@binghamton.edu;chengyu@us.ibm.com;rsferis@us.ibm.com;zhongfei@cs.binghamton.edu,4;4;4,3;5;5,Reject,4,5,0,no,11/5/16,"State University of New York, Binghamton;International Business Machines;International Business Machines;State University of New York, Binghamton",140;-1;-1;140,394;-1;-1;394,5;4;1,11/5/16,17,7,6,0,2,1,576;2428;5144;4826,24;173;163;261,10;24;35;34,56;291;492;346,-1;-1
443,ICLR,2017,An Information Retrieval Approach for Finding Dependent Subspaces of Multiple Views,Ziyuan Lin;Jaakko Peltonen,ziyuan.lin@aalto.fi;jaakko.peltonen@uta.fi,4;4;4,4;4;4,Reject,2,0,0,no,11/5/16,Aalto University;University of Tampere,161;462,228;296,,11/19/15,1,1,0,0,4,0,371;1429,20;116,7;19,3;118,-1;-1
444,ICLR,2017,Differentiable Canonical Correlation Analysis,Matthias Dorfer;Jan Schlüter;Gerhard Widmer,matthias.dorfer@jku.at;jan.schlueter@ofai.at;gerhard.widmer@jku.at,3;4;3,4;4;4,Reject,1,1,0,no,11/5/16,Johannes Kepler University Linz;Austrian Research Institute for Artificial Intelligence;Johannes Kepler University Linz,462;-1;462,498;-1;498,,11/5/16,0,0,0,0,0,0,608;829;8960,41;48;376,15;14;45,76;94;747,-1;-1
445,ICLR,2017,Generative Adversarial Networks for Image Steganography,Denis Volkhonskiy;Boris Borisenko;Evgeny Burnaev,dvolkhonskiy@gmail.com;bborisenko@hse.ru;e.burnaev@skoltech.ru,5;6;4,3;4;3,Reject,2,3,0,no,11/4/16,Skolkovo Institute of Science and Technology;Higher School of Economics;Skolkovo Institute of Science and Technology,-1;462;-1,-1;456;-1,5;4,11/4/16,10,2,2,0,0,0,73;48;233,9;5;37,4;2;10,5;4;6,-1;-1
446,ICLR,2017,A Deep Learning Approach for Joint Video Frame and Reward Prediction in Atari Games,Felix Leibfried;Nate Kushman;Katja Hofmann,felix.leibfried@gmail.com;nkushman@microsoft.com;katja.hofmann@microsoft.com,4;4;4,4;4;4,Reject,2,1,0,no,11/3/16,Max-Planck Institute;Microsoft;Microsoft,-1;-1;-1,-1;-1;-1,,11/3/16,24,10,6,0,25,1,175;1511;1788,20;29;112,7;17;24,7;202;150,-1;-1
447,ICLR,2017,Wav2Letter: an End-to-End ConvNet-based Speech Recognition System,Ronan Collobert;Christian Puhrsch;Gabriel Synnaeve,locronan@fb.com;cpuhrsch@fb.com;gab@fb.com,6;7,4;5,Reject,7,1,0,no,11/4/16,Facebook;Facebook;Facebook,-1;-1;-1,-1;-1;-1,2;10,9/11/16,135,51,58,8,0,17,19909;2038;1501,118;7;62,39;5;19,1921;368;154,-1;-1
448,ICLR,2017,Tree-Structured Variational Autoencoder,Richard Shin;Alexander A. Alemi;Geoffrey Irving;Oriol Vinyals,ricshin@cs.berkeley.edu;alemi@google.com;geoffreyi@google.com;vinyals@google.com,3;3;4,4;4;4,Reject,0,0,0,no,11/4/16,University of California Berkeley;Google;Google;Google,5;-1;-1;-1,10;-1;-1;-1,3;5;1,11/4/16,2,2,0,0,0,0,279;1290;15455;52829,29;53;33;121,9;14;18;55,27;186;1791;6560,-1;-1
449,ICLR,2017,Transformation-based Models of Video Sequences,Joost van Amersfoort;Anitha Kannan;Marc'Aurelio Ranzato;Arthur Szlam;Du Tran;Soumith Chintala,joost@joo.st;akannan@fb.com;ranzato@fb.com;aszlam@fb.com;trandu@fb.com;soumith@fb.com,5;6;3,3;4;3,Reject,6,0,0,no,11/4/16,Twitter;Facebook;Facebook;Facebook;Facebook;Facebook,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,,11/4/16,36,25,9,1,7,4,197;1723;20519;8769;4853;18902,7;66;75;87;42;31,4;20;43;32;16;18,21;186;2079;923;1006;2820,-1;-1
450,ICLR,2017,Nonparametrically Learning Activation Functions in Deep Neural Nets,Carson Eisenach;Zhaoran Wang;Han Liu,eisenach@princeton.edu;zhaoran@princeton.edu;hanliu@princeton.edu,7;6;5,5;4;4,Invite to Workshop Track,1,3,0,no,11/4/16,Princeton University;Princeton University;Princeton University,32;32;32,7;7;7,8,11/4/16,14,5,4,0,0,2,15;1178;5685,8;78;446,2;19;35,0;132;417,-1;-1
451,ICLR,2017,Leveraging Asynchronicity in Gradient Descent for Scalable Deep Learning,Jeff Daily;Abhinav Vishnu;Charles Siegel,jeff.daily@pnnl.gov;abhinav.vishnu@pnnl.gov;charles.siegel@pnnl.gov,5;3;3,4;4;5,Reject,0,1,0,no,11/4/16,Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory,-1;-1;-1,-1;-1;-1,,11/4/16,0,0,0,0,0,0,52;1468;311,12;99;44,4;21;10,1;66;20,-1;-1
452,ICLR,2017,Rule Mining in Feature Space,Stefano Teso;Andrea Passerini,teso@disi.unitn.it;passerini@disi.unitn.it,4;3;4,4;4;4,Reject,0,0,0,no,11/4/16,University of Trento;University of Trento,15;15,236;236,,11/4/16,0,0,0,0,0,0,191;1367,36;116,8;17,8;96,-1;-1
453,ICLR,2017,ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation,Adam Paszke;Abhishek Chaurasia;Sangpil Kim;Eugenio Culurciello,a.paszke@students.mimuw.edu.pl;aabhish@purdue.edu;sangpilkim@purdue.edu;euge@purdue.edu,4;4;5;3,4;4;4;4,Reject,9,0,0,no,11/4/16,"University of Washington, Seattle;Purdue University;Purdue University;Purdue University",6;25;25;25,25;70;70;70,2,6/7/16,658,266,301,18,480,150,7958;913;748;4719,7;8;22;163,5;5;6;33,1035;189;159;429,-1;-1
454,ICLR,2017,Efficient Calculation of Polynomial Features on Sparse Matrices,Andrew Nystrom;John Hughes,awnystrom@gmail.com;jfh@cs.brown.edu,3;3;3,3;3;1,Reject,0,1,0,no,11/4/16,Google;Brown University,-1;61,-1;51,,11/4/16,0,0,0,0,0,0,70;7352,27;302,3;42,3;466,-1;-1
455,ICLR,2017,Learning Word-Like Units from Joint Audio-Visual Analylsis,David Harwath;James R. Glass,dharwath@mit.edu;glass@mit.edu,6;5;5,4;5;4,Reject,1,1,0,no,11/4/16,Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2,5;5,,11/4/16,60,32,14,0,16,5,246;10822,16;342,6;55,15;970,-1;-1
456,ICLR,2017,Rethinking Numerical Representations for Deep Neural Networks,Parker Hill;Babak Zamirai;Shengshuo Lu;Yu-Wei Chao;Michael Laurenzano;Mehrzad Samadi;Marios Papaefthymiou;Scott Mahlke;Thomas Wenisch;Jia Deng;Lingjia Tang;Jason Mars,parkerhh@umich.edu;zamirai@umich.edu;luss@umich.edu;ywchao@umich.edu;mlaurenz@umich.edu;mehrzads@umich.edu;marios@umich.edu;mahlke@umich.edu;twenisch@umich.edu;jiadeng@umich.edu;lingjia@umich.edu;profmars@umich.edu,6;5;5,3;5;2,Reject,2,3,0,no,11/4/16,University of Michigan;University of Michigan;University of Michigan;University of Michigan;University of Michigan;University of Michigan;University of Michigan;University of Michigan;University of Michigan;University of Michigan;University of Michigan;University of Michigan,8;8;8;8;8;8;8;8;8;8;8;8,21;21;21;21;21;21;21;21;21;21;21;21,,11/4/16,6,4,0,0,17,0,255;142;62;896;1553;1038;2060;10392;6582;14608;2696;2906,19;6;8;23;66;29;130;275;167;81;80;80,6;4;4;12;22;16;26;57;41;28;28;29,20;12;4;129;127;114;169;995;658;2504;224;239,-1;-1
457,ICLR,2017,Semi-Supervised Learning with Context-Conditional Generative Adversarial Networks,Emily Denton;Sam Gross;Rob Fergus,denton@cs.nyu.edu;sgross@fb.com;robfergus@fb.com,5;6;6,4;4;4,Reject,6,3,0,no,11/4/16,New York University;Facebook;Facebook,25;-1;-1,32;-1;-1,5;4,11/4/16,67,25,20,2,18,5,2887;7622;52510,20;17;128,9;11;61,239;929;6454,-1;-1
458,ICLR,2017,Neural Graph Machines: Learning Neural Networks Using Graphs,Thang D. Bui;Sujith Ravi;Vivek Ramavajjala,tdb40@cam.ac.uk;sravi@google.com;vramavaj@google.com,3;4;3,4;4;4,Reject,3,1,0,no,11/4/16,University of Cambridge;Google;Google,67;-1;-1,4;-1;-1,10,11/4/16,38,16,16,2,24,6,692;1699;193,30;68;6,12;22;4,93;141;19,-1;-1
459,ICLR,2017,Perception Updating Networks: On architectural constraints for interpretable video generative models,Eder Santana;Jose C Principe,edercsjr@gmail.com;principe@cnel.ufl.edu,4;4;4,3;4;4,Invite to Workshop Track,1,4,1,no,11/4/16,University of Florida;University of Florida,129;129,135;135,10,11/4/16,0,0,0,0,0,0,217;16963,27;1022,6;57,24;1275,-1;-1
460,ICLR,2017,DRAGNN: A Transition-Based Framework for Dynamically Connected Neural Networks,Lingpeng Kong;Chris Alberti;Daniel Andor;Ivan Bogatyy;David Weiss,lingpenk@cs.cmu.edu;chrisalberti@google.com;andor@google.com;bogatyy@google.com;djweiss@google.com,6;7;5,3;4;4,Reject,2,1,0,no,11/4/16,Carnegie Mellon University;Google;Google;Google;Google,1;-1;-1;-1;-1,23;-1;-1;-1;-1,10,11/4/16,26,10,11,0,83,3,1063;975;1107;54;1028,32;14;11;7;49,14;9;8;2;11,109;165;180;7;169,-1;-1
461,ICLR,2017,A Simple yet Effective Method to Prune Dense Layers of Neural Networks,Mohammad Babaeizadeh;Paris Smaragdis;Roy H. Campbell,mb2@illinois.edu.edu;paris@illinois.edu.edu;rhc@illinois.edu.edu,5;5;3,4;3;4,Reject,1,1,0,no,11/4/16,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",4;4;4,36;36;36,1,11/4/16,5,1,2,0,0,0,620;6737;12219,19;181;560,8;36;54,92;619;961,-1;-1
462,ICLR,2017,Beyond Fine Tuning: A Modular Approach to Learning on Small Data,Aryk Anderson;Kyle Shaffer;Artem Yankov;Court Corley;Nathan Hodas,aryk.anderson@eagles.ewu.edu;kyle.shaffer@pnnl.gov;artem.yankov@pnnl.gov;court@pnnl.gov;nathan.hodas@pnnl.gov,4;6;6,4;2;5,Reject,3,0,0,no,11/4/16,Eastern Washington University;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,6,11/4/16,9,5,1,0,3,0,8;162;56;2263;1112,3;10;8;73;52,1;5;3;14;10,0;15;6;195;70,-1;-1
463,ICLR,2017,HFH: Homologically Functional Hashing for Compressing Deep Neural Networks,Lei Shi;Shikun Feng;Zhifan Zhu,shilei06@baidu.com;fengshikun01@baidu.com;zhuzhifan@baidu.com,5;6;4,5;4,Reject,1,2,0,no,11/4/16,Baidu;Baidu;Baidu,-1;-1;-1,-1;-1;-1,,11/4/16,0,0,0,0,0,0,2641;190;3,386;10;10,25;6;1,82;36;0,-1;-1
464,ICLR,2017,Learning to Understand: Incorporating Local Contexts with Global Attention for Sentiment Classification,Zhigang Yuan;Yuting Hu;Yongfeng Huang,yuanzg14@mails.tsinghua.edu.cn;hu-yt12@mails.tsinghua.edu.cn;yfhuang@tsinghua.edu.cn,3;3;4,4;4;4,Reject,0,0,0,no,11/4/16,Tsinghua University;Tsinghua University;Tsinghua University,11;11;11,35;35;35,,11/4/16,1,0,0,0,0,0,250;111;76,47;56;36,10;5;5,15;4;4,-1;-1
465,ICLR,2017,Multiplicative LSTM for sequence modelling,Ben Krause;Iain Murray;Steve Renals;Liang Lu,ben.krause@ed.ac.uk;i.murray@ed.ac.uk;s.renals@ed.ac.uk;llu@ttic.edu,4;4;6,4;5;4,Invite to Workshop Track,3,4,0,no,11/4/16,University of Edinburgh;University of Edinburgh;University of Edinburgh;Toyota Technological Institute at Chicago,33;33;33;-1,27;27;27;-1,,9/26/16,89,25,38,2,34,11,265;7289;1520;489,14;279;93;41,6;29;24;11,37;812;124;37,-1;-1
466,ICLR,2017,Tensorial Mixture Models,Or Sharir;Ronen Tamari;Nadav Cohen;Amnon Shashua,or.sharir@cs.huji.ac.il;ronent@cs.huji.ac.il;cohennadav@cs.huji.ac.il;shashua@cs.huji.ac.il,4;7;5,4;3;3,Reject,3,3,0,no,11/4/16,Hebrew University of Jerusalem;Hebrew University of Jerusalem;Hebrew University of Jerusalem;Hebrew University of Jerusalem,57;57;57;57,186;186;186;186,5,11/4/16,15,9,3,1,15,3,332;77;1063;8319,16;13;63;173,9;5;15;48,19;6;117;727,-1;-1
467,ICLR,2017,Neural Functional Programming,John K. Feser;Marc Brockschmidt;Alexander L. Gaunt;Daniel Tarlow,feser@csail.mit.edu;mabrocks@microsoft.com;t-algaun@microsoft.com;dtarlow@microsoft.com,5;4;7;5;6,3;3;2;2;3,Invite to Workshop Track,4,2,0,no,11/4/16,Massachusetts Institute of Technology;Microsoft;Microsoft;Microsoft,2;-1;-1;-1,5;-1;-1;-1,,11/4/16,7,2,1,0,0,0,208;2508;1194;2537,23;61;38;68,5;22;15;23,22;329;88;306,-1;-1
468,ICLR,2017,Deep Error-Correcting Output Codes,Guoqiang Zhong;Yuchen Zheng;Peng Zhang;Mengqi Li;Junyu Dong,gqzhong@ouc.edu.cn;ouczyc@outlook.com;sdrzbruce@163.com;enri9615@outlook.com;dongjunyu@ouc.edu.cn,3;3;3,4;5;5,Reject,1,3,0,no,11/4/16,"University of Illinois, Urbana-Champaign;University of Illinois, Urbana-Champaign;163;;University of Illinois, Urbana-Champaign",4;4;-1;-1;4,36;36;-1;-1;36,,11/4/16,2,0,1,0,0,0,566;61;198;731;1529,83;24;63;37;259,14;4;8;10;20,27;3;13;49;71,-1;-1
469,ICLR,2017,Marginal Deep Architectures: Deep learning for Small and Middle Scale Applications,Yuchen Zheng;Guoqiang Zhong;Junyu Dong,ouczyc@outlook.com;gqzhong@ouc.edu.cn;dongjunyu@ouc.edu.cn,3;4;4,4;4;4,Reject,3,3,0,no,11/4/16,"University of Illinois, Urbana-Champaign;University of Illinois, Urbana-Champaign;University of Illinois, Urbana-Champaign",4;4;4,36;36;36,,11/4/16,0,0,0,0,0,0,61;566;1529,24;83;259,4;14;20,3;27;71,-1;-1
470,ICLR,2017,Generative Paragraph Vector,Ruqing Zhang;Jiafeng Guo;Yanyan Lan;Jun Xu;Xueqi Cheng,zhangruqing@software.ict.ac.cn;guojiafeng@ict.ac.cn;lanyanyan@ict.ac.cn;junxu@ict.ac.cn;cxq@ict.ac.cn,4;3;2,4;4;5,Reject,1,0,0,no,11/4/16,"Institute of Computing Technology, Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences",67;67;67;67;67,981;981;981;981;981,5,11/4/16,0,0,0,0,0,0,52;3754;2583;4435;7098,16;149;120;593;395,3;33;25;31;42,11;558;435;241;767,-1;-1
471,ICLR,2017,Simple Black-Box Adversarial Perturbations for Deep Networks,Nina Narodytska;Shiva Kasiviswanathan,n.narodytska@gmail.com;kaivisw@gmail.com,4;4;4,3;4;4,Reject,2,3,0,no,11/4/16,;,-1;-1,-1;-1,4;2,11/4/16,103,45,33,1,25,6,1476;2225,99;74,20;21,150;199,-1;-1
472,ICLR,2017,Short and Deep: Sketching and Neural Networks,Amit Daniely;Nevena Lazic;Yoram Singer;Kunal Talwar,amitdaniely@google.com;nevena@google.com;singer@google.com;kunal@google.com,4;5;5,2;4;2,Invite to Workshop Track,5,4,0,no,11/4/16,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,,11/4/16,3,0,2,0,0,0,891;772;34641;16478,46;30;211;144,16;14;58;42,112;116;4928;2159,-1;-1
473,ICLR,2017,Cat2Vec: Learning Distributed Representation of Multi-field Categorical Data,Ying Wen;Jun Wang;Tianyao Chen;Weinan Zhang,ying.wen@cs.ucl.ac.uk;jun.wang@cs.ucl.ac.uk;tychen@apex.sjtu.edu.cn;wnzhang@apex.sjtu.edu.cn,4;5;4,4;5;4,Reject,2,0,0,no,11/4/16,University College London;University College London;Shanghai Jiao Tong University;Shanghai Jiao Tong University,45;45;61;61,15;15;214;214,3,11/4/16,0,0,0,0,0,0,154;370;356;4935,15;71;17;207,4;10;6;31,11;16;44;700,-1;-1
474,ICLR,2017,On Robust Concepts and Small Neural Nets,Amit Deshpande;Sushrut Karmalkar,amitdesh@microsoft.com;sushrutk@cs.utexas.edu,5;5;6,4;4;2,Invite to Workshop Track,3,3,0,no,11/4/16,"Microsoft;University of Texas, Austin",-1;20,-1;50,1;8,11/4/16,0,0,0,0,0,0,1897;72,33;14,15;5,173;4,-1;-1
475,ICLR,2017,Identity Matters in Deep Learning,Moritz Hardt;Tengyu Ma,m@mrtz.org;tengyu@cs.princeton.edu,8;5;6,3;4;5,Accept (Poster),2,4,0,no,11/4/16,University of California - Berkeley;Princeton University,5;32,10;7,1,11/4/16,206,134,22,8,0,21,7775;3902,89;87,33;32,962;503,-1;-1
476,ICLR,2017,SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size,Forrest N. Iandola;Song Han;Matthew W. Moskewicz;Khalid Ashraf;William J. Dally;Kurt Keutzer,forresti@eecs.berkeley.edu;songhan@stanford.edu;moskewcz@eecs.berkeley.edu;kashraf@eecs.berkeley.edu;dally@stanford.edu;keutzer@eecs.berkeley.edu,7;5;7,3;4;4,Reject,3,3,0,no,11/4/16,University of California Berkeley;Stanford University;University of California Berkeley;University of California Berkeley;Stanford University;University of California Berkeley,5;3;5;5;3;5,10;3;10;10;3;10,,2/24/16,2507,838,1014,50,0,426,4659;15090;6624;2526;30398;16886,36;374;39;18;268;417,15;36;13;7;67;60,634;1942;1026;415;3737;1587,-1;-1
477,ICLR,2017,Non-linear Dimensionality Regularizer for Solving Inverse Problems,Ravi Garg;Anders Eriksson;Ian Reid,ravi.garg@adelaide.edu.au;anders.eriksson@qut.edu.au;ian.reid@adelaide.edu.au,4;3;4,4;5;4,Reject,0,2,0,no,11/4/16,The University of Adelaide;South China University of Technology;The University of Adelaide,113;462;113,143;613;143,2,3/16/16,10,6,2,0,0,1,990;1539;17416,49;88;346,16;20;65,126;157;2003,-1;-1
478,ICLR,2017,"ParMAC: distributed optimisation of nested functions, with application to binary autoencoders",Miguel A. Carreira-Perpinan;Mehdi Alizadeh,mcarreira-perpinan@ucmerced.edu;malizadeh@ucmerced.edu,4;5;6;6,2;4;4;4,Reject,1,5,0,no,11/4/16,University of California at Merced;University of California at Merced,462;462,981;981,,5/30/16,5,1,2,0,6,0,4288;329,113;59,28;7,391;7,-1;-1
479,ICLR,2017,Filling in the details: Perceiving from low fidelity visual input,Farahnaz A. Wick;Michael L. Wick;Marc Pomplun,fwick@cs.umb.edu;mwick@cs.umass.edu;mpomplun@gmail.com,4;6;5,3;4;5,Reject,3,4,0,no,11/3/16,"University of Massachusetts, Boston;University of Massachusetts, Amherst;",351;25;-1,166;166;-1,5,11/3/16,0,0,0,0,0,0,61;820;1729,15;39;150,2;15;20,2;77;121,-1;-1
480,ICLR,2017,Pedestrian Detection Based On Fast R-CNN and Batch Normalization ,Zhong-Qiu Zhao;Haiman Bian;Donghui Hu;Herve Glotin,z.zhao@hfut.edu.cn;bhm2164@163.com;hudh@hfut.edu.cn;h.glotin@gmail.com,3;3;2;3,5;5;5;5,Reject,0,0,0,no,11/3/16,South China University of Technology;163;South China University of Technology;CNRS university Toulon,462;-1;462;462,613;-1;613;981,8,11/3/16,9,3,2,1,0,1,1372;25;586;1971,69;4;90;280,13;3;12;25,152;1;32;109,-1;-1
481,ICLR,2017,Ternary Weight Decomposition and Binary Activation Encoding for Fast and Compact Neural Network,Mitsuru Ambai;Takuya Matsumoto;Takayoshi Yamashita;Hironobu Fujiyoshi,manbai@d-itlab.co.jp;tmatsumoto@d-itlab.co.jp;yamashita@cs.chubu.ac.jp;hf@cs.chubu.ac.jp,6;5;4,3;3;4,Reject,1,0,0,no,11/2/16,;;chubu university;chubu university,-1;-1;462;462,-1;-1;630;630,2,11/2/16,3,1,2,0,0,0,120;46;1007;3755,25;17;133;201,5;3;13;17,11;1;66;222,-1;-1
482,ICLR,2017,What Is the Best Practice for CNNs Applied to Visual Instance Retrieval?,Jiedong Hao;Jing Dong;Wei Wang;Tieniu Tan,jiedong.hao@cripac.ia.ac.cn;jdong@nlpr.ia.ac.cn;wwang@nlpr.ia.ac.cn;tnt@nlpr.ia.ac.cn,3;6;3,5;4;5,Reject,4,0,0,no,11/3/16,"Institute of automation, Chinese academy of science, Chinese Academy of Sciences;Institute of automation, Chinese academy of science, Chinese Academy of Sciences;Institute of automation, Chinese academy of science, Chinese Academy of Sciences;Institute of automation, Chinese academy of science, Chinese Academy of Sciences",67;67;67;67,981;981;981;981,,11/3/16,6,3,1,0,3,0,8;1008;877;26066,3;74;48;635,2;15;12;78,0;70;70;2156,-1;-1
483,ICLR,2017,Rectified Factor Networks for Biclustering,Djork-Arné Clevert;Thomas Unterthiner;Sepp Hochreiter,okko@bioinf.jku.at;unterthiner@bioinf.jku.at;hochreit@bioinf.jku.at,4;5;5,4;2;2,Reject,1,0,0,no,11/2/16,Johannes Kepler University Linz;Johannes Kepler University Linz;Johannes Kepler University Linz,462;462;462,498;498;498,5,11/2/16,3,0,1,0,0,1,3770;5565;35982,31;39;111,14;17;28,422;1083;6632,-1;-1
484,ICLR,2017,Vocabulary Selection Strategies for Neural Machine Translation,Gurvan L'Hostis;David Grangier;Michael Auli,gurvan.lhostis@polytechnique.edu;grangier@fb.com;michaelauli@fb.com,5;4;4;5,3;5;4;3,Reject,0,0,0,no,11/2/16,Ecole polytechnique;Facebook;Facebook,462;-1;-1,116;-1;-1,3,10/1/16,27,11,8,1,3,3,35;5119;6880,3;64;71,2;25;31,3;757;1025,-1;-1
485,ICLR,2017,Learning Efficient Algorithms with Hierarchical Attentive Memory,Marcin Andrychowicz;Karol Kurach,marcin@openai.com;kkurach@google.com,3;5;5,4;5;4,Reject,2,0,0,no,11/1/16,OpenAI;Google,-1;-1,-1;-1,,2/9/16,45,19,9,0,80,0,3252;1181,29;28,18;11,377;135,-1;-1
486,ICLR,2017,PREDICTION OF POTENTIAL HUMAN INTENTION USING SUPERVISED COMPETITIVE LEARNING,Masayoshi Ishikawa;Mariko Okude;Takehisa Nishida;Kazuo Muto,masayoshi.ishikawa.gv@hitachi.com;mariko.okude.uh@hitachi.com;takehisa.nishida.cu@hitachi.com;kazuo.muto.ny@hitachi.com,2;2;4,4;4;4,Reject,1,0,0,no,11/1/16,University of Iceland;Hitachi;Hitachi;Hitachi,462;-1;-1;-1,243;-1;-1;-1,,11/1/16,0,0,0,0,0,0,6;8;34;88,18;30;9;49,1;2;2;6,0;0;4;3,-1;-1
487,ICLR,2017,Conditional Image Synthesis With Auxiliary Classifier GANs,Augustus Odena;Christopher Olah;Jonathon Shlens,augustusodena@google.com;colah@google.com;shlens@google.com,3;6;6,4;5;4,Reject,3,5,0,no,11/1/16,Google;Google;Google,-1;-1;-1,-1;-1;-1,5;4,10/30/16,1196,618,484,17,190,220,3272;2059;23803,25;10;80,13;7;37,475;283;4029,-1;-1
488,ICLR,2017,Learning to Protect Communications with Adversarial Neural Cryptography,Martín Abadi;David G. Andersen,abadi@google.com;dga@google.com,5;6;4,4;3;2,Reject,3,3,0,no,10/21/16,Google;Google,-1;-1,-1;-1,4,10/21/16,92,43,19,6,0,10,28596;12685,277;171,70;53,3379;1651,-1;-1
489,ICLR,2017,Surprisal-Driven Feedback in Recurrent Networks,Kamil Rocki,kmrocki@us.ibm.com,4;3;3,4;5;5,Reject,5,1,0,no,10/18/16,International Business Machines,-1,-1,3,8/22/16,10,3,2,0,24,0,159,29,8,20,-1
490,ICLR,2018,Certifying Some Distributional Robustness with Principled Adversarial Training,Aman Sinha;Hongseok Namkoong;John Duchi,amans@stanford.edu;hnamk@stanford.edu;jduchi@stanford.edu,9;9;9,5;4;4,Accept (Oral),0,14,3,yes,10/27/17,Stanford University;Stanford University;Stanford University,4;4;4,3;3;3,4,10/27/17,459,207,87,10,10,45,655;904;12908,42;19;162,8;10;42,67;108;1878,-1;-1
491,ICLR,2018,Parametric Information Bottleneck to Optimize Stochastic Neural Networks,Thanh T. Nguyen;Jaesik Choi,thanhnguyen2792@gmail.com;jaesik@unist.ac.kr,4;6;4,4;4;4,Reject,0,0,0,yes,10/27/17,Ulsan National Institute of Science and Technology;Ulsan National Institute of Science and Technology,468;468,230;230,8,10/27/17,4,2,1,0,2,0,1052;727,86;88,16;15,37;47,-1;-1
492,ICLR,2018,Towards Neural Phrase-based Machine Translation,Po-Sen Huang;Chong Wang;Sitao Huang;Dengyong Zhou;Li Deng,huang.person@gmail.com;chongw@google.com;shuang91@illinois.edu;dennyzhou@gmail.com;l.deng@ieee.org,6;6;8,3;4;5,Accept (Poster),0,3,0,yes,10/26/17,"Microsoft;Google;University of Illinois, Urbana Champaign;Google;",-1;-1;3;-1;-1,-1;-1;37;-1;-1,3;2,6/17/17,30,8,13,1,33,4,1718;17889;161;8800;20766,59;1045;17;78;409,17;54;7;33;63,241;1628;21;1338;1867,-1;-1
493,ICLR,2018,Weightless: Lossy Weight Encoding For Deep Neural Network Compression,Brandon Reagen;Udit Gupta;Robert Adolf;Michael Mitzenmacher;Alexander Rush;Gu-Yeon Wei;David Brooks,reagen@fas.harvard.edu;ugupta@g.harvard.edu;rdadolf@seas.harvard.edu;michaelm@eecs.harvard.edu;srush@seas.harvard.edu;gywei@g.harvard.edu;dbrooks@eecs.harvard.edu,6;6;4,4;4;4,Invite to Workshop Track,0,3,0,yes,10/27/17,Harvard University;Harvard University;Harvard University;Harvard University;Harvard University;Harvard University;Harvard University,37;37;37;37;37;37;37,6;6;6;6;6;6;6,,10/27/17,12,5,5,1,4,2,775;309;41;846;7030;5222;2891,27;40;4;57;86;180;198,12;10;3;11;32;37;27,94;27;3;93;940;454;205,-1;-1
494,ICLR,2018,Interactive Grounded Language Acquisition and Generalization in a 2D World,Haonan Yu;Haichao Zhang;Wei Xu,haonanyu@baidu.com;zhanghaichao@baidu.com;wei.xu@baidu.com,7;6;6,4;4;4,Accept (Poster),2,7,0,yes,10/27/17,Baidu;Baidu;Baidu,-1;-1;-1,-1;-1;-1,6;8,10/27/17,37,25,8,0,13,1,890;1382;9553,37;63;557,12;20;44,97;125;925,-1;-1
495,ICLR,2018,A Boo(n) for Evaluating Architecture Performance,Ondrej Bajgar;Rudolf Kadlec;and Jan Kleindienst,ondrej@bajgar.org;rudolf_kadlec@cz.ibm.com;jankle@cz.ibm.com,4;6;4,4;4;4,Reject,0,4,0,yes,10/27/17,;International Business Machines;International Business Machines,-1;-1;-1,-1;-1;-1,,10/27/17,2,2,0,0,13,0,366;678;658,10;40;76,5;11;9,61;94;89,-1;-1
496,ICLR,2018,Countering Adversarial Images using Input Transformations,Chuan Guo;Mayank Rana;Moustapha Cisse;Laurens van der Maaten,cg563@cornell.edu;mayankrana@fb.com;moustaphacisse@fb.com;lvdmaaten@gmail.com,4;8;7,3;4;3,Accept (Poster),12,16,0,yes,10/27/17,Cornell University;Facebook;Facebook;Facebook,7;-1;-1;-1,19;-1;-1;-1,4,10/27/17,391,228,104,13,43,55,1313;392;2917;17805,23;5;48;93,8;2;19;32,238;53;401;1264,-1;-1
497,ICLR,2018,A Neural Representation of Sketch Drawings,David Ha;Douglas Eck,hadavid@google.com;deck@google.com,8;8;5,4;4;4,Accept (Poster),0,3,0,yes,10/26/17,Google;Google,-1;-1,-1;-1,5,4/11/17,274,136,133,6,0,59,1238;2532,66;84,11;26,166;272,-1;-1
498,ICLR,2018,The Kanerva Machine: A Generative Distributed Memory,Yan Wu;Greg Wayne;Alex Graves;Timothy Lillicrap,yanwu@google.com;gregwayne@google.com;gravesa@google.com;countzero@google.com,6;7;7,4;3;2,Accept (Poster),0,3,0,yes,10/27/17,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,5;11,10/27/17,13,7,6,1,229,0,2383;2530;43616;23365,207;32;87;74,22;15;50;39,170;260;5989;2855,-1;-1
499,ICLR,2018,Minimax Curriculum Learning: Machine Teaching with Desirable Difficulties and Scheduled Diversity,Tianyi Zhou;Jeff Bilmes,tianyi.david.zhou@gmail.com;bilmes@uw.edu,5;6;6,3;4;3,Accept (Poster),0,4,0,yes,10/27/17,"University of Washington;University of Washington, Seattle",6;6,25;25,,10/27/17,9,7,2,0,0,0,1444;13443,80;348,14;53,142;1263,-1;-1
500,ICLR,2018,Multi-View Data Generation Without View Supervision,Mickael Chen;Ludovic Denoyer;Thierry Artières,mickael.chen@lip6.fr;ludovic.denoyer@lip6.fr;thierry.artieres@lif.univ-mrs.fr,7;5;7,3;4;5,Accept (Poster),0,3,0,yes,10/27/17,LIP6;LIP6;Aix Marseille University,-1;-1;468,-1;-1;297,5;4,10/27/17,13,9,6,0,14,1,43;2995;1302,4;128;144,3;22;16,6;524;91,-1;-1
501,ICLR,2018,Towards Binary-Valued Gates for Robust LSTM Training ,Zhuohan Li;Di He;Fei Tian;Wei Chen;Tao Qin;Liwei Wang;Tie-Yan Liu,lizhuohan@pku.edu.cn;di_he@pku.edu.cn;fetia@microsoft.com;wche@microsoft.com;taoqin@microsoft.com;wanglw@cis.pku.edu.cn;tyliu@microsoft.com,6;4;6,3;4;4,Reject,0,4,0,yes,10/26/17,Peking University;Peking University;Microsoft;Microsoft;Microsoft;Peking University;Microsoft,24;24;-1;-1;-1;24;-1,27;27;-1;-1;-1;27;-1,3;8,10/26/17,26,13,11,1,7,2,100;2677;5327;49851;4995;2389;13516,13;258;370;3662;289;75;366,7;27;32;90;34;23;51,10;110;273;2845;587;264;1719,-1;-1
502,ICLR,2018,On the importance of single directions for generalization,Ari S. Morcos;David G.T. Barrett;Neil C. Rabinowitz;Matthew Botvinick,arimorcos@google.com;barrettdavid@google.com;ncr@google.com;botvinick@google.com,7;5;9,3;4;3,Accept (Poster),0,5,1,yes,10/27/17,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,8,10/27/17,133,82,24,13,511,15,992;1159;2960;13268,31;24;37;147,12;11;16;44,111;157;383;1391,-1;-1
503,ICLR,2018,Variational Continual Learning,Cuong V. Nguyen;Yingzhen Li;Thang D. Bui;Richard E. Turner,vcn22@cam.ac.uk;yl494@cam.ac.uk;tdb40@cam.ac.uk;ret26@cam.ac.uk,6;6;6,3;4;2,Accept (Poster),0,4,0,yes,10/27/17,University of Cambridge;University of Cambridge;University of Cambridge;University of Cambridge,71;71;71;71,2;2;2;2,5,10/27/17,181,90,83,10,6,33,415;1040;674;2918,27;56;30;174,10;14;12;30,59;147;92;306,-1;-1
504,ICLR,2018,Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning,Rajarshi Das;Shehzaad Dhuliawala;Manzil Zaheer;Luke Vilnis;Ishan Durugkar;Akshay Krishnamurthy;Alex Smola;Andrew McCallum,rajarshi@cs.umass.edu;sdhuliawala@cs.umass.edu;manzil@cmu.edu;luke@cs.umass.edu;ishand@cs.utexas.edu;akshay@cs.umass.edu;alex@smola.org;mccallum@cs.umass.edu,7;6;5,4;4;4,Accept (Poster),8,16,0,yes,10/27/17,"University of Massachusetts, Amherst;University of Massachusetts, Amherst;Carnegie Mellon University;University of Massachusetts, Amherst;University of Texas, Austin;University of Massachusetts, Amherst;Carnegie-Mellon University;University of Massachusetts, Amherst",30;30;1;30;21;30;1;30,191;191;24;191;49;191;24;191,10,10/27/17,125,64,41,5,21,32,4903;183;1602;1764;335;1537;68208;45557,164;14;63;25;15;75;404;434,33;3;17;10;5;21;99;96,407;43;263;295;47;165;9240;4972,-1;-1
505,ICLR,2018,Learning to Represent Programs with Graphs,Miltiadis Allamanis;Marc Brockschmidt;Mahmoud Khademi,miallama@microsoft.com;mabrocks@microsoft.com;mkhademi@sfu.ca,8;8;8,4;4;4,Accept (Oral),0,10,0,yes,10/27/17,Microsoft;Microsoft;Simon Fraser University,-1;-1;57,-1;-1;253,3;10,10/27/17,174,104,86,5,102,42,1904;2459;273,42;61;26,18;22;8,200;327;52,-1;-1
506,ICLR,2018,Variational image compression with a scale hyperprior,Johannes Ballé;David Minnen;Saurabh Singh;Sung Jin Hwang;Nick Johnston,jballe@google.com;dminnen@google.com;saurabhsingh@google.com;sjhwang@google.com;nickj@google.com,7;7;7,4;5;5,Accept (Poster),1,9,1,yes,10/27/17,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,5,10/27/17,171,85,96,12,2,52,1149;1746;686;1751;869,40;35;59;61;27,14;18;6;18;9,222;212;75;205;149,-1;-1
507,ICLR,2018,Simulating Action Dynamics with Neural Process Networks,Antoine Bosselut;Omer Levy;Ari Holtzman;Corin Ennis;Dieter Fox;Yejin Choi,antoineb@cs.washington.edu;omerlevy@cs.washington.edu;ahai@cs.washington.edu;corin123@uw.edu;fox@cs.washington.edu;yejin@cs.washington.edu,6;9;8,4;4;4,Accept (Poster),0,5,0,yes,10/27/17,"University of Washington;University of Washington;University of Washington;University of Washington, Seattle;University of Washington;University of Washington",6;6;6;6;6;6,25;25;25;25;25;25,,10/27/17,44,34,9,1,13,8,397;7449;600;139;39994;7812,20;58;18;3;374;139,8;30;10;2;97;43,57;1206;94;8;3187;1011,-1;-1
508,ICLR,2018,Evidence Aggregation for Answer Re-Ranking in Open-Domain Question Answering,Shuohang Wang;Mo Yu;Jing Jiang;Wei Zhang;Xiaoxiao Guo;Shiyu Chang;Zhiguo Wang;Tim Klinger;Gerald Tesauro;Murray Campbell,shwang.2014@phdis.smu.edu.sg;yum@us.ibm.com;jingjiang@smu.edu.sg;zhangwei@us.ibm.com;xiaoxiao.guo@ibm.com;shiyu.chang@ibm.com;zhigwang@us.ibm.com;tklinger@us.ibm.com;gtesauro@us.ibm.com;mcam@us.ibm.com,6;8;6,2;3;4,Accept (Poster),0,3,0,yes,10/27/17,Singapore Management University;International Business Machines;Singapore Management University;International Business Machines;International Business Machines;International Business Machines;International Business Machines;International Business Machines;International Business Machines;International Business Machines,90;-1;90;-1;-1;-1;-1;-1;-1;-1,1103;-1;1103;-1;-1;-1;-1;-1;-1;-1,,10/27/17,79,48,42,8,14,23,1107;3563;7163;300;1512;2981;2000;643;7620;1589,20;71;202;11;42;111;75;34;124;57,8;26;32;8;13;28;25;12;45;19,211;456;795;52;214;397;307;115;716;130,-1;-1
509,ICLR,2018,QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension,Adams Wei Yu;David Dohan;Minh-Thang Luong;Rui Zhao;Kai Chen;Mohammad Norouzi;Quoc V. Le,weiyu@cs.cmu.edu;ddohan@google.com;thangluong@google.com;rzhao@google.com;kaichen@google.com;mnorouzi@google.com;qvl@google.com,8;5;6,5;4;3,Accept (Poster),2,11,1,yes,10/27/17,Carnegie Mellon University;Google;Google;Google;Google;Google;Google,1;-1;-1;-1;-1;-1;-1,24;-1;-1;-1;-1;-1;-1,3,10/27/17,387,187,184,7,0,80,815;1093;3101;8364;33496;8079;48714,28;11;33;608;46;126;193,13;4;20;39;12;31;81,146;157;357;505;5417;1019;6064,-1;-1
510,ICLR,2018,Measuring the Intrinsic Dimension of Objective Landscapes,Chunyuan Li;Heerad Farkhoor;Rosanne Liu;Jason Yosinski,chunyuan.li@duke.edu;heerad@uber.com;rosanne@uber.com;jason@yosinski.com,7;7;6,3;4;2,Accept (Poster),0,4,0,yes,10/27/17,Duke University;Uber;Uber;University of Montreal,46;-1;-1;124,17;-1;-1;108,1,10/27/17,78,54,17,5,19,13,1998;78;383;8163,80;4;12;52,25;1;7;21,244;13;66;643,-1;-1
511,ICLR,2018,Residual Loss Prediction: Reinforcement Learning With No Incremental Feedback,Hal Daumé III;John Langford;Amr Sharaf,hal@umiacs.umd.edu;jl@hunch.net;amr@cs.umd.edu,7;7;6,5;2;4,Accept (Poster),0,10,2,yes,10/27/17,"University of Maryland, College Park;;University of Maryland, College Park",12;-1;12,69;-1;69,,10/27/17,5,2,0,0,0,0,10485;11741;69,198;198;20,46;49;4,1072;1281;3,-1;-1
512,ICLR,2018,Reinforcement Learning Algorithm Selection,Romain Laroche;Raphael Feraud,romain.laroche@gmail.com;raphael.feraud@orange.com,6;6;7,5;3;4,Accept (Poster),0,0,0,yes,10/26/17,Microsoft;General Electric,-1;-1,-1;-1,,1/30/17,5,3,1,0,0,1,591;777,80;46,15;14,28;59,-1;-1
513,ICLR,2018,Feature Incay for Representation Regularization,Yuhui Yuan;Kuiyuan Yang;Jianyuan Guo;Jingdong Wang;Chao Zhang,yuyua@microsoft.com;kuiyuanyang@deepmotion.ai;1701214082@pku.edu.cn;jingdw@microsoft.com;chzhang@cis.pku.edu.cn,6;6;6,3;2;4,Invite to Workshop Track,0,3,1,yes,10/25/17,Microsoft;DeepMotion;Peking University;Microsoft;Peking University,-1;-1;24;-1;24,-1;-1;27;-1;27,,5/29/17,5,3,0,0,6,0,454;1603;93;14845;429,20;52;24;228;102,11;17;6;58;9,57;141;9;2108;25,-1;-1
514,ICLR,2018,Zero-Shot Visual Imitation,Deepak Pathak;Parsa Mahmoudieh;Guanghao Luo;Pulkit Agrawal;Dian Chen;Yide Shentu;Evan Shelhamer;Jitendra Malik;Alexei A. Efros;Trevor Darrell,pathak@berkeley.edu;parsa.m@berkeley.edu;michaelluo@berkeley.edu;pulkitag@berkeley.edu;dianchen@berkeley.edu;fredshentu@berkeley.edu;shelhamer@cs.berkeley.edu;malik@eecs.berkeley.edu;efros@eecs.berkeley.edu;trevor@eecs.berkeley.edu,8;8;7,4;3;5,Accept (Oral),0,8,0,yes,10/27/17,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5;5;5;5;5;5;5,18;18;18;18;18;18;18;18;18;18,6,10/27/17,115,74,23,5,7,10,4211;191;111;3015;369;124;27413;70210;37176;90665,40;8;1;52;34;2;26;429;193;559,14;4;1;16;8;2;14;115;77;112,552;14;10;248;23;10;4159;7767;4611;11500,-1;-1
515,ICLR,2018,"Emergent Communication in a Multi-Modal, Multi-Step Referential Game",Katrina Evtimova;Andrew Drozdov;Douwe Kiela;Kyunghyun Cho,kve216@nyu.edu;apd283@nyu.edu;dkiela@fb.com;kyunghyun.cho@nyu.edu,7;7;7,3;4;4,Accept (Poster),0,3,0,yes,10/27/17,New York University;New York University;Facebook;New York University,26;26;-1;26,27;27;-1;27,3;8,5/29/17,58,27,7,2,28,1,61;162;3383;45173,4;10;79;272,3;3;29;52,1;13;576;6528,-1;-1
516,ICLR,2018,Distributed Fine-tuning of Language Models on Private Data,Vadim Popov;Mikhail Kudinov;Irina Piontkovskaya;Petr Vytovtov;Alex Nevidomsky,v.popov@samsung.com;m.kudinov@samsung.com;p.irina@samsung.com;p.vytovtov@partner.samsung.com;a.nevidomsky@samsung.com,5;4;4,4;3;4,Accept (Poster),0,6,0,yes,10/24/17,Samsung;Samsung;Samsung;Samsung;Samsung,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,3,10/24/17,4,3,0,0,0,0,15;5;6;14;11,12;6;3;5;3,2;1;2;2;2,0;0;0;0;1,-1;-1
517,ICLR,2018,Preliminary theoretical troubleshooting in Variational Autoencoder,Shiqi Liu;Qian Zhao;Xiangyong Cao;Deyu Meng;Zilu Ma;Tao Yu,liushiqi@stu.xjtu.edu.cn;dymeng@mail.xjtu.edu.cn;timmy.zhaoqian@gmail.com;460376821@qq.com;1030884089@qq.com;602077855@qq.com,5;3;2,4;4;4,Reject,0,5,0,yes,10/27/17,Xi'an Jiaotong University;Xi'an Jiaotong University;Xi'an Jiaotong University;;;,468;468;468;-1;-1;-1,565;565;565;-1;-1;-1,5;1,10/27/17,0,0,0,0,0,0,109;518;242;6327;48;-1,18;186;18;169;10;-1,6;11;6;40;3;-1,6;28;20;891;2;0,-1;-1
518,ICLR,2018,On the Convergence of Adam and Beyond,Sashank J. Reddi;Satyen Kale;Sanjiv Kumar,sashank@google.com;satyenkale@google.com;sanjivk@google.com,9;8;8,5;4;3,Accept (Oral),2,10,13,yes,10/27/17,Google;Google;Google,-1;-1;-1,-1;-1;-1,9,10/27/17,728,408,408,24,0,172,2242;5260;9915,53;87;298,21;31;44,426;810;1301,-1;-1
519,ICLR,2018,Spatially Transformed Adversarial Examples,Chaowei Xiao;Jun-Yan Zhu;Bo Li;Warren He;Mingyan Liu;Dawn Song,xiaocw@umich.edu;junyanzhu89@gmail.com;lxbosky@gmail.com;_w@eecs.berkeley.edu;mingyan@umich.edu;dawnsong.travel@gmail.com,7;9;7,4;5;4,Accept (Poster),7,13,1,yes,10/27/17,University of Michigan;NAVER;University of California Berkeley;University of California Berkeley;University of Michigan;University of California Berkeley,8;-1;5;5;8;5,21;-1;18;18;21;18,4,10/27/17,160,106,34,3,15,22,1318;15358;2365;1204;12506;43319,30;50;80;22;264;396,12;27;23;14;38;100,147;2975;268;127;1947;4323,-1;-1
520,ICLR,2018,Parametrized Hierarchical Procedures for Neural Programming,Roy Fox;Richard Shin;Sanjay Krishnan;Ken Goldberg;Dawn Song;Ion Stoica,roy.d.fox@gmail.com;shin.richard@gmail.com;sanjay@eecs.berkeley.edu;goldberg@berkeley.edu;dawnsong.travel@gmail.com;istoica@cs.berkeley.edu,6;6;6,3;1;2,Accept (Poster),0,8,0,yes,10/27/17,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5;5;5,18;18;18;18;18;18,,10/27/17,16,9,5,0,0,1,541;275;1486;9978;43319;56622,43;29;88;300;396;492,10;9;22;50;100;113,76;27;118;670;4323;7234,-1;-1
521,ICLR,2018,Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality,Xingjun Ma;Bo Li;Yisen Wang;Sarah M. Erfani;Sudanthi Wijewickrema;Grant Schoenebeck;Dawn Song;Michael E. Houle;James Bailey,xingjunm@student.unimelb.edu.au;crystalboli@berkeley.edu;wangys14@mails.tsinghua.edu.cn;sarah.erfani@unimelb.edu.au;sudanthi.wijewickrema@unimelb.edu.au;schoeneb@umich.edu;dawnsong.travel@gmail.com;meh@nii.ac.jp;baileyj@unimelb.edu.au,8;6;7,3;1;4,Accept (Oral),0,11,2,yes,10/24/17,The University of Melbourne;University of California Berkeley;Tsinghua University;The University of Melbourne;The University of Melbourne;University of Michigan;University of California Berkeley;Meiji University;The University of Melbourne,124;5;10;124;124;8;5;468;124,32;18;30;32;32;21;18;334;32,4;1,10/24/17,198,113,56,9,10,28,569;2458;360;920;513;1536;43319;2190;5622,32;80;23;58;55;59;396;122;330,9;23;8;11;9;17;100;24;35,83;280;41;84;58;137;4323;203;664,-1;-1
522,ICLR,2018,Generating Adversarial Examples with Adversarial Networks,Chaowei Xiao;Bo Li;Jun-Yan Zhu;Warren He;Mingyan Liu;Dawn Song,xiaocw@umich.edu;lxbosky@gmail.com;junyanz@berkeley.edu;_w@eecs.berkeley.edu;mingyan@umich.edu;dawnsong.travel@gmail.com,4;6;7,4;4;3,Reject,1,16,1,yes,10/27/17,University of Michigan;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of Michigan;University of California Berkeley,8;5;5;5;8;5,21;18;18;18;21;18,5;4,10/27/17,175,98,43,6,0,21,1318;2365;15358;1204;12506;43319,30;80;50;22;264;396,12;23;27;14;38;100,147;268;2975;127;1947;4323,-1;-1
523,ICLR,2018,Decision Boundary Analysis of Adversarial Examples,Warren He;Bo Li;Dawn Song,_w@eecs.berkeley.edu;lxbosky@gmail.com;dawnsong.travel@gmail.com,6;6;6,3;2;3,Accept (Poster),0,4,0,yes,10/27/17,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,18;18;18,4,10/27/17,50,29,7,1,0,6,1204;2365;43319,22;80;396,14;23;100,127;268;4323,-1;-1
524,ICLR,2018,Learning Sparse Latent Representations with the Deep Copula Information Bottleneck,Aleksander Wieczorek*;Mario Wieser*;Damian Murezzan;Volker Roth,aleksander.wieczorek@unibas.ch;mario.wieser@unibas.ch;d.murezzan@unibas.ch;volker.roth@unibas.ch,5;6;6;6,4;3;3;1,Accept (Poster),0,2,0,yes,10/27/17,University of Basel;University of Basel;University of Basel;University of Basel,364;364;364;364,95;95;95;95,,10/27/17,11,6,6,0,6,0,49;122;20;1409,17;14;3;70,4;5;2;21,2;2;0;121,-1;-1
525,ICLR,2018,Demystifying MMD GANs,Mikołaj Bińkowski;Dougal J. Sutherland;Michael Arbel;Arthur Gretton,mikbinkowski@gmail.com;dougal@gmail.com;michael.n.arbel@gmail.com;arthur.gretton@gmail.com,4;7;6,4;4;2,Accept (Poster),1,6,0,yes,10/27/17,Imperial College London;University College London;University College London;,74;46;46;-1,8;16;16;-1,5;4,10/27/17,177,82,80,7,0,50,275;718;258;12781,7;36;11;215,4;14;5;48,65;99;65;2215,-1;-1
526,ICLR,2018,Fix your classifier: the marginal value of training the last weight layer,Elad Hoffer;Itay Hubara;Daniel Soudry,elad.hoffer@gmail.com;itayhubara@gmail.com;daniel.soudry@gmail.com,6;6;6,4;5;3,Accept (Poster),2,4,3,yes,10/27/17,Technion;;Technion,24;-1;24,327;-1;327,,10/27/17,35,17,6,2,129,2,1424;2967;4801,27;21;76,11;11;26,162;425;618,-1;-1
527,ICLR,2018,Flipout: Efficient Pseudo-Independent Weight Perturbations on Mini-Batches,Yeming Wen;Paul Vicol;Jimmy Ba;Dustin Tran;Roger Grosse,wenyemin@cs.toronto.edu;pvicol@cs.toronto.edu;jimmy@psi.toronto.edu;trandustin@google.com;rgrosse@cs.toronto.edu,6;8;6,4;3;4,Accept (Poster),0,6,0,yes,10/27/17,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;University of Toronto;Google;Department of Computer Science, University of Toronto",17;17;17;-1;17,22;22;22;-1;22,11,10/27/17,59,16,35,0,71,5,130;141;52924;1692;5598,10;19;56;50;48,3;6;22;20;27,9;16;8625;196;800,-1;-1
528,ICLR,2018,Tree-to-tree Neural Networks for Program Translation,Xinyun Chen;Chang Liu;Dawn Song,xinyun.chen@berkeley.edu;liuchang@eecs.berkeley.edu;dawnsong.travel@gmail.com,6;4;4,4;3;4,Invite to Workshop Track,0,6,0,yes,10/27/17,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,18;18;18,,10/27/17,51,26,23,1,9,5,1634;6509;43319,51;697;396,14;38;100,139;302;4323,-1;-1
529,ICLR,2018,Towards Synthesizing Complex Programs From Input-Output Examples,Xinyun Chen;Chang Liu;Dawn Song,xinyun.chen@berkeley.edu;liuchang@eecs.berkeley.edu;dawnsong.travel@gmail.com,8;7;5,3;4;2,Accept (Poster),0,9,0,yes,10/27/17,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,18;18;18,,6/5/17,23,14,4,0,22,0,1634;6509;43319,51;697;396,14;38;100,139;302;4323,-1;-1
530,ICLR,2018,A DIRT-T Approach to Unsupervised Domain Adaptation,Rui Shu;Hung Bui;Hirokazu Narui;Stefano Ermon,ruishu@stanford.edu;buih@google.com;hirokaz2@stanford.edu;ermon@cs.stanford.edu,8;7;7,4;4;2,Accept (Poster),0,5,0,yes,10/27/17,Stanford University;Google;Stanford University;Stanford University,4;-1;4;4,3;-1;3;3,5;4,10/27/17,181,97,75,8,8,31,529;218;277;4767,47;10;11;201,12;4;3;30,62;35;45;638,-1;-1
531,ICLR,2018,SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data,Alon Brutzkus;Amir Globerson;Eran Malach;Shai Shalev-Shwartz,alonbrutzkus@mail.tau.ac.il;amir.globerson@gmail.com;eran.malach@mail.huji.ac.il;shais@cs.huji.ac.il,7;7;8,3;3;4,Accept (Poster),0,1,0,yes,10/27/17,Tel Aviv University;Tel Aviv University;Hebrew University of Jerusalem;Hebrew University of Jerusalem,37;37;62;62,217;217;205;205,1;9;8,10/27/17,146,102,6,7,3,17,373;4451;255;13702,10;123;12;168,5;32;5;48,40;576;40;1805,-1;-1
532,ICLR,2018,NerveNet: Learning Structured Policy with Graph Neural Networks,Tingwu Wang;Renjie Liao;Jimmy Ba;Sanja Fidler,tingwuwang@cs.toronto.edu;rjliao@cs.toronto.edu;jimmy@psi.toronto.edu;fidler@cs.toronto.edu,7;6;7,3;3;3,Accept (Poster),0,7,0,yes,10/27/17,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;University of Toronto;Department of Computer Science, University of Toronto",17;17;17;17,22;22;22;22,6;10,10/27/17,65,40,26,0,0,6,177;1572;51293;10491,8;63;52;160,5;22;21;48,14;204;8393;1358,-1;-1
533,ICLR,2018,Matrix capsules with EM routing,Geoffrey E Hinton;Sara Sabour;Nicholas Frosst,geoffhinton@google.com;sasabour@google.com;frosst@google.com,7;6;4,3;3;2,Accept (Poster),10,10,18,yes,10/27/17,Google;Google;Google,-1;-1;-1,-1;-1;-1,4,10/27/17,340,170,131,17,0,74,209159;1919;1910,415;13;12,127;7;6,21400;483;478,-1;-1
534,ICLR,2018,Towards better understanding of gradient-based attribution methods for Deep Neural Networks,Marco Ancona;Enea Ceolini;Cengiz Öztireli;Markus Gross,marco.ancona@inf.ethz.ch;enea.ceolini@ini.uzh.ch;cengizo@inf.ethz.ch;grossm@inf.ethz.ch,7;6;7,3;5;4,Accept (Poster),0,3,0,yes,10/26/17,Swiss Federal Institute of Technology;University of Zurich;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,9;139;9;9,10;136;10;10,1,10/26/17,190,80,79,6,0,23,1037;379;291;941,131;24;13;44,16;7;7;11,59;41;36;83,-1;-1
535,ICLR,2018,Linearly Constrained Weights: Resolving the Vanishing Gradient Problem by Reducing Angle Bias,Takuro Kutsuna,kutsuna@mosk.tytlabs.co.jp,5;5;4,4;4;4,Reject,0,6,0,yes,10/24/17,Toyota Central R&D Labs. Inc.,-1,-1,,10/24/17,0,0,0,0,0,0,32,20,3,0,-1
536,ICLR,2018,Understanding Short-Horizon Bias in Stochastic Meta-Optimization,Yuhuai Wu;Mengye Ren;Renjie Liao;Roger Grosse.,ywu@cs.toronto.edu;mren@cs.toronto.edu;rjliao@cs.toronto.edu;rgrosse@cs.toronto.edu,7;6;8,4;4;3,Accept (Poster),0,5,0,yes,10/27/17,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",17;17;17;17,22;22;22;22,,10/27/17,36,22,8,3,38,5,1146;1462;1572;5598,28;18;63;48,13;12;22;27,180;205;204;800,-1;-1
537,ICLR,2018,Memorization Precedes Generation: Learning Unsupervised GANs with Memory Networks,Youngjin Kim;Minjung Kim;Gunhee Kim,youngjin.kim@vision.snu.ac.kr;minjung.kim1994@gmail.com;gunhee@snu.ac.kr,6;6;7,4;4;4,Accept (Poster),0,5,0,yes,10/26/17,Seoul National University;;Seoul National University,46;-1;46,74;-1;74,5;4,10/26/17,15,10,1,2,6,1,335;153;1978,74;54;85,10;6;23,51;14;249,-1;-1
538,ICLR,2018,Universal Agent for Disentangling Environments and Tasks,Jiayuan Mao;Honghua Dong;Joseph J. Lim,mjy14@mails.tsinghua.edu.cn;dhh14@mails.tsinghua.edu.cn;limjj@usc.edu,6;7;6,3;4;3,Accept (Poster),0,0,0,yes,10/27/17,Tsinghua University;Tsinghua University;University of Southern California,10;10;31,30;30;66,,10/27/17,6,5,2,0,0,0,473;63;2915,19;5;51,7;3;20,47;3;263,-1;-1
539,ICLR,2018,Breaking the Softmax Bottleneck: A High-Rank RNN Language Model,Zhilin Yang;Zihang Dai;Ruslan Salakhutdinov;William W. Cohen,zhiliny@cs.cmu.edu;zander.dai@gmail.com;rsalakhu@cs.cmu.edu;wcohen@cs.cmu.edu,7;7;8,5;4;4,Accept (Oral),7,12,0,yes,10/27/17,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1,24;24;24;24,3,10/27/17,188,100,88,7,289,43,4861;2346;66654;22291,90;27;253;423,26;14;81;68,782;426;7740;2614,-1;-1
540,ICLR,2018,Meta-Learning for Semi-Supervised Few-Shot Classification,Mengye Ren;Eleni Triantafillou;Sachin Ravi;Jake Snell;Kevin Swersky;Joshua B. Tenenbaum;Hugo Larochelle;Richard S. Zemel,mren@cs.toronto.edu;eleni@cs.toronto.edu;sachinr@princeton.edu;jsnell@cs.toronto.edu;kswersky@google.com;jbt@mit.edu;hugolarochelle@google.com;zemel@cs.toronto.edu,6;6;6,5;4;4,Accept (Poster),0,4,0,yes,10/27/17,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Princeton University;Department of Computer Science, University of Toronto;Google;Massachusetts Institute of Technology;Google;Department of Computer Science, University of Toronto",17;17;31;17;-1;2;-1;17,22;22;7;22;-1;5;-1;22,6,10/27/17,205,91,92,10,203,36,1519;589;1364;1532;5776;31033;25208;21830,18;31;22;12;52;593;123;208,12;9;10;6;23;83;44;52,216;76;198;388;881;2693;2877;2523,-1;-1
541,ICLR,2018,Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields,Thomas Unterthiner;Bernhard Nessler;Calvin Seward;Günter Klambauer;Martin Heusel;Hubert Ramsauer;Sepp Hochreiter,unterthiner@bioinf.jku.at;nessler@bioinf.jku.at;seward@bioinf.jku.at;klambauer@bioinf.jku.at;mheusel@gmail.com;ramsauer@bioinf.jku.at;hochreit@bioinf.jku.at,5;7;7,4;3;2,Accept (Poster),6,4,0,yes,10/27/17,Johannes Kepler University Linz;Johannes Kepler University Linz;Johannes Kepler University Linz;Johannes Kepler University Linz;Johannes Kepler University Linz;Johannes Kepler University Linz;Johannes Kepler University Linz,468;468;468;468;468;468;468,538;538;538;538;538;538;538,5;4;1,8/29/17,46,21,12,2,113,7,5180;2441;15;62;2016;1570;35205,39;24;7;5;9;5;111,16;12;2;3;7;3;28,1008;636;0;7;618;562;6552,-1;-1
542,ICLR,2018,Memory Architectures in Recurrent Neural Network Language Models,Dani Yogatama;Yishu Miao;Gabor Melis;Wang Ling;Adhiguna Kuncoro;Chris Dyer;Phil Blunsom,dyogatama@google.com;yishu.miao@cs.ox.ac.uk;melisgl@google.com;lingwang@google.com;akuncoro@google.com;cdyer@google.com;pblunsom@google.com,6;5;8,3;5;5,Accept (Poster),2,4,0,yes,10/27/17,Google;University of Oxford;Google;Google;Google;Google;Google,-1;51;-1;-1;-1;-1;-1,-1;1;-1;-1;-1;-1;-1,3;8,10/27/17,33,20,9,1,0,1,3590;701;609;2619;868;21432;11670,41;21;14;757;14;232;144,21;8;8;22;8;61;47,408;114;99;265;109;3156;1352,-1;-1
543,ICLR,2018,Mitigating Adversarial Effects Through Randomization,Cihang Xie;Jianyu Wang;Zhishuai Zhang;Zhou Ren;Alan Yuille,cihangxie306@gmail.com;wjyouch@gmail.com;zhshuai.zhang@gmail.com;zhou.ren@snapchat.com;alan.l.yuille@gmail.com,6;7;6,4;4;3,Accept (Poster),5,4,0,yes,10/27/17,Johns Hopkins University;Baidu;Johns Hopkins University;Snap Inc.;Johns Hopkins University,71;-1;71;-1;71,13;-1;13;-1;13,4,10/27/17,294,175,76,10,3,35,1330;341;1175;2035;32452,28;23;23;29;493,12;5;12;15;80,151;22;118;204;3721,-1;-1
544,ICLR,2018,Learning Awareness Models,Brandon Amos;Laurent Dinh;Serkan Cabi;Thomas Rothörl;Sergio Gómez Colmenarejo;Alistair Muldal;Tom Erez;Yuval Tassa;Nando de Freitas;Misha Denil,bamos@cs.cmu.edu;dinh.laurent@gmail.com;cabi@google.com;tcr@google.com;sergomez@google.com;alimuldal@google.com;etom@google.com;tassa@google.com;nandodefreitas@google.com;mdenil@google.com,7;4;4,4;4;5,Accept (Poster),0,7,0,yes,10/27/17,Carnegie Mellon University;Google;Google;Google;Google;Google;Google;Google;Google;Google,1;-1;-1;-1;-1;-1;-1;-1;-1;-1,24;-1;-1;-1;-1;-1;-1;-1;-1;-1,,10/27/17,16,8,9,1,28,2,448;4751;181;448;1271;387;6580;7211;19365;3366,6;24;12;6;15;9;48;45;184;38,4;12;7;4;10;7;19;24;55;20,21;630;10;21;141;45;1107;1202;1853;286,-1;-1
545,ICLR,2018,Generating Wikipedia by Summarizing Long Sequences,Peter J. Liu*;Mohammad Saleh*;Etienne Pot;Ben Goodrich;Ryan Sepassi;Lukasz Kaiser;Noam Shazeer,peterjliu@google.com;msaleh@google.com;epot@google.com;bgoodrich@google.com;rsepassi@google.com;lukaszkaiser@google.com;noam@google.com,7;8;7,5;3;4,Accept (Poster),0,5,0,yes,10/27/17,Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,,10/27/17,200,109,81,7,0,30,2568;371;204;2640;563;22840;12762,24;51;7;30;6;75;44,12;7;2;11;4;23;19,500;44;30;403;70;3895;2769,-1;-1
546,ICLR,2018,MaskGAN: Better Text Generation via Filling in the _______,William Fedus;Ian Goodfellow;Andrew M. Dai,liam.fedus@gmail.com;goodfellow@google.com;adai@google.com,7;7;7,4;3;5,Accept (Poster),0,8,0,yes,10/27/17,University of Montreal;Google;Google,124;-1;-1,108;-1;-1,3;4;5,10/27/17,182,95,77,2,135,14,665;54752;3687,25;90;49,10;56;19,83;9253;456,-1;-1
547,ICLR,2018,Detecting Statistical Interactions from Neural Network Weights,Michael Tsang;Dehua Cheng;Yan Liu,tsangm@usc.edu;dehuache@usc.edu;yanliu.cs@usc.edu,7;7;7,4;4;5,Accept (Poster),0,7,0,yes,10/26/17,University of Southern California;University of Southern California;University of Southern California,31;31;31,66;66;66,,5/14/17,48,26,18,0,24,8,2303;393;6045,100;34;584,24;11;35,197;33;493,-1;-1
548,ICLR,2018,Model-Ensemble Trust-Region Policy Optimization,Thanard Kurutach;Ignasi Clavera;Yan Duan;Aviv Tamar;Pieter Abbeel,thanard.kurutach@berkeley.edu;iclavera@berkeley.edu;rockyduan@eecs.berkeley.edu;avivt@berkeley.edu;pabbeel@cs.berkeley.edu,7;6;7,4;3;5,Accept (Poster),13,4,0,yes,10/27/17,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5;5,18;18;18;18;18,,10/27/17,133,67,66,5,17,19,190;416;5446;2201;36244,8;15;48;50;433,4;8;19;20;94,27;55;639;331;4370,-1;-1
549,ICLR,2018,Compositional Obverter Communication Learning from Raw Visual Input,Edward Choi;Angeliki Lazaridou;Nando de Freitas,mp2893@gatech.edu;angeliki@google.com;nandodefreitas@google.com,9;3;6,4;4;3,Accept (Poster),0,7,0,yes,10/27/17,Georgia Institute of Technology;Google;Google,13;-1;-1,33;-1;-1,6,10/27/17,31,20,6,0,15,3,974;1714;18797,36;76;184,11;21;54,80;193;1846,-1;-1
550,ICLR,2018,Quantitatively Evaluating GANs With Divergences Proposed for Training,Daniel Jiwoong Im;He Ma;Graham W. Taylor;Kristin Branson,daniel.im@aifounded.com;hma02@uoguelph.ca;gwtaylor@uoguelph.ca;kristinbranson@gmail.com,7;4;7,5;3;4,Accept (Poster),3,7,0,yes,10/27/17,Aifounded;University of Guelph;University of Guelph;Janelia Farm Research Campus- HHMI,-1;291;291;-1,-1;1103;1103;-1,5;4,10/27/17,42,17,17,2,11,4,390;536;5734;1706,23;79;142;41,9;12;31;19,38;30;485;140,-1;-1
551,ICLR,2018,Apprentice: Using Knowledge Distillation Techniques To Improve Low-Precision Network Accuracy,Asit Mishra;Debbie Marr,asit.k.mishra@intel.com;debbie.marr@intel.com,7;7;8,4;4;4,Accept (Poster),0,5,1,yes,10/27/17,Intel;Intel,-1;-1,-1;-1,2,10/27/17,114,45,42,9,12,17,3546;836,85;29,28;10,331;81,-1;-1
552,ICLR,2018,Semi-parametric topological memory for navigation,Nikolay Savinov;Alexey Dosovitskiy;Vladlen Koltun,nikolay.savinov@inf.ethz.ch;adosovitskiy@gmail.com;vkoltun@gmail.com,7;3;7,5;4;4,Accept (Poster),0,11,0,yes,10/27/17,Swiss Federal Institute of Technology;Intel;Intel,9;-1;-1,10;-1;-1,10,10/27/17,87,54,24,4,0,16,689;9371;17484,15;57;188,9;31;62,99;1250;2488,-1;-1
553,ICLR,2018,Automatically Inferring Data Quality for Spatiotemporal Forecasting,Sungyong Seo;Arash Mohegh;George Ban-Weiss;Yan Liu,sungyons@usc.edu;mohegh@usc.edu;banweiss@usc.edu;yanliu.cs@usc.edu,6;6;8,3;4;4,Accept (Poster),0,9,0,yes,10/24/17,University of Southern California;University of Southern California;University of Southern California;University of Southern California,31;31;31;31,66;66;66;66,10,10/24/17,0,0,0,0,0,0,461;56;1475;1159,26;14;76;100,6;5;20;15,45;0;116;111,-1;-1
554,ICLR,2018,"Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks",Pratik Chaudhari;Stefano Soatto,pratikac@ucla.edu;soatto@ucla.edu,8;5;6,5;4;4,Accept (Poster),0,6,0,yes,10/27/17,"University of California, Los Angeles;University of California, Los Angeles",20;20,15;15,1,10/27/17,109,62,22,6,0,14,623;15339,32;458,9;61,75;1430,-1;-1
555,ICLR,2018,Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning,Tianmin Shu;Caiming Xiong;Richard Socher,tianmin.shu@ucla.edu;cxiong@salesforce.com;richard@socher.org,6;6;6,4;3;3,Accept (Poster),0,5,0,yes,10/27/17,"University of California, Los Angeles;SalesForce.com;SalesForce.com",20;-1;-1,15;-1;-1,,10/27/17,32,14,7,0,0,2,257;6187;51920,20;156;180,9;31;48,19;1048;8747,-1;-1
556,ICLR,2018,Not-So-Random Features,Brian Bullins;Cyril Zhang;Yi Zhang,bbullins@cs.princeton.edu;cyril.zhang@cs.princeton.edu;y.zhang@cs.princeton.edu,7;6;4,3;5;5,Accept (Poster),0,6,0,yes,10/27/17,Princeton University;Princeton University;Princeton University,31;31;31,7;7;7,8,10/27/17,4,3,3,0,6,2,381;198;-1,21;18;-1,8;6;-1,63;20;0,-1;-1
557,ICLR,2018,Learning a Generative Model for Validity in Complex Discrete Structures,Dave Janz;Jos van der Westhuizen;Brooks Paige;Matt Kusner;José Miguel Hernández-Lobato,david.janz93@gmail.com;josvdwest@gmail.com;tbpaige@gmail.com;matt.kusner@gmail.com;jmh233@cam.ac.uk,6;7;7,4;3;3,Accept (Poster),0,11,0,yes,10/27/17,University of Cambridge;;Alan Turing Institute;Alan Turing Institute;University of Cambridge,71;-1;-1;-1;71,2;-1;-1;-1;2,5,10/27/17,12,5,0,0,4,0,2284;51;822;2409;3724,126;11;38;44;113,27;5;13;18;28,236;2;87;392;419,-1;-1
558,ICLR,2018,Learning to Multi-Task by Active Sampling,Sahil Sharma*;Ashutosh Kumar Jha*;Parikshit S Hegde;Balaraman Ravindran,sahil@cse.iitm.ac.in;me14b148@smail.iitm.ac.in;ee14b123@ee.iitm.ac.in;ravi@cse.iitm.ac.in,5;7;7,3;3;5,Accept (Poster),3,5,0,yes,10/27/17,Indian Institute of Technology Madras;Indian Institute of Technology Madras;Indian Institute of Technology Madras;Indian Institute of Technology Madras,153;153;153;153,625;625;625;625,6,2/20/17,11,6,4,1,0,3,228;24;28;2415,43;16;5;233,10;3;3;27,13;5;3;196,-1;-1
559,ICLR,2018,Consequentialist conditional cooperation in social dilemmas with imperfect information,Alexander Peysakhovich;Adam Lerer,alexpeys@gmail.com;alerer@fb.com,7;5;6,3;4;4,Accept (Poster),0,4,0,yes,10/17/17,Facebook;Facebook,-1;-1,-1;-1,,10/17/17,25,16,8,0,19,3,1659;7974,46;27,15;15,124;995,-1;-1
560,ICLR,2018,MGAN: Training Generative Adversarial Nets with Multiple Generators,Quan Hoang;Tu Dinh Nguyen;Trung Le;Dinh Phung,qhoang@umass.edu;tu.nguyen@deakin.edu.au;trung.l@deakin.edu.au;dinh.phung@deakin.edu.au,5;7;6,4;5;3,Accept (Poster),0,10,15,yes,10/25/17,"University of Massachusetts, Amherst;Deakin University;Deakin University;Deakin University",30;468;468;468,191;334;334;334,5;4;1,8/8/17,102,54,32,5,20,23,110;725;757;4582,11;61;83;301,3;13;11;30,24;99;76;409,-1;-1
561,ICLR,2018,Residual Connections Encourage Iterative Inference,Stanisław Jastrzebski;Devansh Arpit;Nicolas Ballas;Vikas Verma;Tong Che;Yoshua Bengio,staszek.jastrzebski@gmail.com;devansharpit@gmail.com;ballas.n@gmail.com;vikasverma.iitm@gmail.com;tongcheprivate@gmail.com;yoshua.umontreal@gmail.com,6;5;7,3;5;4,Accept (Poster),0,5,0,yes,10/27/17,University of Montreal;University of Montreal;University of Montreal;;University of Montreal;University of Montreal,124;124;124;-1;124;124,108;108;108;-1;108;108,8,10/13/17,46,26,13,2,41,4,847;880;4871;250;751;200499,27;42;54;45;24;804,12;12;20;8;8;146,99;114;576;14;102;23866,-1;-1
562,ICLR,2018,Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis,Rudy Bunel;Matthew Hausknecht;Jacob Devlin;Rishabh Singh;Pushmeet Kohli,rudy@robots.ox.ac.uk;mahauskn@microsoft.com;jacobdevlin@google.com;risin@microsoft.com;pushmeet@google.com,5;6;7,3;3;3,Accept (Poster),0,5,2,yes,10/26/17,University of Oxford;Microsoft;Google;Microsoft;Google,51;-1;-1;-1;-1,1;-1;-1;-1;-1,3,10/26/17,64,30,31,3,17,13,417;2883;9050;2145;21883,20;38;45;80;313,10;17;20;24;69,51;311;2643;184;2737,-1;-1
563,ICLR,2018,Stochastic Activation Pruning for Robust Adversarial Defense,Guneet S. Dhillon;Kamyar Azizzadenesheli;Zachary C. Lipton;Jeremy D. Bernstein;Jean Kossaifi;Aran Khanna;Animashree Anandkumar,guneetdhillon@utexas.edu;kazizzad@uci.edu;zlipton@cmu.edu;bernstein@caltech.edu;jean.kossaifi@gmail.com;arankhan@amazon.com;animakumar@gmail.com,6;7;6,3;4;4,Accept (Poster),0,4,0,yes,10/27/17,"University of Texas, Austin;University of California, Irvine;Carnegie Mellon University;California Institute of Technology;Imperial College London;Amazon;University of California-Irvine",21;36;1;139;74;-1;36,49;99;24;3;8;-1;99,4,10/27/17,196,114,34,4,6,20,227;686;4853;901;1173;284;5437,8;38;97;263;30;9;187,3;10;29;10;10;5;38,25;99;436;118;128;30;757,-1;-1
564,ICLR,2018,Online Learning Rate Adaptation with Hypergradient Descent,Atilim Gunes Baydin;Robert Cornish;David Martinez Rubio;Mark Schmidt;Frank Wood,gunes@robots.ox.ac.uk;rcornish@robots.ox.ac.uk;david.martinez2@wadh.ox.ac.uk;schmidtm@cs.ubc.ca;fwood@robots.ox.ac.uk,6;7;7,4;3;4,Accept (Poster),1,8,0,yes,10/27/17,University of Oxford;University of Oxford;University of Oxford;University of British Columbia;University of Oxford,51;51;51;34;51,1;1;1;34;1,9,3/14/17,62,32,27,2,90,12,588;186;78;1410;2646,19;15;6;203;77,7;5;3;21;27,52;21;13;137;207,-1;-1
565,ICLR,2018,Temporal Difference Models: Model-Free Deep RL for Model-Based Control,Vitchyr Pong*;Shixiang Gu*;Murtaza Dalal;Sergey Levine,vitchyr@berkeley.edu;sg717@cam.ac.uk;mdalal@berkeley.edu;svlevine@eecs.berkeley.edu,7;4;7,4;4;3,Accept (Poster),0,3,2,yes,10/27/17,University of California Berkeley;University of Cambridge;University of California Berkeley;University of California Berkeley,5;71;5;5,18;2;18;18,,10/27/17,81,44,22,2,0,4,472;3732;310;24235,11;39;9;309,7;20;4;73,47;466;43;3140,-1;-1
566,ICLR,2018,Generative Models of Visually Grounded Imagination,Ramakrishna Vedantam;Ian Fischer;Jonathan Huang;Kevin Murphy,vrama@gatech.edu;iansf@google.com;jonathanhuang@google.com;murphyk@gmail.com,7;7;7,3;4;3,Accept (Poster),0,6,0,yes,10/27/17,Georgia Institute of Technology;Google;Google;Google,13;-1;-1;-1,33;-1;-1;-1,5,5/30/17,61,41,21,2,62,10,4345;2635;2230;15881,19;16;78;83,14;12;21;41,822;356;318;2270,-1;-1
567,ICLR,2018,Spherical CNNs,Taco S. Cohen;Mario Geiger;Jonas Köhler;Max Welling,taco.cohen@gmail.com;geiger.mario@gmail.com;jonas.koehler.ks@gmail.com;m.welling@uva.nl,8;7;9,4;3;4,Accept (Oral),0,7,3,yes,10/27/17,University of Amsterdam;Swiss Federal Institute of Technology Lausanne;;University of Amsterdam,181;468;-1;181,59;38;-1;59,1,10/27/17,184,103,66,7,0,47,1637;565;318;26369,32;24;10;269,17;10;5;57,241;88;52;5056,-1;-1
568,ICLR,2018,Neural Speed Reading via Skim-RNN,Minjoon Seo;Sewon Min;Ali Farhadi;Hannaneh Hajishirzi,minjoon@cs.washington.edu;shmsw25@snu.ac.kr;ali@cs.washington.edu;hannaneh@washington.edu,7;7;8,3;3;3,Accept (Poster),3,10,0,yes,10/27/17,University of Washington;Seoul National University;University of Washington;University of Washington,6;46;6;6,25;74;25;25,3,10/27/17,28,16,9,0,0,5,1711;377;16093;2840,20;12;119;97,14;9;42;24,407;78;2759;595,-1;-1
569,ICLR,2018,On Unifying Deep Generative Models,Zhiting Hu;Zichao Yang;Ruslan Salakhutdinov;Eric P. Xing,zhitinghu@gmail.com;yangtze2301@gmail.com;rsalakhu@cs.cmu.edu;epxing@cs.cmu.edu,7;6;7,4;4;3,Accept (Poster),0,4,0,yes,10/27/17,Carnegie Mellon University;;Carnegie Mellon University;Carnegie Mellon University,1;-1;1;1,24;-1;24;24,5;4,6/2/17,67,41,15,1,216,4,3130;4254;66654;23945,64;40;253;602,29;18;81;75,366;597;7740;2645,-1;-1
570,ICLR,2018,Cascade Adversarial Machine Learning Regularized with a Unified Embedding,Taesik Na;Jong Hwan Ko;Saibal Mukhopadhyay,taesik.na@gatech.edu;jonghwan.ko@gatech.edu;saibal.mukhopadhyay@ece.gatech.edu,6;6;5,4;4;4,Accept (Poster),0,5,0,yes,10/24/17,Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology,13;13;13,33;33;33,4,8/8/17,56,29,16,1,9,5,274;236;4156,31;45;281,9;8;29,21;14;348,-1;-1
571,ICLR,2018,Variance Reduction for Policy Gradient with Action-Dependent Factorized Baselines,Cathy Wu;Aravind Rajeswaran;Yan Duan;Vikash Kumar;Alexandre M Bayen;Sham Kakade;Igor Mordatch;Pieter Abbeel,cathywu@eecs.berkeley.edu;aravraj@cs.washington.edu;rockyduan@eecs.berkeley.edu;vikash@cs.washington.edu;bayen@berkeley.edu;sham@cs.washington.edu;igor.mordatch@gmail.com;pabbeel@cs.berkeley.edu,7;8;6,4;3;4,Accept (Oral),0,4,2,yes,10/27/17,University of California Berkeley;University of Washington;University of California Berkeley;University of Washington;University of California Berkeley;University of Washington;University of Washington;University of California Berkeley,5;6;5;6;5;6;6;5,18;25;18;25;18;25;25;18,,10/27/17,55,26,15,0,28,2,570;901;5676;1210;8356;13691;3069;37166,38;24;55;93;321;198;48;433,12;13;19;18;46;58;27;94,43;85;650;98;509;1982;351;4474,-1;-1
572,ICLR,2018,Lifelong Learning with Dynamically Expandable Networks,Jaehong Yoon;Eunho Yang;Jeongtae Lee;Sung Ju Hwang,mmvc98@unist.ac.kr;eunhoy@kaist.ac.kr;jtlee@unist.ac.kr;sjhwang82@kaist.ac.kr,7;6;8,3;3;2,Accept (Poster),0,4,0,yes,10/27/17,Ulsan National Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Ulsan National Institute of Science and Technology;Korea Advanced Institute of Science and Technology,468;21;468;21,230;95;230;95,,8/4/17,174,108,47,1,13,30,248;1028;175;1093,14;74;19;71,4;16;2;16,38;164;30;124,-1;-1
573,ICLR,2018,A Simple Neural Attentive Meta-Learner,Nikhil Mishra;Mostafa Rohaninejad;Xi Chen;Pieter Abbeel,nmishra@berkeley.edu;rohaninejadm@berkeley.edu;adslcx@gmail.com;pabbeel@gmail.com,7;6;6,4;3;3,Accept (Poster),1,7,0,yes,10/27/17,University of California Berkeley;University of California Berkeley;covariant.ai;University of California-Berkeley,5;5;-1;5,18;18;-1;18,6;8,7/11/17,348,174,120,9,267,42,655;520;13266;36244,20;5;444;433,9;3;40;94,78;69;1537;4370,-1;-1
574,ICLR,2018,Emergence of grid-like representations by training recurrent neural networks to perform spatial localization,Christopher J. Cueva;Xue-Xin Wei,ccueva@gmail.com;weixxpku@gmail.com,8;9;8,4;4;4,Accept (Poster),0,4,0,yes,10/27/17,Columbia University;Columbia University,15;15,14;14,,10/27/17,68,25,6,1,233,2,276;330,9;17,6;6,18;16,-1;-1
575,ICLR,2018,Certified Defenses against Adversarial Examples ,Aditi Raghunathan;Jacob Steinhardt;Percy Liang,aditir@stanford.edu;jsteinhardt@cs.stanford.edu;pliang@cs.stanford.edu,8;8;5,4;4;3,Accept (Poster),4,5,1,yes,10/27/17,Stanford University;Stanford University;Stanford University,4;4;4,3;3;3,4,10/27/17,375,242,99,17,0,42,625;2217;12558,19;57;144,7;19;47,78;201;2038,-1;-1
576,ICLR,2018,Emergence of Linguistic Communication from  Referential Games with Symbolic and Pixel Input,Angeliki Lazaridou;Karl Moritz Hermann;Karl Tuyls;Stephen Clark,angeliki@google.com;kmh@google.com;karltuyls@google.com;clarkstephen@google.com,7;9;5,4;5;4,Accept (Oral),0,6,0,yes,10/27/17,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,,10/27/17,67,50,21,3,13,15,1714;5046;3671;6402,76;41;282;189,21;21;33;43,193;683;272;642,-1;-1
577,ICLR,2018,Unsupervised Representation Learning by Predicting Image Rotations,Spyros Gidaris;Praveer Singh;Nikos Komodakis,spyros.gidaris@enpc.fr;praveer.singh@enpc.fr;nikos.komodakis@enpc.fr,6;6;6,5;4;3,Accept (Poster),0,7,0,yes,10/27/17,ENPC;ENPC;ENPC,468;468;468,280;280;280,2,10/27/17,344,208,152,9,0,82,1344;408;9122,13;19;121,9;5;38,217;85;1292,-1;-1
578,ICLR,2018,Learn to Pay Attention,Saumya Jetley;Nicholas A. Lord;Namhoon Lee;Philip H. S. Torr,saumya.jetley@stx.ox.ac.uk;nicklord@robots.ox.ac.uk;namhoon.lee@eng.ox.ac.uk;philip.torr@eng.ox.ac.uk,5;6;6,4;4;4,Accept (Poster),6,8,0,yes,10/27/17,University of Oxford;University of Oxford;University of Oxford;University of Oxford,51;51;51;51,1;1;1;1,4;2,10/27/17,98,55,32,1,24,14,289;259;875;27914,15;24;32;354,9;8;12;83,29;24;153;3821,-1;-1
579,ICLR,2018,Auto-Encoding Sequential Monte Carlo,Tuan Anh Le;Maximilian Igl;Tom Rainforth;Tom Jin;Frank Wood,tuananh@robots.ox.ac.uk;maximilian.igl@gmail.com;twgr@robots.ox.ac.uk;tom@jin.me.uk;fwood@robots.ox.ac.uk,7;3;7,3;2;4,Accept (Poster),0,7,0,yes,10/27/17,University of Oxford;University of Oxford;University of Oxford;University of Oxford;University of Oxford,51;51;51;51;51,1;1;1;1;1,5;1,5/29/17,58,29,18,2,11,12,1693;268;435;60;510,141;12;43;3;38,18;6;11;2;10,109;43;58;12;62,-1;-1
580,ICLR,2018,Divide-and-Conquer Reinforcement Learning,Dibya Ghosh;Avi Singh;Aravind Rajeswaran;Vikash Kumar;Sergey Levine,dibya.ghosh@berkeley.edu;avisingh@cs.berkeley.edu;aravraj@cs.washington.edu;vikash@cs.washington.edu;svlevine@eecs.berkeley.edu,7;7;4,4;4;4,Accept (Poster),0,4,1,yes,10/27/17,University of California Berkeley;University of California Berkeley;University of Washington;University of Washington;University of California Berkeley,5;5;6;6;5,18;18;25;25;18,,10/27/17,47,25,13,1,16,2,95;660;868;1167;24235,12;24;24;93;309,4;9;13;18;73,3;92;83;92;3140,-1;-1
581,ICLR,2018,SMASH: One-Shot Model Architecture Search through HyperNetworks,Andrew Brock;Theo Lim;J.M. Ritchie;Nick Weston,ajb5@hw.ac.uk;t.lim@hw.ac.uk;j.m.ritchie@hw.ac.uk;nick.weston@renishaw.com,7;7;6,3;4;2,Accept (Poster),0,2,0,yes,9/29/17,Heriot-Watt University;Heriot-Watt University;Heriot-Watt University;Renishaw,291;291;291;-1,363;363;363;-1,,8/17/17,272,125,109,1,202,33,2041;2120;1170;824,88;133;47;17,15;19;16;6,238;187;122;96,-1;-1
582,ICLR,2018,Recasting Gradient-Based Meta-Learning as Hierarchical Bayes,Erin Grant;Chelsea Finn;Sergey Levine;Trevor Darrell;Thomas Griffiths,eringrant@berkeley.edu;cbfinn@eecs.berkeley.edu;svlevine@eecs.berkeley.edu;trevor@eecs.berkeley.edu;tom_griffiths@berkeley.edu,6;7;7,3;3;3,Accept (Poster),1,8,0,yes,10/26/17,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5;5,18;18;18;18;18,11;6,10/26/17,149,69,56,4,30,17,801;7655;24235;88251;21240,37;98;309;558;437,9;33;73;109;70,51;1024;3140;11352;2163,-1;-1
583,ICLR,2018,Neural Sketch Learning for Conditional Program Generation,Vijayaraghavan Murali;Letao Qi;Swarat Chaudhuri;Chris Jermaine,vijay@rice.edu;letao.qi@rice.edu;swarat@rice.edu;cmj4@rice.edu,7;8;7,2;4;3,Accept (Oral),0,4,0,yes,10/27/17,Rice University;Rice University;Rice University;Rice University,85;85;85;85,86;86;86;86,,3/16/17,44,19,9,0,0,1,343;52;2651;2250,22;3;97;98,8;2;26;27,23;1;182;235,-1;-1
584,ICLR,2018,Synthetic and Natural Noise Both Break Neural Machine Translation,Yonatan Belinkov;Yonatan Bisk,belinkov@mit.edu;ybisk@yonatanbisk.com,7;7;8,4;4;4,Accept (Oral),0,7,0,yes,10/27/17,Massachusetts Institute of Technology;University of Washington,2;6,5;25,3,10/27/17,203,126,34,10,0,20,1532;1102,63;43,21;16,165;150,-1;-1
585,ICLR,2018,Policy Optimization by Genetic Distillation ,Tanmay Gangwani;Jian Peng,gangwan2@illinois.edu;jianpeng@illinois.edu,8;6;3,5;4;4,Accept (Poster),0,9,0,yes,10/27/17,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",3;3,37;37,,10/27/17,13,9,2,0,10,0,58;2078,14;120,4;23,5;204,-1;-1
586,ICLR,2018,Active Neural Localization,Devendra Singh Chaplot;Emilio Parisotto;Ruslan Salakhutdinov,chaplot@cs.cmu.edu;eparisot@andrew.cmu.edu;rsalakhu@cs.cmu.edu,6;8;7,4;5;4,Accept (Poster),0,4,1,yes,10/25/17,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1,24;24;24,,10/25/17,36,20,12,0,152,2,764;868;66654,29;17;253,11;9;81,98;76;7740,-1;-1
587,ICLR,2018,Reinforcement Learning on Web Interfaces using Workflow-Guided Exploration,Evan Zheran Liu;Kelvin Guu;Panupong Pasupat;Tianlin Shi;Percy Liang,evzliu@gmail.com;kguu@stanford.edu;ppasupat@cs.stanford.edu;tianlins@cs.stanford.edu;pliang@cs.stanford.edu,7;6;7,4;3;3,Accept (Poster),0,8,0,yes,10/27/17,Stanford University;Stanford University;Stanford University;Stanford University;Stanford University,4;4;4;4;4,3;3;3;3;3,,10/27/17,16,14,7,1,18,3,122;560;661;626;12558,8;12;22;13;144,4;8;14;8;47,23;89;106;81;2038,-1;-1
588,ICLR,2018,Training GANs with Optimism,Constantinos Daskalakis;Andrew Ilyas;Vasilis Syrgkanis;Haoyang Zeng,costis@mit.edu;ailyas@mit.edu;vasy@microsoft.com;haoyangz@mit.edu,7;8;6,4;4;4,Accept (Poster),3,6,0,yes,10/27/17,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Microsoft;Massachusetts Institute of Technology,2;2;-1;2,5;5;-1;5,5;4;1,10/27/17,153,91,58,7,17,35,4862;1848;1437;694,168;28;101;26,40;15;22;9,502;259;160;60,-1;-1
589,ICLR,2018,"A Compressed Sensing View of Unsupervised Text Embeddings, Bag-of-n-Grams, and LSTMs",Sanjeev Arora;Mikhail Khodak;Nikunj Saunshi;Kiran Vodrahalli,arora@cs.princeton.edu;mkhodak@princeton.edu;nsaunshi@cs.princeton.edu;kiran.vodrahalli@columbia.edu,7;7;6,3;1;4,Accept (Poster),0,6,0,yes,10/27/17,Princeton University;Princeton University;Princeton University;Columbia University,31;31;31;15,7;7;7;14,1,10/27/17,24,14,7,1,0,4,15810;348;145;119,349;23;8;18,61;8;4;5,1791;49;32;18,-1;-1
590,ICLR,2018,Scalable Private Learning with PATE,Nicolas Papernot;Shuang Song;Ilya Mironov;Ananth Raghunathan;Kunal Talwar;Ulfar Erlingsson,ngp5056@cse.psu.edu;shs037@eng.ucsd.edu;mironov@google.com;pseudorandom@google.com;kunal@google.com;ulfar@google.com,6;6;7,1;4;3,Accept (Poster),0,3,0,yes,10/27/17,"Pennsylvania State University;University of California, San Diego;Google;Google;Google;Google",40;11;-1;-1;-1;-1,77;31;-1;-1;-1;-1,1,10/27/17,123,64,40,5,43,15,9053;422;5431;952;16245;5200,66;39;64;25;144;65,27;8;27;11;41;29,1040;35;741;129;2137;784,-1;-1
591,ICLR,2018,Deep Bayesian Bandits Showdown:  An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling,Carlos Riquelme;George Tucker;Jasper Snoek,rikel@google.com;gjt@google.com;jsnoek@google.com,5;7;6,5;4;4,Accept (Poster),0,9,0,yes,10/27/17,Google;Google;Google,-1;-1;-1,-1;-1;-1,11,10/27/17,72,41,37,2,7,22,737;2557;4795,53;74;61,15;21;18,86;285;505,-1;-1
592,ICLR,2018,Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training,Yujun Lin;Song Han;Huizi Mao;Yu Wang;Bill Dally,yujunlin@stanford.edu;songhan@stanford.edu;huizi@stanford.edu;yu-wang@mail.tsinghua.edu.cn;dally@stanford.edu,7;6;7,4;5;4,Accept (Poster),6,24,5,yes,10/27/17,Stanford University;Stanford University;Stanford University;Tsinghua University;Stanford University,4;4;4;10;4,3;3;3;30;3,3,10/27/17,280,158,94,10,52,44,462;792;5779;-1;30017,17;48;22;-1;268,5;12;14;-1;67,80;65;743;0;3727,-1;-1
593,ICLR,2018,The power of deeper networks for expressing natural functions,David Rolnick;Max Tegmark,drolnick@mit.edu;tegmark@mit.edu,7;6;6,4;4;4,Accept (Poster),0,3,0,yes,10/27/17,Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2,5;5,1,5/16/17,67,38,1,2,43,1,609;7065,37;269,10;42,39;520,-1;-1
594,ICLR,2018,Auto-Conditioned Recurrent Networks for Extended Complex Human Motion Synthesis,Yi Zhou;Zimo Li;Shuangjiu Xiao;Chong He;Zeng Huang;Hao Li,zhou859@usc.edu;zimoli@usc.edu;xsjiu99@sjtu.edu.cn;sal@sjtu.edu.cn;zenghuang@usc.edu;hao@hao-li.com,7;7;6,3;5;5,Accept (Poster),0,1,0,yes,10/27/17,University of Southern California;University of Southern California;Shanghai Jiao Tong University;Shanghai Jiao Tong University;University of Southern California;Hao-li,31;31;57;57;31;-1,66;66;188;188;66;-1,,7/17/17,51,29,22,2,0,9,12300;1468;238;147;458;998,842;21;69;23;38;146,44;10;8;5;12;15,673;352;28;19;34;50,-1;-1
595,ICLR,2018,A Hierarchical Model for Device Placement,Azalia Mirhoseini;Anna Goldie;Hieu Pham;Benoit Steiner;Quoc V. Le;Jeff Dean,azalia@google.com;agoldie@google.com;hyhieu@cmu.edu;bsteiner@google.com;qvl@google.com;jeff@google.com,5;5;8,4;4;5,Accept (Poster),0,6,1,yes,10/27/17,Google;Google;Carnegie Mellon University;Google;Google;Google,-1;-1;1;-1;-1;-1,-1;-1;24;-1;-1;-1,3;2;10,10/27/17,47,18,19,2,0,6,1054;498;4985;14484;47174;1958,56;18;18;15;193;27,14;6;11;7;79;9,76;44;765;1790;5944;288,-1;-1
596,ICLR,2018,Improving GANs Using Optimal Transport,Tim Salimans;Han Zhang;Alec Radford;Dimitris Metaxas,tim@openai.com;han.zhang@cs.rutgers.edu;alec@openai.com;dnm@cs.rutgers.edu,8;6;6,4;2;3,Accept (Poster),0,4,0,yes,10/26/17,OpenAI;Rutgers University;OpenAI;Rutgers University,-1;34;-1;34,-1;172;-1;172,5;4,10/26/17,102,58,33,1,46,12,6576;4644;13991;21209,35;278;18;659,13;33;12;73,1026;485;2777;1700,-1;-1
597,ICLR,2018,Improving GAN Training via Binarized Representation Entropy (BRE) Regularization,Yanshuai Cao;Gavin Weiguang Ding;Kry Yik-Chau Lui;Ruitong Huang,yanshuai.cao@borealisai.com;gavin.ding@borealisai.com;yikchau.y.lui@borealisai.com;ruitong.huang@borealisai.com,6;7;4,3;4;3,Accept (Poster),0,5,1,yes,10/27/17,Borealis AI;Borealis AI;Borealis AI;Borealis AI,-1;-1;-1;-1,-1;-1;-1;-1,5;4,10/27/17,12,9,3,0,21,1,303;226;59;376,20;21;3;29,7;9;2;10,34;10;11;29,-1;-1
598,ICLR,2018,Hierarchical Density Order Embeddings,Ben Athiwaratkun;Andrew Gordon Wilson,pa338@cornell.edu;andrew@cornell.edu,4;6;8,3;4;5,Accept (Poster),0,5,1,yes,10/27/17,Cornell University;Cornell University,7;7,19;19,3;10,10/27/17,22,12,5,1,74,2,403;2721,11;102,9;27,44;331,-1;-1
599,ICLR,2018,Stabilizing Adversarial Nets with Prediction Methods,Abhay Yadav;Sohil Shah;Zheng Xu;David Jacobs;Tom Goldstein,jaiabhay@cs.umd.edu;sohilas@umd.edu;xuzh@cs.umd.edu;djacobs@umiacs.umd.edu;tomg@cs.umd.edu,4;9;7,4;4;4,Accept (Poster),0,15,0,yes,10/27/17,"University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park",12;12;12;12;12,69;69;69;69;69,4,5/20/17,39,22,11,0,141,3,154;120;646;11693;5742,13;19;44;215;98,6;5;14;48;27,14;13;82;1612;720,-1;-1
600,ICLR,2018,Boosting the Actor with Dual Critic,Bo Dai;Albert Shaw;Niao He;Lihong Li;Le Song,bohr.dai@gmail.com;ashaw596@gatech.edu;niaohe@illinois.edu;lihongli.cs@gmail.com;lsong@cc.gatech.edu,7;6;5,4;3;4,Accept (Poster),0,4,0,yes,10/27/17,"Georgia Institute of Technology;Georgia Institute of Technology;University of Illinois, Urbana Champaign;Google;Georgia Institute of Technology",13;13;3;-1;13,33;33;37;-1;33,,10/27/17,27,21,7,0,45,5,3445;150;867;10475;9225,397;9;48;242;328,30;4;13;46;52,273;11;133;1186;1095,-1;-1
601,ICLR,2018,On the Information Bottleneck Theory of Deep Learning,Andrew Michael Saxe;Yamini Bansal;Joel Dapello;Madhu Advani;Artemy Kolchinsky;Brendan Daniel Tracey;David Daniel Cox,asaxe@fas.harvard.edu;ybansal@g.harvard.edu;dapello@g.harvard.edu;madvani@fas.harvard.edu;artemyk@gmail.com;tracey.brendan@gmail.com;davidcox@fas.harvard.edu,6;7;7,2;3;3,Accept (Poster),7,8,2,yes,10/27/17,Harvard University;Harvard University;Harvard University;Harvard University;Santa Fe Institute;;Harvard University,37;37;37;37;-1;-1;37,6;6;6;6;-1;-1;6,8,10/27/17,139,90,32,11,0,29,2009;191;144;262;526;587;5939,32;6;3;15;51;28;109,12;3;2;6;12;11;31,162;36;29;31;53;52;418,-1;-1
602,ICLR,2018,Syntax-Directed Variational Autoencoder for Structured Data,Hanjun Dai;Yingtao Tian;Bo Dai;Steven Skiena;Le Song,hanjundai@gatech.edu;yittian@cs.stonybrook.edu;bohr.dai@gmail.com;skiena@cs.stonybrook.edu;lsong@cc.gatech.edu,3;5;7,2;1;3,Accept (Poster),0,11,0,yes,10/27/17,"Georgia Institute of Technology;State University of New York, Stony Brook;Georgia Institute of Technology;State University of New York, Stony Brook;Georgia Institute of Technology",13;42;13;42;13,33;258;33;258;33,5,10/27/17,96,51,30,3,121,15,1834;555;3445;10689;9225,57;44;397;351;328,17;11;30;42;52,270;61;273;1609;1095,-1;-1
603,ICLR,2018,Do GANs learn the distribution? Some Theory and Empirics,Sanjeev Arora;Andrej Risteski;Yi Zhang,arora@cs.princeton.edu;risteski@cs.princeton.edu;y.zhang@cs.princeton.edu,7;6;7,4;4;3,Accept (Poster),0,4,0,yes,10/27/17,Princeton University;Princeton University;Princeton University,31;31;31,7;7;7,5;4;1,10/27/17,65,38,10,1,0,8,634;16205;-1,35;349;-1,12;61;-1,68;1802;0,-1;-1
604,ICLR,2018,TRUNCATED HORIZON POLICY SEARCH: COMBINING REINFORCEMENT LEARNING & IMITATION LEARNING,Wen Sun;J. Andrew Bagnell;Byron Boots,wensun@cs.cmu.edu;dbagnell@cs.cmu.edu;bboots@cc.gatech.edu,7;6;3,3;4;5,Accept (Poster),0,5,0,yes,10/27/17,Carnegie Mellon University;Carnegie Mellon University;Georgia Institute of Technology,1;1;13,24;24;33,,10/27/17,40,23,16,1,14,2,627;14006;2507,36;196;140,13;53;28,68;1407;254,-1;-1
605,ICLR,2018,Understanding Deep Neural Networks with Rectified Linear Units,Raman Arora;Amitabh Basu;Poorya Mianjy;Anirbit Mukherjee,arora@cs.jhu.edu;basu.amitabh@jhu.edu;mianjy@jhu.edu;amukhe14@jhu.edu,6;6;7,4;5;4,Accept (Poster),0,0,0,yes,10/27/17,Johns Hopkins University;Johns Hopkins University;Johns Hopkins University;Johns Hopkins University,71;71;71;71,13;13;13;13,1,11/4/16,178,103,42,8,80,26,2637;991;264;244,85;86;12;11,22;17;5;4,441;73;36;34,-1;-1
606,ICLR,2018,Deep Neural Networks as Gaussian Processes,Jaehoon Lee;Yasaman Bahri;Roman Novak;Samuel S. Schoenholz;Jeffrey Pennington;Jascha Sohl-Dickstein,jaehlee@google.com;yasamanb@google.com;romann@google.com;schsam@google.com;jpennin@google.com;jaschasd@google.com,4;6;7,4;4;3,Accept (Poster),0,8,0,yes,10/27/17,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,11,10/27/17,238,145,54,15,0,49,563;951;693;2970;16048;4820,55;30;12;70;51;101,8;10;8;20;20;33,88;116;102;374;2603;672,-1;-1
607,ICLR,2018,Deep Complex Networks,Chiheb Trabelsi;Olexa Bilaniuk;Ying Zhang;Dmitriy Serdyuk;Sandeep Subramanian;Joao Felipe Santos;Soroush Mehri;Negar Rostamzadeh;Yoshua Bengio;Christopher J Pal,chiheb.trabelsi@polymtl.ca;olexa.bilaniuk@umontreal.ca;ying.zhang@umontreal.ca;serdyuk@iro.umontreal.ca;sandeep.subramanian.1@umontreal.ca;jfsantos@emt.inrs.ca;soroush.mehri@microsoft.com;negar@elementai.com;yoshua.bengio@umontreal.ca;christopher.pal@polymtl.ca,7;8;4,4;4;4,Accept (Poster),0,5,0,yes,10/27/17,Polytechnique Montreal;University of Montreal;University of Montreal;University of Montreal;University of Montreal;Institut national de la recherche scientifique;Microsoft;Element AI;University of Montreal;Polytechnique Montreal,364;124;124;124;124;-1;-1;-1;124;364,108;108;108;108;108;-1;-1;-1;108;108,2,5/27/17,146,84,47,3,94,28,275;272;-1;3822;2506;762;720;367;200499;8128,11;14;-1;15;17;43;8;31;804;120,4;7;-1;11;11;13;6;10;146;33,42;36;0;303;495;82;110;51;23866;758,-1;-1
608,ICLR,2018,Non-Autoregressive Neural Machine Translation,Jiatao Gu;James Bradbury;Caiming Xiong;Victor O.K. Li;Richard Socher,jiataogu@eee.hku.hk;james.bradbury@salesforce.com;cxiong@salesforce.com;vli@eee.hku.hk;rsocher@salesforce.com,7;7;6,4;4;4,Accept (Poster),2,3,0,yes,10/27/17,The University of Hong Kong;SalesForce.com;SalesForce.com;The University of Hong Kong;SalesForce.com,90;-1;-1;90;-1,40;-1;-1;40;-1,3,10/27/17,120,68,58,10,8,35,1409;1288;6187;8857;51920,40;14;156;563;180,14;7;31;42;48,211;229;1048;738;8747,-1;-1
609,ICLR,2018,Depthwise Separable Convolutions for Neural Machine Translation,Lukasz Kaiser;Aidan N. Gomez;Francois Chollet,lukaszkaiser@google.com;aidan.n.gomez@gmail.com;fchollet@google.com,5;7;7,4;4;3,Accept (Poster),0,4,0,yes,10/27/17,"Google;Department of Computer Science, University of Toronto;Google",-1;17;-1,-1;22;-1,3,6/9/17,101,47,45,3,211,10,22214;9541;8386,75;15;98,24;8;37,3834;2364;850,-1;-1
610,ICLR,2018,Learning a neural response metric for retinal prosthesis,Nishal P Shah;Sasidhar Madugula;EJ Chichilnisky;Yoram Singer;Jonathon Shlens,nishalps@stanford.edu;sasidhar@stanford.edu;ej@stanford.edu;singer@google.com;shlens@google.com,5;6;7,4;3;4,Accept (Poster),0,0,0,yes,10/27/17,Stanford University;Stanford University;Stanford University;Google;Google,4;4;4;-1;-1,3;3;3;-1;-1,5,10/27/17,4,2,0,0,6,0,30;63;439;34362;23315,14;10;24;211;80,4;5;11;58;37,0;3;27;4924;3983,-1;-1
611,ICLR,2018,Distributed Distributional Deterministic Policy Gradients,Gabriel Barth-Maron;Matthew W. Hoffman;David Budden;Will Dabney;Dan Horgan;Dhruva TB;Alistair Muldal;Nicolas Heess;Timothy Lillicrap,gabrielbm@google.com;mwhoffman@google.com;budden@google.com;wdabney@google.com;horgan@google.com;dhruvat@google.com;alimuldal@google.com;heess@google.com;countzero@google.com,9;6;5,4;5;4,Accept (Poster),2,6,0,yes,10/27/17,Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1,,10/27/17,133,66,62,2,12,19,576;1717;1232;1733;1357;569;387;11276;23365,14;34;55;25;9;4;9;104;74,8;17;14;13;8;4;7;37;39,70;180;128;331;225;61;45;1610;2855,-1;-1
612,ICLR,2018,Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples,Ashwin Kalyan;Abhishek Mohta;Oleksandr Polozov;Dhruv Batra;Prateek Jain;Sumit Gulwani,ashwinkv@gatech.edu;t-abmoht@microsoft.com;polozov@microsoft.com;dbatra@gatech.edu;prajain@microsoft.com;sumitg@microsoft.com,6;6;8,3;4;3,Accept (Poster),0,5,0,yes,10/27/17,Georgia Institute of Technology;Microsoft;Microsoft;Georgia Institute of Technology;Microsoft;Microsoft,13;-1;-1;13;-1;-1,33;-1;-1;33;-1;-1,8,10/27/17,48,25,15,2,6,1,48;51;543;1619;94;7619,1;3;22;26;26;203,1;2;9;15;5;50,1;1;46;278;5;606,-1;-1
613,ICLR,2018,Learning an Embedding Space for Transferable Robot Skills,Karol Hausman;Jost Tobias Springenberg;Ziyu Wang;Nicolas Heess;Martin Riedmiller,hausmankarol@gmail.com;springenberg@google.com;ziyu@google.com;heess@google.com;riedmiller@google.com,7;7;7,4;4;5,Accept (Poster),0,6,0,yes,10/27/17,University of Southern California;Google;Google;Google;Google,31;-1;-1;-1;-1,66;-1;-1;-1;-1,,10/27/17,103,48,26,4,0,6,840;7210;4187;11276;25118,62;54;51;104;190,14;28;21;37;39,50;775;484;1610;3504,-1;-1
614,ICLR,2018,Multi-Mention Learning for Reading Comprehension with Neural Cascades,Swabha Swayamdipta;Ankur P. Parikh;Tom Kwiatkowski,swabha@cs.cmu.edu;aparikh@google.com;tomkwiat@google.com,7;5;6,4;4;4,Accept (Poster),0,6,0,yes,10/27/17,Carnegie Mellon University;Google;Google,1;-1;-1,24;-1;-1,,10/27/17,18,10,5,0,40,1,1013;1474;214,24;43;40,12;16;18,133;184;0,-1;-1
615,ICLR,2018,Learning Differentially Private Recurrent Language Models,H. Brendan McMahan;Daniel Ramage;Kunal Talwar;Li Zhang,mcmahan@google.com;dramage@google.com;kunal@google.com;liqzhang@google.com,7;7;8,4;2;4,Accept (Poster),0,3,0,yes,10/27/17,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,3,10/18/17,189,111,69,9,6,32,5767;20590;16245;4886,68;46;144;662,32;30;41;32,843;3055;2137;182,-1;-1
616,ICLR,2018,Learning Approximate Inference Networks for Structured Prediction,Lifu Tu;Kevin Gimpel,lifu@ttic.edu;kgimpel@ttic.edu,7;5;9,5;3;4,Accept (Poster),0,5,0,yes,10/27/17,Toyota Technological Institute at Chicago;Toyota Technological Institute at Chicago,-1;-1,-1;-1,3,10/27/17,25,8,15,4,12,6,137;5614,11;99,5;30,28;812,-1;-1
617,ICLR,2018,Emergent Communication through Negotiation,Kris Cao;Angeliki Lazaridou;Marc Lanctot;Joel Z Leibo;Karl Tuyls;Stephen Clark,kc391@cam.ac.uk;angeliki@google.com;lanctot@google.com;jzl@google.com;karltuyls@google.com;clarkstephen@google.com,6;7;5,3;4;4,Accept (Poster),7,5,0,yes,10/27/17,University of Cambridge;Google;Google;Google;Google;Google,71;-1;-1;-1;-1;-1,2;-1;-1;-1;-1;-1,,10/27/17,49,39,1,3,42,5,141;1714;10441;3450;3671;6402,7;76;70;75;282;189,4;21;23;29;33;43,15;193;728;346;272;642,-1;-1
618,ICLR,2018,Ensemble Adversarial Training: Attacks and Defenses,Florian Tramèr;Alexey Kurakin;Nicolas Papernot;Ian Goodfellow;Dan Boneh;Patrick McDaniel,tramer@cs.stanford.edu;alexey@kurakin.me;ngp5056@cse.psu.edu;goodfellow@google.com;dabo@cs.stanford.edu;mcdaniel@cse.psu.edu,6;6;6,2;4;4,Accept (Poster),0,4,1,yes,10/27/17,Stanford University;Google;Pennsylvania State University;Google;Stanford University;Pennsylvania State University,4;-1;40;-1;4;40,3;-1;77;-1;3;77,4,5/19/17,818,493,252,17,47,146,2306;4456;9053;58206;50378;21786,36;14;66;90;414;273,18;9;27;56;97;62,319;813;1040;9628;6287;2299,-1;-1
619,ICLR,2018,Maximum a Posteriori Policy Optimisation,Abbas Abdolmaleki;Jost Tobias Springenberg;Yuval Tassa;Remi Munos;Nicolas Heess;Martin Riedmiller,abbas.abdolmaleky@gmail.com;springenberg@google.com;heess@google.com;tassa@google.com;munos@google.com,7;6;5,5;1;4,Accept (Poster),4,5,2,yes,10/27/17,;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,,10/27/17,111,51,55,9,8,22,520;7210;7061;9222;11276;25118,45;54;45;190;104;190,10;28;24;53;37;39,55;775;1173;1301;1610;3504,-1;-1
620,ICLR,2018,Global Optimality Conditions for Deep Neural Networks,Chulhee Yun;Suvrit Sra;Ali Jadbabaie,chulheey@mit.edu;suvrit@mit.edu;jadbabai@mit.edu,5;7;8,5;4;5,Accept (Poster),0,3,0,yes,10/27/17,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2,5;5;5,1;9,7/8/17,75,57,4,6,24,3,194;6325;14884,22;163;300,6;39;46,13;997;1007,-1;-1
621,ICLR,2018,Leave no Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning,Benjamin Eysenbach;Shixiang Gu;Julian Ibarz;Sergey Levine,eysenbach@google.com;sg717@cam.ac.uk;julianibarz@google.com;slevine@google.com,7;6;5;7,4;5;4;4,Accept (Poster),0,10,0,yes,10/27/17,Google;University of Cambridge;Google;Google,-1;71;-1;-1,-1;2;-1;-1,,10/27/17,35,19,9,0,0,3,347;3732;2860;24235,16;39;22;309,6;20;13;73,52;466;349;3140,-1;-1
622,ICLR,2018,Sensitivity and Generalization in Neural Networks: an Empirical Study,Roman Novak;Yasaman Bahri;Daniel A. Abolafia;Jeffrey Pennington;Jascha Sohl-Dickstein,romann@google.com;yasamanb@google.com;danabo@google.com;jpennin@google.com;jaschasd@google.com,8;5;4,4;3;5,Accept (Poster),0,9,4,yes,10/27/17,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,8,10/27/17,150,89,20,13,149,14,693;951;251;16048;4820,12;30;8;51;101,8;10;4;20;33,102;116;29;2603;672,-1;-1
623,ICLR,2018,Combining Symbolic Expressions and Black-box Function Evaluations in Neural Programs,Forough Arabshahi;Sameer Singh;Animashree Anandkumar,farabsha@uci.edu;sameer@uci.edu;animakumar@gmail.com,6;8;5,4;3;4,Accept (Poster),0,7,0,yes,10/27/17,"University of California, Irvine;University of California, Irvine;University of California-Irvine",36;36;36,99;99;99,8,10/27/17,10,6,3,0,22,0,35;5504;5777,15;118;219,4;27;38,2;749;801,-1;-1
624,ICLR,2018,Dynamic Neural Program Embeddings for Program Repair,Ke Wang;Rishabh Singh;Zhendong Su,kbwang@ucdavis.edu;risin@microsoft.com;su@cs.ucdavis.edu,6;7;7,2;3;4,Accept (Poster),0,8,0,yes,10/27/17,"University of California, Davis;Microsoft;University of California, Davis",78;-1;78,54;-1;54,,10/27/17,54,30,10,1,12,4,1703;2145;6975,69;80;165,8;24;42,163;184;732,-1;-1
625,ICLR,2018,Generating Natural Adversarial Examples,Zhengli Zhao;Dheeru Dua;Sameer Singh,zhengliz@uci.edu;ddua@uci.edu;sameer@uci.edu,6;7;6,3;4;3,Accept (Poster),3,5,0,yes,10/27/17,"University of California, Irvine;University of California, Irvine;University of California, Irvine",36;36;36,99;99;99,3;4;5,10/27/17,181,116,42,2,92,20,415;299;5504,7;12;118,4;4;27,28;49;749,-1;-1
626,ICLR,2018,mixup: Beyond Empirical Risk Minimization,Hongyi Zhang;Moustapha Cisse;Yann N. Dauphin;David Lopez-Paz,hongyiz@mit.edu;moustaphacisse@fb.com;ynd@fb.com;dlp@fb.com,6;7;6,4;4;4,Accept (Poster),1,6,0,yes,10/27/17,Massachusetts Institute of Technology;Facebook;Facebook;Facebook,2;-1;-1;-1,5;-1;-1;-1,5;4;8,10/25/17,758,323,413,20,104,169,870;2917;8433;2380,24;48;43;45,3;19;27;19,184;401;1017;412,-1;-1
627,ICLR,2018,Backpropagation through the Void: Optimizing control variates for black-box gradient estimation,Will Grathwohl;Dami Choi;Yuhuai Wu;Geoff Roeder;David Duvenaud,wgrathwohl@cs.toronto.edu;choidami@cs.toronto.edu;ywu@cs.toronto.edu;roeder@cs.toronto.edu;duvenaud@cs.toronto.edu,8;7;6,4;3;2,Accept (Poster),0,11,0,yes,10/27/17,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",17;17;17;17;17,22;22;22;22;22,,10/27/17,129,59,66,2,229,29,368;153;1146;216;5822,13;6;28;9;74,5;3;13;4;28,81;31;180;48;739,-1;-1
628,ICLR,2018,"Don't Decay the Learning Rate, Increase the Batch Size",Samuel L. Smith;Pieter-Jan Kindermans;Chris Ying;Quoc V. Le,slsmith@google.com;pikinder@google.com;chrisying@google.com;qvl@google.com,6;7;6,4;4;4,Accept (Poster),2,6,0,yes,10/27/17,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,,10/27/17,325,174,59,18,309,29,1146;1898;164;47174,26;47;6;193,11;18;4;79,107;181;29;5944,-1;-1
629,ICLR,2018,Fidelity-Weighted Learning,Mostafa Dehghani;Arash Mehrjou;Stephan Gouws;Jaap Kamps;Bernhard Schölkopf,dehghani@uva.nl;amehrjou@tuebingen.mpg.de;sgouws@google.com;kamps@uva.nl;bs@tuebingen.mpg.de,7;5;6,4;4;3,Accept (Poster),0,8,0,yes,10/27/17,"University of Amsterdam;Max Planck Institute for Intelligent Systems, Max-Planck Institute;Google;University of Amsterdam;Max Planck Institute for Intelligent Systems, Max-Planck Institute",181;-1;-1;181;-1,59;-1;-1;59;-1,3,10/27/17,23,8,7,2,0,2,825;127;3545;4308;75018,74;27;23;375;860,13;5;12;30;119,75;12;327;313;9886,-1;-1
630,ICLR,2018,Emergent Complexity via Multi-Agent Competition,Trapit Bansal;Jakub Pachocki;Szymon Sidor;Ilya Sutskever;Igor Mordatch,tbansal@cs.umass.edu;jakub@openai.com;szymon@openai.com;ilyasu@openai.com;mordatch@openai.com,3;9;7,3;5;4,Accept (Poster),0,5,0,yes,10/27/17,"University of Massachusetts, Amherst;OpenAI;OpenAI;OpenAI;OpenAI",30;-1;-1;-1;-1,191;-1;-1;-1;-1,,10/10/17,146,97,26,0,978,15,559;969;935;129341;3002,15;31;15;90;48,8;12;7;52;26,57;65;67;16810;344,-1;-1
631,ICLR,2018,A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks,Behnam Neyshabur;Srinadh Bhojanapalli;Nathan Srebro,bneyshabur@ttic.edu;srinadh@ttic.edu;nati@ttic.edu,9;6;7,4;3;4,Accept (Poster),0,0,0,yes,10/27/17,Toyota Technological Institute at Chicago;Toyota Technological Institute at Chicago;Toyota Technological Institute at Chicago,-1;-1;-1,-1;-1;-1,1;8,7/29/17,211,134,52,19,0,39,2357;1570;13263,27;28;176,18;15;52,313;192;1600,-1;-1
632,ICLR,2018,N2N learning: Network to Network Compression via Policy Gradient Reinforcement Learning,Anubhav Ashok;Nicholas Rhinehart;Fares Beainy;Kris M. Kitani,anubhava@andrew.cmu.edu;nrhineha@cs.cmu.edu;fares.beainy@volvo.com;kkitani@cs.cmu.edu,5;9;4,4;4;4,Accept (Poster),0,4,0,yes,10/27/17,Carnegie Mellon University;Carnegie Mellon University;Volvo Trucks;Carnegie Mellon University,1;1;-1;1,24;24;-1;24,6;2,9/18/17,69,39,29,2,45,9,103;374;218;2418,7;21;32;115,3;9;8;25,15;42;14;228,-1;-1
633,ICLR,2018,AmbientGAN: Generative models from lossy measurements,Ashish Bora;Eric Price;Alexandros G. Dimakis,ashish.bora@utexas.edu;ecprice@cs.utexas.edu;dimakis@austin.utexas.edu,7;7;8,4;4;4,Accept (Oral),0,2,1,yes,10/27/17,"University of Texas, Austin;University of Texas, Austin;University of Texas, Austin",21;21;21,49;49;49,5;4,10/27/17,60,30,18,1,0,11,299;2704;11898,15;63;204,4;21;49,60;394;1440,-1;-1
634,ICLR,2018,Eigenoption Discovery through the Deep Successor Representation,Marlos C. Machado;Clemens Rosenbaum;Xiaoxiao Guo;Miao Liu;Gerald Tesauro;Murray Campbell,machado@ualberta.ca;crosenbaum@umass.edu;xiaoxiao.guo@ibm.com;miao.liu1@ibm.com;gtesauro@us.ibm.com;mcam@us.ibm.com,6;9;7,3;5;4,Accept (Poster),0,3,0,yes,10/27/17,"University of Alberta;University of Massachusetts, Amherst;International Business Machines;International Business Machines;International Business Machines;International Business Machines",99;30;-1;-1;-1;-1,119;191;-1;-1;-1;-1,,10/27/17,44,19,18,3,72,9,615;478;1480;1735;7513;1531,31;70;42;198;124;57,11;10;13;20;45;19,79;35;211;95;714;132,-1;-1
635,ICLR,2018,Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning,Sandeep Subramanian;Adam Trischler;Yoshua Bengio;Christopher J Pal,sandeep.subramanian.1@umontreal.ca;adam.trischler@microsoft.com;yoshua.umontreal@gmail.com;christopher.pal@polymtl.ca,8;8;4,5;5;5,Accept (Poster),1,15,0,yes,10/27/17,University of Montreal;Microsoft;University of Montreal;Polytechnique Montreal,124;-1;124;364,108;-1;108;108,3;6,10/27/17,159,100,66,4,0,32,2506;1533;200499;8128,17;47;804;120,11;17;146;33,495;285;23866;758,-1;-1
636,ICLR,2018,Communication Algorithms via Deep Learning,Hyeji Kim;Yihan Jiang;Ranvir B. Rana;Sreeram Kannan;Sewoong Oh;Pramod Viswanath,hyejikim@illinois.edu;yihanrogerjiang@gmail.com;rbrana2@illinois.edu;ksreeram@uw.edu;sewoong79@gmail.com;pramodv@illinois.edu,2;6;9,4;4;5,Accept (Poster),8,18,1,yes,10/27/17,"University of Illinois, Urbana Champaign;University of Washington, Seattle;University of Illinois, Urbana Champaign;University of Washington, Seattle;University of Illinois at Urbana-Champaign;University of Illinois, Urbana Champaign",3;6;3;6;3;3,37;25;37;25;37;37,,10/27/17,88,43,32,2,6,9,269;201;90;779;4388;16489,41;17;5;60;86;179,8;6;2;16;28;39,24;15;9;69;484;2154,-1;-1
637,ICLR,2018,Variational Inference of Disentangled Latent Concepts from Unlabeled Observations,Abhishek Kumar;Prasanna Sattigeri;Avinash Balakrishnan,abhishk@us.ibm.com;psattig@us.ibm.com;avinash.bala@us.ibm.com,6;7;7,5;4;4,Accept (Poster),0,9,0,yes,10/27/17,International Business Machines;International Business Machines;International Business Machines,-1;-1;-1,-1;-1;-1,5,10/27/17,123,81,48,5,17,34,3062;589;476,41;59;52,20;12;11,512;72;52,-1;-1
638,ICLR,2018,Variational Message Passing with Structured Inference Networks,Wu Lin;Nicolas Hubacher;Mohammad Emtiyaz Khan,wlin2018@cs.ubc.ca;nicolas.hubacher@outlook.com;emtiyaz@gmail.com,7;7;7,3;4;2,Accept (Poster),0,6,0,yes,10/27/17,University of British Columbia;;RIKEN,34;-1;-1,34;-1;-1,5;10,10/27/17,24,17,7,0,0,3,206;24;654,19;2;65,7;1;16,22;3;66,-1;-1
639,ICLR,2018,On the insufficiency of existing momentum schemes for Stochastic Optimization,Rahul Kidambi;Praneeth Netrapalli;Prateek Jain;Sham M. Kakade,rkidambi@uw.edu;praneeth@microsoft.com;prajain@microsoft.com;sham@cs.washington.edu,7;7;8,4;3;5,Accept (Oral),0,4,5,yes,10/27/17,"University of Washington, Seattle;Microsoft;Microsoft;University of Washington",6;-1;-1;6,25;-1;-1;25,,10/27/17,37,18,12,0,7,6,246;3241;7394;13383,22;72;131;196,9;27;41;57,22;414;1119;1958,-1;-1
640,ICLR,2018,CausalGAN: Learning Causal Implicit Generative Models with Adversarial Training,Murat Kocaoglu;Christopher Snyder;Alexandros G. Dimakis;Sriram Vishwanath,mkocaoglu@utexas.edu;22csnyder@gmail.com;dimakis@austin.utexas.edu;sriram@austin.utexas.edu,6;7;9,3;3;3,Accept (Poster),0,5,2,yes,10/27/17,"University of Texas, Austin;University of Texas, Galveston;University of Texas, Austin;University of Texas, Austin",21;468;21;21,49;1103;49;49,5;4;10,9/6/17,39,18,15,2,71,5,1620;52;11898;9119,196;7;204;280,21;3;49;38,119;6;1440;795,-1;-1
641,ICLR,2018,Unsupervised Neural Machine Translation,Mikel Artetxe;Gorka Labaka;Eneko Agirre;Kyunghyun Cho,mikel.artetxe@ehu.eus;gorka.labaka@ehu.eus;e.agirre@ehu.eus;kyunghyun.cho@nyu.edu,6;5;7,4;4;5,Accept (Poster),0,12,0,yes,10/27/17,University of the Basque Country;University of the Basque Country;University of the Basque Country;New York University,468;468;468;26,613;613;613;27,3,10/27/17,340,223,139,11,283,67,1489;1619;9026;45173,30;69;310;272,12;15;44;52,338;314;1216;6528,-1;-1
642,ICLR,2018,Understanding image motion with group representations ,Andrew Jaegle;Stephen Phillips;Daphne Ippolito;Kostas Daniilidis,ajaegle@upenn.edu;stephi@seas.upenn.edu;daphnei@seas.upenn.edu;kostas@seas.upenn.edu,7;5;4,3;4;4,Accept (Poster),0,4,0,yes,10/27/17,University of Pennsylvania;University of Pennsylvania;University of Pennsylvania;University of Pennsylvania,19;19;19;19,10;10;10;10,,12/1/16,2,2,1,0,0,0,135;157;83;9992,22;62;17;342,6;7;5;53,4;12;12;937,-1;-1
643,ICLR,2018,Deep Learning with Logged Bandit Feedback,Thorsten Joachims;Adith Swaminathan;Maarten de Rijke,tj@cs.cornell.edu;adswamin@microsoft.com;derijke@uva.nl,7;8;6,4;3;3,Accept (Poster),0,6,0,yes,10/27/17,Cornell University;Microsoft;University of Amsterdam,7;-1;181,19;-1;59,,10/27/17,46,29,15,2,0,6,41241;950;14772,232;35;795,67;15;58,5769;124;1211,-1;-1
644,ICLR,2018,FearNet: Brain-Inspired Model for Incremental Learning,Ronald Kemker;Christopher Kanan,rmk6217@rit.edu;kanan@rit.edu,7;7;6,2;4;2,Accept (Poster),0,5,0,yes,10/27/17,Rochester Institute of Technology;Rochester Institute of Technology,139;139,666;666,5,10/27/17,108,60,35,1,27,8,730;1949,13;73,8;19,45;157,-1;-1
645,ICLR,2018,On the Discrimination-Generalization Tradeoff in GANs,Pengchuan Zhang;Qiang Liu;Dengyong Zhou;Tao Xu;Xiaodong He,penzhan@microsoft.com;qiang.liu@dartmouth.edu;dennyzhou@google.com;tax313@lehigh.edu;xiaohe@microsoft.com,6;3;7,4;4;4,Accept (Poster),0,9,0,yes,10/27/17,Microsoft;Dartmouth College;Google;Lehigh University;Microsoft,-1;153;-1;244;-1,-1;89;-1;533;-1,5;4;1;8,10/27/17,35,23,8,7,7,9,580;4966;8800;3655;14040,29;645;78;426;165,9;30;33;30;45,96;240;1338;314;1983,-1;-1
646,ICLR,2018,Learning Latent Permutations with Gumbel-Sinkhorn Networks,Gonzalo Mena;David Belanger;Scott Linderman;Jasper Snoek,gem2131@columbia.edu;dbelanger@google.com;scott.linderman@gmail.com;jsnoek@google.com,8;7;6,4;4;2,Accept (Poster),0,6,0,yes,10/27/17,Columbia University;Google;Columbia University;Google,15;-1;15;-1,14;-1;14;-1,,10/27/17,58,28,29,1,12,14,130;1108;743;4795,9;92;44;61,5;15;13;18,14;133;88;505,-1;-1
647,ICLR,2018,Towards Deep Learning Models Resistant to Adversarial Attacks,Aleksander Madry;Aleksandar Makelov;Ludwig Schmidt;Dimitris Tsipras;Adrian Vladu,madry@mit.edu;amakelov@mit.edu;ludwigs@mit.edu;tsipras@mit.edu;avladu@mit.edu,7;6;7,4;3;4,Accept (Poster),3,6,1,yes,10/27/17,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2;2,5;5;5;5;5,4,6/19/17,1933,1199,912,97,50,717,5300;1851;3677;3735;2159,84;2;198;33;28,29;2;21;16;12,1054;681;882;897;705,-1;-1
648,ICLR,2018,Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via Ranking,Aleksandar Bojchevski;Stephan Günnemann,a.bojchevski@in.tum.de;guennemann@in.tum.de,7;6;7,4;4;3,Accept (Poster),2,5,0,yes,10/27/17,Technical University Munich;Technical University Munich,55;55,41;41,10,7/12/17,96,38,50,0,0,18,570;2415,21;138,12;28,103;278,-1;-1
649,ICLR,2018,Predicting Floor-Level for 911 Calls with Neural Networks and Smartphone Sensor Data,William Falcon;Henning Schulzrinne,waf2107@columbia.edu;hgs@cs.columbia.edu,6;7;6,4;5;3,Accept (Poster),0,14,1,yes,10/27/17,Columbia University;Columbia University,15;15,14;14,,10/27/17,4,2,1,0,29,0,29;22100,5;838,2;58,3;2810,-1;-1
650,ICLR,2018,Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples,Kimin Lee;Honglak Lee;Kibok Lee;Jinwoo Shin,kiminlee@kaist.ac.kr;honglak@eecs.umich.edu;kibok@umich.edu;jinwoos@kaist.ac.kr,6;7;6,4;3;3,Accept (Poster),0,4,0,yes,10/27/17,Korea Advanced Institute of Science and Technology;University of Michigan;University of Michigan;Korea Advanced Institute of Science and Technology,21;8;8;21,95;21;21;95,5,10/27/17,180,98,85,11,10,37,471;23804;713;1648,26;166;35;184,7;60;12;18,100;2793;107;214,-1;-1
651,ICLR,2018,Ask the Right Questions: Active Question Reformulation with Reinforcement Learning,Christian Buck;Jannis Bulian;Massimiliano Ciaramita;Wojciech Gajewski;Andrea Gesmundo;Neil Houlsby;Wei Wang.,cbuck@google.com;jbulian@google.com;massi@google.com;wgaj@google.com;agesmundo@google.com;neilhoulsby@google.com;wangwe@google.com,7;6;8,5;4;3,Accept (Oral),0,5,0,yes,10/27/17,Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,3,5/22/17,83,47,26,0,29,7,1466;122;2887;87;136;855;4871,80;11;76;4;4;34;169,19;4;28;2;4;15;18,153;14;266;7;12;114;420,-1;-1
652,ICLR,2018,Towards Image Understanding from Deep Compression Without Decoding,Robert Torfason;Fabian Mentzer;Eirikur Agustsson;Michael Tschannen;Radu Timofte;Luc Van Gool,robertto@student.ethz.ch;mentzerf@vision.ee.ethz.ch;aeirikur@vision.ee.ethz.ch;michaelt@nari.ee.ethz.ch;radu.timofte@vision.ee.ethz.ch;vangool@vision.ee.ethz.ch,6;9;6,4;5;3,Accept (Poster),0,3,0,yes,10/27/17,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,9;9;9;9;9;9,10;10;10;10;10;10,2,10/27/17,34,18,9,0,95,0,49;490;1643;1167;7300;79894,3;13;36;39;179;1261,2;8;14;18;35;120,1;84;227;151;970;10337,-1;-1
653,ICLR,2018,The Implicit Bias of Gradient Descent on Separable Data,Daniel Soudry;Elad Hoffer;Mor Shpigel Nacson;Nathan Srebro,daniel.soudry@gmail.com;elad.hoffer@gmail.com;mor.shpigel@gmail.com;nati@ttic.edu,5;7;8,5;4;4,Accept (Poster),0,3,0,yes,10/27/17,Technion;Technion;Technion;Toyota Technological Institute at Chicago,24;24;24;-1,327;327;327;-1,8,10/27/17,230,141,36,20,170,40,4801;1424;102;13263,76;27;5;176,26;11;3;52,618;162;19;1600,-1;-1
654,ICLR,2018,Distributed Prioritized Experience Replay,Dan Horgan;John Quan;David Budden;Gabriel Barth-Maron;Matteo Hessel;Hado van Hasselt;David Silver,horgan@google.com;johnquan@google.com;budden@google.com;gabrielbm@google.com;mtthss@google.com;hado@google.com;davidsilver@google.com,9;7;6,4;4;3,Accept (Poster),0,6,0,yes,10/27/17,Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,,10/27/17,219,95,80,5,0,31,1357;3225;1232;576;2246;5464;41950,9;16;55;14;26;50;158,8;11;14;8;13;21;56,225;542;128;70;364;904;5825,-1;-1
655,ICLR,2018,Learning One-hidden-layer Neural Networks with Landscape Design,Rong Ge;Jason D. Lee;Tengyu Ma,rongge@cs.duke.edu;jasondlee88@gmail.com;tengyuma@cs.stanford.edu,6;9;7,3;3;3,Accept (Poster),0,4,1,yes,10/27/17,Duke University;University of Southern California;Stanford University,46;31;4,17;66;3,1,10/27/17,121,66,12,5,10,12,5730;4585;3818,78;118;85,32;36;32,800;599;491,-1;-1
656,ICLR,2018,Learning to cluster in order to transfer across domains and tasks,Yen-Chang Hsu;Zhaoyang Lv;Zsolt Kira,yenchang.hsu@gatech.edu;zhaoyang.lv@gatech.edu;zkira@gatech.edu,7;5;9,4;4;5,Accept (Poster),2,6,0,yes,10/20/17,Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology,13;13;13,33;33;33,6,10/20/17,32,16,15,2,14,8,189;119;1108,13;14;69,9;7;16,35;25;157,-1;-1
657,ICLR,2018,Mixed Precision Training of Convolutional Neural Networks using Integer Operations,Dipankar Das;Naveen Mellempudi;Dheevatsa Mudigere;Dhiraj Kalamkar;Sasikanth Avancha;Kunal Banerjee;Srinivas Sridharan;Karthik Vaidyanathan;Bharat Kaul;Evangelos Georganas;Alexander Heinecke;Pradeep Dubey;Jesus Corbal;Nikita Shustrov;Roma Dubtsov;Evarist Fomenko;Vadim Pirogov,dipankar.das@intel.com;naveen.k.mellempudi@intel.com;dheevatsa.mudigere@intel.com;dhiraj.d.kalamkar@intel.com;sasikanth.avancha@intel.com;kunal.banerjee@intel.com;srinivas.sridharan@intel.com;karthikeyan.vaidyanathan@intel.com;bharat.kaul@intel.com;evangelos.georganas@intel.com;alexander.heinecke@intel.com;pradeep.dubey@intel.com;jesus.corbal@intel.com;nikita.a.shustrov@intel.com;roman.s.dubtsov@intel.com;evarist.m.fomenko@intel.com;vadim.o.pirogov@intel.com,7;7;6,4;3;3,Accept (Poster),0,4,0,yes,10/27/17,Intel;Intel;Intel;Intel;Intel;Intel;Intel;Intel;Intel;Intel;Intel;Intel;Intel;Intel;Intel;Intel;Intel,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,,10/27/17,48,25,17,0,9,6,1125;191;1417;478;3262;242;336;978;477;512;1288;8637;741;46;158;57;88,107;11;40;34;58;43;27;67;34;29;80;250;38;6;3;5;4,17;6;15;13;18;9;9;17;11;9;18;45;12;1;2;2;3,65;22;144;34;395;13;27;63;49;43;86;925;83;6;16;6;13,-1;-1
658,ICLR,2018,Generative networks as inverse problems with Scattering transforms,Tomás Angles;Stéphane Mallat,tomas.angles@ens.fr;stephane.mallat@ens.fr,7;8;6,4;4;4,Accept (Poster),0,5,0,yes,10/27/17,Ecole Normale Superieure;Ecole Normale Superieure,99;99,72;72,5;4,10/27/17,12,5,3,1,25,2,40;43132,6;155,3;43,2;4627,-1;-1
659,ICLR,2018,Sparse Persistent RNNs: Squeezing Large Recurrent Networks On-Chip,Feiwen Zhu;Jeff Pool;Michael Andersch;Jeremy Appleyard;Fung Xie,mzhu@nvidia.com;jpool@nvidia.com;mandersch@nvidia.com;jappleyard@nvidia.com;ftse@nvidia.com,6;6;6,2;4;2,Accept (Poster),0,5,0,yes,10/27/17,NVIDIA;NVIDIA;NVIDIA;NVIDIA;NVIDIA,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,,10/27/17,10,4,1,1,8,1,28;2656;145;174;6,3;38;11;17;1,3;11;6;7;1,2;367;22;23;1,-1;-1
660,ICLR,2018,Robustness of Classifiers to Universal Perturbations: A Geometric Perspective,Seyed-Mohsen Moosavi-Dezfooli;Alhussein Fawzi;Omar Fawzi;Pascal Frossard;Stefano Soatto,seyed.moosavi@epfl.ch;fawzi@cs.ucla.edu;omar.fawzi@ens-lyon.fr;pascal.frossard@epfl.ch;soatto@cs.ucla.edu,6;5;7,4;3;3,Accept (Poster),0,3,0,yes,10/26/17,"Swiss Federal Institute of Technology Lausanne;University of California, Los Angeles;Ecole Normale Supérieure de Lyon;Swiss Federal Institute of Technology Lausanne;University of California, Los Angeles",468;20;181;468;20,38;15;182;38;15,1,5/26/17,90,65,6,3,14,7,2851;3394;1881;12025;15339,19;39;65;564;458,14;17;18;39;61,403;450;201;1153;1430,-1;-1
661,ICLR,2018,Deep Learning as a Mixed Convex-Combinatorial Optimization Problem,Abram L. Friesen;Pedro Domingos,afriesen@cs.washington.edu;pedrod@cs.washington.edu,7;7;7,4;4;3,Accept (Poster),0,2,0,yes,10/27/17,University of Washington;University of Washington,6;6,25;25,9,10/27/17,12,8,5,0,269,4,231;28583,20;229,7;72,17;3256,-1;-1
662,ICLR,2018,Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments,Maruan Al-Shedivat;Trapit Bansal;Yura Burda;Ilya Sutskever;Igor Mordatch;Pieter Abbeel,alshedivat@cs.cmu.edu;tbansal@cs.umass.edu;yburda@openai.com;ilyasu@openai.com;mordatch@openai.com;pabbeel@cs.berkeley.edu,8;7;9,4;4;2,Accept (Oral),0,4,3,yes,10/27/17,"Carnegie Mellon University;University of Massachusetts, Amherst;OpenAI;OpenAI;OpenAI;University of California Berkeley",1;30;-1;-1;-1;5,24;191;-1;-1;-1;18,4;6,10/10/17,141,85,39,5,0,13,731;559;1302;129341;3002;36244,34;15;21;90;48;433,13;8;8;52;26;94,79;57;274;16810;344;4370,-1;-1
663,ICLR,2018,Unsupervised Cipher Cracking Using Discrete GANs,Aidan N. Gomez;Sicong Huang;Ivan Zhang;Bryan M. Li;Muhammad Osama;Lukasz Kaiser,aidan.n.gomez@gmail.com;huang@cs.toronto.edu;ivan@for.ai;bryan@for.ai;osama@for.ai;lukaszkaiser@google.com,7;7;8,4;1;4,Accept (Poster),0,5,0,yes,10/27/17,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Google",17;17;-1;17;17;-1,22;22;-1;22;22;-1,5;1,10/27/17,20,9,9,1,0,4,9541;30;120;153;86;22238,15;3;9;39;14;75,8;2;6;7;5;24,2364;4;5;10;13;3839,-1;-1
664,ICLR,2018,Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play,Sainbayar Sukhbaatar;Zeming Lin;Ilya Kostrikov;Gabriel Synnaeve;Arthur Szlam;Rob Fergus,sainbar@cs.nyu.edu;zlin@fb.com;kostrikov@cs.nyu.edu;gab@fb.com;aszlam@fb.com;fergus@cs.nyu.edu,8;5;8,4;3;4,Accept (Poster),0,2,0,yes,10/27/17,New York University;Facebook;New York University;Facebook;Facebook;New York University,26;-1;26;-1;-1;26,27;-1;27;-1;-1;27,,3/15/17,115,75,26,3,0,4,2698;6156;586;1583;8609;51431,21;19;15;62;86;127,13;9;8;19;32;61,300;756;56;156;913;6404,-1;-1
665,ICLR,2018,Mastering the Dungeon: Grounded Language Learning by Mechanical Turker Descent,Zhilin Yang;Saizheng Zhang;Jack Urbanek;Will Feng;Alexander Miller;Arthur Szlam;Douwe Kiela;Jason Weston,zhiliny@cs.cmu.edu;saizheng.zhang@umontreal.ca;jju@fb.com;willfeng@fb.com;ahm@fb.com;aszlam@fb.com;dkiela@fb.com;jase@fb.com,7;7;8,4;4;5,Accept (Poster),0,4,0,yes,10/27/17,Carnegie Mellon University;University of Montreal;Facebook;Facebook;Facebook;Facebook;Facebook;Facebook,1;124;-1;-1;-1;-1;-1;-1,24;108;-1;-1;-1;-1;-1;-1,3,10/27/17,11,7,1,0,0,0,4861;3156;369;129;953;8609;3389;44263,90;22;8;3;16;86;79;243,26;13;5;3;10;32;29;76,782;370;78;11;113;913;579;5812,-1;-1
666,ICLR,2018,Gradient Estimators for Implicit Models,Yingzhen Li;Richard E. Turner,yl494@cam.ac.uk;ret26@cam.ac.uk,7;7;6,2;4;2,Accept (Poster),1,9,1,yes,10/27/17,University of Cambridge;University of Cambridge,71;71,2;2,5;4;6,5/19/17,37,19,20,0,21,6,955;2918,42;175,13;30,136;306,-1;-1
667,ICLR,2018, Neural Map: Structured Memory for Deep Reinforcement Learning,Emilio Parisotto;Ruslan Salakhutdinov,eparisot@andrew.cmu.edu;rsalakhu@cs.cmu.edu,7;9;6,4;5;5,Accept (Poster),0,9,0,yes,10/27/17,Carnegie Mellon University;Carnegie Mellon University,1;1,24;24,,2/27/17,114,70,37,2,0,4,868;66654,17;253,9;81,76;7740,-1;-1
668,ICLR,2018,Memory-based Parameter Adaptation,Pablo Sprechmann;Siddhant M. Jayakumar;Jack W. Rae;Alexander Pritzel;Adria Puigdomenech Badia;Benigno Uria;Oriol Vinyals;Demis Hassabis;Razvan Pascanu;Charles Blundell,psprechmann@google.com;sidmj@google.com;jwrae@google.com;apritzel@google.com;adriap@google.com;buria@google.com;vinyals@google.com;dhcontact@google.com;razp@google.com;cblundell@google.com,6;6;8,4;5;4,Accept (Poster),1,13,1,yes,10/27/17,Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,3,10/27/17,37,16,13,1,19,6,2154;179;631;5148;3958;1155;51899;23607;16567;5959,44;14;19;25;11;20;121;55;101;49,18;8;10;13;5;14;55;31;46;22,213;23;74;1037;758;133;6475;2541;1661;1049,-1;-1
669,ICLR,2018,Model compression via distillation and quantization,Antonio Polino;Razvan Pascanu;Dan Alistarh,antonio.polino1@gmail.com;razp@google.com;d.alistarh@gmail.com,7;6;8,2;4;5,Accept (Poster),0,13,3,yes,10/27/17,;Google;Institute of Science and Technology Austria,-1;-1;99,-1;-1;1103,,10/27/17,155,78,56,3,4,22,159;16567;1738,2;101;125,1;46;19,22;1661;224,-1;-1
670,ICLR,2018,Learning Wasserstein Embeddings,Nicolas Courty;Rémi Flamary;Mélanie Ducoffe,ncourty@irisa.fr;remi.flamary@unice.fr;ducoffe@i3s.unice.fr,7;7;7,3;3;4,Accept (Poster),0,10,0,yes,10/26/17,"IRISA, Université Bretagne Sud;Université Côte d'Azur;Université Côte d'Azur",468;-1;-1,1103;-1;-1,5,10/20/17,24,14,6,0,124,3,1781;1343;34,123;77;5,24;18;2,145;117;3,-1;-1
671,ICLR,2018,Loss-aware Weight Quantization of Deep Networks,Lu Hou;James T. Kwok,lhouab@cse.ust.hk;jamesk@cse.ust.hk,8;6;6,3;4;4,Accept (Poster),0,3,0,yes,10/27/17,The Hong Kong University of Science and Technology;The Hong Kong University of Science and Technology,40;40,44;44,,10/27/17,47,27,18,0,4,4,439;9449,31;197,9;50,29;1045,-1;-1
672,ICLR,2018,Kernel Implicit Variational Inference,Jiaxin Shi;Shengyang Sun;Jun Zhu,shijx15@mails.tsinghua.edu.cn;ssy@cs.toronto.edu;dcszj@tsinghua.edu.cn,5;7;7,4;3;4,Accept (Poster),0,7,0,yes,10/27/17,"Tsinghua University;Department of Computer Science, University of Toronto;Tsinghua University",10;17;10,30;22;30,11,5/29/17,27,17,9,0,30,3,-1;442;4471,-1;23;204,-1;8;35,0;38;520,-1;-1
673,ICLR,2018,Adversarial Dropout Regularization,Kuniaki Saito;Yoshitaka Ushiku;Tatsuya Harada;Kate Saenko,k-saito@mi.t.u-tokyo.ac.jp;ushiku@mi.t.u-tokyo.ac.jp;harada@mi.t.u-tokyo.ac.jp;saenko@bu.edu,5;7;8,4;3;5,Accept (Poster),0,11,0,yes,10/27/17,The University of Tokyo;The University of Tokyo;The University of Tokyo;Boston University,52;52;52;69,45;45;45;70,4;2,10/27/17,66,32,24,1,20,9,5444;1322;2525;16872,281;55;212;177,36;16;25;55,454;233;318;2355,-1;-1
674,ICLR,2018,Hierarchical Subtask Discovery with Non-Negative Matrix Factorization,Adam C. Earle;Andrew M. Saxe;Benjamin Rosman,adam.earle@ymail.com;asaxe@fas.harvard.edu;benjros@gmail.com,6;5;7,2;2;3,Accept (Poster),0,5,0,yes,10/27/17,University of the Witwatersrand;Harvard University;University of the Witwatersrand,468;37;468,293;6;293,,8/1/17,2,1,0,0,6,0,26;1943;439,10;32;78,3;12;12,1;163;22,-1;-1
675,ICLR,2018,A Scalable Laplace Approximation for Neural Networks,Hippolyt Ritter;Aleksandar Botev;David Barber,j.ritter@cs.ucl.ac.uk;botevmg@gmail.com;d.barber@cs.ucl.ac.uk,9;6;6,4;4;4,Accept (Poster),0,5,0,yes,10/27/17,University College London;University College London;University College London,46;46;46,16;16;16,4,10/27/17,62,26,35,1,0,9,171;249;3779,6;9;200,3;6;27,24;33;405,-1;-1
676,ICLR,2018,TreeQN and ATreeC: Differentiable Tree-Structured Models for Deep Reinforcement Learning,Gregory Farquhar;Tim Rocktäschel;Maximilian Igl;Shimon Whiteson,gregory.farquhar@cs.ox.ac.uk;tim.rocktaeschel@gmail.com;maximilian.igl@gmail.com;shimon.whiteson@gmail.com,4;8;5,5;5;3,Accept (Poster),3,5,0,yes,10/27/17,University of Oxford;University of Oxford;University of Oxford;University of Oxford,51;51;51;51,1;1;1;1,,10/27/17,57,30,18,3,372,6,911;2243;268;5278,19;48;12;203,9;21;6;38,168;278;43;572,-1;-1
677,ICLR,2018,Training wide residual networks for deployment using a single bit for each weight,Mark D. McDonnell,mark.mcdonnell@unisa.edu.au,6;6;6,4;3;4,Accept (Poster),0,8,0,yes,10/26/17,University of South Australia,468,248,,10/26/17,36,13,13,1,15,1,2258,155,19,106,-1
678,ICLR,2018,Can Neural Networks Understand Logical Entailment?,Richard Evans;David Saxton;David Amos;Pushmeet Kohli;Edward Grefenstette,richardevans@google.com;saxton@google.com;davidamos@google.com;pushmeet@google.com;etg@google.com,7;7;4,3;3;4,Accept (Poster),6,3,0,yes,10/27/17,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,,10/27/17,59,28,18,4,93,8,1501;1004;365;21883;6940,226;30;26;313;57,21;9;8;69;25,165;150;30;2737;832,-1;-1
679,ICLR,2018,Learning Sparse Neural Networks through L_0 Regularization,Christos Louizos;Max Welling;Diederik P. Kingma,c.louizos@uva.nl;m.welling@uva.nl;dpkingma@openai.com,6;6;7,3;3;4,Accept (Poster),0,5,0,yes,10/27/17,University of Amsterdam;University of Amsterdam;OpenAI,181;181;-1,59;59;-1,8,10/27/17,257,110,133,7,143,51,1225;26369;55051,23;269;25,11;57;19,237;5056;10131,-1;-1
680,ICLR,2018,Unsupervised Learning of Goal Spaces for Intrinsically Motivated Goal Exploration,Alexandre Péré;Sébastien Forestier;Olivier Sigaud;Pierre-Yves Oudeyer,alexandre.pere@inria.fr;sebastien.forestier@inria.fr;olivier.sigaud@upmc.fr;pierre-yves.oudeyer@inria.fr,7;6;7,4;2;4,Accept (Poster),0,6,0,yes,10/26/17,"INRIA;INRIA;Computer Science Lab  - Pierre and Marie Curie University, Paris, France;INRIA",-1;-1;468;-1,-1;-1;123;-1,,10/26/17,41,25,7,1,0,1,85;267;2148;5534,7;17;159;301,2;8;25;35,2;12;137;308,-1;-1
681,ICLR,2018,Learning From Noisy Singly-labeled Data,Ashish Khetan;Zachary C. Lipton;Animashree Anandkumar,khetan2@illinois.edu;zlipton@cmu.edu;anima@amazon.com,7;6;7,4;3;4,Accept (Poster),0,8,1,yes,10/27/17,"University of Illinois, Urbana Champaign;Carnegie Mellon University;Amazon",3;1;-1,37;24;-1,1;8,10/27/17,50,26,16,2,66,7,301;4677;5285,25;97;187,8;27;38,43;425;739,-1;-1
682,ICLR,2018,SEARNN: Training RNNs with global-local losses,Rémi Leblond;Jean-Baptiste Alayrac;Anton Osokin;Simon Lacoste-Julien,remi.leblond@inria.fr;jean-baptiste.alayrac@inria.fr;aosokin@hse.ru;slacoste@iro.umontreal.ca,8;5;7,4;5;3,Accept (Poster),0,12,0,yes,10/27/17,INRIA;INRIA;Higher School of Economics;University of Montreal,-1;-1;468;124,-1;-1;377;108,3,6/14/17,25,18,14,1,59,2,303;386;1189;3705,12;24;36;72,6;10;11;26,26;47;141;601,-1;-1
683,ICLR,2018,"Progressive Growing of GANs for Improved Quality, Stability, and Variation",Tero Karras;Timo Aila;Samuli Laine;Jaakko Lehtinen,tkarras@nvidia.com;taila@nvidia.com;slaine@nvidia.com;jlehtinen@nvidia.com,8;1;8,4;4;4,Accept (Oral),2,5,4,yes,10/27/17,NVIDIA;NVIDIA;NVIDIA;NVIDIA,-1;-1;-1;-1,-1;-1;-1;-1,5;4,10/27/17,1769,919,617,38,122,337,4573;6013;5329;3880,42;68;72;90,21;31;26;26,823;971;890;550,-1;-1
684,ICLR,2018,Graph Attention Networks,Petar Veličković;Guillem Cucurull;Arantxa Casanova;Adriana Romero;Pietro Liò;Yoshua Bengio,petar.velickovic@cst.cam.ac.uk;gcucurull@gmail.com;ar.casanova.8@gmail.com;adriana.romsor@gmail.com;pietro.lio@cst.cam.ac.uk;yoshua.umontreal@gmail.com,6;7;5,4;5;4,Accept (Poster),13,5,0,yes,10/26/17,University of Cambridge;Universitat Autonoma de Barcelona;University of Montreal;Facebook;University of Cambridge;University of Montreal,71;468;124;-1;71;124,2;146;108;-1;2;108,10,10/26/17,1294,554,520,32,33,362,1553;1460;1300;3796;7022;200865,31;12;9;52;381;805,9;8;4;13;36;147,400;369;354;627;721;23899,-1;-1
685,ICLR,2018,i-RevNet: Deep Invertible Networks,Jörn-Henrik Jacobsen;Arnold W.M. Smeulders;Edouard Oyallon,joern.jacobsen@bethgelab.org;a.w.m.smeulders@uva.nl;edouard.oyallon@ens.fr,8;9;8,4;4;4,Accept (Poster),0,9,7,yes,10/27/17,"Centre for Integrative Neuroscience, AG Bethge;University of Amsterdam;Ecole Normale Superieure",-1;181;99,-1;59;72,,10/27/17,105,62,32,2,103,8,396;21901;579,19;468;25,8;50;10,35;1914;50,-1;-1
686,ICLR,2018,On the regularization of Wasserstein GANs,Henning Petzka;Asja Fischer;Denis Lukovnikov,henning.petzka@iais.fraunhofer.de;asja.fischer@gmail.com;lukovnik@cs.uni-bonn.de,7;2;6,4;2;5,Accept (Poster),0,8,0,yes,10/27/17,Fraunhofer IIS;University of Bonn;University of Bonn,-1;124;124,-1;100;100,5;4,9/26/17,74,40,29,3,21,12,101;1700;252,20;53;15,4;16;7,13;196;30,-1;-1
687,ICLR,2018,Initialization matters: Orthogonal Predictive State Recurrent Neural Networks,Krzysztof Choromanski;Carlton Downey;Byron Boots,kchoro@google.com;cmdowney@cs.cmu.edu;bboots@cc.gatech.edu,4;8;7,5;4;2,Accept (Poster),0,0,0,yes,10/27/17,Google;Carnegie Mellon University;Georgia Institute of Technology,-1;1;13,-1;24;33,3,10/27/17,10,4,6,1,0,1,872;247;2389,92;25;135,15;8;27,78;26;246,-1;-1
688,ICLR,2018,Adaptive Quantization of Neural Networks,Soroosh Khoram;Jing Li,khoram@wisc.edu;jli@ece.wisc.edu,6;6;6,4;4;3,Accept (Poster),0,4,1,yes,10/27/17,University of Southern California;University of Southern California,31;31,66;66,,10/27/17,18,9,4,0,0,1,88;1226,10;283,5;18,13;76,-1;-1
689,ICLR,2018,Large Scale Optimal Transport and Mapping Estimation,Vivien Seguy;Bharath Bhushan Damodaran;Remi Flamary;Nicolas Courty;Antoine Rolet;Mathieu Blondel,vivienseguy@gmail.com;bharath-bhushan.damodaran@irisa.fr;remi.flamary@unice.fr;courty@univ-ubs.fr;antoine.rolet@iip.ist.i.kyoto-u.ac.jp;mblondel@gmail.com,7;6;6;8,3;3;3;3,Accept (Poster),0,8,1,yes,10/27/17,"Meiji University;IRISA, Université Bretagne Sud;Université Côte d'Azur;University of Bretagne Sud;Meiji University;NTT",468;468;-1;468;468;-1,334;1103;-1;1103;334;-1,5;1;8,10/27/17,60,33,23,5,7,12,172;361;1343;1781;174;22532,10;22;77;123;4;37,5;8;18;24;3;15,21;28;117;145;22;1531,-1;-1
690,ICLR,2018,Generalizing Across Domains via Cross-Gradient Training,Shiv Shankar*;Vihari Piratla*;Soumen Chakrabarti;Siddhartha Chaudhuri;Preethi Jyothi;Sunita Sarawagi,shivshankariitb@gmail.com;viharipiratla@gmail.com;soumen@cse.iitb.ac.in;sidch@cse.iitb.ac.in;pjyothi@cse.iitb.ac.in;sunita@iitb.ac.in,7;7;8;7,5;4;4;5,Accept (Poster),0,7,0,yes,10/27/17,Indian Institute of Technology Bombay;Indian Institute of Technology Bombay;Indian Institute of Technology Bombay;Indian Institute of Technology Bombay;Indian Institute of Technology Bombay;Indian Institute of Technology Bombay,115;115;115;115;115;115,367;367;367;367;367;367,4;11;8,10/27/17,61,32,27,1,6,15,121;71;9075;1605;282;7750,15;9;238;63;54;150,5;3;43;17;8;40,17;19;972;121;27;698,-1;-1
691,ICLR,2018,Action-dependent Control Variates for Policy Optimization via Stein Identity,Hao Liu*;Yihao Feng*;Yi Mao;Dengyong Zhou;Jian Peng;Qiang Liu,uestcliuhao@gmail.com;yihao@cs.utexas.edu;maoyi@microsoft.com;dennyzhou@google.com;jianpeng@illinois.edu;lqiang@cs.utexas.edu,7;7;7,3;3;4,Accept (Poster),3,9,1,yes,10/27/17,"University of California Berkeley;University of Texas, Austin;Microsoft;Google;University of Illinois, Urbana Champaign;University of Texas, Austin",5;21;-1;-1;3;21,18;49;-1;-1;37;49,,10/27/17,57,30,18,2,4,5,3548;136;1177;8800;2105;12413,504;12;104;78;127;1136,27;6;17;33;23;34,200;13;86;1338;206;820,-1;-1
692,ICLR,2018,Bi-Directional Block Self-Attention for Fast and Memory-Efficient Sequence Modeling,Tao Shen;Tianyi Zhou;Guodong Long;Jing Jiang;Chengqi Zhang,tao.shen@student.uts.edu.au;tianyizh@uw.edu;guodong.long@uts.edu.au;jing.jiang@uts.edu.au;chengqi.zhang@uts.edu.au,6;9;6,4;4;4,Accept (Poster),0,7,3,yes,10/27/17,"University of Technology Sydney;University of Washington, Seattle;University of Technology Sydney;University of Technology Sydney;University of Technology Sydney",115;6;115;115;115,216;25;216;216;216,3,10/27/17,76,40,29,2,15,6,425;1444;1690;1982;6900,15;81;79;53;468,4;14;16;19;39,45;142;151;180;471,-1;-1
693,ICLR,2018,Generalizing Hamiltonian Monte Carlo with Neural Networks,Daniel Levy;Matt D. Hoffman;Jascha Sohl-Dickstein,danilevy@cs.stanford.edu;mhoffman@google.com;jaschasd@google.com,7;6;8,4;3;2,Accept (Poster),0,13,0,yes,10/26/17,Stanford University;Google;Google,4;-1;-1,3;-1;-1,5,10/26/17,39,15,20,1,548,5,200;8308;4877,11;94;101,4;29;33,17;1202;677,-1;-1
694,ICLR,2018,Beyond Word Importance:  Contextual Decomposition to Extract Interactions from LSTMs,W. James Murdoch;Peter J. Liu;Bin Yu,jmurdoch@berkeley.edu;peterjliu@google.com;binyu@berkeley.edu,7;7;7,3;4;2,Accept (Oral),0,8,1,yes,10/27/17,University of California Berkeley;Google;University of California Berkeley,5;-1;5,18;-1;18,,10/27/17,69,36,34,1,110,13,348;2515;5242,10;24;268,6;12;35,29;489;379,-1;-1
695,ICLR,2018,Temporally Efficient Deep Learning with Spikes,Peter O'Connor;Efstratios Gavves;Matthias Reisser;Max Welling,peter.ed.oconnor@gmail.com;e.gavves@uva.nl;reisser.matthias@gmail.com;m.welling@uva.nl,7;6;8,5;4;4,Accept (Poster),0,1,1,yes,10/27/17,;University of Amsterdam;University of Amsterdam;University of Amsterdam,-1;181;181;181,-1;59;59;59,,6/13/17,14,4,7,0,25,0,2854;3268;121;26369,259;72;15;269,26;26;6;57,153;467;13;5056,-1;-1
696,ICLR,2018,Learning Deep Mean Field Games for Modeling Large Population Behavior,Jiachen Yang;Xiaojing Ye;Rakshit Trivedi;Huan Xu;Hongyuan Zha,yjiachen@gmail.com;xye@gsu.edu;rstrivedi@gatech.edu;huan.xu@isye.gatech.edu;zha@cc.gatech.edu,8;8;10,4;3;5,Accept (Oral),0,6,0,yes,10/27/17,Lawrence Livermore National Labs;SUN YAT-SEN UNIVERSITY;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology,-1;468;13;13;13,-1;352;33;33;33,,10/27/17,25,15,6,0,4,0,1508;902;402;7376;14056,148;61;30;333;406,20;18;12;38;62,45;61;35;567;1216,-1;-1
697,ICLR,2018,Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models,Pouya Samangouei;Maya Kabkab;Rama Chellappa,pouya@umiacs.umd.edu;mayak@umiacs.umd.edu;rama@umiacs.umd.edu,6;6;8,3;3;4,Accept (Poster),1,12,0,yes,10/27/17,"University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park",12;12;12,69;69;69,5;4,10/27/17,393,236,99,11,8,52,691;431;40181,14;9;1066,7;4;94,97;54;3156,-1;-1
698,ICLR,2018,Learning from Between-class Examples for Deep Sound Recognition,Yuji Tokozume;Yoshitaka Ushiku;Tatsuya Harada,tokozume@mi.t.u-tokyo.ac.jp;ushiku@mi.t.u-tokyo.ac.jp;harada@mi.t.u-tokyo.ac.jp,9;4;8,4;4;4,Accept (Poster),0,9,2,yes,10/27/17,The University of Tokyo;The University of Tokyo;The University of Tokyo,52;52;52,45;45;45,,10/27/17,55,24,22,2,11,10,176;1322;2525,4;55;212,3;16;25,25;233;318,-1;-1
699,ICLR,2018,META LEARNING SHARED HIERARCHIES,Kevin Frans;Jonathan Ho;Xi Chen;Pieter Abbeel;John Schulman,kevinfrans2@gmail.com;jonathanho@berkeley.edu;c.xi@eecs.berkeley.edu;pabbeel@cs.berkeley.edu;joschu@openai.com,6;4;7,3;4;3,Accept (Poster),0,13,0,yes,10/27/17,OpenAI;University of California Berkeley;University of California Berkeley;University of California Berkeley;OpenAI,-1;5;5;5;-1,-1;18;18;18;-1,,10/26/17,139,61,40,2,0,20,179;2666;13266;36244;14764,4;31;444;433;55,3;12;40;94;31,23;396;1537;4370;2459,-1;-1
700,ICLR,2018,Stochastic Variational Video Prediction,Mohammad Babaeizadeh;Chelsea Finn;Dumitru Erhan;Roy H. Campbell;Sergey Levine,mb2@uiuc.edu;cbfinn@eecs.berkeley.edu;dumitru@google.com;rhc@illinois.edu;svlevine@eecs.berkeley.edu,7;7;7,5;4;4,Accept (Poster),0,9,0,yes,10/27/17,"University of Illinois, Urbana-Champaign;University of California Berkeley;Google;University of Illinois, Urbana Champaign;University of California Berkeley",3;5;-1;3;5,37;18;-1;37;18,,10/27/17,170,95,77,6,8,30,615;7655;41354;12616;24235,19;98;60;622;309,8;33;31;55;73,91;1024;5952;967;3140,-1;-1
701,ICLR,2018,An efficient framework for learning sentence representations,Lajanugen Logeswaran;Honglak Lee,llajan@umich.edu;honglak@eecs.umich.edu,6;8;6,5;4;4,Accept (Poster),8,2,1,yes,10/27/17,University of Michigan;University of Michigan,8;8,21;21,3,10/27/17,141,68,47,4,0,15,1598;23804,12;166,6;60,163;2793,-1;-1
702,ICLR,2018,Evaluating the Robustness of Neural Networks: An Extreme Value Theory Approach,Tsui-Wei Weng*;Huan Zhang*;Pin-Yu Chen;Jinfeng Yi;Dong Su;Yupeng Gao;Cho-Jui Hsieh;Luca Daniel,twweng@mit.edu;ecezhang@ucdavis.edu;pin-yu.chen@ibm.com;jinfengyi.ustc@gmail.com;dong.su@ibm.com;yupeng.gao@ibm.com;chohsieh@ucdavis.edu;dluca@mit.edu,7;7;7,3;3;1,Accept (Poster),0,7,2,yes,10/27/17,"Massachusetts Institute of Technology;University of California, Davis;International Business Machines;JD AI Research;International Business Machines;International Business Machines;University of California, Davis;Massachusetts Institute of Technology",2;78;-1;-1;-1;-1;78;2,5;54;-1;-1;-1;-1;54;5,4,10/27/17,98,52,30,2,0,13,609;14158;2316;1967;848;206;12457;2496,27;722;104;79;44;16;168;144,10;48;24;24;12;5;40;23,67;906;216;238;88;20;1718;205,-1;-1
703,ICLR,2018,Thermometer Encoding: One Hot Way To Resist Adversarial Examples,Jacob Buckman;Aurko Roy;Colin Raffel;Ian Goodfellow,buckman@google.com;aurkor@google.com;craffel@google.com;goodfellow@google.com,6;6;6,2;4;4,Accept (Poster),3,4,0,yes,10/27/17,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,4;8,10/27/17,221,114,52,3,0,22,384;1008;4545;55154,8;21;63;90,6;10;22;56,39;113;465;9293,-1;-1
704,ICLR,2018,Simulated+Unsupervised Learning With Adaptive Data Generation and Bidirectional Mappings,Kangwook Lee;Hoon Kim;Changho Suh,kw1jjang@gmail.com;gnsrla12@kaist.ac.kr;chsuh@kaist.ac.kr,6;6;3,3;4;4,Accept (Poster),0,4,0,yes,10/27/17,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,21;21;21,95;95;95,,10/27/17,6,4,3,0,0,0,29;2596;2664,17;149;103,3;16;23,1;233;287,-1;-1
705,ICLR,2018,Interpretable Counting for Visual Question Answering,Alexander Trott;Caiming Xiong;Richard Socher,atrott@salesforce.com;cxiong@salesforce.com;rsocher@salesforce.com,6;7;7,3;4;4,Accept (Poster),0,5,1,yes,10/27/17,SalesForce.com;SalesForce.com;SalesForce.com,-1;-1;-1,-1;-1;-1,,10/27/17,28,16,10,0,16,4,466;6187;51920,73;156;180,13;31;48,25;1048;8747,-1;-1
706,ICLR,2018,Hierarchical Representations for Efficient Architecture Search,Hanxiao Liu;Karen Simonyan;Oriol Vinyals;Chrisantha Fernando;Koray Kavukcuoglu,hanxiaol@cs.cmu.edu;simonyan@google.com;vinyals@google.com;chrisantha@google.com;korayk@google.com,6;6;8,3;4;4,Accept (Poster),0,3,0,yes,10/27/17,Carnegie Mellon University;Google;Google;Google;Google,1;-1;-1;-1;-1,24;-1;-1;-1;-1,,10/27/17,338,138,138,11,351,42,1961;59360;51899;1731;51980,35;95;121;65;90,12;38;55;18;58,509;10620;6475;156;6771,-1;-1
707,ICLR,2018,Multi-Scale Dense Networks for Resource Efficient Image Classification,Gao Huang;Danlu Chen;Tianhong Li;Felix Wu;Laurens van der Maaten;Kilian Weinberger,gh349@cornell.edu;taineleau@gmail.com;lth14@mails.tsinghua.edu.cn;fw245@cornell.edu;lvdmaaten@fb.com;kqw4@cornell.edu,8;7;10,4;4;4,Accept (Oral),0,3,2,yes,10/27/17,Cornell University;Fudan University;Tsinghua University;Cornell University;Facebook;Cornell University,7;78;10;7;-1;7,19;116;30;19;-1;19,,3/29/17,141,64,44,3,0,20,12053;405;2426;886;17950;23407,60;22;119;18;93;165,22;9;28;10;32;54,2030;35;124;176;1265;3769,-1;-1
708,ICLR,2018,Sobolev GAN,Youssef Mroueh;Chun-Liang Li;Tom Sercu;Anant Raj;Yu Cheng,mroueh@us.ibm.com;chunlial@cs.cmu.edu;tom.sercu1@ibm.com;anant.raj@tuebingen.mpg.de;chengyu@us.ibm.com,8;6;6;7,4;3;4;3,Accept (Poster),0,16,0,yes,10/27/17,"International Business Machines;Carnegie Mellon University;International Business Machines;Max Planck Institute for Intelligent Systems, Max-Planck Institute;International Business Machines",-1;1;-1;-1;-1,-1;24;-1;-1;-1,5;4,10/27/17,26,7,6,0,5,3,930;1372;796;307;2479,53;90;25;31;173,11;18;11;7;25,153;118;71;23;293,-1;-1
709,ICLR,2018,Compositional Attention Networks for Machine Reasoning,Drew A. Hudson;Christopher D. Manning,dorarad@cs.stanford.edu;manning@cs.stanford.edu,7;7;6,4;3;4,Accept (Poster),3,7,1,yes,10/27/17,Stanford University;Stanford University,4;4,3;3,8,10/27/17,178,82,68,10,185,36,345;88144,6;482,4;114,73;11626,-1;-1
710,ICLR,2018,DCN+: Mixed Objective And Deep Residual Coattention for Question Answering,Caiming Xiong;Victor Zhong;Richard Socher,cxiong@salesforce.com;richard@socher.org;victor@victorzhong.com,7;6;8,4;4;2,Accept (Poster),1,13,0,yes,10/27/17,SalesForce.com;SalesForce.com;SalesForce.com,-1;-1;-1,-1;-1;-1,,10/27/17,61,34,27,3,63,10,6187;1841;51920,156;18;180,31;11;48,1048;344;8747,-1;-1
711,ICLR,2018,Empirical Risk Landscape Analysis for Understanding Deep Neural Networks,Pan Zhou;Jiashi Feng,pzhou@u.nus.edu;elefjia@nus.edu.sg;panzhou3@gmail.com,3;7;7,3;3;3,Accept (Poster),0,3,0,yes,10/23/17,National University of Singapore;National University of Singapore;National University of Singapore,16;16;16,22;22;22,1;9;8,10/23/17,5,2,0,0,0,0,2416;9219,254;328,23;51,161;1200,-1;-1
712,ICLR,2018,Monotonic Chunkwise Attention,Chung-Cheng Chiu*;Colin Raffel*,chungchengc@google.com;craffel@gmail.com,7;6;8,5;4;4,Accept (Poster),0,4,0,yes,10/27/17,Google;Google,-1;-1,-1;-1,,10/27/17,72,44,30,2,78,9,1745;4545,99;63,19;22,145;465,-1;-1
713,ICLR,2018,Compressing Word Embeddings via Deep Compositional Code Learning,Raphael Shu;Hideki Nakayama,shu@nlab.ci.i.u-tokyo.ac.jp;nakayama@ci.i.u-tokyo.ac.jp,8;6;7,4;4;4,Accept (Poster),0,8,0,yes,10/27/17,The University of Tokyo;The University of Tokyo,52;52,45;45,3;2,10/27/17,55,30,25,2,69,15,95;618,14;87,4;14,20;67,-1;-1
714,ICLR,2018,A Framework for the Quantitative Evaluation of Disentangled Representations,Cian Eastwood;Christopher K. I. Williams,s1668298@ed.ac.uk;ckiw@inf.ed.ac.uk,7;6;6,5;5;4,Accept (Poster),0,4,0,yes,10/27/17,University of Edinburgh;University of Edinburgh,33;33,27;27,,10/27/17,81,43,31,6,0,21,125;28765,4;224,3;49,24;4723,-1;-1
715,ICLR,2018,Adaptive Dropout with Rademacher Complexity Regularization,Ke Zhai;Huan Wang,zhaikedavy@gmail.com;joyousprince@gmail.com,6;6;7,3;3;5,Accept (Poster),0,17,1,yes,10/26/17,;,-1;-1,-1;-1,1,10/26/17,11,4,3,1,0,1,77;2147,12;317,5;21,8;133,-1;-1
716,ICLR,2018,An Online Learning Approach to Generative Adversarial Networks,Paulina Grnarova;Kfir Y Levy;Aurelien Lucchi;Thomas Hofmann;Andreas Krause,paulina.grnarova@inf.ethz.ch;yehuda.levy@inf.ethz.ch;aurelien.lucchi@inf.ethz.ch;thomas.hofmann@inf.ethz.ch;krausea@ethz.ch,7;8;5,4;5;4,Accept (Poster),0,8,0,yes,10/26/17,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,9;9;9;9;9,10;10;10;10;10,5;4,6/10/17,53,27,13,2,37,7,150;402;6757;22638;16294,9;19;68;173;241,6;11;24;52;65,19;69;1105;3390;1758,-1;-1
717,ICLR,2018,Learning Robust Rewards with Adverserial Inverse Reinforcement Learning,Justin Fu;Katie Luo;Sergey Levine,justinjfu@eecs.berkeley.edu;katieluo@berkeley.edu;svlevine@eecs.berkeley.edu,6;6;7,4;2;3,Accept (Poster),4,11,1,yes,10/27/17,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,18;18;18,4,10/27/17,157,96,62,2,10,36,531;149;24235,20;1;309,9;1;73,79;33;3140,-1;-1
718,ICLR,2018,Variational Network Quantization,Jan Achterhold;Jan Mathias Koehler;Anke Schmeink;Tim Genewein,mail@janachterhold.de;jan.koehler@de.bosch.com;anke.schmeink@rwth-aachen.de;tim.genewein@gmail.com,7;7;7,5;4;3,Accept (Poster),1,5,1,yes,10/27/17,;Bosch;RWTH Aachen University;Bosch,-1;-1;99;-1,-1;-1;79;-1,11,10/27/17,34,17,15,0,0,6,38;255;903;784,5;9;167;27,2;5;16;9,7;33;39;63,-1;-1
719,ICLR,2018,Identifying Analogies Across Domains,Yedid Hoshen;Lior Wolf,yedidh@fb.com;wolf@fb.com,7;5;4,4;4;3,Accept (Poster),0,8,0,yes,10/27/17,Facebook;Facebook,-1;-1,-1;-1,,10/27/17,10,2,1,0,0,0,482;13787,31;199,9;45,57;1636,-1;-1
720,ICLR,2018,Improving the Improved Training of Wasserstein GANs: A Consistency Term and Its Dual Effect,Xiang Wei;Boqing Gong;Zixia Liu;Wei Lu;Liqiang Wang,yqweixiang@knights.ucf.edu;boqinggo@outlook.com;zixia@knights.ucf.edu;luwei@bjtu.edu.cn;lwang@cs.ucf.edu,4;6;7,4;5;4,Accept (Poster),16,11,0,yes,10/27/17,University of Central Florida;International Computer Science Institute;University of Central Florida;Beijing jiaotong univercity;University of Central Florida,81;-1;81;468;81,1103;-1;1103;854;1103,5;4,10/27/17,82,41,38,5,4,19,1506;3750;164;15378;1784,273;85;11;887;142,19;24;6;58;24,54;750;20;921;162,-1;-1
721,ICLR,2018,Unbiased Online Recurrent Optimization,Corentin Tallec;Yann Ollivier,corentin.tallec@polytechnique.edu;yann@yann-ollivier.org,6;7;8,4;4;5,Accept (Poster),0,5,0,yes,10/27/17,Ecole polytechnique;Facebook,468;-1,115;-1,10,2/16/17,36,13,15,0,34,9,148;1547,13;131,5;19,20;191,-1;-1
722,ICLR,2018,Can recurrent neural networks warp time?,Corentin Tallec;Yann Ollivier,corentin.tallec@polytechnique.edu;yol@fb.com,8;8;8,4;4;4,Accept (Poster),0,9,0,yes,10/27/17,Ecole polytechnique;Facebook,468;-1,115;-1,1,10/27/17,45,16,14,1,11,3,148;1547,13;131,5;19,20;191,-1;-1
723,ICLR,2018,Twin Networks: Matching the Future for Sequence Generation,Dmitriy Serdyuk;Nan Rosemary Ke;Alessandro Sordoni;Adam Trischler;Chris Pal;Yoshua Bengio,serdyuk.dmitriy@gmail.com;rosemary.nan.ke@gmail.com;alessandro.sordoni@gmail.com;adam.trischler@microsoft.com;chris.j.pal@gmail.com;yoshua.umontreal@gmail.com,6;7;8,4;4;4,Accept (Poster),0,10,0,yes,10/27/17,Element AI;Polytechnique Montreal;Microsoft;Microsoft;Ecole Polytechnique de Montreal;University of Montreal,-1;364;-1;-1;365;124,-1;108;-1;-1;1103;108,5,8/22/17,31,15,16,0,0,5,3822;744;3780;1533;8128;200865,15;32;54;47;120;807,11;13;19;17;33;147,303;76;535;285;758;23899,-1;-1
724,ICLR,2018,Semantically Decomposing the Latent Spaces of Generative Adversarial Networks,Chris Donahue;Zachary C. Lipton;Akshay Balsubramani;Julian McAuley,cdonahue@ucsd.edu;zlipton@cmu.edu;abalsubr@stanford.edu;jmcauley@cs.ucsd.edu,6;6;7,4;4;4,Accept (Poster),0,4,0,yes,10/27/17,"University of California, San Diego;Carnegie Mellon University;Stanford University;University of California, San Diego",11;1;4;11,31;24;3;31,5;4,5/22/17,64,36,17,0,123,6,426;4677;283;7324,16;97;25;133,7;27;7;32,60;425;33;1026,-1;-1
725,ICLR,2018,Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting,Yaguang Li;Rose Yu;Cyrus Shahabi;Yan Liu,yaguang@usc.edu;rose@caltech.edu;shahabi@usc.edu;yanliu.cs@usc.edu,5;4;9,3;5;5,Accept (Poster),0,4,1,yes,10/27/17,University of Southern California;California Institute of Technology;University of Southern California;University of Southern California,31;139;31;31,66;3;66;66,10,7/6/17,318,177,129,7,54,59,853;845;11014;505,72;38;497;57,13;11;50;9,101;99;956;67,-1;-1
726,ICLR,2018,PixelNN: Example-based Image Synthesis,Aayush Bansal;Yaser Sheikh;Deva Ramanan,aayushb@cs.cmu.edu;yaser@cs.cmu.edu;deva@cs.cmu.edu,8;6;7,4;4;3,Accept (Poster),0,3,0,yes,10/27/17,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1,24;24;24,5,8/17/17,28,17,7,2,0,1,443;8749;31205,24;125;163,8;36;59,39;1293;5320,-1;-1
727,ICLR,2018,Rethinking the Smaller-Norm-Less-Informative Assumption in Channel Pruning of Convolution Layers,Jianbo Ye;Xin Lu;Zhe Lin;James Z. Wang,jxy198@ist.psu.edu;xinl@adobe.com;zlin@adobe.com;jwang@ist.psu.edu,5;7;6,5;3;5,Accept (Poster),0,6,1,yes,10/25/17,Pennsylvania State University;Adobe Systems;Adobe Systems;Pennsylvania State University,40;-1;-1;40,77;-1;-1;77,10,10/25/17,107,51,48,6,0,17,463;15254;9970;2688,33;758;132;141,10;56;47;22,59;1110;1538;308,-1;-1
728,ICLR,2018,A Deep Reinforced Model for Abstractive Summarization,Romain Paulus;Caiming Xiong;Richard Socher,rpaulus@salesforce.com;cxiong@salesforce.com;richard@socher.org,8;7;6,3;5;4,Accept (Poster),0,3,0,yes,10/27/17,SalesForce.com;SalesForce.com;SalesForce.com,-1;-1;-1,-1;-1;-1,,5/11/17,578,330,283,28,0,116,1381;6187;52264,7;156;180,4;31;49,208;1048;8807,-1;-1
729,ICLR,2018,The Role of Minimal Complexity Functions in Unsupervised Learning of Semantic Mappings,Tomer Galanti;Lior Wolf;Sagie Benaim,tomer22g@gmail.com;liorwolf@gmail.com;sagiebenaim@gmail.com,7;7;6,4;2;4,Accept (Poster),0,6,0,yes,10/27/17,Tel Aviv University;Tel Aviv University;Tel Aviv University,37;37;37,217;217;217,,8/31/17,17,9,6,0,5,1,70;13787;168,18;199;15,5;45;5,1;1636;26,-1;-1
730,ICLR,2018,Emergent Translation in Multi-Agent Communication,Jason Lee;Kyunghyun Cho;Jason Weston;Douwe Kiela,jason@cs.nyu.edu;kyunghyun.cho@nyu.edu;jase@fb.com;dkiela@fb.com,8;7;5,5;3;5,Accept (Poster),0,5,0,yes,10/27/17,New York University;New York University;Facebook;Facebook,26;26;-1;-1,27;27;-1;-1,3,10/12/17,32,23,8,0,40,1,400;45318;44263;3389,8;272;243;79,5;52;76;29,77;6549;5812;579,-1;-1
731,ICLR,2018,Few-shot Autoregressive Density Estimation: Towards Learning to Learn Distributions,Scott Reed;Yutian Chen;Thomas Paine;Aäron van den Oord;S. M. Ali Eslami;Danilo Rezende;Oriol Vinyals;Nando de Freitas,reedscot@google.com;yutianc@google.com;tpaine@google.com;avdnoord@google.com;aeslami@google.com;danilor@google.com;vinyals@google.com;nandodefreitas@google.com,6;7;6,5;4;4,Accept (Poster),1,1,0,yes,10/27/17,Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1,6,10/27/17,39,17,15,1,76,0,10951;943;474;8527;4020;8567;51899;18851,28;87;197;40;38;63;121;184,16;16;11;24;19;27;55;54,2049;105;22;1093;511;1106;6475;1849,-1;-1
732,ICLR,2018,Kronecker-factored Curvature Approximations for Recurrent Neural Networks,James Martens;Jimmy Ba;Matt Johnson,james.martens@gmail.com;jimmy@psi.toronto.edu;mattjj@csail.mit.edu,7;5;7,4;4;3,Accept (Poster),1,9,0,yes,10/27/17,Google;University of Toronto;Massachusetts Institute of Technology,-1;17;2,-1;22;5,10,10/27/17,24,6,4,0,0,1,5510;51293;345,39;52;15,18;21;7,634;8393;29,-1;-1
733,ICLR,2018,DORA The Explorer: Directed Outreaching Reinforcement Action-Selection,Lior Fox;Leshem Choshen;Yonatan Loewenstein,lior.fox@mail.huji.ac.il;leshem.choshen@mail.huji.ac.il;yonatan.loewenstein@mail.huji.ac.il,7;6;6,3;4;4,Accept (Poster),0,5,0,yes,10/27/17,Hebrew University of Jerusalem;Hebrew University of Jerusalem;Hebrew University of Jerusalem,62;62;62,205;205;205,8,10/27/17,24,12,11,0,15,7,27;115;1370,4;17;70,2;7;20,7;14;93,-1;-1
734,ICLR,2018,The Reactor: A fast and sample-efficient Actor-Critic agent for  Reinforcement Learning,Audrunas Gruslys;Will Dabney;Mohammad Gheshlaghi Azar;Bilal Piot;Marc Bellemare;Remi Munos,audrunas@google.com;wdabney@google.com;mazar@google.com;piot@google.com;bellemare@google.com;munos@google.com,7;7;7,4;2;4,Accept (Poster),0,6,0,yes,10/27/17,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,,4/15/17,37,17,12,2,0,2,1009;1733;1569;2030;3924;9222,18;25;32;70;57;190,11;13;15;18;24;53,112;331;281;264;620;1301,-1;-1
735,ICLR,2018,Gaussian Process Behaviour in Wide Deep Neural Networks,Alexander G. de G. Matthews;Jiri Hron;Mark Rowland;Richard E. Turner;Zoubin Ghahramani,am554@cam.ac.uk;jh2084@cam.ac.uk;mr504@cam.ac.uk;ret26@cam.ac.uk;zoubin@eng.cam.ac.uk,6;6;6,4;4;4,Accept (Poster),0,3,1,yes,10/27/17,University of Cambridge;University of Cambridge;University of Cambridge;University of Cambridge;University of Cambridge,71;71;71;71;71,2;2;2;2;2,11;9,10/27/17,129,79,23,9,62,18,970;438;839;2936;42348,24;28;36;175;463,12;7;11;30;91,129;61;58;308;5064,-1;-1
736,ICLR,2018,On the Expressive Power of Overlapping Architectures of Deep Learning,Or Sharir;Amnon Shashua,or.sharir@cs.huji.ac.il;shashua@cs.huji.ac.il,6;8;6,4;3;4,Accept (Poster),0,3,0,yes,10/27/17,Hebrew University of Jerusalem;Hebrew University of Jerusalem,62;62,205;205,,3/6/17,9,6,1,2,0,1,328;8190,15;173,9;48,19;735,-1;-1
737,ICLR,2018,An image representation based convolutional network for DNA classification,Bojian Yin;Marleen Balvert;Davide Zambrano;Alexander Schoenhuth;Sander Bohte,yinbojian93@gmail.com;m.balvert@cwi.nl;d.zambrano@cwi.nl;a.schoenhuth@cwi.nl;s.m.bohte@cwi.nl,7;7;7,5;3;5,Accept (Poster),0,11,0,yes,10/27/17,cwi;Centrum voor Wiskunde en Informatica;Centrum voor Wiskunde en Informatica;Centrum voor Wiskunde en Informatica;Centrum voor Wiskunde en Informatica,210;-1;-1;-1;-1,1103;-1;-1;-1;-1,,10/27/17,8,2,1,0,0,1,13;80;115;1762;2240,5;24;20;98;117,2;5;7;20;22,1;3;8;113;234,-1;-1
738,ICLR,2018,Deep Learning and Quantum Entanglement: Fundamental Connections with Implications to Network Design,Yoav Levine;David Yakira;Nadav Cohen;Amnon Shashua,yoavlevine@cs.huji.ac.il;davidyakira@cs.huji.ac.il;cohennadav@cs.huji.ac.il;shashua@cs.huji.ac.il,6;7;8;6,4;3;5;2,Accept (Poster),0,6,0,yes,10/26/17,Hebrew University of Jerusalem;Hebrew University of Jerusalem;Hebrew University of Jerusalem;Hebrew University of Jerusalem,62;62;62;62,205;205;205;205,10,4/5/17,50,26,12,0,179,4,137;83;1024;8190,9;8;63;173,6;5;15;48,7;5;118;735,-1;-1
739,ICLR,2018,Critical Percolation as a Framework to Analyze the Training of Deep Networks,Zohar Ringel;Rodrigo Andrade de Bem,zoharahoz@gmail.com;rodrigo.bem@gmail.com,7;7;6,3;3;1,Accept (Poster),0,3,0,yes,10/26/17,Hebrew University of Jerusalem;University of Oxford,62;51,205;1,10,10/26/17,1,0,0,0,3,0,541;0,33;3,7;0,17;0,-1;-1
740,ICLR,2018,Parallelizing Linear Recurrent Neural Nets Over Sequence Length,Eric Martin;Chris Cundy,eric@ericmart.in;chris.j.cundy@gmail.com,6;7;7,3;2;4,Accept (Poster),0,3,0,yes,10/27/17,California Institute of Technology;University of Cambridge,139;71,3;2,,9/12/17,10,7,2,0,18,1,229;19,26;5,7;3,12;4,-1;-1
741,ICLR,2018,Guide Actor-Critic for Continuous Control,Voot Tangkaratt;Abbas Abdolmaleki;Masashi Sugiyama,voot.tangkaratt@riken.jp;abbas.a@ua.pt;masashi.sugiyama@riken.jp,6;4;7,2;4;4,Accept (Poster),0,5,0,yes,10/27/17,RIKEN;;RIKEN,-1;-1;-1,-1;-1;-1,,5/22/17,6,3,2,1,7,2,202;520;11537,25;45;712,8;10;52,18;55;1293,-1;-1
742,ICLR,2018,Multi-Task Learning for Document Ranking and Query Suggestion,Wasi Uddin Ahmad;Kai-Wei Chang;Hongning Wang,wasiahmad@cs.ucla.edu;kwchang@cs.ucla.edu;hw5x@virginia.edu,4;6;7,4;4;4,Accept (Poster),0,5,0,yes,10/27/17,"University of California, Los Angeles;University of California, Los Angeles;University of Virginia",20;20;62,15;15;113,,10/27/17,12,8,6,1,0,1,78;9957;1921,19;84;93,6;25;22,7;1390;198,-1;-1
743,ICLR,2018,Hyperparameter optimization: a spectral approach,Elad Hazan;Adam Klivans;Yang Yuan,ehazan@cs.princeton.edu;klivans@cs.utexas.edu;yangyuan@cs.cornell.edu,6;6;9,4;3;5,Accept (Poster),0,3,0,yes,10/27/17,"Princeton University;University of Texas, Austin;Cornell University",31;21;7,7;49;19,11,6/2/17,40,19,19,1,110,6,11853;2253;4923,139;92;391,43;27;32,1996;186;384,-1;-1
744,ICLR,2018,Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence Learning,Wei Ping;Kainan Peng;Andrew Gibiansky;Sercan O. Arik;Ajay Kannan;Sharan Narang;Jonathan Raiman;John Miller,pingwei01@baidu.com;pengkainan@baidu.com;gibianskyandrew@baidu.com;sercanarik@baidu.com;kannanajay@baidu.com;sharan@baidu.com;raiman@openai.com;miller_john@berkeley.edu,7;6;6,4;5;3,Accept (Poster),0,4,0,yes,10/24/17,Baidu;Baidu;Baidu;Baidu;Baidu;Baidu;OpenAI;University of California Berkeley,-1;-1;-1;-1;-1;-1;-1;5,-1;-1;-1;-1;-1;-1;-1;18,,10/20/17,104,52,48,1,36,14,966;562;674;1328;458;2495;2061;1607,208;11;7;54;20;15;14;244,12;7;7;17;6;10;8;18,110;72;74;118;64;269;210;139,-1;-1
745,ICLR,2018,Divide and Conquer Networks,Alex Nowak;David Folqué;Joan Bruna,alexnowakvila@gmail.com;david.folque@gmail.com;bruna@cims.nyu.edu,6;7;7,3;3;3,Accept (Poster),0,2,0,yes,10/27/17,Ecole Normale Superieure;Universitat Politècnica de Catalunya;New York University,99;468;26,72;1103;27,8,11/8/16,5,2,1,0,0,0,64;4;11337,10;2;90,4;1;29,3;0;1271,-1;-1
746,ICLR,2018,When is a Convolutional Filter Easy to Learn?,Simon S. Du;Jason D. Lee;Yuandong Tian,ssdu@cs.cmu.edu;jasonlee@marshall.usc.edu;yuandong@fb.com,6;9;8,3;4;3,Accept (Poster),0,5,0,yes,10/25/17,Carnegie Mellon University;University of Southern California;Facebook,1;31;-1,24;66;-1,1;9,9/18/17,89,48,19,6,41,10,1989;4585;2421,55;118;84,19;36;25,302;599;282,-1;-1
747,ICLR,2018,SpectralNet: Spectral Clustering using Deep Neural Networks,Uri Shaham;Kelly Stanton;Henry Li;Ronen Basri;Boaz Nadler;Yuval Kluger,uri.shaham@yale.edu;kelly.stanton@yale.edu;henry.li@yale.edu;ronen.basri@gmail.com;boaz.nadler@gmail.com;yuval.kluger@yale.edu,6;4;7,3;4;5,Accept (Poster),2,7,1,yes,10/26/17,Yale University;Yale University;Yale University;Weizmann Institute;;Yale University,62;62;62;104;-1;62,12;12;12;1103;-1;12,10;1;8,10/26/17,74,44,30,2,30,14,744;86;197;10551;327;7149,24;7;19;173;28;169,11;3;6;44;9;45,56;14;18;1267;30;510,-1;-1
748,ICLR,2018,Boundary Seeking GANs,R Devon Hjelm;Athul Paul Jacob;Adam Trischler;Gerry Che;Kyunghyun Cho;Yoshua Bengio,erroneus@gmail.com;apjacob@uwaterloo.ca;adam.trischler@microsoft.com;tong.che@umontreal.ca;kyunghyun.cho@nyu.edu;yoshua.bengio@umontreal.ca,7;7;4,4;3;3,Accept (Poster),0,12,0,yes,10/27/17,University of Montreal;University of Waterloo;Microsoft;University of Montreal;New York University;University of Montreal,124;26;-1;124;26;124,108;207;-1;108;27;108,3;4;5,10/27/17,5,4,4,1,0,3,1636;362;1533;5;45318;200865,43;6;47;1;272;807,13;5;17;1;52;147,258;55;285;3;6549;23899,-1;-1
749,ICLR,2018,Natural Language Inference over Interaction Space,Yichen Gong;Heng Luo;Jian Zhang,yichen.gong@nyu.edu;heng.luo@hobot.cc;jian.zhang@hobot.cc,5;6;6,5;4;4,Accept (Poster),0,7,2,yes,10/19/17,New York University;Horizon Robotics Inc.;Horizon Robotics Inc.,26;-1;-1,27;-1;-1,3,9/13/17,125,47,47,5,33,28,180;1655;-1,7;118;-1,4;21;-1,31;134;0,-1;-1
750,ICLR,2018,Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks,Shiyu Liang;Yixuan Li;R. Srikant,sliang26@illinois.edu;yli@cs.cornell.edu;rsrikant@illinois.edu;liangshiyu@icloud.com;yl2363@cornell.edu,6;6;9,4;3;3,Accept (Poster),0,6,1,yes,10/27/17,"University of Illinois, Urbana Champaign;Cornell University;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;Cornell University",3;7;3;3;7,37;19;37;37;19,,6/8/17,240,117,126,15,0,62,576;2242;16859,15;162;427,8;20;68,91;258;1485,-1;-1
751,ICLR,2018,Deep Autoencoding Gaussian Mixture Model for Unsupervised Anomaly Detection,Bo Zong;Qi Song;Martin Renqiang Min;Wei Cheng;Cristian Lumezanu;Daeki Cho;Haifeng Chen,bzong@nec-labs.com;qsong@nec-labs.com;renqiang@nec-labs.com;weicheng@nec-labs.com;lume@nec-labs.com;dkcho@nec-labs.com;haifeng@nec-labs.com,8;8;8,5;4;4,Accept (Poster),0,8,0,yes,10/27/17,NEC-Labs;NEC-Labs;NEC-Labs;NEC-Labs;NEC-Labs;NEC-Labs;NEC-Labs,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,,10/27/17,173,64,83,4,0,40,588;1403;875;3726;1328;2084;2563,53;133;58;412;58;7;156,11;18;14;29;21;4;29,62;111;103;267;142;380;227,-1;-1
752,ICLR,2018,Smooth Loss Functions for Deep Top-k Classification,Leonard Berrada;Andrew Zisserman;M. Pawan Kumar,lberrada@robots.ox.ac.uk;az@robots.ox.ac.uk;pawan@robots.ox.ac.uk,6;7;8,5;4;4,Accept (Poster),0,4,0,yes,10/27/17,University of Oxford;University of Oxford;University of Oxford,51;51;51,1;1;1,2,10/27/17,29,13,11,0,3,4,49;136537;2610,5;796;83,3;140;24,6;19871;248,-1;-1
753,ICLR,2018,Active Learning for Convolutional Neural Networks: A Core-Set Approach,Ozan Sener;Silvio Savarese,ozansener@cs.stanford.edu;ssilvio@stanford.edu,7;7;7,4;4;3,Accept (Poster),0,10,2,yes,10/27/17,Stanford University;Stanford University,4;4,3;3,,8/1/17,194,115,81,13,24,42,1145;16417,33;283,12;65,206;2485,-1;-1
754,ICLR,2018,Debiasing Evidence Approximations: On Importance-weighted Autoencoders and Jackknife Variational Inference,Sebastian Nowozin,sebastian.nowozin@microsoft.com,7;6;7,4;3;3,Accept (Poster),2,6,1,yes,10/27/17,Microsoft,-1,-1,5;1,10/27/17,20,10,11,1,0,5,6820,134,39,928,-1
755,ICLR,2018,Unsupervised Machine Translation Using Monolingual Corpora Only,Guillaume Lample;Alexis Conneau;Ludovic Denoyer;Marc'Aurelio Ranzato,glample@fb.com;aconneau@fb.com;ludovic.denoyer@lip6.fr;ranzato@fb.com,8;7;7,5;5;4,Accept (Poster),3,8,0,yes,10/27/17,Facebook;Facebook;LIP6;Facebook,-1;-1;-1;-1,-1;-1;-1;-1,3,10/27/17,441,265,183,18,662,79,4827;3646;3058;20698,24;17;129;75,18;14;22;43,1004;842;535;2083,-1;-1
756,ICLR,2018,Word translation without parallel data,Guillaume Lample;Alexis Conneau;Marc'Aurelio Ranzato;Ludovic Denoyer;Hervé Jégou,glample@fb.com;aconneau@fb.com;ranzato@fb.com;ludovic.denoyer@upmc.fr;rvj@fb.com,9;3;8,4;5;3,Accept (Poster),2,10,0,yes,10/11/17,"Facebook;Facebook;Facebook;Computer Science Lab  - Pierre and Marie Curie University, Paris, France;Facebook",-1;-1;-1;468;-1,-1;-1;-1;123;-1,3;4,10/11/17,583,299,318,40,203,180,4755;3462;20172;3000;13587,24;17;75;128;164,18;14;42;22;40,993;790;2051;524;2363,-1;-1
757,ICLR,2018,Wavelet Pooling for Convolutional Neural Networks,Travis Williams;Robert Li,tlwilli3@aggies.ncat.edu;eeli@ncat.edu,7;9;4,4;3;4,Accept (Poster),0,5,3,yes,10/27/17,North Carolina A&T State University;North Carolina A&T State University,468;468,1103;1103,,10/27/17,31,12,19,1,0,7,71;470,24;39,4;9,10;37,-1;-1
758,ICLR,2018,WRPN: Wide Reduced-Precision Networks,Asit Mishra;Eriko Nurvitadhi;Jeffrey J Cook;Debbie Marr,asit.k.mishra@intel.com;eriko.nurvitadhi@intel.com;jeffrey.j.cook@intel.com;debbie.marr@intel.com,5;9;5,3;4;4,Accept (Poster),0,8,0,yes,10/27/17,Intel;Intel;Intel;Intel,-1;-1;-1;-1,-1;-1;-1;-1,2,9/4/17,120,61,41,7,9,21,3118;1191;716;836,78;54;59;29,28;18;10;10,306;102;54;81,-1;-1
759,ICLR,2018,Skip Connections Eliminate Singularities,Emin Orhan;Xaq Pitkow,aeminorhan@gmail.com;xaq@rice.edu,8;8;6,3;3;4,Accept (Poster),0,4,0,yes,10/27/17,Rice University;Rice University,85;85,86;86,,1/31/17,63,41,7,3,5,4,141;849,10;33,3;11,14;55,-1;-1
760,ICLR,2018,Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step,William Fedus*;Mihaela Rosca*;Balaji Lakshminarayanan;Andrew M. Dai;Shakir Mohamed;Ian Goodfellow,liam.fedus@gmail.com;mihaelacr@google.com;balajiln@google.com;adai@google.com;shakir@google.com;goodfellow@google.com,8;4;7,5;4;3,Accept (Poster),6,7,0,yes,10/27/17,University of Montreal;Google;Google;Google;Google;Google,124;-1;-1;-1;-1;-1,108;-1;-1;-1;-1;-1,5;4,10/23/17,102,59,29,5,0,16,665;416;2879;3687;6867;55154,25;14;43;49;51;90,10;7;22;19;27;56,83;51;374;456;995;9293,-1;-1
761,ICLR,2018,YellowFin and the Art of Momentum Tuning,Jian Zhang;Ioannis Mitliagkas;Christopher Re,zjian@cs.stanford.edu;ioannis@iro.umontreal.ca;chrismre@cs.stanford.edu,4;4;6,3;5;1,Reject,0,11,0,yes,10/27/17,Stanford University;University of Montreal;Stanford University,4;124;4,3;108;3,3,6/12/17,63,21,22,1,52,5,2446;1164;7223,40;43;211,12;18;44,588;189;704,-1;-1
762,ICLR,2018,HexaConv,Emiel Hoogeboom;Jorn W.T. Peters;Taco S. Cohen;Max Welling,e.hoogeboom@gmail.com;jornpeters@gmail.com;taco.cohen@gmail.com;welling.max@gmail.com,7;7;7,4;4;4,Accept (Poster),0,5,0,yes,10/27/17,University of Amsterdam;University of Amsterdam;University of Amsterdam;University of California - Irvine,181;181;181;36,59;59;59;99,,10/27/17,10,3,2,0,6,2,81;48;19;26464,9;3;6;269,4;3;2;58,11;8;2;5073,-1;-1
763,ICLR,2018,Few-Shot Learning with Graph Neural Networks,Victor Garcia Satorras;Joan Bruna Estrach,vgsatorras@gmail.com;bruna@cims.nyu.edu,7;7;7,4;4;4,Accept (Poster),0,13,0,yes,10/27/17,New York University;New York University,26;26,27;27,6;10,10/27/17,256,117,76,6,29,31,262;11337,6;90,2;29,31;1271,-1;-1
764,ICLR,2018,Proximal Backpropagation,Thomas Frerix;Thomas Möllenhoff;Michael Moeller;Daniel Cremers,thomas.frerix@tum.de;thomas.moellenhoff@in.tum.de;michael.moeller@uni-siegen.de;cremers@tum.de,6;5;7,4;4;4,Accept (Poster),0,5,0,yes,10/27/17,Technical University Munich;Technical University Munich;University of Siegen;Technical University Munich,55;55;364;55,41;41;431;41,1,6/14/17,16,7,9,0,5,2,142;145;1128;24856,7;17;97;494,3;6;16;76,20;16;128;2726,-1;-1
765,ICLR,2018,Wasserstein Auto-Encoders,Ilya Tolstikhin;Olivier Bousquet;Sylvain Gelly;Bernhard Schoelkopf,iliya.tolstikhin@gmail.com;obousquet@gmail.com;sylvain.gelly@gmail.com;bs@tuebingen.mpg.de,8;8;8,3;3;4,Accept (Oral),0,9,8,yes,10/27/17,"Max-Planck Institute;Google;Google;Max Planck Institute for Intelligent Systems, Max-Planck Institute",-1;-1;-1;-1,-1;-1;-1;-1,5;4;8,10/27/17,290,110,117,5,0,63,744;13199;3505;75018,30;235;112;860,10;39;25;119,122;1945;454;9886,-1;-1
766,ICLR,2018,The High-Dimensional Geometry of Binary Neural Networks,Alexander G. Anderson;Cory P. Berg,aga@berkeley.edu;cberg500@berkeley.edu,7;4;7,4;3;4,Accept (Poster),0,8,0,yes,10/27/17,University of California Berkeley;University of California Berkeley,5;5,18;18,,5/19/17,36,22,4,1,7,2,344;54,13;3,7;2,19;5,-1;-1
767,ICLR,2018,Critical Points of Linear Neural Networks: Analytical Forms and Landscape Properties,Yi Zhou;Yingbin Liang,zhou.1172@osu.edu;liang.889@osu.edu,7;7;6,3;5;4,Accept (Poster),0,6,0,yes,10/27/17,Ohio State University;Ohio State University,-1;-1,-1;-1,,10/27/17,56,36,3,7,4,6,584;4958,113;212,14;31,40;360,-1;-1
768,ICLR,2018,Overcoming Catastrophic Interference using Conceptor-Aided Backpropagation,Xu He;Herbert Jaeger,x.he@jacobs-university.de;h.jaeger@jacobs-university.de,7;7;7,5;3;3,Accept (Poster),0,4,0,yes,10/27/17,Jacobs University Bremen;Jacobs University Bremen,364;364,1103;1103,,7/16/17,43,18,13,1,6,5,3885;6485,503;111,31;24,165;961,-1;-1
769,ICLR,2018,Self-ensembling for visual domain adaptation,Geoff French;Michal Mackiewicz;Mark Fisher,g.french@uea.ac.uk;m.mackiewicz@uea.ac.uk;m.fisher@uea.ac.uk,7;7;7,4;3;5,Accept (Poster),1,5,2,yes,10/27/17,;;,-1;-1;-1,-1;-1;-1,,6/16/17,151,76,79,7,7,34,281;589;474,16;46;28,5;11;10,41;63;48,-1;-1
770,ICLR,2018,Synthesizing realistic neural population activity patterns using Generative Adversarial Networks,Manuel Molano-Mazon;Arno Onken;Eugenio Piasini*;Stefano Panzeri*,manuel.molano@iit.it;aonken@inf.ed.ac.uk;epiasini@sas.upenn.edu;stefano.panzeri@iit.it,8;4;6,5;4;3,Accept (Poster),1,4,0,yes,10/27/17,Istituto Italiano di Tecnologia;University of Edinburgh;University of Pennsylvania;Istituto Italiano di Tecnologia,468;33;19;468,1103;27;10;1103,5;4,10/27/17,10,0,3,0,4,0,203;188;217;9791,15;34;13;316,6;7;8;52,11;9;19;650,-1;-1
771,ICLR,2018,Minimal-Entropy Correlation Alignment for Unsupervised Deep Domain Adaptation,Pietro Morerio;Jacopo Cavazza;Vittorio Murino,pietro.morerio@iit.it;jacopo.cavazza@iit.it;vittorio.murino@iit.it,6;7;8,5;5;4,Accept (Poster),0,5,1,yes,10/27/17,Istituto Italiano di Tecnologia;Istituto Italiano di Tecnologia;Istituto Italiano di Tecnologia,468;468;468,1103;1103;1103,,10/27/17,46,22,15,0,3,7,604;156;8915,45;21;493,13;6;44,51;19;876,-1;-1
772,ICLR,2018,Activation Maximization Generative Adversarial Nets,Zhiming Zhou;Han Cai;Shu Rong;Yuxuan Song;Kan Ren;Weinan Zhang;Jun Wang;Yong Yu,heyohai@apex.sjtu.edu.cn;hcai@apex.sjtu.edu.cn;shu.rong@yitu-inc.com;songyuxuan@apex.sjtu.edu.cn;kren@apex.sjtu.edu.cn;wnzhang@sjtu.edu.cn;j.wang@cs.ucl.ac.uk;yyu@apex.sjtu.edu.cn,5;7;8,4;4;4,Accept (Poster),3,5,0,yes,10/27/17,Shanghai Jiao Tong University;Shanghai Jiao Tong University;YiTu Technology co. ltd;Shanghai Jiao Tong University;Shanghai Jiao Tong University;Shanghai Jiao Tong University;University College London;Shanghai Jiao Tong University,57;57;-1;57;57;57;46;57,188;188;-1;188;188;188;16;188,5;4,3/6/17,21,13,3,0,42,1,228;1160;276;52;583;4833;5614;27700,29;16;63;23;82;205;106;1552,8;10;6;4;12;31;27;71,20;258;11;3;46;688;932;2744,-1;-1
773,ICLR,2018,Efficient Sparse-Winograd Convolutional Neural Networks,Xingyu Liu;Jeff Pool;Song Han;William J. Dally,xyl@stanford.edu;jpool@nvidia.com;songhan@stanford.edu;dally@stanford.edu,7;7;8,3;4;4,Accept (Poster),0,6,1,yes,10/26/17,Stanford University;NVIDIA;Stanford University;Stanford University,4;-1;4;4,3;-1;3;3,,10/26/17,50,20,22,3,15,7,1446;2756;16093;31509,24;38;374;270,9;11;39;67,162;377;1970;3763,-1;-1
774,ICLR,2018,Neural Language Modeling by Jointly Learning Syntax and Lexicon,Yikang Shen;Zhouhan Lin;Chin-wei Huang;Aaron Courville,yikang.shn@gmail.com;lin.zhouhan@gmail.com;chin-wei.huang@umontreal.ca;aaron.courville@gmail.com,7;8;7,3;4;4,Accept (Poster),2,5,7,yes,10/27/17,University of Montreal;University of Montreal;University of Montreal;University of Montreal,124;124;124;124,108;108;108;108,3,10/27/17,54,32,22,6,9,19,301;4198;604;59549,15;28;53;203,7;13;13;64,71;396;74;7800,-1;-1
775,ICLR,2018,Imitation Learning from Visual Data with Multiple Intentions,Aviv Tamar;Khashayar Rohanimanesh;Yinlam Chow;Chris Vigorito;Ben Goodrich;Michael Kahane;Derik Pridmore,avivt@berkeley.edu;khash@osaro.com;yldick.chow@gmail.com;chris@osaro.com;ben@osaro.com;mk@osaro.com;derik@osaro.com,6;4;6,4;3;4,Accept (Poster),0,1,0,yes,10/27/17,University of California Berkeley;Osaro Inc.;Google;Osaro Inc.;Osaro Inc.;Osaro Inc.;Osaro Inc.,5;-1;-1;-1;-1;-1;-1,18;-1;-1;-1;-1;-1;-1,,10/27/17,3,2,0,0,0,0,2201;902;742;168;2640;19;2,50;34;50;98;30;10;2,20;13;14;7;11;2;1,331;88;85;7;403;1;0,-1;-1
776,ICLR,2018,Neumann Optimizer: A Practical Optimization Algorithm for Deep Neural Networks,Shankar Krishnan;Ying Xiao;Rif. A. Saurous,skrishnan@google.com;yingxiao@google.com;rif@google.com,6;6;6,4;3;3,Accept (Poster),1,9,0,yes,10/27/17,Google;Google;Google,-1;-1;-1,-1;-1;-1,,10/27/17,6,1,0,0,59,0,1954;2074;2474,119;160;31,25;20;15,114;136;420,-1;-1
777,ICLR,2018,TD or not TD: Analyzing the Role of Temporal Differencing in Deep Reinforcement Learning,Artemij Amiranashvili;Alexey Dosovitskiy;Vladlen Koltun;Thomas Brox,amiranas@cs.uni-freiburg.de;adosovitskiy@gmail.com;vkoltun@gmail.com;brox@cs.uni-freiburg.de,7;7;7,4;4;4,Accept (Poster),0,3,0,yes,10/27/17,Universität Freiburg;Intel;Intel;Universität Freiburg,115;-1;-1;115,82;-1;-1;82,,10/27/17,9,5,2,0,3,0,29;9430;17520;37720,9;57;188;255,4;31;62;72,0;1256;2489;5890,-1;-1
778,ICLR,2018,All-but-the-Top: Simple and Effective Postprocessing for Word Representations,Jiaqi Mu;Pramod Viswanath,jiaqimu2@illinois.edu;pramodv@illinois.edu,6;7;7,5;4;4,Accept (Poster),0,3,0,yes,10/26/17,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",3;3,37;37,3,2/5/17,69,26,28,2,137,10,116;369,15;20,4;8,14;37,-1;-1
779,ICLR,2018,Spectral Normalization for Generative Adversarial Networks,Takeru Miyato;Toshiki Kataoka;Masanori Koyama;Yuichi Yoshida,miyato@preferred.jp;kataoka@preferred.jp;koyama.masanori@gmail.com;yyoshida@nii.ac.jp,7;8;7,4;3;2,Accept (Oral),8,15,4,yes,10/23/17,"Preferred Networks, Inc.;Preferred Networks, Inc.;Preferred Networks, Inc.;Meiji University",-1;-1;-1;468,-1;-1;-1;334,5;4,10/23/17,1167,470,615,38,0,240,2758;1164;2374;2982,17;14;35;263,10;4;12;22,578;237;482;461,-1;-1
780,ICLR,2018,Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models,Wieland Brendel *;Jonas Rauber *;Matthias Bethge,wieland.brendel@bethgelab.org;jonas.rauber@bethgelab.org;matthias.bethge@bethgelab.org,7;7;8,4;4;3,Accept (Poster),2,9,0,yes,10/27/17,"Centre for Integrative Neuroscience, AG Bethge;Centre for Integrative Neuroscience, AG Bethge;Centre for Integrative Neuroscience, AG Bethge",-1;-1;-1,-1;-1;-1,4;2,10/27/17,316,188,103,5,114,64,1719;1212;11171,39;17;415,16;11;45,210;138;1245,-1;-1
781,ICLR,2018,Relational Neural Expectation Maximization: Unsupervised Discovery of Objects and their Interactions,Sjoerd van Steenkiste;Michael Chang;Klaus Greff;Jürgen Schmidhuber,sjoerd@idsia.ch;mbchang@berkeley.edu;klaus@idsia.ch;juergen@idsia.ch,8;7;7,5;4;3,Accept (Poster),0,6,0,yes,10/27/17,IDSIA;University of California Berkeley;IDSIA;IDSIA,-1;5;-1;-1,-1;18;-1;-1,,10/27/17,99,54,23,4,15,8,249;202;4124;64399,12;38;24;347,6;7;13;75,28;16;388;8263,-1;-1
782,ICLR,2018,cGANs with Projection Discriminator,Takeru Miyato;Masanori Koyama,miyato@preferred.jp;koyama.masanori@gmail.com,6;7;6,4;5;4,Accept (Poster),0,9,1,yes,10/27/17,"Preferred Networks, Inc.;Preferred Networks, Inc.",-1;-1,-1;-1,5,10/27/17,248,126,124,10,9,57,2758;2374,17;35,10;12,578;482,-1;-1
783,ICLR,2018,Latent Constraints: Learning to Generate Conditionally from Unconditional Generative Models,Jesse Engel;Matthew Hoffman;Adam Roberts,jesseengel@google.com;mhoffman@google.com;adarob@google.com,7;7;7,4;3;3,Accept (Poster),0,4,0,yes,10/27/17,Google;Google;Google,-1;-1;-1,-1;-1;-1,5;6,10/27/17,57,27,21,1,350,2,2164;263;10177,36;11;36,12;6;19,243;12;1426,-1;-1
784,ICLR,2018,Learning Intrinsic Sparse Structures within Long Short-Term Memory,Wei Wen;Yuxiong He;Samyam Rajbhandari;Minjia Zhang;Wenhan Wang;Fang Liu;Bin Hu;Yiran Chen;Hai Li,wei.wen@duke.edu;yuxhe@microsoft.com;samyamr@microsoft.com;minjiaz@microsoft.com;wenhanw@microsoft.com;fangliu@microsoft.com;binhu@microsoft.com;yiran.chen@duke.edu;hai.li@duke.edu,7;6;7,4;4;4,Accept (Poster),1,6,1,yes,10/1/17,Duke University;Microsoft;Microsoft;Microsoft;Microsoft;Microsoft;Microsoft;Duke University;Duke University,46;-1;-1;-1;-1;-1;-1;46;46,17;-1;-1;-1;-1;-1;-1;17;17,3,9/15/17,64,38,38,2,41,14,261;1601;241;907;63;35421;326;585;1452,59;102;21;40;3;2915;62;74;91,7;23;8;17;1;68;7;12;18,33;135;29;73;14;1645;30;46;117,-1;-1
785,ICLR,2018,Coupled Ensembles of Neural Networks,Anuvabh Dutt;Denis Pellerin;Georges Quénot,anuvabh.dutt@univ-grenoble-alpes.fr;denis.pellerin@gipsa-lab.grenoble-inp.fr;georges.quenot@imag.fr,6;6;6,4;4;4,Reject,0,4,0,yes,10/3/17,University of Grenoble-Alpes;University of Grenoble-Alpes;Imag Montpellier Université,468;468;-1,321;321;-1,,9/18/17,14,6,4,1,24,3,40;1283;1858,10;133;153,3;17;18,6;74;151,-1;-1
786,ICLR,2018,Exploring Sentence Vectors Through Automatic Summarization,Adly Templeton;Jugal Kalita,at7@williams.edu;jkalita@uccs.edu,2;2;3,5;5;5,Reject,0,0,0,yes,10/10/17,"Williams College;University of Colorado, Colorado Springs",468;468,1103;1103,3,10/10/17,4,1,0,0,11,0,4;3615,4;201,1;27,0;294,-1;-1
787,ICLR,2018,Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates,Leslie N. Smith;Nicholay Topin,leslie.smith@nrl.navy.mil;ntopin1@umbc.edu,4;4;4,3;4;3,Reject,2,4,0,yes,10/11/17,US Naval Research Laboratory;Boston College,-1;291,-1;309,1;8,8/23/17,105,36,30,3,342,9,1071;205,26;13,10;6,84;16,-1;-1
788,ICLR,2018,DOUBLY STOCHASTIC ADVERSARIAL AUTOENCODER,Mahdi Azarafrooz,mazarafrooz@cylance.com,3;3;2,4;5;5,Reject,0,3,0,yes,10/12/17,Cylance,-1,-1,5;4,10/12/17,1,0,1,0,12,0,29,14,3,3,-1
789,ICLR,2018,Incremental Learning through Deep Adaptation,Amir Rosenfeld;John K. Tsotsos,amir.rosenfeld@gmail.com,6;4;5,4;4;4,Reject,0,3,0,yes,10/13/17,York University,153,350,6,5/11/17,48,25,26,1,21,13,207;8232,22;372,7;40,25;749,-1;-1
790,ICLR,2018,Spontaneous Symmetry Breaking in Deep Neural Networks,Ricky Fok;Aijun An;Xiaogang Wang,ricky.fok3@gmail.com,3;3;3,4;3;3,Reject,0,0,0,yes,10/18/17,York University,153,350,,10/17/17,1,1,0,0,13,0,99;2787;37812,18;180;453,6;26;93,0;217;5059,-1;-1
791,ICLR,2018,Distributed non-parametric deep and wide networks,Biswa Sengupta;Yu Qian,biswasengupta@yahoo.com;yu.qian@cortexica.com,3;3;3,4;5;4,Reject,0,0,0,yes,10/18/17,Imperial College London;,74;-1,8;-1,,8/18/17,4,2,1,0,6,1,717;557,42;98,13;12,49;38,-1;-1
792,ICLR,2018,Ensemble Methods as a Defense to Adversarial Perturbations Against Deep Neural Networks,Thilo Strauss;Markus Hanselmann;Andrej Junginger;Holger Ulmer,thilo.strauss@etas.com;markus.hanselmann@etas.com;andrej.junginger@etas.com;holger.ulmer@etas.com,7;5;4,3;3;4,Reject,0,4,0,yes,10/18/17,ETAS GmbH;ETAS GmbH;ETAS GmbH;ETAS GmbH,-1;-1;-1;-1,-1;-1;-1;-1,4,9/11/17,47,25,15,1,22,3,88;63;127;700,14;8;9;40,4;3;7;15,5;5;9;51,-1;-1
793,ICLR,2018,Learning Less-Overlapping Representations,Hongbao Zhang;Pengtao Xie;Eric Xing,hongbao.zhang@petuum.com;pengtaox@cs.cmu.edu;eric.xing@petuum.com,5;4;3,4;4;5,Reject,0,0,0,yes,10/19/17,Petuum Inc.;Carnegie Mellon University;Petuum Inc.,-1;1;-1,-1;24;-1,8,10/19/17,2,2,0,0,6,1,32;1313;23945,17;70;603,4;17;75,4;153;2645,-1;-1
794,ICLR,2018,Reward Design in Cooperative Multi-agent Reinforcement Learning for Packet Routing,Hangyu Mao;Zhibo Gong;Zhen Xiao,pku.hy.mao@gmail.com;gongzhibo@huawei.com;gtxaio@gmail.com,5;2;5,3;4;2,Reject,0,3,0,yes,10/19/17,Peking University;Huawei Technologies Ltd.;,24;-1;-1,27;-1;-1,,10/19/17,2,0,1,0,0,0,37;41;183,12;9;26,3;4;5,6;6;24,-1;-1
795,ICLR,2018,Iterative temporal differencing with fixed random feedback alignment support spike-time dependent plasticity in vanilla backpropagation for deep learning,Aras Dargazany;Kunal Mankodiya,arasdar@uri.edu,3;2;2,4;5;5,Reject,0,0,0,yes,10/19/17,University of Rhode Island,468,1103,,10/19/17,0,0,0,0,0,0,53;1023,17;65,4;16,1;35,-1;-1
796,ICLR,2018,Reinforcement Learning via Replica Stacking of Quantum Measurements for the Training of Quantum Boltzmann Machines,Anna Levit; Daniel Crawford;Navid Ghadermarzy;Jaspreet S. Oberoi;Ehsan Zahedinejad;Pooya Ronagh,anna.levit@1qbit.com;daniel.crawford@1qbit.com;navid.ghadermarzy@1qbit.com;jaspreet.oberoi@1qbit.com;ehsan.zahedinejad@1qbit.com;pooya.ronagh@1qbit.com,4;6;4,3;4;3,Reject,0,4,0,yes,10/19/17,1qbit;1qbit;1qbit;1qbit;1qbit;1qbit,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,,10/19/17,0,0,0,0,0,0,128;5062;110;140;191;146,15;268;12;24;27;20,5;34;5;5;6;5,8;406;8;4;4;7,-1;-1
797,ICLR,2018,Pixel Deconvolutional Networks,Hongyang Gao;Hao Yuan;Zhengyang Wang;Shuiwang Ji,hongyang.gao@wsu.edu;hao.yuan@wsu.edu;zwang6@eecs.wsu.edu;sji@eecs.wsu.edu,5;5;6,5;4;4,Reject,0,3,0,yes,10/19/17,SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY,468;468;468;468,352;352;352;352,5;2,5/18/17,24,7,9,1,61,2,273;1165;502;8397,21;162;29;136,8;17;9;34,36;68;30;723,-1;-1
798,ICLR,2018,Building effective deep neural networks one feature at a time,Martin Mundt;Tobias Weis;Kishore Konda;Visvanathan Ramesh,mundt@fias.uni-frankfurt.de;weis@ccc.cs.uni-frankfurt.de;kishore.konda@insofe.edu.in;ramesh@fias.uni-frankfurt.de,4;8;5,5;4;4,Reject,2,5,0,yes,10/19/17,Goethe University;Goethe University;;Goethe University,210;210;-1;210,293;293;-1;293,,5/18/17,1,1,0,0,21,0,213;10;1039;9627,21;8;24;106,5;2;11;34,7;1;56;890,-1;-1
799,ICLR,2018,Adversary A3C for Robust Reinforcement Learning,Zhaoyuan Gu;Zhenzhong Jia;Howie Choset,guzhaoyuan14@gmail.com;zhenzhong.jia@gmail.com;choset@cs.cmu.edu,4;4;4,4;4;4,Reject,0,0,0,yes,10/20/17,Tsinghua University;;Carnegie Mellon University,10;-1;1,30;-1;24,4,10/20/17,4,2,0,0,0,0,4;223;12347,4;25;446,1;8;51,0;9;704,-1;-1
800,ICLR,2018,Deep Function Machines: Generalized Neural Networks for Topological Layer Expression,William H. Guss,wguss@cs.cmu.edu,7;3;4,1;4;3,Reject,0,3,0,yes,10/20/17,Carnegie Mellon University,1,24,2;8,12/14/16,6,2,0,0,21,0,70,8,4,4,-1
801,ICLR,2018,Enhance Word Representation for Out-of-Vocabulary on Ubuntu Dialogue Corpus,JIANXIONG DONG;Jim Huang,jdongca2003@gmail.com;ccjimhuang@gmail.com,6;3;5,3;5;4,Reject,0,15,0,yes,10/20/17,;,-1;-1,-1;-1,,10/20/17,18,2,7,1,0,5,30;482,7;31,2;11,6;37,-1;-1
802,ICLR,2018,On Optimality Conditions for Auto-Encoder Signal Recovery,Devansh Arpit;Yingbo Zhou;Hung Q. Ngo;Nils Napp;Venu Govindaraju,devansharpit@gmail.com;zybzmhhj@gmail.com;hungngo@buffalo.edu;nnapp@buffalo.edu;venu@cubs.buffalo.edu,4;5;5,3;4;4,Reject,0,3,0,yes,10/20/17,"University of Montreal;;State University of New York, Buffalo;State University of New York, Buffalo;State University of New York, Buffalo",124;-1;85;85;85,108;-1;270;270;270,,5/23/16,0,0,0,0,0,0,880;1523;1760;473;7859,42;48;97;36;480,12;16;24;13;44,114;213;140;34;523,-1;-1
803,ICLR,2018,Multi-label Learning for Large Text Corpora using Latent Variable Model with Provable Gurantees,Sayantan Dasgupta,sayandg@umich.edu,4;3;4,5;4;5,Reject,0,0,0,yes,10/21/17,University of Michigan,8,21,,10/21/17,0,0,0,0,0,0,48,18,4,2,-1
804,ICLR,2018,Egocentric Spatial Memory Network,Mengmi Zhang;Keng Teck Ma;Joo Hwee Lim;Shih-Cheng Yen;Qi Zhao;Jiashi Feng,a0091624@u.nus.edu;makt@i2r.a-star.edu.sg;joohwee@i2r.a-star.edu.sg;shihcheng@nus.edu.sg;qzhao@cs.umn.edu;elefjia@nus.edu.sg,5;3;4,4;4;4,Reject,0,0,0,yes,10/21/17,"National University of Singapore;A*STAR;A*STAR;National University of Singapore;University of Minnesota, Minneapolis;National University of Singapore",16;-1;-1;16;55;16,22;-1;-1;22;56;22,,10/21/17,2,2,0,0,3,0,80;373;2359;770;7231;9219,20;26;244;96;599;328,4;6;23;17;37;51,8;38;140;26;516;1200,-1;-1
805,ICLR,2018,Entropy-SGD optimizes the prior of a PAC-Bayes bound: Data-dependent PAC-Bayes priors via differential privacy,Gintare Karolina Dziugaite;Daniel M. Roy,gkd22@cam.ac.uk;droy@utstat.toronto.edu,6;6;6,3;3;3,Reject,0,13,0,yes,10/21/17,University of Cambridge;University of Toronto,71;17,2;22,1;8,10/21/17,33,12,9,0,16,4,859;3064,19;96,9;23,103;332,-1;-1
806,ICLR,2018,Decoupling the Layers in Residual Networks,Ricky Fok;Aijun An;Zana Rashidi;Xiaogang Wang,ricky.fok3@gmail.com;aan@cse.yorku.ca;rashidi.zana@gmail.com;stevenw@mathstat.yorku.ca,6;7;6,3;3;3,Accept (Poster),0,22,0,yes,10/21/17,York University;York University;York University;York University,153;153;153;153,350;350;350;350,,10/21/17,0,0,0,0,0,0,98;2686;7;297,18;180;5;44,6;25;1;10,0;216;0;11,-1;-1
807,ICLR,2018,ENRICHMENT OF FEATURES FOR CLASSIFICATION USING AN OPTIMIZED LINEAR/NON-LINEAR COMBINATION OF INPUT FEATURES,Mehran Taghipour-Gorjikolaie;Seyyed Mohammad Razavi;Javad Sadri,mehran.tg.88@gmail.com;razavism@gmail.com;j_sadri@encs.concordia.ca,1;3;2,5;4;3,Reject,0,0,0,yes,10/21/17,";;Concordia University, Montreal",-1;-1;291,-1;-1;560,,10/21/17,0,0,0,0,0,0,53;97;82,15;42;12,3;5;3,3;3;8,-1;-1
808,ICLR,2018,Thinking like a machine — generating visual rationales through latent space optimization,Jarrel Seah;Jennifer Tang;Andy Kitchen;Jonathan Seah,jarrelscy@gmail.com,4;8;7,3;2;4,Reject,0,10,0,yes,10/21/17,Alfred Health,-1,-1,5;8,10/21/17,0,0,0,0,0,0,28;247;54;5,10;11;7;7,4;7;5;1,2;8;4;0,-1;-1
809,ICLR,2018,AANN: Absolute Artificial Neural Network,Animesh Karnewar,animeshsk3@gmail.com,2;3;6,3;5;4,Reject,0,2,0,yes,10/21/17,Google,-1,-1,,10/21/17,0,0,0,0,0,0,29,8,2,3,-1
810,ICLR,2018,Dependent Bidirectional RNN with Extended-long Short-term Memory,Yuanhang Su;Yuzhong Huang;C.-C. Jay Kuo,yuanhans@usc.edu;yuzhongh@usc.edu;cckuo@sipi.usc.edu,3;4;4,4;4;4,Reject,0,10,1,yes,10/22/17,University of Southern California;University of Southern California;University of Southern California,31;31;31,66;66;66,,10/22/17,0,0,0,0,0,0,31;75;20172,8;12;1422,4;4;61,1;4;1552,-1;-1
811,ICLR,2018,Make SVM great again with Siamese kernel for  few-shot learning,Bence Tilk,bence.tilk@gmail.com,5;3;4,4;4;5,Reject,0,3,0,yes,10/22/17,BUDAPEST UNIVERSITY OF TECHNOLOGY AND ECONOMICS,468,807,6;8,10/22/17,1,1,0,0,0,1,1,3,1,1,-1
812,ICLR,2018,Some Considerations on Learning to Explore via Meta-Reinforcement Learning,Bradly Stadie;Ge Yang;Rein Houthooft;Xi Chen;Yan Duan;Yuhuai Wu;Pieter Abbeel;Ilya Sutskever,bstadie@berkeley.edu;yangge1987@gmail.com;rein.hh@gmail.com;adslcx@gmail.com;dementrock@gmail.com;ywu@cs.toronto.edu;pabbeel@gmail.com;ilyasu@openai.com,7;6;4,4;5;4,Invite to Workshop Track,0,8,0,yes,10/22/17,"University of California Berkeley;University of Chicago;;covariant.ai;University of California Berkeley;Department of Computer Science, University of Toronto;University of California-Berkeley;OpenAI",5;46;-1;-1;5;17;5;-1,18;9;-1;-1;18;22;18;-1,,10/22/17,45,27,16,3,6,8,770;757;3605;13266;5446;1163;36452;130182,16;85;22;444;48;28;433;90,7;13;11;40;19;13;94;53,49;53;418;1537;639;181;4390;16864,-1;-1
813,ICLR,2018,Learning non-linear transform with discriminative and minimum information loss priors,Dimche Kostadinov;Slava Voloshynovskiy,dimche.kostadinov@unige.ch;svolos@unige.ch,5;4;5,2;2;1,Reject,0,6,0,yes,10/22/17,"University of Geneva, Switzerland;University of Geneva, Switzerland",468;468,130;130,,10/22/17,4,3,0,0,0,0,101;108,38;41,5;6,3;6,-1;-1
814,ICLR,2018,Continuous Convolutional Neural Networks for Image Classification,Vitor Guizilini;Fabio Ramos,vitor.guizilini@sydney.edu.au;fabio.ramos@sydney.edu.au,5;6;4,3;2;4,Reject,1,6,0,yes,10/22/17,University of Sydney;University of Sydney,90;90,61;61,9,10/22/17,0,0,0,0,0,0,254;20,52;14,10;3,20;2,-1;-1
815,ICLR,2018,Learning Representations and Generative Models for 3D Point Clouds,Panos Achlioptas;Olga Diamanti;Ioannis Mitliagkas;Leonidas Guibas,optas@cs.stanford.edu;diamanti@stanford.edu;ioannis@iro.umontreal.ca;guibas@cs.stanford.edu,6;8;5,5;5;4,Invite to Workshop Track,0,10,0,yes,10/23/17,Stanford University;Stanford University;University of Montreal;Stanford University,4;4;124;4,3;3;108;3,5;8,7/8/17,242,121,101,13,187,53,325;616;1164;45185,13;30;43;699,4;10;18;98,70;85;189;5539,-1;-1
816,ICLR,2018,Siamese Survival Analysis with Competing Risks,Anton Nemchenko;Kartik Ahuja;Mihaela Van Der Schaar,santon834@g.ucla.edu;ahujak@ucla.edu;mihaela@ee.ucla.edu,4;4;4,4;4;5,Reject,0,5,0,yes,10/23/17,"University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles",20;20;20,15;15;15,,10/23/17,1,0,1,0,8,0,14;59;8607,3;32;643,1;5;42,1;3;537,-1;-1
817,ICLR,2018,Training and Inference with Integers in Deep Neural Networks,Shuang Wu;Guoqi Li;Feng Chen;Luping Shi,wus15@mails.tsinghua.edu.cn;liguoqi@mail.tsinghua.edu.cn;chenfeng@mail.tsinghua.edu.cn;lpshi@mail.tsinghua.edu.cn,7;7;8,4;3;4,Accept (Oral),0,3,5,yes,10/23/17,Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University,10;10;10;10,30;30;30;30,,10/23/17,145,70,53,6,12,13,1701;1108;12328;1310,230;161;620;128,18;16;51;17,138;64;1059;73,-1;-1
818,ICLR,2018,Associative Conversation Model: Generating Visual Information from Textual Information,Yoichi Ishibashi;Hisashi Miyamori,g1445539@cc.kyoto-su.ac.jp;miya@cc.kyoto-su.ac.jp,4;3;3,5;4;5,Reject,0,1,0,yes,10/23/17,Meiji University;Meiji University,468;468,334;334,3,10/23/17,0,0,0,0,0,0,2;1505,6;84,1;22,0;97,-1;-1
819,ICLR,2018,The Principle of Logit Separation,Gil Keren;Sivan Sabato;Björn Schuller,cruvadom@gmail.com;sivan.sabato@gmail.com;bjoern.schuller@imperial.ac.uk,6;3;4,3;4;4,Reject,0,2,0,yes,10/23/17,University of Passau;Ben-Gurion University of the Negev;Imperial College London,291;181;74,243;627;8,,10/23/17,3,0,3,0,0,0,172;771;23842,21;59;845,7;13;73,4;70;1642,-1;-1
820,ICLR,2018,Improving image generative models with human interactions,Andrew Kyle Lampinen;David So;Douglas Eck;Fred Bertsch,lampinen@stanford.edu;davidso@google.com;deck@google.com;fredbertsch@google.com,4;5;4,5;3;4,Reject,0,3,0,yes,10/23/17,Stanford University;Google;Google;Google,4;-1;-1;-1,3;-1;-1;-1,5;1,9/29/17,2,0,0,0,2,0,87;11;2540;82,23;2;84;9,5;1;27;4,6;0;272;6,-1;-1
821,ICLR,2018,Large Batch Training of Convolutional Networks with Layer-wise Adaptive Rate Scaling,Boris Ginsburg;Igor Gitman;Yang You,bginsburg@nvidia.com;igitman@andrew.cmu.edu;youyang@cs.berkeley.edu,5;4;5,3;4;5,Reject,0,3,0,yes,10/24/17,NVIDIA;Carnegie Mellon University;University of California Berkeley,-1;1;5,-1;24;18,,8/13/17,115,35,37,3,0,19,980;333;784,30;10;29,14;6;12,123;55;90,-1;-1
822,ICLR,2018,ResBinNet: Residual Binary Neural Network,Mohammad Ghasemzadeh;Mohammad Samragh;Farinaz Koushanfar,mghasemzadeh@ucsd.edu;msamragh@ucsd.edu;farinaz@ucsd.edu,4;4;4,4;4;4,Reject,2,0,0,yes,10/24/17,"University of California, San Diego;University of California, San Diego;University of California, San Diego",11;11;11,31;31;31,9,10/24/17,35,15,19,2,16,9,70;196;9667,8;20;265,4;7;46,10;18;776,-1;-1
823,ICLR,2018,Exponentially vanishing sub-optimal local minima in multilayer neural networks,Daniel Soudry;Elad Hoffer,daniel.soudry@gmail.com;elad.hoffer@gmail.com,5;7;6,3;2;3,Invite to Workshop Track,0,5,0,yes,10/24/17,Technion;Technion,24;24,327;327,1,2/19/17,66,38,2,2,0,2,4820;1445,76;27,26;11,622;167,-1;-1
824,ICLR,2018,A New Method of Region Embedding for Text Classification,chao qiao;bo huang;guocheng niu;daren li;daxiang dong;wei he;dianhai yu;hua wu,chao.qiao@outlook.com;bohuang0321@gmail.com;niuguocheng@baidu.com;lidaren@baidu.com;dongdaxiang@baidu.com;hewei06@baidu.com;yudianhai@baidu.com;wu_hua@baidu.com,6;6;6,4;5;3,Accept (Poster),4,18,0,yes,10/24/17,;Baidu;Baidu;Baidu;Baidu;Baidu;Baidu;Baidu,-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1,,10/24/17,26,7,12,1,0,7,55;32;29;27;608;3960;601;1998,15;6;5;10;12;296;18;79,2;2;2;1;9;29;8;21,10;7;7;7;87;243;87;237,-1;-1
825,ICLR,2018,Predicting Auction Price of Vehicle License Plate with Deep Recurrent Neural Network,Vinci Chow,vincichow@cuhk.edu.hk,6;4;4,5;4;4,Reject,0,3,0,yes,10/24/17,The Chinese University of Hong Kong,57,40,3,1/30/17,2,1,1,1,14,0,17,9,3,0,-1
826,ICLR,2018,Machine Learning by Two-Dimensional Hierarchical Tensor Networks: A Quantum Information Theoretic Perspective on Deep Architectures,Ding Liu;Shi-Ju Ran;Peter Wittek;Cheng Peng;Raul Blázquez García;Gang Su;Maciej Lewenstein,dingliu_thu@126.com;shi-ju.ran@icfo.eu;peter.wittek@icfo.eu;pengcheng12@mails.ucas.ac.cn;raulbzga@gmail.com;gsu@ucas.ac.cn;maciej.lewenstein@icfo.eu,6;4;3,3;3;2,Reject,0,3,0,yes,10/24/17,ICFO-Institut de Ciencies Fotoniques;ICFO-Institut de Ciencies Fotoniques;ICFO-Institut de Ciencies Fotoniques;Chinese Academy of Sciences;;Chinese Academy of Sciences;ICFO-Institut de Ciencies Fotoniques,-1;-1;-1;62;-1;62;-1,-1;-1;-1;1103;-1;1103;-1,,10/24/17,23,10,6,1,0,3,5106;252;1229;742;42;3392;5118,343;55;117;114;3;469;413,28;10;16;16;2;27;36,525;5;41;37;3;124;171,-1;-1
827,ICLR,2018,Learning to play slot cars and Atari 2600 games in just minutes,Lionel Cordesses;Omar Bentahar;Julien Page,lionel.cordesses@renault.com;omar.bentahar@renault.com;ju.page@hotmail.com,3;2;3,2;5;1,Reject,0,4,0,yes,10/24/17,"Blaise Pascal University, France;Renault;",468;-1;-1,565;-1;-1,,10/24/17,0,0,0,0,0,0,335;37;1,36;6;11,8;2;1,23;6;0,-1;-1
828,ICLR,2018,Learning how to explain neural networks: PatternNet and PatternAttribution,Pieter-Jan Kindermans;Kristof T. Schütt;Maximilian Alber;Klaus-Robert Müller;Dumitru Erhan;Been Kim;Sven Dähne,pikinder@google.com;kristof.schuett@tu-berlin.de;maximilian.aber@tu-berlin.de;klaus-robert.mueller@tu-berlin.de;dumitru@google.com;beenkim@google.com;sven.daehne@tu-berlin.de,8;8;6,3;4;4,Accept (Poster),3,5,0,yes,10/24/17,Google;TU Berlin;TU Berlin;TU Berlin;Google;Google;TU Berlin,-1;104;104;104;-1;-1;104,-1;92;92;92;-1;-1;92,8,5/16/17,145,61,65,0,108,14,1921;1298;450;37980;41657;3068;1961,47;19;16;522;60;51;54,18;10;6;82;31;21;21,182;74;36;3860;5966;323;122,-1;-1
829,ICLR,2018,Memory Augmented Control Networks,Arbaaz Khan;Clark Zhang;Nikolay Atanasov;Konstantinos Karydis;Vijay Kumar;Daniel D. Lee,arbaazk@seas.upenn.edu;clarkz@seas.upenn.edu;natanasov@ucsd.edu;konstantinos.karydis@ucr.edu;vijay.kumar@seas.upenn.edu;ddlee@seas.upenn.edu,4;6;9,5;2;4,Accept (Poster),0,6,0,yes,10/24/17,"University of Pennsylvania;University of Pennsylvania;University of California, San Diego;University of California, Riverside;University of Pennsylvania;University of Pennsylvania",19;19;11;62;19;19,10;10;31;197;10;10,,9/17/17,45,25,11,1,24,2,146;409;1030;217;27687;9252,21;35;71;29;925;183,6;11;19;7;83;31,6;20;75;7;1423;1465,-1;-1
830,ICLR,2018,Trace norm regularization and faster inference for embedded speech recognition RNNs,Markus Kliegl;Siddharth Goyal;Kexin Zhao;Kavya Srinet;Mohammad Shoeybi,mkliegl@gmail.com;goyalsiddharth@baidu.com;zhaokexin01@baidu.com;srinetkavya@baidu.com;shoeybim@gmail.com,4;5;5,3;5;3,Reject,0,5,0,yes,10/24/17,Baidu;Baidu;Baidu;Baidu;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,,10/24/17,5,0,3,0,13,1,156;372;686;13;592,12;17;71;6;34,5;9;14;2;10,20;26;49;1;51,-1;-1
831,ICLR,2018,Exploring the Hidden Dimension in Accelerating Convolutional Neural Networks,Zhihao Jia;Sina Lin;Charles R. Qi;Alex Aiken,zhihao@cs.stanford.edu;silin@microsoft.com;rqi@stanford.edu;aiken@cs.stanford.edu,4;5;7,5;4;4,Reject,0,6,0,yes,10/25/17,Stanford University;Microsoft;Stanford University;Stanford University,4;-1;4;4,3;-1;3;3,,10/25/17,6,1,0,0,0,0,710;38;5557;1549,69;7;11;74,14;3;9;16,46;5;1418;117,-1;-1
832,ICLR,2018,Boosting Dilated Convolutional Networks with Mixed Tensor Decompositions,Nadav Cohen;Ronen Tamari;Amnon Shashua,cohennadav@ias.edu;ronent@cs.huji.ac.il;shashua@cs.huji.ac.il,7;9;8,4;4;3,Accept (Oral),0,3,0,yes,10/25/17,"Institue for Advanced Study, Princeton;Hebrew University of Jerusalem;Hebrew University of Jerusalem",-1;62;62,-1;205;205,1,3/20/17,16,12,3,1,18,1,1024;77;8209,63;13;173,15;5;48,118;6;735,-1;-1
833,ICLR,2018,Model Distillation with Knowledge Transfer from Face Classification to Alignment and Verification,Chong Wang;Xipeng Lan;Yangang Zhang,chongwang.nlpr@gmail.com;xipeng.lan@gmail.com;caveman1984@gmail.com,3;5;3,4;5;4,Reject,0,0,0,yes,10/25/17,;;,-1;-1;-1,-1;-1;-1,2,9/9/17,12,7,2,0,3,2,18049;31;-1,1045;2;-1,54;2;-1,1634;4;0,-1;-1
834,ICLR,2018,Distribution Regression Network,Connie Kou;Hwee Kuan Lee;Teck Khim Ng,koukl@comp.nus.edu.sg;leehk@bii.a-star.edu.sg;ngtk@comp.nus.edu.sg,5;7;7,4;2;4,Reject,0,5,0,yes,10/25/17,National University of Singapore;A*STAR;National University of Singapore,16;-1;16,22;-1;22,,10/25/17,3,0,1,0,0,1,10;669;94,6;109;21,2;16;5,1;44;4,-1;-1
835,ICLR,2018,Graph Classification with 2D Convolutional Neural Networks,Antoine J.-P. Tixier;Giannis Nikolentzos;Polykarpos Meladianos;Michalis Vazirgiannis,antoine.tixier-1@colorado.edu;giannisnik@hotmail.com;p.meladianos@gmail.com;mvazirg@lix.polytechnique.fr,3;4;7,5;3;3,Reject,0,0,0,yes,10/25/17,"University of Colorado, Boulder;;;Ecole Polytechnique, France",42;-1;-1;468,100;-1;-1;115,10,7/29/17,18,6,6,1,0,1,90;329;324;8754,8;30;17;322,5;10;10;40,5;18;20;658,-1;-1
836,ICLR,2018,Unsupervised Deep Structure Learning by Recursive Dependency Analysis,Raanan Y. Yehezkel Rohekar;Guy Koren;Shami Nisimov;Gal Novik,raanan.y.yehezkel.rohekar@intel.com;guy.koren@intel.com;shami.nisimov@intel.com;gal.novik@intel.com,4;5;5,4;2;3,Reject,0,3,0,yes,10/25/17,Intel;Intel;Intel;Intel,-1;-1;-1;-1,-1;-1;-1;-1,5;1;10,10/25/17,0,0,0,0,0,0,14;1344;14;19,7;129;7;7,2;20;2;3,0;68;0;1,-1;-1
837,ICLR,2018,Dynamic Integration of Background Knowledge in Neural NLU Systems,Dirk Weissenborn;Tomas Kocisky;Chris Dyer,dirk.weissenborn@dfki.de;tkocisky@google.com;cdyer@google.com,5;5;6,3;4;4,Reject,0,4,0,yes,10/25/17,German Research Center for AI;Google;Google,-1;-1;-1,-1;-1;-1,3,6/8/17,33,20,11,2,97,4,601;33;21075,28;1;230,10;1;59,61;4;3147,-1;-1
838,ICLR,2018,Post-training for Deep Learning,Thomas Moreau;Julien Audiffren,thomas.moreau@cmla.ens-cachan.fr;julien.audiffren@cmla.ens-cachan.fr,5;3;4,4;5;4,Reject,1,3,0,yes,10/25/17,ENS Paris-Saclay;ENS Paris-Saclay,468;468,603;603,,10/25/17,0,0,0,0,0,0,144;107,40;34,7;6,11;14,-1;-1
839,ICLR,2018,Image Segmentation by Iterative Inference from Conditional Score Estimation,Adriana Romero;Michal Drozdzal;Akram Erraqabi;Simon Jégou;Yoshua Bengio,adriana.romsor@gmail.com;michal.drozdzal@gmail.com;akram.er-raqabi@umontreal.ca;simon.jegou@gmail.com;yoshua.umontreal@gmail.com,5;4;4,5;4;4,Reject,0,0,0,yes,10/25/17,Facebook;University of Montreal;University of Montreal;;University of Montreal,-1;124;124;-1;124,-1;108;108;-1;108,2,5/21/17,7,4,3,0,4,0,3796;1629;64;668;201105,52;38;10;5;807,13;13;5;4;147,627;156;6;104;23941,-1;-1
840,ICLR,2018,Towards Safe Deep Learning: Unsupervised Defense Against Generic Adversarial Attacks,Bita Darvish Rouhani;Mohammad Samragh;Tara Javidi;Farinaz Koushanfar,bita@ucsd.edu;msamragh@ucsd.edu;tjavidi@ucsd.edu;farinaz@ucsd.edu,5;7;3,3;3;5,Reject,0,5,0,yes,10/25/17,"University of California, San Diego;University of California, San Diego;University of California, San Diego;University of California, San Diego",11;11;11;11,31;31;31;31,4;1,10/25/17,7,1,4,0,0,0,376;196;2482;9667,47;20;207;265,10;7;24;46,28;18;207;776,-1;-1
841,ICLR,2018,GeoSeq2Seq: Information Geometric Sequence-to-Sequence Networks,Alessandro Bay;Biswa Sengupta,alessandro.bay@cortexica.com;biswasengupta@yahoo.com,5;4;5,2;4;4,Invite to Workshop Track,0,3,0,yes,10/25/17,;Imperial College London,-1;74,-1;8,2;10,10/25/17,1,0,0,0,18,0,37;717,17;42,3;13,2;49,-1;-1
842,ICLR,2018,Hybed: Hyperbolic Neural Graph Embedding,Benjamin Paul Chamberlain;James R Clough;Marc Peter Deisenroth,benjamin.chamberlain@gmail.com;james.clough@kcl.ac.uk;m.deisenroth@imperial.ac.uk,4;7;5;4,3;2;3;3,Reject,0,16,0,yes,10/25/17,Imperial College London;King's College London;Imperial College London,74;181;74,8;36;8,3;10,5/29/17,74,42,21,1,43,8,159;265;5070,21;40;110,6;7;31,13;13;506,-1;-1
843,ICLR,2018,Image Quality Assessment Techniques Improve Training and Evaluation of Energy-Based Generative Adversarial Networks,Michael O. Vertolli;Jim Davies,michaelvertolli@gmail.com;jim@jimdavies.org,5;5;6,3;3;3,Reject,0,3,0,yes,10/25/17,Carleton University;,153;-1,545;-1,5;4,10/25/17,0,0,0,0,0,0,15;2095,11;234,3;21,0;161,-1;-1
844,ICLR,2018,Regularizing and Optimizing LSTM Language Models,Stephen Merity;Nitish Shirish Keskar;Richard Socher,smerity@smerity.com;keskar.nitish@u.northwestern.edu;richard@socher.org,7;7;7,4;4;5,Accept (Poster),0,3,0,yes,10/25/17,Smerity;Northwestern University;SalesForce.com,-1;42;-1,-1;20;-1,3,8/7/17,494,172,278,18,214,124,1835;2129;52263,18;28;180,8;13;49,361;328;8807,-1;-1
845,ICLR,2018,Dynamic Evaluation of Neural Sequence Models,Ben Krause;Emmanuel Kahembwe;Iain Murray;Steve Renals,ben.krause@ed.ac.uk;e.kahembwe@ed.ac.uk;i.murray@ed.ac.uk;s.renals@ed.ac.uk,7;7;3,4;4;3,Reject,0,4,0,yes,10/25/17,University of Edinburgh;University of Edinburgh;University of Edinburgh;University of Edinburgh,33;33;33;33,27;27;27;27,,9/21/17,60,21,39,2,0,14,263;104;749;1507,14;6;24;93,6;5;9;24,36;17;142;123,-1;-1
846,ICLR,2018,Taking Apart Autoencoders: How do They Encode Geometric Shapes ?,Alasdair Newson;Andres Almansa;Yann Gousseau;Said Ladjal,alasdairnewson@gmail.com;andres.almansa@parisdescartes.fr;yann.gousseau@telecom-paristech.fr;said.ladjal@telecom-paristech.fr,4;4;4,5;4;3,Reject,0,4,0,yes,10/25/17,Télécom ParisTech;University Paris Descartes;Télécom ParisTech;Télécom ParisTech,468;468;468;468,1103;1103;1103;1103,5,10/25/17,0,0,0,0,0,0,215;4;2203;314,21;11;122;29,6;1;28;7,42;1;196;29,-1;-1
847,ICLR,2018,Diffusing Policies : Towards Wasserstein Policy Gradient Flows,Pierre H. Richemond;Brendan Maginnis,phr17@imperial.ac.uk;b.maginnis@imperial.ac.uk,4;5;4,3;3;4,Reject,0,3,0,yes,10/25/17,Imperial College London;Imperial College London,74;74,8;8,,10/25/17,0,0,0,0,0,0,13;32,11;7,2;3,2;3,-1;-1
848,ICLR,2018,"To Prune, or Not to Prune: Exploring the Efficacy of Pruning for Model Compression",Michael H. Zhu;Suyog Gupta,mhzhu@cs.stanford.edu;suyoggupta@google.com,5;5;5,4;4;5,Invite to Workshop Track,1,2,0,yes,10/25/17,Stanford University;Google,4;-1,3;-1,,10/5/17,184,83,74,5,52,28,536;1835,29;40,6;14,105;141,-1;-1
849,ICLR,2018,Training Deep AutoEncoders for Recommender Systems,Oleksii Kuchaiev;Boris Ginsburg,kuchaev@gmail.com;boris.ginsburg@gmail.com,4;3;6,5;4;4,Reject,1,3,0,yes,10/25/17,NVIDIA;NVIDIA,-1;-1,-1;-1,,8/5/17,31,14,13,0,32,4,1493;980,26;30,15;14,213;123,-1;-1
850,ICLR,2018,Neural Networks with Block Diagonal Inner Product Layers,Amy Nesky;Quentin Stout,anesky@umich.edu;qstout@umich.edu,5;6;4,4;4;3,Reject,2,4,0,yes,10/26/17,University of Michigan;University of Michigan,8;8,21;21,,10/26/17,1,1,0,0,0,0,6;3233,6;225,1;30,0;205,-1;-1
851,ICLR,2018,Balanced and Deterministic Weight-sharing Helps Network Performance,Oscar Chang;Hod Lipson,oscar.chang@columbia.edu;hod.lipson@columbia.edu,4;4;4,4;4;4,Reject,0,0,0,yes,10/26/17,Columbia University;Columbia University,15;15,14;14,,10/26/17,0,0,0,0,0,0,66;16049,20;338,4;56,6;1028,-1;-1
852,ICLR,2018,Learning Discrete Weights Using the Local Reparameterization Trick,Oran Shayer;Dan Levi;Ethan Fetaya,oran.sh@gmail.com;dan.levi@gm.com;ethanf@cs.toronto.edu,6;7;6,4;3;3,Accept (Poster),0,1,0,yes,10/26/17,"Technion;General Motors;Department of Computer Science, University of Toronto",24;-1;17,327;-1;22,2,10/21/17,35,17,17,1,40,5,43;1258;657,4;30;25,2;12;12,6;114;69,-1;-1
853,ICLR,2018,End-to-End Abnormality Detection in Medical Imaging,Dufan Wu;Kyungsang Kim;Bin Dong;Quanzheng Li,dwu6@mgh.harvard.edu;kkim24@mgh.harvard.edu;dongbin@math.pku.edu.cn;li.quanzheng@mgh.harvard.edu,4;5;6,4;4;3,Reject,0,0,0,yes,10/26/17,Harvard University;Harvard University;Peking University;Harvard University,37;37;24;37,6;6;27;6,2,10/26/17,6,1,1,0,0,0,168;333;1763;1671,21;29;138;136,5;9;20;24,10;11;120;91,-1;-1
854,ICLR,2018,Understanding Deep Learning Generalization by Maximum Entropy,Guanhua Zheng;Jitao Sang;Changsheng Xu,zhenggh@mail.ustc.edu.cn;jtsang@bjtu.edu.cn;csxu@nlpr.ia.ac.cn,2;3;6,3;3;2,Reject,0,0,0,yes,10/26/17,"University of Science and Technology of China;Beijing jiaotong univercity;Institute of automation, Chinese academy of science, Chinese Academy of Sciences",115;468;62,132;854;1103,1;8,10/26/17,6,3,1,0,30,0,6;1191;8479,4;84;392,1;22;52,0;49;489,-1;-1
855,ICLR,2018,Compact Encoding of Words for Efficient Character-level Convolutional Neural Networks Text Classification,Wemerson Marinho;Luis Marti;Nayat Sanchez-pi,wemerson_marinho@id.uff.br;lmarti@ic.uff.br;nayat@ime.uerj.br,4;3;2,5;5;5,Reject,0,12,0,yes,10/26/17,;;Universidade do Estado do Rio de Janeiro,-1;-1;-1,-1;-1;-1,,10/26/17,3,0,0,0,0,0,3;421;250,2;65;60,1;11;8,0;23;9,-1;-1
856,ICLR,2018,Rotational Unit of Memory ,Rumen Dangovski;Li Jing;Marin Soljacic,rumenrd@mit.edu;ljing@mit.edu;soljacic@mit.edu,4;6;5,4;3;4,Invite to Workshop Track,0,11,0,yes,10/26/17,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2,5;5;5,3,10/26/17,3,2,1,0,0,1,15;392;11480,13;28;286,3;7;46,2;34;444,-1;-1
857,ICLR,2018,Style Memory: Making a Classifier Network Generative,Rey Wiyatno;Jeff Orchard,rrwiyatn@uwaterloo.ca;jorchard@uwaterloo.ca,3;3;4,5;5;3,Reject,0,4,0,yes,10/26/17,University of Waterloo;University of Waterloo,26;26,207;207,5,10/26/17,3,1,0,0,6,0,23;861,4;72,3;17,0;55,-1;-1
858,ICLR,2018,CrescendoNet: A Simple Deep Convolutional Neural Network with Ensemble Behavior,Xiang Zhang;Nishant Vishwamitra;Hongxin Hu;Feng Luo,xzhang7@clemson.edu;nvishwa@clemson.edu;luofeng@clemson.edu;hongxih@clemson.edu,4;5;4,5;4;5,Reject,4,11,0,yes,10/26/17,Clemson University;Clemson University;Clemson University;Clemson University,210;210;210;210,1103;1103;1103;1103,,10/26/17,2,0,0,0,23,0,540;94;3424;6838,101;10;150;613,14;5;30;39,29;7;251;414,-1;-1
859,ICLR,2018,Anytime Neural Network: a Versatile Trade-off Between Computation and Accuracy,Hanzhang Hu;Debadeepta Dey;Martial Hebert;J. Andrew Bagnell,hanzhang@cs.cmu.edu;dedey@microsoft.com;hebert@ri.cmu.edu;dbagnell@ri.cmu.edu,7;5;5,2;3;4,Reject,0,0,0,yes,10/26/17,Carnegie Mellon University;Microsoft;Carnegie Mellon University;Carnegie Mellon University,1;-1;1;1,24;-1;24;24,,10/26/17,5,2,2,0,0,1,177;1227;29465;14006,23;48;521;196,7;16;90;53,13;113;2598;1407,-1;-1
860,ICLR,2018,Adversarial Policy Gradient for Alternating Markov Games,Chao Gao;Martin Mueller;Ryan Hayward,cgao3@ualberta.ca;mmueller@ualberta.ca;hayward@ualberta.ca,5;5;5,2;4;4,Invite to Workshop Track,0,6,0,yes,10/26/17,University of Alberta;University of Alberta;University of Alberta,99;99;99,119;119;119,4,10/26/17,5,3,3,1,0,3,1846;2392;945,212;164;91,23;24;15,131;212;83,-1;-1
861,ICLR,2018,THE EFFECTIVENESS OF A TWO-LAYER NEURAL NETWORK FOR RECOMMENDATIONS,Oleg Rybakov;Vijai Mohan;Avishkar Misra;Scott LeGrand;Rejith Joseph;Kiuk Chung;Siddharth Singh;Qian You;Eric Nalisnick;Leo Dirac;Runfei Luo,rybakovo@amazon.com;vijaim@amazon.com;avishkar@gmail.com;slegrand@a9.com;rgeorgej@amazon.com;kiuk@amazon.com;singsidd@amazon.com;qian.you@snapchat.com;enalisni@uci.edu;leodirac@amazon.com;rluo@pstat.ucsb.edu,6;6;7,3;4;3,Invite to Workshop Track,0,4,0,yes,10/26/17,"Amazon;Amazon;;A9;Amazon;Amazon;Amazon;Snap Inc.;University of California, Irvine;Amazon;UC Santa Barbara",-1;-1;-1;-1;-1;-1;-1;-1;36;-1;37,-1;-1;-1;-1;-1;-1;-1;-1;99;-1;53,,10/26/17,0,0,0,0,0,0,2;58;105;0;4;0;235;142;634;3;0,6;13;16;1;3;2;69;38;33;6;2,1;3;5;0;1;0;8;7;12;1;0,0;3;3;0;1;0;21;5;94;0;0,-1;-1
862,ICLR,2018,Normalized Direction-preserving Adam,Zijun Zhang;Lin Ma;Zongpeng Li;Chuan Wu,zijun.zhang@ucalgary.ca;linmawhu@gmail.com;zongpeng@ucalgary.ca;cwu@cs.hku.hk,5;5;4,4;5;4,Reject,2,9,0,yes,10/26/17,University of Calgary;;University of Calgary;The University of Hong Kong,181;-1;181;90,210;-1;210;40,9;8,9/13/17,18,5,10,0,12,1,1251;7087;5087;4985,69;912;241;279,20;36;36;38,54;429;323;322,-1;-1
863,ICLR,2018,Rethinking generalization requires revisiting old ideas: statistical mechanics approaches and complex learning behavior,Charles H. Martin;Michael W. Mahoney,charles@calculationconsulting.com;mmahoney@stat.berkeley.edu,3;6;7,3;5;4,Reject,0,9,0,yes,10/26/17,Calculationconsulting;University of California Berkeley,-1;5,-1;18,8,10/26/17,28,15,5,3,36,2,330;11793,89;230,7;45,18;1114,-1;-1
864,ICLR,2018,Learning Non-Metric Visual Similarity for Image Retrieval,Noa Garcia;George Vogiatzis,garciadn@aston.ac.uk;g.vogiatzis@aston.ac.uk,7;4;3,5;4;5,Reject,0,4,0,yes,10/26/17,Aston University;Aston University,69;69,358;358,,9/5/17,12,4,6,1,4,2,49;2034,12;58,4;21,3;222,-1;-1
865,ICLR,2018,Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks,Víctor Campos;Brendan Jou;Xavier Giró-i-Nieto;Jordi Torres;Shih-Fu Chang,victor.campos@bsc.es;bjou@google.com;xavier.giro@upc.edu;jordi.torres@bsc.es;shih.fu.chang@columbia.edu,6;6;6,4;4;4,Accept (Poster),1,6,0,yes,10/26/17,Barcelona Supercomputing Center;Google;Universitat Politècnica de Catalunya;Barcelona Supercomputing Center;Columbia University,-1;-1;468;-1;15,-1;-1;1103;-1;14,10,8/22/17,91,33,34,2,37,11,312;700;995;3399;31149,35;28;86;188;584,6;13;18;30;86,27;59;88;230;3166,-1;-1
866,ICLR,2018,Iterative Deep Compression : Compressing Deep Networks for Classification and Semantic Segmentation,Sugandha Doda;Vitor Fortes Rey;Dr. Nadereh Hatami;Prof. Dr. Paul Lukowicz,sugandhadoda672@gmail.com;vitor.fortes@dfki.uni-kl.de;nadereh.hatamimazinani@de.bosch.com,6;4;5,4;4;4,Reject,0,6,0,yes,10/26/17,Bosch;TU Kaiserslautern;Bosch,-1;139;-1,-1;452;-1,3;2,10/26/17,0,0,0,0,0,0,10;65;54;9186,2;9;19;344,1;4;5;49,0;8;2;473,-1;-1
867,ICLR,2018,Do Deep Reinforcement Learning Algorithms really Learn to Navigate?,Shurjo Banerjee;Vikas Dhiman;Brent Griffin;Jason J. Corso,shurjo@umich.edu;dhiman@umich.edu;griffb@umich.edu;jjcorso@umich.edu,7;3;3,4;5;4,Reject,0,4,0,yes,10/26/17,University of Michigan;University of Michigan;University of Michigan;University of Michigan,8;8;8;8,21;21;21;21,,10/26/17,2,1,0,0,0,0,20;83;137;5717,8;20;21;231,2;5;5;32,0;0;9;581,-1;-1
868,ICLR,2018,Adversarial Learning for Semi-Supervised Semantic Segmentation,Wei-Chih Hung;Yi-Hsuan Tsai;Yan-Ting Liou;Yen-Yu Lin;Ming-Hsuan Yang,whung8@ucmerced.edu;ytsai@nec-labs.com;lyt@csie.ntu.edu.tw;yylin@citi.sinica.edu.tw;mhyang@ucmerced.edu,5;5;5,5;4;4,Reject,5,6,0,yes,10/26/17,University of California at Merced;NEC-Labs;National Taiwan University;Academia Sinica;University of California at Merced,468;-1;85;-1;468,1103;-1;197;-1;1103,4;2,10/26/17,85,44,46,5,15,24,615;1140;84;1371;38851,39;60;3;95;379,11;13;1;21;87,125;220;24;145;6268,-1;-1
869,ICLR,2018,Towards Interpretable Chit-chat: Open Domain Dialogue Generation with Dialogue Acts,Wei Wu;Can Xu;Yu Wu;Zhoujun Li,wuwei@microsoft.com;can.xu@microsoft.com;wumark@126.com;lizj@buaa.edu.cn,7;4;7,3;5;4,Reject,0,9,0,yes,10/26/17,Microsoft;Microsoft;Beihang University;Beihang University,-1;-1;124;124,-1;-1;658;658,,10/26/17,2,1,0,0,0,0,603;369;23;2804,114;73;7;350,11;8;2;27,37;45;4;241,-1;-1
870,ICLR,2018,PDE-Net: Learning PDEs from Data,Zichao Long;Yiping Lu;Xianzhong Ma;Bin Dong,zlong@pku.edu.cn;luyiping9712@pku.edu.cn;xianzhongma@pku.edu.cn;dongbin@math.pku.edu.cn,7;8;5,4;4;4,Invite to Workshop Track,0,4,0,yes,10/26/17,Peking University;Peking University;Peking University;Peking University,24;24;24;24,27;27;27;27,2,10/26/17,163,84,53,1,35,17,219;460;164;1763,6;11;5;138,2;7;2;20,20;47;17;120,-1;-1
871,ICLR,2018,Unleashing the Potential of CNNs for Interpretable Few-Shot Learning,Boyang Deng;Qing Liu;Siyuan Qiao;Alan Yuille,billydeng@buaa.edu.cn;qingliu@jhu.edu;siyuan.qiao@jhu.edu;alan.yuille@jhu.edu,5;7;4,4;4;5,Reject,0,8,0,yes,10/26/17,Beihang University;Johns Hopkins University;Johns Hopkins University;Johns Hopkins University,124;71;71;71,658;13;13;13,6;2,10/26/17,3,1,0,0,0,0,122;14979;555;32598,10;1136;24;494,5;52;10;80,14;963;77;3737,-1;-1
872,ICLR,2018,Clipping Free Attacks Against Neural Networks,Boussad ADDAD,boussad.addad@thalesgroup.com;boussad83@yahoo.fr,3;4;5,3;3;2,Reject,0,4,0,yes,10/26/17,Thalesgroup;Thales,-1;-1,-1;-1,3;4;2,10/26/17,1,0,0,0,5,0,95,17,6,3,-1
873,ICLR,2018,Noisy Networks For Exploration,Meire Fortunato;Mohammad Gheshlaghi Azar;Bilal Piot;Jacob Menick;Matteo Hessel;Ian Osband;Alex Graves;Volodymyr Mnih;Remi Munos;Demis Hassabis;Olivier Pietquin;Charles Blundell;Shane Legg,meirefortunato@google.com;mazar@google.com;piot@google.com;jmenick@google.com;mtthss@google.com;iosband@google.com;gravesa@google.com;vmnih@google.com;munos@google.com;dhcontact@google.com;pietquin@google.com;cblundell@google.com;legg@google.com,5;7;6,3;4;4,Accept (Poster),1,10,0,yes,10/26/17,Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,,6/30/17,288,145,104,2,503,39,1237;1569;2038;519;2375;2404;43897;284;1685;23607;377;5964;11008,8;32;70;8;29;28;87;2;30;55;39;51;58,4;15;18;6;13;19;50;2;13;31;6;22;21,162;281;265;64;382;346;6010;38;255;2541;46;1050;1788,-1;-1
874,ICLR,2018,The Set Autoencoder: Unsupervised Representation Learning for Sets,Malte Probst,malte.probst@honda-ri.de,4;5;5,5;4;4,Reject,0,2,0,yes,10/26/17,Honda Research Institute,-1,-1,3,10/26/17,2,2,1,0,0,1,68,19,5,13,-1
875,ICLR,2018,Learning to Count Objects in Natural Images for Visual Question Answering,Yan Zhang;Jonathon Hare;Adam Prügel-Bennett,yz5n12@ecs.soton.ac.uk;jsh2@ecs.soton.ac.uk;apb@ecs.soton.ac.uk,6;6;4,3;3;4,Accept (Poster),0,12,1,yes,10/26/17,University of Southampton;University of Southampton;University of Southampton,181;181;181,126;126;126,,10/26/17,80,37,25,2,13,16,3470;1603;2097,699;117;137,28;22;25,200;149;135,-1;-1
876,ICLR,2018,Sensor Transformation Attention Networks,Stefan Braun;Daniel Neil;Enea Ceolini;Jithendar Anumula;Shih-Chii Liu,brauns@ethz.ch;daniel.l.neil@gmail.com;enea.ceolini@ini.uzh.ch;anumula@ini.uzh.ch;shih@ini.ethz.ch,7;4;3,4;4;4,Reject,0,1,0,yes,10/26/17,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;University of Zurich;University of Zurich;Swiss Federal Institute of Technology,9;9;139;139;9,10;10;136;136;10,,8/3/17,1,0,0,0,2,0,192;1354;379;67;4770,41;36;24;14;181,8;17;7;4;29,8;187;41;6;407,-1;-1
877,ICLR,2018,CAYLEYNETS: SPECTRAL GRAPH CNNS WITH COMPLEX RATIONAL FILTERS,Ron Levie;Federico Monti;Xavier Bresson;Michael M. Bronstein,ronlevie@gmail.com;federico.monti@usi.ch;xavier.bresson@gmail.com;michael.bronstein@gmail.com,6;4;8,3;3;3,Reject,0,15,0,yes,10/26/17,;Università della Svizzera Italiana;National Taiwan University;Tel Aviv University,-1;139;85;37,-1;1103;197;217,10,5/22/17,150,60,57,1,13,10,184;1192;6309;11805,12;24;105;295,4;10;34;53,11;142;817;1153,-1;-1
878,ICLR,2018,Convolutional Normalizing Flows,Guoqing Zheng;Yiming Yang;Jaime Carbonell,gzheng@cs.cmu.edu;yiming@cs.cmu.edu;jgc@cs.cmu.edu,3;5;3,5;4;4,Reject,2,4,0,yes,10/26/17,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1,24;24;24,11,10/26/17,4,0,1,0,2,0,630;21147;15678,41;272;506,11;49;54,62;2566;1625,-1;-1
879,ICLR,2018,Baseline-corrected space-by-time non-negative matrix factorization for decoding single trial population spike trains,Arezoo Alizadeh;Marion Mutter;Thomas Münch;Arno Onken;Stefano Panzeri,arezoo.alizadehkhajehiem@iit.it;marion.mutter@gmx.de;thomas.muench@cin.uni-tuebingen.de;aonken@inf.ed.ac.uk;stefano.panzeri@iit.it,4;6;6,4;3;3,Reject,0,3,0,yes,10/26/17,Istituto Italiano di Tecnologia;;University of Tuebingen;University of Edinburgh;Istituto Italiano di Tecnologia,468;-1;153;33;468,1103;-1;94;27;1103,5,10/26/17,0,0,0,0,0,0,6;87;49;188;9791,6;8;17;34;316,2;5;3;7;52,1;6;2;9;650,-1;-1
880,ICLR,2018,DNA-GAN: Learning Disentangled Representations from Multi-Attribute Images,Taihong Xiao;Jiapeng Hong;Jinwen Ma,xiaotaihong@pku.edu.cn;jphong@pku.edu.cn;jwma@math.pku.edu.cn,4;5;6,4;4;5,Invite to Workshop Track,1,9,0,yes,10/26/17,Peking University;Peking University;Peking University,24;24;24,27;27;27,4,10/26/17,32,18,7,0,9,1,139;195;1990,8;5;192,3;5;21,15;17;124,-1;-1
881,ICLR,2018,Avoiding Catastrophic States with Intrinsic Fear,Zachary C. Lipton;Kamyar Azizzadenesheli;Abhishek Kumar;Lihong Li;Jianfeng Gao;Li Deng,zlipton@cmu.edu;kazizzad@uci.edu;abkumar@ucsd.edu;lihongli.cs@gmail.com;jfgao@microsoft.com;l.deng@ieee.org,5;5;7,4;5;3,Reject,1,12,0,yes,10/26/17,"Carnegie Mellon University;University of California, Irvine;University of California, San Diego;Google;Microsoft;",1;36;11;-1;-1;-1,24;99;31;-1;-1;-1,,10/26/17,0,0,0,0,0,0,4731;670;33;59;18496;262,97;38;15;27;353;39,28;10;2;4;61;8,431;96;6;3;2639;19,-1;-1
882,ICLR,2018,DDRprog: A CLEVR Differentiable Dynamic Reasoning Programmer,Joseph Suarez;Justin Johnson;Fei-Fei Li,joseph15@stanford.edu;jcjohns@cs.stanford.edu;feifeili@cs.stanford.edu,5;5;6,2;2;2,Reject,0,6,0,yes,10/26/17,Stanford University;Stanford University;Stanford University,4;4;4,3;3;3,,10/26/17,22,6,5,3,5,1,59;5900;4710,18;14;157,4;9;35,2;773;602,-1;-1
883,ICLR,2018,Espresso: Efficient Forward Propagation for Binary Deep Neural Networks,Fabrizio Pedersoli;George Tzanetakis;Andrea Tagliasacchi,fpeder@uvic.ca;gtzan@uvic.ca;ataiya@uvic.ca,6;7;7,3;4;1,Accept (Poster),0,0,0,yes,10/26/17,University of Victoria;University of Victoria;University of Victoria,181;181;181,346;346;346,,5/19/17,14,9,5,1,24,2,87;6006;2116,10;212;56,5;32;19,6;679;153,-1;-1
884,ICLR,2018,LatentPoison -- Adversarial Attacks On The Latent Space,Antonia Creswell;Biswa Sengupta;Anil A. Bharath,ac2211@ic.ac.uk;b.sengupta@imperial.ac.uk;a.bharath@imperial.ac.uk,5;3;4,3;4;4,Reject,0,3,0,yes,10/26/17,Imperial College London;Imperial College London;Imperial College London,74;74;74,8;8;8,5;4,10/26/17,6,5,0,0,7,1,460;717;2780,15;42;128,7;13;20,34;49;167,-1;-1
885,ICLR,2018,PACT: Parameterized Clipping Activation for Quantized Neural Networks,Jungwook Choi;Zhuo Wang;Swagath Venkataramani;Pierce I-Jen Chuang;Vijayalakshmi Srinivasan;Kailash Gopalakrishnan,choij@us.ibm.com,5;5;5,4;5;4,Reject,0,8,0,yes,10/26/17,International Business Machines,-1,-1,,10/26/17,133,56,77,8,0,36,990;2288;1767;199;3470;2308,99;177;75;14;70;45,18;23;20;6;23;14,89;112;161;49;541;214,-1;-1
886,ICLR,2018,Understanding Grounded Language Learning Agents,Felix Hill;Karl Moritz Hermann;Phil Blunsom;Stephen Clark,felixhill@google.com;kmh@google.com;pblunsom@google.com;clarkstephen@google.com,4;5;7,5;3;4,Reject,0,6,0,yes,10/26/17,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,,10/26/17,17,10,5,0,20,0,3488;5070;11393;6402,53;41;144;189,22;21;47;43,670;684;1326;642,-1;-1
887,ICLR,2018,APPLICATION OF DEEP CONVOLUTIONAL NEURAL NETWORK TO PREVENT ATM FRAUD BY FACIAL DISGUISE IDENTIFICATION,Suraj Nandkishor Kothawade;Sumit Baburao Tamgale,kothawadesuraj@sggs.ac.in;tamgalesumit@sggs.ac.in,1;2;3,5;4;5,Reject,0,0,0,yes,10/26/17,;,-1;-1,-1;-1,,10/26/17,1,0,0,0,0,0,19;1,12;1,3;1,0;0,-1;-1
888,ICLR,2018,Discrete Sequential Prediction of Continuous Actions for Deep RL,Luke Metz;Julian Ibarz;Navdeep Jaitly;James Davidson,lmetz@google.com;julianibarz@google.com;njaitly@google.com;jcdavidson@google.com,7;5;4,5;1;5,Reject,0,4,0,yes,10/26/17,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,,5/14/17,37,15,16,1,65,4,7178;2860;17542;4888,25;22;94;205,10;13;38;28,1200;349;1586;812,-1;-1
889,ICLR,2018,An Ensemble of Retrieval-Based and Generation-Based Human-Computer Conversation Systems.,Yiping Song;Rui Yan;Cheng-Te Li;Jian-Yun Nie;Ming Zhang;Dongyan Zhao,songyiping@pku.edu.cn;ruiyan@pku.edu.cn;chengte@mail.ncku.edu.tw;nie@iro.umontreal.ca;mzhang_cs@pku.edu.cn;zhaody@pku.edu.cn,5;5;6,3;3;3,Reject,0,0,1,yes,10/26/17,Peking University;Peking University;Peking University;University of Montreal;Peking University;Peking University,24;24;24;124;24;24,27;27;27;108;27;27,3;5,10/26/17,36,13,9,1,0,3,781;6288;781;6527;6273;2577,45;640;105;265;340;163,14;37;16;38;32;27,75;424;37;568;1002;305,-1;-1
890,ICLR,2018,Multi-Advisor Reinforcement Learning,Romain Laroche;Mehdi Fatemi;Joshua Romoff;Harm van Seijen,romain.laroche@gmail.com;mehdi.fatemi@microsoft.com;joshua.romoff@mail.mcgill.ca;havansei@microsoft.com,4;4;4,4;4;5,Reject,0,1,0,yes,10/26/17,Microsoft;Microsoft;McGill University;Microsoft,-1;-1;81;-1,-1;-1;42;-1,,4/3/17,10,5,1,0,44,0,594;275;199;397,80;17;13;32,15;6;7;10,28;19;17;35,-1;-1
891,ICLR,2018,Progressive Reinforcement Learning with Distillation for Multi-Skilled Motion Control,Glen Berseth;Cheng Xie;Paul Cernek;Michiel Van de Panne,gberseth@gmail.com;cheng.k.xie@gmail.com;pcernek@cs.ubc.ca;van@cs.ubc.ca,7;7;5,4;4;3,Accept (Poster),0,6,0,yes,10/26/17,University of British Columbia;University of British Columbia;University of British Columbia;University of British Columbia,34;34;34;34,34;34;34;34,6,10/26/17,29,14,4,1,5,1,626;29;31;5122,44;3;3;137,11;1;2;42,21;1;1;319,-1;-1
892,ICLR,2018,UNSUPERVISED METRIC LEARNING VIA NONLINEAR FEATURE SPACE TRANSFORMATIONS,Pin Zhang;Bibo Shi;JundongLiu,pz335412@ohio.edu;bibo.shi@duke.edu;liuj1@ohio.edu,6;4;4,4;5;4,Reject,0,0,0,yes,10/26/17,Ohio University;Duke University;Ohio University,364;46;364,627;17;627,,10/26/17,0,0,0,0,0,0,157;180;0,33;35;1,7;8;0,10;7;0,-1;-1
893,ICLR,2018,Learning Deep Generative Models With Discrete Latent Variables,Hengyuan Hu;Ruslan Salakhutdinov,hengyuah@andrew.cmu.edu;rsalakhu@cs.cmu.edu,4;5;4,4;4;4,Reject,2,3,0,yes,10/26/17,Carnegie Mellon University;Carnegie Mellon University,1;1,24;24,5;10,10/26/17,2,0,0,0,0,0,349;66785,9;253,5;81,34;7750,-1;-1
894,ICLR,2018,Novelty Detection with GAN,Mark Kliger;Shachar Fleishman,mark.kliger@gmail.com;shacharfl@gmail.com,4;5;6,4;4;4,Reject,0,3,0,yes,10/26/17,;,-1;-1,-1;-1,5;4,10/26/17,29,12,9,0,3,2,551;2908,31;30,13;14,51;236,-1;-1
895,ICLR,2018,A Self-Organizing Memory Network,Callie Federer;Joel Zylberberg,callie.federer@ucdenver.edu;joel.zylberberg@ucdenver.edu,3;4;4,2;4;4,Reject,0,0,0,yes,10/26/17,"University of Colorado, Denver;University of Colorado, Denver",468;468,286;286,,10/26/17,6,3,0,0,0,0,9;776,5;57,3;14,0;59,-1;-1
896,ICLR,2018,The Cramer Distance as a Solution to Biased Wasserstein Gradients,Marc G. Bellemare;Ivo Danihelka;Will Dabney;Shakir Mohamed;Balaji Lakshminarayanan;Stephan Hoyer;Remi Munos,bellemare@google.com;danihelka@google.com;shakir@google.com;balajiln@google.com;shoyer@google.com;munos@google.com,5;4;7,3;5;2,Reject,1,4,0,yes,10/26/17,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,5;4,5/30/17,140,69,61,4,325,20,3950;4761;1733;6867;2888;593;9303,57;19;25;51;43;53;190,24;16;13;27;22;12;53,624;544;331;995;376;42;1313,-1;-1
897,ICLR,2018,DEEP DENSITY NETWORKS AND UNCERTAINTY IN RECOMMENDER SYSTEMS,Yoel Zeldes;Stavros Theodorakis;Efrat Solodnik;Aviv Rotman;Gil Chamiel;Dan Friedman,yoel.z@taboola.com;sth@deeplab.ai;efrat.s@taboola.com;aviv.r@taboola.com;gil.c@taboola.com;dan.f@taboola.com,4;3;4,5;4;3,Reject,0,0,0,yes,10/26/17,Taboola;DeepLab;Taboola;Taboola;Taboola;Taboola,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,,10/26/17,4,2,1,0,11,0,15;230;4;49;12;140,3;55;1;2;7;22,2;8;1;2;2;6,0;9;0;0;1;15,-1;-1
898,ICLR,2018,PrivyNet: A Flexible Framework for Privacy-Preserving Deep Neural Network Training,Meng Li;Liangzhen Lai;Naveen Suda;Vikas Chandra;David Z. Pan,meng_li@utexas.edu;liangzhen.lai@arm.com;naveen.suda@arm.com;vikas.chandra@arm.com;dpan@ece.utexas.edu,6;5;3,5;3;3,Reject,1,0,0,yes,10/26/17,"University of Texas, Austin;arm;arm;arm;University of Texas, Austin",21;-1;-1;-1;21,49;-1;-1;-1;49,,9/18/17,25,18,6,0,17,6,4094;715;492;1184;6262,479;35;18;69;486,25;13;8;20;41,324;93;72;128;550,-1;-1
899,ICLR,2018,Simple Fast Convolutional Feature Learning,David Macêdo;Cleber Zanchettin;Teresa Ludermir,dlm@cin.ufpe.br;cz@cin.ufpe.br;tbl@cin.ufpe.br,3;3;2,4;4;4,Reject,0,5,0,yes,10/26/17,Federal University of Pernambuco;Federal University of Pernambuco;Federal University of Pernambuco,468;468;468,958;958;958,,10/26/17,1,0,1,0,0,0,27;445;2166,22;85;323,3;9;21,2;16;98,-1;-1
900,ICLR,2018,Enhancing Batch Normalized Convolutional Networks using Displaced Rectifier Linear Units: A Systematic Comparative Study,David Macêdo;Cleber Zanchettin;Adriano L. I. Oliveira;Teresa Ludermir,dlm@cin.ufpe.br;cz@cin.ufpe.br;alio@cin.ufpe.br;tbl@cin.ufpe.br,3;4;5,5;4;5,Reject,0,4,0,yes,10/26/17,Federal University of Pernambuco;Federal University of Pernambuco;Federal University of Pernambuco;Federal University of Pernambuco,468;468;468;468,958;958;958;958,2,10/26/17,5,2,0,0,0,0,25;445;1129;2166,21;85;98;323,3;9;15;21,2;16;90;98,-1;-1
901,ICLR,2018,ANALYSIS ON GRADIENT PROPAGATION IN BATCH NORMALIZED RESIDUAL NETWORKS,Abhishek Panigrahi;Yueru Chen;C.-C. Jay Kuo,abhishekpanigrahi@iitkgp.ac.in;yueruche@usc.edu;cckuo@sipi.usc.edu,4;1;4,4;5;5,Reject,0,0,0,yes,10/26/17,Indian Institute of Technology Kharagpur;University of Southern California;University of Southern California,291;31;31,506;66;66,,10/26/17,0,0,0,0,0,0,20;26;20243,9;5;1422,2;1;61,2;3;1558,-1;-1
902,ICLR,2018,Deterministic Policy Imitation Gradient Algorithm,Fumihiro Sasaki;Atsuo Kawaguchi,fumihiro.fs.sasaki@nts.ricoh.co.jp;atsuo.kawaguchi@nts.ricoh.co.jp,6;5;5,4;4;3,Reject,0,3,0,yes,10/26/17,"Ricoh Company, Ltd.;Ricoh Company, Ltd.",-1;-1,-1;-1,5;4,10/26/17,1,1,0,0,0,0,36;448,25;27,3;6,3;44,-1;-1
903,ICLR,2018,Data augmentation instead of explicit regularization,Alex Hernández-García;Peter König,alexhg15@gmail.com;pkoenig@uos.de,5;5;5,4;4;4,Reject,0,5,0,yes,9/25/19,University of Osnabrück;University of Osnabrück,364;364,1103;1103,8,2/15/18,31,8,9,2,6,0,86;749,15;135,5;17,1;40,m;m
904,ICLR,2018,Neural Clustering By Predicting And Copying Noise,Sam Coope;Andrej Zukov-Gregoric;Yoram Bachrach,sam@digitalgenius.com;andrej@digitalgenius.com;yoram@digitalgenius.com,5;5;5,4;3;4,Reject,3,4,0,yes,10/26/17,DigitalGenius Ltd.;DigitalGenius Ltd.;DigitalGenius Ltd.,-1;-1;-1,-1;-1;-1,,10/26/17,0,0,0,0,0,0,63;2;2699,9;2;136,5;1;29,5;0;195,-1;-1
905,ICLR,2018,Generative Adversarial Networks using Adaptive Convolution,Nhat M. Nguyen;Nilanjan Ray,nmnguyen@ualberta.ca;nray1@ualberta.ca,4;4;4,5;4;5,Reject,0,0,0,yes,10/26/17,University of Alberta;University of Alberta,99;99,119;119,5,10/26/17,1,0,1,0,21,0,6;1702,8;161,1;21,0;78,-1;-1
906,ICLR,2018,Deep Rewiring: Training very sparse deep networks,Guillaume Bellec;David Kappel;Wolfgang Maass;Robert Legenstein,bellec@igi.tugraz.at;kappel@igi.tugraz.at;maass@igi.tugraz.at;legenstein@igi.tugraz.at,8;5;6,4;5;4,Accept (Poster),0,0,1,yes,10/26/17,Graz University of Technology;Graz University of Technology;Graz University of Technology;Graz University of Technology,104;104;104;104,443;443;443;443,,10/26/17,48,20,12,1,28,2,262;290;11812;3082,17;21;324;97,7;9;54;26,18;8;1028;229,-1;-1
907,ICLR,2018,Network Iterative Learning for Dynamic Deep Neural Networks via Morphism,Tao Wei;Changhu Wang;Chang Wen Chen,taowei@buffalo.edu;wangchanghu@toutiao.com;chencw@buffalo.edu,7;5;5,4;2;3,Reject,0,0,0,yes,10/26/17,"State University of New York, Buffalo;Toutiao AI Lab;State University of New York, Buffalo",85;-1;85,270;-1;270,,10/26/17,0,0,0,0,0,0,4724;2381;6215,436;97;576,30;24;35,284;204;506,-1;-1
908,ICLR,2018,Generalized Graph Embedding Models,Qiao Liu;Xiaohui Yang;Rui Wan;Shouzhong Tu;Zufeng Wu,qliu@uestc.edu.cn;yangxhui@uestc.std.edu.cn;rwan@uestc.std.edu.cn;tusz11@mails.tsinghua.edu.cn;wuzufeng@uestc.edu.cn,6;4;3,4;4;4,Reject,0,0,0,yes,10/26/17,University of Electronic Science and Technology of China;Tsinghua University;Tsinghua University;Tsinghua University;University of Electronic Science and Technology of China,468;10;10;10;468,843;30;30;30;843,10,10/26/17,0,0,0,0,0,0,86;38;335;7;3,31;34;27;6;2,5;3;7;1;1,4;0;31;1;1,-1;-1
909,ICLR,2018,Discriminative k-shot learning using probabilistic models,Matthias Bauer;Mateo Rojas-Carulla;Jakub Bartłomiej Świątkowski;Bernhard Schölkopf;Richard E. Turner,msb55@cam.ac.uk;mrojascarulla@gmail.com;kuba.swiatkowski@gmail.com;bs@tuebingen.mpg.de;ret26@cam.ac.uk,5;5;5,3;3;3,Reject,0,4,0,yes,10/26/17,"University of Cambridge;University of Cambridge;University of Cambridge;Max Planck Institute for Intelligent Systems, Max-Planck Institute;University of Cambridge",71;71;71;-1;71,2;2;2;-1;2,,6/1/17,39,9,16,1,8,4,300;349;38;75101;2936,61;12;1;860;174,5;6;1;119;30,18;32;4;9901;308,-1;-1
910,ICLR,2018,Link Weight Prediction with Node Embeddings,Yuchen Hou;Lawrence B. Holder,yuchen.hou@wsu.edu;holder@wsu.edu,3;3;4,4;5;3,Reject,0,0,0,yes,10/26/17,SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY,468;468,352;352,3;10,10/26/17,0,0,0,0,0,0,8;3949,7;220,2;28,0;293,-1;-1
911,ICLR,2018,Learning Independent Causal Mechanisms,Giambattista Parascandolo;Mateo Rojas Carulla;Niki Kilbertus;Bernhard Schoelkopf,gparascandolo@tue.mpg.de;mrojascarulla@gmail.com;nkilbertus@tue.mpg.de;bs@tue.mpg.de,6;5;5,4;4;4,Reject,0,6,0,yes,10/26/17,"Max Planck Institute for Intelligent Systems, Max-Planck Institute;University of Cambridge;Max Planck Institute for Intelligent Systems, Max-Planck Institute;Max Planck Institute for Intelligent Systems, Max-Planck Institute",-1;71;-1;-1,-1;2;-1;-1,5,10/26/17,31,20,2,1,9,2,903;349;318;75101,15;12;20;860,12;6;7;119,92;32;22;9901,-1;-1
912,ICLR,2018,DNN Feature Map Compression using Learned Representation over GF(2),Denis A. Gudovskiy;Alec Hodgkinson;Luca Rigazio,denis.gudovskiy@us.panasonic.com;alec.hodgkinson@us.panasonic.com;luca.rigazio@us.panasonic.com,7;5;4,3;4;4,Reject,0,4,0,yes,10/26/17,Us.panasonic;Rensselaer Polytechnic Institute;Us.panasonic,-1;153;-1,-1;304;-1,2,10/26/17,5,2,5,1,9,2,50;15;652,10;5;52,4;3;9,5;3;45,-1;-1
913,ICLR,2018,A Bayesian Perspective on Generalization and Stochastic Gradient Descent,Samuel L. Smith;Quoc V. Le,slsmith@google.com;qvl@google.com,3;7;7,4;4;3,Accept (Poster),2,12,0,yes,10/26/17,Google;Google,-1;-1,-1;-1,11;8,10/17/17,129,64,23,4,187,13,1160;47470,26;192,11;79,108;5968,-1;-1
914,ICLR,2018,Mixed Precision Training,Paulius Micikevicius;Sharan Narang;Jonah Alben;Gregory Diamos;Erich Elsen;David Garcia;Boris Ginsburg;Michael Houston;Oleksii Kuchaiev;Ganesh Venkatesh;Hao Wu,pauliusm@nvidia.com;sharan@baidu.com;alben@nvidia.com;gdiamos@baidu.com;eriche@google.com;dagarcia@nvidia.com;bginsburg@nvidia.com;mhouston@nvidia.com;okuchaiev@nvidia.com;gavenkatesh@nvidia.com;skyw@nvidia.com,8;5;7,4;3;4,Accept (Poster),0,6,0,yes,10/26/17,NVIDIA;Baidu;NVIDIA;Baidu;Google;NVIDIA;NVIDIA;NVIDIA;NVIDIA;NVIDIA;NVIDIA,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,,10/10/17,290,122,156,8,44,27,1077;2586;273;1992;4626;1272;1037;672;1523;1529;34374,32;15;2;42;53;83;30;19;26;20;2642,12;10;2;20;21;16;14;7;15;12;76,108;278;26;196;488;109;131;59;214;117;2187,-1;-1
915,ICLR,2018,Multiscale Hidden Markov Models For Covariance Prediction,João Sedoc;Jordan Rodu;Dean Foster;Lyle Ungar,joao@cis.upenn.edu;jsr6q@virginia.edu;dean@foster.net;ungar@cis.upenn.edu,5;6;6,3;4;4,Reject,0,3,0,yes,10/26/17,University of Pennsylvania;University of Virginia;University of Pennsylvania;University of Pennsylvania,19;62;19;19,10;113;10;10,5,10/26/17,2,2,1,0,0,0,127;143;6166;13381,29;22;197;404,7;6;41;58,13;8;681;962,-1;-1
916,ICLR,2018,Latent Topic Conversational Models,Tsung-Hsien Wen;Minh-Thang Luong,thw28@cam.ac.uk;thangluong@google.com,4;5;6,3;4;4,Reject,0,8,0,yes,10/26/17,University of Cambridge;Google,71;-1,2;-1,,10/26/17,1,0,0,0,0,0,2745;3035,49;33,21;20,418;347,-1;-1
917,ICLR,2018,Recurrent Auto-Encoder Model for Multidimensional Time Series Representation,Timothy Wong;Zhiyuan Luo,timothy.wong@centrica.com;zhiyuan.luo@cs.rhul.ac.uk,2;2;4,4;5;4,Reject,0,4,0,yes,10/26/17,"Royal Holloway, University of London;Royal Holloway, University of London",181;181,195;195,,10/26/17,4,1,1,0,11,0,84;738,31;100,3;14,3;41,-1;-1
918,ICLR,2018,Sparse-Complementary Convolution for Efficient Model Utilization on CNNs,Chun-Fu (Richard) Chen;Jinwook Oh;Quanfu Fan;Marco Pistoia;Gwo Giun (Chris) Lee,chenrich@us.ibm.com;ohj@us.ibm.com;qfan@us.ibm.com;pistoia@us.ibm.com;clee@mail.ncku.edu.tw,6;5;5,5;4;4,Reject,0,5,0,yes,10/26/17,International Business Machines;International Business Machines;International Business Machines;International Business Machines;Peking University,-1;-1;-1;-1;24,-1;-1;-1;-1;27,,10/26/17,3,3,0,0,0,0,301;493;1602;1626;443,31;46;62;100;78,7;12;16;21;10,44;26;173;123;20,-1;-1
919,ICLR,2018,Achieving morphological agreement with Concorde,Daniil Polykovskiy;Dmitry Soloviev,daniil.polykovskiy@gmail.com;d.soloviev@corp.mail.ru,2;5;6,5;4;5,Reject,0,0,0,yes,10/26/17,Lomonosov Moscow State University;,468;-1,193;-1,,10/26/17,0,0,0,0,0,0,187;1216,12;52,5;16,9;49,-1;-1
920,ICLR,2018,Discrete-Valued Neural Networks Using Variational Inference,Wolfgang Roth;Franz Pernkopf,roth@tugraz.at;pernkopf@tugraz.at,6;5;5,1;4;4,Reject,0,3,0,yes,10/27/17,Graz University of Technology;Graz University of Technology,104;104,443;443,11,10/27/17,2,1,1,0,0,0,196;1954,137;196,7;21,14;112,-1;-1
921,ICLR,2018,Improving Deep Learning by Inverse Square Root Linear Units (ISRLUs),Brad Carlile;Guy Delamarter;Paul Kinney;Akiko Marti;Brian Whitney,bradcarlile@yahoo.com;info@aiperf.com,4;5;3,4;4;4,Reject,3,1,0,yes,10/27/17,AI Perf Eng;Aiperf,-1;-1,-1;-1,8,10/27/17,10,4,1,0,5,1,16;31;44;9;241,7;13;6;7;24,2;3;2;1;9,2;3;3;1;30,-1;-1
922,ICLR,2018,Feat2Vec:  Dense Vector Representation for Data with Arbitrary Features,Luis Armona;José P. González-Brenes;Ralph Edezhath,luisarmona@gmail.com;jgonzalez@chegg.com;ralph.angelus@gmail.com,7;2;7,3;2;5,Reject,0,6,0,yes,10/27/17,Stanford University;Chegg;,4;-1;-1,3;-1;-1,,10/27/17,0,0,0,0,0,0,93;307;84,11;31;8,3;10;3,16;26;0,-1;-1
923,ICLR,2018,Jointly Learning Sentence Embeddings and Syntax with Unsupervised Tree-LSTMs,Jean Maillard;Stephen Clark;Dani Yogatama,jean@maillard.it;sc609@cam.ac.uk;dyogatama@google.com,5;6;4,4;4;4,Reject,0,0,0,yes,10/27/17,University of Cambridge;University of Cambridge;Google,71;71;-1,2;2;-1,3,5/25/17,43,27,12,0,6,6,222;6402;3499,13;189;41,7;43;21,25;642;402,-1;-1
924,ICLR,2018,COLD FUSION: TRAINING SEQ2SEQ MODELS TOGETHER WITH LANGUAGE MODELS,Anuroop Sriram;Heewoo Jun;Sanjeev Satheesh;Adam Coates,anuroop.sriram@gmail.com;junheewoo@baidu.com;sanjeevsatheesh@baidu.com;adamcoates@baidu.com,5;6;5,5;5;5,Invite to Workshop Track,0,0,0,yes,10/27/17,;Baidu;Baidu;Baidu,-1;-1;-1;-1,-1;-1;-1;-1,3;8,8/21/17,85,38,37,1,7,8,1796;224;17011;12719,12;10;26;64,8;5;13;31,182;18;2566;1797,-1;-1
925,ICLR,2018,Long Short-Term Memory as a Dynamically Computed Element-wise Weighted Sum,Omer Levy;Kenton Lee;Nicholas FitzGerald;Luke Zettlemoyer,omerlevy@gmail.com;kentonl@cs.washington.edu;nfitz@cs.washington.edu;lsz@cs.washington.edu,6;5;7,4;5;3,Reject,8,5,0,yes,10/27/17,University of Washington;University of Washington;University of Washington;University of Washington,6;6;6;6,25;25;25;25,3,10/27/17,16,5,6,1,79,3,7487;12143;705;14948,58;36;37;177,30;19;10;54,1215;3459;62;2558,-1;-1
926,ICLR,2018,AUTOMATED DESIGN USING NEURAL NETWORKS AND GRADIENT DESCENT,Oliver Hennigh,loliverhennigh101@gmail.com,5;7;4,4;4;5,Invite to Workshop Track,0,0,0,yes,10/27/17,,,,,10/27/17,3,0,1,0,6,0,66,6,4,5,-1
927,ICLR,2018,Tensor Contraction & Regression Networks,Jean Kossaifi;Zack Chase Lipton;Aran Khanna;Tommaso Furlanello;Anima Anandkumar,jean.kossaifi@gmail.com;zlipton@cmu.edu;arankhan@amazon.com;tfurlanello@gmail.com;animakumar@gmail.com,4;6;4,4;4;3,Reject,0,5,0,yes,10/27/17,Imperial College London;Carnegie Mellon University;Amazon;University of Southern California;University of California-Irvine,74;1;-1;31;36,8;24;-1;66;99,,7/26/17,38,22,18,0,30,5,1213;4967;289;1364;5676,31;98;9;96;188,10;29;5;22;39,127;448;30;157;781,-1;-1
928,ICLR,2018,Fixing Weight Decay Regularization in Adam,Ilya Loshchilov;Frank Hutter,ilya.loshchilov@gmail.com;fh@cs.uni-freiburg.de,7;4;8,4;4;3,Reject,0,18,1,yes,10/27/17,Universität Freiburg;Universität Freiburg,115;115,82;82,8,10/27/17,172,31,101,5,0,25,2536;12566,38;233,21;49,354;1521,-1;-1
929,ICLR,2018,Exploring the Space of Black-box Attacks on Deep Neural Networks,Arjun Nitin Bhagoji;Warren He;Bo Li;Dawn Song,abhagoji@princeton.edu;_w@eecs.berkeley.edu;lxbosky@gmail.com;dawnsong@gmail.com,5;6;7,4;3;4,Reject,3,6,0,yes,10/27/17,Princeton University;University of California Berkeley;University of California Berkeley;University of California Berkeley,31;5;5;5,7;18;18;18,4,10/27/17,52,25,12,0,9,6,628;1217;2382;43319,19;22;80;396,11;14;23;100,52;128;268;4323,-1;-1
930,ICLR,2018,Code Synthesis with Priority Queue Training,Daniel A. Abolafia;Quoc V. Le;Mohammad Norouzi,danabo@google.com;qvl@google.com;mnorouzi@google.com,6;5;6,4;3;4,Reject,0,8,2,yes,10/27/17,Google;Google;Google,-1;-1;-1,-1;-1;-1,,10/27/17,30,10,16,0,0,5,263;49576;7792,8;196;125,4;82;30,29;6128;992,-1;-1
931,ICLR,2018,Implicit Causal Models for Genome-wide Association Studies,Dustin Tran;David M. Blei,dustin@cs.columbia.edu;david.blei@columbia.edu,5;6;6,5;5;5,Accept (Poster),0,12,0,yes,10/27/17,Columbia University;Columbia University,15;15,14;14,5;11,10/27/17,23,11,4,0,158,1,1740;52172,50;306,20;75,198;9227,-1;-1
932,ICLR,2018,Learning what to learn in a neural program,Richard Shin;Dawn Song,ricshin@berkeley.edu;dawnsong.travel@gmail.com,5;4;5,4;4;2,Reject,3,3,0,yes,10/27/17,University of California Berkeley;University of California Berkeley,5;5,18;18,8,10/27/17,0,0,0,0,0,0,275;43319,29;396,9;100,27;4323,-1;-1
933,ICLR,2018,Generative Models for Alignment and Data Efficiency in Language,Dustin Tran;Yura Burda;Ilya Sutskever,dustin@cs.columbia.edu;yburda@openai.com;ilyasu@openai.com,5;4;2,3;3;3,Reject,0,0,0,yes,10/27/17,Columbia University;OpenAI;OpenAI,15;-1;-1,14;-1;-1,3,10/27/17,0,0,0,0,0,0,1740;0;130182,50;2;90,20;0;53,198;0;16864,-1;-1
934,ICLR,2018,Maintaining cooperation in complex social dilemmas using deep reinforcement learning,Alexander Peysakhovich;Adam Lerer,alex.peys@gmail.com;alerer@fb.com,4;4;3,4;4;5,Reject,0,15,0,yes,10/27/17,Facebook;Facebook,-1;-1,-1;-1,,7/4/17,59,37,14,3,99,5,1681;8009,46;27,15;15,125;996,-1;-1
935,ICLR,2018,TRAINING GENERATIVE ADVERSARIAL NETWORKS VIA PRIMAL-DUAL SUBGRADIENT METHODS: A LAGRANGIAN PERSPECTIVE ON GAN,Xu Chen;Jiang Wang;Hao Ge,chenxugz@gmail.com;wangjiangb@gmail.com;haoge2013@u.northwestern.edu,7;6;7,4;4;3,Accept (Poster),0,14,0,yes,10/27/17,Northwestern University;;Northwestern University,42;-1;42,20;-1;20,5;4;1;9,10/27/17,7,3,3,0,6,1,7299;3969;186,580;142;71,43;26;8,537;439;10,-1;-1
936,ICLR,2018,Stochastic Training of Graph Convolutional Networks,Jianfei Chen;Jun Zhu,chenjian14@mails.tsinghua.edu.cn;dcszj@mail.tsinghua.edu.cn,3;4;7,4;4;3,Reject,0,9,0,yes,10/27/17,Tsinghua University;Tsinghua University,10;10,30;30,10,10/27/17,86,43,42,1,6,19,120;4508,16;204,4;35,18;524,-1;-1
937,ICLR,2018,Transformation Autoregressive Networks,Junier Oliva;Avinava Dubey;Barnabás Póczos;Eric P. Xing;Jeff Schneider,joliva@cs.cmu.edu;akdubey@cs.cmu.edu;bapoczos@cs.cmu.edu;epxing@cs.cmu.edu;schneide@cs.cmu.edu,5;5;8,3;2;4,Reject,0,6,0,yes,10/27/17,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1;1,24;24;24;24;24,,10/27/17,25,6,8,0,5,4,365;534;5679;23985;682,29;37;244;603;47,10;13;40;75;13,45;58;699;2648;52,-1;-1
938,ICLR,2018,SQLNet: Generating Structured Queries From Natural Language Without Reinforcement Learning,Xiaojun Xu;Chang Liu;Dawn Song,xuxiaojun1005@gmail.com;liuchang@eecs.berkeley.edu;dawnsong@cs.berkeley.edu,4;7;5,5;4;5,Reject,0,10,0,yes,10/27/17,Shanghai Jiao Tong University;University of California Berkeley;University of California Berkeley,57;5;5,188;18;18,3;10,10/27/17,120,58,49,8,10,38,1114;6509;43319,98;697;396,15;38;100,99;302;4323,-1;-1
939,ICLR,2018,Nearest Neighbour Radial Basis Function Solvers for Deep Neural Networks,Benjamin J. Meyer;Ben Harwood;Tom Drummond,benjamin.meyer@monash.edu;ben.harwood@monash.edu;tom.drummond@monash.edu,5;3;4,4;4;4,Reject,0,3,0,yes,10/27/17,Monash University;Monash University;Monash University,124;124;124,80;80;80,,5/27/17,5,2,3,1,9,1,45;162;8781,14;10;165,5;4;36,3;17;864,-1;-1
940,ICLR,2018,Deep Learning Inferences with Hybrid Homomorphic Encryption,Anthony Meehan;Ryan K L Ko;Geoff Holmes,anthonymeehan@anthonymeehan.com;ryan.ko@waikato.ac.nz;geoff@waikato.ac.nz,4;4;4,4;5;5,Reject,0,4,0,yes,10/27/17,Anthonymeehan;The University of Waikato;The University of Waikato,-1;291;291,-1;393;393,,10/27/17,2,0,0,0,0,1,290;76;22471,34;20;129,7;4;37,27;6;2916,-1;-1
941,ICLR,2018,Adversarial reading networks for machine comprehension,Quentin Grail;Julien Perez,julien.perez@naverlabs.com,4;5;5,5;5;4,Reject,0,1,0,yes,10/27/17,Naver Labs Europe,-1,-1,3;4,10/27/17,0,0,0,0,0,0,33;427,6;46,2;13,3;22,-1;-1
942,ICLR,2018,Log-DenseNet: How to Sparsify a DenseNet,Hanzhang Hu;Debadeepta Dey;Allie Del Giorno;Martial Hebert;J. Andrew Bagnell,hanzhang@cs.cmu.edu;dedey@microsoft.com;adelgior@ri.cmu.edu;hebert@ri.cmu.edu;dbagnell@ri.cmu.edu,6;6;5,4;4;4,Reject,0,0,0,yes,10/27/17,Carnegie Mellon University;Microsoft;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;-1;1;1;1,24;-1;24;24;24,2,10/27/17,14,8,4,0,14,3,177;1227;75;29465;14006,23;48;7;521;196,7;16;2;90;53,13;113;16;2598;1407,-1;-1
943,ICLR,2018,Contextual memory bandit for pro-active dialog engagement,julien perez;Tomi Silander,julien.perez@naverlabs.com,2;3;3,5;4;4,Reject,0,0,0,yes,10/27/17,Naver Labs Europe,-1,-1,,10/27/17,1,0,0,0,0,0,427;1569,46;108,13;19,22;100,-1;-1
944,ICLR,2018,Sequence Transfer Learning for Neural Decoding,Venkatesh Elango*;Aashish N Patel*;Kai J Miller;Vikash Gilja,velango@eng.ucsd.edu;anp054@eng.ucsd.edu;kai.miller@stanford.edu;vgilja@eng.ucsd.edu,4;6;3,5;5;4,Reject,0,5,0,yes,10/27/17,"University of California, San Diego;University of California, San Diego;Stanford University;University of California, San Diego",11;11;4;11,31;31;3;31,6,10/27/17,6,0,4,0,6,0,10;22;6274;2374,7;5;133;76,2;3;37;23,0;0;474;152,-1;-1
945,ICLR,2018,Unbiased scalable softmax optimization,Francois Fagan;Garud Iyengar,ff2316@columbia.edu;garud@ieor.columbia.edu,5;5;5,4;3;4,Reject,0,4,0,yes,10/27/17,Columbia University;Columbia University,15;15,14;14,3,10/27/17,7,3,3,0,7,0,50;2411,13;138,4;21,1;331,-1;-1
946,ICLR,2018,BLOCK-DIAGONAL HESSIAN-FREE OPTIMIZATION FOR TRAINING NEURAL NETWORKS,Huishuai Zhang;Caiming Xiong;James Bradbury;Richard Socher,hzhan23@syr.edu;cxiong@salesforce.com;james.bradbury@salesforce.com;richard@socher.org,6;4;6,4;4;4,Reject,0,4,0,yes,10/27/17,Syracuse University;SalesForce.com;SalesForce.com;SalesForce.com,244;-1;-1;-1,275;-1;-1;-1,8,10/27/17,6,2,3,0,12,0,399;6210;1293;52263,42;156;14;180,10;31;7;49,47;1052;229;8807,-1;-1
947,ICLR,2018,Polar Transformer Networks,Carlos Esteves;Christine Allen-Blanchette;Xiaowei Zhou;Kostas Daniilidis,machc@seas.upenn.edu;allec@seas.upenn.edu;xiaowz@seas.upenn.edu;kostas@seas.upenn.edu,7;7;8,4;4;3,Accept (Poster),0,5,2,yes,10/27/17,University of Pennsylvania;University of Pennsylvania;University of Pennsylvania;University of Pennsylvania,19;19;19;19,10;10;10;10,,9/6/17,57,29,18,1,40,4,215;176;2766;10023,13;8;64;342,6;5;22;53,34;27;455;937,-1;-1
948,ICLR,2018,Softmax Q-Distribution Estimation for Structured Prediction: A Theoretical Interpretation for RAML,Xuezhe Ma;Pengcheng Yin;Jingzhou Liu;Graham Neubig;Eduard Hovy,xuezhem@cs.cmu.edu;pcyin@cs.cmu.edu;liujingzhou@cs.cmu.edu;gneubig@cs.cmu.edu;hovy@cs.cmu.edu,5;5;6,2;4;3,Reject,0,4,0,yes,10/27/17,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1;1,24;24;24;24;24,3;11,5/19/17,13,6,5,0,8,1,1923;845;380;5424;23510,37;45;23;441;580,14;12;9;39;76,243;72;42;569;2470,-1;-1
949,ICLR,2018,Learning to Optimize Neural Nets,Ke Li;Jitendra Malik,ke.li@eecs.berkeley.edu;jitendram@google.com,5;6;6,3;4;3,Reject,0,3,0,yes,10/27/17,University of California Berkeley;Google,5;-1,18;-1,,3/1/17,43,23,8,0,106,0,6137;68980,781;429,35;115,290;7755,-1;-1
950,ICLR,2018,Character Level Based Detection of DGA Domain Names,Bin Yu;Jie Pan;Jiaming Hu;Anderson Nascimento;Martine De Cock,biny@infoblox.com;jiep@uw.edu;huj22@uw.edu;andclay@uw.edu;mdecock@uw.edu,7;5;4,4;3;4,Reject,0,4,0,yes,10/27/17,"Infoblox;University of Washington, Seattle;University of Washington, Seattle;University of Washington, Seattle;University of Washington, Seattle",-1;6;6;6;6,-1;25;25;25;25,,10/27/17,34,14,12,5,0,2,5268;3519;63;871;3149,268;355;17;85;239,35;29;4;17;33,380;231;3;66;205,-1;-1
951,ICLR,2018,Data Augmentation by Pairing Samples for Images Classification,Hiroshi Inoue,inouehrs@jp.ibm.com,4;5;6,5;4;4,Reject,1,20,0,yes,10/27/17,International Business Machines,-1,-1,,10/27/17,94,33,43,3,80,8,847,216,14,61,m
952,ICLR,2018,Weighted Transformer Network for Machine Translation,Karim Ahmed;Nitish Shirish Keskar;Richard Socher,karim.mmm@gmail.com;keskar.nitish@u.northwestern.edu;richard@socher.org,9;4;6,4;4;5,Reject,2,5,0,yes,10/27/17,Dartmouth College;Northwestern University;SalesForce.com,153;42;-1,89;20;-1,3,10/27/17,63,21,14,0,35,5,218;2137;52263,12;28;180,7;13;49,30;329;8807,-1;-1
953,ICLR,2018,Principled Hybrids of Generative and Discriminative Domain Adaptation,Han Zhao;Zhenyao Zhu;Junjie Hu;Adam Coates;Geoff Gordon,han.zhao@cs.cmu.edu;zhenyaozhu@baidu.com;junjieh@cmu.edu;adamcoates@baidu.com;ggordon@cs.cmu.edu,6;5;5,3;4;4,Reject,0,5,0,yes,10/27/17,Carnegie Mellon University;Baidu;Carnegie Mellon University;Baidu;Carnegie Mellon University,1;-1;1;-1;1,24;-1;24;-1;24,5;1,5/25/17,2,1,0,0,11,0,1026;2439;494;12719;10869,125;15;21;64;186,18;12;11;31;49,66;238;34;1797;1241,-1;-1
954,ICLR,2018,Integrating Episodic Memory into a Reinforcement Learning Agent Using Reservoir Sampling,Kenny J. Young;Shuo Yang;Richard S. Sutton,kjyoung@ualberta.ca;rsutton@ualberta.ca;s-yan14@mails.tsinghua.edu.cn,4;4;4,4;3;3,Reject,0,4,0,yes,10/27/17,University of Alberta;University of Alberta;Tsinghua University,99;99;10,119;119;30,,10/27/17,3,1,1,0,13,1,3;5338;47227,1;423;288,1;31;66,1;466;6990,-1;-1
955,ICLR,2018,Relational Multi-Instance Learning for Concept Annotation from Medical Time Series,Sanjay Purushotham;Zhengping Che;Bo Jiang;Tanachat Nilanon;Yan Liu,spurusho@usc.edu;zche@usc.edu;boj@usc.edu;nilanon@usc.edu;yanliu.cs@usc.edu,6;3;3,4;3;5,Reject,0,3,0,yes,10/27/17,University of Southern California;University of Southern California;University of Southern California;University of Southern California;University of Southern California,31;31;31;31;31,66;66;66;66;66,3,10/27/17,0,0,0,0,0,0,1084;1071;3971;75;223,39;22;550;6;28,13;11;27;4;4,115;109;111;6;33,-1;-1
956,ICLR,2018,VOCABULARY-INFORMED VISUAL FEATURE AUGMENTATION FOR ONE-SHOT LEARNING,jianqi ma;hangyu lin;yinda zhang;yanwei fu;xiangyang xue,14302010017@fudan.edu.cn;16210240036@fudan.edu.cn;yindaz@cs.princeton.edu;y.fu@qmul.ac.uk,4;6;5,3;4;4,Reject,0,0,0,yes,10/27/17,Fudan University;Fudan University;Princeton University;Queen Mary University London,78;78;31;244,116;116;7;121,,10/27/17,0,0,0,0,0,0,169;61;1693;87;6223,26;10;35;22;281,7;4;15;5;39,2;3;302;2;591,-1;-1
957,ICLR,2018,Understanding Local Minima in Neural Networks by Loss Surface Decomposition,Hanock Kwak;Byoung-Tak Zhang,hnkwak@bi.snu.ac.kr;btzhang@bi.snu.ac.kr,4;5;4,4;4;4,Reject,0,0,0,yes,10/27/17,Seoul National University;Seoul National University,46;46,74;74,1,10/27/17,0,0,0,0,0,0,36;302,5;41,2;6,4;24,-1;-1
958,ICLR,2018,Viterbi-based Pruning for Sparse Matrix with Fixed and High Index Compression Ratio,Dongsoo Lee;Daehyun Ahn;Taesu Kim;Pierce I. Chuang;Jae-Joon Kim,dslee3@gmail.com;daehyun.ahn@postech.ac.kr;taesukim@postech.ac.kr;pchuang@us.ibm.com;jaejoon@postech.ac.kr,6;6;7,4;4;3,Accept (Poster),0,4,0,yes,10/27/17,Samsung;POSTECH;POSTECH;International Business Machines;POSTECH,-1;115;115;-1;115,-1;137;137;-1;137,,10/27/17,11,9,8,0,0,4,331;20;1903;199;1345,33;7;106;14;112,10;3;20;6;17,39;7;173;49;114,-1;-1
959,ICLR,2018,Toward learning better metrics for sequence generation training with policy gradient,Joji Toyama;Yusuke Iwasawa;Kotaro Nakayama;Yutaka Matsuo,toyama@weblab.t.u-tokyo.ac.jp;iwasawa@weblab.t.u-tokyo.ac.jp;nakayama@weblab.t.u-tokyo.ac.jp;matsuo@weblab.t.u-tokyo.ac.jp,7;4;7,1;3;3,Reject,0,7,0,yes,10/27/17,The University of Tokyo;The University of Tokyo;The University of Tokyo;The University of Tokyo,52;52;52;52,45;45;45;45,5;4,10/27/17,2,0,0,0,0,0,21;113;674;3403,10;39;71;492,3;5;13;28,5;10;88;193,-1;-1
960,ICLR,2018,Classifier-to-Generator Attack: Estimation of Training Data Distribution from Classifier,Kosuke Kusano;Jun Sakuma,cocuh@mdl.cs.tsukuba.ac.jp;jun@cs.tsukuba.ac.jp,7;4;4,3;3;3,Reject,0,4,0,yes,10/27/17,University of Tsukuba;University of Tsukuba,468;468,1103;1103,5;4;2,10/27/17,1,1,0,0,0,0,1;1201,2;114,1;16,0;100,-1;-1
961,ICLR,2018,Large scale distributed neural network training through online distillation,Rohan Anil;Gabriel Pereyra;Alexandre Passos;Robert Ormandi;George E. Dahl;Geoffrey E. Hinton,rohananil@google.com;pereyra@google.com;apassos@google.com;ormandi@google.com;gdahl@google.com;geoffhinton@google.com,8;4;6,4;3;3,Accept (Poster),0,6,0,yes,10/27/17,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,3,10/27/17,100,50,40,0,166,13,993;573;21497;311;17664;210562,14;12;28;28;45;415,6;3;9;8;27;127,132;76;1489;31;1332;21462,-1;-1
962,ICLR,2018,Better Generalization by Efficient Trust Region Method,Xuanqing Liu;Jason D. Lee;Cho-Jui Hsieh,xqliu@ucdavis.edu;jasondlee88@gmail.com;chohsieh@ucdavis.edu,6;5;6,5;2;3,Reject,0,3,0,yes,10/27/17,"University of California, Davis;University of Southern California;University of California, Davis",78;31;78,54;66;54,9;8,10/27/17,0,0,0,0,0,0,250;4629;12483,20;118;168,6;36;40,35;604;1722,-1;-1
963,ICLR,2018,Enhancing the Transferability of Adversarial Examples with Noise Reduced Gradient,Lei Wu;Zhanxing Zhu;Cheng Tai;Weinan E,leiwu@pku.edu.cn;zhanxing.zhu@pku.edu.cn;chengt@math.princeton.edu;weinan@math.princeton.edu,4;5;5,4;3;4,Reject,0,0,0,yes,10/27/17,Peking University;Peking University;Princeton University;Princeton University,24;24;31;31,27;27;7;7,4,10/27/17,4,1,0,1,0,1,3534;841;488;1394,512;80;23;33,28;14;9;9,101;105;45;114,-1;-1
964,ICLR,2018,Discovering Order in Unordered Datasets: Generative Markov Networks,Yao-Hung Hubert Tsai;Han Zhao;Nebojsa Jojic;Ruslan Salakhutdinov,yaohungt@cs.cmu.edu;han.zhao@cs.cmu.edu;jojic@microsoft.com;rsalakhu@cs.cmu.edu,4;4;4,4;4;4,Reject,0,3,0,yes,10/27/17,Carnegie Mellon University;Carnegie Mellon University;Microsoft;Carnegie Mellon University,1;1;-1;1,24;24;-1;24,5,10/27/17,1,1,1,0,6,0,529;1026;3474;66785,63;125;139;253,12;18;29;81,71;66;322;7750,-1;-1
965,ICLR,2018,Kronecker Recurrent Units,Cijo Jose;Moustapha Cisse;Francois Fleuret,cijo.jose@idiap.ch;moustaphacisse@fb.com;francois.fleuret@idiap.ch,6;5;7,5;4;3,Invite to Workshop Track,0,1,0,yes,10/27/17,Idiap Research Institute;Facebook;Idiap Research Institute,-1;-1;-1,-1;-1;-1,,5/29/17,22,10,0,0,36,0,232;2930;4901,8;48;136,5;19;29,23;402;519,-1;-1
966,ICLR,2018,Characterizing Sparse Connectivity Patterns in Neural Networks,Sourya Dey;Kuan-Wen Huang;Peter A. Beerel;Keith M. Chugg,souryade@usc.edu;kuanwenh@usc.edu;pabeerel@usc.edu;chugg@usc.edu,4;5;4,3;3;3,Reject,0,5,0,yes,10/27/17,University of Southern California;University of Southern California;University of Southern California;University of Southern California,31;31;31;31,66;66;66;66,1,10/27/17,8,6,0,0,7,0,45;157;1970;1475,9;10;193;125,4;5;22;21,0;7;162;170,-1;-1
967,ICLR,2018,Multi-level Residual Networks from Dynamical Systems View,Bo Chang;Lili Meng;Eldad Haber;Frederick Tung;David Begert,bchang@stat.ubc.ca;lilimeng1103@gmail.com;haber@math.ubc.ca;ftung@sfu.ca;david@xtract.ai,7;7;7,4;3;4,Accept (Poster),0,5,0,yes,10/27/17,University of British Columbia;University of British Columbia;University of British Columbia;Simon Fraser University;Xtract AI,34;34;34;57;-1,34;34;34;253;-1,3;2,10/27/17,68,44,16,2,0,7,421;473;3155;693;158,56;53;164;65;4,8;12;33;14;2,42;34;261;48;12,-1;-1
968,ICLR,2018,Pointing Out SQL Queries From Text,Chenglong Wang;Marc Brockschmidt;Rishabh Singh,clwang@cs.washington.edu;mabrocks@microsoft.com;risin@microsoft.com,4;3;7,4;4;4,Reject,0,3,0,yes,10/27/17,University of Washington;Microsoft;Microsoft,6;-1;-1,25;-1;-1,3,10/27/17,20,10,10,0,0,3,716;2475;2149,68;61;80,12;22;24,45;326;185,-1;-1
969,ICLR,2018,Learning Graph Convolution Filters from Data Manifold,Guokun Lai;Hanxiao Liu;Yiming Yang,guokun@cs.cmu.edu;hanxiaol@cs.cmu.edu;yiming@cs.cmu.edu,4;6;5,5;4;3,Reject,0,4,0,yes,10/27/17,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1,24;24;24,2;10,10/27/17,3,0,2,0,13,0,784;1969;21147,14;35;272,6;12;49,149;513;2566,-1;-1
970,ICLR,2018,Interpreting Deep Classification Models With Bayesian Inference,Hanshu Yan;Jiashi Feng,eleyanh@nus.edu.sg;elefjia@nus.edu.sg,3;5;3,3;3;4,Reject,0,0,0,yes,10/27/17,National University of Singapore;National University of Singapore,16;16,22;22,5;11,10/27/17,0,0,0,0,0,0,12;9241,9;328,1;51,0;1200,-1;-1
971,ICLR,2018,Learning Latent Representations in Neural Networks for Clustering through Pseudo Supervision and Graph-based Activity Regularization,Ozsel Kilinc;Ismail Uysal,ozsel@mail.usf.edu;iuysal@usf.edu,6;7;7,5;4;3,Accept (Poster),2,9,0,yes,10/27/17,University of South Florida;University of South Florida,291;291,255;255,10,10/27/17,15,3,3,0,10,2,58;509,11;81,5;11,5;36,-1;-1
972,ICLR,2018,Parametric Manifold Learning Via Sparse Multidimensional Scaling,Gautam Pai;Ronen Talmon;Ron Kimmel,paigautam@cs.technion.ac.il;ronen@ef.technion.ac.il;ron@cs.technion.ac.il,5;4;3,5;4;4,Reject,0,3,0,yes,10/27/17,Technion;Technion;Technion,24;24;24,327;327;327,8,10/27/17,3,2,0,0,3,0,23;1244;16046,13;105;259,3;20;55,1;45;1394,-1;-1
973,ICLR,2018,Fast and Accurate Inference with Adaptive Ensemble Prediction for Deep Networks,Hiroshi Inoue,inouehrs@jp.ibm.com,6;5;5,3;4;4,Reject,0,3,0,yes,10/27/17,International Business Machines,-1,-1,,10/27/17,1,0,0,0,0,0,847,216,14,61,m
974,ICLR,2018,Counterfactual Image Networks,Deniz Oktay;Carl Vondrick;Antonio Torralba,denizokt@mit.edu;vondrick@google.com;torralba@mit.edu,4;5;4,4;4;4,Reject,0,1,0,yes,10/27/17,Massachusetts Institute of Technology;Google;Massachusetts Institute of Technology,2;-1;2,5;-1;5,5;2,10/27/17,0,0,0,0,0,0,14;4034;48036,10;56;281,2;25;88,0;466;6310,-1;-1
975,ICLR,2018,Massively Parallel Hyperparameter Tuning,Lisha Li;Kevin Jamieson;Afshin Rostamizadeh;Katya Gonina;Moritz Hardt;Benjamin Recht;Ameet Talwalkar,lishal@cs.ucla.edu;jamieson@cs.washington.edu;rostami@google.com;kgonina@google.com;hardt@berkeley.edu;brecht@berkeley.edu;talwalkar@cmu.edu,5;6;5,5;3;5,Reject,0,5,0,yes,9/27/18,"University of California, Los Angeles;University of Washington;Google;Google;University of California Berkeley;University of California Berkeley;Carnegie Mellon University",20;6;-1;-1;5;5;1,15;25;-1;-1;18;18;24,,2/15/18,39,17,18,2,54,8,190;1594;4296;331;7643;20105;6281,7;47;61;26;88;141;78,3;17;23;11;33;55;34,34;241;608;37;949;2640;759,-1;-1
976,ICLR,2018,Hallucinating brains with artificial brains,Peiye Zhuang;Alexander G. Schwing;Oluwasanmi Koyejo,py_zhuang@bupt.edu.cn;aschwing@illinois.edu;sanmi@illinois.edu,8;6;5,5;4;3,Reject,0,4,0,yes,10/27/17,"Beijing University of Post and Telecommunication;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",468;3;3,1103;37;37,5;4,10/27/17,1,1,0,0,0,0,14;3682;128,4;116;4,2;31;3,1;340;22,-1;-1
977,ICLR,2018,Still not systematic after all these years: On the compositional skills of sequence-to-sequence recurrent networks,Brenden Lake;Marco Baroni,brenden@nyu.edu;marco.baroni@unitn.it,6;7;6,3;4;5,Invite to Workshop Track,0,3,0,yes,10/27/17,New York University;University of Trento,26;17,27;258,8;3;1;6,10/27/17,41,24,3,2,0,4,2966;10140,47;190,16;43,287;1305,-1;-1
978,ICLR,2018,FusionNet: Fusing via Fully-aware Attention with Application to Machine Comprehension,Hsin-Yuan Huang;Chenguang Zhu;Yelong Shen;Weizhu Chen,momohuang@gmail.com;chezhu@microsoft.com;yeshen@microsoft.com;wzchen@microsoft.com,7;8;7,5;3;4,Accept (Poster),4,17,3,yes,10/27/17,National Taiwan University;Microsoft;Microsoft;Microsoft,85;-1;-1;-1,197;-1;-1;-1,4;8,10/27/17,96,51,25,5,0,13,353;841;2121;1754,39;47;50;62,12;11;16;19,44;95;214;235,-1;-1
979,ICLR,2018,Multi-task Learning on MNIST Image Datasets,Po-Chen Hsieh;Chia-Ping Chen,st70712@gmail.com;cpchen@cse.nsysu.edu.tw,5;4;5,4;5;3,Reject,0,15,0,yes,10/27/17,Purdue University;SUN YAT-SEN UNIVERSITY,28;468,60;352,,10/27/17,1,0,1,0,0,0,17;705,4;84,2;13,0;77,-1;-1
980,ICLR,2018,A closer look at the word analogy problem,Siddharth Krishna Kumar,siddharthkumar@upwork.com,2;3;3,5;4;4,Reject,0,0,0,yes,10/27/17,Upwork,-1,-1,5,10/27/17,0,0,0,0,0,0,64,19,4,7,-1
981,ICLR,2018,Learning Deep ResNet Blocks Sequentially using Boosting Theory,Furong Huang;Jordan T. Ash;John Langford;Robert E. Schapire,furongh@cs.umd.edu;jordantash@gmail.com;jcl@microsoft.com;schapire@microsoft.com,5;4;5,4;4;3,Reject,0,7,0,yes,10/27/17,"University of Maryland, College Park;Princeton University;Microsoft;Microsoft",12;31;-1;-1,69;7;-1;-1,1;8,6/15/17,46,24,14,3,131,9,1789;106;11802;64477,130;15;198;260,21;5;49;75,191;13;1282;9715,-1;-1
982,ICLR,2018,A Flexible Approach to Automated RNN Architecture Generation,Martin Schrimpf;Stephen Merity;James Bradbury;Richard Socher,msch@mit.edu;smerity@smerity.com;james.bradbury@salesforce.com;richard@socher.org,6;4;5,4;4;4,Invite to Workshop Track,0,9,0,yes,10/27/17,Massachusetts Institute of Technology;Smerity;SalesForce.com;SalesForce.com,2;-1;-1;-1,5;-1;-1;-1,3,10/27/17,7,4,3,0,445,0,227;1838;1293;52263,20;18;14;180,6;8;7;49,24;361;229;8807,-1;-1
983,ICLR,2018,Large Scale Multi-Domain Multi-Task Learning with MultiModel,Lukasz Kaiser;Aidan N. Gomez;Noam Shazeer;Ashish Vaswani;Niki Parmar;Llion Jones;Jakob Uszkoreit,lukaszkaiser@google.com;aidan.n.gomez@gmail.com;noam@google.com;avaswani@google.com;nikip@google.com;llion@google.com;usz@google.com,6;3;6,3;5;4,Reject,0,6,0,yes,10/27/17,"Google;Department of Computer Science, University of Toronto;Google;Google;Google;Google;Google",-1;17;-1;-1;-1;-1;-1,-1;22;-1;-1;-1;-1;-1,6,10/27/17,0,0,0,0,0,0,22306;9624;12620;11449;9900;9704;11549,75;15;43;52;20;13;36,24;8;19;22;12;10;22,3854;2382;2744;2659;2398;2359;2652,-1;-1
984,ICLR,2018,Learning Generative Models with Locally Disentangled Latent Factors,Brady Neal;Alex Lamb;Sherjil Ozair;Devon Hjelm;Aaron Courville;Yoshua Bengio;Ioannis Mitliagkas,nealb@seas.upenn.edu;alex6200@gmail.com;sherjilozair@gmail.com;erroneus@gmail.com;aaron.courville@gmail.com;yoshua.umontreal@gmail.com;imitliagkas@gmail.com,4;6;3,4;3;4,Reject,0,3,0,yes,10/27/17,University of Pennsylvania;University of Montreal;University of Montreal;University of Montreal;University of Montreal;University of Montreal;University of Montreal,19;124;124;124;124;124;124,10;108;108;108;108;108;108,5,10/27/17,0,0,0,0,0,0,30;1148;18008;1636;59549;201105;1164,5;21;18;43;203;807;43,1;9;10;13;64;147;18,1;182;3594;258;7800;23941;189,-1;-1
985,ICLR,2018,Towards Reverse-Engineering Black-Box Neural Networks,Seong Joon Oh;Max Augustin;Mario Fritz;Bernt Schiele,joon@mpi-inf.mpg.de;maxaug@mpi-inf.mpg.de;mfritz@mpi-inf.mpg.de;schiele@mpi-inf.mpg.de,7;7;5,3;4;4,Accept (Poster),0,8,0,yes,10/27/17,"Saarland Informatics Campus, Max-Planck Institute;Saarland Informatics Campus, Max-Planck Institute;Saarland Informatics Campus, Max-Planck Institute;Saarland Informatics Campus, Max-Planck Institute",-1;-1;-1;-1,-1;-1;-1;-1,4,10/27/17,69,43,17,2,59,8,637;116;7610;40491,29;10;196;501,12;4;45;98,84;8;1001;5067,-1;-1
986,ICLR,2018,A Deep Predictive Coding Network for Learning Latent Representations,Shirin Dora;Cyriel Pennartz;Sander Bohte,shirin.dora@gmail.com;c.m.a.pennartz@uva.nl;s.m.bohte@cwi.nl,4;3;3,4;4;5,Reject,0,0,0,yes,10/27/17,University of Amsterdam;University of Amsterdam;Centrum voor Wiskunde en Informatica,181;181;-1,59;59;-1,5,10/27/17,4,4,0,0,0,0,92;5112;2246,13;144;117,6;38;22,10;437;234,-1;-1
987,ICLR,2018,On Batch Adaptive Training for Deep Learning: Lower Loss and Larger Step Size,Runyao Chen;Kun Wu;Ping Luo,chenrunyao14@mails.ucas.ac.cn;WuKun14@mails.ucas.ac.cn;luop@ict.ac.cn,5;5;4,3;4;3,Reject,0,4,0,yes,10/27/17,"Chinese Academy of Sciences;Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences",62;62;62,1103;1103;1103,3,10/27/17,0,0,0,0,0,0,53;994;9211,3;133;94,2;16;41,5;52;1645,-1;-1
988,ICLR,2018,Training with Growing Sets: A Simple Alternative to Curriculum Learning and Self Paced Learning,Melike Nur Mermer;Mehmet Fatih Amasyali,melike.mermer@izu.edu.tr;mfatih@ce.yildiz.edu.tr,4;6;4,4;3;4,Reject,0,3,0,yes,10/27/17,Yildiz Technical University;Yildiz Technical University,468;468,904;904,,10/27/17,1,0,0,0,0,0,1;126,1;29,1;5,0;12,-1;-1
989,ICLR,2018,Learning to Mix n-Step Returns: Generalizing Lambda-Returns for Deep Reinforcement Learning,Sahil Sharma;Girish Raguvir J *;Srivatsan Ramesh *;Balaraman Ravindran,sahil@cse.iitm.ac.in;girishraguvir@gmail.com;sriramesh4@gmail.com;ravi@cse.iitm.ac.in,5;6;5,3;4;4,Reject,0,3,0,yes,10/27/17,Indian Institute of Technology Madras;;Indian Institute of Technology Madras;Indian Institute of Technology Madras,153;-1;153;153,625;-1;625;625,8,5/21/17,17,0,0,0,17,0,231;0;20;2415,43;5;10;233,10;0;2;27,13;0;0;196,-1;-1
990,ICLR,2018,Piecewise Linear Neural Networks verification: A comparative study,Rudy Bunel;Ilker Turkaslan;Philip H.S. Torr;Pushmeet Kohli;M. Pawan Kumar,rudy@robots.ox.ac.uk;ilker.turkaslan@lmh.ox.ac.uk;philip.torr@eng.ox.ac.uk;pushmeet@google.com;pawan@robots.ox.ac.uk,6;5;3,3;4;5,Reject,0,7,0,yes,10/27/17,University of Oxford;University of Oxford;University of Oxford;Google;University of Oxford,51;51;51;-1;51,1;1;1;-1;1,,10/27/17,43,19,15,5,20,4,421;95;28029;22034;2613,20;3;354;313;83,10;3;83;69;24,52;8;3831;2746;249,-1;-1
991,ICLR,2018,Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms,Tom Zahavy;Bingyi Kang;Alex Sivak;Jiashi Feng;Huan Xu;Shie Mannor,tomzahavy@gmail.com;bingykang@gmail.com;silex@campus.technion.ac.il;jshfeng@gmail.com;huan.xu@isye.gatech.edu;shiemannor@gmail.com,4;4;8,3;5;4,Invite to Workshop Track,0,10,0,yes,10/27/17,Technion;National University of Singapore;Technion;NUS;Georgia Institute of Technology;Technion,24;16;24;-1;13;24,327;22;327;-1;33;327,4;8,2/7/16,9,6,1,0,0,0,483;825;118;9241;6480;11404,31;41;30;328;298;418,9;14;5;51;37;50,30;69;2;1200;549;1225,-1;-1
992,ICLR,2018,Heterogeneous Bitwidth Binarization in Convolutional Neural Networks,Josh Fromm;Matthai Philipose;Shwetak Patel,jwfromm@uw.edu;matthaip@microsoft.com;shwetak@cs.washington.edu,6;5;4,4;4;4,Reject,0,8,0,yes,10/27/17,"University of Washington, Seattle;Microsoft;University of Washington",6;-1;6,25;-1;25,,10/27/17,13,9,4,0,39,2,143;5582;6533,9;117;213,4;34;43,6;410;469,-1;-1
993,ICLR,2018,Residual Gated Graph ConvNets,Xavier Bresson;Thomas Laurent,xbresson@ntu.edu.sg;tlaurent@lmu.edu,7;3;6,4;4;3,Reject,0,5,0,yes,10/27/17,National Taiwan University;Loyola Marymount University,85;468,197;1103,5;10,10/27/17,24,9,12,1,12,4,6322;1437,105;56,34;20,818;133,-1;-1
994,ICLR,2018,WHAT ARE GANS USEFUL FOR?,Pablo M. Olmos;Briland Hitaj;Paolo Gasti;Giuseppe Ateniese;Fernando Perez-Cruz,olmos@tsc.uc3m.es;bhitaj@stevens.edu;pgasti@nyit.edu;gatenies@stevens.edu;fernando.perezcruz@sdsc.ethz.ch,3;3;3,5;4;5,Reject,0,0,0,yes,10/27/17,Universidad Carlos III de Madrid;Stevens Institute of Technology;New York Institute of Technology;Stevens Institute of Technology;Swiss Federal Institute of Technology,-1;153;468;153;9,-1;512;1103;512;10,5,10/27/17,0,0,0,0,0,0,515;309;1935;10866;2476,70;8;62;108;133,12;3;23;41;26,62;40;213;1262;236,-1;-1
995,ICLR,2018,Learning objects from pixels,David Saxton,saxton@google.com,3;4;4,4;3;4,Reject,0,0,0,yes,10/27/17,Google,-1,-1,8,10/27/17,1,0,0,0,0,0,1004,30,9,150,-1
996,ICLR,2018,Learning to Compute Word Embeddings On the Fly,Dzmitry Bahdanau;Tom Bosc;Stanisław Jastrzębski;Edward Grefenstette;Pascal Vincent;Yoshua Bengio,dimabgv@gmail.com;bosc.tom@gmail.com;staszek.jastrzebski@gmail.com;etg@google.com;pascal.vincent@umontreal.ca;yoshua.umontreal@gmail.com,5;7;5,4;3;4,Reject,0,1,0,yes,10/27/17,University of Montreal;University of Montreal;University of Montreal;Google;University of Montreal;University of Montreal,124;124;124;-1;124;124,108;108;108;-1;108;108,3,6/1/17,46,27,18,0,124,7,26662;96;847;6977;15366;201105,31;7;27;57;118;807,18;4;12;25;33;147,4157;11;99;836;1334;23941,-1;-1
997,ICLR,2018,SIC-GAN: A Self-Improving Collaborative GAN for Decoding Sketch RNNs,Chi-Chun Chuang;Zheng-Xin Weng;Shan-Hung Wu,ccchuang@datalab.cs.nthu.edu.tw;zxweng@datalab.cs.nthu.edu.tw;shwu@cs.nthu.edu.tw,5;4;7,3;5;3,Reject,0,3,0,yes,10/27/17,National Tsing Hua University;National Tsing Hua University;National Tsing Hua University,181;181;181,323;323;323,5,10/27/17,0,0,0,0,0,0,0;0;300,1;1;33,0;0;11,0;0;46,-1;-1
998,ICLR,2018,On the Construction and Evaluation of Color Invariant Networks,Konrad Groh,konrad.groh@de.bosch.com,4;3;3,4;4;4,Reject,0,0,0,yes,10/27/17,Bosch,-1,-1,8,10/27/17,0,0,0,0,0,0,28,13,3,0,-1
999,ICLR,2018,The Mutual Autoencoder: Controlling Information in Latent Code Representations,Mary Phuong;Max Welling;Nate Kushman;Ryota Tomioka;Sebastian Nowozin,bphuong@ist.ac.at;m.welling@uva.nl;nkushman@microsoft.com;ryoto@microsoft.com;sebastian.nowozin@microsoft.com,4;5;4,5;4;4,Reject,0,1,0,yes,10/27/17,Institute of Science and Technology Austria;University of Amsterdam;Microsoft;Microsoft;Microsoft,99;181;-1;-1;-1,1103;59;-1;-1;-1,5;1,10/27/17,16,12,2,0,0,2,42;26464;1488;34;6860,5;269;29;9;134,3;58;17;3;39,5;5073;202;5;930,-1;-1
1000,ICLR,2018,Avoiding degradation in deep feed-forward networks by phasing out skip-connections,Ricardo Pio Monti;Sina Tootoonian;Robin Cao,r.monti@ucl.ac.uk;sina@gatsby.ucl.ac.uk;robin.cao@ucl.ac.uk,6;6;6,4;4;5,Reject,4,9,1,yes,10/27/17,University College London;University College London;University College London,46;46;46,16;16;16,,10/27/17,2,2,0,0,0,0,262;138;55,37;12;11,8;5;4,16;10;1,-1;-1
1001,ICLR,2018,Capturing Human Category Representations by Sampling in Deep Feature Spaces,Joshua Peterson;Krishan Aghi;Jordan Suchow;Alexander Ku;Tom Griffiths,peterson.c.joshua@gmail.com;kaghi@berkeley.edu;suchow@berkeley.edu;alexku@berkeley.edu;tom_griffiths@berkeley.edu,6;5;5,4;5;4,Invite to Workshop Track,0,4,0,yes,10/27/17,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5;5,18;18;18;18;18,5;1;2,10/27/17,1,0,0,0,3,0,263;19;33;358;21294,30;2;29;10;439,9;1;4;6;70,17;0;0;25;2170,-1;-1
1002,ICLR,2018,Deep Sensing: Active Sensing using Multi-directional Recurrent Neural Networks,Jinsung Yoon;William R. Zame;Mihaela van der Schaar,jsyoon0823@gmail.com;zame@econ.ucla.edu;mihaela.vanderschaar@oxford-man.ox.ac.uk,8;6;7,4;3;4,Accept (Poster),0,6,0,yes,10/27/17,"University of California, Los Angeles;University of California, Los Angeles;University of Oxford",20;20;51,15;15;1,,10/27/17,13,6,5,0,0,1,536;2512;8628,55;167;643,13;26;42,57;283;539,-1;-1
1003,ICLR,2018,"Universality, Robustness, and Detectability of Adversarial Perturbations under Adversarial Training",Jan Hendrik Metzen,janhendrik.metzen@de.bosch.com,3;6;6,4;3;3,Reject,2,3,0,yes,10/27/17,Bosch,-1,-1,4,10/27/17,3,2,1,1,0,1,1785,64,15,171,-1
1004,ICLR,2018,Autoregressive Generative Adversarial Networks,Yasin Yazici;Kim-Hui Yap;Stefan Winkler,yasin001@e.ntu.edu.sg;ekhyap@ntu.edu.sg;stefan.winkler@adsc.com.sg,5;3;5,4;5;5,Invite to Workshop Track,0,3,0,yes,10/27/17,National Taiwan University;National Taiwan University;Advanced Digital Sciences Center,85;85;-1,197;197;-1,5;4,10/27/17,2,0,1,0,0,0,75;1066;5060,6;132;206,2;19;33,6;76;378,-1;-1
1005,ICLR,2018,"Learning to Select: Problem, Solution, and Applications",Heechang Ryu;Donghyun Kim;Hayong Shin,rhc93@kaist.ac.kr;dhk618@kaist.ac.kr;hyshin@kaist.ac.kr,4;4;4,4;4;5,Reject,0,0,0,yes,10/27/17,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,21;21;21,95;95;95,10,10/27/17,0,0,0,0,0,0,18;327;423,7;171;80,2;10;11,3;16;30,-1;-1
1006,ICLR,2018,Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update,Su Young Lee;Sungik Choi;Sae-Young Chung,sy9424@kaist.ac.kr;si_choi@kaist.ac.kr;schung@kaist.ac.kr,4;6;5,4;4;4,Reject,0,15,0,yes,10/27/17,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,21;21;21,95;95;95,,10/27/17,7,3,1,0,7,1,572;8;4745,137;4;156,14;1;25,30;1;536,-1;-1
1007,ICLR,2018,Policy Gradient For Multidimensional Action Spaces: Action Sampling and Entropy Bonus,Vuong Ho Quan;Yiming Zhang;Kenny Song;Xiao-Yue Gong;Keith W. Ross,quan.hovuong@gmail.com;yiming.zhang@nyu.edu;kenny.song@nyu.edu;xygong@mit.edu;keithwross@nyu.edu,6;5;5,3;4;5,Reject,0,4,0,yes,10/27/17,New York University;New York University;New York University;Massachusetts Institute of Technology;New York University,26;26;26;2;26,27;27;27;5;27,,10/27/17,0,0,0,0,0,0,0;449;3;3;11976,1;32;3;4;295,0;12;1;1;55,0;30;0;0;1295,-1;-1
1008,ICLR,2018,GENERATIVE LOW-SHOT NETWORK EXPANSION,Adi Hayat;Mark Kliger;Shachar Fleishman;Daniel Cohen-Or,adi.hayat3@gmail.com;mark.kliger@gmail.com;shacharfl@gmail.com;cohenor@gmail.com,6;4;4,4;3;4,Reject,0,3,0,yes,10/27/17,Tel Aviv University;;;,37;-1;-1;-1,217;-1;-1;-1,5,10/27/17,4,0,0,0,4,0,4;498;2767;19305,2;31;30;346,1;13;14;72,0;48;240;1559,-1;-1
1009,ICLR,2018,Cheap DNN Pruning with Performance Guarantees ,Konstantinos Pitas;Mike Davies;Pierre Vandergheynst,konstantinos.pitas@epfl.ch;mike.davies@ed.ac.uk;pierre.vandergheynst@epfl.ch,6;5;5,3;3;4,Reject,0,4,0,yes,10/27/17,Swiss Federal Institute of Technology Lausanne;University of Edinburgh;Swiss Federal Institute of Technology Lausanne,468;33;468,38;27;38,,10/27/17,0,0,0,0,0,0,19;8864;16515,10;346;409,2;42;54,0;899;1868,-1;-1
1010,ICLR,2018,Alternating Multi-bit Quantization for Recurrent Neural Networks,Chen Xu;Jianqiang Yao;Zhouchen Lin;Wenwu Ou;Yuanbin Cao;Zhirong Wang;Hongbin Zha,xuen@pku.edu.cn;tianduo@taobao.com;zlin@pku.edu.cn;santong.oww@taobao.com;lingzun.cyb@alibaba-inc.com;qingfeng@taobao.com;zha@cis.pku.edu.cn,8;7;7,4;4;2,Accept (Poster),0,12,0,yes,10/27/17,Peking University;Taobao;Peking University;Taobao;Alibaba Group;Taobao;Peking University,24;-1;24;-1;-1;-1;24,27;-1;27;-1;-1;-1;27,3,10/27/17,55,32,32,3,4,19,2494;81;11524;245;55;827;4281,169;4;224;29;2;57;386,23;3;47;8;1;17;34,192;20;1993;39;19;55;281,-1;-1
1011,ICLR,2018,Learning to Generate Filters for Convolutional Neural Networks,Wei Shen;Rujie Liu,shenwei@cn.fujitsu.com;rjliu@cn.fujitsu.com,4;5;4,4;4;5,Reject,2,0,0,yes,10/27/17,Fujitsu Laboratories Ltd.;Fujitsu Laboratories Ltd.,-1;-1,-1;-1,,10/27/17,0,0,0,0,0,0,10065;415,721;66,48;8,660;35,-1;-1
1012,ICLR,2018,Theoretical properties of the global optimizer of two-layer Neural Network,Digvijay Boob;Guanghui Lan,digvijaybb40@gatech.edu;george.lan@isye.gatech.edu,4;7;7,5;5;4,Reject,4,6,0,yes,10/27/17,Georgia Institute of Technology;Georgia Institute of Technology,13;13,33;33,,10/27/17,24,11,1,2,2,0,56;4472,10;88,4;27,2;614,-1;-1
1013,ICLR,2018,Regularization for Deep Learning: A Taxonomy,Jan Kukačka;Vladimir Golkov;Daniel Cremers,jan.kukacka@tum.de;vladimir.golkov@tum.de;cremers@tum.de,5;4;4,5;4;5,Reject,0,4,0,yes,10/27/17,Technical University Munich;Technical University Munich;Technical University Munich,55;55;55,41;41;41,,10/27/17,66,33,16,0,41,2,89;1899;24880,5;35;494,3;9;76,3;291;2727,-1;-1
1014,ICLR,2018,Kernel Graph Convolutional Neural Nets,Giannis Nikolentzos;Polykarpos Meladianos;Antoine J-P Tixier;Konstantinos Skianis;Michalis Vazirgiannis,giannisnik@hotmail.com;pmeladianos@aueb.gr;antoine.tixier-1@colorado.edu;kskianis@lix.polytechnique.fr;mvazirg@lix.polytechnique.fr,5;5;4,5;4;5,Reject,0,0,0,yes,10/27/17,";;University of Colorado, Boulder;Ecole Polytechnique, France;Ecole Polytechnique, France",-1;-1;42;468;468,-1;-1;100;115;115,10,10/27/17,27,5,9,3,2,1,348;339;92;112;9324,30;17;8;14;322,11;10;5;5;41,18;20;5;3;662,-1;-1
1015,ICLR,2018,Improving Search Through A3C Reinforcement Learning Based Conversational Agent,Milan Aggarwal;Aarushi Arora;Shagun Sodhani;Balaji Krishnamurthy,milan.ag1994@gmail.com;aarushi.arora043@gmail.com;sshagunsodhani@gmail.com;kbalaji@adobe.com,5;2;3,4;5;5,Reject,0,7,0,yes,10/27/17,Indian Institute of Technology Delhi;;University of Montreal;Adobe Systems,-1;-1;124;-1,-1;-1;108;-1,,9/17/17,1,1,0,0,9,0,78;1;272;70,23;2;68;9,5;1;8;3,7;0;20;3,-1;-1
1016,ICLR,2018,Now I Remember! Episodic Memory For Reinforcement Learning,Ricky Loynd;Matthew Hausknecht;Lihong Li;Li Deng,riloynd@microsoft.com;mahauskn@microsoft.com;lihongli.cs@gmail.com;l.deng@ieee.org,4;4;4,5;4;5,Reject,0,0,0,yes,10/27/17,Microsoft;Microsoft;Google;,-1;-1;-1;-1,-1;-1;-1;-1,,10/27/17,2,1,0,0,0,0,108;2890;10545;262,8;38;242;39,5;17;46;8,11;313;1187;19,-1;-1
1017,ICLR,2018,An Out-of-the-box Full-network Embedding for Convolutional Neural Networks,Dario Garcia-Gasulla;Armand Vilalta;Ferran Parés;Jonatan Moreno;Eduard Ayguadé;Jesús Labarta;Ulises Cortés;Toyotaro Suzumura,dario.garcia@bsc.es;armand.vilalta@bsc.es;ferran.pares@bsc.es;jonatan.moreno@bsc.es;eduard.ayguade@bsc.es;jesus.labarta@bsc.es;ia@cs.upc.edu;suzumurat@gmail.com,3;4;4,4;5;5,Reject,0,0,0,yes,10/27/17,Barcelona Supercomputing Center;Barcelona Supercomputing Center;Barcelona Supercomputing Center;Barcelona Supercomputing Center;Barcelona Supercomputing Center;Barcelona Supercomputing Center;Universitat Politècnica de Catalunya;,-1;-1;-1;-1;-1;-1;468;-1,-1;-1;-1;-1;-1;-1;1103;-1,6,5/22/17,12,7,5,0,9,1,177;82;105;13;6943;6636;1918;1303,52;15;13;5;473;427;232;131,7;4;5;1;39;37;24;20,10;6;8;2;542;460;113;97,-1;-1
1018,ICLR,2018,Parameter Space Noise for Exploration,Matthias Plappert;Rein Houthooft;Prafulla Dhariwal;Szymon Sidor;Richard Y. Chen;Xi Chen;Tamim Asfour;Pieter Abbeel;Marcin Andrychowicz,matthiasplappert@me.com;rein.houthooft@openai.com;prafulla@openai.com;szymon@openai.com;richardchen@openai.com;peter@openai.com;asfour@kit.edu;pabbeel@cs.berkeley.edu;marcin@openai.com,6;7;7,4;4;5,Accept (Poster),0,8,0,yes,10/27/17,OpenAI;OpenAI;OpenAI;OpenAI;OpenAI;OpenAI;Karlsruhe Institute of Technology;University of California Berkeley;OpenAI,-1;-1;-1;-1;-1;-1;153;5;-1,-1;-1;-1;-1;-1;-1;133;18;-1,,6/6/17,248,121,64,6,40,18,843;3605;3327;935;770;13299;7170;36452;3192,11;22;12;15;21;444;294;433;29,7;11;6;7;10;40;43;94;18,74;418;906;67;80;1540;363;4390;367,-1;-1
1019,ICLR,2018,Global Convergence of Policy Gradient Methods for Linearized  Control Problems,Maryam Fazel;Rong Ge;Sham M. Kakade;Mehran Mesbahi,mfazel@uw.edu;rongge@cs.duke.edu;sham@cs.washington.edu;mesbahi@aa.washington.edu,6;5;5,4;3;3,Reject,0,3,0,yes,10/27/17,"University of Washington, Seattle;Duke University;University of Washington;University of Washington",6;46;6;6,25;17;25;25,9,10/27/17,166,93,48,16,5,38,6096;5744;13419;6235,99;78;196;268,24;32;57;33,571;801;1960;444,-1;-1
1020,ICLR,2018,A Bayesian Nonparametric Topic Model with Variational Auto-Encoders,Xuefei Ning;Yin Zheng;Zhuxi Jiang;Yu Wang;Huazhong Yang;Junzhou Huang,foxdoraame@gmail.com;yzheng3xg@gmail.com;zjiang9310@gmail.com;yu-wang@mail.tsinghua.edu.cn;yanghz@tsinghua.edu.cn;joehhuang@tencent.com,7;3;5,4;4;2,Reject,0,8,0,yes,10/27/17,Tsinghua University;Tencent AI Lab;BIT;Tsinghua University;Tsinghua University;Tencent AI Lab,10;-1;-1;10;10;-1,30;-1;-1;30;30;-1,5;11,10/27/17,0,0,0,0,0,0,126;23;377;5601;-1;342,21;28;10;505;-1;49,6;3;6;36;-1;8,7;1;62;509;0;42,-1;-1
1021,ICLR,2018,DropMax: Adaptive Stochastic Softmax,Hae Beom Lee;Juho Lee;Eunho Yang;Sung Ju Hwang,hblee@unist.ac.kr;stonecold@postech.ac.kr;yangeh@gmail.com;sjhwang82@gmail.com,6;6;4,3;4;3,Invite to Workshop Track,0,8,0,yes,10/27/17,Ulsan National Institute of Science and Technology;POSTECH;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,468;115;21;21,230;137;95;95,,10/27/17,2,1,1,0,0,0,48;1498;1032;1093,88;108;74;71,4;20;16;16,3;103;164;124,-1;-1
1022,ICLR,2018,Graph Partition Neural Networks for Semi-Supervised Classification,Renjie Liao;Marc Brockschmidt;Daniel Tarlow;Alexander Gaunt;Raquel Urtasun;Richard S. Zemel,rjliao@cs.toronto.edu;mabrocks@microsoft.com;dtarlow@google.com;algaunt@microsoft.com;urtasun@cs.toronto.edu;zemel@cs.toronto.edu,6;5;6,3;3;3,Invite to Workshop Track,0,3,0,yes,10/27/17,"Department of Computer Science, University of Toronto;Microsoft;Google;Microsoft;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",17;-1;-1;-1;17;17,22;-1;-1;-1;22;22,10,10/27/17,27,11,12,1,9,3,1581;2475;2499;1142;24327;21384,63;61;67;38;245;208,22;22;23;14;73;52,204;326;301;88;3432;2479,-1;-1
1023,ICLR,2018,Semi-supervised Outlier Detection using Generative And Adversary Framework,Jindong Gu;Matthias Schubert;Volker Tresp,jindong.gu@siemens.com;schubert@dbs.ifi.lmu.de;volker.tresp@siemens.com,4;4;3,3;4;5,Reject,0,4,0,yes,10/27/17,Siemens Corporate Research;Institut für Informatik;Siemens Corporate Research,-1;-1;-1,-1;-1;-1,5;4,10/27/17,4,1,1,0,0,1,24;1567;8126,8;118;287,2;21;44,4;138;794,-1;-1
1024,ICLR,2018,Deep Asymmetric Multi-task Feature Learning,Hae Beom Lee;Eunho Yang;Sung Ju Hwang,hblee@unist.ac.kr;yangeh@gmail.com;sjhwang@unist.ac.kr,6;3;5,4;4;4,Reject,0,0,0,yes,10/27/17,Ulsan National Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Ulsan National Institute of Science and Technology,468;21;468,230;95;230,,8/1/17,8,3,2,0,17,0,48;1032;1093,88;74;71,4;16;16,3;164;124,-1;-1
1025,ICLR,2018,Cross-Corpus Training with TreeLSTM for the Extraction of Biomedical Relationships from Text,Legrand Joël;Yannick Toussaint;Chedy Raïssi;Adrien Coulet,joel.legrand@loria.fr;yannick.toussaint@loria.fr;chedy.raissi@inria.fr;adrien.coulet@loria.fr,4;5;3,4;4;5,Invite to Workshop Track,0,0,0,yes,10/27/17,University of Lorraine;University of Lorraine;INRIA;University of Lorraine,468;468;-1;468,535;535;-1;535,,10/27/17,0,0,0,0,0,0,0;713;507;801,1;136;66;73,0;15;14;11,0;23;40;73,-1;-1
1026,ICLR,2018,Learning to navigate by distilling visual information and natural language instructions,Abhishek Sinha;Akilesh B;Mausoom Sarkar;Balaji Krishnamurthy,abhsinha@adobe.com;akb@adobe.com;msarkar@adobe.com;kbalaji@adobe.com,4;4;5,4;5;3,Reject,0,16,0,yes,10/27/17,Adobe Systems;Adobe Systems;Adobe Systems;Adobe Systems,-1;-1;-1;-1,-1;-1;-1;-1,3;6;8,10/27/17,0,0,0,0,0,0,24;2;18;272,3;2;13;68,2;1;3;8,1;0;2;20,-1;-1
1027,ICLR,2018,Statestream: A toolbox to explore layerwise-parallel deep neural networks,Volker Fischer,volker.fischer@de.bosch.com,3;5;5,4;4;3,Reject,0,3,0,yes,10/27/17,Bosch,-1,-1,10,10/27/17,0,0,0,0,0,0,92,37,7,12,-1
1028,ICLR,2018,Beyond Finite Layer Neural Networks: Bridging Deep Architectures and Numerical Differential Equations,Yiping Lu;Aoxiao Zhong;Quanzheng Li;Bin Dong,luyiping9712@pku.edu.cn;zhongaoxiao@gmail.com;quanzhengli5@gmail.com;dongbin@math.pku.edu.cn,7;6;5;5,4;1;3;1,Invite to Workshop Track,0,4,0,yes,10/27/17,Peking University;Zhejiang University;;Peking University,24;124;-1;24,27;658;-1;27,8,10/27/17,124,70,38,1,168,13,464;755;1694;1763,11;9;136;138,7;5;24;20,47;40;91;120,-1;-1
1029,ICLR,2018,Training RNNs as Fast as CNNs,Tao Lei;Yu Zhang;Yoav Artzi,tao@asapp.com;yzhang87@csail.mit.edu;yoav@cs.cornell.edu,7;8;4,4;5;5,Reject,8,10,0,yes,10/27/17,ASAPP Inc;Massachusetts Institute of Technology;Cornell University,-1;2;7,-1;5;19,3,9/8/17,115,59,43,2,550,31,1395;1482;2155,191;113;46,20;17;22,115;142;321,-1;-1
1030,ICLR,2018,Revisiting Bayes by Backprop,Meire Fortunato;Charles Blundell;Oriol Vinyals,meirefortunato@google.com;cblundell@google.com;vinyals@google.com,5;6;6,4;4;5,Reject,0,5,0,yes,10/27/17,Google;Google;Google,-1;-1;-1,-1;-1;-1,3;11,10/27/17,0,0,0,0,0,0,1330;5964;52009,8;50;121,5;22;55,171;1050;6497,-1;-1
1031,ICLR,2018,PARAMETRIZED DEEP Q-NETWORKS LEARNING: PLAYING ONLINE BATTLE ARENA WITH DISCRETE-CONTINUOUS HYBRID ACTION SPACE,Jiechao Xiong;Qing Wang;Zhuoran Yang;Peng Sun;Yang Zheng;Lei Han;Haobo Fu;Xiangru Lian;Carson Eisenach;Haichuan Yang;Emmanuel Ekwedike;Bei Peng;Haoyue Gao;Tong Zhang;Ji Liu;Han Liu,jcxiong@tencent.com;drwang@tencent.com;pythonsun@tencent.com;zakzheng@tencent.com;lxhan@tencent.com;haobofu@tencent.com;tongzhang@tongzhang-ml.org;ji.liu.uwisc@gmail.com,5;5;4,4;3;4,Reject,0,0,0,yes,10/27/17,Tencent AI Lab;Tencent AI Lab;Tencent AI Lab;Tencent AI Lab;Tencent AI Lab;Tencent AI Lab;;University of Rochester,-1;-1;-1;-1;-1;-1;-1;104,-1;-1;-1;-1;-1;-1;-1;153,,10/27/17,1,1,0,0,0,0,187;1201;522;176;3032;205;107;928;15;184;21;537;-1;18847;3693;1178,29;190;47;62;419;59;12;21;8;32;5;71;-1;184;63;78,8;16;12;8;26;7;7;10;2;8;2;13;-1;69;25;19,11;91;70;11;167;13;12;180;0;5;1;18;0;2319;533;132,-1;-1
1032,ICLR,2018,Towards Building Affect sensitive Word Distributions,Kushal Chawla;Sopan Khosla;Niyati Chhaya;Kokil Jaidka,kchawla@adobe.com;skhosla@adobe.com;nchhaya@adobe.com;jaidka@sas.upenn.edu,6;4;4,3;5;4,Reject,0,4,0,yes,10/27/17,Adobe Systems;Adobe Systems;Adobe Systems;University of Pennsylvania,-1;-1;-1;19,-1;-1;-1;10,3,10/27/17,0,0,0,0,0,0,17;25;93;500,13;13;41;61,2;2;5;14,0;3;3;33,-1;-1
1033,ICLR,2018,Unsupervised Learning of Entailment-Vector Word Embeddings,James Henderson,james.henderson@idiap.ch,3;7;3,5;3;5,Reject,0,3,0,yes,10/27/17,Idiap Research Institute,-1,-1,3,10/27/17,0,0,0,0,0,0,2092,141,26,181,-1
1034,ICLR,2018,Overcoming the vanishing gradient problem in plain recurrent networks,Yuhuang Hu;Adrian Huber;Shih-Chii Liu,yuhuang.hu@ini.uzh.ch;huberad@ini.uzh.ch;shih@ini.uzh.ch,4;2;7,5;4;4,Reject,0,5,0,yes,10/27/17,University of Zurich;University of Zurich;University of Zurich,139;139;139,136;136;136,,10/27/17,11,6,1,0,38,0,231;29;4770,12;8;181,4;3;29,35;0;407,-1;-1
1035,ICLR,2018,VoiceLoop: Voice Fitting and Synthesis via a Phonological Loop,Yaniv Taigman;Lior Wolf;Adam Polyak;Eliya Nachmani,yaniv@fb.com;wolf@fb.com;adampolyak@fb.com;enk100@gmail.com,8;5;6,4;4;4,Accept (Poster),1,6,0,yes,10/27/17,Facebook;Facebook;Facebook;Facebook,-1;-1;-1;-1,-1;-1;-1;-1,,7/20/17,67,32,29,2,51,9,5727;13811;757;560,26;199;12;13,16;45;7;8,564;1636;58;38,-1;-1
1036,ICLR,2018,Graph Topological Features via GAN,Weiyi Liu;Hal Cooper;Min-Hwan Oh,weiyiliu@us.ibm.com;hal.cooper@columbia.edu;m.oh@columbia.edu,3;4;4,4;4;5,Reject,0,0,0,yes,10/27/17,International Business Machines;Columbia University;Columbia University,-1;15;15,-1;14;14,5;4;10,10/27/17,0,0,0,0,0,0,933;55;78,169;30;21,13;4;4,60;5;7,-1;-1
1037,ICLR,2018,Improve Training Stability of Semi-supervised Generative Adversarial Networks with Collaborative Training,Dalei Wu;Xiaohua Liu,daleiwu@gmail.com;dalei.wu@huawei.com;liuxh3@huawei.com,3;2;3,4;4;5,Reject,0,0,0,yes,10/27/17,;Huawei Technologies Ltd.;Huawei Technologies Ltd.,-1;-1;-1,-1;-1;-1,5;4,10/27/17,1,1,0,0,0,0,1382;193,124;81,20;7,65;12,-1;-1
1038,ICLR,2018,Disentangled activations in deep networks,Mikael Kågebäck;Olof Mogren,kageback@chalmers.se;olof@mogren.one,6;5;4,3;4;3,Reject,0,3,0,yes,10/27/17,Chalmers University;Chalmers University,153;153,240;240,,10/27/17,3,0,0,0,0,0,181;339,14;26,7;7,16;28,-1;-1
1039,ICLR,2018,On the State of the Art of Evaluation in Neural Language Models,Gábor Melis;Chris Dyer;Phil Blunsom,melisgl@google.com;cdyer@cs.cmu.edu;phil.blunsom@cs.ox.ac.uk,7;5;8,2;5;3,Accept (Poster),0,16,0,yes,10/27/17,Google;Carnegie Mellon University;University of Oxford,-1;1;51,-1;24;1,3,7/18/17,272,149,73,20,0,34,602;21075;11400,14;230;143,8;59;47,99;3147;1326,-1;-1
1040,ICLR,2018,Interpretable Classification via Supervised Variational Autoencoders and Differentiable Decision Trees,Eleanor Quint;Garrett Wirka;Jacob Williams;Stephen Scott;N.V. Vinodchandran,pquint@cse.unl.edu;gwirka@cse.unl.edu;jwilliam@cse.unl.edu;sscott@cse.unl.edu;vinod@cse.unl.edu,3;4;5,5;4;4,Reject,0,2,0,yes,10/27/17,"University of Nebraska, Lincoln;University of Nebraska, Lincoln;University of Nebraska, Lincoln;University of Nebraska, Lincoln;University of Nebraska, Lincoln",244;244;244;244;244,337;337;337;337;337,5,10/27/17,2,1,0,0,0,0,2;2;172;909;666,4;1;29;86;85,1;1;7;16;14,0;0;18;97;64,-1;-1
1041,ICLR,2018,Zero-shot Cross Language Text Classification,Dan Svenstrup;Jonas Meinertz Hansen;Ole Winther,dsve@dtu.dk;jonas@meinertz.org;olwi@dtu.dk,4;2;3,3;4;4,Reject,0,0,0,yes,10/27/17,Technical University of Denmark;;Technical University of Denmark,210;-1;210,153;-1;153,,10/27/17,0,0,0,0,0,0,35;25;6051,4;4;201,2;2;34,4;4;711,-1;-1
1042,ICLR,2018,Long Term Memory Network for Combinatorial Optimization Problems,Hazem A. A. Nomer;Abdallah Aboutahoun;Ashraf Elsayed,hazemahmed@alexu.edu.eg;abdallah_aboutahoun@alexu.edu.eg;ashraf.elsayed@alexu.edu.eg,4;4;3,1;2;4,Reject,3,0,0,yes,10/27/17,Alexandria University;Alexandria University;Alexandria University,468;468;468,896;896;896,,10/27/17,0,0,0,0,0,0,0;36;176,1;13;40,0;2;7,0;1;11,-1;-1
1043,ICLR,2018,Autoregressive Convolutional Neural Networks for Asynchronous Time Series,Mikolaj Binkowski;Gautier Marti;Philippe Donnat,mikbinkowski@gmail.com;gautier.marti@gmail.com;pdonnat@helleborecapital.com,4;5;5,5;3;4,Reject,0,1,0,yes,10/27/17,Imperial College London;;Hellebore Capital Ltd.,74;-1;-1,8;-1;-1,,3/12/17,36,15,10,0,183,4,276;87;98,7;17;16,4;5;5,65;5;8,-1;-1
1044,ICLR,2018,Towards Unsupervised Classification with Deep Generative Models,Dimitris Kalatzis;Konstantia Kotta;Ilias Kalamaras;Anastasios Vafeiadis;Andrew Rawstron;Dimitris Tzovaras;Kostas Stamatopoulos,dkal@iti.gr;ntina_kotta@yahoo.com;kalamar@iti.gr;anasvaf@iti.gr;a.c.rawstron@leeds.ac.uk;dimitrios.tzovaras@iti.gr;kostas.stamatopoulos@gmail.com,4;4;4,4;4;5,Reject,0,1,0,yes,10/27/17,CERTH/ITI;;CERTH/ITI;CERTH/ITI;University of Leeds;CERTH/ITI;,-1;-1;-1;-1;244;-1;-1,-1;-1;-1;-1;139;-1;-1,5,10/27/17,0,0,0,0,0,0,3;115;83;33;5219;13;3930,2;24;24;13;266;7;244,1;6;4;3;37;2;32,0;2;3;1;269;0;205,-1;-1
1045,ICLR,2018,Topic-Based Question Generation,Wenpeng Hu;Bing Liu;Rui Yan;Dongyan Zhao;Jinwen Ma,wenpeng.hu@pku.edu.cn;liub@cs.uic.edu;ruiyan@pku.edu.cn;zhaody@pku.edu.cn;jwma@math.pku.edu.cn,3;4;8,5;4;3,Invite to Workshop Track,0,3,0,yes,10/27/17,"Peking University;University of Illinois, Chicago;Peking University;Peking University;Peking University",24;57;24;24;24,27;255;27;27;27,,10/27/17,0,0,0,0,0,0,141;105;6288;2577;1990,19;35;640;163;192,7;5;37;27;21,15;8;424;305;124,-1;-1
1046,ICLR,2018,Variance Regularized Counterfactual Risk Minimization via Variational Divergence Minimization,Hang Wu,hwu340@gatech.edu;hangwu@gatech.edu,4;5;7,4;5;3,Reject,0,6,0,yes,10/27/17,Georgia Institute of Technology;Georgia Institute of Technology,13;13,33;33,8,10/27/17,4,2,2,0,0,2,396,60,12,17,-1
1047,ICLR,2018,Lifelong Learning with Output Kernels,Keerthiram Murugesan;Jaime Carbonell,kmuruges@cs.cmu.edu;jgc@cs.cmu.edu,3;2;4,4;5;4,Reject,0,4,0,yes,10/27/17,Carnegie Mellon University;Carnegie Mellon University,1;1,24;24,1,10/27/17,1,1,0,0,0,0,99;15678,22;505,6;54,13;1625,-1;-1
1048,ICLR,2018,Deep Continuous Clustering,Sohil Atul Shah;Vladlen Koltun,sohilas@umd.edu;vkoltun@gmail.com,6;3;7,3;5;4,Reject,0,10,0,yes,10/27/17,"University of Maryland, College Park;Intel",12;-1,69;-1,,10/27/17,18,7,8,0,4,3,81;17520,4;188,2;62,25;2489,-1;-1
1049,ICLR,2018,Label Embedding Network: Learning Label Representation for Soft Training of Deep Networks,Xu Sun;Bingzhen Wei;Xuancheng Ren;Shuming Ma,xusun@pku.edu.cn;weibz@pku.edu.cn;renxc@pku.edu.cn;shumingma@pku.edu.cn,4;4;3,5;3;4,Reject,0,1,0,yes,10/27/17,Peking University;Peking University;Peking University;Peking University,24;24;24;24,27;27;27;27,,10/27/17,23,8,6,0,9,0,5530;112;443;538,542;14;40;50,41;6;11;13,330;11;51;55,-1;-1
1050,ICLR,2018,Semantic Code Repair using Neuro-Symbolic Transformation Networks,Jacob Devlin;Jonathan  Uesato;Rishabh Singh;Pushmeet Kohli,jacobdevlin@google.com;juesato@gmail.com;risin@microsoft.com;pushmeet@google.com,4;6;6,4;4;4,Invite to Workshop Track,0,4,0,yes,10/27/17,Google;;Microsoft;Google,-1;-1;-1;-1,-1;-1;-1;-1,,10/27/17,15,6,8,0,8,0,9116;895;2149;22034,45;17;80;313,20;11;24;69,2663;110;185;2746,-1;-1
1051,ICLR,2018,Simple and efficient architecture search for Convolutional Neural Networks,Thomas Elsken;Jan Hendrik Metzen;Frank Hutter,thomas.elsken@de.bosch.com;janhendrik.metzen@de.bosch.com;fh@cs.uni-freiburg.de,6;5;4,4;5;4,Invite to Workshop Track,0,4,0,yes,10/27/17,Bosch;Bosch;Universität Freiburg,-1;-1;115,-1;-1;82,,10/27/17,86,36,34,1,124,12,674;1785;12566,24;64;233,8;15;49,69;171;1521,-1;-1
1052,ICLR,2018,Natural Language Inference with External Knowledge,Qian Chen;Xiaodan Zhu;Zhen-Hua Ling;Diana Inkpen,cq1231@mail.ustc.edu.cn;xiaodan.zhu@queensu.ca;zhling@ustc.edu.cn;diana.inkpen@uottawa.ca,6;5;3;7,5;4;5;4,Invite to Workshop Track,3,4,0,yes,10/27/17,University of Science and Technology of China;Queens University;University of Science and Technology of China;University of Ottawa,115;244;115;291,132;261;132;233,3,10/27/17,86,48,29,1,9,12,3820;4225;3029;3675,257;89;155;166,27;30;26;30,421;658;360;458,-1;-1
1053,ICLR,2018,AirNet: a machine learning dataset for air quality forecasting,Songgang Zhao;Xingyuan Yuan;Da Xiao;Jianyuan Zhang;Zhouyuan Li,gfgkmn@gmail.com;yuan@caiyunapp.com;xiaoda99@gmail.com;littletree@caiyunapp.com;joeyzhouyuanli@caiyunapp.com,5;4;4,4;4;4,Reject,0,4,0,yes,10/27/17,;ColorfulClouds Tech.;Beijing University of Post and Telecommunication;ColorfulClouds Tech.;ColorfulClouds Tech.,-1;-1;468;-1;-1,-1;-1;1103;-1;-1,,10/27/17,6,4,0,0,0,0,6;12;67;805;296,1;3;32;51;14,1;2;6;15;8,0;0;4;34;17,-1;-1
1054,ICLR,2018,Recursive Binary Neural Network Learning Model  with 2-bit/weight Storage Requirement,Tianchan Guan;Xiaoyang Zeng;Mingoo Seok,tg2569@columbia.edu;xyzeng@fudan.edu;ms4415@columbia.edu,6;7;5,3;4;3,Reject,0,3,0,yes,10/27/17,Columbia University;Fudan University;Columbia University,15;78;15,14;116;14,,9/15/17,4,1,2,0,31,0,12;1316;2408,7;351;148,2;16;25,0;115;190,-1;-1
1055,ICLR,2018,Learning Dynamic State Abstractions for Model-Based Reinforcement Learning,Lars Buesing;Theophane Weber;Sebastien Racaniere;S. M. Ali Eslami;Danilo Rezende;David Reichert;Fabio Viola;Frederic Besse;Karol Gregor;Demis Hassabis;Daan Wierstra,lbuesing@google.com;theophane@google.com;sracaniere@google.com;aeslami@google.com;danilor@google.com;reichert@google.com;fviola@google.com;fbesse@google.com;demishassabis@google.com;wierstra@google.com,6;5;8,4;4;4,Reject,0,6,1,yes,10/27/17,Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,,10/27/17,1,1,0,0,0,1,2394;1471;772;4034;8601;36;190;208;4712;23857,54;35;32;38;62;11;6;5;44;55,21;14;12;19;27;2;1;2;19;31,250;147;74;514;1112;1;32;34;528;2564,-1;-1
1056,ICLR,2018,Towards Effective GANs for Data Distributions with Diverse Modes,Sanchit Agrawal;Gurneet Singh;Mitesh Khapra,sanchit@cse.iitm.ac.in;garry@cse.iitm.ac.in;miteshk@cse.iitm.ac.in,6;4;4,5;3;3,Invite to Workshop Track,0,3,0,yes,10/27/17,Indian Institute of Technology Madras;Indian Institute of Technology Madras;Indian Institute of Technology Madras,153;153;153,625;625;625,5;4,10/27/17,1,0,0,0,0,0,1;11;1327,2;10;91,1;2;18,0;1;152,-1;-1
1057,ICLR,2018,Unsupervised Adversarial Anomaly  Detection using One-Class Support Vector Machines,Prameesha Sandamal Weerasinghe;Tansu Alpcan;Sarah Monazam Erfani;Christopher Leckie,pweerasinghe@student.unimelb.edu.au;tansu.alpcan@unimelb.edu.au;sarah.erfani@unimelb.edu.au;caleckie@unimelb.edu.au,4;4;4,4;4;4,Reject,0,0,0,yes,10/27/17,The University of Melbourne;The University of Melbourne;The University of Melbourne;The University of Melbourne,124;124;124;124,32;32;32;32,4,10/27/17,1,0,0,0,0,0,0;5199;892;5941,1;297;57;296,0;34;11;36,0;297;81;425,-1;-1
1058,ICLR,2018,Estimation of cross-lingual news similarities using text-mining methods,Zhouhao Wang;Enda Liu;Hiroki Sakaji;Tomoki Ito;Kiyoshi Izumi;Kota Tsubouchi;Tatsuo Yamashita,wangzhouhao94@gmail.com;m2015eliu@socsim.org;sakaji@sys.t.u-tokyo.ac.jp;m2015titoh@socsim.org;izumi@sys.t.u-tokyo.ac.jp;ktsubouc@yahoo-corp.jp;tayamash@yahoo-corp.jp,2;6;2,5;4;4,Reject,0,0,0,yes,10/27/17,The University of Tokyo;;The University of Tokyo;;The University of Tokyo;;,52;-1;52;-1;52;-1;-1,45;-1;45;-1;45;-1;-1,,10/27/17,3,0,0,0,3,0,10;0;102;5936;502;329;371,3;2;44;191;108;87;38,1;0;6;31;11;9;8,0;0;4;414;20;10;56,-1;-1
1059,ICLR,2018,Data-driven Feature Sampling for Deep Hyperspectral Classification and Segmentation,William M. Severa;Jerilyn A. Timlin;Suraj Kholwadwala;Conrad D. James;James B. Aimone,wmsever@sandia.gov;jatimli@sandia.gov;skholwadwala@gmail.com;cdjame@sandia.gov;jbaimon@sandia.gov,3;6;4,5;5;5,Reject,0,0,0,yes,10/27/17,Sandia National Laboratories;Sandia National Laboratories;;Sandia National Laboratories;Sandia National Laboratories,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,2,10/26/17,5,0,0,0,5,0,48;1339;0;1096;4461,22;157;1;144;96,3;18;0;18;20,0;60;0;34;303,-1;-1
1060,ICLR,2018,Grouping-By-ID: Guarding Against Adversarial Domain Shifts,Christina Heinze-Deml;Nicolai Meinshausen,heinzedeml@stat.math.ethz.ch;meinshausen@stat.math.ethz.ch,7;4;5,3;5;4,Reject,0,4,0,yes,10/27/17,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,9;9,10;10,4;6;7;10,10/27/17,2,0,1,0,0,0,132;2701,15;28,5;8,11;402,-1;-1
1061,ICLR,2018,"MACH: Embarrassingly parallel $K$-class classification in $O(d\log{K})$ memory and $O(K\log{K} + d\log{K})$ time, instead of $O(Kd)$",Qixuan Huang;Anshumali Shrivastava;Yiqiu Wang,qh5@rice.edu;anshumali@rice.edu;yiqiu.wang@rice.edu,6;6;6,4;4;4,Reject,0,7,0,yes,10/27/17,Rice University;Rice University;Rice University,85;85;85,86;86;86,,10/27/17,2,1,0,0,0,0,7;1097;40,6;100;8,2;16;3,0;110;0,-1;-1
1062,ICLR,2018,SHADE: SHAnnon DEcay Information-Based Regularization for Deep Learning,Michael Blot;Thomas Robert;Nicolas Thome;Matthieu Cord,michael.blot@lip6.fr;thomas.robert@lip6.fr;nicolas.thome@lip6.fr;matthieu.cord@lip6.fr,4;5;7;4,3;4;3;3,Reject,0,7,0,yes,10/27/17,LIP6;LIP6;LIP6;LIP6,-1;-1;-1;-1,-1;-1;-1;-1,,10/27/17,5,4,3,0,35,1,65;2138;2120;3325,12;215;99;203,4;25;26;33,8;124;213;289,-1;-1
1063,ICLR,2018,Lifelong Generative Modeling,Jason Ramapuram;Magda Gregorova;Alexandros Kalousis,jason.ramapuram@etu.unige.ch;magda.gregorova@unige.ch;alexandros.kalousis@hesge.ch,9;4;4,5;5;2,Reject,0,4,0,yes,10/27/17,"University of Geneva, Switzerland;University of Geneva, Switzerland;Geneva School of Business Administration, HES-SO University of Applied Sciences of Western Switzerland;",468;468;468;-1,130;130;1103;-1,5;6,5/27/17,21,15,6,1,11,1,35;29;2075,9;7;97,2;2;24,3;1;213,-1;-1
1064,ICLR,2018,A Self-Training Method for Semi-Supervised GANs,Alan Do-Omri;Dalei Wu;Xiaohua Liu,alan.do-omri@mail.mcgill.ca;daleiwu@gmail.com;liuxiaohua3@huawei.com,3;4;3,5;4;4,Reject,0,4,0,yes,10/27/17,McGill University;;Huawei Technologies Ltd.,81;-1;-1,42;-1;-1,5;4,10/27/17,1,1,0,0,5,0,6;1387;193,4;124;81,2;20;7,0;67;12,-1;-1
1065,ICLR,2018,Neighbor-encoder,Chin-Chia Michael Yeh;Yan Zhu;Evangelos E. Papalexakis;Abdullah Mueen;Eamonn Keogh,myeh003@ucr.edu;yzhu015@ucr.edu;epapalex@cs.ucr.edu;mueen@unm.edu;eamonn@cs.ucr.edu,5;6;4,5;4;4,Reject,0,3,2,yes,10/27/17,"University of California, Riverside;University of California, Riverside;University of California, Riverside;University of New Mexico;University of California, Riverside",62;62;62;210;62,197;197;197;1103;197,8,10/27/17,0,0,0,0,0,0,595;3273;2413;3358;27473,34;133;133;85;334,12;11;25;27;77,78;304;177;427;3367,-1;-1
1066,ICLR,2018,Benefits of Depth for Long-Term Memory of Recurrent Networks,Yoav Levine;Or Sharir;Amnon Shashua,yoavlevine@cs.huji.ac.il;or.sharir@cs.huji.ac.il;shashua@cs.huji.ac.il,5;7;6,2;3;3,Invite to Workshop Track,0,6,0,yes,10/27/17,Hebrew University of Jerusalem;Hebrew University of Jerusalem;Hebrew University of Jerusalem,62;62;62,205;205;205,1,10/25/17,6,1,3,0,0,0,138;328;8210,9;15;173,6;9;48,7;19;735,-1;-1
1067,ICLR,2018,A Neural Method for Goal-Oriented Dialog Systems to interact with Named Entities,Janarthanan Rajendran;Jatin Ganhotra;Xiaoxiao Guo;Mo Yu;Satinder Singh,rjana@umich.edu;jatinganhotra@us.ibm.com;xiaoxiao.guo@ibm.com;yum@us.ibm.com;baveja@umich.edu,6;4;3,3;3;3,Reject,0,4,0,yes,10/27/17,University of Michigan;International Business Machines;International Business Machines;International Business Machines;University of Michigan,8;-1;-1;-1;8,21;-1;-1;-1;21,,10/27/17,0,0,0,0,0,0,104;115;141;84;84,14;19;14;48;28,6;7;3;4;6,10;5;19;12;8,-1;-1
1068,ICLR,2018,Loss Functions for Multiset Prediction,Sean Welleck;Zixin Yao;Yu Gai;Jialin Mao;Zheng Zhang;Kyunghyun Cho,wellecks@nyu.edu;zy566@nyu.edu;yg1246@nyu.edu;jm5830@nyu.edu;zz@nyu.edu;kyunghyun.cho@nyu.edu,5;7;4,4;3;3,Reject,0,3,0,yes,10/27/17,New York University;New York University;New York University;New York University;New York University;New York University,26;26;26;26;26;26,27;27;27;27;27;27,,10/27/17,9,4,4,0,15,0,115;9;64;467;-1;45318,11;1;7;34;-1;272,6;1;2;13;-1;52,14;0;6;30;0;6549,-1;-1
1069,ICLR,2018,Improving the Universality and Learnability of Neural Programmer-Interpreters with Combinator Abstraction,Da Xiao;Jo-Yu Liao;Xingyuan Yuan,xiaoda99@gmail.com;liaoruoyu@caiyunapp.com;yuan@caiyunapp.com,3;7;7,4;4;4,Accept (Poster),0,10,0,yes,10/27/17,Beijing University of Post and Telecommunication;ColorfulClouds Tech.;ColorfulClouds Tech.,468;-1;-1,1103;-1;-1,,10/27/17,7,4,2,0,7,0,67;7;12,32;1;3,6;1;2,4;0;0,-1;-1
1070,ICLR,2018,Efficiently applying attention to sequential data with the Recurrent Discounted Attention unit,Brendan Maginnis;Pierre Richemond,brendan.maginnis@gmail.com;pierre.richemond@gmail.com,4;6;3,5;4;4,Reject,0,0,0,yes,10/27/17,Imperial College London;Imperial College London,74;74,8;8,,5/23/17,17,0,0,0,17,0,32;13,7;11,3;2,3;2,-1;-1
1071,ICLR,2018,Comparison of Paragram and GloVe Results for Similarity Benchmarks,Jakub Dutkiewicz;Czesław Jędrzejek,jakub.dutkiewicz@put.poznan.pl;czeslaw.jedrzejek@put.poznan.pl,2;4;3,4;5;4,Reject,0,1,0,yes,10/27/17,Poznan University of Technology;Poznan University of Technology,468;468,1103;1103,3,10/27/17,0,0,0,0,0,0,13;92,18;66,2;4,0;4,-1;-1
1072,ICLR,2018,Domain Adaptation for Deep Reinforcement Learning in Visually Distinct Games,Dino S. Ratcliffe;Luca Citi;Sam Devlin;Udo Kruschwitz,d.ratcliffe@qmul.ac.uk;lciti@essex.ac.uk;sam.devlin@york.ac.uk;udo@essex.ac.uk,3;2;4,3;4;5,Reject,0,0,0,yes,10/27/17,Queen Mary University London;University of Sussex;University of York;University of Sussex,244;244;210;244,121;146;137;146,10,10/27/17,1,0,0,0,0,0,9;2141;621;1254,3;125;73;174,1;23;12;19,0;84;33;84,-1;-1
1073,ICLR,2018,Flexible Prior Distributions for Deep Generative Models,Yannic Kilcher;Aurelien Lucchi;Thomas Hofmann,yannic.kilcher@inf.ethz.ch;aurelien.lucchi@inf.ethz.ch;thomas.hofmann@inf.ethz.ch,6;6;5,4;3;4,Reject,0,3,0,yes,10/27/17,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,9;9;9,10;10;10,5;8,10/27/17,10,0,0,0,10,0,113;6789;22653,14;68;173,4;24;52,26;1108;3390,-1;-1
1074,ICLR,2018,Ego-CNN: An Ego Network-based Representation of Graphs Detecting Critical Structures,Ruo-Chun Tzeng;Shan-Hung Wu,rctzeng@datalab.cs.nthu.edu.tw;shwu@cs.nthu.edu.tw,7;4;4,3;4;4,Reject,1,4,0,yes,10/27/17,National Tsing Hua University;National Tsing Hua University,181;181,323;323,10,10/27/17,0,0,0,0,0,0,2;300,2;33,1;11,0;46,-1;-1
1075,ICLR,2018,Lifelong Learning by Adjusting Priors,Ron Amit;Ron Meir,ronamit@campus.technion.ac.il;rmeir@ee.technion.ac.il,6;6;6,4;4;4,Reject,0,9,0,yes,10/27/17,Technion;Technion,24;24,327;327,6;8,10/27/17,0,0,0,0,0,0,44;3451,6;145,2;29,11;404,-1;-1
1076,ICLR,2018,A Simple Fully Connected Network for Composing Word Embeddings from Characters,Michael Traynor;Thomas Trappenberg,mike.sk.traynor@gmail.com;trappenberg@gmail.com,3;5;4,4;4;5,Reject,0,0,0,yes,10/27/17,Dalhousie University;,291;-1,289;-1,3,10/27/17,0,0,0,0,0,0,1154;880,177;139,16;16,114;49,-1;-1
1077,ICLR,2018,"Dissecting Adam: The Sign, Magnitude and Variance of Stochastic Gradients",Lukas Balles;Philipp Hennig,lukas.balles@tuebingen.mpg.de;ph@tue.mpg.de,4;4;6,4;4;3,Reject,1,6,0,yes,10/27/17,"Max Planck Institute for Intelligent Systems, Max-Planck Institute;Max Planck Institute for Intelligent Systems, Max-Planck Institute",-1;-1,-1;-1,,5/22/17,35,21,6,1,74,4,224;2146,10;99,6;24,31;202,-1;-1
1078,ICLR,2018,Censoring Representations with Multiple-Adversaries over Random Subspaces,Yusuke Iwasawa;Kotaro Nakayama;Yutaka Matsuo,iwasawa@weblab.t.u-tokyo.ac.jp;nakayama@weblab.t.u-tokyo.ac.jp;matsuo@weblab.t.u-tokyo.ac.jp,6;5;6,3;4;4,Reject,0,5,0,yes,10/27/17,The University of Tokyo;The University of Tokyo;The University of Tokyo,52;52;52,45;45;45,4,10/27/17,0,0,0,0,0,0,113;674;7438,39;71;382,5;13;34,10;88;506,-1;-1
1079,ICLR,2018,Evolutionary Expectation Maximization for Generative Models with Binary Latents,Enrico Guiraud;Jakob Drefs;Joerg Luecke,enrico.guiraud@cern.ch;jakob.heinrich.drefs@uni-oldenburg.de;joerg.luecke@uni-oldenburg.de,4;4;4,4;4;4,Reject,0,6,0,yes,10/27/17,CERN;University of Oldenburg;University of Oldenburg,-1;364;364,-1;1103;1103,5;1,10/27/17,0,0,0,0,0,0,6;12;652,11;10;75,1;2;14,0;0;50,-1;-1
1080,ICLR,2018,Learnability of Learned Neural Networks,Rahul Anand Sharma;Navin Goyal;Monojit Choudhury;Praneeth Netrapalli,t-rahsha@microsoft.com;navingo@microsoft.com;monojitc@microsoft.com;praneeth@microsoft.com,7;6;4,4;4;4,Reject,0,6,0,yes,10/27/17,Microsoft;Microsoft;Microsoft;Microsoft,-1;-1;-1;-1,-1;-1;-1;-1,8,10/27/17,0,0,0,0,0,0,61;1795;1681;3241,27;80;150;72,4;16;22;27,11;236;130;414,-1;-1
1081,ICLR,2018,Compact Neural Networks based on the Multiscale Entanglement Renormalization Ansatz,Andrew Hallam;Edward Grant;Vid Stojevic;Simone Severini;Andrew G. Green,andrew.hallam.10@ucl.ac.uk;edward.grant.16@ucl.ac.uk;vstojevic@gtn.ai;s.severini@ucl.ac.uk;andrew.green@ucl.ac.uk,5;5;4,3;4;4,Reject,0,3,0,yes,10/27/17,University College London;University College London;;University College London;University College London,46;46;-1;46;46,16;16;-1;16;16,,10/27/17,3,0,0,0,8,0,358;764;143;2870;509,22;84;20;273;67,6;16;7;30;12,25;29;1;226;19,-1;-1
1082,ICLR,2018,Online Hyper-Parameter Optimization,Damien Vincent;Sylvain Gelly;Nicolas Le Roux;Olivier Bousquet,damienv@google.com;sylvain.gelly@gmail.com;nicolas@le-roux.name;obousquet@gmail.com,4;5;4,3;3;3,Reject,0,1,0,yes,10/27/17,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,,10/27/17,0,0,0,0,0,0,866;3525;4883;13223,40;112;172;235,11;25;23;39,113;455;536;1950,-1;-1
1083,ICLR,2018,Learning to Teach,Yang Fan;Fei Tian;Tao Qin;Xiang-Yang Li;Tie-Yan Liu,fyabc@mail.ustc.edu.cn;fetia@microsoft.com;taoqin@microsoft.com;tyliu@microsoft.com,8;9;5,4;3;4,Accept (Poster),0,10,0,yes,10/27/17,University of Science and Technology of China;Microsoft;Microsoft;Microsoft,115;-1;-1;-1,132;-1;-1;-1,,10/27/17,48,23,17,5,19,6,582;6393;5498;22919;14727,560;373;289;1269;373,15;38;35;69;54,44;288;608;1301;1752,-1;-1
1084,ICLR,2018,Word2net: Deep Representations of Language,Maja Rudolph;Francisco Ruiz;David Blei,marirudolph@gmail.com;f.ruiz@columbia.edu;david.blei@columbia.edu,5;4;4,5;4;4,Reject,0,3,0,yes,10/27/17,Columbia University;Columbia University;Columbia University,15;15;15,14;14;14,3,10/27/17,0,0,0,0,0,0,378;570;52201,12;31;306,6;13;75,42;67;9229,-1;-1
1085,ICLR,2018,The loss surface and expressivity of deep convolutional neural networks,Quynh Nguyen;Matthias Hein,quynh@cs.uni-saarland.de;hein@cs.uni-saarland.de,4;7;5;6,4;2;2;3,Invite to Workshop Track,0,4,0,yes,10/27/17,Saarland University;Saarland University,90;90,1103;1103,,10/27/17,73,40,6,7,20,5,430;5516,43;171,8;41,32;598,-1;-1
1086,ICLR,2018,Combination of Supervised and Reinforcement Learning For Vision-Based Autonomous Control,Dmitry Kangin;Nicolas Pugeault,d.kangin@exeter.ac.uk;n.pugeault@exeter.ac.uk,4;5;3,5;3;4,Reject,0,4,0,yes,10/27/17,University of Exeter;University of Exeter,364;364,130;130,,10/27/17,1,0,0,0,0,0,155;1330,23;91,8;19,10;113,-1;-1
1087,ICLR,2018,Gating out sensory noise in a spike-based Long Short-Term Memory network,Davide Zambrano;Isabella Pozzi;Roeland Nusselder;Sander Bohte,d.zambrano@cwi.nl;isabella.pozzi@cwi.nl;roeland.nusselder@gmail.com;s.m.bohte@cwi.nl,5;5;4,4;3;4,Reject,0,0,0,yes,10/27/17,Centrum voor Wiskunde en Informatica;Centrum voor Wiskunde en Informatica;;Centrum voor Wiskunde en Informatica,-1;-1;-1;-1,-1;-1;-1;-1,,10/27/17,0,0,0,0,0,0,115;9;31;2246,20;6;5;117,7;2;3;22,8;0;3;234,-1;-1
1088,ICLR,2018,Learning to Infer Graphics Programs from Hand-Drawn Images,Kevin Ellis;Daniel Ritchie;Armando Solar-Lezama;Joshua B. Tenenbaum,ellisk@mit.edu;daniel_richie@brown.edu;asolar@csail.mit.edu;jbt@mit.edu,4;6;4,4;4;2,Reject,0,4,0,yes,10/27/17,Massachusetts Institute of Technology;Brown University;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;62;2;2,5;50;5;5,10,7/30/17,77,34,23,0,0,5,420;948;3896;30421,48;50;129;590,9;13;29;83,23;59;395;2658,-1;-1
1089,ICLR,2018,Federated Learning: Strategies for Improving Communication Efficiency,Jakub Konečný;H. Brendan McMahan;Felix X. Yu;Ananda Theertha Suresh;Dave Bacon;Peter Richtárik,konkey@google.com;mcmahan@google.com;felixyu@google.com;theertha@google.com;dabacon@google.com;peter.richtarik@kaust.edu.sa,5;7;5,3;5;5,Reject,0,4,0,yes,10/27/17,Google;Google;Google;Google;Google;KAUST,-1;-1;-1;-1;-1;124,-1;-1;-1;-1;-1;1103,,10/18/16,597,334,157,7,5,55,2216;5801;2662;1860;2214;5651,36;68;66;64;93;159,16;32;25;19;19;37,231;847;312;200;187;587,-1;-1
1090,ICLR,2018,Convolutional Sequence Modeling Revisited,Shaojie Bai;J. Zico Kolter;Vladlen Koltun,shaojieb@cs.cmu.edu;zkolter@cs.cmu.edu;vkoltun@gmail.com,8;5;4,4;4;3,Invite to Workshop Track,1,9,4,yes,10/27/17,Carnegie Mellon University;Carnegie Mellon University;Intel,1;1;-1,24;24;-1,,10/27/17,12,7,5,0,0,3,697;7544;17520,15;104;188,6;35;62,116;1052;2489,-1;-1
1091,ICLR,2018,Searching for Activation Functions,Prajit Ramachandran;Barret Zoph;Quoc V. Le,prajitram@gmail.com;barretzoph@google.com;qvl@google.com,4;5;7,4;5;5,Invite to Workshop Track,5,8,0,yes,10/27/17,"University of Illinois, Urbana Champaign;Google;Google",3;-1;-1,37;-1;-1,,10/16/17,502,173,171,8,0,63,1052;6924;47503,14;30;192,8;20;79,135;1299;5974,-1;-1
1092,ICLR,2018,On Convergence and Stability of GANs,Naveen Kodali;James Hays;Jacob Abernethy;Zsolt Kira,nkodali3@gatech.edu;hays@gatech.edu;prof@gatech.edu;zkira@gatech.edu,5;4;3,2;5;3,Reject,17,6,0,yes,10/27/17,Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology,13;13;13;13,33;33;33;33,5,10/27/17,169,82,45,4,82,18,212;17359;1844;1141,3;98;96;69,2;38;22;16,25;2976;234;163,-1;-1
1093,ICLR,2018,Training Autoencoders by Alternating Minimization,Sneha Kudugunta;Adepu Shankar;Surya Chavali;Vineeth Balasubramanian;Purushottam Kar,cs14btech11020@iith.ac.in;cs14resch11001@iith.ac.in;cs13b1028@iith.ac.in;vineethnb@iith.ac.in;purushot@cse.iitk.ac.in,6;4;7,4;4;5,Reject,0,3,0,yes,10/27/17,Indian Institute of Technology Hyderabad;Indian Institute of Technology Hyderabad;Indian Institute of Technology Hyderabad;Indian Institute of Technology Hyderabad;IIT Kanpur,210;210;210;210;139,1103;1103;1103;1103;578,9,10/27/17,0,0,0,0,0,0,84;0;1;1024;1369,6;1;3;116;51,2;0;1;15;15,6;0;0;132;241,-1;-1
1094,ICLR,2018,Learning temporal evolution of probability distribution with Recurrent Neural Network,Kyongmin Yeo;Igor Melnyk;Nam Nguyen;Eun Kyung Lee,kyeo@us.ibm.com;igor.melnyk@ibm.com;nnguyen@us.ibm.com;eunkyung.lee@us.ibm.com,6;5;6,2;4;4,Reject,0,3,0,yes,10/27/17,International Business Machines;International Business Machines;International Business Machines;International Business Machines,-1;-1;-1;-1,-1;-1;-1;-1,,10/27/17,2,0,1,0,0,0,467;248;11;459,60;33;10;62,13;10;2;12,41;17;0;23,-1;-1
1095,ICLR,2018,LEARNING TO ORGANIZE KNOWLEDGE WITH N-GRAM MACHINES,Fan Yang;Jiazhong Nie;William W. Cohen;Ni Lao,fanyang1@cs.cmu.edu;niejiazhong@google.com;wcohen@cs.cmu.edu;nlao@google.com,4;5;4,4;4;3,Invite to Workshop Track,0,3,0,yes,10/27/17,Carnegie Mellon University;Google;Carnegie Mellon University;Google,1;-1;1;-1,24;-1;24;-1,3,10/27/17,2,1,1,0,0,0,3795;120;22375;2895,207;12;423;50,31;5;68;17,41;10;2622;383,-1;-1
1096,ICLR,2018,Stabilizing GAN Training with Multiple Random Projections,Behnam Neyshabur;Srinadh Bhojanapalli;Ayan Chakrabarti,bneyshabur@ttic.edu;srinadh@ttic.edu;ayan@wustl.edu,5;3;8,4;5;4,Reject,0,4,0,yes,10/27/17,"Toyota Technological Institute at Chicago;Toyota Technological Institute at Chicago;Washington University, St. Louis",-1;-1;104,-1;-1;50,5;4,5/22/17,35,25,6,0,0,4,2366;1590;1355,27;28;47,18;16;17,313;193;155,-1;-1
1097,ICLR,2018,Meta-Learning and Universality: Deep Representations and Gradient Descent can Approximate any Learning Algorithm,Chelsea Finn;Sergey Levine,cbfinn@eecs.berkeley.edu;svlevine@eecs.berkeley.edu,6;6;7,3;1;1,Accept (Poster),0,6,0,yes,10/27/17,University of California Berkeley;University of California Berkeley,5;5,18;18,6,10/27/17,89,35,22,2,39,2,7689;24386,98;309,33;73,1026;3167,-1;-1
1098,ICLR,2018,TCAV: Relative concept importance testing with Linear Concept Activation Vectors,Been Kim;Justin Gilmer;Martin Wattenberg;Fernanda Viégas,beenkim@google.com;viegas@google.com;wattenberg@google.com;gilmer@google.com,4;4;5;3,4;3;2;5,Reject,0,6,0,yes,10/27/17,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,7,10/27/17,21,15,2,0,0,4,3068;3306;16568;13591,51;45;100;90,21;18;43;35,323;457;1775;1501,-1;-1
1099,ICLR,2018,Shifting Mean Activation Towards Zero with Bipolar Activation Functions,Lars Hiller Eidnes;Arild Nøkland,larseidnes@gmail.com;arild.nokland@gmail.com,4;5;5,4;5;3,Invite to Workshop Track,0,4,2,yes,10/27/17,Itema;Norges teknisk-naturvitenskapelige universitet,-1;-1,-1;-1,3,9/12/17,8,3,3,0,89,0,8;168,1;4,1;4,0;24,-1;-1
1100,ICLR,2018,Continuous-Time Flows for Efficient Inference and Density Estimation,Changyou Chen;Chunyuan Li;Liqun Chen;Wenlin Wang;Yunchen Pu;Lawrence Carin,cchangyou@gmail.com;chunyuan.li@duke.edu;lc267@duke.edu;wenlin.wang@duke.edu;yunchen.pu@duke.edu;lcarin@duke.edu,3;6;6,3;4;4,Reject,0,3,0,yes,10/27/17,"State University of New York, Buffalo;Duke University;Duke University;Duke University;Duke University;Duke University",85;46;46;46;46;46,270;17;17;17;17;17,5;4,9/4/17,21,11,8,3,6,4,1797;2011;250;838;1135;19178,94;80;34;41;44;819,24;25;7;15;14;65,244;243;11;72;118;1986,-1;-1
1101,ICLR,2018,Large-scale Cloze Test Dataset Designed by Teachers,Qizhe Xie;Guokun Lai;Zihang Dai;Eduard Hovy,qizhex@gmail.com;guokun@cs.cmu.edu;zander.dai@gmail.com;hovy@cs.cmu.edu,4;7;4,4;4;4,Reject,0,4,0,yes,10/27/17,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1,24;24;24;24,,10/27/17,26,16,11,0,22,8,713;784;2364;23512,18;14;27;580,8;6;14;76,157;149;425;2471,-1;-1
1102,ICLR,2018,Deep Learning is Robust to Massive Label Noise,David Rolnick;Andreas Veit;Serge Belongie;Nir Shavit,drolnick@mit.edu;av443@cornell.edu;sjb344@cornell.edu;shanir@csail.mit.edu,5;4;5,4;5;5,Reject,2,4,0,yes,10/27/17,Massachusetts Institute of Technology;Cornell University;Cornell University;Massachusetts Institute of Technology,2;7;7;2,5;19;19;5,,5/30/17,176,100,9,7,115,12,615;1671;48487;9355,37;28;281;223,10;17;80;48,39;207;7615;1344,-1;-1
1103,ICLR,2018,Unsupervised Hierarchical Video Prediction,Nevan Wichers;Dumitru Erhan;Honglak Lee,wichersn@google.com;dumitru@google.com;honglak@google.com,4;4;4,4;4;4,Reject,0,2,0,yes,10/27/17,Google;Google;Google,-1;-1;-1,-1;-1;-1,,10/27/17,0,0,0,0,0,0,42;41676;23861,7;60;166,1;31;60,6;5966;2806,-1;-1
1104,ICLR,2018,One-shot and few-shot learning of word embeddings,Andrew Kyle Lampinen;James Lloyd McClelland,lampinen@stanford.edu;mcclelland@stanford.edu,4;3;4,4;4;4,Reject,3,3,0,yes,10/27/17,Stanford University;Stanford University,4;4,3;3,3;6,10/27/17,9,4,2,0,19,0,91;41502,22;423,5;81,6;3261,-1;-1
1105,ICLR,2018,Unbiasing Truncated Backpropagation Through Time,Corentin Tallec;Yann Ollivier,corentin.tallec@polytechnique.edu;yann@yann-ollivier.org,6;5;5,3;4;4,Reject,0,0,0,yes,10/27/17,Ecole polytechnique;Facebook,468;-1,115;-1,3;10,5/23/17,24,12,13,1,20,5,149;1568,13;131,5;20,20;191,-1;-1
1106,ICLR,2018,Joint autoencoders: a flexible meta-learning framework,Baruch Epstein;Ron Meir;Tomer Michaeli,baruch.epstein@gmail.com;rmeir@ee.technion.ac.il;tomer.m@ee.technion.ac.il,4;5;5,4;3;4,Reject,0,4,0,yes,10/27/17,Technion;Technion;Technion,24;24;24,327;327;327,6,10/27/17,2,2,2,0,0,0,21;3451;1313,10;145;76,2;29;17,1;404;130,-1;-1
1107,ICLR,2018,On the Use of Word Embeddings Alone to Represent Natural Language Sequences,Dinghan Shen;Guoyin Wang;Wenlin Wang;Martin Renqiang Min;Qinliang Su;Yizhe Zhang;Ricardo Henao;Lawrence Carin,dinghan.shen@duke.edu;guoyin.wang@duke.edu;wenlin.wang@duke.edu;renqiang@nec-labs.com;qinliang.su@duke.edu;yizhe.zhang@duke.edu;ricardo.henao@duke.edu;lcarin@duke.edu,7;5;6,4;4;5,Reject,0,14,0,yes,10/27/17,Duke University;Duke University;Duke University;NEC-Labs;Duke University;Duke University;Duke University;Duke University,46;46;46;-1;46;46;46;46,17;17;17;-1;17;17;17;17,3,10/27/17,4,2,3,1,0,2,852;1232;838;875;331;27;2036;19178,34;72;41;58;26;13;124;819,13;14;15;14;8;3;24;65,103;50;72;103;29;2;216;1986,-1;-1
1108,ICLR,2018,Byte-Level Recursive Convolutional Auto-Encoder for Text,Xiang Zhang;Yann LeCun,xiang@cs.nyu.edu;yann@cs.nyu.edu,7;5;5,4;3;5,Reject,0,0,1,yes,10/27/17,New York University;New York University,26;26,27;27,,10/27/17,3,0,1,0,6,0,8285;91110,334;345,26;107,740;10302,-1;-1
1109,ICLR,2018,Phase Conductor on Multi-layered Attentions for Machine Comprehension,Rui Liu;Wei Wei;Weiguang Mao;Maria Chikina,ult.rui.liu@gmail.com;weiwei@cs.cmu.edu;mwg10.thu@gmail.com;mchikina@gmail.com,8;5;5,3;5;4,Reject,0,4,0,yes,10/27/17,Carnegie Mellon University;Carnegie Mellon University;University of Pittsburgh;,1;1;78;-1,24;24;100;-1,3,10/27/17,22,6,6,1,4,0,7132;5445;168;755,618;549;11;53,42;38;4;14,425;326;2;47,-1;-1
1110,ICLR,2018,Gaussian Prototypical Networks for Few-Shot Learning on Omniglot,Stanislav Fort,sfort1@stanford.edu,4;3;3,4;4;4,Reject,0,0,0,yes,10/27/17,Stanford University,4,3,6,8/9/17,15,7,1,1,5,1,93,11,7,6,-1
1111,ICLR,2018,Anomaly Detection with Generative Adversarial Networks,Lucas Deecke;Robert Vandermeulen;Lukas Ruff;Stephan Mandt;Marius Kloft,ldeecke@gmail.com;vandermeulen@cs.uni-kl.de;contact@lukasruff.com;stephan.mandt@disneyresearch.com;kloft@cs.uni-kl.de,4;6;4,5;4;4,Reject,0,3,0,yes,10/27/17,"Freie Universität Berlin;TU Kaiserslautern;Hasso Plattner Institute;Disney Research, Disney;TU Kaiserslautern",-1;139;-1;-1;139,-1;452;-1;-1;452,5;4,10/27/17,41,16,16,0,0,9,211;236;227;1359;2286,7;14;8;67;98,4;5;3;17;23,47;53;54;117;275,-1;-1
1112,ICLR,2018,Learning Gaussian Policies from Smoothed Action Value Functions,Ofir Nachum;Mohammad Norouzi;George Tucker;Dale Schuurmans,ofirnachum@google.com;mnorouzi@google.com;gjt@google.com;schuurmans@google.com,6;6;5,4;4;3,Reject,0,3,0,yes,10/27/17,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,,10/27/17,12,8,3,0,18,0,1038;7796;2809;6071,41;125;75;247,15;30;21;41,155;992;305;636,-1;-1
1113,ICLR,2018,Spectral Graph Wavelets for Structural Role Similarity in Networks,Claire Donnat;Marinka Zitnik;David Hallac;Jure Leskovec,cdonnat@stanford.edu;marinka@cs.stanford.edu;hallac@stanford.edu;jure@cs.stanford.edu,5;5;3,5;4;4,Reject,0,3,0,yes,10/27/17,Stanford University;Stanford University;Stanford University;Stanford University,4;4;4;4,3;3;3;3,1;10,10/27/17,12,8,4,0,0,1,155;1878;521;46769,13;89;22;298,7;19;11;92,14;189;78;5996,-1;-1
1114,ICLR,2018,Adversarial Spheres,Justin Gilmer;Luke Metz;Fartash Faghri;Sam Schoenholz;Maithra Raghu;Martin Wattenberg;Ian Goodfellow,gilmer@google.com;lmetz@google.com;fartash.faghri@google.com;schsam@google.com;maithra@google.com;goodfellow@google.com,4;5;3,4;3;3,Invite to Workshop Track,0,2,0,yes,10/27/17,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,4;1;2,10/27/17,589,56,9,4,471,14,3306;7178;800;3006;1075;16568;73755,45;25;11;70;27;100;292,18;10;5;21;10;43;56,457;1200;133;376;144;1775;10486,-1;-1
1115,ICLR,2018,Modifying memories in a Recurrent Neural Network Unit,Vlad Velici;Adam Prügel-Bennett,vsv1g12@soton.ac.uk;apb@soton.ac.uk,4;4;3,3;3;4,Reject,1,1,0,yes,10/27/17,University of Southampton;University of Southampton,181;181,126;126,,10/27/17,0,0,0,0,0,0,0;2097,2;137,0;25,0;135,-1;-1
1116,ICLR,2018,Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,Tuomas Haarnoja;Aurick Zhou;Pieter Abbeel;Sergey Levine,haarnoja@berkeley.edu;azhou42@berkeley.edu;pabbeel@cs.berkeley.edu;svlevine@eecs.berkeley.edu,3;7;5,4;4;4,Invite to Workshop Track,0,4,0,yes,10/27/17,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5,18;18;18;18,,10/27/17,648,333,338,18,47,200,1457;965;36468;24386,22;6;433;309,7;5;94;73,346;261;4391;3167,-1;-1
1117,ICLR,2018,Challenges in Disentangling Independent Factors of Variation,Attila  Szabo;Qiyang  Hu;Tiziano  Portenier;Matthias  Zwicker;Paolo  Favaro,szabo@inf.unibe.ch;hu@inf.unibe.ch;portenier@inf.unibe.ch;zwicker@inf.unibe.ch;paolo.favaro@inf.unibe.ch,6;5;5,4;3;3,Invite to Workshop Track,0,3,0,yes,10/27/17,University of Bern;University of Bern;University of Bern;University of Bern;University of Bern,364;364;364;364;364,105;105;105;105;105,5;4;1,10/27/17,29,20,12,0,0,4,2074;578;129;5501;4139,156;55;14;161;108,23;14;5;40;33,171;14;10;406;427,-1;-1
1118,ICLR,2018,Ground-Truth Adversarial Examples,Nicholas Carlini;Guy Katz;Clark Barrett;David L. Dill,nicholas@carlini.com;katz911@gmail.com;barrett@cs.stanford.edu;dill@cs.stanford.edu,5;4;6,4;4;3,Reject,0,0,0,yes,10/27/17,University of California Berkeley;Hebrew University of Jerusalem;Stanford University;Stanford University,5;62;4;4,18;205;3;3,4,9/29/17,59,21,11,2,0,6,5765;1322;5050;21403,44;68;140;299,20;17;30;60,989;118;591;2727,-1;-1
1119,ICLR,2018,Learning Parsimonious Deep Feed-forward Networks,Zhourong Chen;Xiaopeng Li;Nevin L. Zhang,zchenbb@cse.ust.hk;xlibo@cse.ust.hk;lzhang@cse.ust.hk,5;4;5,5;2;2,Reject,0,5,0,yes,10/27/17,The Hong Kong University of Science and Technology;The Hong Kong University of Science and Technology;The Hong Kong University of Science and Technology,40;40;40,44;44;44,,10/27/17,0,0,0,0,0,0,1855;623;2671,15;83;143,7;14;24,327;31;189,-1;-1
1120,ICLR,2018,Learning Deep Generative Models of Graphs,Yujia Li;Oriol Vinyals;Chris Dyer;Razvan Pascanu;Peter Battaglia,yujiali@google.com;vinyals@google.com;cdyer@google.com;razp@google.com;peterbattaglia@google.com,5;6;6,3;3;4,Invite to Workshop Track,2,7,0,yes,10/27/17,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,5;10,10/27/17,195,98,61,6,84,25,2918;52009;21134;16667;4482,54;121;231;101;88,15;55;59;46;29,396;6497;3164;1666;417,-1;-1
1121,ICLR,2018,An information-theoretic analysis of deep latent-variable models,Alex Alemi;Ben Poole;Ian Fischer;Josh Dillon;Rif A. Saurus;Kevin Murphy,alemi@google.com;poole@cs.stanford.edu;iansf@google.com;jvdillon@google.com;rif@google.com;kpmurphy@google.com,5;7;5,4;5;5,Reject,1,5,0,yes,10/27/17,Google;Stanford University;Google;Google;Google;Google,-1;4;-1;-1;-1;-1,-1;3;-1;-1;-1;-1,5;1,10/27/17,51,14,3,0,0,1,1257;4164;2635;975;2474;15925,53;41;16;28;31;83,14;19;12;13;15;41,185;686;356;165;420;2278,-1;-1
1122,ICLR,2018,Trust-PCL: An Off-Policy Trust Region Method for Continuous Control,Ofir Nachum;Mohammad Norouzi;Kelvin Xu;Dale Schuurmans,ofirnachum@google.com;mnorouzi@google.com;iamkelvinxu@gmail.com;schuurmans@google.com,6;5;5,4;4;1,Accept (Poster),0,5,0,yes,10/27/17,Google;Google;University of California Berkeley;Google,-1;-1;5;-1,-1;-1;18;-1,,7/6/17,52,28,25,1,52,9,1038;7796;7305;6071,41;125;19;247,15;30;11;41,155;992;754;636,-1;-1
1123,ICLR,2018,Modular Continual Learning in a Unified Visual Environment,Kevin T. Feigelis;Blue Sheffer;Daniel L. K. Yamins,feigelis@stanford.edu;bsheffer@stanford.edu;yamins@stanford.edu,6;8;8,2;3;2,Accept (Poster),0,5,0,yes,10/27/17,Stanford University;Stanford University;Stanford University,4;4;4,3;3;3,,10/27/17,31,0,0,0,31,0,0;5;71,4;2;15,0;1;5,0;0;4,-1;-1
1124,ICLR,2018,PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples,Yang Song;Taesup Kim;Sebastian Nowozin;Stefano Ermon;Nate Kushman,yangsong@cs.stanford.edu;taesup.kim@umontreal.ca;sebastian.nowozin@microsoft.com;ermon@cs.stanford.edu;nkushman@microsoft.com,7;7;7,4;4;4,Accept (Poster),0,3,0,yes,10/27/17,Stanford University;University of Montreal;Microsoft;Stanford University;Microsoft,4;124;-1;4;-1,3;108;-1;3;-1,4,10/27/17,281,169,71,5,12,34,7254;734;6872;4821;1488,348;21;134;203;29,38;9;39;31;17,659;89;931;646;202,-1;-1
1125,ICLR,2018,BinaryFlex: On-the-Fly Kernel Generation in Binary Convolutional Networks,Vincent W.-S. Tseng;Sourav Bhattachary;Javier Fernández Marqués;Milad Alizadeh;Catherine Tong;Nicholas Donald Lane,wt262@cornell.edu;sourav.bhattacharya@nokia-bell-labs.com;javier.fernandezmarques@cs.ox.ac.uk;milad.alizadeh@cs.ox.ac.uk;eu.tong@cs.ox.ac.uk;nicholas.lane@cs.ox.uk,5;5;3,3;3;4,Reject,0,4,0,yes,10/27/17,Cornell University;Nokia Bell Labs;University of Oxford;University of Oxford;University of Oxford;,7;-1;51;51;51;-1,19;-1;1;1;1;-1,2,10/27/17,0,0,0,0,0,0,113;0;0;82;112;10101,10;1;1;27;19;167,4;0;0;5;4;44,6;0;0;12;11;702,-1;-1
1126,ICLR,2018,Neural Networks for irregularly observed continuous-time Stochastic Processes,Francois W. Belletti;Alexander Ku;Joseph E. Gonzalez,francois.belletti@berkeley.edu;alexku@berkeley.edu;jegonzal@berkeley.edu,5;5;2,5;4;3,Reject,0,4,0,yes,10/27/17,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,18;18;18,,10/27/17,1,0,1,0,0,0,270;358;1659,31;10;53,8;6;7,17;25;257,-1;-1
1127,ICLR,2018,Learning to diagnose from scratch by exploiting dependencies among labels,Li Yao;Eric Poblenz;Dmitry Dagunts;Ben Covington;Devon Bernard;Kevin Lyman,li@enlitic.com;eric@enlitic.com;dmitry@enlitic.com;ben@enlitic.com;devon@entlic.com;kevin@enlitic.com,6;6;6,3;4;3,Reject,0,3,0,yes,10/27/17,Enlitic;Enlitic;Enlitic;Enlitic;Entlic;Enlitic,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,,10/27/17,113,45,38,10,0,13,5643;143;110;155;110;303,671;4;1;5;1;10,35;2;1;3;1;6,490;20;13;20;13;36,-1;-1
1128,ICLR,2018,Towards Provable Control for Unknown Linear Dynamical Systems,Sanjeev Arora;Elad Hazan;Holden Lee;Karan Singh;Cyril Zhang;Yi Zhang,arora@cs.princeton.edu;ehazan@cs.princeton.edu;holdenl@princeton.edu;karans@cs.princeton.edu;cyril.zhang@cs.princeton.edu;y.zhang@cs.princeton.edu,4;7;5,3;3;4,Invite to Workshop Track,0,4,0,yes,10/27/17,Princeton University;Princeton University;Princeton University;Princeton University;Princeton University;Princeton University,31;31;31;31;31;31,7;7;7;7;7;7,,10/27/17,22,14,3,0,0,0,15912;11863;158;4833;198;1448,349;148;23;261;18;433,61;43;6;39;6;17,1795;1996;7;345;20;77,-1;-1
1129,ICLR,2018,Depth separation and weight-width trade-offs for sigmoidal neural networks,Amit Deshpande;Navin Goyal;Sushrut Karmalkar,amitdesh@microsoft.com;navingo@microsoft.com;sushrutk@cs.utexas.edu,6;5;3,4;4;5,Reject,0,4,0,yes,10/27/17,"Microsoft;Microsoft;University of Texas, Austin",-1;-1;21,-1;-1;49,,10/27/17,0,0,0,0,0,0,1870;1795;62,33;80;14,15;16;4,171;236;4,-1;-1
1130,ICLR,2018,Time Limits in Reinforcement Learning,Fabio Pardo;Arash Tavakoli;Vitaly Levdik;Petar Kormushev,f.pardo@imperial.ac.uk;a.tavakoli@imperial.ac.uk;v.levdik@imperial.ac.uk;p.kormushev@imperial.ac.uk,5;4;4,4;5;4,Reject,0,7,0,yes,10/27/17,Imperial College London;Imperial College London;Imperial College London;Imperial College London,74;74;74;74,8;8;8;8,10,10/27/17,23,7,7,0,14,1,53;65;30;1262,6;9;6;102,2;4;2;19,5;6;1;60,-1;-1
1131,ICLR,2018,Learning Independent Features with Adversarial Nets for Non-linear ICA,Philemon Brakel;Yoshua Bengio,pbpop3@gmail.com;yoshua.bengio@umontreal.ca,5;3;6,5;5;3,Reject,0,4,0,yes,10/27/17,Google;University of Montreal,-1;124,-1;108,4,10/13/17,33,15,18,0,2,8,1721;201719,21;807,16;147,161;23989,-1;-1
1132,ICLR,2018,Discovering the mechanics of hidden neurons,Simon Carbonnelle;Christophe De Vleeschouwer,simon.carbonnelle@uclouvain.be;christophe.devleeschouwer@uclouvain.be,7;4;5,4;4;4,Reject,1,7,0,yes,10/27/17,UCL;UCL,291;291,16;16,,10/27/17,1,1,0,0,0,0,5;2372,7;168,1;26,1;184,-1;-1
1133,ICLR,2018,DNN Model Compression Under Accuracy Constraints,Soroosh Khoram;Jing Li,khoram@wisc.edu;jli@ece.wisc.edu,4;3;3,3;3;5,Reject,0,0,0,yes,10/27/17,University of Southern California;University of Southern California,31;31,66;66,,10/27/17,2,2,1,0,0,0,88;1226,10;283,5;18,13;76,-1;-1
1134,ICLR,2018,Using Deep Reinforcement Learning to Generate Rationales for Molecules,Benson Chen;Connor Coley;Regina Barzilay;Tommi Jaakkola,bensonc@mit.edu;ccoley@mit.edu;regina@csail.mit.edu;tommi@csail.mit.edu,5;5;5,4;4;4,Reject,0,3,0,yes,10/27/17,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2,5;5;5;5,10,10/27/17,0,0,0,0,0,0,78;787;11910;21902,19;46;233;292,4;14;55;69,3;36;1212;2317,-1;-1
1135,ICLR,2018,Fast Node Embeddings: Learning Ego-Centric Representations,Tiago Pimentel;Adriano Veloso;Nivio Ziviani,tpimentel@dcc.ufmg.br;adrianov@dcc.ufmg.br;nivio@dcc.ufmg.br,5;6;4,4;4;5,Invite to Workshop Track,0,4,0,yes,10/27/17,Universidade Federal de Minas Gerais;Universidade Federal de Minas Gerais;Universidade Federal de Minas Gerais,468;468;468,715;715;715,3;10,10/27/17,6,3,2,0,0,0,39;1736;2964,14;124;169,4;23;30,3;89;203,-1;-1
1136,ICLR,2018,HybridNet: A Hybrid Neural Architecture to Speed-up Autoregressive  Models,Yanqi Zhou;Wei Ping;Sercan Arik;Kainan Peng;Greg Diamos,zhouyanqi@baidu.com;pingwei01@baidu.com;sercanarik@baidu.com;pengkainan@baidu.com;gregdiamos@baidu.com,6;4;4,5;5;5,Reject,0,3,0,yes,10/27/17,Baidu;Baidu;Baidu;Baidu;Baidu,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,,10/27/17,0,0,0,0,0,0,858;973;16;569;2547,33;208;4;12;16,13;12;2;7;7,86;110;1;73;246,-1;-1
1137,ICLR,2018,Sparse Regularized Deep Neural Networks For Efficient Embedded Learning,Jia Bi,jb4e14@soton.ac.uk,4;4;2,4;5;3,Reject,0,3,0,yes,10/27/17,University of Southampton,181,126,9,10/27/17,0,0,0,0,0,0,69,55,4,3,-1
1138,ICLR,2018,GANITE: Estimation of Individualized Treatment Effects using Generative Adversarial Nets,Jinsung Yoon;James Jordon;Mihaela van der Schaar,jsyoon0823@gmail.com;james.jordon@hertford.ox.ac.uk;mihaela.vanderschaar@oxford-man.ox.ac.uk,6;6;6,4;3;3,Accept (Poster),0,3,0,yes,10/27/17,"University of California, Los Angeles;University of Oxford;University of Oxford",20;51;51,15;1;1,5;4,10/27/17,44,18,19,1,0,6,536;240;8628,55;17;643,13;8;42,57;43;539,-1;-1
1139,ICLR,2018,Network Signatures from Image Representation of Adjacency Matrices: Deep/Transfer Learning for Subgraph Classification,Kshiteesh Hegde;Malik Magdon-Ismail;Ram Ramanathan;Bishal Thapa,hegdek2@rpi.edu;magdon@rpi.edu;ram@gotenna.com;bishal.thapa@raytheon.com,3;6;6,3;3;3,Reject,0,6,0,yes,10/27/17,Rensselaer Polytechnic Institute;Rensselaer Polytechnic Institute;Gotenna;Raytheon,153;153;-1;-1,304;304;-1;-1,6;10,10/27/17,6,4,2,0,12,0,33;3375;5393;508,7;211;111;31,3;29;29;9,2;262;411;38,-1;-1
1140,ICLR,2018,SCAN: Learning Hierarchical Compositional Visual Concepts,Irina Higgins;Nicolas Sonnerat;Loic Matthey;Arka Pal;Christopher P Burgess;Matko Bošnjak;Murray Shanahan;Matthew Botvinick;Demis Hassabis;Alexander Lerchner,irinah@google.com;sonnerat@google.com;lmatthey@google.com;arkap@google.com;cpburgess@google.com;matko@google.com;mshanahan@google.com;botvinick@google.com;demishassabis@google.com;lerchner@google.com,5;6;7,4;4;4,Accept (Poster),0,6,0,yes,10/27/17,Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,5,7/11/17,39,24,6,0,0,2,1975;487;307;1763;513;3464;4799;13428;23857;2060,25;9;9;11;18;24;166;146;55;23,11;6;6;10;9;11;38;45;31;15,294;43;28;272;50;476;455;1413;2564;310,-1;-1
1141,ICLR,2018,Variance-based Gradient Compression for Efficient Distributed Deep Learning,Yusuke Tsuzuku;Hiroto Imachi;Takuya Akiba,tsuzuku@ms.k.u-tokyo.ac.jp;imachi@preferred.jp;akiba@preferred.jp,6;4;7,4;4;4,Invite to Workshop Track,0,7,1,yes,10/27/17,"The University of Tokyo;Preferred Networks, Inc.;Preferred Networks, Inc.",52;-1;-1,45;-1;-1,1,10/27/17,16,10,4,0,6,0,107;70;1258,4;14;61,4;5;21,17;1;191,-1;-1
1142,ICLR,2018,Learning to Encode Text as Human-Readable Summaries using Generative Adversarial Networks,Yau-Shian Wang;Hung-Yi Lee,king6101@gmail.com;tlkagkb93901106@gmail.com,5;4;6,4;4;4,Reject,0,5,0,yes,10/27/17,National Taiwan University;,85;-1,197;-1,,10/27/17,24,13,7,2,8,7,43;1094,4;134,3;17,8;73,-1;-1
1143,ICLR,2018,Subspace Network: Deep Multi-Task Censored Regression for Modeling Neurodegenerative Diseases,Mengying Sun;Inci M. Baytas;Zhangyang Wang;Jiayu Zhou,sunmeng2@msu.edu;baytasin@msu.edu;atlaswang@tamu.edu;jiayuz@msu.edu,4;5;5,5;3;4,Reject,0,7,0,yes,10/27/17,SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;Texas A&M;SUN YAT-SEN UNIVERSITY,468;468;42;468,352;352;160;352,5,10/27/17,2,1,1,0,11,0,55;200;2827;2185,13;12;168;126,4;5;27;22,3;24;363;221,-1;-1
1144,ICLR,2018,UCB EXPLORATION VIA Q-ENSEMBLES,Richard Y. Chen;Szymon Sidor;Pieter Abbeel;John Schulman,richardchen@openai.com;szymon@openai.com;pabbeel@cs.berkeley.edu;joschu@openai.com,6;7;5,5;4;3,Reject,0,12,0,yes,10/27/17,OpenAI;OpenAI;University of California Berkeley;OpenAI,-1;-1;5;-1,-1;-1;18;-1,,6/5/17,34,12,17,0,14,3,770;935;36468;14794,21;15;433;55,10;7;94;31,80;67;4391;2466,-1;-1
1145,ICLR,2018,Convergence rate of sign stochastic gradient descent for non-convex functions,Jeremy Bernstein;Kamyar Azizzadenesheli;Yu-Xiang Wang;Anima Anandkumar,bernstein@caltech.edu;kazizzad@uci.edu;yuxiangw@cs.cmu.edu;animakumar@gmail.com,4;4;5,4;4;5,Reject,0,15,2,yes,10/27/17,"California Institute of Technology;University of California, Irvine;Carnegie Mellon University;University of California-Irvine",139;36;1;36,3;99;24;99,1;9,10/27/17,4,0,3,0,0,1,870;670;1618;5315,263;38;59;187,10;10;23;38,116;96;218;744,-1;-1
1146,ICLR,2018,Empirical Analysis of the Hessian of Over-Parametrized Neural Networks,Levent Sagun;Utku Evci;V. Ugur Guney;Yann Dauphin;Leon Bottou,leventsagun@gmail.com;ue225@nyu.edu;vug@fb.com;yann@dauphin.io;leonb@fb.com,5;4;5,2;4;4,Invite to Workshop Track,0,7,0,yes,10/27/17,CEA;New York University;Facebook;Facebook;Facebook,-1;26;-1;-1;-1,-1;27;-1;-1;-1,9,6/14/17,128,75,21,4,7,15,948;147;322;8489;47558,22;8;5;43;167,12;3;3;27;58,128;17;60;1020;7054,-1;-1
1147,ICLR,2018,CNNs as Inverse Problem Solvers and Double Network Superresolution,Cem TARHAN;Gözde BOZDAĞI AKAR,cemtarhan@aselsan.com.tr;bozdagi@metu.edu.tr,6;3;4,2;5;4,Reject,0,3,0,yes,10/27/17,METU;METU,210;210,654;654,,10/27/17,0,0,0,0,0,0,0;2,2;3,0;1,0;0,-1;-1
1148,ICLR,2018,IVE-GAN: Invariant Encoding Generative Adversarial Networks,Robin Winter;Djork-Arnè Clevert,robin.winter@bayer.com;djork-arne.clevert@bayer.com,5;4;5,4;5;4,Reject,2,0,0,yes,10/27/17,Bayer Ag;Bayer Ag,-1;-1,-1;-1,5;4,10/27/17,14,0,0,0,14,0,720;3691,26;31,11;13,39;419,-1;-1
1149,ICLR,2018,Composable Planning with Attributes,Amy Zhang;Adam Lerer;Sainbayar Sukhbaatar;Rob Fergus;Arthur Szlam,amyzhang@fb.com;alerer@fb.com;sainbar@cs.nyu.edu;fergus@cs.nyu.edu;aszlam@fb.com,5;4;7,4;5;3,Reject,0,5,0,yes,10/27/17,Facebook;Facebook;New York University;New York University;Facebook,-1;-1;26;26;-1,-1;-1;27;27;-1,10,10/27/17,21,11,6,1,14,2,771;8009;2700;51660;8697,47;27;21;127;86,12;15;13;61;32,94;996;300;6416;921,-1;-1
1150,ICLR,2018,How do deep convolutional neural networks learn from raw audio waveforms?,Yuan Gong;Christian Poellabauer,ygong1@nd.edu;cpoellab@nd.edu,3;3;2,4;5;5,Reject,0,7,0,yes,10/27/17,University of Notre Dame;University of Notre Dame,124;124,150;150,,10/27/17,7,2,1,1,0,1,80;1562,11;190,5;20,11;72,-1;-1
1151,ICLR,2018,Relevance of Unsupervised Metrics in Task-Oriented Dialogue for Evaluating Natural Language Generation,Shikhar Sharma;Layla El Asri;Hannes Schulz;Jeremie Zumer,shikhar.sharma@microsoft.com;layla.elasri@microsoft.com;hannes.schulz@microsoft.com;jeremie_zumer@hotmail.com,4;5;5,4;4;3,Reject,0,3,0,yes,10/27/17,Microsoft;Microsoft;Microsoft;,-1;-1;-1;-1,-1;-1;-1;-1,3,6/29/17,70,21,36,3,6,5,741;547;1115;408,15;25;65;7,9;12;17;4,74;46;86;34,-1;-1
1152,ICLR,2018,Evaluation of generative networks through their data augmentation capacity,Timothée Lesort;Florian Bordes;Jean-Francois Goudou;David Filliat,t.lesort@gmail.com;florian.bordes@umontreal.ca;jean-francois.goudou@thalesgroup.com;david.filliat@ensta-paristech.fr,3;3;5,5;5;3,Reject,0,3,0,yes,10/27/17,ENSTA ParisTech;University of Montreal;Thalesgroup;ENSTA ParisTech,468;124;-1;468,1103;108;-1;1103,5;4,10/27/17,2,0,0,0,0,0,226;29;92;2325,18;5;4;134,8;2;2;23,16;1;5;158,-1;-1
1153,ICLR,2018,FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling,Jie Chen;Tengfei Ma;Cao Xiao,chenjie@us.ibm.com;tengfei.ma1@ibm.com;cxiao@us.ibm.com,6;7;7;8,4;2;4;4,Accept (Poster),2,20,1,yes,10/27/17,International Business Machines;International Business Machines;International Business Machines,-1;-1;-1,-1;-1;-1,10,10/27/17,231,103,108,7,0,45,29015;730;903,2993;35;82,70;12;13,1892;85;111,-1;-1
1154,ICLR,2018,Interpretable and Pedagogical Examples,Smitha Milli;Pieter Abbeel;Igor Mordatch,smilli@berkeley.edu;pabbeel@cs.berkeley.edu;igor.mordatch@gmail.com,8;8;4,3;4;3,Reject,0,3,0,yes,10/27/17,University of California Berkeley;University of California Berkeley;University of Washington,5;5;6,18;18;25,,10/27/17,10,5,2,0,30,0,226;36468;3006,13;433;48,7;94;26,13;4391;344,-1;-1
1155,ICLR,2018,On the Generalization Effects of DenseNet Model Structures ,Yin Liu;Vincent Chen,liuyin14@mails.tsinghua.edu.cn;389091983@qq.com,2;3;3,5;4;4,Reject,0,0,0,yes,10/27/17,Tsinghua University;QQ.com,10;-1,30;-1,8,10/27/17,0,0,0,0,0,0,162;132,34;22,8;7,4;14,-1;-1
1156,ICLR,2018,INTERPRETATION OF NEURAL NETWORK IS FRAGILE,Amirata Ghorbani;Abubakar Abid;James Zou,amiratag@stanford.edu;a12d@stanford.edu;jamesz@stanford.edu,6;4;5,2;4;5,Reject,9,8,0,yes,10/27/17,Stanford University;Stanford University;Stanford University,4;4;4,3;3;3,4,10/27/17,141,97,26,6,71,15,286;378;7457,18;19;40,7;8;18,28;39;603,-1;-1
1157,ICLR,2018,Faster Discovery of Neural Architectures by Searching for Paths in a Large Model,Hieu Pham;Melody Y. Guan;Barret Zoph;Quoc V. Le;Jeff Dean,hyhieu@cmu.edu;mguan@stanford.edu;barretzoph@google.com;qvl@google.com;jeff@google.com,6;5;5,2;3;2,Invite to Workshop Track,4,14,0,yes,10/27/17,Carnegie Mellon University;Stanford University;Google;Google;Google,1;4;-1;-1;-1,24;3;-1;-1;-1,,10/27/17,10,4,3,0,0,1,5005;821;6924;47503;1958,18;8;30;192;27,11;6;20;79;9,769;203;1299;5974;288,-1;-1
1158,ICLR,2018,Deep Lipschitz networks and Dudley GANs,Ehsan Abbasnejad;Javen Shi;Anton van den Hengel,ehsan.abbasnejad@adelaide.edu.au;javen.shi@adelaide.edu.au;anton.vandenhengel@adelaide.edu.au,8;5;5,4;3;1,Reject,0,3,0,yes,10/27/17,The University of Adelaide;The University of Adelaide;The University of Adelaide,124;124;124,134;134;134,5;4;8,10/27/17,10,4,3,1,0,3,367;45;10456,49;11;279,10;4;51,34;7;911,-1;-1
1159,ICLR,2018,Acquiring Target Stacking Skills by Goal-Parameterized Deep Reinforcement Learning,Wenbin Li;Jeannette Bohg;Mario Fritz,wenbinli@mpi-inf.mpg.de;bohg@stanford.edu;mfritz@mpi-inf.mpg.de,5;4;5,4;4;3,Reject,0,0,0,yes,10/27/17,"Saarland Informatics Campus, Max-Planck Institute;Stanford University;Saarland Informatics Campus, Max-Planck Institute",-1;4;-1,-1;3;-1,8,10/27/17,5,2,1,0,11,0,287;1853;7610,30;87;196,7;21;45,11;97;1001,-1;-1
1160,ICLR,2018,XGAN: Unsupervised Image-to-Image Translation for many-to-many Mappings,Amelie Royer;Konstantinos Bousmalis;Stephan Gouws;Fred Bertsch;Inbar Mosseri;Forrester Cole;Kevin Murphy,aroyer@ist.ac.at;konstantinos@google.com;sgouws@google.com;fredbertsch@google.com;inbarm@google.com;fcole@google.com;kpmurphy@google.com,3;4;4,5;4;3,Reject,2,9,0,yes,10/27/17,Institute of Science and Technology Austria;Google;Google;Google;Google;Google;Google,99;-1;-1;-1;-1;-1;-1,1103;-1;-1;-1;-1;-1;-1,4,10/27/17,50,17,16,0,49,4,99;1943;3575;82;443;1120;2546,13;22;23;9;10;38;127,3;12;12;4;7;16;26,11;191;331;6;43;81;201,-1;-1
1161,ICLR,2018,Meta-Learning Transferable Active Learning Policies by Deep Reinforcement Learning,Kunkun Pang;Mingzhi Dong;Timothy Hospedales,k.pang@ed.ac.uk;mingzhi.dong.13@ucl.ac.uk;t.hospedales@ed.ac.uk,6;7;6,3;4;4,Reject,0,6,0,yes,10/27/17,University of Edinburgh;University College London;University of Edinburgh,33;46;33,27;16;27,6,10/27/17,23,16,7,1,0,3,30;94;5063,5;19;157,3;4;36,3;7;658,-1;-1
1162,ICLR,2018,Feature Map Variational Auto-Encoders,Lars Maaløe;Ole Winther,larsma@dtu.dk;olwi@dtu.dk,5;3;6,4;3;4,Reject,14,5,0,yes,10/27/17,Technical University of Denmark;Technical University of Denmark,210;210,153;153,5,10/27/17,1,1,1,0,0,0,818;6051,21;201,7;34,118;711,-1;-1
1163,ICLR,2018,Structured Exploration via Hierarchical Variational Policy Networks,Stephan Zheng;Yisong Yue,stephan@caltech.edu;yyue@caltech.edu,4;7;5,5;3;3,Reject,0,8,0,yes,10/27/17,California Institute of Technology;California Institute of Technology,139;139,3;3,,10/27/17,3,2,0,0,0,0,458;3207,29;121,7;29,31;390,-1;-1
1164,ICLR,2018,Bayesian Time Series Forecasting with Change Point and Anomaly Detection,Anderson Y. Zhang;Miao Lu;Deguang Kong;Jimmy Yang,ye.zhang@yale.edu;mlu@oath.com;dkong@oath.com;jianyang@oath.com,5;6;4,5;3;5,Reject,0,3,0,yes,10/27/17,Yale University;Yahoo;Oath;Oath,62;-1;-1;-1,12;-1;-1;-1,11,10/27/17,1,1,0,0,0,0,318;680;958;66,13;21;49;25,6;6;15;4,44;63;81;2,-1;-1
1165,ICLR,2018,An empirical study on evaluation metrics of generative adversarial networks,Gao Huang;Yang Yuan;Qiantong Xu;Chuan Guo;Yu Sun;Felix Wu;Kilian Weinberger,gh349@cornell.edu;yy528@cornell.edu;qx57@cornell.edu;cg563@cornell.edu;yusun@berkeley.edu;fw245@cornell.edu;kqw4@cornell.edu,8;7;5;5,3;4;5;3,Reject,2,11,0,yes,10/27/17,Cornell University;Cornell University;Cornell University;Cornell University;University of California Berkeley;Cornell University;Cornell University,7;7;7;7;5;7;7,19;19;19;19;18;19;19,5;4,10/27/17,63,23,20,4,59,11,12184;4929;218;1325;3096;900;23564,60;391;14;23;95;18;165,22;32;7;8;15;10;54,2049;385;20;240;531;176;3794,-1;-1
1166,ICLR,2018,Explaining the Mistakes of Neural Networks with Latent Sympathetic Examples,Riaan Zoetmulder;Efstratios Gavves;Peter O'Connor,riaan.zoetmulder@student.uva.nl;egavves@uva.nl;peter.ed.oconnor@gmail.com,4;4;6,5;3;4,Reject,0,4,0,yes,10/27/17,University of Amsterdam;University of Amsterdam;,181;181;-1,59;59;-1,5;4,10/27/17,0,0,0,0,0,0,0;3281;1733,1;73;81,0;26;17,0;470;132,-1;-1
1167,ICLR,2018,FAST READING COMPREHENSION WITH CONVNETS,Felix Wu;Ni Lao;John Blitzer;Guandao Yang;Kilian Weinberger,fw245@cornell.edu;nlao@google.com;blitzer@google.com;gy46@cornell.edu;kqw4@cornell.edu,4;7;5,4;3;4,Reject,0,0,0,yes,10/27/17,Cornell University;Google;Google;Cornell University;Cornell University,7;-1;-1;7;7,19;-1;-1;19;19,,10/27/17,9,5,1,0,6,1,900;2895;6061;132;23564,18;50;43;9;165,10;17;21;6;54,176;383;900;15;3794,-1;-1
1168,ICLR,2018,Block-Sparse Recurrent Neural Networks,Sharan Narang;Eric Undersander;Gregory Diamos,sharan@baidu.com;undersandereric@baidu.com;gdiamos@baidu.com,5;7;5,3;4;4,Reject,0,8,0,yes,10/27/17,Baidu;Baidu;Baidu,-1;-1;-1,-1;-1;-1,3,10/27/17,39,19,18,0,20,5,2495;36;1946,15;1;42,10;1;19,269;5;192,-1;-1
1169,ICLR,2018,Bias-Variance Decomposition for Boltzmann Machines,Mahito Sugiyama;Koji Tsuda;Hiroyuki Nakahara,mahito@nii.ac.jp;tsuda@k.u-tokyo.ac.jp;hiro@brain.riken.jp,5;7;5,2;5;5,Reject,0,3,0,yes,10/27/17,Meiji University;The University of Tokyo;RIKEN,468;52;-1,334;45;-1,8,10/27/17,0,0,0,0,0,0,292;7772;4075,54;308;128,9;34;22,28;590;330,-1;-1
1170,ICLR,2018,Unseen Class Discovery in Open-world Classification,Lei Shu;Hu Xu;Bing Liu,lshu3@uic.edu;hxu48@uic.edu;liub@uic.edu,5;5;4,4;5;4,Reject,0,0,0,yes,10/27/17,"University of Illinois, Chicago;University of Illinois, Chicago;University of Illinois, Chicago",57;57;57,255;255;255,,10/27/17,17,10,3,0,5,1,362;326;105,28;65;35,9;8;5,51;47;8,-1;-1
1171,ICLR,2018,DLVM: A modern compiler infrastructure for deep learning systems,Richard Wei;Lane Schwartz;Vikram Adve,xwei12@illinois.edu;lanes@illinois.edu;vadve@illinois.edu,5;7;5,4;4;3,Invite to Workshop Track,0,4,0,yes,10/27/17,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",3;3;3,37;37;37,10,10/27/17,36,21,13,0,46,4,42;591;8158,5;50;148,2;12;40,3;53;1184,-1;-1
1172,ICLR,2018,Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design,Daniel Neil;Marwin Segler;Laura Guasch;Mohamed Ahmed;Dean Plumbley;Matthew Sellwood;Nathan Brown,daniel.neil@benevolent.ai;marwin.segler@benevolent.ai;laura.guasch@benevolent.ai;mohamed.ahmed@benevolent.ai;dean.plumbley@benevolent.ai;matthew.sellwood@benevolent.ai;nathan.brown@benevolent.ai,4;7;6,2;4;3,Invite to Workshop Track,0,6,1,yes,10/27/17,BenevolentAI;BenevolentAI;BenevolentAI;BenevolentAI;BenevolentAI;BenevolentAI;BenevolentAI,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,,10/27/17,21,6,5,0,0,2,1355;1095;33;2061;30;40;814,36;23;4;70;2;3;99,17;11;3;14;2;3;17,187;36;3;232;2;2;36,-1;-1
1173,ICLR,2018,Transfer Learning to Learn with Multitask Neural Model Search,Catherine Wong;Andrea Gesmundo,catwong@cs.stanford.edu;agesmundo@google.com,5;7;4,2;3;4,Reject,0,7,0,yes,10/27/17,Stanford University;Google,4;-1,3;-1,6,10/27/17,4,2,1,0,11,0,228;267,58;26,8;8,19;29,-1;-1
1174,ICLR,2018,Multiple Source Domain Adaptation with Adversarial Learning,Han Zhao;Shanghang Zhang;Guanhang Wu;Jo\~{a}o  P. Costeira;Jos\'{e} M. F.  Moura;Geoffrey J. Gordon,han.zhao@cs.cmu.edu;shanghaz@andrew.cmu.edu;guanhanw@andrew.cmu.edu;jpc@isr.ist.utl.pt;moura@andrew.cmu.edu;ggordon@cs.cmu.edu,6;6;6;6,3;5;5;4,Invite to Workshop Track,0,9,0,yes,10/27/17,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;University of Lisbon;Carnegie Mellon University;Carnegie Mellon University,1;1;1;468;1;1,24;24;24;509;24;24,4;1;8,5/26/17,40,23,13,0,15,9,1026;339;2161;340;12947;10541,125;30;98;11;533;186,18;9;20;7;56;48,66;27;236;29;929;1211,-1;-1
1175,ICLR,2018,Deep Generative Dual Memory Network for Continual Learning,Nitin Kamra;Umang Gupta;Yan Liu,nkamra@usc.edu;umanggup@usc.edu;yanliu.cs@usc.edu,5;6;7,4;4;2,Reject,0,7,0,yes,10/27/17,University of Southern California;University of Southern California;University of Southern California,31;31;31,66;66;66,5,10/27/17,45,19,14,1,6,2,152;175;6065,11;19;584,5;6;35,14;13;494,-1;-1
1176,ICLR,2018,Learning to Infer,Joseph Marino;Yisong Yue;Stephan Mandt,jmarino@caltech.edu;yyue@caltech.edu;stephan.mandt@disneyresearch.com,5;6;5,4;5;4,Invite to Workshop Track,0,9,0,yes,10/27/17,"California Institute of Technology;California Institute of Technology;Disney Research, Disney",139;139;-1,3;3;-1,11;5;1,10/27/17,7,3,0,0,0,0,323;3472;1397,63;124;67,9;31;17,17;391;119,-1;-1
1177,ICLR,2018,Latent Space Oddity: on the Curvature of Deep Generative Models,Georgios Arvanitidis;Lars Kai Hansen;Søren Hauberg,gear@dtu.dk;lkai@dtu.dk;sohau@dtu.dk,3;7;7,4;4;3,Accept (Poster),0,5,0,yes,10/27/17,Technical University of Denmark;Technical University of Denmark;Technical University of Denmark,210;210;210,153;153;153,5,10/27/17,58,37,18,2,7,7,107;10737;961,10;526;52,5;51;17,10;805;81,-1;-1
1178,ICLR,2018,The Context-Aware Learner,Conor Durkan;Amos Storkey;Harrison Edwards,conor.durkan@ed.ac.uk;a.storkey@ed.ac.uk;h.l.edwards@sms.ed.ac.uk,6;4;4,5;3;4,Reject,0,0,0,yes,10/27/17,University of Edinburgh;University of Edinburgh;University of Edinburgh,33;33;33,27;27;27,6;8,10/27/17,0,0,0,0,0,0,88;3794;1158,6;198;18,4;31;8,16;434;190,-1;-1
1179,ICLR,2018,When and where do feed-forward neural networks learn localist representations?,Ella M. Gale;Nicolas Martin;Jeffrey Bowers,eg16993@bristol.ac.uk;nm13850@bristol.ac.uk;j.bowers@bristol.ac.uk,3;3;5,3;5;4,Reject,0,5,0,yes,10/27/17,University of Bristol;University of Bristol;University of Bristol,124;124;124,76;76;76,,10/27/17,4,0,0,0,4,0,7;212;2419,9;65;122,2;6;27,0;11;238,-1;-1
1180,ICLR,2018,Generating Differentially Private Datasets Using GANs,Aleksei Triastcyn;Boi Faltings,aleksei.triastcyn@epfl.ch;boi.faltings@epfl.ch,6;5;4,4;4;4,Reject,2,4,0,yes,10/27/17,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne,468;468,38;38,5;4,10/27/17,18,10,7,0,0,2,59;6875,10;431,5;43,5;604,-1;-1
1181,ICLR,2018,Auxiliary Guided Autoregressive Variational Autoencoders,Thomas Lucas;Jakob Verbeek,thomas.lucas@inria.fr;jakob.verbeek@inria.fr,5;7;5,4;4;4,Reject,0,12,0,yes,10/27/17,INRIA;INRIA,-1;-1,-1;-1,5,10/27/17,14,5,4,0,4,0,192;1511,30;43,6;16,6;131,-1;-1
1182,ICLR,2018,Orthogonal Recurrent Neural Networks with Scaled Cayley Transform,Kyle Helfrich;Devin Willmott;Qiang Ye,kyle.helfrich@uky.edu;devin.willmott@uky.edu;qiang.ye@uky.edu,7;6;5,3;3;4,Reject,0,5,0,yes,10/27/17,University of Kentucky;University of Kentucky;University of Kentucky,210;210;210,346;346;346,,7/29/17,27,11,12,1,13,6,25;44;3811,1;10;238,1;3;28,5;5;269,-1;-1
1183,ICLR,2018,Can Deep Reinforcement Learning solve Erdos-Selfridge-Spencer Games?,Maithra Raghu;Alex Irpan;Jacob Andreas;Robert Kleinberg;Quoc Le;Jon Kleinberg,maithrar@gmail.com;alexirpan@google.com;j.d.andreas@gmail.com;rdk@cs.cornell.edu;qvl@google.com;kleinber@cs.cornell.edu,5;6;6,3;3;3,Invite to Workshop Track,0,13,0,yes,10/27/17,Cornell University;Google;University of California Berkeley;Cornell University;Google;Cornell University,7;-1;5;7;-1;7,19;-1;18;19;-1;19,8,10/27/17,14,10,2,0,69,2,1136;611;2553;8088;49563;48464,27;14;42;191;193;442,11;6;24;48;82;97,149;37;339;954;6128;5205,-1;-1
1184,ICLR,2018,DNN Representations as Codewords: Manipulating Statistical Properties via Penalty Regularization,Daeyoung Choi;Changho Shin;Hyunghun Cho;Wonjong Rhee,choid@snu.ac.kr;ch.shin@snu.ac.kr;webofthink@snu.ac.kr;wrhee@snu.ac.kr,5;5;5,5;3;4,Reject,0,3,0,yes,10/27/17,Seoul National University;Seoul National University;Seoul National University;Seoul National University,46;46;46;46,74;74;74;74,,10/27/17,0,0,0,0,0,0,27;121;41;2334,11;42;8;57,2;6;2;14,5;8;3;273,-1;-1
1185,ICLR,2018,A Tensor Analysis on Dense Connectivity via Convolutional Arithmetic Circuits,Emilio Rafael Balda;Arash Behboodi;Rudolf Mathar,emilio.balda@ti.rwth-aachen.de;arash.behboodi@ti.rwth-aachen.de;mathar@ti.rwth-aachen.de,4;4;5,3;3;3,Reject,0,3,0,yes,10/27/17,RWTH Aachen University;RWTH Aachen University;RWTH Aachen University,99;99;99,79;79;79,8,10/27/17,1,1,0,0,0,0,40;309;3489,16;66;472,4;8;28,2;15;187,-1;-1
1186,ICLR,2018,Classification and Disease Localization in Histopathology Using Only Global Labels: A Weakly-Supervised Approach,Pierre Courtiol;Eric W. Tramel;Marc Sanselme;Gilles Wainrib,pierre.courtiol@owkin.com;eric.tramel@owkin.com;marc.sanselme@owkin.com;gilles.wainrib@owkin.com,5;6;5,4;3;3,Reject,0,8,0,yes,10/27/17,;;;,-1;-1;-1;-1,-1;-1;-1;-1,2,10/27/17,18,7,9,1,12,0,45;974;21;536,7;35;2;56,3;16;2;12,1;118;0;36,-1;-1
1187,ICLR,2018,Realtime query completion via deep language models,Po-Wei Wang;J. Zico Kolter;Vijai Mohan;Inderjit S. Dhillon,poweiw@cs.cmu.edu;zkolter@cs.cmu.edu;vijaim@amazon.com;isd@a9.com,4;6;5,5;3;3,Reject,3,2,0,yes,10/27/17,Carnegie Mellon University;Carnegie Mellon University;Amazon;A9,1;1;-1;-1,24;24;-1;-1,3,10/27/17,8,5,1,0,0,1,242;7544;58;20520,17;105;13;265,7;35;3;70,30;1052;3;2752,-1;-1
1188,ICLR,2018,LEARNING SEMANTIC WORD RESPRESENTATIONS VIA TENSOR FACTORIZATION,Eric Bailey;Charles Meyer;Shuchin Aeron,popcorncolonel@gmail.com;cmey63@gmail.com;shuchin@ece.tufts.edu,5;5;5,3;5;5,Reject,0,6,0,yes,10/27/17,Tufts University;;Tufts University,153;-1;153,169;-1;169,3,10/27/17,0,0,0,0,0,0,6;397;1427,5;103;103,1;10;17,1;64;154,-1;-1
1189,ICLR,2018,Finding ReMO (Related Memory Object): A Simple neural architecture for Text based Reasoning,Jihyung Moon;Hyochang Yang;Sungzoon Cho,jhmoon@dm.snu.ac.kr;hyochang@dm.snu.ac.kr;zoon@snu.ac.kr,4;4;4,4;4;4,Reject,0,4,0,yes,10/27/17,Seoul National University;Seoul National University;Seoul National University,46;46;46,74;74;74,,10/27/17,4,2,0,0,45,0,21;11;2529,10;2;178,3;2;29,0;0;194,-1;-1
1190,ICLR,2018,Representing dynamically: An active process for describing sequential data,Juan Sebastian Olier;Emilia Barakova;Matthias Rauterberg;Carlo Regazzoni,j.s.olier.jauregui@tue.nl;e.i.barakova@tue.nl;g.w.m.rauterberg@tue.nl;carlo.regazzoni@unige.it,6;3;4;4,3;3;4;4,Reject,0,0,0,yes,10/27/17,Eindhoven University of Technology;Eindhoven University of Technology;Eindhoven University of Technology;Università degli Studi di Genova,181;181;181;-1,141;141;141;-1,5;11,10/27/17,0,0,0,0,0,0,47;1024;113;4477,10;150;21;492,4;15;4;31,1;51;12;183,-1;-1
1191,ICLR,2018,Modeling Latent Attention Within Neural Networks,Christopher Grimm;Dilip Arumugam;Siddharth Karamcheti;David Abel;Lawson L.S. Wong;Michael L. Littman,crgrimm@umich.edu;dilip_arumugam@brown.edu;siddharth_karamcheti@brown.edu;david_abel@brown.edu;lsw@brown.edu;mlittman@cs.brown.edu,4;5;7,4;4;4,Reject,0,3,0,yes,10/27/17,University of Michigan;Brown University;Brown University;Brown University;Brown University;Brown University,8;62;62;62;62;62,21;50;50;50;50;50,3;2,10/27/17,0,0,0,0,0,0,42;157;84;274;0;27135,22;18;11;34;2;368,4;7;4;9;0;68,5;5;5;25;0;2808,-1;-1
1192,ICLR,2018,Deep Active Learning for Named Entity Recognition,Yanyao Shen;Hyokun Yun;Zachary C. Lipton;Yakov Kronrod;Animashree Anandkumar,shenyanyao@utexas.edu;yunhyoku@amazon.com;zlipton@cmu.edu;kronrod@amazon.com;animakumar@gmail.com,6;6;7,3;4;4,Accept (Poster),0,5,0,yes,10/27/17,"University of Texas, Austin;Amazon;Carnegie Mellon University;Amazon;University of California-Irvine",21;-1;1;-1;36,49;-1;24;-1;99,3,7/19/17,111,42,45,2,46,15,304;372;4731;233;5315,16;19;97;10;187,8;7;28;7;38,27;49;431;23;744,-1;-1
1193,ICLR,2018,On the limitations of first order approximation in GAN dynamics,Jerry Li;Aleksander Madry;John Peebles;Ludwig Schmidt,jerryzli@mit.edu;madry@mit.edu;jpeebles@mit.edu;ludwigs@mit.edu,4;5;7,4;3;3,Invite to Workshop Track,0,3,0,yes,10/27/17,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2,5;5;5;5,5;4,6/29/17,20,13,4,0,26,1,1266;5359;440;3694,45;84;24;198,18;29;11;21,188;1068;35;885,-1;-1
1194,ICLR,2018,Learning Covariate-Specific Embeddings with Tensor Decompositions,Kevin Tian;Teng Zhang;James Zou,kjtian@stanford.edu;tengz@stanford.edu;jamesz@stanford.edu,5;5;5,3;5;4,Reject,0,0,0,yes,10/27/17,Stanford University;Stanford University;Stanford University,4;4;4,3;3;3,3;7,10/27/17,7,0,0,0,7,0,381;5242;1572,40;360;93,7;33;20,7;157;159,-1;-1
1195,ICLR,2018,"LSD-Net: Look, Step and Detect for Joint Navigation and Multi-View Recognition with Deep Reinforcement Learning",N dinesh reddy,dnarapur@andrew.cmu.edu,4;6;3,4;4;4,Reject,0,0,0,yes,10/27/17,Carnegie Mellon University,1,24,,10/27/17,0,0,0,0,0,0,108,15,6,4,-1
1196,ICLR,2018,Cross-View Training for Semi-Supervised Learning,Kevin Clark;Thang Luong;Quoc V. Le,kevclark@cs.stanford.edu;qvl@google.com;thangluong@google.com,2;5;7,4;4;4,Invite to Workshop Track,0,6,0,yes,10/27/17,Stanford University;Google;Google,4;-1;-1,3;-1;-1,3;4,10/27/17,4,3,2,0,0,1,1046;5193;47503,10;27;192,9;11;79,141;647;5974,-1;-1
1197,ICLR,2018,Learning Weighted Representations for Generalization Across Designs,Fredrik D. Johansson;Nathan Kallus;Uri Shalit;David Sontag,fredrikj@mit.edu;kallus@cornell.edu;urish22@gmail.com;dsontag@csail.mit.edu,5;8;7,3;3;4,Reject,0,4,0,yes,10/27/17,Massachusetts Institute of Technology;Cornell University;Technion;Massachusetts Institute of Technology,2;7;24;2,5;19;327;5,1;8,10/27/17,15,8,6,0,0,3,759;1047;1614;49,32;66;42;6,13;16;15;3,135;102;290;8,-1;-1
1198,ICLR,2018,Exploring Asymmetric Encoder-Decoder Structure for Context-based Sentence Representation Learning,Shuai Tang;Hailin Jin;Chen Fang;Zhaowen Wang;Virginia R. de Sa,shuaitang93@ucsd.edu;hljin@adobe.com;cfang@adobe.com;zhawang@adobe.com;desa@ucsd.edu,7;6;3,4;4;5,Reject,4,5,0,yes,10/27/17,"University of California, San Diego;Adobe Systems;Adobe Systems;Adobe Systems;University of California, San Diego",11;-1;-1;-1;11,31;-1;-1;-1;31,,10/27/17,4,3,0,0,7,0,48;5954;2490;3448;1044,15;128;176;72;81,4;41;19;26;16,1;718;318;443;54,-1;-1
1199,ICLR,2018,Data Augmentation Generative Adversarial Networks,Anthreas Antoniou;Amos Storkey;Harrison Edwards,a.antoniou@sms.ed.ac.uk;a.storkey@ed.ac.uk;h.l.edwards@sms.ed.ac.uk,4;9;6,4;5;3,Invite to Workshop Track,0,3,0,yes,10/27/17,University of Edinburgh;University of Edinburgh;University of Edinburgh,33;33;33,27;27;27,5;4;6,10/27/17,264,120,96,1,290,27,766;3794;1158,104;198;18,11;31;8,96;434;190,-1;-1
1200,ICLR,2018,Long-term Forecasting using Tensor-Train RNNs,Rose Yu;Stephan Zheng;Anima Anandkumar;Yisong Yue,rose@caltech.edu;stephan@caltech.edu;anima@caltech.edu;yyue@caltech.edu,4;5;6,4;3;4,Reject,0,0,0,yes,10/27/17,California Institute of Technology;California Institute of Technology;California Institute of Technology;California Institute of Technology,139;139;139;139,3;3;3;3,,10/27/17,60,26,15,0,70,6,856;467;5315;3213,38;29;187;121,12;8;38;29,100;32;744;390,-1;-1
1201,ICLR,2018,Autostacker: an Automatic Evolutionary Hierarchical  Machine Learning System,Boyuan Chen;Warren Mo;Ishanu Chattopadhyay;Hod Lipson,boyuan.chen@columbia.edu;warrenmo@uchicago.edu;ishanu@uchicago.edu;hod.lipson@columbia.edu,4;3;4,5;4;5,Reject,0,0,0,yes,10/27/17,Columbia University;University of Chicago;University of Chicago;Columbia University,15;46;46;15,14;9;9;14,,10/27/17,0,0,0,0,0,0,115;24;31;1107,22;3;7;73,5;1;3;15,16;5;5;28,-1;-1
1202,ICLR,2018,Inducing Grammars with and for Neural Machine Translation,Ke Tran;Yonatan Bisk,ketranmanh@gmail.com;ybisk@yonatanbisk.com,3;6;5,5;5;4,Reject,0,7,0,yes,10/27/17,University of Amsterdam;University of Washington,181;6,59;25,3,10/27/17,15,5,0,0,23,0,69;1109,21;43,3;16,5;151,-1;-1
1203,ICLR,2018,Reinforcement and Imitation Learning for Diverse Visuomotor Skills,Yuke Zhu;Ziyu Wang;Josh Merel;Andrei Rusu;Tom Erez;Serkan Cabi;Saran Tunyasuvunakool;János Kramár;Raia Hadsell;Nando de Freitas;Nicolas Heess,yukez@cs.stanford.edu;ziyu@google.com;jsmerel@google.com;andreirusu@google.com;etom@google.com;cabi@google.com;stunya@google.com;janosk@google.com;raia@google.com;nandodefreitas@google.com;heess@google.com,4;4;6,4;4;5,Reject,0,6,0,yes,10/27/17,Stanford University;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,4;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,3;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,6,10/27/17,102,55,20,2,40,4,2367;4187;1629;2960;6431;179;190;405;8110;18886;11371,56;51;29;16;48;12;7;14;63;184;104,20;21;15;13;19;6;4;7;25;54;37,291;484;127;430;1079;11;9;36;778;1852;1617,-1;-1
1204,ICLR,2018,Regret Minimization for Partially Observable Deep Reinforcement Learning,Peter H. Jin;Sergey Levine;Kurt Keutzer,phj@eecs.berkeley.edu;svlevine@eecs.berkeley.edu;keutzer@berkeley.edu,4;7;5,4;4;5,Invite to Workshop Track,0,5,0,yes,10/27/17,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,18;18;18,,10/27/17,16,10,5,0,12,2,55;24386;16681,11;309;417,4;73;59,4;3167;1575,-1;-1
1205,ICLR,2018,Dense Recurrent Neural Network with Attention Gate,Yong-Ho Yoo;Kook Han;Sanghyun Cho;Kyoung-Chul Koh;Jong-Hwan Kim,yhyoo@rit.kaist.ac.kr;khan@rit.kaist.ac.kr;scho@rit.kaist.ac.kr;kckoh@rit.kaist.ac.kr;johkim@rit.kaist.ac.kr,2;4;4,4;4;4,Reject,0,2,0,yes,10/27/17,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,21;21;21;21;21,95;95;95;95;95,3,10/27/17,0,0,0,0,0,0,139;145;258;59;4115,31;65;69;5;352,7;6;9;2;26,6;11;27;4;390,-1;-1
1206,ICLR,2018,Stabilizing Gradients for Deep Neural Networks via Efficient SVD Parameterization,Jiong Zhang;Qi Lei;Inderjit S. Dhillon,zhangjiong724@utexas.edu;leiqi@ices.utexas.edu;inderjit@cs.utexas.edu,7;5;5,4;4;3,Reject,0,3,0,yes,10/27/17,"University of Texas, Austin;University of Texas, Austin;University of Texas, Austin",21;21;21,49;49;49,8,10/27/17,32,18,12,3,2,7,1164;1345;20520,86;149;265,16;20;70,80;90;2752,-1;-1
1207,ICLR,2018,Semantic Interpolation in Implicit Models,Yannic Kilcher;Aurelien Lucchi;Thomas Hofmann,yannic.kilcher@inf.ethz.ch;aurelien.lucchi@inf.ethz.ch;thomas.hofmann@inf.ethz.ch,6;5;7,3;4;4,Accept (Poster),0,5,0,yes,10/27/17,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,9;9;9,10;10;10,,10/27/17,9,5,4,1,26,3,113;6789;22675,14;68;173,4;24;52,26;1108;3389,-1;-1
1208,ICLR,2018,A Painless Attention Mechanism for Convolutional Neural Networks,Pau Rodríguez;Guillem Cucurull;Jordi Gonzàlez;Josep M. Gonfaus;Xavier Roca,pau.rodriguez@cvc.uab.es;gcucurull@cvc.uab.cat;poal@cvc.uab.cat;xavir@cvc.uab.es,5;5;6,4;4;4,Reject,0,10,1,yes,10/27/17,"Computer Vision Center, Universitat Autònoma de Barcelona;Universitat Autonoma de Barcelona;Universitat Autonoma de Barcelona;Computer Vision Center, Universitat Autònoma de Barcelona",468;468;468;468,146;146;146;146,7,10/27/17,5,2,2,1,0,1,258;1460;19;422;703,39;12;5;16;90,6;8;2;6;14,24;369;1;46;41,-1;-1
1209,ICLR,2018,Bayesian Hypernetworks,David Krueger;Chin-Wei Huang;Riashat Islam;Ryan Turner;Alexandre Lacoste;Aaron Courville,david.scott.krueger@gmail.com;chin-wei.huang@umontreal.ca;riashat.islam@mail.mcgill.ca;turnerry@iro.umontreal.ca;allac@elementai.com;aaron.courville@gmail.com,6;6;6,4;4;4,Reject,1,12,0,yes,10/27/17,University of Montreal;University of Montreal;McGill University;University of Montreal;Element AI;University of Montreal,124;124;81;124;-1;124,108;108;42;108;-1;108,11;4;1,10/13/17,36,15,10,0,190,3,1460;610;1195;256;963;59549,48;53;25;29;38;203,12;13;8;7;14;64,214;74;125;27;127;7800,-1;-1
1210,ICLR,2018,Distributional Adversarial Networks,Chengtao Li;David Alvarez-Melis;Keyulu Xu;Stefanie Jegelka;Suvrit Sra,ctli@mit.edu;dalvmel@mit.edu;keyulu@mit.edu;stefje@csail.mit.edu;suvrit@mit.edu,6;6;6,3;4;3,Invite to Workshop Track,0,4,0,yes,10/27/17,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2;2,5;5;5;5;5,5;4,6/29/17,12,3,3,0,13,2,141;512;801;3265;6337,44;18;7;114;163,7;11;7;28;39,7;45;216;530;999,-1;-1
1211,ICLR,2018,Structured Deep Factorization Machine: Towards General-Purpose Architectures,José P. González-Brenes;Ralph Edezhath,jgonzalez@chegg.com;redezhath@chegg.com,3;4;4,5;5;3,Reject,0,1,0,yes,10/27/17,Chegg;Chegg,-1;-1,-1;-1,3,10/27/17,0,0,0,0,0,0,307;84,31;8,10;3,26;0,-1;-1
1212,ICLR,2018,Learning to search with MCTSnets,Arthur Guez;Theophane Weber;Ioannis Antonoglou;Karen Simonyan;Oriol Vinyals;Daan Wierstra;Remi Munos;David Silver,aguez@google.com;theophane@google.com;ioannisa@google.com;simonyan@google.com;vinyals@google.com;wierstra@google.com;munos@google.com;davidsilver@google.com,7;4;5,3;4;4,Reject,0,3,0,yes,10/27/17,Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1,,10/27/17,37,28,14,0,62,3,12984;1471;22915;59726;52161;27010;9303;42155,32;35;16;95;121;64;190;158,17;14;12;39;55;40;53;56,917;147;2736;10673;6516;4714;1313;5862,-1;-1
1213,ICLR,2018,Open Loop Hyperparameter Optimization and Determinantal Point Processes,Jesse Dodge;Kevin Jamieson;Noah A. Smith,jessed@cs.cmu.edu;jamieson@cs.washington.edu;nasmith@cs.washington.edu,4;4;4,5;5;5,Reject,0,3,0,yes,9/27/18,Carnegie Mellon University;University of Washington;University of Washington,1;6;6,24;25;25,,2/15/18,2,1,0,0,0,0,1238;1687;17574,21;47;292,13;17;64,122;247;2088,-1;-1
1214,ICLR,2018,Understanding and Exploiting the Low-Rank Structure of Deep Networks,Craig Bakker;Michael J. Henry;Nathan O. Hodas,craig.bakker@pnnl.gov;michael.j.henry@pnnl.gov;nathan.hodas@pnnl.gov,4;5;2,4;4;4,Reject,0,0,0,yes,10/27/17,Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory,-1;-1;-1,-1;-1;-1,,10/27/17,1,1,0,0,0,0,30;39;308,23;28;27,3;4;10,2;0;24,-1;-1
1215,ICLR,2018,Adversarially Regularized Autoencoders,Junbo (Jake) Zhao;Yoon Kim;Kelly Zhang;Alexander M. Rush;Yann LeCun,jakezhao@cs.nyu.edu;yoonkim@seas.harvard.edu;kz918@nyu.edu;srush@seas.harvard.edu;yann@cs.nyu.edu,5;6;3;9,4;3;4;3,Invite to Workshop Track,0,7,0,yes,10/27/17,New York University;Harvard University;New York University;Harvard University;New York University,26;37;26;37;26,27;6;27;6;27,5;4,6/13/17,117,68,52,4,131,35,3319;8001;231;6943;91479,14;18;19;87;345,9;15;6;32;107,447;1311;49;936;10337,-1;-1
1216,ICLR,2018,Expressive power of recurrent neural networks,Valentin Khrulkov;Alexander Novikov;Ivan Oseledets,khrulkov.v@gmail.com;sasha.v.novikov@gmail.com;i.oseledets@skoltech.ru,6;6;6,4;5;3,Accept (Poster),0,4,0,yes,10/27/17,Skolkovo Institute of Science and Technology;Higher School of Economics;Skolkovo Institute of Science and Technology,-1;468;-1,-1;377;-1,1,10/27/17,46,24,5,2,116,1,162;62;4282,12;7;197,7;3;29,17;1;401,-1;-1
1217,ICLR,2018,The (Un)reliability of saliency methods,Pieter-Jan Kindermans;Sara Hooker;Julius Adebayo;Kristof T. Schütt;Maximilian Alber;Sven Dähne;Dumitru Erhan;Been Kim,pikinder@google.com;shooker@google.com;juliusad@google.com;kristof.schuett@tu-berlin.de;maximilian.aber@tu-berlin.de;sven.daehne@tu-berlin.de;dumitru@google.com;beenkim@google.com,5;4;4,3;4;4,Reject,0,4,0,yes,10/27/17,Google;Google;Google;TU Berlin;TU Berlin;TU Berlin;Google;Google,-1;-1;-1;104;104;104;-1;-1,-1;-1;-1;92;92;92;-1;-1,,10/27/17,150,102,24,6,183,12,1933;312;557;1301;450;1961;41809;3068,47;6;17;19;16;54;60;51,18;4;7;10;6;21;31;21,182;39;60;74;36;122;5976;323,-1;-1
1218,ICLR,2018,Three factors influencing minima in SGD,Stanisław Jastrzębski;Zac Kenton;Devansh Arpit;Nicolas Ballas;Asja Fischer;Amos Storkey;Yoshua Bengio,staszek.jastrzebski@gmail.com;zakenton@gmail.com;devansh.arpit@umontreal.ca;ballas.n@gmail.com;asja.fischer@gmail.com;a.storkey@ed.ac.uk;yoshua.umontreal@gmail.com,6;3;5,4;4;4,Reject,0,8,0,yes,10/27/17,University of Montreal;University of Montreal;University of Montreal;University of Montreal;University of Bonn;University of Edinburgh;University of Montreal,124;124;124;124;124;33;124,108;108;108;108;100;27;108,8,10/27/17,130,88,24,6,34,18,847;231;886;4922;1713;3794;201719,27;15;42;54;53;198;807,12;7;12;20;16;31;147,99;30;116;577;199;434;23989,-1;-1
1219,ICLR,2018,A Deep Learning Approach for Survival Clustering without End-of-life Signals,S Chandra Mouli;Bruno Ribeiro;Jennifer Neville,chandr@purdue.edu;ribeiro@cs.purdue.edu;neville@cs.purdue.edu,6;4;6,1;4;5,Reject,0,3,0,yes,10/27/17,Purdue University;Purdue University;Purdue University,28;28;28,60;60;60,,10/27/17,0,0,0,0,0,0,15;1235;5127,4;84;158,2;21;33,1;131;502,-1;-1
1220,ICLR,2018,Predicting Multiple Actions for Stochastic Continuous Control,Sanjeev Kumar;Christian Rupprecht;Federico Tombari;Gregory D. Hager,sanjeev.kumar@in.tum.de;christian.rupprecht@in.tum.de;tombari@in.tum.de;hager@cs.tum.edu,3;7;4,4;3;4,Reject,0,5,0,yes,10/27/17,Technical University Munich;Technical University Munich;Technical University Munich;TU Munich,55;55;55;55,41;41;41;34,,10/27/17,1,1,0,0,0,0,1610;1224;6519;16128,349;45;178;517,16;13;36;57,129;209;958;1207,-1;-1
1221,ICLR,2018,Multi-Task Learning by Deep Collaboration and Application in Facial Landmark Detection,Ludovic Trottier;Philippe Giguère;Brahim Chaib-draa,ludovic.trottier.1@ulaval.ca;philippe.giguere@ift.ulaval.ca;brahim.chaib-draa@ift.ulaval.ca,5;6;6,5;4;4,Reject,0,6,0,yes,10/27/17,Laval university;Laval university;Laval university,468;468;468,265;265;265,8,10/27/17,9,5,3,0,24,0,111;1582;2783,12;111;207,4;22;27,13;94;261,-1;-1
1222,ICLR,2018,State Space LSTM Models with Particle MCMC Inference,Xun Zheng;Manzil Zaheer;Amr Ahmed;Yuan Wang;Eric P. Xing;Alex Smola,xunzheng90@gmail.com;manzil@cmu.edu;amra@google.com;yuanwang@google.com;epxing@cs.cmu.edu;alex@smola.org,3;5;7,5;5;5,Reject,0,6,0,yes,10/27/17,Carnegie Mellon University;Carnegie Mellon University;Google;Google;Carnegie Mellon University;Carnegie-Mellon University,1;1;-1;-1;1;1,24;24;-1;-1;24;24,,10/27/17,13,3,7,0,0,0,951;1541;4816;12096;24050;63968,43;63;148;566;603;389,13;17;27;46;75;97,106;260;517;693;2652;9072,-1;-1
1223,ICLR,2018,NOVEL AND EFFECTIVE PARALLEL MIX-GENERATOR GENERATIVE ADVERSARIAL NETWORKS,Xia Xiao;Sanguthevar Rajasekaran,xia.xiao@uconn.edu;sanguthevar.rajasekaran@uconn.edu,3;6;5,5;4;4,Reject,1,1,0,yes,10/27/17,University of Connecticut;University of Connecticut,153;153,324;324,5;4,10/27/17,0,0,0,0,0,0,61;3364,12;447,4;27,6;234,-1;-1
1224,ICLR,2018,Topology Adaptive Graph Convolutional  Networks,Jian Du;Shanghang Zhang;Guanhang Wu;José M. F. Moura;Soummya Kar,jiand@andrew.cmu.edu;shanghaz@andrew.cmu.edu;guanhanw@andrew.cmu.edu;moura@andrew.cmu.edu;soummyak@andrew.cmu.edu,6;4;5,3;4;4,Reject,0,10,0,yes,10/27/17,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1;1,24;24;24;24;24,10,10/27/17,31,18,6,2,4,3,811;339;340;12947;6869,102;30;11;533;289,13;9;7;56;37,37;27;29;929;344,-1;-1
1225,ICLR,2018,From Information Bottleneck To Activation Norm Penalty,Allen Nie;Mihir Mongia;James Zou,anie@stanford.edu;mihir.mongia@mssm.edu;jamesz@stanford.edu,7;4;4,3;3;4,Reject,0,2,0,yes,10/27/17,Stanford University;Arnhold Institute;Stanford University,4;-1;4,3;-1;3,3;1,10/27/17,0,0,0,0,0,0,136;4;1574,15;7;93,4;1;21,19;0;160,-1;-1
1226,ICLR,2018,TESLA: Task-wise Early Stopping and Loss Aggregation for Dynamic Neural Network Inference,Chun-Min Chang;Chia-Ching Lin;Hung-Yi Ou Yang;Chin-Laung Lei;Kuan-Ta Chen,cmchang@iis.sinica.edu.tw;d05921018@ntu.edu.tw;frank840925@gmail.com;cllei@ntu.edu.tw;swc@iis.sinica.edu.tw,4;5;4,4;2;2,Reject,0,3,0,yes,10/27/17,Academia Sinica;National Taiwan University;;National Taiwan University;Academia Sinica,-1;85;-1;85;-1,-1;197;-1;197;-1,,10/27/17,0,0,0,0,0,0,59;1478;10;3219;3864,16;131;7;183;201,5;19;2;26;34,2;68;2;319;357,-1;-1
1227,ICLR,2018,Stochastic Hyperparameter Optimization through Hypernetworks,Jonathan Lorraine;David Duvenaud,lorraine@cs.toronto.edu;duvenaud@cs.toronto.edu,6;6;6,4;3;1,Reject,0,4,0,yes,10/27/17,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",17;17,22;22,,10/27/17,23,8,11,0,210,0,54;5869,6;74,4;28,7;747,-1;-1
1228,ICLR,2018,Analyzing and Exploiting NARX Recurrent Neural Networks for Long-Term Dependencies,Robert DiPietro;Christian Rupprecht;Nassir Navab;Gregory D. Hager,rdipietro@gmail.com;christian.rupprecht@in.tum.de;nassir.navab@tum.de;hager@cs.jhu.edu,3;7;6,4;5;4,Invite to Workshop Track,0,4,0,yes,10/27/17,Johns Hopkins University;Technical University Munich;Technical University Munich;Johns Hopkins University,71;55;55;71,13;41;41;13,3,2/24/17,12,5,8,0,8,4,260;1224;18085;16128,26;45;1032;517,8;13;59;57,32;209;1500;1207,-1;-1
1229,ICLR,2018,Deep Temporal Clustering: Fully unsupervised learning of time-domain features,Naveen Sai Madiraju;Seid M. Sadat;Dimitry Fisher;Homa Karimabadi,naveen@avlab.ai;behnam@avlab.ai;dimitry@avlab.ai;homa@avlab.ai,3;5;4,5;4;4,Reject,0,0,0,yes,10/27/17,Arizona State University;;;,90;-1;-1;-1,126;-1;-1;-1,,10/27/17,11,4,5,0,12,2,12;326;68;760,4;19;9;98,1;11;4;15,2;7;2;45,-1;-1
1230,ICLR,2018,Hierarchical Adversarially Learned Inference,Mohamed Ishmael Belghazi;Sai Rajeswar;Olivier Mastropietro;Negar Rostamzadeh;Jovana Mitrovic;Aaron Courville,ishmael.belghazi@gmail.com;rajsai24@gmail.com;oli.mastro@gmail.com;negar.rostamzadeh@gmail.com;jovana.mitrovic@spc.ox.ac.uk;aaron.courville@gmail.com,5;5;7,5;5;3,Reject,0,13,0,yes,10/27/17,University of Montreal;University of Montreal;University of Montreal;Element AI;University of Oxford;University of Montreal,124;124;124;-1;51;124,108;108;108;-1;1;108,5;4,10/27/17,19,7,6,1,35,1,152;634;2268;375;74;59549,3;16;4;31;8;203,3;7;3;10;4;64,42;116;257;51;8;7800,-1;-1
1231,ICLR,2018,Intriguing Properties of Adversarial Examples,Ekin Dogus Cubuk;Barret Zoph;Samuel Stern Schoenholz;Quoc V. Le,cubuk@google.com;barretzoph@google.com;schsam@google.com;qvl@google.com,5;8;3,2;3;4,Invite to Workshop Track,2,5,0,yes,10/27/17,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,4,10/27/17,36,17,9,1,14,5,1320;6947;3006;47654,50;30;70;192,19;20;21;79,134;1298;376;5995,-1;-1
1232,ICLR,2018,TRL: Discriminative Hints for Scalable Reverse Curriculum Learning,Chen Wang;Xiangyu Chen;Zelin Ye;Jialu Wang;Ziruo Cai;Shixiang Gu;Cewu Lu,jere.wang@sjtu.edu.cn;cxy_1997@sjtu.edu.cn;h_e_r_o@sjtu.edu.cn;faldict@sjtu.edu.cn;sjtu_caiziruo@sjtu.edu.cn;sg717@cam.ac.uk;lucewu@sjtu.edu.cn,4;4;5,4;4;4,Reject,0,4,0,yes,10/27/17,Shanghai Jiao Tong University;Shanghai Jiao Tong University;Shanghai Jiao Tong University;Shanghai Jiao Tong University;Shanghai Jiao Tong University;University of Cambridge;Shanghai Jiao Tong University,57;57;57;57;57;71;57,188;188;188;188;188;2;188,,10/27/17,0,0,0,0,0,0,21327;44;1;43;1;3782;3845,1986;20;5;8;2;39;102,58;3;1;3;1;21;26,1291;2;0;5;0;470;693,-1;-1
1233,ICLR,2018,Continuous-fidelity Bayesian Optimization with Knowledge Gradient,Jian Wu;Peter I. Frazier,jw926@cornell.edu;pf98@cornell.edu,5;4;6,4;5;5,Reject,0,4,0,yes,10/27/17,Cornell University;Cornell University,7;7,19;19,11,10/27/17,4,2,1,0,0,0,4867;2721,502;155,35;26,143;288,-1;-1
1234,ICLR,2018,Neural Program Search: Solving Data Processing Tasks from Description and Examples,Illia Polosukhin;Alexander Skidanov,illia@near.ai;alex@near.ai,4;5;7,4;4;4,Invite to Workshop Track,0,2,0,yes,10/27/17,NEAR;NEAR,-1;-1,-1;-1,3,10/27/17,20,9,9,1,12,3,9375;27,13;5,8;2,2362;4,-1;-1
1235,ICLR,2018,GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks,Zhao Chen;Vijay Badrinarayanan;Chen-Yu Lee;Andrew Rabinovich,zchen@magicleap.com;vbadrinarayanan@magicleap.com;clee@magicleap.com;arabinovich@magicleap.com,6;4;4,2;4;4,Reject,0,9,0,yes,10/27/17,Magic Leap;Magic Leap;Magic Leap;Magic Leap,-1;-1;-1;-1,-1;-1;-1;-1,,10/27/17,127,63,60,2,46,21,984;5934;1981;20423,171;44;69;41,15;16;13;18,55;966;165;2732,-1;-1
1236,ICLR,2018,Lung Tumor Location and Identification with AlexNet and a Custom CNN,Allison M Rossetto;Wenjin Zhou,allison_rossetto@student.uml.edu;wenjin_zhou@uml.edu,2;3;3,5;4;4,Reject,0,4,0,yes,10/27/17,"University of Massachusetts, Lowell;University of Massachusetts, Lowell",-1;-1,-1;-1,,10/27/17,1,0,0,0,0,0,43;105,15;31,4;6,2;4,-1;-1
1237,ICLR,2018,Image Transformer,Ashish Vaswani;Niki Parmar;Jakob Uszkoreit;Noam Shazeer;Lukasz Kaiser,avaswani@google.com;nikip@google.com;uszkoreit@google.com;noam@google.com;lukaszkaiser@google.com,6;3;5,4;3;4,Reject,0,3,0,yes,10/27/17,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,5,10/27/17,125,48,36,4,206,7,11450;9900;11549;12620;22378,52;20;36;44;75,22;12;22;19;24,2659;2398;2652;2744;3865,-1;-1
1238,ICLR,2018,Not-So-CLEVR: Visual Relations Strain Feedforward Neural Networks,Junkyung Kim;Matthew Ricci;Thomas Serre,junkyung_kim@brown.edu;matthew_ricci_1@brown.edu;thomas_serre@brown.edu,6;6;6,3;3;4,Invite to Workshop Track,0,6,0,yes,10/27/17,Brown University;Brown University;Brown University,62;62;62,50;50;50,,10/27/17,72,9,3,1,58,7,706;99;7944,86;17;117,13;6;28,38;16;1221,-1;-1
1239,ICLR,2018,No Spurious Local Minima in a Two Hidden Unit ReLU Network,Chenwei Wu;Jiajun Luo;Jason D. Lee,wucw14@mails.tsinghua.edu.cn;jiajunlu@usc.edu;jasonlee@marshall.usc.edu,4;6;6,4;3;2,Invite to Workshop Track,0,3,0,yes,10/27/17,Tsinghua University;University of Southern California;University of Southern California,10;31;31,30;66;66,,10/27/17,11,7,1,1,0,1,11;1029;4667,3;92;118,1;12;36,1;10;601,-1;-1
1240,ICLR,2018,Semi-Supervised Learning via New Deep Network Inversion,Balestriero R.;Roger V.;Glotin H.;Baraniuk R.,randallbalestriero@gmail.com;roger.dyni@gmail.com;herve.glotin@univ-tln.fr;richb@rice.edu,5;4;7,4;5;2,Reject,0,5,0,yes,10/27/17,Rice University;;CNRS university Toulon;Rice University,85;-1;468;85,86;-1;1103;86,,10/27/17,3,2,0,0,16,0,112;21;1910;29471,38;40;278;659,5;2;24;84,2;1;109;2746,-1;-1
1241,ICLR,2018,Stable Distribution Alignment Using the Dual of the Adversarial Distance,Ben Usman;Kate Saenko;Brian Kulis,usmn@bu.edu;saenko@bu.edu;bkulis@bu.edu,5;6;6,4;4;3,Invite to Workshop Track,0,7,0,yes,10/27/17,Boston University;Boston University;Boston University,69;69;69,70;70;70,5;4,7/13/17,2,2,0,0,27,0,144;16930;9668,12;177;66,4;56;32,26;2363;1402,-1;-1
1242,ICLR,2018,Discovery of Predictive Representations With a Network of General Value Functions,Matthew Schlegel;Andrew Patterson;Adam White;Martha White,mkschleg@ualberta.ca;andnpatt@indiana.edu;amw8@ualberta.ca;whitem@ualberta.ca,4;4;5,1;4;4,Reject,0,6,0,yes,10/27/17,University of Alberta;University of Arizona;University of Alberta;University of Alberta,99;181;99;99,119;161;119;119,,10/27/17,2,1,0,0,0,0,61;48;459;1458,12;6;31;66,3;4;8;17,4;8;41;167,-1;-1
1243,ICLR,2018,Generation and Consolidation of Recollections for Efficient Deep Lifelong Learning,Matt Riemer;Michele Franceschini;and Tim Klinger,mdriemer@us.ibm.com;franceschini@us.ibm.com;tklinger@us.ibm.com,5;5;5,2;3;3,Reject,0,4,0,yes,10/27/17,International Business Machines;International Business Machines;International Business Machines,-1;-1;-1,-1;-1;-1,5,10/27/17,3,2,2,0,0,0,274;1596;619,27;49;34,9;15;11,25;232;112,-1;-1
1244,ICLR,2018,CyCADA: Cycle-Consistent Adversarial Domain Adaptation,Judy Hoffman;Eric Tzeng;Taesung Park;Jun-Yan Zhu;Phillip Isola;Kate Saenko;Alyosha Efros;Trevor Darrell,jhoffman@eecs.berkeley.edu;etzeng@eecs.berkeley.edu;taesung_park@berkeley.edu;junyanz@berkeley.edu;isola@eecs.berkeley.edu;saenko@bu.edu;efros@eecs.berkeley.edu;trevor@eecs.berkeley.edu,5;5;9,5;5;5,Reject,2,5,0,yes,10/27/17,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;Boston University;University of California Berkeley;University of California Berkeley,5;5;5;5;5;69;5;5,18;18;18;18;18;70;18;18,5;4;2,10/27/17,696,357,291,19,50,100,9269;7117;6744;15471;12266;16930;36507;88648,60;32;304;51;73;177;192;558,31;14;30;27;27;56;77;111,1101;812;491;3000;2143;2363;4565;11396,-1;-1
1245,ICLR,2018,The Manifold Assumption and Defenses Against Adversarial Perturbations,Xi Wu;Uyeong Jang;Lingjiao Chen;Somesh Jha,xiwu@cs.wisc.edu;wjang@cs.wisc.edu;lchen@cs.wisc.edu;jha@cs.wisc.edu,3;4;5,3;3;3,Reject,0,4,0,yes,10/27/17,University of Southern California;University of Southern California;University of Southern California;University of Southern California,31;31;31;31,66;66;66;66,4;8,10/27/17,4,0,1,0,0,0,365;54;285;18505,88;8;33;329,11;4;10;67,15;5;20;1792,-1;-1
1246,ICLR,2018,Graph2Seq: Scalable Learning Dynamics for Graphs,Shaileshh Bojja Venkatakrishnan;Mohammad Alizadeh;Pramod Viswanath,bjjvnkt@csail.mit.edu;alizadeh@csail.mit.edu;pramodv@illinois.edu,4;4;4,4;4;3,Reject,0,6,0,yes,9/27/18,"Massachusetts Institute of Technology;Massachusetts Institute of Technology;University of Illinois, Urbana Champaign",2;2;3,5;5;37,10;8,2/14/18,7,6,1,0,3,1,271;5176;16527,27;109;180,11;33;39,36;998;2159,-1;-1
1247,ICLR,2018,Reinforcement Learning from Imperfect Demonstrations,Yang Gao;Huazhe(Harry) Xu;Ji Lin;Fisher Yu;Sergey Levine;Trevor Darrell,yg@eecs.berkeley.edu;huazhe_xu@eecs.berkeley.edu;lin-j14@mails.tsinghua.edu.cn;fy@eecs.berkeley.edu;svlevine@eecs.berkeley.edu;trevor@eecs.berkeley.edu,5;6;5,3;5;4,Invite to Workshop Track,1,4,0,yes,10/27/17,University of California Berkeley;University of California Berkeley;Tsinghua University;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;10;5;5;5,18;18;30;18;18;18,,10/27/17,80,42,18,0,17,4,11323;798;1580;9682;24386;88648,920;13;52;44;309;558,43;7;18;25;73;111,681;99;178;1615;3167;11396,-1;-1
1248,ICLR,2018,FigureQA: An Annotated Figure Dataset for Visual Reasoning,Samira Ebrahimi Kahou;Adam Atkinson;Vincent Michalski;Ákos Kádár;Adam Trischler;Yoshua Bengio,samira.ebrahimi@microsoft.com;adatkins@microsoft.com;vincent.michalski@umontreal.ca;kadar.akos@gmail.com;adam.trischler@microsoft.com;yoshua.bengio@umontreal.ca,6;6;6,4;3;4,Invite to Workshop Track,0,10,0,yes,10/27/17,Microsoft;Microsoft;University of Montreal;;Microsoft;University of Montreal,-1;-1;124;-1;-1;124,-1;-1;108;-1;-1;108,10,10/19/17,43,25,14,4,24,15,3778;112;2570;340;1547;201719,69;14;56;18;47;807,14;5;12;8;17;147,375;25;235;42;287;23989,-1;-1
1249,ICLR,2018,Seq2SQL: Generating Structured Queries From Natural Language Using Reinforcement Learning ,Victor Zhong;Caiming Xiong;Richard Socher,victor@victorzhong.com;cxiong@salesforce.com;richard@socher.org,5;4;5,5;4;4,Reject,7,4,0,yes,10/27/17,SalesForce.com;SalesForce.com;SalesForce.com,-1;-1;-1,-1;-1;-1,3,10/27/17,251,163,119,8,169,92,1849;6230;52392,18;156;180,11;31;49,346;1054;8829,-1;-1
1250,ICLR,2018,Learning to select examples for program synthesis,Yewen Pu;Zachery Miranda;Armando Solar-Lezama;Leslie Pack Kaelbling,yewenpu@mit.edu;zmiranda@mit.edu;asolar@csail.mit.edu;lpk@csail.mit.edu,4;5;5,4;4;3,Reject,0,3,0,yes,10/27/17,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2,5;5;5;5,,10/27/17,1,0,0,0,0,0,217;23;3896;19915,20;5;130;364,8;2;29;53,13;0;395;1741,-1;-1
1251,ICLR,2018,Neuron as an Agent,Shohei Ohsawa;Kei Akuzawa;Tatsuya Matsushima;Gustavo Bezerra;Yusuke Iwasawa;Hiroshi Kajino;Seiya Takenaka;Yutaka Matsuo,ohsawa@weblab.t.u-tokyo.ac.jp;akuzawa-kei@weblab.t.u-tokyo.ac.jp;matsushima@weblab.t.u-tokyo.ac.jp;gustavo@weblab.t.u-tokyo.ac.jp;iwasawa@weblab.t.u-tokyo.ac.jp;kjn@jp.ibm.com;s.takenaka@aediworks.com;matsuo@weblab.t.u-tokyo.ac.jp,6;7;3,4;3;5,Invite to Workshop Track,6,5,1,yes,10/27/17,The University of Tokyo;The University of Tokyo;The University of Tokyo;The University of Tokyo;The University of Tokyo;International Business Machines;Aediworks;The University of Tokyo,52;52;52;52;52;-1;-1;52,45;45;45;45;45;-1;-1;45,,10/27/17,2,1,0,0,0,0,21;52;3;476;113;199;2;7438,14;10;5;125;39;24;2;382,3;2;1;12;5;7;1;34,3;5;0;28;10;22;0;506,-1;-1
1252,ICLR,2018,Toward predictive machine learning for active vision,Emmanuel Daucé,emmanuel.dauce@centrale-marseille.fr,5;3;3,2;4;5,Reject,0,5,0,yes,10/27/17,Centrale Marseille,-1,-1,2,10/27/17,3,1,0,0,5,0,182,48,6,12,-1
1253,ICLR,2018,Value Propagation Networks,Nantas Nardelli;Gabriel Synnaeve;Zeming Lin;Pushmeet Kohli;Nicolas Usunier,nantas@robots.ox.ac.uk;gab@fb.com;zlin@fb.com;pushmeet@google.com;usunier@fb.com,5;7;5,4;3;2,Invite to Workshop Track,0,3,0,yes,9/27/18,University of Oxford;Facebook;Facebook;Google;Facebook,51;-1;-1;-1;-1,1;-1;-1;-1;-1,,2/15/18,8,3,3,0,17,2,800;1487;5986;22034;6096,16;62;19;313;109,8;19;9;69;30,124;154;738;2746;1164,-1;-1
1254,ICLR,2018,Correcting Nuisance Variation using Wasserstein Distance,Gil Tabak;Minjie Fan;Samuel J. Yang;Stephan Hoyer;Geoff Davis,tabak.gil@gmail.com;mjfan@google.com;samuely@google.com;shoyer@google.com;geoffd@google.com,5;4;7,3;5;3,Reject,0,6,0,yes,10/27/17,Stanford University;Google;Google;Google;Google,4;-1;-1;-1;-1,3;-1;-1;-1;-1,,10/27/17,3,1,2,0,0,0,18;48;66;602;172,9;19;5;53;31,3;5;3;12;7,0;4;8;44;20,-1;-1
1255,ICLR,2018,Parametric Adversarial Divergences are Good Task Losses for Generative Modeling,Gabriel Huang;Hugo Berard;Ahmed Touati;Gauthier Gidel;Pascal Vincent;Simon Lacoste-Julien,gbxhuang@gmail.com;berard.hugo@gmail.com;ahmed.touati@umontreal.ca;gauthier.gidel@inria.fr;pascal.vincent@umontreal.ca;slacoste@iro.umontreal.ca,6;4;4,3;4;3,Invite to Workshop Track,0,12,1,yes,10/27/17,University of Montreal;University of Montreal;University of Montreal;INRIA;University of Montreal;University of Montreal,124;124;124;-1;124;124,108;108;108;-1;108;108,5;4,8/8/17,4,0,0,0,42,1,83;100;97;306;8124;6,6;4;17;21;51;7,4;3;6;10;13;2,17;20;11;47;656;1,-1;-1
1256,ICLR,2018,Adaptive Memory Networks,Daniel Li;Asim Kadav,li.daniel@berkeley.edu;asim@nec-labs.com,5;7;4,4;5;3,Invite to Workshop Track,2,7,0,yes,10/27/17,University of California Berkeley;NEC-Labs,5;-1,18;-1,,10/27/17,3,1,0,0,14,0,2702;1750,127;40,17;16,130;294,-1;-1
1257,ICLR,2018,Discrete Autoencoders for Sequence Models,Lukasz Kaiser;Samy Bengio,lukaszkaiser@google.com;bengio@google.com,5;4;6,5;4;1,Reject,0,4,0,yes,10/27/17,Google;Google,-1;-1,-1;-1,3,10/27/17,19,7,12,2,0,3,22378;26187,75;332,24;67,3865;3450,-1;-1
1258,ICLR,2018,“Style” Transfer for Musical Audio Using Multiple Time-Frequency Representations,Shaun Barry;Youngmoo Kim,smb484@drexel.edu;ykim@drexel.edu,6;4;7,4;4;3,Reject,0,2,0,yes,10/27/17,Drexel University;Drexel University,291;291,392;392,,10/27/17,8,5,5,1,0,2,8;1935,2;110,1;20,2;173,-1;-1
1259,ICLR,2018,Model-based imitation learning from state trajectories,Subhajit Chaudhury;Daiki Kimura;Tadanobu Inoue;Ryuki Tachibana,subhajit@jp.ibm.com;daiki@jp.ibm.com;inouet@jp.ibm.com;ryuki@jp.ibm.com,7;4;3,3;4;5,Reject,1,3,0,yes,10/27/17,International Business Machines;International Business Machines;International Business Machines;International Business Machines,-1;-1;-1;-1,-1;-1;-1;-1,,10/27/17,0,0,0,0,0,0,71;59;501;403,23;33;44;64,5;5;11;10,2;2;23;35,-1;-1
1260,ICLR,2018,Neural Task Graph Execution,Sungryull Sohn;Junhyuk Oh;Honglak Lee,srsohn@umich.edu;junhyuk@umich.edu;honglak@eecs.umich.edu,6;6;4,4;3;4,Reject,0,4,0,yes,10/27/17,University of Michigan;University of Michigan;University of Michigan,8;8;8,21;21;21,10,10/27/17,1,1,0,0,0,0,5;1351;23861,6;24;166,1;13;60,0;134;2806,-1;-1
1261,ICLR,2018,Efficient Exploration through Bayesian   Deep Q-Networks,Kamyar Azizzadenesheli;Emma Brunskill;Animashree Anandkumar,kazizzad@uci.edu;ebrun@cs.stanford.edu;animakumar@gmail.com,6;5;5,4;4;4,Reject,2,14,0,yes,10/27/17,"University of California, Irvine;Stanford University;University of California-Irvine",36;4;36,99;3;99,11,10/27/17,57,28,24,1,35,6,675;3097;5337,38;150;187,10;28;38,96;293;744,-1;-1
1262,ICLR,2018,UNSUPERVISED SENTENCE EMBEDDING USING DOCUMENT STRUCTURE-BASED CONTEXT,Taesung Lee;Youngja Park,taesung.lee@ibm.com;young_park@us.ibm.com,7;5;5,4;4;4,Reject,0,7,0,yes,10/27/17,International Business Machines;International Business Machines,-1;-1,-1;-1,,10/27/17,0,0,0,0,0,0,476;573,33;48,10;11,31;57,-1;-1
1263,ICLR,2018,Time-Dependent Representation for Neural Event Sequence Prediction,Yang Li;Nan Du;Samy Bengio,liyang@google.com;dunan@google.com;bengio@google.com,4;4;5,4;5;3,Invite to Workshop Track,0,4,0,yes,10/27/17,Google;Google;Google,-1;-1;-1,-1;-1;-1,3,7/31/17,21,9,8,0,0,2,2379;2124;26187,37;136;332,20;24;67,238;179;3450,-1;-1
1264,ICLR,2018,Directing Generative Networks with Weighted Maximum Mean Discrepancy,Maurice Diesendruck;Guy W. Cole;Sinead Williamson,momod@utexas.edu;guywcole@utexas.edu;sinead.williamson@mccombs.utexas.edu,4;4;4,4;4;5,Reject,0,3,0,yes,10/27/17,"University of Texas, Austin;University of Texas, Austin;University of Texas, Austin",21;21;21,49;49;49,5;7,10/27/17,0,0,0,0,0,0,11;5;538,4;5;43,2;2;10,2;0;81,-1;-1
1265,ICLR,2018,Decoding Decoders: Finding Optimal Representation Spaces for Unsupervised Similarity Tasks,Vitalii Zhelezniak;Dan Busbridge;April Shen;Samuel L. Smith;Nils Y. Hammerla,vitali.zhelezniak@babylonhealth.com;dan.busbridge@babylonhealth.com;april.shen@babylonhealth.com;slsmith@google.com;nils.hammerla@babylonhealth.com,6;5;4,4;5;4,Invite to Workshop Track,0,6,0,yes,10/27/17,babylon health;babylon health;babylon health;Google;babylon health,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,3,10/27/17,3,2,0,0,18,0,25;12;25;1178;2297,9;4;7;26;43,3;2;3;11;19,2;1;2;109;216,-1;-1
1266,ICLR,2018,Learning Parametric Closed-Loop Policies for Markov Potential Games,Sergio Valcarcel Macua;Javier Zazo;Santiago Zazo,sergio@prowler.io;javier.zazo.ruiz@upm.es;santiago@gaps.ssr.upm.es,7;6;6,2;3;1,Accept (Poster),0,4,0,yes,10/27/17,Prowler.io;Universidad Politécnica de Madrid;Universidad Politécnica de Madrid,-1;-1;-1,-1;-1;-1,,10/27/17,3,2,1,0,0,0,191;75;1117,30;31;172,7;5;17,15;2;63,-1;-1
1267,ICLR,2018,Soft Value Iteration Networks for Planetary Rover Path Planning,Max Pflueger;Ali Agha;Gaurav S. Sukhatme,mpflueger@gmail.com;aliahga@jpl.nasa.gov;gaurav@usc.edu,3;3;4,5;3;4,Reject,0,3,0,yes,10/27/17,University of Southern California;NASA;University of Southern California,31;-1;31,66;-1;66,,10/27/17,6,3,0,0,0,0,14;6;19214,7;7;615,2;1;69,0;0;984,-1;-1
1268,ICLR,2018,A Classification-Based Perspective on GAN Distributions,Shibani Santurkar;Ludwig Schmidt;Aleksander Madry,shibani@mit.edu;ludwigs@mit.edu;madry@mit.edu,5;6;3,5;4;4,Reject,0,3,0,yes,10/27/17,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2,5;5;5,5;4,10/27/17,13,8,2,1,0,0,1486;3694;5359,26;198;84,14;21;29,173;885;1068,-1;-1
1269,ICLR,2018,Inference Suboptimality in Variational Autoencoders,Chris Cremer;Xuechen Li;David Duvenaud,ccremer@cs.toronto.edu;lxuechen@cs.toronto.edu;duvenaud@cs.toronto.edu,6;6;6,5;5;4,Invite to Workshop Track,0,4,0,yes,10/27/17,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",17;17;17,22;22;22,5;1,10/27/17,86,58,16,3,31,14,153;576;5869,10;37;74,3;11;28,18;94;747,-1;-1
1270,ICLR,2018,Achieving Strong Regularization for Deep Neural Networks,Dae Hoon Park;Chiu Man Ho;Yi Chang,pdhvip@gmail.com;chiuman100@gmail.com;yi.chang@huawei.com,6;4;5,2;5;5,Reject,0,5,0,yes,10/27/17,Huawei Technologies Ltd.;;Huawei Technologies Ltd.,-1;-1;-1,-1;-1;-1,8,10/27/17,3,2,1,0,0,0,326;302;31,36;59;13,10;10;3,19;5;4,-1;-1
1271,ICLR,2018,A Spectral Approach to Generalization and Optimization in Neural Networks,Farzan Farnia;Jesse Zhang;David Tse,farnia@stanford.edu;jessez@stanford.edu;dntse@stanford.edu,6;6;4,3;3;4,Reject,1,11,0,yes,10/27/17,Stanford University;Stanford University;Stanford University,4;4;4,3;3;3,1;8,10/27/17,3,2,1,0,0,1,181;138;46007,19;11;337,6;5;70,23;11;5818,-1;-1
1272,ICLR,2018,Small Coresets to Represent Large Training Data for Support Vector Machines,Cenk Baykal;Murad Tukan;Dan Feldman;Daniela Rus,baykal@mit.edu;muradtuk@gmail.com;dannyf.post@gmail.com;rus@csail.mit.edu,5;7;5,3;3;4,Reject,0,8,0,yes,10/27/17,Massachusetts Institute of Technology;University of Haifa;University of Haifa;Massachusetts Institute of Technology,2;153;153;2,5;608;608;5,,10/27/17,1,1,1,0,0,0,129;2;1785;19781,22;6;112;503,6;1;20;77,8;0;126;1010,-1;-1
1273,ICLR,2018,Investigating Human Priors for Playing Video Games,Rachit Dubey;Pulkit Agrawal;Deepak Pathak;Thomas L. Griffiths;Alexei A. Efros,rach0012@berkeley.edu;pulkitag@berkeley.edu;pathak@berkeley.edu;tom_griffiths@berkeley.edu;efros@eecs.berkeley.edu,4;5;7,3;4;4,Invite to Workshop Track,0,6,0,yes,10/27/17,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5;5,18;18;18;18;18,,10/27/17,55,34,2,0,0,1,148;2961;4130;21330;36507,16;52;40;439;192,5;16;14;70;77,10;242;546;2169;4565,-1;-1
1274,ICLR,2018,Learning Deep Models: Critical Points and Local Openness,Maher Nouiehed;Meisam Razaviyayn,nouiehed@usc.edu;razaviya@usc.edu,6;5;6,4;4;4,Invite to Workshop Track,0,3,0,yes,10/27/17,University of Southern California;University of Southern California,31;31,66;66,,10/27/17,17,10,1,0,0,1,97;3504,11;83,5;23,9;474,-1;-1
1275,ICLR,2018,Contextual Explanation Networks,Maruan Al-Shedivat;Avinava Dubey;Eric P. Xing,alshedivat@cs.cmu.edu;akdubey@cs.cmu.edu;epxing@cs.cmu.edu,6;6;6,5;2;3,Reject,0,4,0,yes,10/27/17,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1,24;24;24,1;10,5/29/17,28,9,12,0,104,2,731;534;24050,34;37;603,13;13;75,79;58;2652,-1;-1
1276,ICLR,2018,LSH Softmax: Sub-Linear Learning and Inference of the Softmax Layer in Deep Architectures,Daniel Levy;Danlu Chan;Stefano Ermon,danilevy@cs.stanford.edu;taineleau@gmail.com;ermon@cs.stanford.edu,5;5;5,4;3;4,Reject,7,3,0,yes,10/27/17,Stanford University;Fudan University;Stanford University,4;78;4,3;116;3,3,10/27/17,0,0,0,0,0,0,62912;0;4821,489;1;203,123;0;31,2463;0;646,-1;-1
1277,ICLR,2018,Recurrent Relational Networks for complex relational reasoning,Rasmus Berg Palm;Ulrich Paquet;Ole Winther,rasmusbergpalm@gmail.com;upaq@google.com;olwi@dtu.dk,3;5;5,5;3;3,Reject,0,4,0,yes,10/27/17,Technical University of Denmark;Google;Technical University of Denmark,210;-1;210,153;-1;153,,10/27/17,21,14,4,2,0,4,482;1044;6051,31;51;201,8;18;34,42;134;711,-1;-1
1278,ICLR,2018,Influence-Directed Explanations for Deep Convolutional Networks,Anupam Datta;Matt Fredrikson;Klas Leino;Linyi Li;Shayak Sen,danupam@cmu.edu;mfredrik@cs.cmu.edu;kleino@cs.cmu.edu;ly-li14@mails.tsinghua.edu.cn;shayaks@cs.cmu.edu,5;4;4,3;5;3,Reject,0,0,0,yes,10/27/17,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Tsinghua University;Carnegie Mellon University,1;1;1;10;1,24;24;24;30;24,,10/27/17,20,13,8,0,9,3,4173;3479;33;77;468,165;70;5;12;21,31;22;3;3;8,349;413;4;4;33,-1;-1
1279,ICLR,2018,Recurrent Neural Networks with Top-k Gains for Session-based Recommendations,Balázs Hidasi;Alexandros Karatzoglou,hidasib@gmail.com;alexk@tid.es,8;4;6,4;5;5,Reject,0,3,0,yes,10/27/17,Gravity R&D;Telefonica Research,-1;-1,-1;-1,,6/12/17,132,57,55,1,60,19,2903;4928,28;91,10;28,364;610,-1;-1
1280,ICLR,2018, Explicit Induction Bias for Transfer Learning with Convolutional Networks,Xuhong LI;Yves GRANDVALET;Franck DAVOINE,xuhong.li@utc.fr;yves.grandvalet@utc.fr;franck.davoine@utc.fr,6;7;6,5;4;4,Reject,0,4,1,yes,10/27/17,Université de technologie de Compiègne;Université de technologie de Compiègne;Université de technologie de Compiègne,-1;-1;-1,-1;-1;-1,6,10/27/17,40,22,21,0,3,15,207;3585;1230,36;114;112,6;24;18,26;424;90,-1;-1
1281,ICLR,2018,Simple Nearest Neighbor Policy Method for Continuous Control Tasks,Elman Mansimov;Kyunghyun Cho,mansimov@cs.nyu.edu;kyunghyun.cho@nyu.edu,4;4;3,5;4;5,Reject,0,6,0,yes,10/27/17,New York University;New York University,26;26,27;27,,10/27/17,3,2,1,0,0,0,1859;45405,11;273,7;52,241;6560,-1;-1
1282,ICLR,2018,Deep Mean Field Theory: Layerwise Variance and Width Variation as Methods to Control Gradient Explosion,Greg Yang;Sam S. Schoenholz,gregyang@microsoft.com;schsam@google.com,7;5;5,3;1;3,Invite to Workshop Track,0,6,0,yes,10/27/17,Microsoft;Google,-1;-1,-1;-1,,10/27/17,14,6,4,1,0,0,496;3006,24;70,10;21,53;376,-1;-1
1283,ICLR,2018,Optimizing the Latent Space of Generative Networks,Piotr Bojanowski;Armand Joulin;David Lopez-Paz;Arthur Szlam,bojanowski@fb.com;ajoulin@fb.com;dlp@fb.com;aszlam@fb.com,4;6;6,4;4;3,Reject,0,3,0,yes,10/27/17,Facebook;Facebook;Facebook;Facebook,-1;-1;-1;-1,-1;-1;-1;-1,5;4,7/18/17,118,43,45,1,262,19,7595;10425;2393;8722,41;74;45;86,20;32;19;32,1120;1521;413;921,-1;-1
1284,ICLR,2018,Accelerating Neural Architecture Search using Performance Prediction,Bowen Baker*;Otkrist Gupta*;Ramesh Raskar;Nikhil Naik,bowen@mit.edu;otkrist@mit.edu;raskar@mit.edu;naik@mit.edu,6;6;4,4;5;3,Invite to Workshop Track,0,5,0,yes,10/27/17,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2,5;5;5;5,3;11,5/30/17,123,60,51,5,21,23,1233;1417;16582;1080,13;31;555;25,7;12;63;11,145;154;1118;124,-1;-1
1285,ICLR,2018,Data-efficient Deep Reinforcement Learning for Dexterous Manipulation,Ivo Popov;Nicolas Heess;Timothy P. Lillicrap;Roland Hafner;Gabriel Barth-Maron;Matej Vecerik;Thomas Lampe;Tom Erez;Yuval Tassa;Martin Riedmiller,ivaylo.popov@hotmail.com;heess@google.com;countzero@google.com;rhafner@google.com;gabrielbm@google.com;matejvecerik@google.com;thomaslampe@google.com;etom@google.com;tassa@google.com;riedmiller@google.com,4;2;3,4;5;4,Reject,1,0,0,yes,10/27/17,;Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,,4/10/17,110,66,20,1,0,4,160;11371;24363;710;587;901;1080;6431;7095;25262,58;104;74;24;14;6;85;48;45;190,5;37;39;10;8;6;15;19;24;40,8;1617;2962;32;72;86;51;1079;1177;3523,-1;-1
1286,ICLR,2018,Routing Networks: Adaptive Selection of Non-Linear Functions for Multi-Task Learning,Clemens Rosenbaum;Tim Klinger;Matthew Riemer,crosenbaum@umass.edu;tklinger@us.ibm.com;mdriemer@us.ibm.com,7;6;8,3;3;4,Accept (Poster),0,9,0,yes,10/27/17,"University of Massachusetts, Amherst;International Business Machines;International Business Machines",30;-1;-1,191;-1;-1,,10/27/17,54,36,21,2,0,10,478;619;274,70;34;27,10;11;9,35;112;25,-1;-1
1287,ICLR,2018,Gradients explode - Deep Networks are shallow - ResNet explained,George Philipp;Dawn Song;Jaime G. Carbonell,george.philipp@email.de;dawnsong@gmail.com;jgc@cs.cmu.edu,3;5;8,2;4;1,Invite to Workshop Track,5,6,2,yes,10/27/17,Carnegie Mellon University;University of California Berkeley;Carnegie Mellon University,1;5;1,24;18;24,,10/27/17,13,9,5,0,0,3,52;36706;15711,13;276;507,5;95;55,8;4088;1628,-1;-1
1288,ICLR,2018,GraphVAE: Towards Generation of Small Graphs Using Variational Autoencoders,Martin Simonovsky;Nikos Komodakis,simonovm@imagine.enpc.fr;nikos.komodakis@enpc.fr,5;7;7,3;2;4,Reject,0,13,0,yes,10/27/17,ENPC;ENPC,468;468,280;280,5;10,10/27/17,175,79,64,2,29,26,835;9146,13;121,5;38,116;1297,-1;-1
1289,ICLR,2018,UPS: optimizing Undirected Positive Sparse graph for neural graph filtering,Mikhail Yurochkin;Dung Thai;Hung Hai Bui;XuanLong Nguyen,moonfolk@umich.edu;dthai@iesl.cs.umass.edu;bui.h.hung@gmail.com;xuanlong@umich.edu,6;3;4,3;3;3,Reject,0,4,0,yes,10/27/17,"University of Michigan;University of Massachusetts, Amherst;Google;University of Michigan",8;30;-1;8,21;191;-1;21,10,10/27/17,0,0,0,0,0,0,136;64;2871;1909,27;17;104;79,6;5;24;19,7;2;186;154,-1;-1
1290,ICLR,2018,Learning Document Embeddings With CNNs,Shunan Zhao;Chundi Lui;Maksims Volkovs,shunan@layer6.ai;chundi@layer6.ai;maksims.volkovs@gmail.com,6;4;2,4;3;5,Reject,3,6,0,yes,10/27/17,Layer 6 AI;Layer 6 AI;,-1;-1;-1,-1;-1;-1,3,10/27/17,2,0,0,0,2,0,192;19;533,27;13;32,8;2;11,18;1;64,-1;-1
1291,ICLR,2018,Predict Responsibly: Increasing Fairness by Learning to Defer,David Madras;Toniann Pitassi;Richard Zemel,david.madras@mail.utoronto.ca;zemel@cs.toronto.edu;toni@cs.toronto.edu,5;4;6,3;5;3,Invite to Workshop Track,0,5,0,yes,10/27/17,"Toronto University;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",17;17;17,22;22;22,7,10/27/17,7,5,2,0,13,1,216;6861;21386,13;202;208,5;44;52,34;713;2479,-1;-1
1292,ICLR,2018,Reward Estimation via State Prediction,Daiki Kimura;Subhajit Chaudhury;Ryuki Tachibana;Sakyasingha Dasgupta,daiki@jp.ibm.com;subhajit@jp.ibm.com;ryuki@jp.ibm.com;sakya@leapmind.io,4;5;3,4;3;4,Reject,0,3,0,yes,10/27/17,"International Business Machines;International Business Machines;International Business Machines;LeapMind, Inc.",-1;-1;-1;-1,-1;-1;-1;-1,5,10/27/17,0,0,0,0,0,0,59;71;403;279,33;23;64;44,5;5;10;11,2;2;35;8,-1;-1
1293,ICLR,2018,Sparse Attentive Backtracking: Long-Range Credit Assignment in Recurrent Networks,Nan Rosemary Ke;Anirudh Goyal;Olexa Bilaniuk;Jonathan Binas;Laurent Charlin;Chris Pal;Yoshua Bengio,rosemary.nan.ke@gmail.com;anirudhgoyal9119@gmail.com;obilaniu@gmail.com;jbinas@gmail.com;lcharlin@gmail.com;chris.j.pal@gmail.com;yoshua.umontreal@gmail.com,5;5;8,3;4;4,Reject,0,7,0,yes,10/27/17,Polytechnique Montreal;University of Montreal;University of Montreal;University of Montreal;University of Montreal;Ecole Polytechnique de Montreal;University of Montreal,364;124;124;124;124;365;124,108;108;108;108;108;1103;108,,10/27/17,9,3,5,0,61,0,744;1110;272;658;2427;8128;201719,32;46;14;29;41;120;807,13;12;7;10;20;33;147,76;127;36;91;322;758;23989,-1;-1
1294,ICLR,2018,GraphGAN: Generating Graphs via Random Walks,Aleksandar Bojchevski;Oleksandr Shchur;Daniel Zügner;Stephan Günnemann,a.bojchevski@in.tum.de;shchur@in.tum.de;daniel.zuegner@gmail.com;guennemann@in.tum.de,6;7;4,4;4;5,Reject,4,12,0,yes,10/27/17,Technical University Munich;Technical University Munich;Technical University Munich;Technical University Munich,55;55;55;55,41;41;41;41,10;5;8,10/27/17,94,56,27,2,37,18,576;247;275;2437,21;12;13;138,12;4;6;28,105;36;64;278,-1;-1
1295,ICLR,2018,LEAP: Learning Embeddings for Adaptive Pace,Vithursan Thangarasa;Graham W. Taylor,vthangar@uoguelph.ca;gwtaylor@uoguelph.ca,6;3;4,3;4;4,Reject,0,2,0,yes,10/27/17,University of Guelph;University of Guelph,291;291,1103;1103,,10/27/17,0,0,0,0,0,0,5;5768,6;143,1;31,0;488,-1;-1
1296,ICLR,2018,Prototype Matching Networks for Large-Scale Multi-label  Genomic Sequence Classification,Jack Lanchantin;Arshdeep Sekhon;Ritambhara Singh;Yanjun Qi,jjl5sw@virginia.edu;as5cu@virginia.edu;rs3zz@virginia.edu;yq2h@virginia.edu,5;5;5,4;5;3,Reject,0,15,0,yes,10/27/17,University of Virginia;University of Virginia;University of Virginia;University of Virginia,62;62;62;62,113;113;113;113,6,10/27/17,30,0,0,0,30,0,904;44;461;3076,24;12;37;114,8;3;8;25,33;2;23;261,-1;-1
1297,ICLR,2018,Forced Apart: Discovering Disentangled Representations Without Exhaustive Labels,Alexey Romanov;Anna Rumshisky,jgc128@outlook.com;arum@cs.uml.edu,5;5;4,5;4;4,Reject,0,4,0,yes,10/27/17,"University of Massachusetts, Lowell;University of Massachusetts, Lowell",-1;-1,-1;-1,,5/1/17,2,0,1,0,9,0,302;1500,27;91,10;19,39;124,-1;-1
1298,ICLR,2018,Beyond Shared Hierarchies: Deep Multitask Learning through Soft Layer Ordering,Elliot Meyerson;Risto Miikkulainen,ekm@cs.utexas.edu;risto@cs.utexas.edu,7;6;7,4;3;4,Accept (Poster),0,5,0,yes,10/27/17,"University of Texas, Austin;University of Texas, Austin",21;21,49;49,,10/27/17,44,28,14,1,17,4,580;12600,23;441,10;49,39;1316,-1;-1
1299,ICLR,2018,Attention-based Graph Neural Network for Semi-supervised Learning,Kiran K. Thekumparampil;Sewoong Oh;Chong Wang;Li-Jia Li,kirankoshy@gmail.com;sewoong79@gmail.com;chongw@google.com;lijiali@cs.stanford.edu,6;6;7,3;2;4,Reject,4,5,0,yes,10/27/17,"University of Illinois, Urbana Champaign;University of Illinois at Urbana-Champaign;Google;Stanford University",3;3;-1;4,37;37;-1;3,10,10/27/17,55,28,17,0,27,11,160;4388;18144;23871,12;86;1045;71,7;28;55;32,20;484;1634;4300,-1;-1
1300,ICLR,2018,Attacking Binarized Neural Networks,Angus Galloway;Graham W. Taylor;Medhat Moussa,gallowaa@uoguelph.ca;gwtaylor@uoguelph.ca;mmoussa@uoguelph.ca,7;7;6,3;4;5,Accept (Poster),1,3,0,yes,10/27/17,University of Guelph;University of Guelph;University of Guelph,291;291;291,1103;1103;1103,4,10/27/17,33,23,6,1,8,6,84;5768;398,12;143;43,5;31;12,10;488;37,-1;-1
1301,ICLR,2018,A Semantic Loss Function for Deep Learning with Symbolic Knowledge,Jingyi Xu;Zilu Zhang;Tal Friedman;Yitao Liang;Guy Van den Broeck,jixu@g.ucla.edu;zhangzilu@pku.edu.cn;tal@cs.ucla.edu;yliang@cs.ucla.edu;guyvdb@cs.ucla.edu,7;5;4,3;3;4,Reject,0,3,0,yes,10/27/17,"University of California, Los Angeles;Peking University;University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles",20;24;20;20;20,15;27;15;15;15,,10/27/17,76,41,14,1,22,5,158;230;152;251;1913,28;20;10;16;131,5;4;4;4;26,14;21;7;23;158,-1;-1
1302,ICLR,2018,Representing Entropy : A short proof of the equivalence between soft Q-learning and policy gradients,Pierre H. Richemond;Brendan Maginnis,phr17@imperial.ac.uk;b.maginnis@imperial.ac.uk,5;2;5,5;5;4,Reject,0,3,0,yes,10/27/17,Imperial College London;Imperial College London,74;74,8;8,1,10/27/17,3,1,0,1,8,1,13;32,11;7,2;3,2;3,-1;-1
1303,ICLR,2018,Generative Discovery of Relational Medical Entity Pairs,Chenwei Zhang;Yaliang Li;Nan Du;Wei Fan;Philip S. Yu,czhang99@uic.edu;yaliangli@baidu.com;nandu@baidu.com;davidwfan@tencent.com;psyu@uic.edu,4;4;2,3;4;5,Reject,0,3,1,yes,10/27/17,"University of Illinois, Chicago;Baidu;Baidu;Tencent AI Lab;University of Illinois, Chicago",57;-1;-1;-1;57,255;-1;-1;-1;255,5,10/27/17,1,0,0,0,0,0,570;1590;30;14;60254,80;114;11;14;1579,12;18;2;2;111,34;164;2;2;5758,-1;-1
1304,ICLR,2018,Understanding GANs: the LQG Setting,Soheil Feizi;Changho Suh;Fei Xia;David Tse,sfeizi@stanford.edu;chsuh@kaist.ac.kr;feixia@stanford.edu;dntse@stanford.edu,4;4;5,4;5;4,Reject,8,2,0,yes,10/27/17,Stanford University;Korea Advanced Institute of Science and Technology;Stanford University;Stanford University,4;21;4;4,3;95;3;3,5;4,10/27/17,35,18,4,1,4,6,501;2677;3553;46007,38;103;329;337,13;23;27;70,55;286;398;5818,-1;-1
1305,ICLR,2018,Synthesizing Robust Adversarial Examples,Anish Athalye;Logan Engstrom;Andrew Ilyas;Kevin Kwok,aathalye@mit.edu;engstrom@mit.edu;ailyas@mit.edu;kevink16@gmail.com,5;6;8,4;4;3,Reject,9,8,0,yes,10/27/17,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2,5;5;5;5,4,7/24/17,513,336,99,7,807,46,2172;1998;1884;542,19;27;28;9,10;15;15;4,320;264;262;48,-1;-1
1306,ICLR,2018,Extending the Framework of Equilibrium Propagation to General Dynamics,Benjamin Scellier;Anirudh Goyal;Jonathan Binas;Thomas Mesnard;Yoshua Bengio,benjamin.scellier@polytechnique.edu;anirudhgoyal9119@gmail.com;jbinas@gmail.com;thomas.mesnard@gmail.com;yoshua.umontreal@gmail.com,4;3;6,4;4;2,Invite to Workshop Track,0,3,0,yes,10/27/17,Ecole polytechnique;University of Montreal;University of Montreal;University of Montreal;University of Montreal,468;124;124;124;124,115;108;108;108;108,8,10/27/17,7,5,1,3,0,0,276;1110;658;101;201719,13;46;29;14;807,7;12;10;6;147,26;127;91;9;23989,-1;-1
1307,ICLR,2018,Lifelong Word Embedding via Meta-Learning,Hu Xu;Bing Liu;Lei Shu;Philip S. Yu,hxu48@uic.edu;liub@uic.edu;lshu3@uic.edu;psyu@uic.edu,4;5;3,4;4;4,Reject,0,0,0,yes,10/27/17,"University of Illinois, Chicago;University of Illinois, Chicago;University of Illinois, Chicago;University of Illinois, Chicago",57;57;57;57,255;255;255;255,3;6,10/27/17,15,11,4,0,2,0,398;108;374;63985,80;35;28;1582,8;5;9;115,56;8;56;5844,-1;-1
1308,ICLR,2018,A comparison of second-order methods for deep convolutional neural networks,Patrick H. Chen;Cho-jui Hsieh,phpchen@ucdavis.edu;chohsieh@ucdavis.edu,5;6;3,5;3;4,Reject,0,3,0,yes,10/27/17,"University of California, Davis;University of California, Davis",78;78,54;54,,10/27/17,1,1,0,0,0,0,32;12522,6;168,3;41,5;1726,-1;-1
1309,ICLR,2018,Automatic Parameter Tying in Neural Networks,Yibo Yang;Nicholas Ruozzi;Vibhav Gogate,yibo.yang@utdallas.edu;nicholas.ruozzi@utdallas.edu;vgogate@hlt.utdallas.edu,6;6;6,5;4;4,Reject,0,4,0,yes,10/27/17,"University of Texas, Dallas;University of Texas, Dallas;University of Texas, Dallas",85;85;85,239;239;239,,10/27/17,0,0,0,0,0,0,101;216;1294,18;35;81,6;8;21,3;24;122,-1;-1
1310,ICLR,2018,Learning to Treat Sepsis with Multi-Output Gaussian Process Deep Recurrent Q-Networks,Joseph Futoma;Anthony Lin;Mark Sendak;Armando Bedoya;Meredith Clement;Cara O'Brien;Katherine Heller,jfutoma14@gmail.com;anthony.lin@duke.edu;mark.sendak@duke.edu;armando.bedoya@duke.edu;meredith.edwards@duke.edu;cara.obrien@duke.edu;kheller@gmail.com,6;3;4,3;4;4,Reject,0,8,0,yes,10/27/17,Duke University;Duke University;Duke University;Duke University;Duke University;Duke University;Duke University,46;46;46;46;46;46;46,17;17;17;17;17;17;17,,10/27/17,8,6,5,0,0,4,264;766;158;56;275;603;2033,20;72;27;10;34;27;91,6;13;7;2;8;12;26,26;73;12;11;28;39;208,-1;-1
1311,ICLR,2018,Latent forward model for Real-time Strategy game planning with incomplete information,Yuandong Tian;Qucheng Gong,yuandong@fb.com;qucheng@fb.com,5;4;4,4;5;3,Reject,0,1,0,yes,10/27/17,Facebook;Facebook,-1;-1,-1;-1,,10/27/17,3,2,0,0,0,0,2435;126,84;8,25;4,285;5,-1;-1
1312,ICLR,2018,WHAI: Weibull Hybrid Autoencoding Inference for Deep Topic Modeling,Hao Zhang;Bo Chen;Dandan Guo;Mingyuan Zhou,zhanghao_xidian@163.com;bchen@mail.xidian.edu.cn;gdd_xidian@126.com;mzhou@utexas.edu,6;6;5,4;2;4,Accept (Poster),0,3,1,yes,10/27/17,"Xidian University;Tsinghua University;Xidian University;University of Texas, Austin",468;10;468;21,917;30;917;49,5;1,10/27/17,25,12,9,0,2,4,936;7126;71;1969,171;682;12;115,16;39;5;24,41;465;5;233,-1;-1
1313,ICLR,2018,Faster Distributed Synchronous SGD with Weak Synchronization,Cong Xie;Oluwasanmi O. Koyejo;Indranil Gupta,cx2@illinois.edu;sanmi@illinois.edu;indy@illinois.edu,4;3;4,5;4;5,Reject,0,0,0,yes,10/27/17,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",3;3;3,37;37;37,,10/27/17,2,0,0,0,0,0,230;1195;4302,18;96;210,9;17;34,40;122;366,-1;-1
1314,ICLR,2018,Deep Learning for Physical Processes: Incorporating Prior Scientific Knowledge,Emmanuel de Bezenac;Arthur Pajot;Patrick Gallinari,emmanuel.de_bezenac@lip6.fr;arthur.pajot@lip6.fr;patrick.gallinari@lip6.fr,7;7;6,3;3;2,Accept (Poster),6,4,2,yes,10/27/17,LIP6;LIP6;LIP6,-1;-1;-1,-1;-1;-1,,10/27/17,56,26,17,0,67,8,84;78;4777,9;9;450,3;3;33,9;9;378,-1;-1
1315,ICLR,2018,DeepArchitect: Automatically Designing and Training Deep Architectures,Renato Negrinho;Geoff Gordon,negrinho@cs.cmu.edu;ggordon@cs.cmu.edu,4;5;4,5;3;5,Reject,0,6,0,yes,10/27/17,Carnegie Mellon University;Carnegie Mellon University,1;1,24;24,10,4/28/17,104,44,36,0,0,6,136;10541,7;186,4;48,10;1211,-1;-1
1316,ICLR,2018,Improving generalization by regularizing in $L^2$ function space,Ari S Benjamin;Konrad Kording,aarrii@seas.upenn.edu,6;5;4,3;4;3,Reject,0,1,0,yes,10/27/17,University of Pennsylvania,19,10,8,10/27/17,0,0,0,0,0,0,76;9814,13;286,5;31,5;583,-1;-1
1317,ICLR,2018,"Fast and Accurate Text Classification: Skimming, Rereading and Early Stopping",Keyi Yu;Yang Liu;Alexander G. Schwing;Jian Peng,yu-ky14@mails.tsinghua.edu.cn;liu301@illinois.edu;aschwing@illinois.edu;jianpeng@illinois.edu,5;5;7,4;3;4,Invite to Workshop Track,3,6,0,yes,10/27/17,"Tsinghua University;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",10;3;3;3,30;37;37;37,3,10/27/17,9,6,4,0,0,1,30;279;3703;2080,10;52;116;120,4;8;31;23,2;27;340;204,-1;-1
1318,ICLR,2018,Gaussian Process Neurons,Sebastian Urban;Patrick van der Smagt,surban@tum.de;smagt@brml.org,5;4;7,4;5;2,Reject,0,1,0,yes,10/27/17,Technical University Munich;TU Munich,55;55,41;34,11,10/27/17,2,1,1,0,9,0,1764;3888,25;121,8;26,142;412,-1;-1
1319,ICLR,2018,Neural Tree Transducers for Tree to Tree Learning,João Sedoc;Dean Foster;Lyle Ungar,joao@cis.upenn.edu;dean@foster.net;ungar@cis.upenn.edu,3;7;2,4;4;5,Reject,0,0,0,yes,10/27/17,University of Pennsylvania;University of Pennsylvania;University of Pennsylvania,19;19;19,10;10;10,,10/27/17,1,1,0,0,0,0,127;6183;13388,29;197;404,7;41;58,13;685;962,-1;-1
1320,ICLR,2018,Network of Graph Convolutional Networks Trained on Random Walks,Sami Abu-El-Haija;Amol Kapoor;Bryan Perozzi;Joonseok Lee,haija@google.com;ajk2227@columbia.edu;bperozzi@acm.org;joonseok@google.com,5;5;6,2;4;5,Reject,2,7,0,yes,10/27/17,Google;Columbia University;;Google,-1;15;-1;-1,-1;14;-1;-1,10;4;8,10/27/17,0,0,0,0,0,0,875;103;4447;1186,18;7;48;38,8;3;19;12,122;16;1095;164,-1;-1
1321,ICLR,2018,LSH-SAMPLING BREAKS THE COMPUTATIONAL CHICKEN-AND-EGG LOOP IN ADAPTIVE STOCHASTIC GRADIENT ESTIMATION,Beidi Chen;Yingchen Xu;Anshumali Shrivastava,beidi.chen@rice.edu;yingchen.xu@rice.edu;anshumali@rice.edu,8;4;4,4;5;5,Invite to Workshop Track,0,10,0,yes,10/27/17,Rice University;Rice University;Rice University,85;85;85,86;86;86,,10/27/17,7,2,2,0,0,0,88;20;1097,15;6;100,4;3;16,4;0;110,-1;-1
1322,ICLR,2018,Deep Boosting of Diverse Experts,Wei Zhang;Qiuyu Chen;Jun Yu;Jianping Fan,weizh@fudan.edu.cn;qchen12@uncc.edu;yujun@hdu.edu.cn;jfan@uncc.edu,2;6;5,5;3;4,Reject,0,2,0,yes,10/27/17,"Fudan University;University of North Carolina, Charlotte;Shandong University;University of North Carolina, Charlotte",78;74;4;74,116;1103;3;1103,,10/27/17,0,0,0,0,0,0,2852;398;162;1188,497;54;33;92,25;8;5;17,192;2;41;84,-1;-1
1323,ICLR,2018,Bounding and Counting Linear Regions of Deep Neural Networks,Thiago Serra;Christian Tjandraatmadja;Srikumar Ramalingam,tserra@gmail.com;ctjandra@andrew.cmu.edu;srikumar.ramalingam@gmail.com,6;4;6,5;5;3,Reject,0,6,0,yes,10/27/17,Carnegie Mellon University;Carnegie Mellon University;University of Utah,1;1;52,24;24;200,,10/27/17,58,38,15,3,46,9,110;123;2762,21;17;106,6;5;26,10;20;314,-1;-1
1324,ICLR,2018,Jointly Learning to Construct and Control Agents using Deep Reinforcement Learning,Charles Schaff;David Yunis;Ayan Chakrabarti;Matthew R. Walter,cbschaff@ttic.edu;dyunis@uchicago.edu;ayan@wustl.edu;mwalter@ttic.edu,9;4;5,5;4;3,Invite to Workshop Track,0,3,0,yes,10/27/17,"Toyota Technological Institute at Chicago;University of Chicago;Washington University, St. Louis;Toyota Technological Institute at Chicago",-1;46;104;-1,-1;9;50;-1,,10/27/17,9,2,7,1,5,1,11;11;1358;3093,3;2;47;77,2;2;17;27,2;2;155;186,-1;-1
1325,ICLR,2018,Tandem Blocks in Deep Convolutional Neural Networks,Chris Hettinger;Tanner Christensen;Jeff Humpherys;Tyler J Jarvis,chrishettinger@gmail.com;tkchristensen@byu.edu;jeffh@math.byu.edu;jarvis@math.byu.edu,5;7;4,4;4;4,Reject,0,12,0,yes,10/27/17,Brigham Young University;Brigham Young University;Brigham Young University;Brigham Young University,-1;-1;-1;-1,-1;-1;-1;-1,,10/27/17,2,0,0,0,2,0,40;29;598;110,3;4;46;8,2;3;14;6,8;3;41;16,-1;-1
1326,ICLR,2018,On the difference between building and extracting patterns: a causal analysis of deep generative models.,Michel Besserve;Dominik Janzing;Bernhard Schoelkopf,michel.besserve@tuebingen.mpg.de;dominik.janzing@tuebingen.mpg.de;bs@tuebingen.mpg.de,2;7;7,4;3;2,Reject,0,0,0,yes,10/27/17,"Max Planck Institute for Intelligent Systems, Max-Planck Institute;Max Planck Institute for Intelligent Systems, Max-Planck Institute;Max Planck Institute for Intelligent Systems, Max-Planck Institute",-1;-1;-1,-1;-1;-1,5;4,10/27/17,0,0,0,0,0,0,123;4114;78778,14;176;860,4;29;120,7;461;9974,-1;-1
1327,ICLR,2018,Generalization of Learning using Reservoir Computing,Sanjukta Krishnagopal;Yiannis Aloimonos;Michelle Girvan,sanjukta@umd.edu;yiannis@cs.umd.edu;girvan@umd.edu,4;4;4,3;5;4,Reject,0,4,0,yes,10/27/17,"University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park",12;12;12,69;69;69,8,10/27/17,0,0,0,0,0,0,37;6259;456,9;326;14,3;42;5,1;339;53,-1;-1
1328,ICLR,2018,Adversarial Examples for Natural Language Classification Problems,Volodymyr Kuleshov;Shantanu Thakoor;Tingfung Lau;Stefano Ermon,vol.kuleshov@gmail.com;shanu.thakoor@gmail.com;ldf921@126.com;ermon@cs.stanford.edu,4;6;4,5;3;4,Reject,1,13,0,yes,10/27/17,Stanford University;Stanford University;126;Stanford University,4;4;-1;4,3;3;-1;3,3;4,10/27/17,26,14,10,0,0,6,1038;56;36;4821,40;5;4;203,13;2;2;31,108;11;7;646,-1;-1
1329,ICLR,2018,"Model Specialization for Inference Via End-to-End Distillation, Pruning, and Cascades",Daniel Kang;Karey Shi;Thao Ngyuen;Stephanie Mallard;Peter Bailis;Matei Zaharia,ddkang@stanford.edu;kareyshi@stanford.edu;thao2605@stanford.edu;pbailis@cs.stanford.edu;matei@cs.stanford.edu,6;4;3,3;4;4,Reject,0,0,0,yes,10/27/17,Stanford University;Stanford University;Stanford University;Stanford University;Stanford University,4;4;4;4;4,3;3;3;3;3,,10/27/17,0,0,0,0,0,0,114;0;0;213;2185;33408,21;1;1;9;120;166,5;0;0;5;24;40,13;0;0;24;227;3540,-1;-1
1330,ICLR,2018,Learning Sparse Structured Ensembles with SG-MCMC and Network Pruning,Yichi Zhang;Zhijian Ou,zhangyic17@mails.tsinghua.edu.cn;ozj@tsinghua.edu.cn,4;6;6,4;3;5,Reject,1,5,0,yes,10/27/17,Tsinghua University;Tsinghua University,10;10,30;30,3,10/27/17,4,0,0,0,4,0,13;227,6;63,2;9,3;15,-1;-1
1331,ICLR,2018,A dynamic game approach to training robust deep policies,Olalekan Ogunmolu,opo140030@utdallas.edu,5;3;5,4;3;2,Reject,0,1,0,yes,10/27/17,"University of Texas, Dallas",85,239,4,10/27/17,0,0,0,0,0,0,60,17,4,1,-1
1332,ICLR,2018,MACHINE VS MACHINE: MINIMAX-OPTIMAL DEFENSE AGAINST ADVERSARIAL EXAMPLES,Jihun Hamm,hammj@cse.ohio-state.edu,5;6;5,3;3;4,Reject,1,7,0,yes,10/27/17,Ohio State University,-1,-1,4,10/27/17,3,1,0,0,5,0,511,32,11,40,-1
1333,ICLR,2018,EXPLORING NEURAL ARCHITECTURE SEARCH FOR LANGUAGE TASKS,Minh-Thang Luong;David Dohan;Adams Wei Yu;Quoc V. Le;Barret Zoph;Vijay Vasudevan,luong.m.thang@gmail.com;ddohan@google.com;adamsyuwei@gmail.com;qvl@google.com;barretzoph@google.com;vrv@google.com,3;4;3,4;4;4,Reject,1,1,0,yes,10/27/17,Google;Google;;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,3,10/27/17,3,1,0,0,0,0,3047;1079;794;47654;6947;17951,33;11;28;192;30;55,20;4;12;79;20;18,350;156;142;5995;1298;2331,-1;-1
1334,ICLR,2018,Multimodal Sentiment Analysis To Explore the Structure of Emotions,Anthony Hu;Seth Flaxman,anthony.hu@stats.ox.ac.uk;s.flaxman@imperial.ac.uk,6;4;5,5;5;5,Reject,0,4,0,yes,10/27/17,University of Oxford;Imperial College London,51;74,1;8,3,10/27/17,16,9,5,0,13,3,42;8974,10;99,2;32,4;482,-1;-1
1335,ICLR,2018,Building Generalizable Agents with a Realistic and Rich 3D Environment,Yi Wu;Yuxin Wu;Georgia Gkioxari;Yuandong Tian,jxwuyi@gmail.com;ppwwyyxxc@gmail.com;georgia.gkioxari@gmail.com;yuandong.tian@gmail.com,4;5;8,5;4;4,Invite to Workshop Track,4,6,0,yes,10/27/17,University of California Berkeley;Facebook;Facebook;Facebook,5;-1;-1;-1,18;-1;-1;-1,8,10/27/17,148,86,44,0,8,16,1276;2066;7274;2435,101;13;28;84,11;10;18;25,196;323;1465;285,-1;-1
1336,ICLR,2018,LEARNING TO SHARE: SIMULTANEOUS PARAMETER TYING AND SPARSIFICATION IN DEEP LEARNING,Dejiao Zhang;Haozhu Wang;Mario Figueiredo;Laura Balzano,dejiao@umich.edu;hzwang@umich.edu;mario.figueiredo@lx.it.pt;girasole@umich.edu,6;8;7,3;5;4,Accept (Poster),0,8,0,yes,10/27/17,"University of Michigan;University of Michigan;Instituto de Telecomunicações, Portugal;University of Michigan",8;8;468;8,21;21;1103;21,8,10/27/17,16,7,5,0,0,2,141;18;18034;2650,15;6;299;88,7;1;52;23,17;2;1899;425,-1;-1
1337,ICLR,2018,Self-Supervised Learning of Object Motion Through Adversarial Video Prediction,Alex X. Lee;Frederik Ebert;Richard Zhang;Chelsea Finn;Pieter Abbeel;Sergey Levine,rich.zhang@eecs.berkeley.edu;febert@berkeley.edu;cbfinn@eecs.berkeley.edu;pabbeel@cs.berkeley.edu;svlevine@eecs.berkeley.edu,7;3;3;3,5;4;5;5,Reject,2,0,0,yes,10/27/17,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5;5,18;18;18;18;18,,10/27/17,0,0,0,0,0,0,1330;449;2799;7706;36532;25807,18;16;27;98;433;310,15;8;12;33;94;75,151;68;497;1033;4402;3343,-1;-1
1338,ICLR,2018,ElimiNet: A Model for Eliminating Options for Reading Comprehension with Multiple Choice Questions,Soham Parikh;Ananya Sai;Preksha Nema;Mitesh M Khapra,sohamp@cse.iitm.ac.in;ananyasb@cse.iitm.ac.in;preksha@cse.iitm.ac.in;miteshk@cse.iitm.ac.in,5;5;4,3;3;4,Reject,0,0,0,yes,10/27/17,Indian Institute of Technology Madras;Indian Institute of Technology Madras;Indian Institute of Technology Madras;Indian Institute of Technology Madras,153;153;153;153,625;625;625;625,,10/27/17,17,7,4,1,0,2,18;17;118;1329,5;2;9;91,1;1;5;18,2;2;13;152,-1;-1
1339,ICLR,2018,Forward Modeling for Partial Observation Strategy Games - A StarCraft Defogger,Gabriel Synnaeve;Zeming Lin;Jonas Gehring;Vasil Khalidov;Nicolas Carion;Nicolas Usunier,gab@fb.com;zlin@fb.com;jgehring@fb.com;vkhalidov@fb.com;alcinos@fb.com;usunier@fb.com,5;4;5,4;1;3,Reject,0,2,0,yes,10/27/17,Facebook;Facebook;Facebook;Facebook;Facebook;Facebook,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,,10/27/17,12,2,4,0,12,0,1487;5986;1990;234;33;6096,62;19;17;24;6;109,19;9;9;10;4;30,154;738;259;11;4;1164,-1;-1
1340,ICLR,2018,Real-valued (Medical) Time Series Generation with Recurrent Conditional GANs,Stephanie Hyland;Cristóbal Esteban;Gunnar Rätsch,stephanie.hyland@inf.ethz.ch;cr_est@ethz.ch;raetsch@inf.ethz.ch,4;6;5,4;4;4,Reject,0,3,0,yes,10/27/17,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,9;9;9,10;10;10,5;4,6/8/17,148,67,56,3,0,14,306;1776;9285,18;90;243,5;22;51,32;88;769,-1;-1
1341,ICLR,2018,Regularization Neural Networks via Constrained Virtual  Movement Field,Zhendong Zhang;Cheolkon Jung,zhd.zhang.ai@gmail.com;zhengzk@xidian.edu.cn,5;5;6,4;4;4,Invite to Workshop Track,0,5,0,yes,10/27/17,Tsinghua University;Tsinghua University,10;10,30;30,4,10/27/17,0,0,0,0,0,0,159;923,38;183,7;15,5;85,m;m
1342,ICLR,2018,Learning to Write by Learning the Objective,Ari Holtzman;Jan Buys;Maxwell Forbes;Antoine Bosselut;Yejin Choi,ahai@cs.washington.edu;jbuys@cs.washington.edu;mbforbes@cs.washington.edu;antoineb@cs.washington.edu;yejin@cs.washington.edu,6;5;4,5;4;5,Invite to Workshop Track,0,4,0,yes,10/27/17,University of Washington;University of Washington;University of Washington;University of Washington;University of Washington,6;6;6;6;6,25;25;25;25;25,3;5,10/27/17,1,1,0,0,0,0,611;399;572;410;7849,18;20;15;20;139,10;8;8;8;43,96;63;90;57;1015,-1;-1
1343,ICLR,2018,Key Protected Classification for GAN Attack Resilient Collaborative Learning,Mert Bülent Sarıyıldız;Ramazan Gökberk Cinbiş;Erman Ayday,mbsariyildiz@gmail.com;gokberkcinbis@gmail.com;erman@cs.bilkent.edu.tr,4;5;3,4;2;4,Reject,0,12,0,yes,10/27/17,;METU;Bilkent University,-1;210;291,-1;654;426,5;4,10/27/17,0,0,0,0,0,0,0;1241;1446,1;32;94,0;16;22,0;164;111,-1;-1
1344,ICLR,2018,Bit-Regularized Optimization of Neural Nets,Mohamed Amer;Aswin Raghavan;Graham W. Taylor;Sek Chai,mohamed.amer@sri.com;aswin.raghavan@sri.com;gwtaylor@uoguelph.ca;sek.chai@sri.com,4;3;4,5;4;4,Reject,2,6,0,yes,10/27/17,SRI International;SRI International;University of Guelph;SRI International,-1;-1;291;-1,-1;-1;1103;-1,,10/27/17,0,0,0,0,0,0,893;72;5768;619,68;20;143;67,15;4;31;12,72;2;488;43,-1;-1
1345,ICLR,2018,ShakeDrop regularization,Yoshihiro Yamada;Masakazu Iwamura;Koichi Kise,yamada@m.cs.osakafu-u.ac.jp;masa@cs.osakafu-u.ac.jp;kise@cs.osakafu-u.ac.jp,5;4;4,2;4;3,Reject,3,10,0,yes,10/27/17,Meiji University;Meiji University;Meiji University,468;468;468,334;334;334,,10/27/17,49,10,16,0,9,10,166;1859;2022,25;170;306,5;19;22,21;278;128,-1;-1
1346,ICLR,2018,AUTOMATA GUIDED HIERARCHICAL REINFORCEMENT LEARNING FOR ZERO-SHOT SKILL COMPOSITION,Xiao Li;Yao Ma;Calin Belta,xli87@bu.edu;yaoma@bu.edu;cbelta@bu.edu,5;3;4,4;4;3,Reject,0,3,0,yes,10/27/17,Boston University;Boston University;Boston University,69;69;69,70;70;70,,10/27/17,2,1,1,0,7,0,4549;2204;6316,402;215;304,29;25;41,330;132;367,-1;-1
1347,ICLR,2018,Quadrature-based features for kernel approximation,Marina Munkhoeva;Yermek Kapushev;Evgeny Burnaev;Ivan Oseledets,marina.munkhoeva@skolkovotech.ru;kapushev@gmail.com;e.burnaev@skoltech.ru;i.oseledets@skoltech.ru,4;7;6,3;5;4,Reject,0,3,0,yes,10/27/17,Skolkovo Institute of Science and Technology;Skolkovo Institute of Science and Technology;Skolkovo Institute of Science and Technology;Skolkovo Institute of Science and Technology,-1;-1;-1;-1,-1;-1;-1;-1,,10/27/17,15,5,5,0,5,1,19;63;734;4295,5;7;86;197,2;4;16;30,1;1;36;399,-1;-1
1348,ICLR,2018,Parametrizing filters of a CNN with a GAN,Yannic Kilcher;Gary Becigneul;Thomas Hofmann,yannic.kilcher@inf.ethz.ch;gary.becigneul@inf.ethz.ch;thomas.hofmann@inf.ethz.ch,2;4;4,4;4;5,Reject,0,1,0,yes,10/27/17,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,9;9;9,10;10;10,5;4,10/27/17,2,0,1,0,15,1,113;234;22675,14;17;173,4;5;52,26;47;3389,-1;-1
1349,ICLR,2018,Jiffy: A Convolutional Approach to Learning Time Series Similarity,Divya Shanmugam;Davis Blalock;John Guttag,divyas@mit.edu;dblalock@mit.edu;jguttag@mit.edu,6;4;8,4;4;3,Reject,0,10,0,yes,10/27/17,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2,5;5;5,,10/27/17,5,2,1,0,0,0,7;57;9428,5;8;220,1;4;46,0;6;981,-1;-1
1350,ICLR,2018,Transfer Learning on Manifolds via Learned Transport Operators,Marissa Connor;Christopher Rozell,marissa.connor@gatech.edu;crozell@gatech.edu,5;4;4,4;4;4,Reject,0,0,0,yes,10/27/17,Georgia Institute of Technology;Georgia Institute of Technology,13;13,33;33,5;6,10/27/17,0,0,0,0,0,0,2;1610,8;144,1;20,0;163,-1;-1
1351,ICLR,2018,Visualizing the Loss Landscape of Neural Nets,Hao Li;Zheng Xu;Gavin Taylor;Tom Goldstein,haoli@cs.umd.edu;xuzh@cs.umd.edu;taylor@usna.edu;tomg@cs.umd.edu,5;4;5,4;3;3,Invite to Workshop Track,0,5,0,yes,10/27/17,"University of Maryland, College Park;University of Maryland, College Park;University of Arizona;University of Maryland, College Park",12;12;181;12,69;69;161;69,8,10/27/17,322,184,63,11,0,29,581;463;1032;5769,43;35;44;98,7;6;12;27,50;40;98;724,-1;-1
1352,ICLR,2018,Demystifying overcomplete nonlinear auto-encoders: fast SGD convergence towards sparse representation from random initialization,Cheng Tang;Claire Monteleoni,tangch@gwu.edu;cmontel@gwu.edu,2;3;2,4;3;4,Reject,0,4,0,yes,10/27/17,George Washington University;George Washington University,210;210,226;226,9,10/27/17,0,0,0,0,0,0,154;1869,40;56,7;14,7;232,-1;-1
1353,ICLR,2018,On Characterizing the Capacity of Neural Networks Using Algebraic Topology,William H. Guss;Ruslan Salakhutdinov,wguss@cs.cmu.edu;rsalakhu@cs.cmu.edu,3;4;4,5;5;5,Reject,0,4,0,yes,10/27/17,Carnegie Mellon University;Carnegie Mellon University,1;1,24;24,,10/27/17,33,20,4,0,183,0,72;67470,8;254,4;82,5;7779,-1;-1
1354,ICLR,2018,Gated ConvNets for Letter-Based ASR,Vitaliy Liptchinsky;Gabriel Synnaeve;Ronan Collobert,vitaliy888@fb.com;gab@fb.com;locronan@fb.com,3;6;4,5;5;4,Reject,1,4,0,yes,10/27/17,Facebook;Facebook;Facebook,-1;-1;-1,-1;-1;-1,3,10/27/17,49,16,30,1,13,3,211;1487;19667,17;62;118,7;19;39,13;154;1915,-1;-1
1355,ICLR,2018,Variance Regularizing Adversarial Learning,Karan Grewal;R Devon Hjelm;Yoshua Bengio,karanraj.grewal@mail.utoronto.ca;erroneus@gmail.com;yoshua.umontreal@gmail.com,5;4;6,4;4;3,Reject,0,0,0,yes,10/27/17,Toronto University;University of Montreal;University of Montreal,17;124;124,22;108;108,5;4,7/2/17,5,1,1,0,65,0,276;1660;201719,8;43;807,3;13;147,61;259;23989,-1;-1
1356,ICLR,2018,The Information-Autoencoding Family: A Lagrangian Perspective on Latent Variable Generative Modeling,Shengjia Zhao;Jiaming Song;Stefano Ermon,sjzhao@stanford.edu;tsong@cs.stanford.edu;ermon@cs.stanford.edu,4;5;6,4;4;4,Reject,0,5,0,yes,10/27/17,Stanford University;Stanford University;Stanford University,4;4;4,3;3;3,5;4;8,10/27/17,14,7,1,0,0,2,580;779;4821,29;44;203,13;13;31,94;118;646,-1;-1
1357,ICLR,2018,Learning Priors for Adversarial Autoencoders,Hui-Po Wang;Wei-Jan Ko;Wen-Hsiao Peng,a88575847@gmail.com;ts771164@gmail.com;wpeng@cs.nctu.edu.tw,6;5;6,3;3;4,Reject,1,5,0,yes,10/27/17,National Chiao Tung University;;National Chiao Tung University,153;-1;153,452;-1;452,5;4,10/27/17,2,1,1,0,0,0,137;78;463,30;8;83,6;2;10,14;7;42,-1;-1
1358,ICLR,2018,Faster Reinforcement Learning with Expert State Sequences,Xiaoxiao Guo;Shiyu Chang;Mo Yu;Miao Liu;Gerald Tesauro,xiaoxiao.guo@ibm.com;shiyu.chang@ibm.com;yum@us.ibm.com,6;5;6,3;5;4,Reject,2,4,0,yes,10/27/17,International Business Machines;International Business Machines;International Business Machines,-1;-1;-1,-1;-1;-1,,10/27/17,0,0,0,0,0,0,1550;2903;3506;1944;7937,43;111;71;198;124,13;28;26;20;45,217;386;450;99;716,-1;-1
1359,ICLR,2018,Training Neural Machines with Partial Traces,Matthew Mirman;Dimitar Dimitrov;Pavle Djordjevich;Timon Gehr;Martin Vechev,matthew.mirman@inf.ethz.ch;dpavle@student.ethz.ch;dimitar.dimitrov@inf.ethz.ch;timon.gehr@inf.ethz.ch;martin.vechev@inf.ethz.ch,4;5;4,5;4;3,Reject,0,5,0,yes,10/27/17,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,9;9;9;9;9,10;10;10;10;10,8,10/27/17,0,0,0,0,0,0,481;1607;0;720;4181,15;112;1;22;153,4;21;0;9;35,63;108;0;99;467,-1;-1
1360,ICLR,2018,STRUCTURED ALIGNMENT NETWORKS,Yang Liu;Matt Gardner,yang.liu2@ed.ac.uk;mattg@allenai.org,6;5;5,4;4;4,Reject,0,1,0,yes,10/27/17,University of Edinburgh;Allen Institute for Artificial Intelligence,33;-1,27;-1,3,10/27/17,5,3,3,0,0,0,978;5583,79;55,15;19,101;1050,-1;-1
1361,ICLR,2018,Revisiting The Master-Slave Architecture In Multi-Agent Deep Reinforcement Learning,Xiangyu Kong;Fangchen Liu;Bo Xin;Yizhou Wang,kxyzc1992@gmail.com;liufangchen@pku.edu.cn;boxin@microsoft.com;yizhou.wang@pku.edu.cn,5;5;4,5;4;3,Reject,0,6,0,yes,10/27/17,;Peking University;Microsoft;Peking University,-1;24;-1;24,-1;27;-1;27,,10/27/17,21,10,5,0,8,1,530;447;618;807,136;15;232;47,12;5;13;16,19;73;58;86,-1;-1
1362,ICLR,2018,GATED FAST WEIGHTS FOR ASSOCIATIVE RETRIEVAL,Imanol Schlag;Jürgen Schmidhuber,imanol@idsia.ch;juergen@idsia.ch,3;5;4,5;4;4,Reject,0,0,0,yes,10/27/17,IDSIA;IDSIA,-1;-1,-1;-1,,10/27/17,0,0,0,0,0,0,44;6590,5;117,4;26,3;638,-1;-1
1363,ICLR,2018,Connectivity Learning in Multi-Branch Networks,Karim Ahmed;Lorenzo Torresani,karim.mmm@gmail.com;lt@dartmouth.edu,5;5;5,5;4;4,Reject,0,3,0,yes,10/27/17,Dartmouth College;Dartmouth College,153;153,89;89,,9/27/17,15,4,4,0,13,1,221;8164,12;103,7;35,30;1356,-1;-1
1364,ICLR,2018,AMPNet: Asynchronous Model-Parallel Training for Dynamic Neural Networks,Alexander L. Gaunt;Matthew A. Johnson;Alan Lawrence;Maik Riechert;Daniel Tarlow;Ryota Tomioka;Dimitrios Vytiniotis;Sam Webster,algaunt@microsoft.com;matjoh@microsoft.com;allawr@microsoft.com;a-mariec@microsoft.com;dannytarlow@gmail.com;ryoto@microsoft.com;dimitris@microsoft.com;sweb@microsoft.com,6;6;4,5;4;5,Reject,0,6,0,yes,10/27/17,Microsoft;Microsoft;Microsoft;Microsoft;MSR Cambridge;Microsoft;Microsoft;Microsoft,-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1,,5/27/17,9,2,3,0,71,0,22;161;-1;39;2516;4790;2034;15,4;23;-1;15;67;86;89;6,2;3;-1;4;23;29;20;2,1;17;0;1;303;650;226;2,-1;-1
1365,ICLR,2018,POLICY DRIVEN GENERATIVE ADVERSARIAL NETWORKS FOR ACCENTED SPEECH GENERATION,Prannay Khosla;Preethi Jyothi;Vinay P. Namboodiri;Mukundhan Srinivasan,prannayk@iitk.ac.in;pjyothi@cse.iitb.ac.in;vinaypn@cse.iitk.ac.in;msrinivasan@nvidia.com,5;3;4,4;4;4,Reject,0,6,0,yes,10/27/17,IIT Kanpur;Indian Institute of Technology Bombay;IIT Kanpur;NVIDIA,139;115;139;-1,578;367;578;-1,5;4,10/27/17,0,0,0,0,0,0,23;282;659;50,4;54;94;23,3;8;14;4,1;27;52;2,-1;-1
1366,ICLR,2018,Autonomous Vehicle Fleet Coordination With Deep Reinforcement Learning,Cane Punma,cane.cane@live.com,3;3;4,5;3;4,Reject,0,0,0,yes,10/27/17,,,,6,10/27/17,1,1,0,0,0,0,1,1,1,0,-1
1367,ICLR,2018,BLOCK-NORMALIZED GRADIENT METHOD: AN EMPIRICAL STUDY FOR TRAINING DEEP NEURAL NETWORK,Adams Wei Yu;Lei Huang;Qihang Lin;Ruslan Salakhutdinov;Jaime Carbonell,weiyu@cs.cmu.edu;huanglei@nlsde.buaa.edu.cn;qihang-lin@uiowa.edu;rsalakhu@cs.cmu.edu;jgc@cs.cmu.edu,4;9;2,5;5;5,Reject,0,5,0,yes,10/27/17,Carnegie Mellon University;Beihang University;University of Iowa;Carnegie Mellon University;Carnegie Mellon University,1;124;153;1;1,24;658;223;24;24,8,10/27/17,10,1,4,0,50,0,794;45;1442;67470;15711,28;29;88;254;507,12;4;22;82;55,142;3;183;7779;1628,-1;-1
1368,ICLR,2018,Learning Representations for Faster Similarity Search,Ludwig Schmidt;Kunal Talwar,ludwigs@mit.edu;kunal@google.com,4;4;4,5;5;4,Reject,0,0,0,yes,10/27/17,Massachusetts Institute of Technology;Google,2;-1,5;-1,,10/27/17,1,0,0,0,0,0,3694;16306,198;144,21;42,885;2149,-1;-1
1369,ICLR,2018,Optimal transport maps for distribution preserving operations on latent spaces of Generative Models,Eirikur Agustsson;Alexander Sage;Radu Timofte;Luc Van Gool,aeirikur@vision.ee.ethz.ch;sagea@student.ethz.ch;radu.timofte@vision.ee.ethz.ch;vangool@vision.ee.ethz.ch,4;6;6,3;3;4,Reject,0,4,0,yes,10/27/17,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,9;9;9;9,10;10;10;10,5;4,10/27/17,10,5,2,0,39,1,1669;24;7469;80600,36;3;180;1263,14;2;36;121,232;4;994;10366,-1;-1
1370,ICLR,2018,Neural Compositional Denotational Semantics for Question Answering,Nitish Gupta;Mike Lewis,nitishg@cis.upenn.edu;mikelewis@facebook.com,4;5;7,4;4;4,Reject,0,5,0,yes,10/27/17,University of Pennsylvania;,19;-1,10;-1,10,10/27/17,8,7,0,2,9,0,137;3934,15;104,5;21,12;670,-1;-1
1371,ICLR,2018,Variational Bi-LSTMs,Samira Shabanian;Devansh Arpit;Adam Trischler;Yoshua Bengio,s.shabanian@gmail.com;devansharpit@gmail.com;adam.trischler@microsoft.com;yoshua.umontreal@gmail.com,4;7;6,4;3;4,Reject,6,5,0,yes,10/27/17,Microsoft;University of Montreal;Microsoft;University of Montreal,-1;124;-1;124,-1;108;-1;108,,10/27/17,10,6,2,0,73,2,1666;898;1555;201719,11;42;47;807,4;12;17;147,129;118;287;23989,-1;-1
1372,ICLR,2018,TOWARDS ROBOT VISION MODULE DEVELOPMENT WITH EXPERIENTIAL ROBOT LEARNING,Ahmed A Aly;Joanne Bechta Dugan,aaa2cn@virginia.edu;jbd@virginia.edu,3;2;2,4;3;4,Reject,0,0,0,yes,10/27/17,University of Virginia;University of Virginia,62;62,113;113,5;4;2,10/27/17,0,0,0,0,0,0,202;2664,16;112,5;28,11;202,-1;-1
1373,ICLR,2018,Towards a Testable Notion of Generalization for Generative Adversarial Networks,Robert Cornish;Hongseok Yang;Frank Wood,rcornish@robots.ox.ac.uk;hongseok.yang@cs.ox.ac.uk;fwood@robots.ox.ac.uk,5;6;4,3;3;4,Reject,6,3,0,yes,10/27/17,University of Oxford;University of Oxford;University of Oxford,51;51;51,1;1;1,5;4,10/27/17,4,3,0,0,0,2,186;5171;2649,15;152;77,5;35;27,21;488;207,-1;-1
1374,ICLR,2018,3C-GAN: AN CONDITION-CONTEXT-COMPOSITE GENERATIVE ADVERSARIAL NETWORKS FOR GENERATING IMAGES SEPARATELY,Yeu-Chern Harn;Vladimir Jojic,ycharn@cs.unc.edu;vjojic@gmail.com,5;4;4,5;4;5,Reject,0,0,0,yes,10/27/17,"University of North Carolina, Chapel Hill;University of North Carolina, Chapel Hill",74;74,56;56,5,10/27/17,0,0,0,0,0,0,17;2249,7;59,2;20,0;131,-1;-1
1375,ICLR,2018,Alpha-divergence bridges maximum likelihood and reinforcement learning in neural sequence generation,Sotetsu Koyamada;Yuta Kikuchi;Atsunori Kanemura;Shin-ichi Maeda;Shin Ishii,sotetsu.koyamada@gmail.com,4;4;4,5;1;3,Reject,0,0,0,yes,10/27/17,,,,3,10/27/17,2,1,1,0,0,0,39;558;325;2203;4687,7;87;69;139;381,2;13;8;21;32,1;21;14;283;454,-1;-1
1376,ICLR,2018,Don't encrypt the data; just approximate the model \ Towards Secure Transaction and Fair Pricing of Training Data,Xinlei Xu,xxu@hmc.edu,2;4;3,4;5;5,Reject,0,0,0,yes,10/27/17,Harvey Mudd College,468,981,4,10/27/17,0,0,0,0,0,0,4,7,1,0,-1
1377,ICLR,2018,An inference-based policy gradient method for learning options,Matthew J. A. Smith;Herke van Hoof;Joelle Pineau,matthew.smith5@mail.mcgill.ca;herke.vanhoof@mail.mcgill.ca;jpineau@cs.mcgill.ca,3;3;4,4;5;4,Reject,0,5,0,yes,10/27/17,McGill University;McGill University;McGill University,81;81;81,42;42;42,,10/27/17,20,11,10,1,0,2,374;1209;11129,69;44;264,10;16;45,42;197;1218,-1;-1
1378,ICLR,2018,Sequential Coordination of Deep Models for Learning Visual Arithmetic,Eric Crawford;Guillaume Rabusseau;Joelle Pineau,eric.crawford@mail.mcgill.ca;guillaume.rabusseau@mail.mcgill.ca;jpineau@cs.mcgill.ca,4;3;2,4;4;4,Reject,0,4,0,yes,10/27/17,McGill University;McGill University;McGill University,81;81;81,42;42;42,,10/27/17,12,0,0,0,12,0,637;84;11129,49;35;264,11;5;45,50;11;1218,-1;-1
1379,ICLR,2018,What is image captioning made of?,Pranava Madhyastha;Josiah Wang;Lucia Specia,p.madhyastha@sheffield.ac.uk;j.k.wang@sheffield.ac.uk;l.specia@sheffield.ac.uk,4;4;4,5;4;5,Reject,0,10,0,yes,10/27/17,University of Sheffield;University of Sheffield;University of Sheffield,210;210;210,104;104;104,,10/27/17,0,0,0,0,0,0,133;485;6555,27;33;275,7;10;38,7;44;681,-1;-1
1380,ICLR,2018,Fraternal Dropout,Konrad Zolna;Devansh Arpit;Dendi Suhubdy;Yoshua Bengio,konrad.zolna@gmail.com;devansh.arpit@umontreal.ca;dasuhubd@ncsu.edu;bengioy@iro.umontreal.ca,5;5;6,4;3;3,Accept (Poster),0,7,0,yes,10/27/17,Jagiellonian University;University of Montreal;North Carolina State University;University of Montreal,468;124;85;124,695;108;275;108,3,10/27/17,19,9,4,1,0,2,107;921;176;207596,22;42;6;807,7;12;4;147,6;118;10;24238,-1;-1
1381,ICLR,2018,TD Learning with Constrained Gradients,Ishan Durugkar;Peter Stone,ishand@cs.utexas.edu;pstone@cs.utexas.edu,2;3;4,4;4;4,Reject,1,4,0,yes,10/27/17,"University of Texas, Austin;University of Texas, Austin",21;21,49;49,,10/27/17,4,2,2,0,0,1,335;18831,15;682,5;67,47;1480,-1;-1
1382,ICLR,2018,WSNet: Learning Compact and Efficient Networks with Weight Sampling,Xiaojie Jin;Yingzhen Yang;Ning Xu;Jianchao Yang;Jiashi Feng;Shuicheng Yan,xiaojie.jin@u.nus.edu;superyyzg@gmail.com;ning.xu@snap.com;jiachao.yang@snap.com;elefjia@nus.edu.sg;yanshuicheng@360.com,6;6;5,4;3;5,Invite to Workshop Track,0,0,0,yes,10/27/17,National University of Singapore;;Snap Inc.;Snap Inc.;National University of Singapore;360,16;-1;-1;-1;16;-1,22;-1;-1;-1;22;-1,,10/27/17,4,1,2,0,0,0,997;1791;6568;2204;9305;41702,52;83;654;20;328;782,15;19;35;8;51;95,115;170;450;431;1208;5172,-1;-1
1383,ICLR,2018,Generative Entity Networks: Disentangling Entitites and Attributes in Visual Scenes using Partial Natural Language Descriptions,Charlie Nash;Sebastian Nowozin;Nate Kushman,charlie.nash@ed.ac.uk;sebastian.nowozin@microsoft.com;nate@kushman.org,4;5;5,4;4;5,Reject,0,3,0,yes,10/27/17,University of Edinburgh;Microsoft;Microsoft Research,33;-1;-1,27;-1;-1,3;5,10/27/17,1,1,0,0,0,0,133;6885;1488,13;134;29,5;39;17,10;932;202,-1;-1
1384,ICLR,2018,Prediction Under Uncertainty with Error Encoding Networks,Mikael Henaff;Junbo Zhao;Yann Lecun,mbh305@nyu.edu;j.zhao@nyu.edu;yann@cs.nyu.edu,4;5;5,4;3;2,Reject,7,3,0,yes,10/27/17,New York University;New York University;New York University,26;26;26,27;27;27,4,10/27/17,10,6,2,0,8,1,2350;3331;91479,23;14;345,13;9;107,201;450;10337,-1;-1
1385,ICLR,2018,Noise-Based Regularizers for Recurrent Neural Networks,Adji B. Dieng;Jaan Altosaar;Rajesh Ranganath;David M. Blei,abd2141@columbia.edu;altosaar@princeton.edu;rajeshr@cs.princeton.edu;david.blei@columbia.edu,2;5;3,5;4;3,Reject,2,2,0,yes,10/27/17,Columbia University;Princeton University;Princeton University;Columbia University,15;31;31;15,14;7;7;14,3,10/27/17,0,0,0,0,0,0,464;266;4889;52408,14;14;77;306,7;5;27;76,59;42;511;9236,-1;-1
1386,ICLR,2018,Learning To Generate Reviews and Discovering Sentiment,Alec Radford;Rafal Jozefowicz;Ilya Sutskever,alec@openai.com;rafal@openai.com;ilya@openai.com,4;2;4,3;5;5,Reject,1,0,0,yes,10/27/17,OpenAI;OpenAI;OpenAI,-1;-1;-1,-1;-1;-1,3;5,4/5/17,254,153,64,8,0,24,14105;11069;130568,18;30;90,12;13;53,2792;1349;16899,-1;-1
1387,ICLR,2018,Deep learning mutation prediction enables early stage lung cancer detection in liquid biopsy,Steven T. Kothen-Hill;Asaf Zviran;Rafael C. Schulman;Sunil Deochand;Federico Gaiti;Dillon Maloney;Kevin Y. Huang;Will Liao;Nicolas Robine;Nathaniel D. Omans;Dan A. Landau,sth2022@med.cornell.edu;azviran@nygenome.org;rschulman@nygenome.org;sdd325@nyu.edu;fgaiti@nygenome.org;dmaloney@nygenome.org;khuang@nygenome.org;wliao@nygenome.org;nrobine@nygenome.org;nao2013@med.cornell.edu;dal3005@med.cornell.edu,8;4;5,4;3;4,Invite to Workshop Track,0,7,0,yes,10/27/17,Cornell University;;;New York University;;;Johns Hopkins University;;;Cornell University;Cornell University,7;-1;-1;26;-1;-1;71;-1;-1;7;7,19;-1;-1;27;-1;-1;13;-1;-1;19;19,,10/27/17,5,1,1,0,0,0,29;1320;41;8;242;25;154;1444;2975;77;2548,5;26;8;4;25;4;16;27;48;8;67,2;8;3;2;7;2;5;13;18;5;17,0;105;0;0;5;0;2;122;175;0;127,-1;-1
1388,ICLR,2018,Covariant Compositional Networks For Learning Graphs,Risi Kondor;Truong Son Hy;Horace Pan;Brandon M. Anderson;Shubhendu Trivedi,risi@cs.uchicago.edu;hytruongson@uchicago.edu;hopan@cs.uchicago.edu;brandona@uchicago.edu;shubhendu@ttic.edu,5;5;6,3;2;3,Invite to Workshop Track,0,12,0,yes,10/27/17,University of Chicago;University of Chicago;University of Chicago;University of Chicago;Toyota Technological Institute at Chicago,46;46;46;46;-1,9;9;9;9;-1,10,10/27/17,68,37,18,5,25,7,4717;66;159;116;437,63;1;4;9;27,22;1;3;4;11,442;7;20;9;29,-1;-1
1389,ICLR,2018,A novel method to determine the number of latent dimensions with SVD,Asana Neishabouri;Michel Desmarais,asana.neishabouri@polymtl.ca;michel.desmarais@polymtl.ca,1;2;3,4;5;4,Reject,1,0,0,yes,10/27/17,Polytechnique Montreal;Polytechnique Montreal,364;364,108;108,,10/27/17,0,0,0,0,0,0,0;1281,3;165,0;20,0;85,-1;-1
1390,ICLR,2018,Automatic Goal Generation for Reinforcement Learning Agents,David Held;Xinyang Geng;Carlos Florensa;Pieter Abbeel,dheld@andrew.cmu.edu;young.geng@berkeley.edu;florensa@berkeley.edu;pabbeel@berkeley.edu,8;4;6,4;4;4,Reject,0,19,0,yes,10/27/17,Carnegie Mellon University;University of California Berkeley;University of California Berkeley;University of California Berkeley,1;5;5;5,24;18;18;18,5;4,5/17/17,132,71,44,4,85,10,7454;313;468;36532,166;8;10;432,37;4;6;94,594;36;45;4402,-1;-1
1391,ICLR,2018,A Goal-oriented Neural Conversation Model by Self-Play,Wei Wei;Quoc V. Le;Andrew M. Dai;Li-Jia Li,wewei@google.com;adai@google.com;qvl@google.com;lijiali@google.com,6;4;3,3;3;4,Reject,0,0,0,yes,10/27/17,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,3,10/27/17,1,0,1,0,0,0,4260;47654;3720;24007,389;193;50;71,33;79;19;32,154;5995;459;4339,-1;-1
1392,ICLR,2018,Bayesian Uncertainty Estimation for Batch Normalized Deep Networks,Mattias Teye;Hossein Azizpour;Kevin Smith,teye@kth.se;azizpour@kth.se;ksmith@kth.se,5;5;6,3;4;4,Reject,0,5,1,yes,10/27/17,"KTH Royal Institute of Technology, Stockholm, Sweden;KTH Royal Institute of Technology, Stockholm, Sweden;KTH Royal Institute of Technology, Stockholm, Sweden",124;124;124,173;173;173,11,10/27/17,63,23,23,4,5,8,72;4123;6103,4;30;96,3;11;15,10;288;1008,-1;-1
1393,ICLR,2018,Learning Efficient Tensor Representations with Ring Structure Networks,Qibin Zhao;Masashi Sugiyama;Longhao Yuan;Andrzej Cichocki,qibin.zhao@riken.jp;sugi@k.u-tokyo.ac.jp;longhao.yuan@riken.jp;a.cichocki@riken.jp,5;5;6,4;4;3,Invite to Workshop Track,0,5,0,yes,10/27/17,RIKEN;The University of Tokyo;RIKEN;RIKEN,-1;52;-1;-1,-1;45;-1;-1,,5/20/17,34,19,4,0,2,2,2177;11582;123;22740,106;712;20;842,23;52;8;70,175;1299;2;1986,-1;-1
1394,ICLR,2018,Video Action Segmentation with Hybrid Temporal Networks,Li Ding;Chenliang Xu,liding@rochester.edu;chenliang.xu@rochester.edu,3;4;3,5;4;5,Reject,0,0,0,yes,10/27/17,University of Rochester;University of Rochester,104;104,153;153,2,5/22/17,24,9,7,1,17,1,8699;1270,411;60,28;18,536;199,-1;-1
1395,ICLR,2018,The Variational Homoencoder: Learning to Infer High-Capacity Generative Models from Few Examples,Luke Hewitt;Andrea Gane;Tommi Jaakkola;Joshua B. Tenenbaum,lbh@mit.edu;agane@mit.edu;tommi@csail.mit.edu;jbt@mit.edu,7;5;6,4;5;3,Reject,0,5,0,yes,10/27/17,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2,5;5;5;5,5;11,10/27/17,19,11,5,0,53,3,38;171;21955;30492,6;9;292;590,2;4;69;83,3;18;2320;2665,-1;-1
1396,ICLR,2018,Hyperedge2vec: Distributed Representations for Hyperedges,Ankit Sharma;Shafiq Joty;Himanshu Kharkwal;Jaideep Srivastava,sharm170@umn.edu;srjoty@ntu.edu.sg;himanshukharkwal765@gmail.com;srivasta@umn.edu,5;5;5,3;3;4,Reject,0,5,0,yes,10/27/17,"University of Minnesota, Minneapolis;National Taiwan University;;University of Minnesota, Minneapolis",55;85;-1;55,56;197;-1;56,10,10/27/17,2,2,0,0,0,0,188;1986;6;13408,53;130;5;476,7;25;2;43,9;202;0;950,-1;-1
1397,ICLR,2018,Combining Model-based and Model-free RL via Multi-step Control Variates,Tong Che;Yuchen Lu;George Tucker;Surya Bhupatiraju;Shane Gu;Sergey Levine;Yoshua Bengio,gerryche@berkeley.edu;luyuchen.paul@gmail.com;gjt@google.com;sbhupatiraju@google.com;shanegu@google.com;svlevine@eecs.berkeley.edu;bengioy@iro.umontreal.ca,5;5;4,4;4;3,Reject,0,2,0,yes,10/27/17,University of California Berkeley;University of Montreal;Google;Google;Google;University of California Berkeley;University of Montreal,5;124;-1;-1;-1;5;124,18;108;-1;-1;-1;18;108,,10/27/17,1,1,0,0,0,0,755;13;2603;245;2;24386;201719,24;6;74;14;3;309;807,8;2;21;5;1;73;147,103;1;292;35;0;3167;23989,-1;-1
1398,ICLR,2018,Tree2Tree Learning with Memory Unit,Ning Miao;Hengliang Wang;Ran Le;Chongyang Tao;Mingyue Shang;Rui Yan;Dongyan Zhao,miaoning@pku.edu.cn;wanghl@pku.edu.cn;leran@buaa.edu.cn;chongyangtao@pku.edu.cn;shangmy@pku.edu.cn;ruiyan@pku.edu.cn;zhaody@pku.edu.cn,2;5;4,4;4;4,Reject,0,1,0,yes,10/27/17,Peking University;Peking University;Beihang University;Peking University;Peking University;Peking University;Peking University,24;24;124;24;24;24;24,27;27;658;27;27;27;27,3,10/27/17,1,0,1,0,0,0,177;238;6;192;62;6288;16,28;27;8;19;11;640;8,8;7;2;7;3;37;2,15;11;1;23;6;424;3,-1;-1
1399,ICLR,2018,Revisiting Knowledge Base Embedding as Tensor Decomposition,Jiezhong Qiu;Hao Ma;Yuxiao Dong;Kuansan Wang;Jie Tang,xptree@gmail.com;haoma@microsoft.com;yuxdong@microsoft.com;kuansanw@microsoft.com;jietang@tsinghua.edu.cn,3;5;3,4;4;4,Reject,5,0,0,yes,10/27/17,;Microsoft;Microsoft;Microsoft;Tsinghua University,-1;-1;-1;-1;10,-1;-1;-1;-1;30,,10/27/17,1,0,1,0,0,0,553;387;1789;2455;4563,14;27;80;105;442,8;7;16;23;29,77;33;217;324;350,-1;-1
1400,ICLR,2018,Assessing the scalability of biologically-motivated deep learning algorithms and architectures,Anonymous,ICLR.cc/2018/Conference/Paper607/Authors,8;5;6,5;3;4,Withdrawn,1,0,,yes,1/13/18,,,,,1/13/18,76,0,0,0,0,0,-1,-1,-1,0,-1
1401,ICLR,2018,Neural Variational Sparse Topic Model,Anonymous,ICLR.cc/2018/Conference/Paper79/Authors,5;3;3,4;4;4,Withdrawn,0,0,,yes,1/21/18,,,,3;5,1/21/18,0,0,0,0,0,0,-1,-1,-1,0,-1
1402,ICLR,2018,Melody Generation for Pop Music via Word Representation of Musical Properties,Andrew Shin;Leopold Crestel;Hiroharu Kato;Kuniaki Saito;Katsunori Ohnishi;Masataka Yamaguchi;Masahiro Nakawaki;Yoshitaka Ushiku;Tatsuya Harada,andrew@mi.t.u-tokyo.ac.jp;crestel@ircam.fr;kato@mi.t.u-tokyo.ac.jp;k-saito@mi.t.u-tokyo.ac.jp;ohnishi@mi.t.u-tokyo.ac.jp;yamaguchi@mi.t.u-tokyo.ac.jp;nakawaki.ici@gmail.com;ushiku@mi.t.u-tokyo.ac.jp;harada@mi.t.u-tokyo.ac.jp,4;4;5,5;4;4,Withdrawn,0,0,,yes,1/29/18,The University of Tokyo;;The University of Tokyo;The University of Tokyo;The University of Tokyo;The University of Tokyo;;The University of Tokyo;The University of Tokyo,52;-1;52;52;52;52;-1;52;52,45;-1;45;45;45;45;-1;45;45,,10/31/17,6,0,0,0,0,0,212;16;1308;6739;182;148;4;1475;2786,23;4;182;281;18;160;1;55;216,7;2;15;43;8;6;1;17;28,8;1;91;478;8;10;0;256;336,-1;-1
1403,ICLR,2018,Spatial Variational Auto-Encoding via Matrix-Variate Normal Distributions,Zhengyang Wang;Hao Yuan;Shuiwang Ji,zwang6@eecs.wsu.edu;hao.yuan@wsu.edu;sji@eecs.wsu.edu,5;4;3,2;4;5,Withdrawn,0,0,,yes,12/3/17,SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY,468;468;468,352;352;352,5,5/18/17,3,0,0,0,0,0,613;1411;9664,29;162;140,10;19;36,35;70;735,-1;-1
1404,ICLR,2018,Dense Transformer Networks,Jun Li;Yongjun Chen;Lei Cai;Ian Davidson;Shuiwang Ji,jun.li3@wsu.edu;yongjun.chen@wsu.edu;lei.cai@wsu.edu;davidson@cs.ucdavis.edu;sji@eecs.wsu.edu,3;4;4,5;4;4,Withdrawn,0,0,,yes,12/2/17,"SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;University of California, Davis;SUN YAT-SEN UNIVERSITY",468;468;468;78;468,352;352;352;54;352,2,5/24/17,12,0,0,0,0,0,30651;1109;3093;4200;9664,2609;125;305;263;140,67;19;26;35;36,1551;49;81;331;735,-1;-1
1405,ICLR,2018,Self-Organization adds application robustness to deep learners,Pitoyo Hartono;Thomas Trappenberg,hartono@ieee.org;tt@cs.dal.edu,4;2;2,4;5;5,Withdrawn,0,0,,yes,12/25/17,Meiji University;,468;-1,334;-1,,12/25/17,0,0,0,0,0,0,180;1004,66;138,7;17,6;48,-1;-1
1406,ICLR,2018,Information Theoretic Co-Training,David McAllester,mcallester@ttic.edu,4;5;4,4;3;4,Withdrawn,0,0,,yes,1/5/18,Toyota Technological Institute at Chicago,-1,-1,,1/5/18,12,0,0,0,0,0,505,50,6,46,-1
1407,ICLR,2018,Towards Quantum Inspired Convolution Networks,Davi Geiger;Zvi Kedem,dg1@nyu.edu;kedem@nyu.edu,3;4;5,5;3;3,Withdrawn,3,0,,yes,12/2/17,New York University;New York University,26;26,27;27,11,12/2/17,0,0,0,0,0,0,2629;2909,100;104,21;20,151;171,-1;-1
1408,ICLR,2018,A cluster-to-cluster framework for neural machine translation,Anonymous,ICLR.cc/2018/Conference/Paper150/Authors,6;3;5,3;4;2,Withdrawn,0,0,,yes,12/13/17,,,,3,12/13/17,-1,-1,-1,-1,-1,-1,-1,-1,-1,0,-1
1409,ICLR,2018,Deep Epitome for Unravelling Generalized Hamming Network: A Fuzzy Logic Interpretation of Deep Learning,Anonymous,ICLR.cc/2018/Conference/Paper167/Authors,3;7;4,3;2;4,Withdrawn,0,0,,yes,1/4/18,,,,1,11/15/17,2,0,0,0,0,0,-1,-1,-1,0,-1
1410,ICLR,2018,Interactive Boosting of Neural Networks for Small-sample Image Classification,Xiaoxu Li;Dongliang Chang;Zheng-Hua Tan;Zhanyu Ma;Jun Guo;Jie Cao,xiaoxulilut@gmail.com;dlchanglut@hotmai.com;zt@es.aau.dk;mazhanyu@bupt.edu.cn;guojun@bupt.edu.cn;caoj@lut.cn,5;5;4,4;5;4,Withdrawn,1,4,,yes,1/2/18,;Hotmai;Aarhus University;Beijing University of Post and Telecommunication;Beijing University of Post and Telecommunication;,-1;-1;28;468;468;-1,-1;-1;60;1103;1103;-1,8,1/2/18,0,0,0,0,0,0,703;99;1981;2002;292;336,72;19;184;125;89;83,14;4;22;25;9;9,14;3;134;64;20;7,-1;-1
1411,ICLR,2018,Tensor-Based Preposition Representation,Hongyu Gong;Suma Bhat;Pramod Viswanath,hgong6@illinois.edu;pramodv@illinois.edu;spbhat2@illinois.edu,6;4;5,4;4;4,Withdrawn,0,0,,yes,12/12/17,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",3;3;3,37;37;37,,12/12/17,3,0,0,0,0,0,132;365;18026,17;64;179,6;10;40,12;32;2074,-1;-1
1412,ICLR,2018,Tactical Decision Making for Lane Changing with Deep Reinforcement Learning,Mustafa Mukadam;Akansel Cosgun;Alireza Nakhaei;Kikuo Fujimura,mmukadam3@gatech.edu;acosgun@hra.com;anakhaei@hra.com;kfujimura@hra.com,3;3;3,4;5;5,Withdrawn,0,0,,yes,12/13/17,Georgia Institute of Technology;Hra;Hra;Hra,13;-1;-1;-1,33;-1;-1;-1,,12/13/17,34,0,0,0,0,0,312;491;379;4281,23;31;43;161,9;11;10;29,19;20;19;229,-1;-1
1413,ICLR,2018, A Matrix Approximation View of NCE that Justifies Self-Normalization,Jacob Goldberger;Oren Melamud,jacob.goldberger@biu.ac.il;oren@melamuds.com,6;2;3,3;4;5,Withdrawn,0,0,,yes,12/14/17,Bar Ilan University;Melamuds,-1;-1,-1;-1,3;1,12/14/17,0,0,0,0,0,0,5740;552,180;20,36;8,559;85,-1;-1
1414,ICLR,2018,Empirical Investigation on Model Capacity and Generalization of Neural Networks for Text,Anonymous,ICLR.cc/2018/Conference/Paper265/Authors,4;3;4,5;4;5,Withdrawn,0,0,,yes,1/22/18,,,,3;8,1/22/18,-1,-1,-1,-1,-1,-1,-1,-1,-1,0,-1
1415,ICLR,2018,Detecting Anomalies in Communication Packet Streams based on  Generative Adversarial Networks,Anonymous,ICLR.cc/2018/Conference/Paper280/Authors,6;4;5,4;5;3,Withdrawn,0,3,,yes,1/3/18,,,,4;6;10,1/3/18,1,0,0,0,0,0,-1,-1,-1,0,-1
1416,ICLR,2018,Distributional Inclusion Vector Embedding for Unsupervised Hypernymy Detection,Haw-Shiuan Chang;ZiYun Wang;Luke Vilnis;Andrew McCallum,hschang@cs.umass.edu;wang-zy14@mails.tsinghua.edu.cn;luke@cs.umass.edu;mccallum@cs.umass.edu,4;5;5,5;5;5,Withdrawn,0,3,,yes,12/15/17,"University of Massachusetts, Amherst;Tsinghua University;University of Massachusetts, Amherst;University of Massachusetts, Amherst",30;10;30;30,191;30;191;191,3;8,10/2/17,25,0,0,0,0,0,315;82;1805;52577,19;29;25;434,8;5;11;98,30;9;303;5357,-1;-1
1417,ICLR,2018,Embedding Multimodal Relational Data,Pouya Pezeshkpour;Liyan Chen;Sameer Singh,pezeshkp@uci.edu;liyanc@uci.edu;sameer@uci.edu,6;4;5,4;4;5,Withdrawn,0,0,,yes,12/13/17,"University of California, Irvine;University of California, Irvine;University of California, Irvine",36;36;36,99;99;99,,12/13/17,4,0,0,0,0,0,40;36;5910,9;5;125,3;3;28,5;3;794,-1;-1
1418,ICLR,2018,pix2code: Generating Code from a Graphical User Interface Screenshot,Anonymous,ICLR.cc/2018/Conference/Paper334/Authors,2;5;5,5;4;4,Withdrawn,0,0,,yes,12/14/17,,,,10,5/22/17,68,0,0,0,0,0,-1,-1,-1,0,-1
1419,ICLR,2018,Improved Learning in Convolutional Neural Networks with Shifted Exponential Linear Units (ShELUs),Anonymous,ICLR.cc/2018/Conference/Paper459/Authors,1;4;3,5;5;5,Withdrawn,0,5,,yes,1/17/18,,,,,1/17/18,1,0,0,0,0,0,-1,-1,-1,0,-1
1420,ICLR,2018,Robust Task Clustering for Deep and Diverse Multi-Task and Few-Shot Learning,Mo Yu;Xiaoxiao Guo;Jinfeng Yi;Shiyu Chang;Saloni Potdar;Gerald Tesauro;Haoyu Wang;Bowen Zhou,yum@us.ibm.com;xiaoxiao.guo@ibm.com;jinfengyi.ustc@gmail.com,5;4;5,4;4;4,Withdrawn,0,0,,yes,12/14/17,International Business Machines;International Business Machines;JD AI Research,-1;-1;-1,-1;-1;-1,6,12/14/17,1,1,1,0,20,0,3711;1551;2110;3104;152;8204;100;6771,71;43;79;112;9;124;7;187,27;13;24;29;4;46;4;31,465;217;254;415;14;707;11;909,-1;-1
1421,ICLR,2018,Per-Weight Class-Based Learning Rates via Analytical Continuation,Michael Rotman;Lior Wolf,migo007@gmail.com;wolf@fb.com,3;3;3,4;2;4,Withdrawn,0,0,,yes,12/2/17,Tel Aviv University;Facebook,37;-1,217;-1,,12/2/17,0,0,0,0,0,0,24;15230,7;199,2;47,9;1643,-1;-1
1422,ICLR,2018,VSE++: Improving Visual-Semantic Embeddings with Hard Negatives,Fartash Faghri;David J. Fleet;Jamie Ryan Kiros;Sanja Fidler,faghri@cs.toronto.edu;fleet@cs.toronto.edu;rkiros@cs.toronto.edu;fidler@cs.toronto.edu,4,4,Withdrawn,0,0,,yes,11/16/17,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",17;17;17;17,22;22;22;22,,7/18/17,208,0,0,0,0,0,858;19103;1435;11171,12;218;18;160,5;60;9;49,140;1691;177;1429,-1;-1
1423,ICLR,2018,Dynamically Learning the Learning Rates:  Online Hyperparameter Optimization,Tuhin Sarkar;Anima Anandkumar;Leo Dirac,tsarkar@mit.edu;animakumar@gmail.com;leodirac@amazon.com,4;5;4,4;4;4,Withdrawn,0,0,,yes,1/3/18,Massachusetts Institute of Technology;University of California-Irvine;Amazon,2;36;-1,5;99;-1,11,1/3/18,0,0,0,0,0,0,234;5836;3,26;191;6,9;39;1,14;781;0,-1;-1
1424,ICLR,2018,MULTI-MODAL GEOLOCATION ESTIMATION USING DEEP NEURAL NETWORKS,Jesse Johns;Jeremiah Rounds;Michael Henry,jesse.johns@pnnl.gov;jeremiah.rounds@pnnl.gov;michael.j.henry@pnnl.gov,4;4;3,4;4;4,Withdrawn,0,0,,yes,1/3/18,Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory,-1;-1;-1,-1;-1;-1,,12/26/17,1,0,0,0,0,0,35;104;43,18;11;28,4;3;4,0;7;1,-1;-1
1425,ICLR,2018,Accelerating Convolutional Neural Networks using Iterative Two-Pass Decomposition,Wei-Shiang Lin;Hao-Ning Wu;Chih-Tsun Huang,weishianglin1993@gmail.com;wuhoward2002@gmail.com;cthuang@cs.nthu.edu.tw,4;3;5,4;5;4,Withdrawn,0,0,,yes,1/5/18,;;National Tsing Hua University,-1;-1;181,-1;-1;323,,1/5/18,0,0,0,0,0,0,1191;4;1553,76;2;102,17;1;22,47;0;130,-1;-1
1426,ICLR,2018,Adaptive Weight Sparsity for Training Deep Neural Networks,Michael James;Jack Lindsey;Ilya Sharapov,michael@cerebras.net;jacklindsey@stanford.edu;ilya@cerebras.net,5;3;4,3;4;2,Withdrawn,0,1,,yes,1/20/18,"Cerebras Systems, Inc;Stanford University;Cerebras Systems, Inc",-1;4;-1,-1;3;-1,,1/20/18,1,0,0,0,0,0,1588;33;969,175;13;22,22;3;9,82;1;63,-1;-1
1427,ICLR,2018,Deep Net Triage: Assessing The Criticality of Network Layers by Structural Compression,Theodore S. Nowak;Jason J. Corso,tsnowak@umich.edu;jjcorso@umich.edu,5;4;5,4;4;3,Withdrawn,0,0,,yes,1/6/18,University of Michigan;University of Michigan,8;8,21;21,,1/6/18,1,0,0,0,0,0,2;6349,2;233,1;34,1;594,-1;-1
1428,ICLR,2018,Deep Active Learning over the Long Tail,Anonymous,ICLR.cc/2018/Conference/Paper718/Authors,5;4;4,3;4;4,Withdrawn,0,1,,yes,1/5/18,,,,,11/2/17,22,0,0,0,0,0,-1,-1,-1,0,-1
1429,ICLR,2018,THE LOCAL DIMENSION OF DEEP MANIFOLD,Mengxiao Zhang;Wangquan Wu;Yanren Zhang;Kun He;Tao Yu;Huan Long;John E. Hopcroft,zmx@hust.edu.cn;u201514497@hust.edu.cn;hhxjzyr@hust.edu.cn;brooklet60@hust.edu.cn;ydtydr@sjtu.edu.cn;longhuan@cs.sjtu.edu.cn;jeh@cs.cornell.edu,3;3;5,4;3;4,Withdrawn,0,0,,yes,1/3/18,Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Shanghai Jiao Tong University;Shanghai Jiao Tong University;Cornell University,40;40;40;40;57;57;7,44;44;44;44;188;188;19,,11/5/17,2,0,0,0,0,0,131;2;4;662;4057;425;31889,29;2;9;39;397;37;303,7;1;2;16;26;9;64,4;1;1;61;259;9;2764,-1;-1
1430,ICLR,2018,Learning to Imagine Manipulation Goals for Robot Task Planning,Chris Paxton;Kapil Katyal;Christian Rupprecht;Raman Arora;Gregory D Hager,cpaxton@jhu.edu;kkatyal2@jhu.edu;christian.rupprecht@in.tum.de;arora@cs.jhu.edu;hager@cs.jhu.edu,3;3;3,4;3;3,Withdrawn,1,0,,yes,12/22/17,Johns Hopkins University;Johns Hopkins University;Technical University Munich;Johns Hopkins University;Johns Hopkins University,71;71;55;71;71,13;13;41;13;13,,11/8/17,1,0,0,0,0,0,350;596;1312;2852;17706,36;40;45;86;519,10;12;13;22;60,14;26;214;459;1200,-1;-1
1431,ICLR,2018,Human-like Clustering with Deep Convolutional Neural Networks,Ali Borji;Aysegul Dundar,aliborji@gmail.com;adundar@purdue.edu,4;3,5;5,Withdrawn,1,1,,yes,12/4/17,University of Central Florida;Purdue University,81;28,1103;60,2,12/4/17,2,0,0,0,0,0,7843;615,138;21,39;10,816;39,-1;-1
1432,ICLR,2018,Attribute-aware Collaborative Filtering: Survey and Classification,Wen-Hao Chen;Chin-Chi Hsu;Mi-Yen Yeh;Shou-De Lin,b02902023@ntu.edu.tw;chinchi@iis.sinica.edu.tw;miyen@iis.sinica.edu.tw;sdlin@csie.ntu.edu.tw,5;5;4,4;5;5,Withdrawn,0,0,,yes,12/11/17,National Taiwan University;Academia Sinica;Academia Sinica;National Taiwan University,85;-1;-1;85,197;-1;-1;197,,12/11/17,0,0,0,0,0,0,197;392;602;1947,20;36;56;181,8;11;14;24,10;12;52;108,-1;-1
1433,ICLR,2018,Continuous Propagation: Layer-Parallel Training,Michael James;Devansh Arpit;Herman Sahota;Ilya Sharapov,michae@cerebras.net;devansharpit@gmail.com;herman@cerebras.net;ilya@cerebras.net,5;4;3,4;3;4,Withdrawn,1,3,,yes,1/19/18,"Cerebras Systems, Inc;University of Montreal;Cerebras Systems, Inc;Cerebras Systems, Inc",-1;124;-1;-1,-1;108;-1;-1,9,1/19/18,1,0,0,0,0,0,1588;950;113;969,175;42;9;22,22;12;5;9,82;120;8;63,-1;-1
1434,ICLR,2018,Learning Topics using Semantic Locality,Ziyi Zhao;Krittaphat Pugdeethosapol;Sheng Lin;Zhe Li;Yanzhi Wang;Qinru Qiu,zzhao37@syr.edu;kpugdeet@syr.edu;shlin@syr.edu;zli89@syr.edu;ywang393@syr.edu;qiqiu@syr.edu,3;4;3,4;4;5,Withdrawn,0,0,,yes,1/24/18,Syracuse University;Syracuse University;Syracuse University;Syracuse University;Syracuse University;Syracuse University,244;244;244;244;244;244,275;275;275;275;275;275,,1/24/18,3,0,0,0,0,0,100;4;2941;1216;3939;3023,35;5;268;194;262;184,6;1;27;17;31;30,3;0;77;84;228;180,-1;-1
1435,ICLR,2018,Anticipatory Asynchronous Advantage Actor-Critic (A4C): The power of Anticipation in Deep Reinforcement Learning,Xun Luan;Tharun Medini;Anshumali Shrivastava,xun.luan@rice.edu;trm3@rice.edu;anshumali@rice.edu,4;2;3,4;5;5,Withdrawn,0,2,,yes,1/13/18,Rice University;Rice University;Rice University,85;85;85,86;86;86,,1/13/18,0,0,0,0,0,0,0;6;1205,2;13;102,0;2;16,0;0;116,-1;-1
1436,ICLR,2018,Sparse Deep Scattering Croisé Network,Romain Cosentino;Randall Balestriero;Richard Baraniuk;Ankit Patel,rom.cosentino@gmail.com;randallbalestriero@gmail.com;ankitpatel715@gmail.com;baraniuk@gmail.com,6,4,Withdrawn,0,0,,yes,11/25/17,Rice University;Rice University;;,85;85;-1;-1,86;86;-1;-1,,11/25/17,0,0,0,0,0,0,26;119;34070;385,13;39;663;30,4;5;88;6,1;2;2700;40,-1;-1
1437,ICLR,2018,Incremental Learning in Deep Convolutional Neural Networks Using Partial Network Sharing,Syed Shakib Sarwar;Aayush Ankit;Kaushik Roy,sarwar@purdue.edu;aankit@purdue.edu;kaushik@purdue.edu,4;2;4,4;5;5,Withdrawn,0,0,,yes,12/7/17,Purdue University;Purdue University;Purdue University,28;28;28,60;60;60,6,12/7/17,32,0,0,0,0,0,363;239;22262,20;26;765,10;9;76,27;11;1551,-1;-1
1438,ICLR,2018,Bayesian Embeddings for Long-Tailed Datasets,Victor Fragoso;Deva Ramanan,victor.fragoso@mail.wvu.edu;deva@andrew.cmu.edu,5;5;5,4;4;4,Withdrawn,1,3,,yes,1/17/18,West Virginia University;Carnegie Mellon University,-1;1,-1;24,11,1/17/18,3,0,0,0,0,0,265;34118,24;163,8;61,20;5373,-1;-1
1439,ICLR,2018,Deep Hyperspherical Defense against Adversarial Perturbations,Weiyang Liu;Zhen Liu;Zhehui Chen;Bo Dai;Tuo Zhao;Le Song,wyliu@gatech.edu;liuzhen1994@gatech.edu;zchen451@gatech.edu;bohr.dai@gmail.com;tourzhao@gatech.edu;lsong@cc.gatech.edu,4;5;5,5;4;3,Withdrawn,0,0,,yes,12/4/17,Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology,13;13;13;13;13;13,33;33;33;33;33;33,4,12/4/17,0,0,0,0,0,0,2127;679;46;42;2952;10105,62;217;14;15;109;329,15;14;4;4;21;55,326;40;5;3;214;1133,-1;-1
1440,ICLR,2018,FastNorm: Improving Numerical Stability of Deep Network Training with Efficient Normalization,Sadhika Malladi;Ilya Sharapov,sadhika@mit.edu;ilya@cerebras.net,4;4;3,3;4;3,Withdrawn,0,2,,yes,1/20/18,"Massachusetts Institute of Technology;Cerebras Systems, Inc",2;-1,5;-1,,1/20/18,0,0,0,0,0,0,34;969,5;22,3;9,0;63,-1;-1
1441,ICLR,2018,Cluster-based Warm-Start Nets,Anonymous,ICLR.cc/2018/Conference/Paper998/Authors,6;3;3,5;4;4,Withdrawn,0,4,,yes,1/5/18,,,,,1/5/18,-1,-1,-1,-1,-1,-1,-1,-1,-1,0,-1
1442,ICLR,2018,Do Convolutional Neural Networks act  as Compositional Nearest Neighbors?,Anonymous,ICLR.cc/2018/Conference/Paper1109/Authors,4;3;3,5;5;3,Withdrawn,1,13,,yes,1/17/18,,,,5;2,11/29/17,1,0,0,0,0,0,-1,-1,-1,0,-1
1443,ICLR,2018,DETECTING ADVERSARIAL PERTURBATIONS WITH SALIENCY,Chiliang Zhang;Zuochang Ye;Bo Zhang;Deli Zhao,zhangcl16@mails.tsinghua.edu.cn;zuochang@tsinhua.edu.cn;zhangbo@xiaomi.com;zhaodeli@gmail.com,3;4;4;4,5;4;4;4,Withdrawn,0,1,,yes,12/12/17,Tsinghua University;Tsinghua University;Xiaomi;Xiaomi,10;10;-1;-1,30;30;-1;-1,4;8,12/12/17,11,0,0,0,0,0,37;457;-1;1375,4;78;-1;40,3;13;-1;16,4;28;0;182,-1;-1
1444,ICLR,2018,Withdraw,Liyuan Liu;Jingbo Shang;Xiaotao Gu;Xiang Ren;Jian Peng;Jiawei Han,ll2@illinois.edu;shang7@illinois.edu;xiaotao2@illinois.du;xiangren@usc.edu;jianpeng@illinois.edu;hanj@illinois.edu,5;4;3,3;5;5,Withdrawn,0,0,,yes,12/15/17,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;;University of Southern California;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",3;3;-1;31;3;3,37;37;-1;66;37;37,,12/15/17,-1,-1,-1,-1,-1,-1,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,0;0;0;0;0;0,-1;-1
1445,ICLR,2018,Complex- and Real-Valued Neural Network Architectures,Nils Moenning;Suresh Manandhar,nm819@york.ac.uk;suresh.manandhar@york.ac.uk,2;4;3,5;4;4,Withdrawn,0,3,,yes,1/3/18,University of York;University of York,210;210,137;137,,1/3/18,1,0,0,0,0,0,3;302,3;25,1;5,2;28,-1;-1
1446,ICLR,2018,Learning with Mental Imagery,Anonymous,ICLR.cc/2018/Conference/Paper839/Authors,4;3;3,4;4;4,Withdrawn,0,0,,yes,1/5/18,,,,5;4,1/5/18,-1,-1,-1,-1,-1,-1,-1,-1,-1,0,-1
1447,ICLR,2018,Withdrawn,withdrawn.,withdrawn,5;7;5,5;4;4,Withdrawn,0,0,,yes,1/2/18,,,,,1/2/18,-1,-1,-1,-1,-1,-1,-1,-1,-1,0,-1
1448,ICLR,2018,HyperNetworks with statistical filtering for defending adversarial examples,Anonymous,ICLR.cc/2018/Conference/Paper293/Authors,5;4;5,3;3;4,Withdrawn,0,0,,yes,12/18/17,,,,4,11/6/17,7,0,0,0,0,0,-1,-1,-1,0,-1
1449,ICLR,2018,withdrawn,withdrawn,withdrawn,4;5;4,4;4;4,Withdrawn,0,0,,yes,9/27/18,,,,,9/27/18,-1,-1,-1,-1,-1,-1,-1,-1,-1,0,-1
1450,ICLR,2019,RANDOM MASK: Towards Robust Convolutional Neural Networks,Tiange Luo;Tianle Cai;Mengxiao Zhang;Siyu Chen;Liwei Wang,luotg@pku.edu.cn;caitianle1998@pku.edu.cn;zhan147@usc.edu;siyuchen@pku.edu.cn;wanglw@cis.pku.edu.cn,4;6;7,3;3;3,Reject,10,18,1,yes,9/27/18,Peking University;Peking University;University of Southern California;Peking University;Peking University,24;24;30;24;24,27;27;66;27;27,4,9/27/18,8,3,3,0,0,0,101;56;114;498;275,8;10;27;93;35,4;4;7;13;10,23;4;4;27;28,-1;-1
1451,ICLR,2019,"Unsupervised Discovery of Parts, Structure, and Dynamics",Zhenjia Xu*;Zhijian Liu*;Chen Sun;Kevin Murphy;William T. Freeman;Joshua B. Tenenbaum;Jiajun Wu,xuzhenjia1997@gmail.com;zhijian@mit.edu;chensun@google.com;kpmurphy@google.com;billf@mit.edu;jbt@mit.edu;jiajunwu@mit.edu,6;6;7;5,3;3;4;3,Accept (Poster),0,14,0,yes,9/27/18,Shanghai Jiao Tong University;Massachusetts Institute of Technology;Google;Google;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,52;2;-1;-1;2;2;2,188;5;-1;-1;5;5;5,,9/27/18,22,11,0,0,5,0,38;570;4823;16161;43808;30883;4001,4;20;307;83;402;593;89,3;7;33;41;92;83;30,1;110;469;2301;4342;2687;384,-1;-1
1452,ICLR,2019,BabyAI: A Platform to Study the Sample Efficiency of Grounded Language Learning,Maxime Chevalier-Boisvert;Dzmitry Bahdanau;Salem Lahlou;Lucas Willems;Chitwan Saharia;Thien Huu Nguyen;Yoshua Bengio,maximechevalierb@gmail.com;dimabgv@gmail.com;salemlahlou9@gmail.com;lcswillems@gmail.com;chitwaniit@gmail.com;thien@cs.uoregon.edu;yoshua.bengio@umontreal.ca,7;6;6,5;4;4,Accept (Poster),0,10,0,yes,9/27/18,University of Montreal;University of Montreal;University of Montreal;Ecole Normale Superieure;Indian Institute of Technology Bombay;University of Oregon;University of Montreal,123;123;123;99;115;199;123,108;108;108;603;367;267;108,,9/27/18,58,36,16,3,0,8,166;26850;63;234;68;1136;203485,23;31;3;18;7;52;807,7;18;3;6;4;16;147,16;4174;8;19;10;126;24050,-1;-1
1453,ICLR,2019,Environment Probing Interaction Policies,Wenxuan Zhou;Lerrel Pinto;Abhinav Gupta,wenxuanz@andrew.cmu.edu;lerrelp@andrew.cmu.edu;abhinavg@cs.cmu.edu,6;6;6,3;2;4,Accept (Poster),0,5,0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1,24;24;24,8,9/27/18,11,5,2,1,0,2,863;1298;16855,30;32;233,8;12;63,97;100;1890,-1;-1
1454,ICLR,2019,Classifier-agnostic saliency map extraction,Konrad Zolna;Krzysztof J. Geras;Kyunghyun Cho,konrad.zolna@gmail.com;k.j.geras@nyu.edu;kyunghyun.cho@nyu.edu,4;5;4,3;4;4,Reject,0,5,1,yes,9/27/18,Jagiellonian University;New York University;New York University,478;26;26,695;27;27,,5/21/18,7,1,1,0,46,0,93;519;44303,22;35;272,6;12;51,6;46;6476,-1;-1
1455,ICLR,2019,Reinforced Imitation Learning from Observations,Konrad Zolna;Negar Rostamzadeh;Yoshua Bengio;Sungjin Ahn;Pedro O. Pinheiro,konrad.zolna@gmail.com;negar.rostamzadeh@gmail.com;yoshua.umontreal@gmail.com;sjn.ahn@gmail.com;pedro@opinheiro.com,5;6;4,5;2;4,Reject,0,6,0,yes,9/27/18,Jagiellonian University;Element AI;University of Montreal;Rutgers University;Opinheiro,478;-1;123;34;-1,695;-1;108;172;-1,,9/27/18,1,0,0,0,0,0,93;367;195412;1311;2333,22;31;804;41;36,6;10;145;12;13,6;51;23614;160;236,-1;-1
1456,ICLR,2019,Max-MIG: an Information Theoretic Approach for Joint Learning from Crowds,Peng Cao*;Yilun Xu*;Yuqing Kong;Yizhou  Wang,caopeng2016@pku.edu.cn;xuyilun@pku.edu.cn;yuqing.kong@pku.edu.cn;yizhou.wang@pku.edu.cn,6;7;6,4;4;4,Accept (Poster),0,8,0,yes,9/27/18,Peking University;Peking University;Peking University;Peking University,24;24;24;24,27;27;27;27,,9/27/18,5,3,1,0,0,0,19;18;113;179,24;5;17;37,2;3;6;7,0;3;8;24,-1;-1
1457,ICLR,2019,Understanding Straight-Through Estimator in Training Activation Quantized Neural Nets,Penghang Yin;Jiancheng Lyu;Shuai Zhang;Stanley Osher;Yingyong Qi;Jack Xin,yph@ucla.edu;jianchel@uci.edu;shuazhan@qti.qualcomm.com;sjo@math.ucla.edu;yingyong@qti.qualcomm.com;jxin@math.uci.edu,6;7;7,3;4;4,Accept (Poster),1,10,0,yes,9/27/18,"University of California, Los Angeles;University of California, Irvine;Qualcomm Inc, QualComm;University of California, Los Angeles;Qualcomm Inc, QualComm;University of California, Irvine",20;35;-1;20;-1;35,15;99;-1;15;-1;99,1,9/27/18,31,9,11,0,3,3,511;109;345;56442;634;2285,34;16;53;560;84;179,13;5;9;94;15;23,36;10;20;5053;39;191,-1;-1
1458,ICLR,2019,Learning Multi-Level Hierarchies with Hindsight,Andrew Levy;George Konidaris;Robert Platt;Kate Saenko,andrew_levy2@brown.edu;gdk@cs.brown.edu;saenko@bu.edu;rplatt@ccs.neu.edu,6;7;5,4;4;4,Accept (Poster),0,7,0,yes,9/27/18,Brown University;Brown University;Boston University;Northeastern University,65;65;65;16,50;50;70;839,,12/4/17,33,25,12,2,0,10,177;2922;3592;16713,82;145;169;177,7;29;34;54,19;188;237;2341,-1;-1
1459,ICLR,2019,NOODL: Provable Online Dictionary Learning and Sparse Coding,Sirisha Rambhatla;Xingguo Li;Jarvis Haupt,rambh002@umn.edu;lixx1661@umn.edu;jdhaupt@umn.edu,7;6;7,2;2;2,Accept (Poster),0,3,0,yes,9/27/18,"University of Minnesota, Minneapolis;University of Minnesota, Minneapolis;University of Minnesota, Minneapolis",57;57;57,56;56;56,,9/27/18,3,1,0,0,0,0,45;562;3631,15;76;105,4;12;24,1;61;259,-1;-1
1460,ICLR,2019,Learning to Infer and Execute 3D Shape Programs,Yonglong Tian;Andrew Luo;Xingyuan Sun;Kevin Ellis;William T. Freeman;Joshua B. Tenenbaum;Jiajun Wu,yonglong@mit.edu;aluo@mit.edu;xs5@princeton.edu;ellisk@mit.edu;billf@mit.edu;jbt@mit.edu;jiajunwu@mit.edu,6;7;7,4;5;4,Accept (Poster),3,13,0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Princeton University;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;30;2;2;2;2,5;5;7;5;5;5;5,,9/27/18,30,16,3,0,7,0,1621;181;296;424;43621;30684;3974,22;14;12;48;402;590;90,13;5;5;9;92;83;30,180;5;43;23;4337;2668;380,-1;-1
1461,ICLR,2019,Meta-learning with differentiable closed-form solvers,Luca Bertinetto;Joao F. Henriques;Philip Torr;Andrea Vedaldi,luca@robots.ox.ac.uk;joao@robots.ox.ac.uk;philip.torr@eng.ox.ac.uk;vedaldi@robots.ox.ac.uk,5;2;7,3;5;4,Accept (Poster),0,21,3,yes,9/27/18,University of Oxford;University of Oxford;University of Oxford;University of Oxford,50;50;50;50,1;1;1;1,6,5/21/18,101,41,51,5,28,28,3960;7040;27765;33252,23;44;354;201,15;19;83;61,1041;1775;3818;4595,-1;-1
1462,ICLR,2019,Adaptive Gradient Methods with Dynamic Bound of Learning Rate,Liangchen Luo;Yuanhao Xiong;Yan Liu;Xu Sun,luolc@pku.edu.cn;xiongyh@zju.edu.cn;yanliu.cs@usc.edu;xusun@pku.edu.cn,7;4;6,4;5;4,Accept (Poster),9,6,1,yes,9/27/18,Peking University;Zhejiang University;University of Southern California;Peking University,24;57;30;24,27;177;66;27,1;8,9/27/18,118,36,48,5,0,30,156;143;5961;5364,7;7;584;539,4;3;35;39,32;30;492;325,-1;-1
1463,ICLR,2019,AD-VAT: An Asymmetric Dueling mechanism for learning Visual Active Tracking,Fangwei Zhong;Peng Sun;Wenhan Luo;Tingyun Yan;Yizhou Wang,zfw@pku.edu.cn;pengsun000@gmail.com;whluo.china@gmail.com;yanty18@pku.edu.cn;yizhou.wang@pku.edu.cn,5;4;6,4;3;4,Accept (Poster),0,5,0,yes,9/27/18,Peking University;Tecent Inc.;Tencent AI Lab;Peking University;Peking University,24;-1;-1;24;24,27;-1;-1;27;27,4;8,9/27/18,3,1,1,0,0,0,139;1471;853;4;179,9;204;38;3;37,4;22;16;1;7,11;77;81;0;24,-1;-1
1464,ICLR,2019,Variational Autoencoder with Arbitrary Conditioning,Oleg Ivanov;Michael Figurnov;Dmitry Vetrov,tigvarts@gmail.com;michael@figurnov.ru;vetrovd@yandex.ru,6;7;6,3;3;3,Accept (Poster),0,8,2,yes,9/27/18,Samsung;Google;Higher School of Economics,-1;-1;478,-1;-1;377,5,6/6/18,32,17,18,0,3,13,312;276;2006,25;11;123,7;5;16,26;39;275,-1;-1
1465,ICLR,2019,Optimistic mirror descent in saddle-point problems: Going the extra (gradient) mile,Panayotis Mertikopoulos;Bruno Lecouat;Houssam Zenati;Chuan-Sheng Foo;Vijay Chandrasekhar;Georgios Piliouras,panayotis.mertikopoulos@imag.fr;bruno_lecouat@i2r.a-star.edu.sg;houssam_zenati@i2r.a-star.edu.sg;foocs@i2r.a-star.edu.sg;vijay@i2r.a-star.edu.sg;georgios@sutd.edu.sg,7;6;5,3;5;5,Accept (Poster),0,5,2,yes,9/27/18,Imag Montpellier Université;A*STAR;A*STAR;A*STAR;A*STAR;Singapore University of Technology and Design,-1;-1;-1;-1;-1;478,-1;-1;-1;-1;-1;1103,5;4,7/7/18,64,43,19,1,4,8,1178;249;254;444;2955;1214,131;11;11;26;109;99,20;6;6;8;27;19,86;53;53;49;221;78,-1;-1
1466,ICLR,2019,A2BCD: Asynchronous Acceleration with Optimal Complexity,Robert Hannah;Fei Feng;Wotao Yin,roberthannah89@gmail.com;fei.feng@math.ucla.edu;wotaoyin@math.ucla.edu,7;7;9,4;5;5,Accept (Poster),0,6,0,yes,9/27/18,"University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles",20;20;20,15;15;15,1,9/27/18,5,4,0,0,0,0,359;65;16593,76;47;221,9;5;56,29;1;1887,-1;-1
1467,ICLR,2019,Minimal Random Code Learning: Getting Bits Back from Compressed Model Parameters,Marton Havasi;Robert Peharz;José Miguel Hernández-Lobato,mh740@cam.ac.uk;rp587@cam.ac.uk;jmh233@cam.ac.uk,7;6;7,4;2;3,Accept (Poster),2,6,0,yes,9/27/18,University of Cambridge;University of Cambridge;University of Cambridge,71;71;71,2;2;2,,9/27/18,8,3,4,1,0,1,39;656;3685,7;46;113,3;13;28,10;52;419,-1;-1
1468,ICLR,2019,Model-Predictive Policy Learning with Uncertainty Regularization for Driving in Dense Traffic,Mikael Henaff;Alfredo Canziani;Yann LeCun,mbh305@nyu.edu;canziani@nyu.edu;yann@cs.nyu.edu,6;7;6,4;5;5,Accept (Poster),0,6,0,yes,9/27/18,New York University;New York University;New York University,26;26;26,27;27;27,,9/27/18,29,6,8,1,0,1,2315;543;90005,23;11;345,13;5;107,198;33;10225,-1;-1
1469,ICLR,2019,Feature-Wise Bias Amplification,Klas Leino;Emily Black;Matt Fredrikson;Shayak Sen;Anupam Datta,kleino@cs.cmu.edu;emilybla@cs.cmu.edu;mfredrik@cs.cmu.edu;shayaks@cs.cmu.edu;danupam@cmu.edu,6;6;7,5;4;4,Accept (Poster),0,13,0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1;1,24;24;24;24;24,,9/27/18,5,4,0,0,6,0,32;183;3437;459;20,5;35;70;21;8,3;5;22;8;2,4;24;412;33;0,-1;-1
1470,ICLR,2019,RelGAN: Relational Generative Adversarial Networks for Text Generation,Weili Nie;Nina Narodytska;Ankit Patel,wn8@rice.edu;nnarodytska@vmware.com;abp4@rice.edu,6;8;6,4;4;4,Accept (Poster),0,0,6,yes,9/27/18,Rice University;VMware Inc;Rice University,85;-1;85,86;-1;86,5;4,9/27/18,29,14,11,0,0,6,240;1435;354,22;99;30,9;20;6,33;147;39,-1;-1
1471,ICLR,2019,Neural Speed Reading with Structural-Jump-LSTM,Christian Hansen;Casper Hansen;Stephen Alstrup;Jakob Grue Simonsen;Christina Lioma,chrh@di.ku.dk;c.hansen@di.ku.dk;s.alstrup@di.ku.dk;simonsen@di.ku.dk;c.lioma@di.ku.dk,7;7;5,5;4;4,Accept (Poster),0,4,0,yes,9/27/18,University of Copenhagen;University of Copenhagen;University of Copenhagen;University of Copenhagen;University of Copenhagen,99;99;99;99;99,109;109;109;109;109,3,9/27/18,10,4,2,1,0,0,3887;76;1399;849;1488,71;18;81;99;89,24;6;20;15;15,710;8;168;91;154,-1;-1
1472,ICLR,2019,Time-Agnostic Prediction: Predicting Predictable Video Frames,Dinesh Jayaraman;Frederik Ebert;Alexei Efros;Sergey Levine,dineshjayaraman@berkeley.edu;febert@berkeley.edu;efros@eecs.berkeley.edu;svlevine@eecs.berkeley.edu,8;7;7,4;3;5,Accept (Poster),0,7,1,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5,18;18;18;18,,8/23/18,37,16,6,2,121,3,1154;441;36187;23990,41;16;191;309,16;8;77;73,121;67;4544;3116,-1;-1
1473,ICLR,2019,Generative Question Answering: Learning to Answer the Whole Question,Mike Lewis;Angela Fan,mikelewis@fb.com;angelafan@fb.com,7;8;6,4;4;4,Accept (Poster),1,6,0,yes,9/27/18,Facebook;Facebook,-1;-1,-1;-1,5;4,9/27/18,18,11,3,0,0,0,3877;1769,104;26,21;14,662;264,-1;-1
1474,ICLR,2019,Episodic Curiosity through Reachability,Nikolay Savinov;Anton Raichuk;Damien Vincent;Raphael Marinier;Marc Pollefeys;Timothy Lillicrap;Sylvain Gelly,nikolay.savinov@inf.ethz.ch;raveman@google.com;damienv@google.com;raphaelm@google.com;marc.pollefeys@inf.ethz.ch;countzero@google.com;sylvaingelly@google.com,8;7;8;6,4;4;3;4,Accept (Poster),2,17,0,yes,9/27/18,Swiss Federal Institute of Technology;Google;Google;Google;Swiss Federal Institute of Technology;Google;Google,10;-1;-1;-1;10;-1;-1,10;-1;-1;-1;10;-1;-1,,9/27/18,55,31,17,2,0,12,701;65;866;60;23582;23726;3555,15;4;40;6;543;74;112,9;2;11;2;81;39;26,99;12;113;11;1825;2898;457,-1;-1
1475,ICLR,2019,Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks,Yikang Shen;Shawn Tan;Alessandro Sordoni;Aaron Courville,yikang.shn@gmail.com;shawn@wtf.sg;alsordon@microsoft.com;aaron.courville@gmail.com,7;9;8,3;4;4,Accept (Oral),0,6,0,yes,9/27/18,University of Montreal;University of Montreal;Microsoft;University of Montreal,123;123;-1;123,108;108;-1;108,3,9/27/18,74,44,35,8,9,26,291;133;3756;58064,15;15;54;203,7;4;19;64,68;31;535;7730,-1;-1
1476,ICLR,2019,Automatically Composing Representation Transformations as a Means for Generalization,Michael Chang;Abhishek Gupta;Sergey Levine;Thomas L. Griffiths,mbchang@berkeley.edu;abhigupta@berkeley.edu;svlevine@eecs.berkeley.edu;tom_griffiths@berkeley.edu,7;9;7,2;4;3,Accept (Poster),1,8,0,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5,18;18;18;18,10;8,7/12/18,18,9,7,0,54,5,728;1368;23990;21111,34;89;309;437,15;18;73;69,41;139;3116;2160,-1;-1
1477,ICLR,2019,Bayesian Policy Optimization for Model Uncertainty,Gilwoo Lee;Brian Hou;Aditya Mandalika;Jeongseok Lee;Sanjiban Choudhury;Siddhartha S. Srinivasa,gilwoo@cs.uw.edu;bhou@cs.uw.edu;adityavk@cs.uw.edu;jslee02@cs.uw.edu;sanjibac@cs.uw.edu;siddh@cs.uw.edu,5;7;6;7,4;4;3;3,Accept (Poster),0,9,1,yes,9/27/18,"University of Washington, Seattle;University of Washington, Seattle;University of Washington, Seattle;University of Washington, Seattle;University of Washington, Seattle;University of Washington, Seattle",6;6;6;6;6;6,25;25;25;25;25;25,11,9/27/18,8,2,5,0,3,0,89;201;34;10;463;10202,29;15;8;7;53;329,6;5;4;2;12;52,2;15;1;0;11;800,-1;-1
1478,ICLR,2019,Diversity-Sensitive Conditional Generative Adversarial Networks,Dingdong Yang;Seunghoon Hong;Yunseok Jang;Tianchen Zhao;Honglak Lee,didoyang@umich.edu;hongseu@umich.edu;yunseokj@umich.edu;ericolon@umich.edu;honglak@eecs.umich.edu,6;7;7,5;3;5,Accept (Poster),1,5,0,yes,9/27/18,University of Michigan;University of Michigan;University of Michigan;University of Michigan;University of Michigan,8;8;8;8;8,21;21;21;21;21,5;4,9/27/18,24,11,7,0,4,2,130;2513;717;67;23627,5;25;78;14;166,3;15;16;4;60,19;254;40;2;2789,-1;-1
1479,ICLR,2019,Poincare Glove: Hyperbolic Word Embeddings,Alexandru Tifrea*;Gary Becigneul*;Octavian-Eugen Ganea*,tifreaa@student.ethz.ch;gary.becigneul@inf.ethz.ch;octavian.ganea@inf.ethz.ch,6;7;6,4;3;4,Accept (Poster),0,5,0,yes,9/27/18,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10,10;10;10,3;1;10,9/27/18,51,26,14,3,15,7,52;226;509,5;17;17,2;5;9,7;46;94,-1;-1
1480,ICLR,2019,Stochastic Optimization of Sorting Networks via Continuous Relaxations,Aditya Grover;Eric Wang;Aaron Zweig;Stefano Ermon,adityag@cs.stanford.edu;ejwang@cs.stanford.edu;azweig@cs.stanford.edu;ermon@cs.stanford.edu,7;8;6,3;4;3,Accept (Poster),0,5,0,yes,9/27/18,Stanford University;Stanford University;Stanford University;Stanford University,4;4;4;4,3;3;3;3,10,9/27/18,19,8,11,1,0,6,3430;400;109;4694,54;62;8;201,16;10;3;30,781;31;23;633,-1;-1
1481,ICLR,2019,Hyperbolic Attention Networks,Caglar Gulcehre;Misha Denil;Mateusz Malinowski;Ali Razavi;Razvan Pascanu;Karl Moritz Hermann;Peter Battaglia;Victor Bapst;David Raposo;Adam Santoro;Nando de Freitas,ca9lar@gmail.com;mdenil@google.com;mateuszm@google.com;alirazavi@google.com;razp@google.com;kmh@google.com;vbapst@google.com;drapso@google.com;adamsantoro@google.com;nandodefreitas@google.com,6;7;7,5;5;4,Accept (Poster),2,5,0,yes,9/27/18,Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,3;10;8,5/24/18,38,17,9,0,243,5,19438;3291;2614;520;16705;5112;4491;1574;1550;3010;18934,36;38;33;23;101;41;88;32;15;35;184,26;20;14;7;46;21;29;15;8;20;54,2982;282;330;67;1671;686;419;167;212;340;1853,-1;-1
1482,ICLR,2019,Adv-BNN: Improved Adversarial Defense through Robust Bayesian Neural Network,Xuanqing Liu;Yao Li;Chongruo Wu;Cho-Jui Hsieh,xqliu@cs.ucla.edu;yaoli@ucdavis.edu;crwu@ucdavis.edu;chohsieh@cs.ucla.edu,7;6;7,3;4;3,Accept (Poster),0,13,0,yes,9/27/18,"University of California, Los Angeles;University of California, Davis;University of California, Davis;University of California, Los Angeles",20;81;81;20,15;54;54;15,4;11,9/27/18,37,23,10,2,4,6,244;1279;60;12363,20;215;9;168,6;18;3;40,33;81;10;1715,-1;-1
1483,ICLR,2019,How Important is a Neuron,Kedar Dhamdhere;Mukund Sundararajan;Qiqi Yan,kedar@google.com;mukunds@google.com;qiqiyan@google.com,7;7;7,4;2;5,Accept (Poster),2,7,0,yes,9/27/18,Google;Google;Google,-1;-1;-1,-1;-1;-1,,5/30/18,17,9,8,2,19,4,742;2264;1581,39;78;68,14;20;16,61;304;248,-1;-1
1484,ICLR,2019,Small nonlinearities in activation functions create bad local minima in neural networks,Chulhee Yun;Suvrit Sra;Ali Jadbabaie,chulheey@mit.edu;suvrit@mit.edu;jadbabai@mit.edu,7;7;8,3;3;4,Accept (Poster),0,5,1,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2,5;5;5,1,2/10/18,46,26,1,2,6,4,193;6243;14833,22;163;299,6;38;46,13;994;1007,-1;-1
1485,ICLR,2019,Efficiently testing local optimality and escaping saddles for ReLU networks,Chulhee Yun;Suvrit Sra;Ali Jadbabaie,chulheey@mit.edu;suvrit@mit.edu;jadbabai@mit.edu,3;6;6;8,4;2;3;3,Accept (Poster),0,10,0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2,5;5;5,1;9,9/27/18,6,2,2,0,0,2,193;6243;14833,22;163;299,6;38;46,13;994;1007,-1;-1
1486,ICLR,2019,Local SGD Converges Fast and Communicates Little,Sebastian U. Stich,sebastian.stich@epfl.ch,8;5;8,5;5;4,Accept (Poster),0,7,0,yes,9/27/18,Swiss Federal Institute of Technology Lausanne,478,38,1;9,5/24/18,117,73,35,10,4,28,921,48,15,128,-1
1487,ICLR,2019,Learning to Learn without Forgetting by Maximizing Transfer and Minimizing Interference,Matthew Riemer;Ignacio Cases;Robert Ajemian;Miao Liu;Irina Rish;Yuhai Tu;and Gerald Tesauro,mdriemer@us.ibm.com;cases@stanford.edu;ajemian@mit.edu;miao.liu1@ibm.com;rish@us.ibm.com;yuhai@us.ibm.com;gtesauro@us.ibm.com,6;8;7,5;4;5,Accept (Poster),2,6,0,yes,9/27/18,International Business Machines;Stanford University;Massachusetts Institute of Technology;International Business Machines;International Business Machines;International Business Machines;International Business Machines,-1;4;2;-1;-1;-1;-1,-1;3;5;-1;-1;-1;-1,6,9/27/18,62,27,15,3,3,7,277;206;160;1774;3767;3565;7561,27;17;13;198;141;104;124,9;9;5;20;26;29;45,25;20;11;97;320;233;713,-1;-1
1488,ICLR,2019,Excessive Invariance Causes Adversarial Vulnerability,Joern-Henrik Jacobsen;Jens Behrmann;Richard Zemel;Matthias Bethge,j.jacobsen@vectorinstitute.ai;jensb@uni-bremen.de;zemel@cs.toronto.edu;matthias.bethge@uni-tuebingen.de,6;6;7,4;2;4,Accept (Poster),0,8,6,yes,9/27/18,"Vector Institute;Universität Bremen;Department of Computer Science, University of Toronto;University of Tuebingen",-1;153;18;153,-1;291;22;94,4,9/27/18,52,32,11,0,19,7,393;269;21136;11051,19;13;208;415,8;5;52;45,35;43;2466;1237,-1;-1
1489,ICLR,2019,Decoupled Weight Decay Regularization,Ilya Loshchilov;Frank Hutter,ilya.loshchilov@gmail.com;fh@cs.uni-freiburg.de,6;7;5,4;4;4,Accept (Poster),0,6,0,yes,9/27/18,;Universität Freiburg,-1;123,-1;82,8,11/14/17,264,34,139,4,0,52,2495;12359,38;233,21;49,348;1501,-1;-1
1490,ICLR,2019,Learning Robust Representations by Projecting Superficial Statistics Out,Haohan Wang;Zexue He;Zachary C. Lipton;Eric P. Xing,haohanw@cs.cmu.edu;zexueh@mail.bnu.edu.cn;zlipton@cmu.edu;epxing@cs.cmu.edu,7;7;9,4;4;3,Accept (Oral),0,4,0,yes,9/27/18,Carnegie Mellon University;Australian National University;Carnegie Mellon University;Carnegie Mellon University,1;106;1;1,24;48;24;24,8,9/27/18,25,17,9,1,3,5,52;64;4649;23758,17;6;97;602,3;4;27;75,5;13;424;2640,-1;-1
1491,ICLR,2019,Spectral Inference Networks: Unifying Deep and Spectral Learning,David Pfau;Stig Petersen;Ashish Agarwal;David G. T. Barrett;Kimberly L. Stachenfeld,pfau@google.com;svp@google.com;agarwal@google.com;barrettdavid@google.com;stachenfeld@google.com,5;7;5,3;3;3,Accept (Poster),0,3,0,yes,9/27/18,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,5;10,6/6/18,14,6,8,0,77,1,1949;10485;11126;1159;328,29;47;210;24;17,12;15;25;11;7,204;1657;1246;157;27,-1;-1
1492,ICLR,2019,A rotation-equivariant convolutional neural network model of primary visual cortex,Alexander S. Ecker;Fabian H. Sinz;Emmanouil Froudarakis;Paul G. Fahey;Santiago A. Cadena;Edgar Y. Walker;Erick Cobos;Jacob Reimer;Andreas S. Tolias;Matthias Bethge,alexander.ecker@uni-tuebingen.de;sinz@bcm.edu;froudara@bcm.edu;paul.fahey@bcm.edu;sa.cadena721@gmail.com;eywalker@bcm.edu;emcobost@gmail.com;reimer@bcm.edu;astolias@bcm.edu;matthias.bethge@bethgelab.org,5;7;8,4;4;3,Accept (Poster),0,21,1,yes,9/27/18,"University of Tuebingen;Baylor College of Medicine;Baylor College of Medicine;Baylor College of Medicine;University of Tuebingen;Baylor College of Medicine;;Baylor College of Medicine;Baylor College of Medicine;Centre for Integrative Neuroscience, AG Bethge",153;-1;-1;-1;153;-1;-1;-1;-1;153,94;-1;-1;-1;94;-1;-1;-1;-1;94,,9/27/18,10,5,3,0,7,0,6171;123;819;96;66;126;46;685;5699;11319,124;17;36;24;8;18;10;34;167;416,24;6;11;5;4;6;4;12;39;45,797;5;77;4;3;8;1;32;490;1254,-1;-1
1493,ICLR,2019,Backpropamine: training self-modifying neural networks with differentiable neuromodulated plasticity,Thomas Miconi;Aditya Rawal;Jeff Clune;Kenneth O. Stanley,tmiconi@uber.com;aditya.rawal@uber.com;jeffclune@uber.com;kstanley@uber.com,5;4;9,4;4;4,Accept (Poster),0,9,0,yes,9/27/18,Uber;Uber;Uber;Uber,-1;-1;-1;-1,-1;-1;-1;-1,3,9/27/18,19,11,5,1,0,1,429;772;10593;11827,41;49;113;260,13;13;36;52,53;41;761;1512,-1;-1
1494,ICLR,2019,Enabling Factorized Piano Music Modeling and Generation with the MAESTRO Dataset,Curtis Hawthorne;Andriy Stasyuk;Adam Roberts;Ian Simon;Cheng-Zhi Anna Huang;Sander Dieleman;Erich Elsen;Jesse Engel;Douglas Eck,fjord@google.com;astas@google.com;adarob@google.com;iansimon@google.com;annahuang@google.com;sedielem@google.com;eriche@google.com;jesseengel@google.com;deck@google.com,8;8;8,5;2;4,Accept (Oral),0,4,0,yes,9/27/18,Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1,,9/27/18,46,17,25,1,0,8,387;44;10411;2477;255;13609;4513;2183;2555,20;3;36;40;16;47;53;36;84,9;1;19;16;7;20;21;12;27,70;7;1450;219;31;1093;480;245;274,-1;-1
1495,ICLR,2019,Invariant and Equivariant Graph Networks,Haggai Maron;Heli Ben-Hamu;Nadav Shamir;Yaron Lipman,haggai.maron@weizmann.ac.il;heli.benhamu@weizmann.ac.il;nadav13@gmail.com;yaron.lipman@weizmann.ac.il,8;4;9,5;5;4,Accept (Poster),0,8,0,yes,9/27/18,Weizmann Institute;Weizmann Institute;;Weizmann Institute,106;106;-1;106,1103;1103;-1;1103,10;8,9/27/18,51,37,20,3,2,17,462;131;51;5079,19;7;1;99,9;4;1;40,77;36;17;523,-1;-1
1496,ICLR,2019,Solving the Rubik's Cube with Approximate Policy Iteration,Stephen McAleer;Forest Agostinelli;Alexander Shmakov;Pierre Baldi,smcaleer@uci.edu;fagostin@uci.edu;ashmakov@uci.edu;pfbaldi@ics.uci.edu,7;7;7,4;4;3,Accept (Poster),0,3,1,yes,9/27/18,"University of California, Irvine;University of California, Irvine;University of California, Irvine;University of California, Irvine",35;35;35;35,99;99;99;99,,9/27/18,4,1,1,0,0,0,4;291;20;17573,9;14;9;302,1;6;3;63,0;33;1;1548,-1;-1
1497,ICLR,2019,Execution-Guided Neural Program Synthesis,Xinyun Chen;Chang Liu;Dawn Song,xinyun.chen@berkeley.edu;liuchang@eecs.berkeley.edu;dawnsong.travel@gmail.com,7;7;7,4;2;5,Accept (Poster),0,12,0,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,18;18;18,,9/27/18,22,11,10,1,0,4,1625;5271;2514,51;698;117,14;35;24,139;263;331,-1;-1
1498,ICLR,2019,Transferring Knowledge across Learning Processes,Sebastian Flennerhag;Pablo G. Moreno;Neil D. Lawrence;Andreas Damianou,sflennerhag@turing.ac.uk;morepabl@amazon.com;lawrennd@amazon.com;damianou@amazon.com,8;8;6,3;4;4,Accept (Oral),0,13,0,yes,9/27/18,Alan Turing Institute;Amazon;Amazon;Amazon,-1;-1;-1;-1,-1;-1;-1;-1,6;2,9/27/18,17,9,5,0,29,2,37;56;10845;1247,7;7;264;42,3;2;52;17,4;7;1373;185,-1;-1
1499,ICLR,2019,Theoretical Analysis of Auto Rate-Tuning by Batch Normalization,Sanjeev Arora;Zhiyuan Li;Kaifeng Lyu,arora@cs.princeton.edu;zhiyuanli@cs.princeton.edu;vfleaking@gmail.com,7;7;7,4;2;2,Accept (Poster),0,7,0,yes,9/27/18,Princeton University;Princeton University;Tsinghua University,30;30;8,7;7;30,1;9;8,9/27/18,22,12,2,4,9,4,15727;623;65,349;14;6,61;9;3,1791;105;8,-1;-1
1500,ICLR,2019,Neural Logic Machines,Honghua Dong;Jiayuan Mao;Tian Lin;Chong Wang;Lihong Li;Denny Zhou,dhh19951@gmail.com;maojiayuan@gmail.com;tianlin@google.com;chongw@google.com;lihongli.cs@gmail.com;dennyzhou@gmail.com,6;7;5,3;2;5,Accept (Poster),0,0,9,yes,9/27/18,Tsinghua University;Tsinghua University;Google;Google;Google;Google,8;8;-1;-1;-1;-1,30;30;-1;-1;-1;-1,10;8,9/27/18,31,12,4,0,0,1,63;465;489;17692;10345;65,5;19;53;1045;242;10,3;7;11;54;46;4,3;47;36;1627;1183;7,-1;-1
1501,ICLR,2019,"The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision",Jiayuan Mao;Chuang Gan;Pushmeet Kohli;Joshua B. Tenenbaum;Jiajun Wu,maojiayuan@gmail.com;ganchuang1990@gmail.com;pushmeet@google.com;jbt@mit.edu;jiajunwu@mit.edu,6;7;9,4;4;5,Accept (Oral),0,9,1,yes,9/27/18,Tsinghua University;International Business Machines;Google;Massachusetts Institute of Technology;Massachusetts Institute of Technology,8;-1;-1;2;2,30;-1;-1;5;5,8,9/27/18,87,33,18,4,0,6,465;2138;21843;30100;3884,19;81;313;588;90,7;26;69;82;29,47;216;2735;2639;373,-1;-1
1502,ICLR,2019,Disjoint Mapping Network for Cross-modal Matching of Voices and Faces,Yandong Wen;Mahmoud Al Ismail;Weiyang Liu;Bhiksha Raj;Rita Singh,yandongw@andrew.cmu.edu;mahmoudi@andrew.cmu.edu;wyliu@gatech.edu;bhiksha@cs.cmu.edu;rsingh@cs.cmu.edu,7;6;7,4;3;4,Accept (Poster),0,3,0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;Georgia Institute of Technology;Carnegie Mellon University;Carnegie Mellon University,1;1;13;1;1,24;24;33;24;24,,7/12/18,14,9,6,1,7,2,2907;90;1958;7871;2044,29;20;62;315;157,9;6;13;44;21,460;8;298;777;168,-1;-1
1503,ICLR,2019,Learning Neural PDE Solvers with Convergence Guarantees,Jun-Ting Hsieh;Shengjia Zhao;Stephan Eismann;Lucia Mirabella;Stefano Ermon,junting@stanford.edu;sjzhao@stanford.edu;seismann@stanford.edu;lucia.mirabella@siemens.com;ermon@cs.stanford.edu,7;8;6,4;4;3,Accept (Poster),2,3,2,yes,9/27/18,Stanford University;Stanford University;Stanford University;Siemens Corporate Research;Stanford University,4;4;4;-1;4,3;3;3;-1;3,,9/27/18,23,10,9,0,0,4,194;558;38;100;4694,10;29;8;12;201,6;13;4;4;30,23;90;4;7;633,-1;-1
1504,ICLR,2019,Dimensionality Reduction for Representing the Knowledge of Probabilistic Models,Marc T Law;Jake Snell;Amir-massoud Farahmand;Raquel Urtasun;Richard S Zemel,law@cs.toronto.edu;jsnell@cs.toronto.edu;farahmand@vectorinstitute.ai;urtasun@cs.toronto.edu;zemel@cs.toronto.edu,7;6;9,4;1;3,Accept (Poster),0,4,0,yes,9/27/18,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Vector Institute;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",18;18;-1;18;18,22;22;-1;22;22,8;1;6,9/27/18,7,6,3,0,0,1,305;1474;225;24119;21136,33;12;32;245;208,8;6;7;73;52,25;373;16;3420;2466,-1;-1
1505,ICLR,2019,Improving the Generalization of Adversarial Training with Domain Adaptation,Chuanbiao Song;Kun He;Liwei Wang;John E. Hopcroft,cbsong@hust.edu.cn;brooklet60@hust.edu.cn;wanglw@pku.edu.cn;jeh@cs.cornell.edu,6;6;6,2;3;4,Accept (Poster),0,7,0,yes,9/27/18,Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Peking University;Cornell University,39;39;24;7,44;44;27;19,4;8,9/27/18,40,23,10,2,6,8,47;557;277;27990,4;38;35;303,2;15;10;60,9;59;28;2752,-1;-1
1506,ICLR,2019,Towards Understanding Regularization in Batch Normalization,Ping Luo;Xinjiang Wang;Wenqi Shao;Zhanglin Peng,pluo@ie.cuhk.edu.hk;wangxinjiang@sensetime.com;shaowenqi@sensetime.com;zhanglinpeng@sensetime.com,5;6;6,3;5;2,Accept (Poster),0,4,0,yes,9/27/18,The Chinese University of Hong Kong;SenseTime Group Limited;SenseTime Group Limited;SenseTime Group Limited,57;-1;-1;-1,40;-1;-1;-1,8,9/4/18,49,29,9,3,8,2,9128;729;72;229,94;80;13;36,40;14;3;7,1644;45;3;17,-1;-1
1507,ICLR,2019,Differentiable Learning-to-Normalize via Switchable Normalization,Ping Luo;Jiamin Ren;Zhanglin Peng;Ruimao Zhang;Jingyu Li,pluo@ie.cuhk.edu.hk;renjiamin@sensetime.com;pengzhanglin@sensetime.com;zhangruimao@sensetime.com;lijingyu@sensetime.com,7;7;7,5;3;4,Accept (Poster),0,2,1,yes,9/27/18,The Chinese University of Hong Kong;SenseTime Group Limited;SenseTime Group Limited;SenseTime Group Limited;SenseTime Group Limited,57;-1;-1;-1;-1,40;-1;-1;-1;-1,,6/28/18,55,20,29,2,372,10,9128;79;229;770;283,94;9;36;44;51,40;4;7;10;9,1644;10;17;83;16,-1;-1
1508,ICLR,2019,Neural Graph Evolution: Towards Efficient Automatic Robot Design,Tingwu Wang;Yuhao Zhou;Sanja Fidler;Jimmy Ba,tingwuwang@cs.toronto.edu;henryzhou@cs.toronto.edu;fidler@cs.toronto.edu;jba@cs.toronto.edu,5;8;6,4;4;4,Accept (Poster),0,8,0,yes,9/27/18,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",18;18;18;18,22;22;22;22,10,9/27/18,1,0,0,0,2,0,177;99;10419;51070,8;42;160;52,5;5;48;21,14;5;1353;8381,-1;-1
1509,ICLR,2019,Systematic Generalization: What Is Required and Can It Be Learned?,Dzmitry Bahdanau*;Shikhar Murty*;Michael Noukhovitch;Thien Huu Nguyen;Harm de Vries;Aaron Courville,dimabgv@gmail.com;shikhar.murty@gmail.com;michael.noukhovitch@umontreal.ca;thien@cs.uoregon.edu;mail@harmdevries.com;aaron.courville@gmail.com,6;6;4,3;5;4,Accept (Poster),0,9,0,yes,9/27/18,University of Montreal;University of Montreal;University of Montreal;University of Oregon;University of Montreal;University of Montreal,123;123;123;199;123;123,108;108;108;267;108;108,8,9/27/18,24,17,3,4,8,4,26335;107;35;1122;2909;58353,31;11;4;52;31;203,18;5;2;16;16;64,4135;18;7;125;300;7739,-1;-1
1510,ICLR,2019,Preferences Implicit in the State of the World,Rohin Shah;Dmitrii Krasheninnikov;Jordan Alexander;Pieter Abbeel;Anca Dragan,rohinmshah@berkeley.edu;dmitrii.krasheninnikov@student.uva.nl;jfalex@stanford.edu;pabbeel@cs.berkeley.edu;anca@berkeley.edu,6;6;7;7,3;4;3;4,Accept (Poster),0,7,0,yes,9/27/18,University of California Berkeley;University of Amsterdam;Stanford University;University of California Berkeley;University of California Berkeley,5;169;4;5;5,18;59;3;18;18,1,9/27/18,7,4,1,0,4,0,77;7;117;36080;3565,7;3;22;433;126,4;1;4;94;30,5;0;4;4359;281,-1;-1
1511,ICLR,2019,"Learning Particle Dynamics for Manipulating Rigid Bodies, Deformable Objects, and Fluids",Yunzhu Li;Jiajun Wu;Russ Tedrake;Joshua B. Tenenbaum;Antonio Torralba,liyunzhu@mit.edu;jiajunwu@mit.edu;russt@mit.edu;jbt@mit.edu;torralba@mit.edu,6;8;7,4;3;3,Accept (Poster),0,10,0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2;2,5;5;5;5;5,10,9/27/18,38,24,12,0,0,3,402;3884;8228;30100;47639,12;90;184;588;281,9;29;45;82;88,39;373;596;2639;6286,-1;-1
1512,ICLR,2019,On the Relation Between the Sharpest Directions of DNN Loss and the SGD Step Length,Stanisław Jastrzębski;Zachary Kenton;Nicolas Ballas;Asja Fischer;Yoshua Bengio;Amos Storkey,staszek.jastrzebski@gmail.com;zakenton@gmail.com;ballasn@fb.com;asja.fischer@gmail.com;yoshua.umontreal@gmail.com;a.storkey@ed.ac.uk,7;6;6,3;4;3,Accept (Poster),0,4,0,yes,9/27/18,New York University;;Facebook;Institute for Cognitive Neuroscience/ Inst. for Neuroinformatics;University of Montreal;University of Edinburgh,26;-1;-1;-1;123;33,27;-1;-1;-1;108;27,8,7/13/18,30,17,4,7,12,5,843;228;4871;1683;197759;3776,27;15;54;53;804;198,12;6;20;16;145;31,99;29;576;196;23765;431,-1;-1
1513,ICLR,2019,A Variational Inequality Perspective on Generative Adversarial Networks,Gauthier Gidel;Hugo Berard;Gaëtan Vignoud;Pascal Vincent;Simon Lacoste-Julien,gauthier.gidel@umontreal.ca;hugo.berard@gmail.com;gaetan.vignoud@gmail.com;vincentp@iro.umontreal.ca;slacoste@iro.umontreal.ca,8;8;7,3;3;4,Accept (Poster),0,5,1,yes,9/27/18,University of Montreal;;INRIA;University of Montreal;University of Montreal,123;-1;-1;123;123,108;-1;-1;108;108,5;4,2/28/18,82,45,46,1,11,18,296;100;4;7211;3668,21;4;7;48;72,10;3;1;13;26,45;20;1;639;594,-1;-1
1514,ICLR,2019,Learning-Based Frequency Estimation Algorithms,Chen-Yu Hsu;Piotr Indyk;Dina Katabi;Ali Vakilian,cyhsu@mit.edu;indyk@mit.edu;dina@csail.mit.edu;vakilian@mit.edu,6;7;6,1;4;4,Accept (Poster),0,4,0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2,5;5;5;5,1,9/27/18,20,9,6,1,0,3,427;23697;19088;165,31;286;252;26,9;66;66;8,34;3228;2613;22,-1;-1
1515,ICLR,2019,Learning to Design RNA,Frederic Runge;Danny Stoll;Stefan Falkner;Frank Hutter,runget@cs.uni-freiburg.de;d.stoll@tutanota.com;sfalkner@cs.uni-freiburg.de;fh@cs.uni-freiburg.de,8;6;6,4;4;1,Accept (Poster),0,16,0,yes,9/27/18,Universität Freiburg;Universität Freiburg;Universität Freiburg;Universität Freiburg,123;123;123;123,82;82;82;82,6,9/27/18,9,3,2,1,0,1,9;13;740;12359,1;5;32;233,1;2;11;49,1;1;74;1501,-1;-1
1516,ICLR,2019,L-Shapley and C-Shapley: Efficient Model Interpretation for Structured Data,Jianbo Chen;Le Song;Martin J. Wainwright;Michael I. Jordan,jianbochen@berkeley.edu;lsong@cc.gatech.edu;wainwrig@berkeley.edu;jordan@cs.berkeley.edu,6;7;7,4;3;2,Accept (Poster),2,5,0,yes,9/27/18,University of California Berkeley;Georgia Institute of Technology;University of California Berkeley;University of California Berkeley,5;13;5;5,18;33;18;18,10,8/8/18,35,14,18,1,4,6,825;9128;23947;114147,71;328;354;844,16;52;76;138,95;1088;3455;15869,-1;-1
1517,ICLR,2019,Stochastic Gradient/Mirror Descent: Minimax Optimality and Implicit Regularization,Navid Azizan;Babak Hassibi,azizan@caltech.edu;hassibi@caltech.edu,5;7;5,3;4;3,Accept (Poster),0,5,0,yes,9/27/18,California Institute of Technology;California Institute of Technology,140;140,3;3,,6/4/18,19,9,3,2,10,0,182;17936,23;468,7;59,8;1904,-1;-1
1518,ICLR,2019,Boosting Robustness Certification of Neural Networks,Gagandeep Singh;Timon Gehr;Markus Püschel;Martin Vechev,gsingh@inf.ethz.ch;timon.gehr@inf.ethz.ch;pueschel@inf.ethz.ch;martin.vechev@inf.ethz.ch,4;5;6,3;4;3,Accept (Poster),0,4,0,yes,9/27/18,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10;10,10;10;10;10,4;1,9/27/18,21,6,13,1,0,2,456;716;275;4142,78;22;19;153,10;9;7;35,52;98;14;461,-1;-1
1519,ICLR,2019,Two-Timescale Networks for Nonlinear Value Function Approximation,Wesley Chung;Somjit Nath;Ajin Joseph;Martha White,wchung@ualberta.ca;somjit@ualberta.ca;ajoseph@ualberta.ca;whitem@ualberta.ca,6;7;6;6,4;4;4;4,Accept (Poster),0,7,0,yes,9/27/18,University of Alberta;University of Alberta;University of Alberta;University of Alberta,99;99;99;99,119;119;119;119,1,9/27/18,9,3,2,0,0,0,17;26;27;1440,8;7;15;66,3;2;4;17,0;1;0;167,-1;-1
1520,ICLR,2019,Deep Lagrangian Networks: Using Physics as Model Prior for Deep Learning,Michael Lutter;Christian Ritter;Jan Peters,michael@robot-learning.de;ritter@stud.tu-darmstadt.de;peters@ias.tu-darmstadt.de,7;4;7,4;5;3,Accept (Poster),0,9,0,yes,9/27/18,TU Darmstadt;TU Darmstadt;TU Darmstadt,65;65;65,244;244;244,8,9/27/18,44,22,18,0,0,1,115;257;15262,38;36;461,5;6;59,2;12;1257,-1;-1
1521,ICLR,2019,Improving MMD-GAN Training with Repulsive Loss Function,Wei Wang;Yuan Sun;Saman Halgamuge,weiw8@student.unimelb.edu.au;yuan.sun@rmit.edu.au;saman@unimelb.edu.au,7;7;6,5;5;2,Accept (Poster),2,5,0,yes,9/27/18,The University of Melbourne;Massachusetts Institute of Technology;The University of Melbourne,123;2;123,32;5;32,5;4,9/27/18,20,5,6,2,2,5,679;289;5153,137;62;282,14;8;31,32;12;475,-1;-1
1522,ICLR,2019,Deep Anomaly Detection with Outlier Exposure,Dan Hendrycks;Mantas Mazeika;Thomas Dietterich,hendrycks@berkeley.edu;mantas@ttic.edu;tgd@oregonstate.edu,6;6;8,4;5;4,Accept (Poster),3,9,1,yes,9/27/18,University of California Berkeley;Toyota Technological Institute at Chicago;Oregon State University,5;123;76,18;1103;318,3;5,9/27/18,132,81,56,10,2,26,1439;342;31532,26;7;367,14;5;64,268;64;3105,-1;-1
1523,ICLR,2019,Temporal Difference Variational Auto-Encoder,Karol Gregor;George Papamakarios;Frederic Besse;Lars Buesing;Theophane Weber,karol.gregor@gmail.com;g.papamakarios@ed.ac.uk;fbesse@google.com;lbuesing@google.com;theophane@google.com,8;9;7,4;4;5,Accept (Oral),0,3,1,yes,9/27/18,Google;University of Edinburgh;Google;Google;Google,-1;33;-1;-1;-1,-1;27;-1;-1;-1,5,6/8/18,25,17,2,0,17,3,4670;679;539;2577;1840,44;2;22;54;54,19;8;9;21;14,526;124;59;258;194,-1;-1
1524,ICLR,2019,Meta-Learning with Latent Embedding Optimization,Andrei A. Rusu;Dushyant Rao;Jakub Sygnowski;Oriol Vinyals;Razvan Pascanu;Simon Osindero;Raia Hadsell,andreirusu@google.com;dushyantr@google.com;sygi@google.com;vinyals@google.com;razp@google.com;osindero@google.com;raia@google.com,6;5;8,5;3;5,Accept (Poster),2,11,0,yes,9/27/18,Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,5;6,7/16/18,224,99,82,5,37,48,2989;654;1824;52477;16705;14818;8187,16;22;7;121;101;38;63,13;11;6;55;46;21;26,440;74;174;6537;1671;1866;789,-1;-1
1525,ICLR,2019,Active Learning with Partial Feedback,Peiyun Hu;Zachary C. Lipton;Anima Anandkumar;Deva Ramanan,peiyunh@cs.cmu.edu;zlipton@cmu.edu;anima@caltech.edu;deva@cs.cmu.edu,7;6;7,4;3;4,Accept (Poster),0,4,0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;California Institute of Technology;Carnegie Mellon University,1;1;140;1,24;24;3;24,,2/21/18,10,8,2,1,59,0,587;4649;5263;31105,16;97;186;163,7;27;38;59,85;424;739;5317,-1;-1
1526,ICLR,2019,A Universal Music Translation Network,Noam Mor;Lior Wolf;Adam Polyak;Yaniv Taigman,noam.mor@gmail.com;wolf@fb.com;adampolyak@fb.com;yaniv@fb.com,7;6;8,4;4;4,Accept (Poster),2,6,1,yes,9/27/18,Tel Aviv University;Facebook;Facebook;Facebook,37;-1;-1;-1,217;-1;-1;-1,,5/21/18,45,30,12,1,0,4,75;13690;757;5668,4;199;12;26,3;45;7;16,11;1627;58;563,-1;-1
1527,ICLR,2019,Query-Efficient Hard-label Black-box Attack: An Optimization-based Approach,Minhao Cheng;Thong Le;Pin-Yu Chen;Huan Zhang;JinFeng Yi;Cho-Jui Hsieh,mhcheng@ucla.edu;thmle@ucdavis.edu;pin-yu.chen@ibm.com;huan@huan-zhang.com;yijinfeng@jd.com;chohsieh@cs.ucla.edu,7;6;7,3;5;4,Accept (Poster),0,3,0,yes,9/27/18,"University of California, Los Angeles;University of California, Davis;International Business Machines;University of California, Los Angeles;JD AI Research;University of California, Los Angeles",20;81;-1;20;-1;20,15;54;-1;15;-1;15,4;1;9,7/12/18,78,44,25,2,13,12,297;90;180;1984;1914;12363,20;14;43;31;79;168,6;3;5;19;23;40,39;13;18;274;236;1715,-1;-1
1528,ICLR,2019,Multi-step Retriever-Reader Interaction for Scalable Open-domain Question Answering,Rajarshi Das;Shehzaad Dhuliawala;Manzil Zaheer;Andrew McCallum,rajarshi@cs.umass.edu;sdhuliawala@cs.umass.edu;manzil@cmu.edu;mccallum@cs.umass.edu,7;6;6,4;5;4,Accept (Poster),0,11,6,yes,9/27/18,"University of Massachusetts, Amherst;University of Massachusetts, Amherst;Carnegie Mellon University;University of Massachusetts, Amherst",30;30;1;30,191;191;24;191,,9/27/18,40,26,17,5,0,8,4796;180;1516;43231,164;14;63;434,32;3;17;96,406;40;256;4669,-1;-1
1529,ICLR,2019,Analysing Mathematical Reasoning Abilities of Neural Models,David Saxton;Edward Grefenstette;Felix Hill;Pushmeet Kohli,saxton@google.com;etg@google.com;felixhill@google.com;pushmeet@google.com,7;6;6,3;3;4,Accept (Poster),0,0,4,yes,9/27/18,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,8,9/27/18,41,22,10,1,0,3,995;6911;3460;21843,30;57;53;313,9;25;22;69,151;831;667;2735,-1;-1
1530,ICLR,2019,Bias-Reduced Uncertainty Estimation for Deep Neural Classifiers,Yonatan Geifman;Guy Uziel;Ran El-Yaniv,yonatan.g@cs.technion.ac.il;uzielguy@gmail.com;rani@cs.technion.ac.il,7;7;7,3;4;2,Accept (Poster),0,9,2,yes,9/27/18,Technion;Technion;Technion,25;25;25,327;327;327,11,5/21/18,17,11,2,0,4,2,173;28;6869,8;14;99,5;2;31,13;1;860,-1;-1
1531,ICLR,2019,The Deep Weight Prior,Andrei Atanov;Arsenii Ashukha;Kirill Struminsky;Dmitriy Vetrov;Max Welling,andrewatanov@yandex.ru;ars.ashuha@gmail.com;k.struminsky@gmail.com;vetrovd@yandex.ru;m.welling@uva.nl,6;8;7,4;4;3,Accept (Poster),0,8,0,yes,9/27/18,;Samsung;Higher School of Economics;Higher School of Economics;University of Amsterdam,-1;-1;478;478;169,-1;-1;377;377;59,5;11,9/27/18,14,6,2,0,51,1,39;460;21;2006;26150,12;14;9;123;269,3;8;2;16;57,4;79;1;275;5027,-1;-1
1532,ICLR,2019,Complement Objective Training,Hao-Yun Chen;Pei-Hsin Wang;Chun-Hao Liu;Shih-Chieh Chang;Jia-Yu Pan;Yu-Ting Chen;Wei Wei;Da-Cheng Juan,haoyunchen@gapp.nthu.edu.tw;peihsin@gapp.nthu.edu.tw;newgod1992@gapp.nthu.edu.tw;scchang@cs.nthu.edu.tw;jypan@google.com;yutingchen@google.com;wewei@google.com;dacheng@google.com,8;5;7,4;4;4,Accept (Poster),0,19,0,yes,9/27/18,National Tsing Hua University;National Tsing Hua University;National Tsing Hua University;National Tsing Hua University;Google;Google;Google;Google,199;199;199;199;-1;-1;-1;-1,323;323;323;323;-1;-1;-1;-1,3;4;2,9/27/18,6,5,4,0,0,2,14;5;234;1970;2548;102;5518;617,6;4;33;244;58;35;549;47,3;1;9;24;16;5;38;17,3;2;11;215;287;16;323;76,-1;-1
1533,ICLR,2019,Eidetic 3D LSTM: A Model for Video Prediction and Beyond,Yunbo Wang;Lu Jiang;Ming-Hsuan Yang;Li-Jia Li;Mingsheng Long;Li Fei-Fei,yunbowang1989@gmail.com;lujiang@google.com;mhyang@ucmerced.edu;lijiali@google.com;mingsheng@tsinghua.edu.cn;feifeili@cs.stanford.edu,7;7;7,5;4;4,Accept (Poster),0,6,0,yes,9/27/18,Tsinghua University;Google;University of California at Merced;Google;Tsinghua University;Stanford University,8;-1;478;-1;8;4,30;-1;1103;-1;30;3,,9/27/18,26,10,12,2,0,6,635;2007;5260;23584;6329;77542,29;49;125;71;85;448,11;25;26;32;35;94,51;236;1236;4251;1269;11452,-1;-1
1534,ICLR,2019,Diagnosing and Enhancing VAE Models,Bin Dai;David Wipf,v-bindai@microsoft.com;davidwipf@gmail.com,6;7;9,3;4;4,Accept (Poster),0,18,4,yes,9/27/18,Microsoft;Microsoft,-1;-1,-1;-1,5,9/27/18,80,54,24,4,6,20,348;4605,54;101,9;30,48;605,-1;-1
1535,ICLR,2019,Posterior Attention Models for Sequence to Sequence Learning,Shiv Shankar;Sunita Sarawagi,sshankar@umass.edu;sunita@iitb.ac.in,9;8;7,4;5;4,Accept (Poster),0,11,0,yes,9/27/18,"University of Massachusetts, Amherst;Indian Institute of Technology Bombay",30;115,191;367,,9/27/18,14,7,4,0,0,1,121;7698,15;150,5;39,17;697,-1;-1
1536,ICLR,2019,Building Dynamic Knowledge Graphs from Text using Machine Reading Comprehension,Rajarshi Das;Tsendsuren Munkhdalai;Xingdi Yuan;Adam Trischler;Andrew McCallum,rajarshi@cs.umass.edu;tsmunkhd@microsoft.com;eric.yuan@microsoft.com;adam.trischler@microsoft.com;mccallum@cs.umass.edu,7;6;7,4;4;4,Accept (Poster),0,6,0,yes,9/27/18,"University of Massachusetts, Amherst;Microsoft;Microsoft;Microsoft;University of Massachusetts, Amherst",30;-1;-1;-1;30,191;-1;-1;-1;191,10,9/27/18,34,21,10,2,45,6,4796;671;798;1533;43231,164;30;28;47;434,32;12;10;17;96,406;41;131;285;4669,-1;-1
1537,ICLR,2019,An analytic theory of generalization dynamics and transfer learning in deep linear networks,Andrew K. Lampinen;Surya Ganguli,lampinen@stanford.edu;sganguli@stanford.edu,8;7;6,4;4;3,Accept (Poster),5,3,0,yes,9/27/18,Stanford University;Stanford University,4;4,3;3,8;1;6,9/27/18,28,15,3,2,46,2,83;5819,23;129,5;39,6;588,-1;-1
1538,ICLR,2019,Deep Learning 3D Shapes Using Alt-az Anisotropic 2-Sphere Convolution,Min Liu;Fupin Yao;Chiho Choi;Ayan Sinha;Karthik Ramani,liu66@purdue.edu;yao153@purdue.edu;chihochoi@purdue.edu;asinha@magicleap.com;ramani@purdue.edu,6;8;7,3;5;5,Accept (Poster),0,10,0,yes,9/27/18,Purdue University;Purdue University;Purdue University;Magic Leap;Purdue University,26;26;26;-1;26,60;60;60;-1;60,,9/27/18,7,2,7,0,0,0,97;15;265;699;3600,61;2;20;24;159,5;2;7;8;27,7;0;18;37;236,-1;-1
1539,ICLR,2019,Exemplar Guided Unsupervised Image-to-Image Translation with Semantic Consistency,Liqian Ma;Xu Jia;Stamatios Georgoulis;Tinne Tuytelaars;Luc Van Gool,liqian.ma@esat.kuleuven.be;xu.jia@esat.kuleuven.be;georgous@ee.ethz.ch;tinne.tuytelaars@esat.kuleuven.be;luc.vangool@esat.kuleuven.be,6;5;8,4;5;4,Accept (Poster),0,5,0,yes,9/27/18,KU Leuven;KU Leuven;Swiss Federal Institute of Technology;KU Leuven;KU Leuven,115;115;10;115;115,47;47;10;47;47,,5/28/18,34,17,17,1,3,4,621;2919;435;28704;79894,19;547;22;339;1260,9;20;8;55;120,104;369;55;4211;10337,-1;-1
1540,ICLR,2019,Fixup Initialization: Residual Learning Without Normalization,Hongyi Zhang;Yann N. Dauphin;Tengyu Ma,hongyiz@mit.edu;yann@dauphin.io;tengyuma@stanford.edu,5;7;7,4;3;3,Accept (Poster),0,19,3,yes,9/27/18,Massachusetts Institute of Technology;Facebook;Stanford University,2;-1;4,5;-1;3,3;8,9/27/18,92,31,39,7,47,12,870;8386;3761,24;43;85,3;27;32,184;1017;490,-1;-1
1541,ICLR,2019,Capsule Graph Neural Network,Zhang Xinyi;Lihui Chen,xinyi001@e.ntu.edu.sg;elhchen@ntu.edu.sg,6;6;6,4;4;4,Accept (Poster),0,3,3,yes,9/27/18,National Taiwan University;National Taiwan University,85;85,197;197,10,9/27/18,46,18,18,2,0,9,48;109,23;20,1;6,9;11,-1;-1
1542,ICLR,2019,Self-Tuning Networks: Bilevel Optimization of Hyperparameters using Structured Best-Response Functions,Matthew Mackay;Paul Vicol;Jonathan Lorraine;David Duvenaud;Roger Grosse,mmackay@cs.toronto.edu;pvicol@cs.toronto.edu;lorraine@cs.toronto.edu;duvenaud@cs.toronto.edu;rgrosse@cs.toronto.edu,6;7;6,3;4;3,Accept (Poster),0,6,0,yes,9/27/18,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",18;18;18;18;18,22;22;22;22;22,,9/27/18,10,4,2,1,13,2,33;141;53;5748;5579,3;19;6;74;48,3;6;4;28;27,5;16;7;738;801,-1;-1
1543,ICLR,2019,Learning to Make Analogies by Contrasting Abstract Relational Structure,Felix Hill;Adam Santoro;David Barrett;Ari Morcos;Timothy Lillicrap,felixhill@google.com;adamsantoro@google.com;barrettdavid@google.com;arimorcos@google.com;countzero@google.com,6;7;7,3;3;5,Accept (Poster),1,0,11,yes,9/27/18,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,,9/27/18,15,8,0,0,23,1,3460;2930;1159;986;23113,53;36;24;31;74,22;19;11;12;39,667;337;157;110;2841,-1;-1
1544,ICLR,2019,DPSNet: End-to-end Deep Plane Sweep Stereo,Sunghoon Im;Hae-Gon Jeon;Stephen Lin;In So Kweon,dlarl8927@kaist.ac.kr;haegonj@andrew.cmu.edu;stevelin@microsoft.com;iskweon77@kaist.ac.kr,6;7;6,5;4;4,Accept (Poster),2,7,0,yes,9/27/18,Korea Advanced Institute of Science and Technology;Carnegie Mellon University;Microsoft;Korea Advanced Institute of Science and Technology,20;1;-1;20,95;24;-1;95,,9/27/18,36,21,20,1,0,10,226;899;9235;9392,24;33;227;514,7;13;53;41,27;140;905;1028,-1;-1
1545,ICLR,2019,M^3RL: Mind-aware Multi-agent Management Reinforcement Learning,Tianmin Shu;Yuandong Tian,tianmin.shu@ucla.edu;yuandong@fb.com,6;7;6,3;4;1,Accept (Poster),0,6,1,yes,9/27/18,"University of California, Los Angeles;Facebook",20;-1,15;-1,8,9/27/18,10,5,0,1,15,0,256;2412,20;84,9;25,19;281,-1;-1
1546,ICLR,2019,Hierarchical Reinforcement Learning via Advantage-Weighted Information Maximization,Takayuki Osa;Voot Tangkaratt;Masashi Sugiyama,osa@mfg.t.u-tokyo.ac.jp;voot.tangkaratt@riken.jp;sugi@k.u-tokyo.ac.jp,6;7;5,4;4;4,Accept (Poster),0,8,0,yes,9/27/18,The University of Tokyo;RIKEN;The University of Tokyo,54;-1;54,45;-1;45,,9/27/18,11,4,4,1,10,1,408;200;11435,32;25;712,11;8;51,13;18;1284,-1;-1
1547,ICLR,2019,ACCELERATING NONCONVEX LEARNING VIA REPLICA EXCHANGE LANGEVIN DIFFUSION,Yi Chen;Jinglin Chen;Jing Dong;Jian Peng;Zhaoran Wang,yichen2016@u.northwestern.edu;jinglinc@illinois.edu;jd2736@columbia.edu;jianpeng@illinois.edu;zhaoranwang@gmail.com,4;7;6,4;4;4,Accept (Poster),0,4,0,yes,9/27/18,"Northwestern University;University of Illinois, Urbana Champaign;Columbia University;University of Illinois, Urbana Champaign;Northwestern University",44;3;15;3;44,20;37;14;37;20,9,9/27/18,3,2,1,0,0,0,746;79;1154;2056;1127,193;19;149;120;77,14;4;19;23;19,38;6;62;203;128,-1;-1
1548,ICLR,2019,Improving Sequence-to-Sequence Learning via Optimal Transport,Liqun Chen;Yizhe Zhang;Ruiyi Zhang;Chenyang Tao;Zhe Gan;Haichao Zhang;Bai Li;Dinghan Shen;Changyou Chen;Lawrence Carin,liqun.chen@duke.edu;yizhe.zhang@microsoft.com;rz68@duke.edu;chenyang.tao@duke.edu;zhe.gan@microsoft.com;hczhang1@gmail.com;bai.li@duke.edu;dinghan.shen@duke.edu;cchangyou@gmail.com;lcarin@duke.edu,6;7;5,3;4;4,Accept (Poster),0,5,2,yes,9/27/18,"Duke University;Microsoft;Duke University;Duke University;Microsoft;Horizon Robotics;Duke University;Duke University;State University of New York, Buffalo;Duke University",44;-1;44;44;-1;-1;44;44;81;44,17;-1;17;17;-1;-1;17;17;270;17,3,9/27/18,17,7,8,0,0,3,161;1537;232;124;2304;1403;8721;857;1797;19323,53;79;30;21;85;63;641;34;94;819,8;17;10;5;25;20;48;13;24;65,12;172;32;13;319;125;517;103;244;1984,-1;-1
1549,ICLR,2019,Overcoming Catastrophic Forgetting for Continual Learning via Model Adaptation,Wenpeng Hu;Zhou Lin;Bing Liu;Chongyang Tao;Zhengwei Tao;Jinwen Ma;Dongyan Zhao;Rui Yan,wenpeng.hu@pku.edu.cn;scene@pku.edu.cn;liub@uic.edu;chongyangtao@pku.edu.cn;tttzw@pku.edu.cn;jwma@math.pku.edu.cn;zhaody@pku.edu.cn;ruiyan@pku.edu.cn,5;6;7,4;2;4,Accept (Poster),0,13,0,yes,9/27/18,"Peking University;Peking University;University of Illinois, Chicago;Peking University;Peking University;Peking University;Peking University;Peking University",24;24;57;24;24;24;24;24,27;27;255;27;27;27;27;27,,9/27/18,21,8,5,0,0,2,141;32;286;192;20;1990;2577;260,19;131;85;19;3;192;163;37,7;2;9;7;1;21;27;9,15;3;15;23;2;124;305;13,-1;-1
1550,ICLR,2019,Adaptive Posterior Learning: few-shot learning with a surprise-based memory module,Tiago Ramalho;Marta Garnelo,tiago.mpramalho@gmail.com;garnelo@google.com,7;6;7,4;4;3,Accept (Poster),0,6,1,yes,9/27/18,Google;Google,-1;-1,-1;-1,6,9/27/18,8,3,2,0,3,0,1887;914,12;23,5;11,339;108,-1;-1
1551,ICLR,2019,A Mean Field Theory of Batch Normalization,Greg Yang;Jeffrey Pennington;Vinay Rao;Jascha Sohl-Dickstein;Samuel S. Schoenholz,gregyang@microsoft.com;jpennin@google.com;vinaysrao@google.com;jaschasd@google.com;schsam@google.com,7;6;7,3;1;3,Accept (Poster),2,6,0,yes,9/27/18,Microsoft;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,,9/27/18,56,25,6,4,19,3,474;15984;165;4812;2894,23;51;19;101;70,10;20;8;33;20,50;2601;9;671;370,-1;-1
1552,ICLR,2019,Rethinking the Value of Network Pruning,Zhuang Liu;Mingjie Sun;Tinghui Zhou;Gao Huang;Trevor Darrell,zhuangl@berkeley.edu;sunmj15@gmail.com;tinghuiz@eecs.berkeley.edu;gaohuang.thu@gmail.com;trevor@eecs.berkeley.edu,6;7;6,5;5;4,Accept (Poster),20,40,1,yes,9/27/18,University of California Berkeley;Tsinghua University;University of California Berkeley;Cornell University;University of California Berkeley,5;8;5;7;5,18;30;18;19;18,,9/27/18,227,117,71,25,54,46,32305;417;7889;11982;87420,341;34;24;60;557,81;9;13;22;109,2315;51;1611;2026;11296,-1;-1
1553,ICLR,2019,Benchmarking Neural Network Robustness to Common Corruptions and Perturbations,Dan Hendrycks;Thomas Dietterich,hendrycks@berkeley.edu;tgd@oregonstate.edu,7;9;9,3;5;4,Accept (Poster),0,16,2,yes,9/27/18,University of California Berkeley;Oregon State University,5;76,18;318,4,7/4/18,194,117,78,7,0,44,1439;31532,26;367,14;64,268;3105,-1;-1
1554,ICLR,2019,Universal Transformers,Mostafa Dehghani;Stephan Gouws;Oriol Vinyals;Jakob Uszkoreit;Lukasz Kaiser,dehghani@uva.nl;sgouws@google.com;vinyals@google.com;usz@google.com;lukaszkaiser@google.com,8;6;6,4;4;2,Accept (Poster),0,13,0,yes,9/27/18,University of Amsterdam;Google;Google;Google;Google,169;-1;-1;-1;-1,59;-1;-1;-1;-1,3;8,7/10/18,145,65,32,4,0,17,821;3517;51380;11549;22012,74;23;121;36;75,13;12;55;22;24,75;320;6446;2652;3813,-1;-1
1555,ICLR,2019,Characterizing Audio Adversarial Examples Using Temporal Dependency,Zhuolin Yang;Bo Li;Pin-Yu Chen;Dawn Song,lucas110550@sjtu.edu.cn;lxbosky@gmail.com;pin-yu.chen@ibm.com;dawnsong@gmail.com,7;6;6,3;3;3,Accept (Poster),4,4,0,yes,9/27/18,Shanghai Jiao Tong University;University of California Berkeley;International Business Machines;University of California Berkeley,52;5;-1;5,188;18;-1;18,4,9/27/18,32,23,9,0,10,10,43;2355;2300;36440,6;80;104;275,3;23;23;95,13;268;216;4084,-1;-1
1556,ICLR,2019,Contingency-Aware Exploration in Reinforcement Learning,Jongwook Choi;Yijie Guo;Marcin Moczulski;Junhyuk Oh;Neal Wu;Mohammad Norouzi;Honglak Lee,jwook@umich.edu;guoyijie@umich.edu;marcin.lukasz.moczulski@gmail.com;junhyuk@umich.edu;neal@nealwu.com;mnorouzi@google.com;honglak@eecs.umich.edu,7;6;7,4;3;2,Accept (Poster),0,10,0,yes,9/27/18,University of Michigan;University of Michigan;University of Oxford;University of Michigan;Google;Google;University of Michigan,8;8;50;8;-1;-1;8,21;21;1;21;-1;-1;21,,9/27/18,25,14,3,1,13,4,542;632;437;1357;25;7899;24054,38;30;19;24;2;125;166,12;11;7;13;1;30;60,33;101;40;134;4;1002;2815,-1;-1
1557,ICLR,2019,Visual Semantic Navigation using Scene Priors,Wei Yang;Xiaolong Wang;Ali Farhadi;Abhinav Gupta;Roozbeh Mottaghi,wyang@ee.cuhk.edu.hk;xiaolonw@cs.cmu.edu;ali@cs.washington.edu;abhinavg@cs.cmu.edu;roozbehm@allenai.org,7;7;7,3;1;4,Accept (Poster),0,7,0,yes,9/27/18,The Chinese University of Hong Kong;Carnegie Mellon University;University of Washington;Carnegie Mellon University;Allen Institute for Artificial Intelligence,57;1;6;1;-1,40;24;25;24;-1,10;8,9/27/18,35,19,13,4,7,9,-1;3886;16016;17010;2952,-1;88;119;233;45,-1;20;42;63;18,0;557;2752;1903;451,-1;-1
1558,ICLR,2019,On Computation and Generalization of Generative Adversarial Networks under Spectrum Control,Haoming Jiang;Zhehui Chen;Minshuo Chen;Feng Liu;Dingding Wang;Tuo Zhao,jianghm@gatech.edu;zhchen@gatech.edu;mchen393@gatech.edu;fliu2016@fau.edu;wangd@fau.edu;tourzhao@gatech.edu,8;6;7,4;2;4,Accept (Poster),0,4,0,yes,9/27/18,Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Florida Atlantic University;Florida Atlantic University;Georgia Institute of Technology,13;13;13;314;314;13,33;33;33;712;712;33,5;4;8,9/27/18,3,2,1,0,0,1,237;44;50;1232;2133;2288,29;14;21;291;70;108,6;4;4;17;23;19,36;5;4;91;164;197,-1;-1
1559,ICLR,2019,The Comparative Power of ReLU Networks and Polynomial Kernels in the Presence of Sparse Latent Structure,Frederic Koehler;Andrej Risteski,fkoehler@mit.edu;risteski@mit.edu,7;7;7,3;3;3,Accept (Poster),0,1,0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2,5;5,4,9/27/18,0,0,0,0,0,0,93;613,22;35,6;12,7;68,-1;-1
1560,ICLR,2019,"The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks",Jonathan Frankle;Michael Carbin,jfrankle@mit.edu;mcarbin@csail.mit.edu,5;9;9,4;4;4,Accept (Oral),0,14,2,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2,5;5,,3/9/18,336,166,102,24,334,58,603;2200,22;61,8;19,87;202,-1;-1
1561,ICLR,2019,Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer,David Berthelot*;Colin Raffel*;Aurko Roy;Ian Goodfellow,dberth@google.com;craffel@gmail.com;aurkor@google.com;goodfellow@google.com,7;8;9,4;3;4,Accept (Poster),0,18,0,yes,9/27/18,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,,7/19/18,58,31,23,6,189,14,1368;4484;994;54462,30;63;21;90,10;22;10;56,218;461;109;9235,-1;-1
1562,ICLR,2019,Learning Implicitly Recurrent CNNs Through Parameter Sharing,Pedro Savarese;Michael Maire,savarese@ttic.edu;mmaire@uchicago.edu,8;7;6,4;3;4,Accept (Poster),0,7,0,yes,9/27/18,Toyota Technological Institute at Chicago;University of Chicago,123;48,1103;9,,9/27/18,8,3,4,1,3,1,39;15471,4;62,2;24,5;2754,-1;-1
1563,ICLR,2019,G-SGD: Optimizing ReLU Neural Networks in its Positively Scale-Invariant Space,Qi Meng;Shuxin Zheng;Huishuai Zhang;Wei Chen;Qiwei Ye;Zhi-Ming Ma;Nenghai Yu;Tie-Yan Liu,meq@microsoft.com;zhengsx@mail.ustc.edu.cn;huzhang@microsoft.com;wche@microsoft.com;qiwye@microsoft.com;mazm@amt.ac.cn;ynh@ustc.edu.cn;tyliu@microsoft.com,7;7;7,4;2;3,Accept (Poster),0,7,1,yes,9/27/18,Microsoft;University of Science and Technology of China;Microsoft;Microsoft;Microsoft;Chinese Academy of Sciences;University of Science and Technology of China;Microsoft,-1;478;-1;-1;-1;62;478;-1,-1;132;-1;-1;-1;1103;132;-1,1,2/11/18,5,2,3,0,5,1,1166;127;408;209;995;656;5687;13355,39;21;42;170;15;79;359;366,10;4;10;9;6;14;38;51,221;13;48;9;196;40;542;1719,-1;-1
1564,ICLR,2019,SGD Converges to Global Minimum in Deep Learning via Star-convex Path,Yi Zhou;Junjie Yang;Huishuai Zhang;Yingbin Liang;Vahid Tarokh,yi.zhou610@duke.edu;baymax@mail.ustc.edu.cn;huishuai.zhang@microsoft.com;liang.889@osu.edu;vahid.tarokh@duke.edu,8;6;5,4;4;5,Accept (Poster),0,5,0,yes,9/27/18,Duke University;University of Science and Technology of China;Microsoft;Ohio State University;Duke University,44;478;-1;76;44,17;132;-1;318;17,9,9/27/18,15,8,3,0,2,1,572;284;395;4938;20566,113;81;41;212;386,13;8;10;31;55,40;16;47;360;2372,-1;-1
1565,ICLR,2019,Discovery of Natural Language Concepts in Individual Units of CNNs,Seil Na;Yo Joong Choe;Dong-Hyun Lee;Gunhee Kim,seil.na@vision.snu.ac.kr;yj.c@kakaocorp.com;benjamin.lee@kakaobrain.com;gunhee@snu.ac.kr,6;6;6,4;4;3,Accept (Poster),0,6,0,yes,9/27/18,Seoul National University;Kakao;Kakao Brain;Seoul National University,41;-1;-1;41,74;-1;-1;74,3,9/27/18,2,2,0,0,0,0,55;28;2674;1968,3;10;233;85,2;3;22;23,8;1;198;249,-1;-1
1566,ICLR,2019,LanczosNet: Multi-Scale Deep Graph Convolutional Networks,Renjie Liao;Zhizhen Zhao;Raquel Urtasun;Richard Zemel,rjliao@cs.toronto.edu;zhizhenz@illinois.edu;urtasun@uber.com;zemel@cs.toronto.edu,7;7;8,3;5;4,Accept (Poster),0,4,0,yes,9/27/18,"Department of Computer Science, University of Toronto;University of Illinois, Urbana Champaign;Uber;Department of Computer Science, University of Toronto",18;3;-1;18,22;37;-1;22,10,9/27/18,29,15,10,0,2,3,1564;530;24119;21173,63;46;245;208,21;12;73;52,204;32;3420;2466,-1;-1
1567,ICLR,2019,Learning to Learn with Conditional Class Dependencies,Xiang Jiang;Mohammad Havaei;Farshid Varno;Gabriel Chartrand;Nicolas Chapados;Stan Matwin,xiang.jiang@dal.ca;mohammad@imagia.com;f.varno@dal.ca;gabriel@imagia.com;nic@imagia.com;stan@cs.dal.ca,6;8;4,3;3;5,Accept (Poster),0,4,0,yes,9/27/18,Dalhousie University;Imagia;Dalhousie University;Imagia;Imagia;Dalhousie University,314;-1;314;-1;-1;314,289;-1;289;-1;-1;289,6,9/27/18,14,4,5,1,0,0,68;1513;16;855;469;7066,12;27;3;24;44;387,5;9;2;12;11;32,3;80;0;36;32;519,-1;-1
1568,ICLR,2019,Optimal Transport Maps For Distribution Preserving Operations on Latent Spaces of Generative Models,Eirikur Agustsson;Alexander Sage;Radu Timofte;Luc Van Gool,aeirikur@vision.ee.ethz.ch;alexander.sage@gmail.com;radu.timofte@vision.ee.ethz.ch;vangool@vision.ee.ethz.ch,5;7;5,3;5;3,Accept (Poster),0,4,0,yes,9/27/18,Swiss Federal Institute of Technology;;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;-1;10;10,10;-1;10;10,5;4,11/6/17,10,5,2,0,39,1,1625;24;7248;79894,36;3;178;1260,14;2;35;120,226;4;971;10337,-1;-1
1569,ICLR,2019,MARGINALIZED AVERAGE ATTENTIONAL NETWORK FOR WEAKLY-SUPERVISED LEARNING,Yuan Yuan;Yueming Lyu;Xi Shen;Ivor W. Tsang;Dit-Yan Yeung,yuanyuan910115@gmail.com;lv_yueming@outlook.com;shenxiluc@gmail.com;ivor.tsang@uts.edu.au;dyyeung@cse.ust.hk,5;6;3,3;4;3,Accept (Poster),0,4,0,yes,9/27/18,The Hong Kong University of Science and Technology;University of Technology Sydney;ENPC;University of Technology Sydney;The Hong Kong University of Science and Technology,39;106;478;106;39,44;216;280;216;44,1,9/27/18,14,7,1,0,0,0,109;64;865;10456;11555,16;9;208;253;193,5;5;14;49;52,9;3;32;1257;1432,-1;-1
1570,ICLR,2019,Predict then Propagate: Graph Neural Networks meet Personalized PageRank,Johannes Klicpera;Aleksandar Bojchevski;Stephan Günnemann,klicpera@in.tum.de;a.bojchevski@in.tum.de;guennemann@in.tum.de,5;5;7,4;4;4,Accept (Poster),0,10,5,yes,9/27/18,Technical University Munich;Technical University Munich;Technical University Munich,54;54;54,41;41;41,10,9/27/18,65,35,25,1,0,18,120;563;2414,6;21;138,5;12;28,34;103;278,-1;-1
1571,ICLR,2019,Variance Reduction for Reinforcement Learning in Input-Driven Environments,Hongzi Mao;Shaileshh Bojja Venkatakrishnan;Malte Schwarzkopf;Mohammad Alizadeh,hongzi@csail.mit.edu;bjjvnkt@csail.mit.edu;malte@csail.mit.edu;alizadeh@csail.mit.edu,6;7;9,4;4;4,Accept (Poster),0,6,0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2,5;5;5;5,6,7/6/18,16,9,4,0,11,2,1452;268;1505;5116,24;27;52;109,11;11;15;33,180;36;155;988,-1;-1
1572,ICLR,2019,Algorithmic Framework for Model-based Deep Reinforcement Learning with Theoretical Guarantees,Yuping Luo;Huazhe Xu;Yuanzhi Li;Yuandong Tian;Trevor Darrell;Tengyu Ma,yupingl@cs.princeton.edu;huazhe_xu@eecs.berkeley.edu;yuanzhili92@gmail.com;yuandong@fb.com;trevor@eecs.berkeley.edu;tengyuma@stanford.edu,7;6;6,4;4;2,Accept (Poster),0,8,0,yes,9/27/18,Princeton University;University of California Berkeley;;Facebook;University of California Berkeley;Stanford University,30;5;-1;-1;5;4,7;18;-1;-1;18;3,1,7/10/18,41,23,24,2,33,10,0;786;3562;2412;87420;3761,2;13;116;84;557;85,0;7;29;25;109;32,0;98;428;281;11296;490,-1;-1
1573,ICLR,2019,Efficient Training on Very Large Corpora via Gramian Estimation,Walid Krichene;Nicolas Mayoraz;Steffen Rendle;Li Zhang;Xinyang Yi;Lichan Hong;Ed Chi;John Anderson,walidk@google.com;nmayoraz@google.com;srendle@google.com;liqzhang@google.com;xinyang@google.com;lichan@google.com;edchi@google.com;janders@google.com,8;7;7,4;4;2,Accept (Poster),8,4,0,yes,9/27/18,Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1,8,7/18/18,14,9,6,0,17,0,428;19;7038;4444;444;4415;10225;39310,51;4;51;417;27;82;210;598,12;2;23;27;10;27;47;86,27;0;1340;369;62;363;903;4354,-1;-1
1574,ICLR,2019,Representing Formal Languages: A Comparison Between Finite Automata and Recurrent Neural Networks ,Joshua J. Michalenko;Ameesh Shah;Abhinav Verma;Richard G. Baraniuk;Swarat Chaudhuri;Ankit B. Patel,jjm7@rice.edu;ameesh@rice.edu;averma@rice.edu;richb@rice.edu;swarat@rice.edu;abp4@rice.edu,7;5;5,3;3;3,Accept (Poster),1,10,0,yes,9/27/18,Rice University;Rice University;Rice University;Rice University;Rice University;Rice University,85;85;85;85;85;85,86;86;86;86;86;86,,9/27/18,5,0,1,0,2,0,8;247;71;29302;2637;354,4;8;6;660;97;30,2;5;3;83;26;6,0;7;1;2744;182;39,-1;-1
1575,ICLR,2019,From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following,Justin Fu;Anoop Korattikara;Sergey Levine;Sergio Guadarrama,justinjfu@eecs.berkeley.edu;kbanoop@google.com;svlevine@eecs.berkeley.edu;sguada@google.com,9;5;5,5;4;4,Accept (Poster),0,4,0,yes,9/27/18,University of California Berkeley;Google;University of California Berkeley;Google,5;-1;5;-1,18;-1;18;-1,3,9/27/18,28,13,8,0,12,1,520;1429;23990;17344,20;17;309;74,9;7;73;23,75;181;3116;2212,-1;-1
1576,ICLR,2019,Learning from Positive and Unlabeled Data with a Selection Bias,Masahiro Kato;Takeshi Teshima;Junya Honda,mkato@ms.k.u-tokyo.ac.jp;teshima@ms.k.u-tokyo.ac.jp;honda@edu.k.u-tokyo.ac.jp,7;6;5,2;4;4,Accept (Poster),0,3,0,yes,9/27/18,The University of Tokyo;The University of Tokyo;The University of Tokyo,54;54;54,45;45;45,,9/27/18,15,9,3,0,0,2,138;24;570,77;9;47,6;3;13,10;2;85,-1;-1
1577,ICLR,2019,Dynamic Channel Pruning: Feature Boosting and Suppression,Xitong Gao;Yiren Zhao;Łukasz Dudziak;Robert Mullins;Cheng-zhong Xu,xt.gao@siat.ac.cn;yaz21@cam.ac.uk;lukaszd.mail@gmail.com;robert.mullins@cl.cam.ac.uk;czxu@um.edu.mo,6;7;6;7,4;5;3;4,Accept (Poster),0,10,2,yes,9/27/18,Chinese Academy of Sciences;University of Cambridge;Samsung;University of Cambridge;,62;71;-1;71;-1,1103;2;-1;2;-1,,9/27/18,46,22,17,3,3,7,107;130;91;210;5489,14;17;12;40;343,5;7;4;8;41,16;12;11;18;361,-1;-1
1578,ICLR,2019,Maximal Divergence Sequential Autoencoder for Binary Software Vulnerability Detection,Tue Le;Tuan Nguyen;Trung Le;Dinh Phung;Paul Montague;Olivier De Vel;Lizhen Qu,tue.le.ict@jvn.edu.vn;nguyenvutuan1995@gmail.com;trunglm@monash.edu;dinh.phung@monash.edu;paul.montague@dst.defence.gov.au;olivier.devel@dst.defence.gov.au;lizhen.qu@data61.csiro.au,6;7;6;6,4;2;3;2,Accept (Poster),0,8,0,yes,9/27/18,";;Monash University;Monash University;;;, CSIRO",-1;-1;123;123;-1;-1;-1,-1;-1;80;80;-1;-1;-1,5,9/27/18,6,2,3,1,0,1,26;8;764;4582;308;1398;903,5;3;84;301;33;87;34,3;1;11;30;8;16;14,2;1;77;409;18;105;136,-1;-1
1579,ICLR,2019,Large Scale GAN Training for High Fidelity Natural Image Synthesis,Andrew Brock;Jeff Donahue;Karen Simonyan,ajb5@hw.ac.uk;jeffdonahue@google.com;simonyan@google.com,9;7;8,4;3;4,Accept (Oral),10,11,1,yes,9/27/18,Heriot-Watt University;Google;Google,261;-1;-1,363;-1;-1,5;4,9/27/18,913,543,267,28,0,156,2036;34151;58825,88;56;95,15;25;38,238;4652;10592,-1;-1
1580,ICLR,2019,Learning Exploration Policies for Navigation,Tao Chen;Saurabh Gupta;Abhinav Gupta,taoc1@andrew.cmu.edu;sgupta@eecs.berkeley.edu;abhinavg@cs.cmu.edu,7;3;7,4;5;5,Accept (Poster),0,18,0,yes,9/27/18,Carnegie Mellon University;University of California Berkeley;Carnegie Mellon University,1;5;1,24;18;24,,9/27/18,26,18,8,0,2,6,13908;472;17010,1016;77;233,55;11;63,678;52;1903,-1;-1
1581,ICLR,2019,Variational Autoencoders with Jointly Optimized Latent Dependency Structure,Jiawei He;Yu Gong;Joseph Marino;Greg Mori;Andreas Lehrmann,jha203@sfu.ca;yu_gong@sfu.ca;jmarino@caltech.edu;mori@cs.sfu.ca;andreas.lehrmann@gmail.com,7;8;6,4;5;3,Accept (Poster),0,9,0,yes,9/27/18,Simon Fraser University;Simon Fraser University;California Institute of Technology;Simon Fraser University;Facebook,62;62;140;62;-1,253;253;3;253;-1,5;11;10,9/27/18,5,2,2,0,0,1,190;47;283;9498;192,42;25;63;196;17,8;4;8;44;6,21;5;16;812;22,-1;-1
1582,ICLR,2019,Efficient Multi-Objective Neural Architecture Search via Lamarckian Evolution,Thomas Elsken;Jan Hendrik Metzen;Frank Hutter,thomas.elsken@de.bosch.com;janhendrik.metzen@de.bosch.com;fh@cs.uni-freiburg.de,6;6;6,3;3;4,Accept (Poster),0,4,0,yes,9/27/18,Bosch;Bosch;Universität Freiburg,-1;-1;123,-1;-1;82,,4/24/18,124,51,46,0,15,11,659;1774;12359,24;64;233,8;15;49,69;171;1501,-1;-1
1583,ICLR,2019,Amortized Bayesian Meta-Learning,Sachin Ravi;Alex Beatson,sachinr@princeton.edu;abeatson@cs.princeton.edu,6;6;5,3;4;3,Accept (Poster),0,5,0,yes,9/27/18,Princeton University;Princeton University,30;30,7;7,4;6,9/27/18,26,10,13,0,0,2,1312;114,22;14,10;5,195;7,-1;-1
1584,ICLR,2019,How to train your MAML,Antreas Antoniou;Harrison Edwards;Amos Storkey,a.antoniou@sms.ed.ac.uk;h.l.edwards@sms.ac.uk;a.storkey@sms.ed.ac.uk,5;6;7,3;5;4,Accept (Poster),4,5,0,yes,9/27/18,University of Edinburgh;;University of Edinburgh,33;-1;33,27;-1;27,6;8,9/27/18,86,50,30,3,6,18,763;1146;3776,104;18;198,11;8;31,96;186;431,-1;-1
1585,ICLR,2019,Dynamically Unfolding Recurrent Restorer: A Moving Endpoint Control Method for Image Restoration,Xiaoshuai Zhang;Yiping Lu;Jiaying Liu;Bin Dong,jet@pku.edu.cn;luyiping9712@pku.edu.cn;liujiaying@pku.edu.cn;dongbin@math.pku.edu.cn,6;6;7,3;5;4,Accept (Poster),0,9,1,yes,9/27/18,Peking University;Peking University;Peking University;Peking University,24;24;24;24,27;27;27;27,,5/20/18,22,12,8,0,2,2,277;457;2395;1739,38;11;194;138,10;7;23;19,9;47;325;120,-1;-1
1586,ICLR,2019,Learning To Simulate,Nataniel Ruiz;Samuel Schulter;Manmohan Chandraker,nruiz9@bu.edu;samuel@nec-labs.com;manu@nec-labs.com,6;7;6,4;4;5,Accept (Poster),0,5,0,yes,9/27/18,Boston University;NEC-Labs;NEC-Labs,65;-1;-1,70;-1;-1,2,9/27/18,26,10,9,1,25,4,150;1141;3267,10;34;93,4;15;33,24;183;395,-1;-1
1587,ICLR,2019,What do you learn from context? Probing for sentence structure in contextualized word representations,Ian Tenney;Patrick Xia;Berlin Chen;Alex Wang;Adam Poliak;R Thomas McCoy;Najoung Kim;Benjamin Van Durme;Samuel R. Bowman;Dipanjan Das;Ellie Pavlick,iftenney@google.com;paxia@cs.jhu.edu;bchen6@swarthmore.edu;alexwang@nyu.edu;azpoliak@cs.jhu.edu;tom.mccoy@jhu.edu;n.kim@jhu.edu;vandurme@cs.jhu.edu;bowman@nyu.edu;dipanjand@google.com;ellie_pavlick@brown.edu,7;7;7,4;4;4,Accept (Poster),0,5,0,yes,9/27/18,Google;Johns Hopkins University;Swarthmore College;New York University;Johns Hopkins University;Johns Hopkins University;Johns Hopkins University;Johns Hopkins University;New York University;Google;Brown University,-1;72;478;26;72;72;72;72;26;-1;65,-1;13;1103;27;13;13;13;13;27;-1;50,3,9/27/18,157,79,40,9,0,13,503;350;1680;1182;486;392;229;4988;5922;5258;1289,28;15;238;27;21;18;16;194;80;68;61,8;8;22;10;10;9;4;38;27;29;16,45;28;89;229;60;51;21;551;1216;721;232,-1;-1
1588,ICLR,2019,"Towards Robust, Locally Linear Deep Networks",Guang-He Lee;David Alvarez-Melis;Tommi S. Jaakkola,guanghe@csail.mit.edu;davidam@csail.mit.edu;tommi@csail.mit.edu,8;8;7,3;4;4,Accept (Poster),0,9,1,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2,5;5;5,,9/27/18,13,7,1,0,0,0,97;500;21774,18;18;292,6;11;69,13;44;2308,-1;-1
1589,ICLR,2019,Gradient descent aligns the layers of deep linear networks,Ziwei Ji;Matus Telgarsky,ziweiji2@illinois.edu;mjt@illinois.edu,9;6;7,4;5;4,Accept (Poster),0,6,0,yes,9/27/18,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",3;3,37;37,,9/27/18,47,29,5,5,23,8,149;1935,16;44,5;15,24;335,-1;-1
1590,ICLR,2019,Bayesian Deep Convolutional Networks with Many Channels are Gaussian Processes,Roman Novak;Lechao Xiao;Yasaman Bahri;Jaehoon Lee;Greg Yang;Jiri Hron;Daniel A. Abolafia;Jeffrey Pennington;Jascha Sohl-dickstein,romann@google.com;xlc@google.com;yasamanb@google.com;jaehlee@google.com;gregyang@microsoft.com;jh2084@cam.ac.uk;danabo@google.com;jpennin@google.com;jaschasd@google.com,7;7;7;6,3;2;5;4,Accept (Poster),0,18,1,yes,9/27/18,Google;Google;Google;Google;Microsoft;University of Cambridge;Google;Google;Google,-1;-1;-1;-1;-1;71;-1;-1;-1,-1;-1;-1;-1;-1;2;-1;-1;-1,11,9/27/18,72,48,17,5,3,9,724;427;995;594;500;454;270;16291;4982,12;19;30;55;24;28;8;51;101,9;8;10;8;10;7;4;20;33,108;55;117;94;53;62;31;2631;700,-1;-1
1591,ICLR,2019,MisGAN: Learning from Incomplete Data with Generative Adversarial Networks,Steven Cheng-Xian Li;Bo Jiang;Benjamin Marlin,cxl@cs.umass.edu;bjiang@sjtu.edu.cn;marlin@cs.umass.edu,7;6;7,4;5;4,Accept (Poster),0,3,0,yes,9/27/18,"University of Massachusetts, Amherst;Shanghai Jiao Tong University;University of Massachusetts, Amherst",30;52;30,191;188;191,5;4,9/27/18,21,8,6,1,4,5,76;3725;2697,4;550;80,3;26;25,11;111;286,-1;-1
1592,ICLR,2019,Information Theoretic lower bounds on negative log likelihood,Luis A. Lastras-Montaño,lastrasl@us.ibm.com,6;7;6,3;3;4,Accept (Poster),0,8,0,yes,9/27/18,International Business Machines,-1,-1,1,9/27/18,2,2,0,0,2,0,1220,24,8,179,-1
1593,ICLR,2019,A Data-Driven and Distributed Approach to Sparse Signal Representation and Recovery,Ali Mousavi;Gautam Dasarathy;Richard G. Baraniuk,ali.mousavi1988@gmail.com;gautamd@asu.edu;richb@rice.edu,7;8;6,3;4;3,Accept (Poster),0,6,0,yes,9/27/18,Google;Arizona State University;Rice University,-1;95;85,-1;126;86,,9/27/18,11,4,3,0,0,0,614;467;29302,63;40;660,11;13;83,52;47;2744,-1;-1
1594,ICLR,2019,Efficient Augmentation via Data Subsampling,Michael Kuchnik;Virginia Smith,mkuchnik@andrew.cmu.edu;smithv@cmu.edu,6;7;6,4;4;3,Accept (Poster),0,3,0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University,1;1,24;24,,9/27/18,5,2,1,0,10,0,12;1734,9;100,2;17,1;193,-1;-1
1595,ICLR,2019,Explaining Image Classifiers by Counterfactual Generation,Chun-Hao Chang;Elliot Creager;Anna Goldenberg;David Duvenaud,kingsley@cs.toronto.edu;creager@cs.toronto.edu;anna.goldenberg@utoronto.ca;duvenaud@cs.toronto.edu,5;7;5,4;3;5,Accept (Poster),0,6,2,yes,9/27/18,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Toronto University;Department of Computer Science, University of Toronto",18;18;18;18,22;22;22;22,5,7/20/18,41,22,12,5,3,5,203;249;386;5748,34;12;21;74,8;5;8;28,26;41;25;738,-1;-1
1596,ICLR,2019,A Max-Affine Spline Perspective of Recurrent Neural Networks,Zichao Wang;Randall Balestriero;Richard Baraniuk,richb@rice.edu;zw16@rice.edu;randallbalestriero@gmail.com,6;6;6,3;3;3,Accept (Poster),0,3,0,yes,9/27/18,Rice University;Rice University;Rice University,85;85;85,86;86;86,1;8,9/27/18,2,1,0,0,0,0,8;112;29302,15;38;660,2;5;83,0;2;2744,-1;-1
1597,ICLR,2019,Learning Actionable Representations with Goal Conditioned Policies,Dibya Ghosh;Abhishek Gupta;Sergey Levine,dibya.ghosh@berkeley.edu;abhigupta@berkeley.edu;svlevine@eecs.berkeley.edu,6;6;5,4;4;4,Accept (Poster),0,13,0,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,18;18;18,5,9/27/18,22,17,1,0,73,1,95;1683;23990,12;204;309,4;18;73,3;169;3116,-1;-1
1598,ICLR,2019,Supervised Community Detection with Line Graph Neural Networks,Zhengdao Chen;Lisha Li;Joan Bruna,zc1216@nyu.edu;lapis.lazuli.8@gmail.com;bruna@cims.nyu.edu,6;9;8,4;4;4,Accept (Poster),0,5,0,yes,9/27/18,New York University;;New York University,26;-1;26,27;-1;27,5;10,5/23/17,57,22,13,1,12,10,216;203;11209,12;150;90,7;6;28,27;36;1266,-1;-1
1599,ICLR,2019,Unsupervised Learning via Meta-Learning,Kyle Hsu;Sergey Levine;Chelsea Finn,kyle.hsu@mail.utoronto.ca;svlevine@eecs.berkeley.edu;cbfinn@eecs.berkeley.edu,7;6;6;8,4;3;3;4,Accept (Poster),0,8,0,yes,9/27/18,Toronto University;University of California Berkeley;University of California Berkeley,18;5;5,22;18;18,6,9/27/18,50,24,19,3,288,12,85;23990;7600,8;309;98,4;73;33,16;3116;1021,-1;-1
1600,ICLR,2019,Music Transformer: Generating Music with Long-Term Structure,Cheng-Zhi Anna Huang;Ashish Vaswani;Jakob Uszkoreit;Ian Simon;Curtis Hawthorne;Noam Shazeer;Andrew M. Dai;Matthew D. Hoffman;Monica Dinculescu;Douglas Eck,chengzhiannahuang@gmail.com;avaswani@google.com;uszkoreit@google.com;iansimon@google.com;fjord@google.com;noam@google.com;adai@google.com;mhoffman@google.com;noms@google.com;deck@google.com,7;6;4;5,3;4;4;3,Accept (Poster),0,12,0,yes,9/27/18,Harvard University;Google;Google;Google;Google;Google;Google;Google;Google;Google,39;-1;-1;-1;-1;-1;-1;-1;-1;-1,6;-1;-1;-1;-1;-1;-1;-1;-1;-1,,9/27/18,47,27,25,4,0,11,255;11499;11549;2477;387;12752;3743;8418;113;876,16;52;36;40;20;44;50;94;19;23,7;22;22;16;9;19;19;29;4;11,31;2668;2652;219;70;2769;464;1213;17;86,-1;-1
1601,ICLR,2019,"Deep, Skinny Neural Networks are not Universal Approximators",Jesse Johnson,jejo.math@gmail.com,6;8;7,4;4;4,Accept (Poster),2,2,1,yes,9/27/18,,,,,9/27/18,6,4,1,1,0,1,332,93,10,39,-1
1602,ICLR,2019,Multilingual Neural Machine Translation with Knowledge Distillation,Xu Tan;Yi Ren;Di He;Tao Qin;Zhou Zhao;Tie-Yan Liu,xuta@microsoft.com;rayeren613@gmail.com;dihe@microsoft.com;taoqin@microsoft.com;zhaozhou@zju.edu.cn;tyliu@microsoft.com,7;7;7,4;3;4,Accept (Poster),0,16,6,yes,9/27/18,Microsoft;Zhejiang University;Microsoft;Microsoft;Zhejiang University;Microsoft,-1;57;-1;-1;57;-1,-1;177;-1;-1;177;-1,3,9/27/18,43,23,19,3,0,4,4746;1240;2385;4828;1796;13112,154;289;257;289;171;365,24;18;26;33;24;51,426;64;109;585;168;1708,-1;-1
1603,ICLR,2019,textTOvec: DEEP CONTEXTUALIZED NEURAL AUTOREGRESSIVE TOPIC MODELS OF LANGUAGE WITH DISTRIBUTED COMPOSITIONAL PRIOR,Pankaj Gupta;Yatin Chaudhary;Florian Buettner;Hinrich Schuetze,pankaj_gupta96@yahoo.com;yatinchaudhary91@gmail.com;fbuettner.phys@gmail.com;hinrich@hotmail.com,8;7;6,4;4;4,Accept (Poster),6,18,1,yes,9/27/18,Siemens Corporate Research;Technical University Munich;Siemens Corporate Research;Institut für Informatik,-1;54;-1;-1,-1;41;-1;-1,3;5;8,9/27/18,5,3,2,1,9,0,3738;16;1217;18455,166;8;32;319,26;2;12;46,384;0;65;2284,-1;-1
1604,ICLR,2019,Learning Embeddings into Entropic Wasserstein Spaces,Charlie Frogner;Farzaneh Mirzazadeh;Justin Solomon,frogner@mit.edu;farzaneh@ibm.com;jsolomon@mit.edu,7;7;3,3;4;4,Accept (Poster),0,12,3,yes,9/27/18,Massachusetts Institute of Technology;International Business Machines;Massachusetts Institute of Technology,2;-1;2,5;-1;5,,9/27/18,6,6,4,0,2,1,363;79;2044,11;16;140,6;6;21,30;3;164,-1;-1
1605,ICLR,2019,Deep Layers as Stochastic Solvers,Adel Bibi;Bernard Ghanem;Vladlen Koltun;Rene Ranftl,adel.bibi@kaust.edu.sa;bernard.ghanem@kaust.edu.sa;vkoltun@gmail.com;ranftlr@gmail.com,7;7;8,5;4;1,Accept (Poster),2,8,0,yes,9/27/18,KAUST;KAUST;Intel;Intel,123;123;-1;-1,1103;1103;-1;-1,9,9/27/18,10,4,3,1,0,0,438;6139;17398;1259,25;199;187;34,9;36;62;17,63;986;2484;186,-1;-1
1606,ICLR,2019,Learning to Describe Scenes with Programs,Yunchao Liu;Zheng Wu;Daniel Ritchie;William T. Freeman;Joshua B. Tenenbaum;Jiajun Wu,georgeycliu@gmail.com;14wuzheng@sjtu.edu.cn;daniel_ritchie@brown.edu;billf@mit.edu;jbt@mit.edu;jiajunwu@mit.edu,6;4;6,3;3;4,Accept (Poster),0,13,1,yes,9/27/18,Tsinghua University;Shanghai Jiao Tong University;Brown University;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,8;52;65;2;2;2,30;188;50;5;5;5,,9/27/18,15,5,5,0,0,0,119;29;188;43118;30100;3884,10;13;53;402;588;90,5;3;8;91;82;29,0;0;6;4333;2639;373,-1;-1
1607,ICLR,2019,BA-Net: Dense Bundle Adjustment Networks,Chengzhou Tang;Ping Tan,cta73@sfu.ca;pingtan@sfu.ca,8;7;9,4;4;4,Accept (Oral),0,6,0,yes,9/27/18,Simon Fraser University;Simon Fraser University,62;62,253;253,1,6/13/18,63,23,20,3,45,11,120;5715,13;215,5;39,21;443,-1;-1
1608,ICLR,2019,Stochastic Prediction of Multi-Agent Interactions from Partial Observations,Chen Sun;Per Karlsson;Jiajun Wu;Joshua B Tenenbaum;Kevin Murphy,chensun@google.com;perk@google.com;jiajunwu@mit.edu;jbt@mit.edu;kpmurphy@google.com,6;6;6,4;4;4,Accept (Poster),0,6,0,yes,9/27/18,Google;Google;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Google,-1;-1;2;2;-1,-1;-1;5;5;-1,10,9/27/18,23,15,6,0,2,4,4626;1585;3884;30100;15719,307;98;90;588;83,31;22;29;82;40,465;81;373;2639;2261,-1;-1
1609,ICLR,2019,"Plan Online, Learn Offline: Efficient Learning and Exploration via Model-Based Control",Kendall Lowrey;Aravind Rajeswaran;Sham Kakade;Emanuel Todorov;Igor Mordatch,kendall.lowrey@gmail.com;rajeswaran.aravind@gmail.com;sham@cs.washington.edu;etodorov@gmail.com;mordatch@openai.com,5;6;4,5;4;3,Accept (Poster),0,7,0,yes,9/27/18,"University of Washington, Seattle;University of Washington;University of Washington;Roboti LLC;OpenAI",6;6;6;-1;-1,25;25;25;-1;-1,,9/27/18,45,19,17,1,236,5,476;861;13295;6839;2989,10;24;196;121;48,7;13;57;36;26,34;85;1953;675;345,-1;-1
1610,ICLR,2019,Composing Complex Skills by Learning Transition Policies,Youngwoon Lee*;Shao-Hua Sun*;Sriram Somasundaram;Edward S. Hu;Joseph J. Lim,lee504@usc.edu;shaohuas@usc.edu;sriramso@usc.edu;hues@usc.edu;limjj@usc.edu,7;9;7,4;4;4,Accept (Poster),0,5,0,yes,9/27/18,University of Southern California;University of Southern California;University of Southern California;University of Southern California;University of Southern California,30;30;30;30;30,66;66;66;66;66,,9/27/18,10,7,0,0,0,0,399;162;59;160;2901,11;36;15;18;51,5;6;5;7;20,68;15;5;16;262,-1;-1
1611,ICLR,2019,The role of over-parametrization in generalization of neural networks,Behnam Neyshabur;Zhiyuan Li;Srinadh Bhojanapalli;Yann LeCun;Nathan Srebro,bneyshabur@gmail.com;zhiyuanli@cs.princeton.edu;srinadh@ttic.edu;yann@cs.nyu.edu;nati@ttic.edu,7;7;7,3;5;3,Accept (Poster),0,5,0,yes,9/27/18,New York University;Princeton University;Toyota Technological Institute at Chicago;New York University;Toyota Technological Institute at Chicago,26;30;123;26;123,27;7;1103;27;1103,1;8,5/30/18,165,89,14,6,331,12,2336;623;1558;90005;13175,27;14;28;345;176,18;9;15;107;52,311;105;190;10225;1596,-1;-1
1612,ICLR,2019,Learning Procedural Abstractions and Evaluating Discrete Latent Temporal Structure,Karan Goel;Emma Brunskill,kgoel93@gmail.com;ebrun@cs.stanford.edu,5;6;7,2;3;3,Accept (Poster),0,6,1,yes,9/27/18,Stanford University;Stanford University,4;4,3;3,11,9/27/18,0,0,0,0,0,0,11;3038,13;149,2;28,0;287,-1;-1
1613,ICLR,2019,Learning Mixed-Curvature Representations in Product Spaces,Albert Gu;Frederic Sala;Beliz Gunel;Christopher Ré,albertgu@stanford.edu;fredsala@stanford.edu;bgunel@stanford.edu;chrismre@cs.stanford.edu,7;7;7,5;2;3,Accept (Poster),0,8,0,yes,9/27/18,Stanford University;Stanford University;Stanford University;Stanford University,4;4;4;4,3;3;3;3,3;10,9/27/18,28,18,8,1,0,9,222;1716;124;7210,31;107;6;210,7;25;3;44,28;108;13;704,-1;-1
1614,ICLR,2019,Synthetic Datasets for Neural Program Synthesis,Richard Shin;Neel Kant;Kavi Gupta;Chris Bender;Brandon Trabucco;Rishabh Singh;Dawn Song,ricshin@berkeley.edu;kantneel@berkeley.edu;kavi@berkeley.edu;chrisbender@berkeley.edu;btrabucco@berkeley.edu;rising@google.com;dawnsong@cs.berkeley.edu,6;6;7,4;2;3,Accept (Poster),0,6,0,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;Google;University of California Berkeley,5;5;5;5;5;-1;5,18;18;18;18;18;-1;18,8,9/27/18,9,5,3,0,0,1,279;69;9;17;9;897;36930,29;14;2;5;2;53;276,9;4;1;2;1;12;95,27;4;1;1;1;96;4079,-1;-1
1615,ICLR,2019,Energy-Constrained Compression for Deep Neural Networks via Weighted Sparse Projection and Layer Input Masking,Haichuan Yang;Yuhao Zhu;Ji Liu,h.yang@rochester.edu;yzhu@rochester.edu;ji.liu.uwisc@gmail.com,7;7;7,4;4;3,Accept (Poster),0,9,0,yes,9/27/18,University of Rochester;University of Rochester;University of Rochester,106;106;106,153;153;153,1,6/12/18,10,4,5,0,4,1,160;713;3307,32;59;63,8;15;23,4;57;523,-1;-1
1616,ICLR,2019,The Unusual Effectiveness of Averaging in GAN Training,Yasin Yaz{\i}c{\i};Chuan-Sheng Foo;Stefan Winkler;Kim-Hui Yap;Georgios Piliouras;Vijay Chandrasekhar,yasin001@e.ntu.edu.sg;foocs@i2r.a-star.edu.sg;stefan.winkler@adsc-create.edu.sg;ekhyap@ntu.edu.sg;georgios@sutd.edu.sg;vijay@i2r.a-star.edu.sg,6;5;6,4;2;4,Accept (Poster),3,6,0,yes,9/27/18,National Taiwan University;A*STAR;ADSC;National Taiwan University;Singapore University of Technology and Design;A*STAR,85;-1;-1;85;478;-1,197;-1;-1;197;1103;-1,5,6/12/18,41,12,18,2,20,5,71;259;5030;1060;1214;2955,6;11;206;131;99;109,2;6;33;19;19;27,5;59;377;76;78;221,-1;-1
1617,ICLR,2019,Selfless Sequential Learning,Rahaf Aljundi;Marcus Rohrbach;Tinne Tuytelaars,rahaf.aljundi@gmail.com;mrf@fb.com;tinne.tuytelaars@esat.kuleuven.be,6;6;7,5;4;4,Accept (Poster),0,7,0,yes,9/27/18,KU Leuven;Facebook;KU Leuven,115;-1;115,47;-1;47,,6/14/18,26,15,8,1,15,7,567;11157;28704,21;90;339,11;42;55,78;1514;4211,-1;-1
1618,ICLR,2019,Learning Protein Structure with a Differentiable Simulator,John Ingraham;Adam Riesselman;Chris Sander;Debora Marks,john.ingraham@gmail.com;adam.riesselman@gmail.com;cccsander@gmail.com;deboramarks@gmail.com,6;7;6;7,3;5;5;3,Accept (Oral),4,7,0,yes,9/27/18,Massachusetts Institute of Technology;Harvard University;Harvard University;Harvard University,2;39;39;39,5;6;6;6,,9/27/18,31,16,8,0,0,5,519;256;49142;14217,13;14;274;141,8;7;85;39,33;17;3578;870,-1;-1
1619,ICLR,2019,DOM-Q-NET:  Grounded RL on Structured Language,Sheng Jia;Jamie Ryan Kiros;Jimmy Ba,sheng.jia@mail.utoronto.ca;kirosjamie@gmail.com;jba@cs.utoronto.ca,7;6;7,3;3;1,Accept (Poster),0,10,0,yes,9/27/18,Toronto University;Google;Toronto University,18;-1;18,22;-1;22,10,9/27/18,2,1,2,0,2,0,11;1417;51070,26;18;52,2;9;21,2;176;8381,-1;-1
1620,ICLR,2019,Predicting the Generalization Gap in Deep Networks with Margin Distributions,Yiding Jiang;Dilip Krishnan;Hossein Mobahi;Samy Bengio,ydjiang@google.com;dilipkay@google.com;hmobahi@google.com;bengio@google.com,6;5;9,4;4;4,Accept (Poster),2,5,0,yes,9/27/18,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,8,9/27/18,36,22,10,3,29,3,253;4825;2098;25964,9;73;43;331,6;22;16;67,20;630;171;3437,-1;-1
1621,ICLR,2019,Neural TTS Stylization with Adversarial and Collaborative Games,Shuang Ma;Daniel Mcduff;Yale Song,shuangma@buffalo.edu;damcduff@microsoft.com;yalesong@csail.mit.edu,6;6;7;6,5;5;5;3,Accept (Poster),0,14,1,yes,9/27/18,"State University of New York, Buffalo;Microsoft;Massachusetts Institute of Technology",81;-1;2,270;-1;5,5;4,9/27/18,2,2,1,0,0,0,1082;3579;1575,207;103;43,15;28;20,60;416;227,-1;-1
1622,ICLR,2019,Learning a Meta-Solver for Syntax-Guided Program Synthesis,Xujie Si;Yuan Yang;Hanjun Dai;Mayur Naik;Le Song,xsi@cis.upenn.edu;yyang754@gatech.edu;hanjundai@gatech.edu;mhnaik@cis.upenn.edu;lsong@cc.gatech.edu,7;7;7,5;4;2,Accept (Poster),0,14,0,yes,9/27/18,University of Pennsylvania;Georgia Institute of Technology;Georgia Institute of Technology;University of Pennsylvania;Georgia Institute of Technology,19;13;13;19;13,10;33;33;10;33,6;10,9/27/18,15,5,4,0,0,1,146;254;1819;5601;9128,17;43;55;87;328,7;10;17;28;52,14;11;268;635;1088,-1;-1
1623,ICLR,2019,DyRep: Learning Representations over Dynamic Graphs,Rakshit Trivedi;Mehrdad Farajtabar;Prasenjeet Biswal;Hongyuan Zha,rstrivedi@gatech.edu;farajtabar@google.com;bprasenjeet1108@gmail.com;zha@cc.gatech.edu,6;7;8,4;5;4,Accept (Poster),4,9,4,yes,9/27/18,Georgia Institute of Technology;Google;Georgia Institute of Technology;Georgia Institute of Technology,13;-1;13;13,33;-1;33;33,10,9/27/18,26,16,7,0,0,5,401;1119;73;14010,30;45;6;405,12;19;3;62,35;95;10;1215,-1;-1
1624,ICLR,2019,Modeling the Long Term Future in Model-Based Reinforcement Learning,Nan Rosemary Ke;Amanpreet Singh;Ahmed Touati;Anirudh Goyal;Yoshua Bengio;Devi Parikh;Dhruv Batra,rosemary.nan.ke@gmail.com;asg@fb.com;ahmed.touati@umontreal.ca;anirudhgoyal9119@gmail.com;yoshua.umontreal@gmail.com;parikh@gatech.edu;dbatra@gatech.edu,7;6;6,4;4;4,Accept (Poster),0,23,0,yes,9/27/18,Polytechnique Montreal;Facebook;University of Montreal;University of Montreal;University of Montreal;Georgia Institute of Technology;Georgia Institute of Technology,386;-1;123;123;123;13;13,108;-1;108;108;108;33;33,,9/27/18,6,2,3,0,0,2,754;1085;97;1115;203485;80;8415,32;15;17;46;807;18;147,13;8;6;12;147;4;41,76;238;11;127;24050;9;1147,-1;-1
1625,ICLR,2019,ProbGAN: Towards Probabilistic GAN with Theoretical Guarantees,Hao He;Hao Wang;Guang-He Lee;Yonglong Tian,haohe@mit.edu;hwang87@mit.edu;guanghe@mit.edu;yonglong@mit.edu,5;6;9,4;3;4,Accept (Poster),0,9,0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2,5;5;5;5,5;4;11,9/27/18,3,0,0,0,0,0,267;-1;97;1578,74;-1;18;22,7;-1;6;13,19;0;13;175,-1;-1
1626,ICLR,2019,Defensive Quantization: When Efficiency Meets Robustness,Ji Lin;Chuang Gan;Song Han,jilin@mit.edu;ganchuang1990@gmail.com;songhan@mit.edu,7;6;7,4;3;2,Accept (Poster),13,4,0,yes,9/27/18,Massachusetts Institute of Technology;International Business Machines;Massachusetts Institute of Technology,2;-1;2,5;-1;5,4,9/27/18,40,21,16,1,3,4,1642;2202;15090,52;81;374,18;26;36,185;226;1942,-1;-1
1627,ICLR,2019,Identifying and Controlling Important Neurons in Neural Machine Translation,Anthony Bau;Yonatan Belinkov;Hassan Sajjad;Nadir Durrani;Fahim Dalvi;James Glass,abau@mit.edu;belinkov@mit.edu;hsajjad@hbku.edu.qa;ndurrani@qf.org.qa;faimaduddin@qf.org.qa;glass@mit.edu,7;10;6,3;3;4,Accept (Poster),0,5,1,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Peking University;QCRI;QCRI;Massachusetts Institute of Technology,2;2;24;199;199;2,5;5;27;1103;1103;5,3,9/27/18,38,24,10,0,6,3,142;1526;1156;1317;446;10669,7;63;62;65;34;342,5;21;18;20;10;54,9;164;108;102;36;961,-1;-1
1628,ICLR,2019,Knowledge Flow: Improve Upon Your Teachers,Iou-Jen Liu;Jian Peng;Alexander Schwing,iliu3@illinois.edu;jianpeng@illinois.edu;aschwing@illinois.edu,6;8;7,3;5;4,Accept (Poster),0,7,1,yes,9/27/18,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",3;3;3,37;37;37,,9/27/18,9,6,4,0,0,3,63;3589;3644,8;137;116,6;24;31,15;311;340,-1;-1
1629,ICLR,2019,FUNCTIONAL VARIATIONAL BAYESIAN NEURAL NETWORKS,Shengyang Sun;Guodong Zhang;Jiaxin Shi;Roger Grosse,ssy@cs.toronto.edu;gdzhang.cs@gmail.com;shijx15@mails.tsinghua.edu.cn;rgrosse@cs.toronto.edu,7;6;6,4;4;3,Accept (Poster),0,7,0,yes,9/27/18,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Tsinghua University;Department of Computer Science, University of Toronto",18;18;8;18,22;22;30;22,11;1,9/27/18,44,21,13,1,6,4,437;1272;36;5579,23;16;10;48,8;11;3;27,38;213;5;801,-1;-1
1630,ICLR,2019,K for the Price of 1: Parameter-efficient Multi-task and Transfer Learning,Pramod Kaushik Mudrakarta;Mark Sandler;Andrey Zhmoginov;Andrew Howard,pramodkm@uchicago.edu;mark.sandler@gmail.com;azhmogin@google.com;howarda@google.com,6;7;8,3;5;4,Accept (Poster),0,5,0,yes,9/27/18,University of Chicago;Google;Google;Google,48;-1;-1;-1,9;-1;-1;-1,6,9/27/18,7,4,2,0,19,2,115;3081;2522;7662,16;67;46;27,5;15;9;10,8;733;643;1761,-1;-1
1631,ICLR,2019,Neural Program Repair by Jointly Learning to Localize and Repair,Marko Vasic;Aditya Kanade;Petros Maniatis;David Bieber;Rishabh Singh,vasic@utexas.edu;akanade@google.com;maniatis@google.com;dbieber@google.com;rising@google.com,7;6;5,4;5;5,Accept (Poster),0,11,0,yes,9/27/18,"University of Texas, Austin;Google;Google;Google;Google",22;-1;-1;-1;-1,49;-1;-1;-1;-1,,9/27/18,23,15,5,2,0,3,61;694;5795;27;2128,8;49;86;4;80,4;13;36;2;24,5;45;652;3;183,-1;-1
1632,ICLR,2019,On the Minimal Supervision for Training Any Binary Classifier from Only Unlabeled Data,Nan Lu;Gang Niu;Aditya Krishna Menon;Masashi Sugiyama,lu@ms.k.u-tokyo.ac.jp;gang.niu@riken.jp;adityakmenon@google.com;sugi@k.u-tokyo.ac.jp,7;8;8;7,4;3;3;4,Accept (Poster),0,15,0,yes,9/27/18,The University of Tokyo;RIKEN;Google;The University of Tokyo,54;-1;-1;54,45;-1;-1;45,1,8/31/18,19,14,3,0,17,0,3243;1124;2263;11435,336;77;77;712,31;16;23;51,141;133;280;1284,-1;-1
1633,ICLR,2019,Regularized Learning for  Domain Adaptation under Label Shifts,Kamyar Azizzadenesheli;Anqi Liu;Fanny Yang;Animashree Anandkumar,kazizzad@uci.edu;anqiliu@caltech.edu;fan.yang@stat.math.ethz.ch;anima@caltech.edu,7;6;6,3;4;4,Accept (Poster),0,6,0,yes,9/27/18,"University of California, Irvine;California Institute of Technology;Swiss Federal Institute of Technology;California Institute of Technology",35;140;10;140,99;3;10;3,1;8,9/27/18,24,16,6,1,0,5,665;275;218;5263,37;46;17;186,10;9;9;38,96;16;23;739,-1;-1
1634,ICLR,2019,Toward Understanding the Impact of Staleness in Distributed Machine Learning,Wei Dai;Yi Zhou;Nanqing Dong;Hao Zhang;Eric Xing,daviddai@apple.com;zhou.1172@osu.edu;nanqing.dong@petuum.com;hao.zhang@petuum.com;eric.xing@petuum.com,7;4;9,5;5;4,Accept (Poster),0,5,0,yes,9/27/18,Apple;Ohio State University;Petuum Inc.;Petuum Inc.;Petuum Inc.,-1;76;-1;-1;-1,-1;318;-1;-1;-1,9,9/27/18,11,10,1,2,0,2,1673;-1;235;17053;23758,50;-1;11;1218;602,21;-1;8;62;75,233;0;30;1030;2640,-1;-1
1635,ICLR,2019,Generalizable Adversarial Training via Spectral Normalization,Farzan Farnia;Jesse Zhang;David Tse,farnia@stanford.edu;jessez@stanford.edu;dntse@stanford.edu,6;6;5,4;5;3,Accept (Poster),2,6,0,yes,9/27/18,Stanford University;Stanford University;Stanford University,4;4;4,3;3;3,4;1;8,9/27/18,25,14,5,1,0,3,176;134;45885,19;11;337,6;5;70,23;11;5828,-1;-1
1636,ICLR,2019,Don't let your Discriminator  be fooled,Brady Zhou;Philipp Krähenbühl,brady.zhou@utexas.edu;philkr@cs.utexas.edu,6;7;7,4;3;3,Accept (Poster),0,4,0,yes,9/27/18,"University of Texas, Austin;University of Texas, Austin",22;22,49;49,5;4,9/27/18,6,5,1,0,0,0,36;8643,4;43,3;26,7;1531,-1;-1
1637,ICLR,2019,Diversity and Depth in Per-Example Routing Models,Prajit Ramachandran;Quoc V. Le,prajitram@gmail.com;qvl@google.com,7;6;6,5;4;5,Accept (Poster),0,5,0,yes,9/27/18,"University of Illinois, Urbana Champaign;Google",3;-1,37;-1,,9/27/18,7,5,2,1,0,3,1042;46847,14;193,8;79,133;5930,-1;-1
1638,ICLR,2019,Latent Convolutional Models,ShahRukh Athar;Evgeny Burnaev;Victor Lempitsky,sathar@cs.stonybrook.edu;e.burnaev@skoltech.ru;lempitsky@skoltech.ru,7;6;7,4;3;2,Accept (Poster),0,4,0,yes,9/27/18,"State University of New York, Stony Brook;Skolkovo Institute of Science and Technology;Skolkovo Institute of Science and Technology",41;-1;-1,258;-1;-1,5;4,6/16/18,9,5,0,0,5,0,11;16;14288,3;5;131,2;2;49,0;0;2212,-1;-1
1639,ICLR,2019,Learning Grid Cells as Vector Representation of Self-Position Coupled with Matrix Representation of Self-Motion,Ruiqi Gao;Jianwen Xie;Song-Chun Zhu;Ying Nian Wu,ruiqigao@ucla.edu;jianwen@ucla.edu;sczhu@stat.ucla.edu;ywu@stat.ucla.edu,7;7;8,5;4;4,Accept (Poster),0,10,0,yes,9/27/18,"University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles",20;20;20;20,15;15;15;15,,9/27/18,9,4,6,0,0,2,221;662;13798;5652,30;58;450;276,8;14;61;37,17;48;984;594,-1;-1
1640,ICLR,2019,Meta-Learning Update Rules for Unsupervised Representation Learning,Luke Metz;Niru Maheswaranathan;Brian Cheung;Jascha Sohl-Dickstein,lmetz@google.com;nirum@google.com;bcheung@berkeley.edu;jaschasd@google.com,8;8;8,3;4;3,Accept (Oral),0,3,1,yes,9/27/18,Google;Google;University of California Berkeley;Google,-1;-1;5;-1,-1;-1;18;-1,5;6,3/31/18,67,32,12,2,281,6,7122;678;1465;4812,25;38;46;101,10;11;12;33,1195;50;144;671,-1;-1
1641,ICLR,2019,Discriminator Rejection Sampling,Samaneh Azadi;Catherine Olsson;Trevor Darrell;Ian Goodfellow;Augustus Odena,sazadi@berkeley.edu;catherio@google.com;trevor@eecs.berkeley.edu;goodfellow@google.com;augustusodena@google.com,7;6;6,4;3;4,Accept (Poster),0,9,1,yes,9/27/18,University of California Berkeley;Google;University of California Berkeley;Google;Google,5;-1;5;-1;-1,18;-1;18;-1;-1,5,9/27/18,39,18,13,0,317,9,263;2402;87420;54462;3202,12;22;557;90;25,7;13;109;56;12,36;97;11296;9235;473,-1;-1
1642,ICLR,2019,TimbreTron: A WaveNet(CycleGAN(CQT(Audio))) Pipeline for Musical Timbre Transfer,Sicong Huang;Qiyang Li;Cem Anil;Xuchan Bao;Sageev Oore;Roger B. Grosse,huang@cs.toronto.edu;colinli@cs.toronto.edu;anilcem@cs.toronto.edu;jennybao@cs.toronto.edu;sageev@dal.ca;rgrosse@cs.toronto.edu,4;7;8,5;4;4,Accept (Poster),3,7,5,yes,9/27/18,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Dalhousie University;Department of Computer Science, University of Toronto",18;18;18;18;314;18,22;22;22;22;289;22,,9/27/18,10,3,4,0,23,0,30;55;248;86;325;5579,3;11;6;3;28;48,2;4;4;3;9;27,4;1;20;2;31;801,-1;-1
1643,ICLR,2019,Smoothing the Geometry of Probabilistic Box Embeddings,Xiang Li;Luke Vilnis;Dongxu Zhang;Michael Boratko;Andrew McCallum,xiangl@cs.umass.edu;luke@cs.umass.edu;dongxuzhang@cs.umass.edu;mboratko@math.umass.edu;mccallum@cs.umass.edu,7;8;8,3;3;4,Accept (Oral),0,6,0,yes,9/27/18,"University of Massachusetts, Amherst;University of Massachusetts, Amherst;University of Massachusetts, Amherst;University of Massachusetts, Amherst;University of Massachusetts, Amherst",30;30;30;30;30,191;191;191;191;191,10,9/27/18,11,4,7,0,0,1,111;1713;203;29;43287,5;25;14;5;434,4;10;6;2;96,19;292;32;1;4672,-1;-1
1644,ICLR,2019,Bounce and Learn: Modeling Scene Dynamics with Real-World Bounces,Senthil Purushwalkam;Abhinav Gupta;Danny Kaufman;Bryan Russell,spurushw@andrew.cmu.edu;abhinavg@cs.cmu.edu;dkaufman@adobe.com;brussell@adobe.com,8;6;7,4;3;4,Accept (Poster),0,8,0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;Adobe Systems;Adobe Systems,1;1;-1;-1,24;24;-1;-1,,9/27/18,8,7,1,1,0,0,211;17010;783;7330,12;233;55;57,7;63;15;32,31;1903;35;777,-1;-1
1645,ICLR,2019,Critical Learning Periods in Deep Networks,Alessandro Achille;Matteo Rovere;Stefano Soatto,achille@cs.ucla.edu;matrovere@gmail.com;soatto@cs.ucla.edu,9;8;6,4;4;5,Accept (Poster),0,6,0,yes,9/27/18,"University of California, Los Angeles;;University of California, Los Angeles",20;-1;20,15;-1;15,,11/24/17,41,29,7,5,59,8,598;82;15269,31;16;458,11;5;61,74;7;1426,-1;-1
1646,ICLR,2019,GANSynth: Adversarial Neural Audio Synthesis,Jesse Engel;Kumar Krishna Agrawal;Shuo Chen;Ishaan Gulrajani;Chris Donahue;Adam Roberts,jesseengel@google.com;kumarkagrawal@gmail.com;chenshuo@google.com;igul222@gmail.com;christopherdonahue@gmail.com;adarob@google.com,6;7;8,3;4;3,Accept (Poster),0,4,0,yes,9/27/18,"Google;Google;Google;Google;University of California, San Diego;Google",-1;-1;-1;-1;11;-1,-1;-1;-1;-1;31;-1,5;4,9/27/18,64,32,18,2,0,9,2150;136;9499;4148;419;10042,36;14;598;9;16;36,12;5;44;7;7;19,242;20;444;897;59;1421,-1;-1
1647,ICLR,2019,A Closer Look at Few-shot Classification,Wei-Yu Chen;Yen-Cheng Liu;Zsolt Kira;Yu-Chiang Frank Wang;Jia-Bin Huang,weiyuc@andrew.cmu.edu;ycliu@gatech.edu;zkira@gatech.edu;ycwang@ntu.edu.tw;jbhuang@vt.edu,6;6;6,2;5;4,Accept (Poster),0,7,12,yes,9/27/18,Carnegie Mellon University;Georgia Institute of Technology;Georgia Institute of Technology;National Taiwan University;Virginia Tech,1;13;13;85;81,24;33;33;197;300,6;8,9/27/18,186,92,90,23,0,59,1360;464;1108;2765;5280,94;40;69;150;81,18;9;16;29;30,181;102;157;365;1081,-1;-1
1648,ICLR,2019,Robustness May Be at Odds with Accuracy,Dimitris Tsipras;Shibani Santurkar;Logan Engstrom;Alexander Turner;Aleksander Madry,tsipras@mit.edu;shibani@mit.edu;engstrom@mit.edu;turneram@mit.edu;madry@mit.edu,8;7;8,3;4;2,Accept (Poster),0,5,0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2;2,5;5;5;5;5,4;8,5/30/18,358,180,53,35,4,49,3696;1468;1918;426;5263,33;26;27;13;84,16;14;15;4;29,893;173;254;59;1054,-1;-1
1649,ICLR,2019,Visceral Machines: Risk-Aversion in  Reinforcement Learning with Intrinsic Physiological Rewards,Daniel McDuff;Ashish Kapoor,damcduff@microsoft.com;akapoor@microsoft.com,6;6;7,4;4;5,Accept (Poster),0,4,0,yes,9/27/18,Microsoft;Microsoft,-1;-1,-1;-1,,5/25/18,6,3,1,0,7,0,3579;5693,103;185,28;41,416;445,-1;-1
1650,ICLR,2019,Diffusion Scattering Transforms on Graphs,Fernando Gama;Alejandro Ribeiro;Joan Bruna,fgama@seas.upenn.edu;aribeiro@seas.upenn.edu;bruna@cims.nyu.edu,7;6;9,3;4;5,Accept (Poster),0,7,0,yes,9/27/18,University of Pennsylvania;University of Pennsylvania;New York University,19;19;26,10;10;27,2;10,6/22/18,27,15,10,3,9,6,289;7569;11209,44;313;90,8;44;28,19;581;1266,-1;-1
1651,ICLR,2019,Learning Self-Imitating Diverse Policies,Tanmay Gangwani;Qiang Liu;Jian Peng,gangwan2@uiuc.edu;lqiang@cs.utexas.edu;jianpeng@illinois.edu,6;8;8,2;3;4,Accept (Poster),0,11,2,yes,9/27/18,"University of Illinois, Urbana-Champaign;University of Texas, Austin;University of Illinois, Urbana Champaign",3;22;3,37;49;37,,5/25/18,22,9,12,1,4,1,57;4804;2056,14;646;120,4;30;23,5;240;203,-1;-1
1652,ICLR,2019,Adaptive Input Representations for Neural Language Modeling,Alexei Baevski;Michael Auli,alexei.b@gmail.com;michael.auli@gmail.com,7;8;8,4;4;4,Accept (Poster),0,6,0,yes,9/27/18,Facebook;Facebook,-1;-1,-1;-1,3,9/27/18,79,25,37,2,13,16,691;6740,17;71,8;30,104;1015,-1;-1
1653,ICLR,2019,Top-Down Neural Model For Formulae,Karel Chvalovský,karel@chvalovsky.cz,6;6;6,2;3;4,Accept (Poster),0,5,0,yes,9/27/18,Czech Technical University in Prague,314,740,,9/27/18,4,1,2,0,0,0,51,8,4,6,-1
1654,ICLR,2019,Deep learning generalizes because the parameter-function map is biased towards simple functions,Guillermo Valle-Perez;Chico Q. Camargo;Ard A. Louis,guillermo.valle@dtc.ox.ac.uk;chico.camargo@gmail.com;ard.louis@physics.ox.ac.uk,7;5;4,4;3;4,Accept (Poster),0,8,0,yes,9/27/18,University of Oxford;;University of Oxford,50;-1;50,1;-1;1,1;8,5/22/18,36,23,7,3,154,1,39;60;2490,6;10;166,2;3;27,1;2;80,-1;-1
1655,ICLR,2019,GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding,Alex Wang;Amanpreet Singh;Julian Michael;Felix Hill;Omer Levy;Samuel R. Bowman,alexwang@nyu.edu;amanpreet@nyu.edu;julianjm@cs.washington.edu;felixhill@google.com;omerlevy@cs.washington.edu;bowman@nyu.edu,8;7;5,4;1;2,Accept (Poster),2,3,0,yes,9/27/18,New York University;New York University;University of Washington;Google;University of Washington;New York University,26;26;6;-1;6;26,27;27;25;-1;25;27,3;8,4/20/18,688,341,358,32,21,167,1162;1066;969;3460;7424;5834,27;15;15;53;58;80,10;8;8;22;30;27,225;235;226;667;1206;1204,-1;-1
1656,ICLR,2019,Learning to Navigate the Web,Izzeddin Gur;Ulrich Rueckert;Aleksandra Faust;Dilek Hakkani-Tur,izzeddingur@gmail.com;rueckert@google.com;sandrafaust@google.com;dilek@ieee.org,8;7;7,3;3;3,Accept (Poster),0,4,0,yes,9/27/18,UC Santa Barbara;Google;Google;Amazon Alexa AI,37;-1;-1;-1,53;-1;-1;-1,3;6,9/27/18,5,3,3,0,8,0,158;10;381;5557,15;8;39;210,7;2;9;41,17;0;9;447,-1;-1
1657,ICLR,2019,Learnable Embedding Space for Efficient Neural Architecture Compression,Shengcao Cao;Xiaofang Wang;Kris M. Kitani,caoshengcao@pku.edu.cn;xiaofan2@cs.cmu.edu;kkitani@cs.cmu.edu,6;7;5,3;4;3,Accept (Poster),1,15,11,yes,9/27/18,Peking University;Carnegie Mellon University;Carnegie Mellon University,24;1;1,27;24;24,11,9/27/18,15,6,4,0,0,2,14;5862;2403,1;400;115,1;35;25,2;381;228,-1;-1
1658,ICLR,2019,Universal  Stagewise Learning for Non-Convex Problems with  Convergence on  Averaged Solutions,Zaiyi Chen;Zhuoning Yuan;Jinfeng Yi;Bowen Zhou;Enhong Chen;Tianbao Yang,czy6516@hotmail.com;zhuoning-yuan@uiowa.edu;jinfengyi.ustc@gmail.com;bwen@jd.com;cheneh@ustc.edu.cn;tianbao-yang@uiowa.edu,6;8;6,4;4;4,Accept (Poster),0,8,0,yes,9/27/18,;University of Iowa;JD AI Research;JD AI Research;University of Science and Technology of China;University of Iowa,-1;153;-1;-1;478;153,-1;223;-1;-1;132;223,9;8,8/20/18,28,14,15,3,0,2,91;109;1914;6305;7521;3112,13;12;79;187;429;187,5;4;23;31;40;28,13;9;236;880;529;340,-1;-1
1659,ICLR,2019,Discriminator-Actor-Critic: Addressing Sample Inefficiency and Reward Bias in Adversarial Imitation Learning,Ilya Kostrikov;Kumar Krishna Agrawal;Debidatta Dwibedi;Sergey Levine;Jonathan Tompson,kostrikov@cs.nyu.edu;kumarkagrawal@gmail.com;debidatta@google.com;slevine@google.com;tompson@google.com,6;8;7,4;2;3,Accept (Poster),6,15,3,yes,9/27/18,New York University;Google;Google;Google;Google,26;-1;-1;-1;-1,27;-1;-1;-1;-1,4,9/9/18,32,15,14,3,0,8,585;136;209;23990;3122,15;14;17;309;30,8;5;6;73;15,56;20;30;3116;370,-1;-1
1660,ICLR,2019,Learning concise representations for regression by evolving networks of trees,William La Cava;Tilak Raj Singh;James Taggart;Srinivas Suri;Jason H. Moore,lacava@upenn.edu;tilakraj@seas.upenn.edu;surisr@seas.upenn.edu;jhmoore@upenn.edu,6;7;8,3;1;4,Accept (Poster),0,5,0,yes,9/27/18,University of Pennsylvania;University of Pennsylvania;University of Pennsylvania;University of Pennsylvania,19;19;19;19,10;10;10;10,8,7/3/18,4,1,2,0,3,0,590;7;90;4;16622,42;9;25;3;615,11;2;5;1;60,39;1;3;0;934,-1;-1
1661,ICLR,2019,Interpolation-Prediction Networks for Irregularly Sampled Time Series,Satya Narayan Shukla;Benjamin Marlin,snshukla@cs.umass.edu;marlin@cs.umass.edu,6;6;6,4;4;4,Accept (Poster),0,10,0,yes,9/27/18,"University of Massachusetts, Amherst;University of Massachusetts, Amherst",30;30,191;191,,9/27/18,12,8,6,0,0,3,52;2697,22;80,4;25,4;286,-1;-1
1662,ICLR,2019,GAN Dissection: Visualizing and Understanding Generative Adversarial Networks,David Bau;Jun-Yan Zhu;Hendrik Strobelt;Bolei Zhou;Joshua B. Tenenbaum;William T. Freeman;Antonio Torralba,davidbau@csail.mit.edu;junyanz@csail.mit.edu;hendrik.strobelt@ibm.com;bzhou@csail.mit.edu;jbt@csail.mit.edu;billf@csail.mit.edu;torralba@csail.mit.edu,7;7;8,3;4;4,Accept (Poster),0,4,0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology;International Business Machines;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;-1;2;2;2;2,5;5;-1;5;5;5;5,5;4;2,9/27/18,108,54,21,4,2,11,1385;15561;1468;9706;30684;43621;48368,39;51;56;93;590;402;281,14;27;17;33;83;92;88,122;3010;94;1670;2668;4337;6312,-1;-1
1663,ICLR,2019,Deterministic PAC-Bayesian generalization bounds for deep networks via generalizing noise-resilience,Vaishnavh Nagarajan;Zico Kolter,vaishnavh@cs.cmu.edu;zkolter@cs.cmu.edu,5;7;8;7,4;3;5;2,Accept (Poster),0,30,1,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University,1;1,24;24,11;1;8,9/27/18,26,16,7,0,0,5,303;7460,22;104,8;34,39;1045,-1;-1
1664,ICLR,2019,Deep reinforcement learning with relational inductive biases,Vinicius Zambaldi;David Raposo;Adam Santoro;Victor Bapst;Yujia Li;Igor Babuschkin;Karl Tuyls;David Reichert;Timothy Lillicrap;Edward Lockhart;Murray Shanahan;Victoria Langston;Razvan Pascanu;Matthew Botvinick;Oriol Vinyals;Peter Battaglia,vzambaldi@google.com;draposo@google.com;adamsantoro@google.com;vbapst@google.com;yujiali@google.com;ibab@google.com;karltuyls@google.com;reichert@google.com;countzero@google.com;locked@google.com;mshanahan@google.com;vlangston@google.com;razp@google.com;botvinick@google.com;vinyals@google.com;peterbattaglia@google.com,6;7;7,4;3;4,Accept (Poster),0,7,0,yes,9/27/18,Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,8,9/27/18,44,26,13,0,0,5,1517;564;3010;1574;2927;2995;3723;949;23726;744;4839;1012;16705;13541;52477;4491,18;23;35;32;54;262;282;18;74;18;166;13;101;146;121;88,12;7;20;15;15;26;33;11;39;8;38;11;46;45;55;29,158;60;340;167;398;224;277;84;2898;107;454;95;1671;1412;6537;419,-1;-1
1665,ICLR,2019,Tree-Structured Recurrent Switching Linear Dynamical Systems for Multi-Scale Modeling,Josue Nassar;Scott Linderman;Monica Bugallo;Il Memming Park,josue.nassar@stonybrook.edu;scott.linderman@columbia.edu;monica.bugallo@stonybrook.edu;memming.park@stonybrook.edu,7;7;6,2;2;4,Accept (Poster),0,7,0,yes,9/27/18,"State University of New York, Stony Brook;Columbia University;State University of New York, Stony Brook;State University of New York, Stony Brook",41;15;41;41,258;14;258;258,11,9/27/18,12,4,3,0,52,1,15;741;1199;640,4;44;157;60,1;13;21;13,1;88;68;45,-1;-1
1666,ICLR,2019,Biologically-Plausible Learning Algorithms Can Scale to Large Datasets,Will Xiao;Honglin Chen;Qianli Liao;Tomaso Poggio,xiaow@fas.harvard.edu;chenhonglin@g.ucla.edu;lql@mit.edu;tp@csail.mit.edu,9;9;4,5;4;4,Accept (Poster),0,14,0,yes,9/27/18,"Harvard University;University of California, Los Angeles;Massachusetts Institute of Technology;Massachusetts Institute of Technology",39;20;2;2,6;15;5;5,,9/27/18,18,11,6,1,9,3,73;43;1043;34272,5;23;58;359,4;4;15;75,6;5;65;2751,-1;-1
1667,ICLR,2019,Generative predecessor models for sample-efficient imitation learning,Yannick Schroecker;Mel Vecerik;Jon Scholz,yannickschroecker@gatech.edu;vec@google.com;jscholz@google.com,7;5;6,4;5;3,Accept (Poster),0,12,0,yes,9/27/18,Georgia Institute of Technology;Google;Google,13;-1;-1,33;-1;-1,5,9/27/18,12,3,7,0,0,1,64;98;1151,9;7;40,4;5;14,3;13;112,-1;-1
1668,ICLR,2019,Kernel RNN Learning (KeRNL),Christopher Roth;Ingmar Kanitscheider;Ila Fiete,christopher_roth@utexas.edu;ingmar@openai.com;fiete@mit.edu,7;5;6,4;1;4,Accept (Poster),0,3,0,yes,9/27/18,"University of Texas, Austin;OpenAI;Massachusetts Institute of Technology",22;-1;2,49;-1;5,,9/27/18,5,3,1,1,0,2,69;1007;1226,12;19;39,3;10;16,10;104;113,-1;-1
1669,ICLR,2019,Integer Networks for Data Compression with Latent-Variable Models,Johannes Ballé;Nick Johnston;David Minnen,jballe@google.com;nickj@google.com;dminnen@google.com,7;8;6,3;3;3,Accept (Poster),0,9,1,yes,9/27/18,Google;Google;Google,-1;-1;-1,-1;-1;-1,,9/27/18,3,2,0,0,0,0,1141;854;1745,40;27;35,14;9;18,220;146;212,-1;-1
1670,ICLR,2019,Sample Efficient Imitation Learning for Continuous Control,Fumihiro Sasaki;Tetsuya Yohira;Atsuo Kawaguchi,fumihiro.fs.sasaki@jp.ricoh.com,7;5;5;5,5;4;5;5,Accept (Poster),0,2,0,yes,9/27/18,"Ricoh software limited, Beijing",-1,-1,5;4,9/27/18,21,10,4,1,0,3,36;21;445,25;5;27,3;1;5,3;3;44,-1;-1
1671,ICLR,2019,Janossy Pooling: Learning Deep Permutation-Invariant Functions for Variable-Size Inputs,Ryan L. Murphy;Balasubramaniam Srinivasan;Vinayak Rao;Bruno Ribeiro,murph213@purdue.edu;bsriniv@purdue.edu;varao@purdue.edu;ribeiro@cs.purdue.edu,5;7;8,4;4;4,Accept (Poster),0,8,1,yes,9/27/18,Purdue University;Purdue University;Purdue University;Purdue University,26;26;26;26,60;60;60;60,,9/27/18,39,14,12,0,19,3,64;62;518;1228,5;11;32;84,2;2;12;21,13;12;77;131,-1;-1
1672,ICLR,2019,Modeling Uncertainty with Hedged Instance Embeddings,Seong Joon Oh;Kevin P. Murphy;Jiyan Pan;Joseph Roth;Florian Schroff;Andrew C. Gallagher,coallaoh@linecorp.com;agallagher@google.com;kpmurphy@google.com;fschroff@google.com;jiyanpan@google.com;josephroth@google.com,7;7;7,5;3;3,Accept (Poster),0,7,0,yes,9/27/18,LINE;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,,9/27/18,10,6,5,0,33,4,624;17748;284;413;9513;3480,29;228;20;123;22;116,12;51;9;8;14;28,84;1914;16;20;1770;377,-1;-1
1673,ICLR,2019,Deep Decoder: Concise Image Representations from Untrained Non-convolutional Networks,Reinhard Heckel;Paul Hand,rh43@rice.edu;p.hand@northeastern.edu,7;8;8,3;4;4,Accept (Poster),0,6,0,yes,9/27/18,Rice University;Northeastern University,85;16,86;839,,9/27/18,47,20,11,2,3,8,909;1566,57;67,16;19,110;150,-1;-1
1674,ICLR,2019,Probabilistic Planning with Sequential Monte Carlo methods,Alexandre Piche;Valentin Thomas;Cyril Ibrahim;Yoshua Bengio;Chris Pal,alexandrelpiche@gmail.com;vltn.thomas@gmail.com;cyril.ibrahim@elementai.com;yoshua.umontreal@gmail.com;christopher.pal@polymtl.ca,5;6;8,4;4;4,Accept (Poster),0,14,5,yes,9/27/18,University of Montreal;University of Montreal;Element AI;University of Montreal;Polytechnique Montreal,123;123;-1;123;386,108;108;-1;108;108,11,9/27/18,11,3,7,0,0,3,81;131;13;197759;747,33;10;3;804;58,5;6;2;145;10,9;8;3;23765;73,-1;-1
1675,ICLR,2019,Random mesh projectors for inverse problems,Konik Kothari*;Sidharth Gupta*;Maarten v. de Hoop;Ivan Dokmanic,kkothar3@illinois.edu;gupta67@illinois.edu;mdehoop@rice.edu;dokmanic@illinois.edu,6;7;4,4;4;3,Accept (Poster),0,19,0,yes,9/27/18,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;Rice University;University of Illinois, Urbana Champaign",3;3;85;3,37;37;86;37,,5/29/18,11,3,3,0,0,0,18;32;2960;934,6;20;400;72,2;3;27;15,0;0;122;59,-1;-1
1676,ICLR,2019,A Direct Approach to Robust Deep Learning Using Adversarial Networks,Huaxia Wang;Chun-Nam Yu,hwang38@stevens.edu;cnyu@cs.cornell.edu,5;7;6,4;3;3,Accept (Poster),3,6,0,yes,9/27/18,Stevens Institute of Technology;Cornell University,153;7,512;19,5;4,9/27/18,22,12,4,0,0,3,205;267,21;15,7;6,15;23,-1;-1
1677,ICLR,2019,"Learning to Adapt in Dynamic, Real-World Environments through Meta-Reinforcement Learning",Anusha Nagabandi;Ignasi Clavera;Simin Liu;Ronald S. Fearing;Pieter Abbeel;Sergey Levine;Chelsea Finn,nagaban2@berkeley.edu;iclavera@berkeley.edu;simin.liu@berkeley.edu;ronf@berkeley.edu;pabbeel@berkeley.edu;svlevine@eecs.berkeley.edu;cbfinn@eecs.berkeley.edu,7;7;2,3;5;5,Accept (Poster),2,10,0,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5;5;5;5,18;18;18;18;18;18;18,6;2,3/30/18,62,36,19,1,12,7,606;428;120;461;36752;24519;7756,16;15;12;9;433;309;98,9;8;4;4;94;73;33,54;56;14;42;4427;3185;1036,-1;-1
1678,ICLR,2019,Robust Conditional Generative Adversarial Networks,Grigorios G. Chrysos;Jean Kossaifi;Stefanos Zafeiriou,greggchrysos@gmail.com;jean.kossaifi@gmail.com;s.zafeiriou@imperial.ac.uk,6;6;6,4;4;4,Accept (Poster),0,10,1,yes,9/27/18,Imperial College London;Imperial College London;Imperial College London,72;72;72,8;8;8,5;4;1;2,5/22/18,15,6,3,0,10,1,151;1119;8343,16;30;249,5;10;46,12;126;980,-1;-1
1679,ICLR,2019,signSGD with Majority Vote is Communication Efficient and Fault Tolerant,Jeremy Bernstein;Jiawei Zhao;Kamyar Azizzadenesheli;Anima Anandkumar,bernstein@caltech.edu;jiaweizhao.zjw@qq.com;kazizzad@uci.edu;anima@caltech.edu,6;6;7,5;5;4,Accept (Poster),1,9,1,yes,9/27/18,"California Institute of Technology;;University of California, Irvine;California Institute of Technology",140;-1;35;140,3;-1;99;3,4;1,9/27/18,32,19,10,2,0,9,856;31;665;5263,263;3;37;186,10;1;10;38,116;8;96;739,-1;-1
1680,ICLR,2019,The Singular Values of Convolutional Layers,Hanie Sedghi;Vineet Gupta;Philip M. Long,hsedghi@google.com;vineet@google.com;plong@google.com,8;4;7,4;5;3,Accept (Poster),0,24,2,yes,9/27/18,Google;Google;Google,-1;-1;-1,-1;-1;-1,,5/26/18,63,25,33,2,115,13,552;1809;5140,30;57;156,12;22;35,50;188;400,-1;-1
1681,ICLR,2019,Deep Online Learning Via Meta-Learning: Continual Adaptation for Model-Based RL,Anusha Nagabandi;Chelsea Finn;Sergey Levine,nagaban2@berkeley.edu;cbfinn@eecs.berkeley.edu;svlevine@eecs.berkeley.edu,7;7;7,3;3;3,Accept (Poster),0,3,0,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,18;18;18,6,9/27/18,47,30,15,0,7,5,599;7600;23990,16;98;309,9;33;73,54;1021;3116,-1;-1
1682,ICLR,2019,InfoBot: Transfer and Exploration via the Information Bottleneck,Anirudh Goyal;Riashat Islam;DJ Strouse;Zafarali Ahmed;Hugo Larochelle;Matthew Botvinick;Yoshua Bengio;Sergey Levine,anirudhgoyal9119@gmail.com;riashat.islam@mail.mcgill.ca;danieljstrouse@gmail.com;zafarali.ahmed@mail.mcgill.ca;hugolarochelle@google.com;botvinick@google.com;svlevine@eecs.berkeley.edu;yoshua.bengio@mila.quebec,7;7;3,3;3;3,Accept (Poster),0,28,1,yes,9/27/18,University of Montreal;McGill University;Princeton University;McGill University;Google;Google;University of California Berkeley;University of Montreal,123;85;30;85;-1;-1;5;123,108;42;7;42;-1;-1;18;108,,9/27/18,37,21,7,1,5,3,1115;1205;35;93;24485;13541;203485;24519,46;25;2;13;122;146;807;309,12;8;1;5;43;45;147;73,127;127;3;8;2853;1412;24050;3185,-1;-1
1683,ICLR,2019,Optimal Control Via Neural Networks: A Convex Approach,Yize Chen;Yuanyuan Shi;Baosen Zhang,yizechen@uw.edu;yyshi@uw.edu;zhangbao@uw.edu,6;8;7,3;4;4,Accept (Poster),1,10,1,yes,9/27/18,"University of Washington, Seattle;University of Washington, Seattle;University of Washington, Seattle",6;6;6,25;25;25,,5/30/18,15,7,2,0,0,0,209;288;1547,21;24;79,7;8;19,16;12;78,-1;-1
1684,ICLR,2019,Preventing Posterior Collapse with delta-VAEs,Ali Razavi;Aaron van den Oord;Ben Poole;Oriol Vinyals,alirazavi@google.com;avdnoord@google.com;pooleb@google.com;vinyals@google.com,6;7;6,3;4;3,Accept (Poster),2,4,1,yes,9/27/18,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,5,9/27/18,37,22,8,1,4,5,498;8426;4102;51380,23;40;41;121,7;24;19;55,67;1085;681;6446,-1;-1
1685,ICLR,2019,Combinatorial Attacks on Binarized Neural Networks,Elias B Khalil;Amrita Gupta;Bistra Dilkina,lyes@gatech.edu;agupta375@gatech.edu;dilkina@usc.edu,5;6;7,4;4;4,Accept (Poster),0,10,0,yes,9/27/18,Georgia Institute of Technology;Georgia Institute of Technology;University of Southern California,13;13;30,33;33;66,4,9/27/18,17,9,6,1,15,2,670;381;1234,15;50;78,10;11;17,80;31;125,-1;-1
1686,ICLR,2019,How Powerful are Graph Neural Networks?,Keyulu Xu*;Weihua Hu*;Jure Leskovec;Stefanie Jegelka,keyulu@mit.edu;weihuahu@stanford.edu;jure@cs.stanford.edu;stefje@mit.edu,7;7;8,5;5;5,Accept (Oral),22,30,4,yes,9/27/18,Massachusetts Institute of Technology;Stanford University;Stanford University;Massachusetts Institute of Technology,2;4;4;2,5;3;3;5,10,9/27/18,516,286,230,23,38,159,775;1264;46180;3198,7;63;298;113,7;12;92;28,211;282;5970;522,-1;-1
1687,ICLR,2019,On the Convergence of A Class of Adam-Type Algorithms  for Non-Convex Optimization,Xiangyi Chen;Sijia Liu;Ruoyu Sun;Mingyi Hong,chen5719@umn.edu;sijia.liu@ibm.com;ruoyus@illinois.edu;mhong@umn.edu,7;7;6,3;2;3,Accept (Poster),0,8,1,yes,9/27/18,"University of Minnesota, Minneapolis;International Business Machines;University of Illinois, Urbana Champaign;University of Minnesota, Minneapolis",57;-1;3;57,56;-1;37;56,9,8/8/18,66,40,28,7,10,20,398;674;1865;5066,23;62;85;204,7;14;20;33,70;88;148;524,-1;-1
1688,ICLR,2019,Learning when to Communicate at Scale in Multiagent Cooperative and Competitive Tasks,Amanpreet Singh;Tushar Jain;Sainbayar Sukhbaatar,amanpreet@nyu.edu;tushar@nyu.edu;sainbar@cs.nyu.edu,7;6;6,3;3;3,Accept (Poster),0,12,0,yes,9/27/18,New York University;New York University;New York University,26;26;26,27;27;27,9,9/27/18,28,13,4,0,20,3,1066;230;2679,15;62;21,8;9;13,235;8;300,-1;-1
1689,ICLR,2019,signSGD via Zeroth-Order Oracle,Sijia Liu;Pin-Yu Chen;Xiangyi Chen;Mingyi Hong,sijia.liu@ibm.com;pin-yu.chen@ibm.com;chen5719@umn.edu;mhong@umn.edu,7;8;6,5;3;2,Accept (Poster),0,11,0,yes,9/27/18,"International Business Machines;International Business Machines;University of Minnesota, Minneapolis;University of Minnesota, Minneapolis",-1;-1;57;57,-1;-1;56;56,4;9,9/27/18,22,12,10,3,0,5,674;2300;398;5066,62;104;23;204,14;23;7;33,88;216;70;524,-1;-1
1690,ICLR,2019,Information asymmetry in KL-regularized RL,Alexandre Galashov;Siddhant M. Jayakumar;Leonard Hasenclever;Dhruva Tirumala;Jonathan Schwarz;Guillaume Desjardins;Wojciech M. Czarnecki;Yee Whye Teh;Razvan Pascanu;Nicolas Heess,agalashov@google.com;sidmj@google.com;leonardh@google.com;dhruvat@google.com;schwarzjn@google.com;gdesjardins@google.com;lejlot@google.com;ywteh@google.com;razp@google.com;heess@google.com,7;5;7,3;5;4,Accept (Poster),0,4,0,yes,9/27/18,Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,,9/27/18,17,10,2,0,0,0,78;181;316;517;551;5218;2316;23281;16705;11371,7;14;17;10;27;48;44;249;101;104,5;8;10;6;8;18;20;52;46;37,2;23;25;59;92;595;246;3214;1671;1617,-1;-1
1691,ICLR,2019,Cost-Sensitive Robustness against Adversarial Examples,Xiao Zhang;David Evans,xz7bc@virginia.edu;evans@virginia.edu,5;8;5,4;3;3,Accept (Poster),0,12,0,yes,9/27/18,University of Virginia;University of Virginia,65;65,113;113,4,9/27/18,10,6,0,0,2,0,-1;7489,-1;80,-1;35,0;740,-1;-1
1692,ICLR,2019,Learning deep representations by mutual information estimation and maximization,R Devon Hjelm;Alex Fedorov;Samuel Lavoie-Marchildon;Karan Grewal;Phil Bachman;Adam Trischler;Yoshua Bengio,devon.hjelm@microsoft.com;eidos92@gmail.com;samuel.lavoie-marchildon@umontreal.ca;karang@cs.toronto.edu;phil.bachman@gmail.com;adam.trischler@microsoft.com;yoshua.umontreal@gmail.com,7;9;7,5;3;4,Accept (Oral),2,7,0,yes,9/27/18,"Microsoft;;University of Montreal;Department of Computer Science, University of Toronto;Microsoft;Microsoft;University of Montreal",-1;-1;123;18;-1;-1;123,-1;-1;108;22;-1;-1;108,4,8/20/18,293,172,138,5,88,64,1619;378;272;276;1825;1533;197759,43;15;4;8;32;47;804,13;6;2;3;14;17;145,257;74;61;61;234;285;23765,-1;-1
1693,ICLR,2019,A Generative Model For Electron Paths,John Bradshaw;Matt J. Kusner;Brooks Paige;Marwin H. S. Segler;José Miguel Hernández-Lobato,jab255@cam.ac.uk;mkusner@turing.ac.uk;bpaige@turing.ac.uk;marwin.segler@benevolent.ai;jmh233@cam.ac.uk,8;4;8,4;4;4,Accept (Poster),0,8,0,yes,9/27/18,University of Cambridge;Alan Turing Institute;Alan Turing Institute;BenevolentAI;University of Cambridge,71;-1;-1;-1;71,2;-1;-1;-1;2,5,5/23/18,11,5,6,0,0,0,52;2394;818;1079;3685,11;44;38;23;113,4;18;13;11;28,1;391;87;36;419,-1;-1
1694,ICLR,2019,Whitening and Coloring Batch Transform for GANs,Aliaksandr Siarohin;Enver Sangineto;Nicu Sebe,aliaksandr.siarohin@unitn.it;enver.sangineto@unitn.it;niculae.sebe@unitn.it,7;7;7,4;2;4,Accept (Poster),0,13,0,yes,9/27/18,University of Trento;University of Trento;University of Trento,18;18;18,258;258;258,5;4,6/1/18,14,6,7,0,3,1,229;1092;14856,16;68;546,6;18;65,38;118;1005,-1;-1
1695,ICLR,2019,Learning protein sequence embeddings using information from structure,Tristan Bepler;Bonnie Berger,tbepler@mit.edu;bab@mit.edu,8;7;7,4;3;4,Accept (Poster),0,8,0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2,5;5,,9/27/18,23,11,5,0,3,2,194;9934,10;299,7;48,13;951,-1;-1
1696,ICLR,2019,DARTS: Differentiable Architecture Search,Hanxiao Liu;Karen Simonyan;Yiming Yang,hanxiaol@cs.cmu.edu;simonyan@google.com;yiming@cs.cmu.edu,6;7;8,2;5;3,Accept (Poster),24,10,6,yes,9/27/18,Carnegie Mellon University;Google;Carnegie Mellon University,1;-1;1,24;-1;24,3,6/24/18,682,388,385,36,0,273,1946;58825;21038,35;95;272,12;38;49,508;10592;2560,-1;-1
1697,ICLR,2019,Representation Degeneration Problem in Training Natural Language Generation Models,Jun Gao;Di He;Xu Tan;Tao Qin;Liwei Wang;Tieyan Liu,jungao@cs.toronto.edu;dihe@microsoft.com;xu.tan@microsoft.com;taoqin@microsoft.com;wanglw@cis.pku.edu.cn;tyliu@microsoft.com,7;7;7,4;3;3,Accept (Poster),0,5,0,yes,9/27/18,"Department of Computer Science, University of Toronto;Microsoft;Microsoft;Microsoft;Peking University;Microsoft",18;-1;-1;-1;24;-1,22;-1;-1;-1;27;-1,3,9/27/18,7,5,4,2,0,3,136;2409;4746;2874;2297;13112,64;257;154;236;75;365,7;26;24;28;23;51,16;109;426;159;256;1708,-1;-1
1698,ICLR,2019,Structured Adversarial Attack:  Towards General Implementation and Better Interpretability,Kaidi Xu;Sijia Liu;Pu Zhao;Pin-Yu Chen;Huan Zhang;Quanfu Fan;Deniz Erdogmus;Yanzhi Wang;Xue Lin,xu.kaid@husky.neu.edu;sijia.liu@ibm.com;zhao.pu@husky.neu.edu;pin-yu.chen@ibm.com;ecezhang@ucdavis.edu;qfan@us.ibm.com;erdogmus@ece.neu.edu;yanz.wang@northeastern.edu;xue.lin@northeastern.edu,6;7;7,2;2;3,Accept (Poster),0,8,0,yes,9/27/18,"Northeastern University;International Business Machines;Northeastern University;International Business Machines;University of California, Davis;International Business Machines;Northeastern University;Northeastern University;Northeastern University",16;-1;16;-1;81;-1;16;16;16,839;-1;839;-1;54;-1;839;839;839,4,8/5/18,41,23,8,2,17,2,246;298;210;194;2043;1717;6300;3901;1903,27;51;41;44;31;62;480;262;256,10;11;8;6;19;17;40;31;24,17;24;6;22;282;176;416;227;112,-1;-1
1699,ICLR,2019,Learning sparse relational transition models,Victoria Xia;Zi Wang;Kelsey Allen;Tom Silver;Leslie Pack Kaelbling,victoria.f.xia281@gmail.com;ziw@mit.edu;krallen@mit.edu;tslvr@mit.edu;lpk@csail.mit.edu,6;7;8,4;2;3,Accept (Poster),0,0,1,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2;2,5;5;5;5;5,,9/27/18,5,2,1,0,7,0,24;16;1110;188;19814,4;13;28;14;363,2;2;9;4;53,1;0;101;15;1736,-1;-1
1700,ICLR,2019,ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware,Han Cai;Ligeng Zhu;Song Han,hancai@mit.edu;ligeng@mit.edu;songhan@mit.edu,7;6;6,2;4;2,Accept (Poster),1,20,7,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2,5;5;5,,9/27/18,379,214,196,7,8,122,1146;453;14536,16;15;374,10;5;36,255;128;1904,-1;-1
1701,ICLR,2019,No Training Required: Exploring Random Encoders for Sentence Classification,John Wieting;Douwe Kiela,jwieting@cs.cmu.edu;dkiela@fb.com,7;7;8,4;4;4,Accept (Poster),0,7,0,yes,9/27/18,Carnegie Mellon University;Facebook,1;-1,24;-1,3,9/27/18,45,22,10,1,0,0,1168;3368,26;79,12;29,162;576,-1;-1
1702,ICLR,2019,Equi-normalization of Neural Networks,Pierre Stock;Benjamin Graham;Rémi Gribonval;Hervé Jégou,pstock@fb.com;benjamingraham@fb.com;remi.gribonval@inria.fr;rvj@fb.com,7;7;5,4;3;3,Accept (Poster),2,6,1,yes,9/27/18,Facebook;Facebook;INRIA;Facebook,-1;-1;-1;-1,-1;-1;-1;-1,,9/27/18,3,1,1,0,2,0,98;915;9537;13505,10;48;215;164,4;11;44;40,4;98;1123;2361,-1;-1
1703,ICLR,2019,Deep Frank-Wolfe For Neural Network Optimization,Leonard Berrada;Andrew Zisserman;M. Pawan Kumar,lberrada@robots.ox.ac.uk;az@robots.ox.ac.uk;pawan@robots.ox.ac.uk,7;7;8,4;5;4,Accept (Poster),0,8,0,yes,9/27/18,University of Oxford;University of Oxford;University of Oxford,50;50;50,1;1;1,9;8,9/27/18,8,3,4,0,31,1,48;136537;2597,5;795;82,3;140;24,6;19871;247,-1;-1
1704,ICLR,2019,Learning what and where to attend,Drew Linsley;Dan Shiebler;Sven Eberhardt;Thomas Serre,drewlinsley@gmail.com;danshiebler@gmail.com;sven2sven2sven2@gmail.com;thomas_serre@brown.edu,6;6;8,4;3;3,Accept (Poster),0,9,0,yes,9/27/18,Brown University;Twitter;Amazon;Brown University,65;-1;-1;65,50;-1;-1;50,1,5/22/18,13,7,1,1,0,0,127;18;95;7872,21;9;22;117,7;2;5;28,6;0;0;1218,-1;-1
1705,ICLR,2019,Minimum Divergence vs. Maximum Margin: an Empirical Comparison on Seq2Seq Models,Huan Zhang;Hai Zhao,zhanghuan0468@gmail.com;zhaohai@cs.sjtu.edu.cn,7;7;5,4;4;4,Accept (Poster),0,4,0,yes,9/27/18,Shanghai Jiao Tong University;Shanghai Jiao Tong University,52;52,188;188,3,9/27/18,6,1,0,0,0,1,160;2617,30;121,7;29,4;221,-1;-1
1706,ICLR,2019,Adaptivity of deep ReLU network for learning in Besov and mixed smooth Besov spaces: optimal rate and curse of dimensionality,Taiji Suzuki,taiji@mist.i.u-tokyo.ac.jp,8;6;6,2;2;2,Accept (Poster),0,7,0,yes,9/27/18,The University of Tokyo,54,45,3;9,9/27/18,43,23,11,2,9,8,2226,177,25,233,-1
1707,ICLR,2019,Von Mises-Fisher Loss for Training Sequence to Sequence Models with Continuous Outputs,Sachin Kumar;Yulia Tsvetkov,sachink@cs.cmu.edu;ytsvetko@cs.cmu.edu,6;6;7,5;4;4,Accept (Poster),6,5,5,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University,1;1,24;24,3,9/27/18,21,12,9,0,14,0,36;1700,6;67,3;21,1;213,-1;-1
1708,ICLR,2019,Feed-forward Propagation in Probabilistic Neural Networks with Categorical and Max Layers,Alexander Shekhovtsov;Boris Flach,shekhovtsov@gmail.com;bflach@inf.tu-dresden.de,6;6;6,3;5;4,Accept (Poster),0,7,0,yes,9/27/18,Czech Technical University in Prague;TU Dresden,314;169,740;155,,9/27/18,3,3,2,0,0,0,554;263,46;45,13;7,54;20,-1;-1
1709,ICLR,2019,Generative Code Modeling with Graphs,Marc Brockschmidt;Miltiadis Allamanis;Alexander L. Gaunt;Oleksandr Polozov,mabrocks@microsoft.com;miallama@microsoft.com;algaunt@microsoft.com;polozov@microsoft.com,7;7;7,5;4;4,Accept (Poster),0,14,0,yes,9/27/18,Microsoft;Microsoft;Microsoft;Microsoft,-1;-1;-1;-1,-1;-1;-1;-1,5;10,5/22/18,34,20,9,2,6,4,2437;1900;1109;541,61;42;38;22,22;18;14;9,327;200;88;46,-1;-1
1710,ICLR,2019,Do Deep Generative Models Know What They Don't Know? ,Eric Nalisnick;Akihiro Matsukawa;Yee Whye Teh;Dilan Gorur;Balaji Lakshminarayanan,e.nalisnick@eng.cam.ac.uk;amatsukawa@google.com;ywteh@google.com;dilang@google.com;balajiln@google.com,7;6;7,4;4;3,Accept (Poster),2,18,1,yes,9/27/18,University of Cambridge;Google;Google;Google;Google,71;-1;-1;-1;-1,2;-1;-1;-1;-1,5,9/27/18,127,78,39,10,265,32,626;3613;22813;698;2853,33;248;249;17;43,12;32;51;11;22,93;207;3196;120;371,-1;-1
1711,ICLR,2019,Harmonizing Maximum Likelihood with GANs for Multimodal Conditional Generation,Soochan Lee;Junsoo Ha;Gunhee Kim,soochan.lee@vision.snu.ac.kr;kuc2477@gmail.com;gunhee@snu.ac.kr,4;8;7,5;4;3,Accept (Poster),0,5,3,yes,9/27/18,Seoul National University;Hanyang University;Seoul National University,41;228;41,74;377;74,5;1,9/27/18,4,2,1,0,0,1,33;32;1968,3;14;85,3;4;23,5;2;249,-1;-1
1712,ICLR,2019,Visual Explanation by Interpretation: Improving Visual Feedback Capabilities of Deep Neural Networks,Jose Oramas;Kaili Wang;Tinne Tuytelaars,jose.oramas@esat.kuleuven.be;kaili.wang@esat.kuleuven.be;tinne.tuytelaars@esat.kuleuven.be,8;5;4,4;3;5,Accept (Poster),0,6,0,yes,9/27/18,KU Leuven;KU Leuven;KU Leuven,115;115;115,47;47;47,,12/18/17,19,8,7,1,14,0,567;237;28704,27;46;339,9;8;55,50;5;4211,-1;-1
1713,ICLR,2019,Learning Finite State Representations of Recurrent Policy Networks,Anurag Koul;Alan Fern;Sam Greydanus,koula@oregonstate.edu;alan.fern@oregonstate.edu;sgrey@google.com,7;7;6,3;5;3,Accept (Poster),0,9,0,yes,9/27/18,Oregon State University;Oregon State University;Google,76;76;-1,318;318;-1,,9/27/18,18,11,9,1,13,3,94;3847;166,5;225;10,3;34;5,15;315;25,-1;-1
1714,ICLR,2019,Human-level Protein Localization with Convolutional Neural Networks,Elisabeth Rumetshofer;Markus Hofmarcher;Clemens Röhrl;Sepp Hochreiter;Günter Klambauer,rumetshofer@ml.jku.at;hofmarcher@ml.jku.at;clemens.roehrl@meduniwien.ac.at;hochreit@ml.jku.at;klambauer@ml.jku.at,4;5;8,4;3;4,Accept (Poster),0,4,0,yes,9/27/18,Johannes Kepler University Linz;Johannes Kepler University Linz;;Johannes Kepler University Linz;Johannes Kepler University Linz,228;228;-1;228;228,538;538;-1;538;538,,9/27/18,4,1,1,0,0,0,24;136;34;34842;2014,5;11;12;111;51,3;4;4;28;17,1;3;1;6534;250,-1;-1
1715,ICLR,2019,Value Propagation Networks,Nantas Nardelli;Gabriel Synnaeve;Zeming Lin;Pushmeet Kohli;Philip H. S. Torr;Nicolas Usunier,nantas@robots.ox.ac.uk;gab@fb.com;zlin@fb.com;pushmeet@google.com;philip.torr@eng.ox.ac.uk;usunier@fb.com,6;7;7,3;3;3,Accept (Poster),0,3,0,yes,9/27/18,University of Oxford;Facebook;Facebook;Google;University of Oxford;Facebook,50;-1;-1;-1;50;-1,1;-1;-1;-1;1;-1,,2/15/18,7,3,3,0,17,2,768;1476;5876;21843;27765;6046,16;62;19;313;354;109,7;19;9;69;83;30,121;153;727;2735;3818;1159,-1;-1
1716,ICLR,2019,Adversarial Attacks on Graph Neural Networks via Meta Learning,Daniel Zügner;Stephan Günnemann,zuegnerd@in.tum.de;guennemann@in.tum.de,7;7;6,4;4;4,Accept (Poster),0,6,0,yes,9/27/18,Technical University Munich;Technical University Munich,54;54,41;41,4;10,9/27/18,54,33,17,1,4,11,345;2752,14;145,6;29,79;316,-1;-1
1717,ICLR,2019,Learning Latent Superstructures in Variational Autoencoders for Deep Multidimensional Clustering,Xiaopeng Li;Zhourong Chen;Leonard K. M. Poon;Nevin L. Zhang,xlibo@cse.ust.hk;zchenbb@cse.ust.hk;kmpoon@eduhk.hk;lzhang@cse.ust.hk,7;7;8,3;4;4,Accept (Poster),0,8,0,yes,9/27/18,The Hong Kong University of Science and Technology;The Hong Kong University of Science and Technology;The Education University of Hong Kong;The Hong Kong University of Science and Technology,39;39;89;39,44;44;40;44,5,3/14/18,7,4,3,0,4,1,47;1815;175;125,21;15;28;15,3;7;8;5,1;324;7;11,-1;-1
1718,ICLR,2019,Off-Policy Evaluation and Learning from Logged Bandit Feedback: Error Reduction via Surrogate Policy,Yuan Xie;Boyi Liu;Qiang Liu;Zhaoran Wang;Yuan Zhou;Jian Peng,xieyuan@umail.iu.edu;boyiliu2018@u.northwestern.edu;lqiang@cs.utexas.edu;zhaoranwang@gmail.com;yzhoucs@iu.edu;jianpeng@illinois.edu,6;8;6,4;4;3,Accept (Poster),0,4,0,yes,9/27/18,"Indiana University, Bloomington;Northwestern University;University of Texas, Austin;Northwestern University;Indiana University, Bloomington;University of Illinois, Urbana Champaign",72;44;22;44;72;3,117;20;49;20;117;37,1,8/1/18,2,1,0,0,3,0,495;567;4804;1127;3260;2056,51;34;646;77;301;120,10;9;30;19;25;23,48;47;240;128;233;203,-1;-1
1719,ICLR,2019,SNIP: SINGLE-SHOT NETWORK PRUNING BASED ON CONNECTION SENSITIVITY,Namhoon Lee;Thalaiyasingam Ajanthan;Philip Torr,namhoon@robots.ox.ac.uk;ajanthan@robots.ox.ac.uk;phst@robots.ox.ac.uk,8;7;9,5;4;4,Accept (Poster),8,15,4,yes,9/27/18,University of Oxford;University of Oxford;University of Oxford,50;50;50,1;1;1,,9/27/18,95,50,45,7,12,31,867;312;27765,32;25;354,11;7;83,153;70;3818,-1;-1
1720,ICLR,2019,Generating Multi-Agent Trajectories using Programmatic Weak Supervision,Eric Zhan;Stephan Zheng;Yisong Yue;Long Sha;Patrick Lucey,ezhan@caltech.edu;stzheng@caltech.edu;yyue@caltech.edu;lsha@stats.com;plucey@stats.com,7;6;6,3;3;3,Accept (Poster),0,4,0,yes,9/27/18,California Institute of Technology;California Institute of Technology;California Institute of Technology;STATS LLC;STATS LLC,140;140;140;-1;-1,3;3;3;-1;-1,5,3/20/18,15,9,5,0,0,0,34;447;3176;295;4064,6;29;120;32;104,3;7;29;10;27,1;31;388;17;561,-1;-1
1721,ICLR,2019,Learning to Represent Edits,Pengcheng Yin;Graham Neubig;Miltiadis Allamanis;Marc Brockschmidt;Alexander L. Gaunt,pcyin@cs.cmu.edu;gneubig@cs.cmu.edu;miallama@microsoft.com;mabrocks@microsoft.com;algaunt@microsoft.com,6;7;6,3;3;4,Accept (Poster),0,13,2,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;Microsoft;Microsoft;Microsoft,1;1;-1;-1;-1,24;24;-1;-1;-1,3,9/27/18,33,17,8,1,0,1,837;5356;1900;2437;1109,45;441;42;61;38,12;38;18;22;14,72;562;200;327;88,-1;-1
1722,ICLR,2019,Transfer Learning for Sequences via Learning to Collocate,Wanyun Cui;Guangyu Zheng;Zhiqiang Shen;Sihang Jiang;Wei Wang,cui.wanyun@sufe.edu.cn;simonzgy@outlook.com;shen54@illinois.edu;tedjiangfdu@gmail.com;weiwang1@fudan.edu.cn,5;6;6,4;4;3,Accept (Poster),0,7,5,yes,9/27/18,"Shanghai University of Finance and Economics;Fudan University;University of Illinois, Urbana Champaign;;Fudan University",261;78;3;-1;78,1103;116;37;-1;116,3;6,9/27/18,8,5,2,0,3,0,376;75;1084;16;5297,14;14;31;11;240,6;4;10;2;42,61;3;171;0;655,-1;-1
1723,ICLR,2019,Multi-Agent Dual Learning,Yiren Wang;Yingce Xia;Tianyu He;Fei Tian;Tao Qin;ChengXiang Zhai;Tie-Yan Liu,yiren@illinois.edu;yingce.xia@gmail.com;hetianyu@mail.ustc.edu.cn;fetia@microsoft.com;taoqin@microsoft.com;czhai@illinois.edu;tie-yan.liu@microsoft.com,6;6;6,2;3;4,Accept (Poster),4,8,0,yes,9/27/18,"University of Illinois, Urbana Champaign;Microsoft;University of Science and Technology of China;Microsoft;Microsoft;University of Illinois, Urbana Champaign;Microsoft",3;-1;478;-1;-1;3;-1,37;-1;132;-1;-1;37;-1,3;2,9/27/18,14,8,4,0,0,2,264;1279;125;5135;344;473;13355,22;48;14;370;101;49;366,7;14;6;31;10;12;51,23;138;5;268;22;53;1719,-1;-1
1724,ICLR,2019,On the Sensitivity of Adversarial Robustness to Input Data Distributions,Gavin Weiguang Ding;Kry Yik Chau Lui;Xiaomeng Jin;Luyu Wang;Ruitong Huang,gavin.ding@borealisai.com;yikchau.y.lui@borealisai.com;tracy.jin@mail.utoronto.ca;luyu.wang@borealisai.com;ruitong.huang@borealisai.com,7;5;7,3;4;2,Accept (Poster),0,3,0,yes,9/27/18,Borealis AI;Borealis AI;Toronto University;Borealis AI;Borealis AI,-1;-1;18;-1;-1,-1;-1;22;-1;-1,4,9/27/18,16,8,2,0,0,2,226;36;219;37;368,21;4;23;13;29,9;3;6;3;10,10;4;9;4;29,-1;-1
1725,ICLR,2019,GENERATING HIGH FIDELITY IMAGES WITH SUBSCALE PIXEL NETWORKS AND MULTIDIMENSIONAL UPSCALING,Jacob Menick;Nal Kalchbrenner,jmenick@google.com;nalk@google.com,9;10;7,3;5;3,Accept (Oral),2,9,1,yes,9/27/18,Google;Google,-1;-1,-1;-1,,9/27/18,47,21,13,2,15,6,519;15715,8;27,6;18,64;1396,-1;-1
1726,ICLR,2019,Scalable Unbalanced Optimal Transport using Generative Adversarial Networks,Karren D. Yang;Caroline Uhler,karren@mit.edu;cuhler@mit.edu,7;6;6,4;4;4,Accept (Poster),0,8,0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2,5;5,5;4,9/27/18,10,2,5,0,3,0,280;1233,20;65,9;17,20;108,-1;-1
1727,ICLR,2019,ANYTIME MINIBATCH: EXPLOITING STRAGGLERS IN ONLINE DISTRIBUTED OPTIMIZATION,Nuwan Ferdinand;Haider Al-Lawati;Stark Draper;Matthew Nokleby,nuwan.ferdinand@utoronto.ca;haider.al.lawati@mail.utoronto.ca;stark.draper@utoronto.ca;matthew.nokleby@target.com,4;7;7,4;4;4,Accept (Poster),0,5,0,yes,9/27/18,Toronto University;Toronto University;Toronto University;Target,18;18;18;-1,22;22;22;-1,,9/27/18,6,5,1,0,0,0,416;15;2225;395,40;6;153;67,9;2;27;10,22;0;196;26,-1;-1
1728,ICLR,2019,Aggregated Momentum: Stability Through Passive Damping,James Lucas;Shengyang Sun;Richard Zemel;Roger  Grosse,jlucas@cs.toronto.edu;ssy@cs.toronto.edu;zemel@cs.toronto.edu;rgrosse@cs.toronto.edu,7;6;5,3;3;4,Accept (Poster),0,7,0,yes,9/27/18,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",18;18;18;18,22;22;22;22,,4/1/18,18,11,8,0,78,5,299;476;21173;5579,114;23;208;48,9;9;52;27,37;38;2466;801,-1;-1
1729,ICLR,2019,ClariNet: Parallel Wave Generation in End-to-End Text-to-Speech,Wei Ping;Kainan Peng;Jitong Chen,weiping.thu@gmail.com;pengkainan@baidu.com;jitongc@gmail.com,6;9;7,3;4;4,Accept (Poster),0,7,0,yes,9/27/18,Baidu;Baidu;,-1;-1;-1,-1;-1;-1,,7/19/18,116,59,64,2,96,23,959;562;439,208;11;14,12;7;8,109;72;62,-1;-1
1730,ICLR,2019,Adaptive Estimators Show Information Compression in Deep Neural Networks,Ivan Chelombiev;Conor Houghton;Cian O'Donnell,ic14436@bristol.ac.uk;conor.houghton@bristol.ac.uk;cian.odonnell@bristol.ac.uk,7;6;7,4;4;3,Accept (Poster),0,6,0,yes,9/27/18,University of Bristol;University of Bristol;University of Bristol,123;123;123,76;76;76,8,9/27/18,7,6,2,2,3,1,7;277;401,2;29;39,1;7;9,1;34;33,-1;-1
1731,ICLR,2019,A Unified Theory of Early Visual Representations from Retina to Cortex through Anatomically Constrained Deep CNNs,Jack Lindsey;Samuel A. Ocko;Surya Ganguli;Stephane Deny,lindsey6@stanford.edu;socko@stanford.edu;sganguli@stanford.edu;sdeny@stanford.edu,8;8;8,5;5;3,Accept (Oral),0,6,0,yes,9/27/18,Stanford University;Stanford University;Stanford University;Stanford University,4;4;4;4,3;3;3;3,5,9/27/18,16,4,1,0,26,1,30;205;5819;232,13;25;129;15,3;8;39;6,1;16;588;20,-1;-1
1732,ICLR,2019,RNNs implicitly implement tensor-product representations,R. Thomas McCoy;Tal Linzen;Ewan Dunbar;Paul Smolensky,tom.mccoy@jhu.edu;tal.linzen@jhu.edu;ewan.dunbar@univ-paris-diderot.fr;smolensky@jhu.edu,7;6;6,4;4;4,Accept (Poster),0,7,0,yes,9/27/18,Johns Hopkins University;Johns Hopkins University;Université Paris Diderot;Johns Hopkins University,72;72;478;72,13;13;234;13,,9/27/18,15,6,4,1,16,2,388;1250;287;9558,17;72;32;205,9;15;7;39,48;143;18;882,-1;-1
1733,ICLR,2019,An Empirical study of Binary Neural Networks' Optimisation,Milad Alizadeh;Javier Fernández-Marqués;Nicholas D. Lane;Yarin Gal,milad.alizadeh@cs.ox.ac.uk;javier.fernandezmarques@cs.ox.ac.uk;nicholas.lane@cs.ox.ac.uk;yarin.gal@cs.ox.ac.uk,8;4;6,4;4;3,Accept (Poster),2,3,0,yes,9/27/18,University of Oxford;University of Oxford;University of Oxford;University of Oxford,50;50;50;50,1;1;1;1,,9/27/18,14,7,4,0,0,4,82;35;10008;6347,26;11;166;88,5;3;44;20,12;7;702;945,-1;-1
1734,ICLR,2019,A Statistical Approach to Assessing Neural Network Robustness,Stefan Webb;Tom Rainforth;Yee Whye Teh;M. Pawan Kumar,info@stefanwebb.me;twgr@robots.ox.ac.uk;y.w.teh@stats.ox.ac.uk;pawan@robots.ox.ac.uk,6;7;8,5;4;3,Accept (Poster),0,10,0,yes,9/27/18,;University of Oxford;University of Oxford;University of Oxford,-1;50;50;50,-1;1;1;1,,9/27/18,10,3,3,0,37,0,60;433;22813;2597,11;43;249;82,3;11;51;24,1;58;3196;247,-1;-1
1735,ICLR,2019,Meta-Learning For Stochastic Gradient MCMC,Wenbo Gong;Yingzhen Li;José Miguel Hernández-Lobato,wg242@cam.ac.uk;yl494@cam.ac.uk;jmh233@cam.ac.uk,7;7;6,4;4;3,Accept (Poster),0,6,0,yes,9/27/18,University of Cambridge;University of Cambridge;University of Cambridge,71;71;71,2;2;2,11;6,6/12/18,13,8,7,0,10,1,50;1054;3685,13;55;113,5;14;28,3;147;419,-1;-1
1736,ICLR,2019,Wizard of Wikipedia: Knowledge-Powered Conversational Agents,Emily Dinan;Stephen Roller;Kurt Shuster;Angela Fan;Michael Auli;Jason Weston,edinan@fb.com;roller@fb.com;kshuster@fb.com;angelafan@fb.com;michaelauli@fb.com;jase@fb.com,7;6;8,4;5;4,Accept (Poster),0,0,7,yes,9/27/18,Facebook;Facebook;Facebook;Facebook;Facebook;Facebook,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,,9/27/18,95,60,33,3,11,25,581;903;299;1769;6740;44028,19;30;16;26;71;243,8;15;7;14;30;76,121;136;49;264;1015;5808,-1;-1
1737,ICLR,2019,Information-Directed Exploration for Deep Reinforcement Learning,Nikolay Nikolov;Johannes Kirschner;Felix Berkenkamp;Andreas Krause,nikolay.nikolov14@imperial.ac.uk;jkirschner@inf.ethz.ch;befelix@inf.ethz.ch;krausea@ethz.ch,7;7;7,4;3;4,Accept (Poster),2,3,0,yes,9/27/18,Imperial College London;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,72;10;10;10,8;10;10;10,1,9/27/18,6,3,1,0,6,0,267;66;804;16097,44;14;25;241,8;4;12;65,13;2;47;1747,-1;-1
1738,ICLR,2019,FlowQA: Grasping Flow in History for Conversational Machine Comprehension,Hsin-Yuan Huang;Eunsol Choi;Wen-tau Yih,hsinyuan@caltech.edu;eunsol@cs.washington.edu;scottyih@allenai.org,7;6;7,5;4;4,Accept (Poster),0,0,8,yes,9/27/18,California Institute of Technology;University of Washington;Allen Institute for Artificial Intelligence,140;6;-1,3;25;-1,,9/27/18,36,18,18,1,16,16,348;1400;8925,39;27;100,12;13;43,44;272;1368,-1;-1
1739,ICLR,2019,FFJORD: Free-Form Continuous Dynamics for Scalable Reversible Generative Models,Will Grathwohl;Ricky T. Q. Chen;Jesse Bettencourt;Ilya Sutskever;David Duvenaud,wgrathwohl@cs.toronto.edu;rtqichen@cs.toronto.edu;jessebett@cs.toronto.edu;ilyasu@openai.com;duvenaud@cs.toronto.edu,7;7;7,4;3;4,Accept (Oral),1,8,2,yes,9/27/18,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;OpenAI;Department of Computer Science, University of Toronto",18;18;18;-1;18,22;22;22;-1;22,5,9/27/18,182,104,72,4,243,41,368;296;721;128441;5759,13;9;7;90;74,5;5;3;52;28,81;68;171;16779;737,-1;-1
1740,ICLR,2019,Towards GAN Benchmarks Which Require Generalization,Ishaan Gulrajani;Colin Raffel;Luke Metz,igul222@gmail.com;craffel@gmail.com;lmetz@google.com,7;6;3,4;4;4,Accept (Poster),0,4,0,yes,9/27/18,Google;Google;Google,-1;-1;-1,-1;-1;-1,5;8,9/27/18,11,5,2,0,0,1,4148;4484;7122,9;63;25,7;22;10,897;461;1195,-1;-1
1741,ICLR,2019,Multi-Domain Adversarial Learning,Alice Schoenauer-Sebag;Louise Heinrich;Marc Schoenauer;Michele Sebag;Lani F. Wu;Steve J. Altschuler,alice.schoenauer@polytechnique.org;louise.heinrich@ucsf.edu;marc.schoenauer@inria.fr;sebag@lri.fr;lani.wu@ucsf.edu;steven.altschuler@ucsf.edu,5;8;6,4;5;5,Accept (Poster),0,3,0,yes,9/27/18,"University of California, San Francisco;University of California, San Francisco;INRIA;CNRS, Université Paris-Saclay;University of California, San Francisco;University of California, San Francisco",-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,4;1,9/27/18,9,3,3,0,0,1,27;14;7260;3993;107;4404,6;3;425;250;10;67,3;2;39;36;5;25,1;2;595;377;7;279,-1;-1
1742,ICLR,2019,Mode Normalization,Lucas Deecke;Iain Murray;Hakan Bilen,l.deecke@ed.ac.uk;i.murray@ed.ac.uk;hbilen@ed.ac.uk,6;5;6,4;4;4,Accept (Poster),6,5,0,yes,9/27/18,University of Edinburgh;University of Edinburgh;University of Edinburgh,33;33;33,27;27;27,,9/27/18,7,2,0,0,8,0,204;5032;1730,7;222;49,4;28;16,46;570;345,-1;-1
1743,ICLR,2019,Universal Successor Features Approximators,Diana Borsa;Andre Barreto;John Quan;Daniel J. Mankowitz;Hado van Hasselt;Remi Munos;David Silver;Tom Schaul,borsa@google.com;andrebarreto@google.com;johnquan@google.com;dmankowitz@google.com;hado@google.com;munos@google.com;davidsilver@google.com;schaul@google.com,7;5;6,3;2;4,Accept (Poster),0,0,0,yes,9/27/18,Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1,,9/27/18,16,9,6,0,8,2,184;24;3225;547;5534;1685;42470;8440,15;5;16;29;50;30;158;83,9;2;11;12;21;13;56;30,8;3;542;32;914;255;5895;1125,-1;-1
1744,ICLR,2019,Adversarial Imitation via Variational Inverse Reinforcement Learning,Ahmed H. Qureshi;Byron Boots;Michael C. Yip,a1quresh@eng.ucsd.edu;bboots@cc.gatech.edu;yip@ucsd.edu,6;6;6,4;3;4,Accept (Poster),0,11,0,yes,9/27/18,"University of California, San Diego;Georgia Institute of Technology;University of California, San Diego",11;13;11,31;33;31,5;4;6,9/17/18,11,7,4,1,15,1,282;2556;638,21;136;43,10;28;14,11;259;52,-1;-1
1745,ICLR,2019,On Self Modulation for Generative Adversarial Networks,Ting Chen;Mario Lucic;Neil Houlsby;Sylvain Gelly,iamtingchen@gmail.com;lucic@google.com;neilhoulsby@google.com;sylvaingelly@google.com,5;7;7,5;4;4,Accept (Poster),0,4,0,yes,9/27/18,"University of California, Los Angeles;Google;Google;Google",20;-1;-1;-1,15;-1;-1;-1,5;4,9/27/18,31,18,11,2,41,2,3617;1547;833;3463,154;48;34;112,29;20;14;25,322;188;111;453,-1;-1
1746,ICLR,2019,Attentive Neural Processes,Hyunjik Kim;Andriy Mnih;Jonathan Schwarz;Marta Garnelo;Ali Eslami;Dan Rosenbaum;Oriol Vinyals;Yee Whye Teh,hyunjikk@google.com;amnih@google.com;schwarzjn@google.com;garnelo@google.com;aeslami@google.com;danro@google.com;vinyals@google.com;ywteh@google.com,6;6;7,4;4;4,Accept (Poster),0,5,0,yes,9/27/18,Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1,,9/27/18,50,19,20,1,28,9,435;9708;551;930;4062;499;52477;23281,13;37;27;23;38;17;121;249,5;21;8;11;19;8;55;52,97;1552;92;109;516;81;6537;3214,-1;-1
1747,ICLR,2019,Relaxed Quantization for Discretized Neural Networks,Christos Louizos;Matthias Reisser;Tijmen Blankevoort;Efstratios Gavves;Max Welling,c.louizos@uva.nl;m.reisser@uva.nl;tijmen@qti.qualcomm.com;egavves@uva.nl;m.welling@uva.nl,7;7;7,4;3;4,Accept (Poster),0,12,0,yes,9/27/18,"University of Amsterdam;University of Amsterdam;Qualcomm Inc, QualComm;University of Amsterdam;University of Amsterdam",169;169;-1;169;169,59;59;-1;59;59,,9/27/18,37,17,15,2,5,7,1217;121;87;3243;26177,22;15;13;72;269,11;6;3;26;57,233;13;17;468;5028,-1;-1
1748,ICLR,2019,Learning Factorized Multimodal Representations,Yao-Hung Hubert Tsai;Paul Pu Liang;Amir Zadeh;Louis-Philippe Morency;Ruslan Salakhutdinov,yaohungt@cs.cmu.edu;pliang@cs.cmu.edu;abagherz@cs.cmu.edu;morency@cs.cmu.edu;rsalakhu@cs.cmu.edu,7;7;6,3;2;3,Accept (Poster),0,5,0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1;1,24;24;24;24;24,5,6/16/18,41,27,15,1,10,4,520;631;1461;10448;65975,63;44;39;324;253,12;13;16;53;81,70;97;235;1112;7721,-1;-1
1749,ICLR,2019,code2seq: Generating Sequences from Structured Representations of Code,Uri Alon;Shaked Brody;Omer Levy;Eran Yahav,urialon1@gmail.com;shakedbr@cs.technion.ac.il;omerlevy@gmail.com;yahave@cs.technion.ac.il,6;7;5,4;4;4,Accept (Poster),0,15,0,yes,9/27/18,Technion;Technion;Facebook;Technion,25;25;-1;25,327;327;-1;327,3,8/4/18,81,44,37,3,18,14,346;0;7424;3968,9;1;58;144,5;0;30;37,54;0;1206;378,-1;-1
1750,ICLR,2019,Spreading vectors for similarity search,Alexandre Sablayrolles;Matthijs Douze;Cordelia Schmid;Hervé Jégou,asablayrolles@fb.com;matthijs@fb.com;cordelia.schmid@inria.fr;rvj@fb.com,6;6;7,4;3;4,Accept (Poster),0,8,0,yes,9/27/18,Facebook;Facebook;INRIA;Facebook,-1;-1;-1;-1,-1;-1;-1;-1,,6/8/18,12,3,7,0,17,3,129;10164;71838;13505,8;98;430;164,6;31;113;40,14;1792;10101;2361,-1;-1
1751,ICLR,2019,DeepOBS: A Deep Learning Optimizer Benchmark Suite,Frank Schneider;Lukas Balles;Philipp Hennig,frank.schneider@tuebingen.mpg.de;lukas.balles@tuebingen.mpg.de;philipp.hennig@uni-tuebingen.de,6;7;6,4;4;4,Accept (Poster),0,9,0,yes,9/27/18,"Max Planck Institute for Intelligent Systems, Max-Planck Institute;Max Planck Institute for Intelligent Systems, Max-Planck Institute;University of Tuebingen",-1;-1;153,-1;-1;94,8,9/27/18,12,4,8,2,3,6,12;222;2118,2;10;99,1;6;24,6;31;202,-1;-1
1752,ICLR,2019,Conditional Network Embeddings,Bo Kang;Jefrey Lijffijt;Tijl De Bie,bo.kang@ugent.be;jefrey.lijffijt@ugent.be;tijl.debie@ugent.be,6;4;5,3;4;4,Accept (Poster),0,6,0,yes,9/27/18,Ghent University;Ghent University;Ghent University,478;478;478,107;107;107,11,5/19/18,9,4,6,0,3,0,956;320;3171,147;68;160,15;10;26,55;16;251,-1;-1
1753,ICLR,2019,Neural Persistence: A Complexity Measure for Deep Neural Networks Using Algebraic Topology,Bastian Rieck;Matteo Togninalli;Christian Bock;Michael Moor;Max Horn;Thomas Gumbsch;Karsten Borgwardt,bastian.rieck@bsse.ethz.ch;matteo.togninalli@bsse.ethz.ch;christian.bock@bsse.ethz.ch;michael.moor@bsse.ethz.ch;max.horn@bsse.ethz.ch;thomas.gumbsch@bsse.ethz.ch;karsten.borgwardt@bsse.ethz.ch,7;6;4,4;5;4,Accept (Poster),2,6,0,yes,9/27/18,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10;10;10;10;10,10;10;10;10;10;10;10,10,9/27/18,16,9,4,0,0,0,218;81;39;47;52;27;12360,39;9;25;11;10;7;170,9;5;3;4;5;2;41,10;3;1;3;3;0;1808,-1;-1
1754,ICLR,2019,Large-Scale Answerer in Questioner's Mind for Visual Dialog Question Generation,Sang-Woo Lee;Tong Gao;Sohee Yang;Jaejun Yoo;Jung-Woo Ha,sang.woo.lee@navercorp.com;tong.gao@navercorp.com;sh.yang@navercorp.com;jaejun.yoo@navercorp.com;jungwoo.ha@navercorp.com,6;7;6,4;5;2,Accept (Poster),0,13,1,yes,9/27/18,NAVER;NAVER;NAVER;NAVER;NAVER,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,,9/27/18,3,2,0,0,0,0,2222;629;8;196;1907,432;68;6;13;67,23;12;2;6;15,105;38;4;16;368,-1;-1
1755,ICLR,2019,Learning to Screen for Fast Softmax  Inference on Large Vocabulary Neural Networks,Patrick Chen;Si Si;Sanjiv Kumar;Yang Li;Cho-Jui Hsieh,patrickchen@g.ucla.edu;sisidaisy@google.com;sanjivk@google.com;liyang@google.com;chohsieh@cs.ucla.edu,7;6;8,4;3;4,Accept (Poster),0,6,0,yes,9/27/18,"University of California, Los Angeles;Google;Google;Google;University of California, Los Angeles",20;-1;-1;-1;20,15;-1;-1;-1;15,3,9/27/18,5,3,3,0,4,1,32;934;8953;2186;12363,6;52;298;37;168,3;15;41;19;40,5;99;1289;234;1715,-1;-1
1756,ICLR,2019,Learning to Schedule Communication in Multi-agent Reinforcement Learning,Daewoo Kim;Sangwoo Moon;David Hostallero;Wan Ju Kang;Taeyoung Lee;Kyunghwan Son;Yung Yi,kdw2139@gmail.com;swmoon00@gmail.com;ddhostallero@kaist.ac.kr;soarhigh0714@gmail.com;tylee0325@gmail.com;khson@lanada.kaist.ac.kr;yiyung@kaist.edu,7;7;8,3;2;5,Accept (Poster),0,6,0,yes,9/27/18,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;KAIST,20;20;20;20;20;20;20,95;95;95;95;95;95;95,,9/27/18,25,14,4,0,3,2,126;398;101;70;3088;70;4797,28;32;9;6;138;9;224,6;9;4;3;28;3;29,8;25;17;12;249;12;507,-1;-1
1757,ICLR,2019,"On Random Deep Weight-Tied Autoencoders: Exact Asymptotic Analysis, Phase Transitions, and Implications to Training",Ping Li;Phan-Minh Nguyen,pingli98@gmail.com;npminh@stanford.edu,8;9;8,4;4;4,Accept (Oral),0,7,0,yes,9/27/18,Rutgers University New Brunswick;Stanford University,34;4,172;3,,9/27/18,13,6,2,0,0,0,1362;343,297;13,17;6,65;31,-1;-1
1758,ICLR,2019,DISTRIBUTIONAL CONCAVITY REGULARIZATION FOR GANS,Shoichiro Yamaguchi;Masanori Koyama,guguchi@preferred.jp;masomatics@preferred.jp,6;7;8;7,4;4;1;1,Accept (Poster),0,6,0,yes,9/27/18,"Preferred Networks, Inc.;Preferred Networks, Inc.",-1;-1,-1;-1,5;4,9/27/18,3,2,1,0,0,0,304;2374,54;35,8;12,19;482,-1;-1
1759,ICLR,2019,ROBUST ESTIMATION VIA GENERATIVE ADVERSARIAL NETWORKS,Chao GAO;jiyi LIU;Yuan YAO;Weizhi ZHU,chaogao@galton.uchicago.edu;jiyi.liu@yale.edu;yuany@ust.hk;wzhuai@connect.ust.hk,7;5;7,4;5;5,Accept (Poster),0,10,0,yes,9/27/18,University of Chicago;Yale University;The Hong Kong University of Science and Technology;The Hong Kong University of Science and Technology,48;62;39;39,9;12;44;44,5,9/27/18,14,7,6,1,11,2,326;120;2431;17,80;21;422;7,9;6;25;2,22;6;108;2,-1;-1
1760,ICLR,2019,Pay Less Attention with Lightweight and Dynamic Convolutions,Felix Wu;Angela Fan;Alexei Baevski;Yann Dauphin;Michael Auli,fw245@cornell.edu;angelfan@fb.com;alexei.b@gmail.com;yann@dauphin.io;michael.auli@gmail.com,8;8;8,4;4;4,Accept (Oral),0,13,6,yes,9/27/18,Cornell University;Facebook;Facebook;Facebook;Facebook,7;-1;-1;-1;-1,19;-1;-1;-1;-1,3;5,9/27/18,132,54,55,7,14,31,886;1769;691;8386;6740,18;26;17;43;71,10;14;8;27;30,176;264;104;1017;1015,-1;-1
1761,ICLR,2019,Analysis of Quantized Models,Lu Hou;Ruiliang Zhang;James T. Kwok,lhouab@cse.ust.hk;ruiliang.zhang@tusimple.ai;jamesk@cse.ust.hk,7;6;7,4;4;4,Accept (Poster),0,9,0,yes,9/27/18,The Hong Kong University of Science and Technology;TuSimple;The Hong Kong University of Science and Technology,39;-1;39,44;-1;44,,9/27/18,9,4,1,0,0,0,436;369;255,31;34;44,9;7;8,29;39;20,-1;-1
1762,ICLR,2019,Prior Convictions: Black-box Adversarial Attacks with Bandits and Priors,Andrew Ilyas;Logan Engstrom;Aleksander Madry,ailyas@mit.edu;engstrom@mit.edu;madry@mit.edu,7;7;8,5;3;2,Accept (Poster),1,12,0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2,5;5;5,4,7/20/18,76,47,33,4,61,25,1837;1918;5263,28;27;84,15;15;29,256;254;1054,-1;-1
1763,ICLR,2019,Approximability of Discriminators Implies Diversity in GANs,Yu Bai;Tengyu Ma;Andrej Risteski,yub@stanford.edu;tengyuma@stanford.edu;risteski@mit.edu,8;7;7,2;3;3,Accept (Poster),2,4,0,yes,9/27/18,Stanford University;Stanford University;Massachusetts Institute of Technology,4;4;2,3;3;5,5;4,6/27/18,23,15,5,1,15,3,652;3902;634,157;87;35,11;32;12,50;503;68,-1;-1
1764,ICLR,2019,Self-Monitoring Navigation Agent via Auxiliary Progress Estimation,Chih-Yao Ma;Jiasen Lu;Zuxuan Wu;Ghassan AlRegib;Zsolt Kira;Richard Socher;Caiming Xiong,cyma@gatech.edu;jiasenlu@gatech.edu;zxwu@cs.umd.edu;alregib@gatech.edu;zkira@gatech.edu;rsocher@salesforce.com;cxiong@salesforce.com,6;7;8,4;4;5,Accept (Poster),11,17,0,yes,9/27/18,"Georgia Institute of Technology;Georgia Institute of Technology;University of Maryland, College Park;Georgia Institute of Technology;Georgia Institute of Technology;SalesForce.com;SalesForce.com",13;13;12;13;13;-1;-1,33;33;69;33;33;-1;-1,,9/27/18,42,21,18,3,5,11,269;3750;1554;2201;1141;52693;6270,14;24;45;259;69;180;156,7;13;18;24;16;49;31,28;737;201;145;163;8861;1057,-1;-1
1765,ICLR,2019,Residual Non-local Attention Networks for Image Restoration,Yulun Zhang;Kunpeng Li;Kai Li;Bineng Zhong;Yun Fu,yulun100@gmail.com;kunpengli@ece.neu.edu;li.kai.gml@gmail.com;bnzhong@hqu.edu.cn;yunfu@ece.neu.edu,7;7;6,3;5;3,Accept (Poster),0,6,0,yes,9/27/18,Northeastern University;Northeastern University;Northeastern University;Tsinghua University;Northeastern University,16;16;16;8;16,839;839;839;30;839,,9/27/18,61,37,18,1,0,16,1624;186;25459;1875;10314,27;25;1397;77;328,11;8;68;18;51,338;24;2790;315;1138,-1;-1
1766,ICLR,2019,Emergent Coordination Through Competition,Siqi Liu;Guy Lever;Josh Merel;Saran Tunyasuvunakool;Nicolas Heess;Thore Graepel,liusiqi@google.com;guylever@google.com;jsmerel@google.com;stunya@google.com;heess@google.com;thore@google.com,6;7;7,3;3;3,Accept (Poster),0,3,0,yes,9/27/18,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,,9/27/18,41,21,8,1,43,3,1315;1994;1604;190;11185;18426,97;30;29;7;104;161,17;14;15;4;37;45,81;315;126;9;1603;1367,-1;-1
1767,ICLR,2019,Differentiable Perturb-and-Parse: Semi-Supervised Parsing with a Structured Variational Autoencoder,Caio Corro;Ivan Titov,c.f.corro@uva.nl;i.a.titov@uva.nl,8;7;5,4;3;3,Accept (Poster),0,5,0,yes,9/27/18,University of Amsterdam;University of Amsterdam,169;169,59;59,3;5,7/25/18,16,9,7,0,9,1,28;4656,7;118,3;31,3;563,-1;-1
1768,ICLR,2019,Meta-Learning Probabilistic Inference for Prediction,Jonathan Gordon;John Bronskill;Matthias Bauer;Sebastian Nowozin;Richard Turner,jg801@cam.ac.uk;jfb54@cam.ac.uk;bauer@tue.mpg.de;nowozin@google.com;ret26@cam.ac.uk,7;6;8,4;2;4,Accept (Poster),0,10,3,yes,9/27/18,"University of Cambridge;University of Cambridge;Max Planck Institute for Intelligent Systems, Max-Planck Institute;Google;University of Cambridge",71;71;-1;-1;71,2;2;-1;-1;2,6,5/24/18,45,22,20,1,11,10,272;61;543;6792;2903,24;7;91;134;174,7;3;12;38;30,42;16;40;926;305,-1;-1
1769,ICLR,2019,Data-Dependent Coresets for Compressing Neural Networks with Applications to Generalization Bounds,Cenk Baykal;Lucas Liebenwein;Igor Gilitschenski;Dan Feldman;Daniela Rus,baykal@mit.edu;lucasl@mit.edu;igilitschenski@mit.edu;dannyf@gmail.com;rus@csail.mit.edu,6;7;6,4;4;3,Accept (Poster),0,4,0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;;Massachusetts Institute of Technology,2;2;2;-1;2,5;5;5;-1;5,8,4/15/18,20,10,8,1,11,4,125;47;823;1754;19668,22;10;78;112;503,6;4;15;20;77,8;4;44;126;1011,-1;-1
1770,ICLR,2019,Beyond Greedy Ranking: Slate Optimization via List-CVAE,Ray Jiang;Sven Gowal;Yuqiu Qian;Timothy Mann;Danilo J. Rezende,rayjiang@google.com;sgowal@google.com;yqqian@cs.hku.hk;timothymann@google.com;danilor@google.com,6;6;7,3;4;4,Accept (Poster),0,7,0,yes,9/27/18,Google;Google;The University of Hong Kong;Google;Google,-1;-1;89;-1;-1,-1;-1;40;-1;-1,5,3/5/18,12,6,3,0,2,1,58;544;153;679;8503,11;34;18;43;62,5;10;4;12;27,3;59;15;71;1107,-1;-1
1771,ICLR,2019,Generating Liquid Simulations with Deformation-aware Neural Networks,Lukas Prantl;Boris Bonev;Nils Thuerey,lukas.prantl@tum.de;boris.bonev@tum.de;nils.thuerey@tum.de,7;5;7,4;4;3,Accept (Poster),0,4,0,yes,9/27/18,Technical University Munich;Technical University Munich;Technical University Munich,54;54;54,41;41;41,,4/25/17,6,0,1,0,8,0,1702;31;2521,208;9;119,19;3;31,55;0;189,-1;-1
1772,ICLR,2019,Soft Q-Learning with Mutual-Information Regularization,Jordi Grau-Moya;Felix Leibfried;Peter Vrancx,jordi@prowler.io;felix@prowler.io;peter@prowler.io,7;6;6,4;3;4,Accept (Poster),4,12,0,yes,9/27/18,Prowler.io;Prowler.io;Prowler.io,-1;-1;-1,-1;-1;-1,,9/27/18,17,7,4,0,0,1,216;173;787,23;20;74,8;7;12,7;7;47,-1;-1
1773,ICLR,2019,Hindsight policy gradients,Paulo Rauber;Avinash Ummadisingu;Filipe Mutz;Jürgen Schmidhuber,paulo@idsia.ch;avinash.ummadisingu@usi.ch;filipe.mutz@ifes.edu.br;juergen@idsia.ch,7;7;7,4;4;4,Accept (Poster),2,7,0,yes,9/27/18,IDSIA;Università della Svizzera Italiana;Instituto Federal do Espirito Santo;IDSIA,-1;153;-1;-1,-1;1103;-1;-1,,11/16/17,23,14,4,2,34,3,290;4;188;533,13;3;17;14,7;1;7;7,24;0;7;48,-1;-1
1774,ICLR,2019,Structured Neural Summarization,Patrick Fernandes;Miltiadis Allamanis;Marc Brockschmidt,t-pafern@microsoft.com;miallama@microsoft.com;mabrocks@microsoft.com,7;6;6,3;4;4,Accept (Poster),0,11,0,yes,9/27/18,Microsoft;Microsoft;Microsoft,-1;-1;-1,-1;-1;-1,3;10,9/27/18,30,16,8,0,0,6,51;1900;2437,30;42;61,2;18;22,7;200;327,-1;-1
1775,ICLR,2019,ALISTA: Analytic Weights Are As Good As Learned Weights in LISTA,Jialin Liu;Xiaohan Chen;Zhangyang Wang;Wotao Yin,liujl11@math.ucla.edu;chernxh@tamu.edu;atlaswang@tamu.edu;wotaoyin@math.ucla.edu,7;9;10,4;5;5,Accept (Poster),0,7,0,yes,9/27/18,"University of California, Los Angeles;Texas A&M;Texas A&M;University of California, Los Angeles",20;44;44;20,15;160;160;15,,9/27/18,26,15,10,1,0,5,576;393;2803;16593,80;35;168;221,15;11;27;56,43;40;363;1887,-1;-1
1776,ICLR,2019,Subgradient Descent Learns Orthogonal Dictionaries,Yu Bai;Qijia Jiang;Ju Sun,yub@stanford.edu;qjiang2@stanford.edu;sunju@stanford.edu,6;7;7;7;7,1;2;3;4;3,Accept (Poster),0,8,0,yes,9/27/18,Stanford University;Stanford University;Stanford University,4;4;4,3;3;3,,9/27/18,26,16,15,6,5,11,449;199;409,76;13;14,11;7;6,44;22;40,-1;-1
1777,ICLR,2019,A new dog learns old tricks:  RL finds classic optimization algorithms,Weiwei Kong;Christopher Liaw;Aranyak Mehta;D. Sivakumar,wkong37@gatech.edu;cvliaw@cs.ubc.ca;aranyak@google.com;siva@google.com,6;6;7,3;3;5,Accept (Poster),0,11,0,yes,9/27/18,Georgia Institute of Technology;University of British Columbia;Google;Google,13;36;-1;-1,33;34;-1;-1,4,9/27/18,5,1,1,0,0,0,22;255;2468;6,4;16;63;5,2;6;20;2,3;36;284;1,-1;-1
1778,ICLR,2019,Rigorous Agent Evaluation: An Adversarial Approach to Uncover Catastrophic Failures,Jonathan Uesato*;Ananya Kumar*;Csaba Szepesvari*;Tom Erez;Avraham Ruderman;Keith Anderson;Krishnamurthy (Dj) Dvijotham;Nicolas Heess;Pushmeet Kohli,juesato@gmail.com;ananya@cs.stanford.edu;szepi@google.com;etom@google.com;aruderman@google.com;keithanderson@google.com;dvij@google.com;heess@google.com;pushmeet@google.com,6;6;6,3;3;3,Accept (Poster),0,9,0,yes,9/27/18,;Stanford University;Google;Google;Google;Google;Google;Google;Google,-1;4;-1;-1;-1;-1;-1;-1;-1,-1;3;-1;-1;-1;-1;-1;-1;-1,4,9/27/18,16,11,5,1,0,4,921;70;11222;6431;597;651;1121;11371;22034,17;8;310;48;13;77;76;104;313,11;4;47;19;8;11;17;37;69,112;8;1783;1079;65;42;101;1617;2746,-1;-1
1779,ICLR,2019,AntisymmetricRNN: A Dynamical System View on Recurrent Neural Networks,Bo Chang;Minmin Chen;Eldad Haber;Ed H. Chi,bchang@stat.ubc.ca;minminc@google.com;haber@math.ubc.ca;edchi@google.com,7;7;6,5;5;5,Accept (Poster),0,3,0,yes,9/27/18,University of British Columbia;Google;University of British Columbia;Google,36;-1;36;-1,34;-1;34;-1,,9/27/18,37,17,11,0,0,10,415;1826;3122;10135,56;44;164;209,8;18;32;47,39;290;258;897,-1;-1
1780,ICLR,2019,Recall Traces: Backtracking Models for Efficient Reinforcement Learning,Anirudh Goyal;Philemon Brakel;William Fedus;Soumye Singhal;Timothy Lillicrap;Sergey Levine;Hugo Larochelle;Yoshua Bengio,anirudhgoyal9119@gmail.com;philemon@google.com;liam.fedus@gmail.com;singhalsoumye@gmail.com;countzero@google.com;svlevine@eecs.berkeley.edu;hugolarochelle@google.com;yoshua.bengio@mila.quebec,7;7;6,2;3;3,Accept (Poster),0,10,0,yes,9/27/18,University of Montreal;Google;University of Montreal;IIT Kanpur;Google;University of California Berkeley;Google;University of Montreal,123;-1;123;123;-1;5;-1;123,108;-1;108;578;-1;18;-1;108,,4/2/18,28,15,5,3,95,1,1115;1721;679;23;23726;24519;24485;203485,46;21;25;3;74;309;122;807,12;16;10;1;39;73;43;147,127;161;88;0;2898;3185;2853;24050,-1;-1
1781,ICLR,2019,Auxiliary Variational MCMC,Raza Habib;David Barber,raza.habib@cs.ucl.ac.uk;david.barber@ucl.ac.uk,7;7;7,5;4;4,Accept (Poster),0,11,0,yes,9/27/18,University College London;University College London,50;50,16;16,11,9/27/18,6,1,4,0,0,0,517;3754,9;200,3;27,39;405,-1;-1
1782,ICLR,2019,Training for Faster Adversarial Robustness Verification via Inducing ReLU Stability,Kai Y. Xiao;Vincent Tjeng;Nur Muhammad (Mahi) Shafiullah;Aleksander Madry,kaix@mit.edu;vtjeng@mit.edu;nshafiul@mit.edu;madry@mit.edu,5;8;7,3;2;3,Accept (Poster),0,14,0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2,5;5;5;5,4,9/9/18,71,43,20,4,16,11,365;274;68;5263,44;5;1;84,8;3;1;29,43;43;10;1054,-1;-1
1783,ICLR,2019,RotDCF: Decomposition of Convolutional Filters for Rotation-Equivariant Deep Networks,Xiuyuan Cheng;Qiang Qiu;Robert Calderbank;Guillermo Sapiro,xiuyuan.cheng@duke.edu;qiang.qiu@duke.edu;robert.calderbank@duke.edu;guillermo.sapiro@duke.edu,7;7;7,3;2;4,Accept (Poster),0,4,1,yes,9/27/18,Duke University;Duke University;Duke University;Duke University,44;44;44;44,17;17;17;17,,5/17/18,11,8,3,1,7,2,361;1222;19982;42594,51;90;408;650,11;18;53;85,30;117;2086;3902,-1;-1
1784,ICLR,2019,Three Mechanisms of Weight Decay Regularization,Guodong Zhang;Chaoqi Wang;Bowen Xu;Roger Grosse,gdzhang.cs@gmail.com;cqwang@cs.toronto.edu;bowenxu@cs.toronto.com;rgrosse@cs.toronto.edu,6;7;7,4;5;4,Accept (Poster),0,9,0,yes,9/27/18,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",18;18;18;18,22;22;22;22,8,9/27/18,47,31,16,1,107,8,1412;145;571;6065,39;20;44;101,11;6;10;28,225;10;37;834,-1;-1
1785,ICLR,2019,Imposing Category Trees Onto Word-Embeddings Using A Geometric Construction,Tiansi Dong;Chrisitan Bauckhage;Hailong Jin;Juanzi Li;Olaf Cremers;Daniel Speicher;Armin B. Cremers;Joerg Zimmermann,tian1shi2@gmail.com;christian.bauckhage@iais.fraunhofer.de;jinhl15@mails.tsinghua.edu.cn;lijuanzi2008@gmail.com;cremerso@iai.uni-bonn.de;dsp@bit.uni-bonn.de;abc@iai.uni-bonn.de;jz@bit.uni-bonn.de,3;4;4,4;5;4,Accept (Poster),0,2,32,yes,9/27/18,University of Bonn;Fraunhofer IIS;Tsinghua University;;University of Bonn;University of Bonn;University of Bonn;University of Bonn,123;-1;8;-1;123;123;123;123,100;-1;30;-1;100;100;100;100,3,9/27/18,7,3,0,0,0,1,89;27;128;216;3;255;5686;91,39;6;38;39;2;46;307;49,5;3;7;7;1;7;27;6,6;2;8;13;0;7;271;5,-1;-1
1786,ICLR,2019,PeerNets: Exploiting Peer Wisdom Against Adversarial Attacks,Jan Svoboda;Jonathan Masci;Federico Monti;Michael Bronstein;Leonidas Guibas,jan.svoboda@usi.ch;jonathan@nnaisense.com;federico.monti@usi.ch;michael.bronstein@usi.ch;guibas@cs.stanford.edu,6;7,5;4,Accept (Poster),2,8,0,yes,9/27/18,Università della Svizzera Italiana;NNAISENSE;Università della Svizzera Italiana;Università della Svizzera Italiana;Stanford University,153;-1;153;153;4,1103;-1;1103;1103;3,4;10,5/31/18,16,10,3,0,25,0,2191;5131;1113;11710;44944,331;47;17;295;698,18;22;10;53;97,150;463;140;1151;5532,-1;-1
1787,ICLR,2019,Measuring Compositionality in Representation Learning,Jacob Andreas,jda@cs.berkeley.edu,7;6;6,4;4;4,Accept (Poster),0,4,0,yes,9/27/18,University of California Berkeley,5,18,8,9/27/18,27,18,4,1,0,6,2438,42,23,329,-1
1788,ICLR,2019,ProxQuant: Quantized Neural Networks via Proximal Operators,Yu Bai;Yu-Xiang Wang;Edo Liberty,yub@stanford.edu;yuxiangw@cs.ucsb.edu;libertye@amazon.com,7;5;8,4;4;4,Accept (Poster),2,9,0,yes,9/27/18,Stanford University;UC Santa Barbara;Amazon,4;37;-1,3;53;-1,9,9/27/18,28,17,11,2,3,4,449;1592;1893,76;59;66,11;23;19,44;217;213,-1;-1
1789,ICLR,2019,Directed-Info GAIL: Learning Hierarchical Policies from Unsegmented Demonstrations using Directed Information,Mohit Sharma;Arjun Sharma;Nicholas Rhinehart;Kris M. Kitani,mohits1@andrew.cmu.edu;arjuns2@andrew.cmu.edu;nrhineha@cs.cmu.edu;kkitani@cs.cmu.edu,6;6;8,4;4;4,Accept (Poster),0,6,0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1,24;24;24;24,5;4;10,9/27/18,13,8,2,0,6,1,263;365;372;2403,60;48;21;115,11;5;9;25,28;24;42;228,-1;-1
1790,ICLR,2019,Hierarchical RL Using an Ensemble of Proprioceptive Periodic Policies,Kenneth Marino;Abhinav Gupta;Rob Fergus;Arthur Szlam,kdmarino@cs.cmu.edu;abhinavg@cs.cmu.edu;fergus@cs.nyu.edu;aszlam@fb.com,7;6;7,5;4;3,Accept (Poster),0,7,0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;New York University;Facebook,1;1;26;-1,24;24;27;-1,,9/27/18,7,5,2,0,0,1,297;17010;51177;8609,5;233;126;85,4;63;61;32,38;1903;6390;913,-1;-1
1791,ICLR,2019,ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness,Robert Geirhos;Patricia Rubisch;Claudio Michaelis;Matthias Bethge;Felix A. Wichmann;Wieland Brendel,robert@geirhos.de;patricia@rubisch.net;claudio.michaelis@bethgelab.org;matthias.bethge@uni-tuebingen.de;felix.wichmann@uni-tuebingen.de;wieland.brendel@bethgelab.org,8;7;8,4;4;4,Accept (Oral),2,9,3,yes,9/27/18,"University of Tuebingen;;Centre for Integrative Neuroscience, AG Bethge;University of Tuebingen;University of Tuebingen;Centre for Integrative Neuroscience, AG Bethge",153;-1;153;153;153;153,94;-1;94;94;94;94,2,9/27/18,334,203,57,25,32,38,548;325;383;11087;5398;1692,9;3;11;415;175;39,5;2;6;45;29;16,56;37;42;1241;642;206,-1;-1
1792,ICLR,2019,Verification of Non-Linear Specifications for Neural Networks,Chongli Qin;Krishnamurthy (Dj) Dvijotham;Brendan O'Donoghue;Rudy Bunel;Robert Stanforth;Sven Gowal;Jonathan Uesato;Grzegorz Swirszcz;Pushmeet Kohli,chongliqin@google.com;dvij@google.com;bodonoghue@google.com;rbunel@google.com;stanforth@google.com;sgowal@google.com;juesato@google.com;swirszcz@google.com;pushmeet@google.com,7;7;5,3;5;3,Accept (Poster),0,12,0,yes,9/27/18,Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1,4,9/27/18,14,5,1,0,2,0,224;1121;2390;426;484;557;921;643;22034,8;76;30;20;18;34;17;50;313,7;17;16;10;9;11;11;15;69,32;101;297;52;57;61;112;52;2746,-1;-1
1793,ICLR,2019,LEARNING FACTORIZED REPRESENTATIONS FOR OPEN-SET DOMAIN ADAPTATION,Mahsa Baktashmotlagh;Masoud Faraki;Tom Drummond;Mathieu Salzmann,m.baktashmotlagh@qut.edu.au;masoud.faraki@monash.edu;tom.drummond@monash.edu;mathieu.salzmann@epfl.ch,7;6;6,3;4;5,Accept (Poster),2,10,0,yes,9/27/18,South China University of Technology;Monash University;Monash University;Swiss Federal Institute of Technology Lausanne,478;123;123;478,576;80;80;38,,5/31/18,11,4,3,0,0,0,556;193;8728;5449,32;14;165;198,11;8;36;41,58;25;860;607,-1;-1
1794,ICLR,2019,Variance Networks: When Expectation Does Not Meet Your Expectations,Kirill Neklyudov;Dmitry Molchanov;Arsenii Ashukha;Dmitry Vetrov,k.necludov@gmail.com;dmolch111@gmail.com;ars.ashuha@gmail.com;vetrovd@yandex.ru,6;6;6,3;4;4,Accept (Poster),0,3,0,yes,9/27/18,Higher School of Economics;Samsung;Samsung;Higher School of Economics,478;-1;-1;478,377;-1;-1;377,4;11,3/10/18,9,3,3,0,0,0,141;461;460;2006,9;17;14;123,4;7;8;16,27;78;79;275,-1;-1
1795,ICLR,2019,Function Space Particle Optimization for Bayesian Neural Networks,Ziyu Wang;Tongzheng Ren;Jun Zhu;Bo Zhang,wzy196@gmail.com;rtz19970824@gmail.com;dcszj@mail.tsinghua.edu.cn;dcszb@mail.tsinghua.edu.cn,7;7;7,3;4;3,Accept (Poster),0,10,2,yes,9/27/18,Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University,8;8;8;8,30;30;30;30,4;11,9/27/18,13,10,3,0,0,2,102;44;4464;10344,43;11;204;1042,6;3;35;46,6;5;520;759,-1;-1
1796,ICLR,2019,CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model,Florian Mai;Lukas Galke;Ansgar Scherp,florian.ren.mai@googlemail.com;lga@informatik.uni-kiel.de;mail@ansgarscherp.net,6;5;6,3;4;4,Accept (Poster),0,6,0,yes,9/27/18,Idiap Research Institute;Kiel University;University of Stirling,-1;62;478,-1;12;309,3,9/27/18,2,1,0,1,2,0,26;47;1667,20;18;210,3;5;20,0;5;104,-1;-1
1797,ICLR,2019,Evaluating Robustness of Neural Networks with Mixed Integer Programming,Vincent Tjeng;Kai Y. Xiao;Russ Tedrake,vtjeng@mit.edu;kaix@mit.edu;russt@mit.edu,7;8;7,5;5;1,Accept (Poster),0,14,0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2,5;5;5,4,11/20/17,137,71,68,4,7,30,274;365;8244,5;44;184,3;8;45,43;43;597,-1;-1
1798,ICLR,2019,Post Selection Inference with Incomplete Maximum Mean Discrepancy Estimator,Makoto Yamada;Denny Wu;Yao-Hung Hubert Tsai;Hirofumi Ohta;Ruslan Salakhutdinov;Ichiro Takeuchi;Kenji Fukumizu,makoto.yamada@riken.jp;yiwu1@andrew.cmu.edu;yaohungt@cs.cmu.edu;hirofumi-ohta@g.ecc.u-tokyo.ac.jp;rsalakhu@cs.cmu.edu;takeuchi.ichiro@nitech.ac.jp;fukumizu@ism.ac.jp,6;5;8,4;4;4,Accept (Poster),0,6,0,yes,9/27/18,"RIKEN;Carnegie Mellon University;Carnegie Mellon University;The University of Tokyo;Carnegie Mellon University;Meiji University;The Institute of Statistical Mathematics, Japan",-1;1;1;54;1;478;-1,-1;24;24;45;24;334;-1,5;4,2/17/18,5,4,3,1,0,1,1623;32;534;20;67473;1697;5740,245;10;63;16;254;183;194,19;4;12;3;82;19;35,126;1;72;2;7780;93;762,-1;-1
1799,ICLR,2019,Opportunistic Learning: Budgeted Cost-Sensitive Learning from Data Streams,Mohammad Kachuee;Orpaz Goldstein;Kimmo Kärkkäinen;Sajad Darabi;Majid Sarrafzadeh,mkachuee@cs.ucla.edu;orpgol@cs.ucla.edu;kimmo@cs.ucla.edu;sajad.darabi@cs.ucla.edu;majid@cs.ucla.edu,7;6;6,4;4;4,Accept (Poster),0,14,0,yes,9/27/18,"University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles",20;20;20;20;20,15;15;15;15;15,,9/27/18,9,5,5,1,7,1,93;24;24;57;8966,17;11;10;13;637,4;3;3;4;45,9;1;4;8;631,-1;-1
1800,ICLR,2019,Riemannian Adaptive Optimization Methods,Gary Becigneul;Octavian-Eugen Ganea,gary.becigneul@inf.ethz.ch;octavian.ganea@inf.ethz.ch,7;7;7,3;5;4,Accept (Poster),5,6,3,yes,9/27/18,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10,10;10,1;8,9/27/18,41,10,13,0,0,8,226;509,17;17,5;9,46;94,-1;-1
1801,ICLR,2019,StrokeNet: A Neural Painting Environment,Ningyuan Zheng;Yifan Jiang;Dingjiang Huang,zhengningyuan@qq.com;winhehe@163.com;djhuang@dase.ecnu.edu.cn,7;6;8,4;4;5,Accept (Poster),0,8,2,yes,9/27/18,Australian National University;Australian National University;Australian National University,106;106;106,48;48;48,,9/27/18,19,9,5,0,0,0,19;32;132,1;12;16,1;3;7,0;0;5,-1;-1
1802,ICLR,2019,Non-vacuous Generalization Bounds at the ImageNet Scale: a PAC-Bayesian Compression Approach,Wenda Zhou;Victor Veitch;Morgane Austern;Ryan P. Adams;Peter Orbanz,wz2335@columbia.edu;victorveitch@gmail.com;ma3293@columbia.edu;rpa@princeton.edu;porbanz@stat.columbia.edu,6;6;8,4;5;4,Accept (Poster),0,5,0,yes,9/27/18,Columbia University;Columbia University;Columbia University;Princeton University;Columbia University,15;15;15;30;15,14;14;14;7;14,1;8,4/16/18,64,29,17,1,52,6,82;784;67;12150;793,9;30;9;177;37,4;11;3;45;12,7;68;7;1316;79,-1;-1
1803,ICLR,2019,RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space,Zhiqing Sun;Zhi-Hong Deng;Jian-Yun Nie;Jian Tang,1500012783@pku.edu.cn;zhdeng@pku.edu.cn;nie@iro.umontreal.ca;jian.tang@hec.ca,7;7;7,4;3;4,Accept (Poster),1,21,13,yes,9/27/18,Peking University;Peking University;University of Montreal;HEC Montreal,24;24;123;-1,27;27;108;-1,4;10,9/27/18,119,68,71,8,3,45,176;1394;6486;2649,13;132;265;71,6;20;38;13,61;208;564;720,-1;-1
1804,ICLR,2019,STCN: Stochastic Temporal Convolutional Networks,Emre Aksan;Otmar Hilliges,eaksan@inf.ethz.ch;otmar.hilliges@inf.ethz.ch,6;6;6,4;3;5,Accept (Poster),0,8,0,yes,9/27/18,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10,10;10,,9/27/18,8,3,2,0,2,0,157;7476,24;136,7;33,17;863,-1;-1
1805,ICLR,2019,From Hard to Soft: Understanding Deep Network Nonlinearities via Vector Quantization and Statistical Inference,Randall Balestriero;Richard Baraniuk,randallbalestriero@gmail.com;richb@rice.edu,6;6;7,3;4;5,Accept (Poster),0,4,0,yes,9/27/18,Rice University;Rice University,85;85,86;86,2,9/27/18,1,1,0,0,3,0,112;29329,38;660,5;84,2;2745,-1;-1
1806,ICLR,2019,Competitive experience replay,Hao Liu;Alexander Trott;Richard Socher;Caiming Xiong,lhao499@gmail.com;atrott@salesforce.com;rsocher@salesforce.com;cxiong@salesforce.com,5;7;6;7,4;4;4;5,Accept (Poster),0,6,0,yes,9/27/18,University of California Berkeley;SalesForce.com;SalesForce.com;SalesForce.com,5;-1;-1;-1,18;-1;-1;-1,4,9/27/18,10,8,2,0,2,0,282;466;52263;6210,90;73;180;156,9;13;49;31,17;25;8807;1052,-1;-1
1807,ICLR,2019,Deterministic Variational Inference for Robust Bayesian Neural Networks,Anqi Wu;Sebastian Nowozin;Edward Meeds;Richard E. Turner;José Miguel Hernández-Lobato;Alexander L. Gaunt,anqiw@princeton.edu;sebastian.nowozin@microsoft.com;ted.meeds@microsoft.com;ret26@cam.ac.uk;jmh233@cam.ac.uk;algaunt@microsoft.com,7;7;7,3;3;5,Accept (Oral),0,3,1,yes,9/27/18,Princeton University;Microsoft;Microsoft;University of Cambridge;University of Cambridge;Microsoft,30;-1;-1;71;71;-1,7;-1;-1;2;2;-1,11,9/27/18,48,25,16,1,6,9,236;6820;577;2936;3724;1142,31;134;21;175;113;38,8;39;10;30;28;14,13;928;81;308;419;88,-1;-1
1808,ICLR,2019,Marginal Policy Gradients: A Unified Family of Estimators for Bounded Action Spaces with Applications,Carson Eisenach;Haichuan Yang;Ji Liu;Han Liu,eisenach@princeton.edu;h.yang@rochester.edu;ji.liu.uwisc@gmail.com;hanliu.cmu@gmail.com,7;7;6,4;3;3,Accept (Poster),0,4,0,yes,9/27/18,Princeton University;University of Rochester;University of Rochester;,30;106;106;-1,7;153;153;-1,,6/13/18,4,2,1,0,0,0,15;170;74;5489,8;32;22;446,2;8;5;34,0;4;3;415,-1;-1
1809,ICLR,2019,Harmonic Unpaired Image-to-image Translation,Rui Zhang;Tomas Pfister;Jia Li,zhangrui@ict.ac.cn;tpfister@google.com;lijiali@google.com,6;5;4,5;5;5,Accept (Poster),3,7,0,yes,9/27/18,"Institute of Computing Technology, Chinese Academy of Sciences;Google;Google",62;-1;-1,1103;-1;-1,10,9/27/18,10,4,1,0,0,0,5621;2272;25540,491;47;71,33;16;33,468;284;4491,-1;-1
1810,ICLR,2019,Trellis Networks for Sequence Modeling,Shaojie Bai;J. Zico Kolter;Vladlen Koltun,shaojieb@cs.cmu.edu;zkolter@cs.cmu.edu;vkoltun@gmail.com,7;6;7,3;3;3,Accept (Poster),0,6,0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;Intel,1;1;-1,24;24;-1,3,9/27/18,24,10,8,1,359,0,697;7542;17520,15;104;188,6;35;62,116;1052;2489,-1;-1
1811,ICLR,2019,Adversarial Reprogramming of Neural Networks,Gamaleldin F. Elsayed;Ian Goodfellow;Jascha Sohl-Dickstein,gamaleldin.elsayed@gmail.com;goodfellow@google.com;jaschasd@google.com,8;6;4,4;5;3,Accept (Poster),2,6,2,yes,9/27/18,Google;Google;Google,-1;-1;-1,-1;-1;-1,4;2,6/28/18,13,6,3,1,0,2,488;55154;4877,20;90;101,10;56;33,37;9293;677,-1;-1
1812,ICLR,2019,Variational Bayesian Phylogenetic Inference,Cheng Zhang;Frederick A. Matsen IV,zc.rabbit@gmail.com;matsen@fredhutch.org,6;5;7,3;3;1,Accept (Poster),0,6,0,yes,9/27/18,Fred Hutchinson Cancer Research Center;Fred Hutchinson Cancer Research Center,-1;-1,-1;-1,11;10,9/27/18,4,0,1,0,0,0,151;755,111;25,7;9,5;72,-1;-1
1813,ICLR,2019,Learning to Understand Goal Specifications by Modelling Reward,Dzmitry Bahdanau;Felix Hill;Jan Leike;Edward Hughes;Arian Hosseini;Pushmeet Kohli;Edward Grefenstette,dimabgv@gmail.com;felixhill@google.com;leike@google.com;edwardhughes@google.com;seyedarian.hosseini@umontreal.ca;pushmeet@google.com;etg@google.com,7;7;6,4;5;4,Accept (Poster),0,0,21,yes,9/27/18,University of Montreal;Google;Google;Google;University of Montreal;Google;Google,123;-1;-1;-1;123;-1;-1,108;-1;-1;-1;108;-1;-1,,6/5/18,40,22,8,1,183,4,26850;3513;805;152;36;22034;7041,31;52;47;20;2;313;57,18;22;14;7;1;69;25,4174;672;64;13;4;2746;839,-1;-1
1814,ICLR,2019,Overcoming the Disentanglement vs Reconstruction Trade-off via Jacobian Supervision,José Lezama,jlezama@fing.edu.uy,5;7;7,3;4;4,Accept (Poster),2,5,0,yes,9/27/18,Facultad de Ingeniería,478,631,4,9/27/18,6,1,1,1,0,1,223,36,6,23,-1
1815,ICLR,2019,Relational Forward Models for Multi-Agent Learning,Andrea Tacchetti;H. Francis Song;Pedro A. M. Mediano;Vinicius Zambaldi;János Kramár;Neil C. Rabinowitz;Thore Graepel;Matthew Botvinick;Peter W. Battaglia,atacchet@google.com;songf@google.com;pmediano@imperial.ac.uk;vzambaldi@google.com;janosk@google.com;ncr@google.com;thore@google.com;botvinick@google.com;peterbattaglia@google.com,7;6;7;6,3;4;3;3,Accept (Poster),0,21,1,yes,9/27/18,Google;Google;Imperial College London;Google;Google;Google;Google;Google;Google,-1;-1;72;-1;-1;-1;-1;-1;-1,-1;-1;8;-1;-1;-1;-1;-1;-1,,9/27/18,20,11,10,0,0,3,44;1208;341;1517;405;2994;18894;13541;4491,6;27;20;18;14;37;161;146;88,3;14;8;12;7;17;45;45;29,12;114;29;158;36;394;1394;1412;419,-1;-1
1816,ICLR,2019,NADPEx: An on-policy temporally consistent exploration method for deep reinforcement learning,Sirui Xie;Junning Huang;Lanxin Lei;Chunxiao Liu;Zheng Ma;Wei Zhang;Liang Lin,xiesirui@sensetime.com;huangjunning@sensetime.com;leilanxin@sensetime.com;liuchunxiao@sensetime.com;mazheng@sensetime.com;wayne.zhang@sensetime.com;linliang@ieee.org,6;8;7,3;3;3,Accept (Poster),2,9,0,yes,9/27/18,SenseTime Group Limited;SenseTime Group Limited;SenseTime Group Limited;SenseTime Group Limited;SenseTime Group Limited;SenseTime Group Limited;SUN YAT-SEN UNIVERSITY,-1;-1;-1;-1;-1;-1;478,-1;-1;-1;-1;-1;-1;352,,9/27/18,3,0,2,0,13,0,226;20;2;472;3167;630;8915,7;8;3;48;354;118;294,2;3;1;10;27;11;49,64;1;0;80;163;36;906,-1;-1
1817,ICLR,2019,"Woulda, Coulda, Shoulda: Counterfactually-Guided Policy Search",Lars Buesing;Theophane Weber;Yori Zwols;Nicolas Heess;Sebastien Racaniere;Arthur Guez;Jean-Baptiste Lespiau,lbuesing@google.com;theophane@google.com;yori@google.com;heess@google.com;sracaniere@google.com;aguez@google.com;jblespiau@google.com,7;7;7,2;3;3,Accept (Poster),0,5,1,yes,9/27/18,Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,,9/27/18,28,20,5,1,195,4,2403;244;1146;11371;785;13154;100,54;20;25;104;32;32;9,21;7;9;37;12;17;6,250;31;136;1617;76;931;10,-1;-1
1818,ICLR,2019,Big-Little Net: An Efficient Multi-Scale Feature Representation for Visual and Speech Recognition,Chun-Fu (Richard) Chen;Quanfu Fan;Neil Mallinar;Tom Sercu;Rogerio Feris,chenrich@us.ibm.com;qfan@us.ibm.com;neil.r.mallinar@ibm.com;tom.sercu1@ibm.com;rsferis@us.ibm.com,7;6;7,4;5;4,Accept (Poster),0,4,0,yes,9/27/18,International Business Machines;International Business Machines;International Business Machines;International Business Machines;International Business Machines,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,8,7/10/18,18,11,7,0,2,4,301;1586;29;749;5072,31;62;6;25;163,7;16;3;11;34,44;174;4;65;490,-1;-1
1819,ICLR,2019,There Are Many Consistent Explanations of Unlabeled Data: Why You Should Average,Ben Athiwaratkun;Marc Finzi;Pavel Izmailov;Andrew Gordon Wilson,pa338@cornell.edu;maf388@cornell.edu;izmailovpavel@gmail.com;andrew@cornell.edu,6;8;6,4;4;1,Accept (Poster),2,6,5,yes,9/27/18,Cornell University;Cornell University;Cornell University;Cornell University,7;7;7;7,19;19;19;19,8,6/14/18,57,25,27,4,10,14,403;91;502;2728,11;5;16;102,9;4;10;27,44;16;86;331,-1;-1
1820,ICLR,2019,Wasserstein Barycenter Model Ensembling,Pierre Dognin*;Igor Melnyk*;Youssef Mroueh*;Jarret Ross*;Cicero Dos Santos*;Tom Sercu*,pdognin@us.ibm.com;igor.melnyk@ibm.com;mroueh@us.ibm.com;rossja@us.ibm.com;cicerons@us.ibm.com;tom.sercu1@ibm.com,6;6;6,4;3;4,Accept (Poster),0,5,0,yes,9/27/18,International Business Machines;International Business Machines;International Business Machines;International Business Machines;International Business Machines;International Business Machines,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,3,9/27/18,5,2,1,0,0,0,119;248;878;392;4447;749,27;33;52;12;60;25,6;10;11;4;19;11,6;17;145;86;500;65,-1;-1
1821,ICLR,2019,Learning what you can do before doing anything,Oleh Rybkin;Karl Pertsch;Konstantinos G. Derpanis;Kostas Daniilidis;Andrew Jaegle,oleh@seas.upenn.edu;pertsch@usc.edu;kosta@ryerson.ca;kostas@seas.upenn.edu;ajaegle@upenn.edu,6;7;6,4;4;3,Accept (Poster),0,6,0,yes,9/27/18,University of Pennsylvania;University of Southern California;Ryerson University;University of Pennsylvania;University of Pennsylvania,19;30;314;19;19,10;66;1103;10;10,,6/25/18,3,1,1,1,0,0,21;45;2427;10023;135,12;10;68;342;22,3;4;22;53;6,1;8;314;937;4,-1;-1
1822,ICLR,2019,Learning Localized Generative Models for 3D Point Clouds via Graph Convolution,Diego Valsesia;Giulia Fracastoro;Enrico Magli,diego.valsesia@polito.it;giulia.fracastoro@polito.it;enrico.magli@polito.it,6;9;7,4;3;3,Accept (Poster),0,7,0,yes,9/27/18,Politecnico di Torino;Politecnico di Torino;Politecnico di Torino,478;478;478,407;407;407,10;5;2;8,9/27/18,28,19,11,2,0,6,294;224;3543,44;29;324,10;10;29,22;15;255,-1;-1
1823,ICLR,2019,Policy Transfer with Strategy Optimization,Wenhao Yu;C. Karen Liu;Greg Turk,wyu68@gatech.edu;karenliu@cc.gatech.edu;turk@cc.gatech.edu,7;6;7,4;4;4,Accept (Poster),0,7,0,yes,9/27/18,Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology,13;13;13,33;33;33,6,9/27/18,18,12,5,0,13,2,806;2599;11207,60;100;98,16;29;47,59;131;879,-1;-1
1824,ICLR,2019,Measuring and regularizing networks in function space,Ari Benjamin;David Rolnick;Konrad Kording,aarrii@seas.upenn.edu;drolnick@mit.edu;koerding@gmail.com,6;6;6,4;3;4,Accept (Poster),0,5,1,yes,9/27/18,University of Pennsylvania;Massachusetts Institute of Technology;University of Pennsylvania,19;2;19,10;5;10,,5/21/18,12,8,5,0,28,2,76;609;5068,13;37;110,5;10;30,5;39;332,-1;-1
1825,ICLR,2019,Learning Two-layer Neural Networks with Symmetric Inputs,Rong Ge;Rohith Kuditipudi;Zhize Li;Xiang Wang,rongge@cs.duke.edu;rohith.kuditipudi@duke.edu;zz-li14@mails.tsinghua.edu.cn;xwang@cs.duke.edu,7;6;7,4;4;5,Accept (Poster),0,3,0,yes,9/27/18,Duke University;Duke University;Tsinghua University;Duke University,44;44;8;44,17;17;30;17,9,9/27/18,19,7,1,0,4,0,5730;32;138;494,78;4;23;136,32;3;7;9,800;0;12;28,-1;-1
1826,ICLR,2019,CEM-RL: Combining evolutionary and gradient-based methods for policy search,Pourchot;Sigaud,alois.pourchot@telecom-paristech.fr;olivier.sigaud@upmc.fr,6;7;7,3;5;4,Accept (Poster),0,7,0,yes,9/27/18,"Télécom ParisTech;Computer Science Lab  - Pierre and Marie Curie University, Paris, France",478;478,188;123,,9/27/18,30,14,16,3,2,4,33;2153,2;159,2;25,4;139,-1;-1
1827,ICLR,2019,Multiple-Attribute Text Rewriting,Guillaume Lample;Sandeep Subramanian;Eric Smith;Ludovic Denoyer;Marc'Aurelio Ranzato;Y-Lan Boureau,glample@fb.com;sandeep.subramanian.1@umontreal.ca;ems@fb.com;ludovic.denoyer@lip6.fr;ranzato@fb.com;ylan@fb.com,7;6;6,3;4;3,Accept (Poster),0,11,0,yes,9/27/18,Facebook;University of Montreal;Facebook;LIP6;Facebook;Facebook,-1;123;-1;-1;-1;-1,-1;108;-1;-1;-1;-1,4;7,9/27/18,67,48,30,2,35,20,4755;2506;124;3000;20172;4387,24;17;9;128;75;24,18;11;4;22;42;14,993;495;36;524;2051;334,-1;-1
1828,ICLR,2019,Improving Generalization and Stability of Generative Adversarial Networks,Hoang Thanh-Tung;Truyen Tran;Svetha Venkatesh,hoangtha@deakin.edu.au;truyen.tran@deakin.edu.au;svetha.venkatesh@deakin.edu.au,6;7;7,4;3;3,Accept (Poster),0,14,0,yes,9/27/18,Deakin University;Deakin University;Deakin University,478;478;478,334;334;334,5;4;8,9/27/18,33,14,8,1,3,4,52;1857;1719,3;133;75,2;24;17,5;124;128,-1;-1
1829,ICLR,2019,Sliced Wasserstein Auto-Encoders,Soheil Kolouri;Phillip E. Pope;Charles E. Martin;Gustavo K. Rohde,skolouri@hrl.com;pepope@hrl.com;cemartin@hrl.com;gustavo@virginia.edu,6;4;6,4;4;4,Accept (Poster),0,8,0,yes,9/27/18,HRL Labs;HRL Labs;HRL Labs;University of Virginia,-1;-1;-1;65,-1;-1;-1;113,5;4,9/27/18,25,9,12,0,0,1,806;44;24;2860,60;4;6;138,17;3;1;25,41;1;0;190,-1;-1
1830,ICLR,2019,The Laplacian in RL: Learning Representations with Efficient Approximations,Yifan Wu;George Tucker;Ofir Nachum,yw4@andrew.cmu.edu;gjt@google.com;ofirnachum@google.com,7;7;7,3;3;4,Accept (Poster),0,3,0,yes,9/27/18,Carnegie Mellon University;Google;Google,1;-1;-1,24;-1;-1,10,9/27/18,8,3,5,0,19,2,515;2557;1032,112;74;41,13;21;15,43;285;152,-1;-1
1831,ICLR,2019,Sparse Dictionary Learning by Dynamical Neural Networks,Tsung-Han Lin;Ping Tak Peter Tang,tsung-han.lin@intel.com;peter.tang@intel.com,6;9;8,4;4;4,Accept (Poster),0,3,0,yes,9/27/18,Intel;Intel,-1;-1,-1;-1,,5/23/18,4,2,0,0,6,0,1105;2187,62;72,14;20,114;209,-1;-1
1832,ICLR,2019,Learning Programmatically Structured Representations with Perceptor Gradients,Svetlin Penkov;Subramanian Ramamoorthy,sv.penkov@gmail.com;s.ramamoorthy@ed.ac.uk,7;5;6,5;3;1,Accept (Poster),0,8,0,yes,9/27/18,University of Edinburgh;University of Edinburgh,33;33,27;27,,9/27/18,5,3,0,0,6,0,49;934,16;139,4;17,1;53,-1;-1
1833,ICLR,2019,Adversarial Audio Synthesis,Chris Donahue;Julian McAuley;Miller Puckette,cdonahue@ucsd.edu;jmcauley@eng.ucsd.edu;msp@ucsd.edu,6;5;6,3;4;4,Accept (Poster),0,7,0,yes,9/27/18,"University of California, San Diego;University of California, San Diego;University of California, San Diego",11;11;11,31;31;31,5;4,2/12/18,102,42,45,3,0,20,426;7324;2167,16;133;94,7;32;22,60;1026;224,-1;-1
1834,ICLR,2019,PATE-GAN: Generating Synthetic Data with Differential Privacy Guarantees,James Jordon;Jinsung Yoon;Mihaela van der Schaar,james.jordon@wolfson.ox.ac.uk;jsyoon0823@gmail.com;mihaela.vanderschaar@eng.ox.ac.uk,6;7;7,4;4;3,Accept (Poster),0,11,1,yes,9/27/18,"University of Oxford;University of California, Los Angeles;University of Oxford",50;20;50,1;15;1,5;4;1,9/27/18,31,14,13,0,0,7,240;536;8607,17;55;643,8;13;42,43;57;537,-1;-1
1835,ICLR,2019,Caveats for information bottleneck in deterministic scenarios,Artemy Kolchinsky;Brendan D. Tracey;Steven Van Kuyk,artemyk@gmail.com;tracey.brendan@gmail.com;steven.jvk@gmail.com,2;8;6,4;4;4,Accept (Poster),0,18,0,yes,9/27/18,Santa Fe Institute;;,-1;-1;-1,-1;-1;-1,,8/23/18,15,10,2,0,0,1,497;561;28,52;28;5,12;11;3,54;53;1,-1;-1
1836,ICLR,2019,Unsupervised Adversarial Image Reconstruction,Arthur Pajot;Emmanuel de Bezenac;Patrick Gallinari,arthur.pajot@lip6.fr;emmanuel.de-bezenac@lip6.fr;patrick.gallinari@lip6.fr,6;8;4,3;4;3,Accept (Poster),0,7,0,yes,9/27/18,LIP6;LIP6;LIP6,-1;-1;-1,-1;-1;-1,5;4,9/27/18,6,2,2,0,0,1,78;84;4770,9;9;450,3;3;33,9;9;377,-1;-1
1837,ICLR,2019,LEARNING TO PROPAGATE LABELS: TRANSDUCTIVE PROPAGATION NETWORK FOR FEW-SHOT LEARNING,Yanbin Liu;Juho Lee;Minseop Park;Saehoon Kim;Eunho Yang;Sung Ju Hwang;Yi Yang,csyanbin@gmail.com;juho.lee@stats.ox.ac.uk;mike_seop@aitrics.com;shkim@aitrics.com;eunhoy@kaist.ac.kr;sjhwang82@kaist.ac.kr;yi.yang@uts.edu.au,5;6;7,3;3;4,Accept (Poster),7,12,0,yes,9/27/18,University of Technology Sydney;University of Oxford;AITRICS;AITRICS;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;University of Technology Sydney,106;50;-1;-1;20;20;106,216;1;-1;-1;95;95;216,6;10,5/25/18,68,32,22,5,5,17,80;74;123;413;1044;67;4933,10;19;7;35;76;4;344,3;2;4;10;16;1;30,17;17;28;65;169;17;196,-1;-1
1838,ICLR,2019,Approximating CNNs with Bag-of-local-Features models works surprisingly well on ImageNet,Wieland Brendel;Matthias Bethge,wieland.brendel@bethgelab.org;matthias.bethge@uni-tuebingen.de,6;7;7,4;4;4,Accept (Poster),2,19,2,yes,9/27/18,"Centre for Integrative Neuroscience, AG Bethge;University of Tuebingen",153;153,94;94,,9/27/18,125,67,28,4,0,14,1719;11274,39;415,16;45,210;1251,-1;-1
1839,ICLR,2019,"Attention, Learn to Solve Routing Problems!",Wouter Kool;Herke van Hoof;Max Welling,w.w.m.kool@uva.nl;h.c.vanhoof@uva.nl;m.welling@uva.nl,7;6;7,5;5;5,Accept (Poster),0,5,0,yes,9/27/18,University of Amsterdam;University of Amsterdam;University of Amsterdam,169;169;169,59;59;59,,3/22/18,86,34,35,3,31,19,661;1199;26464,28;43;269,11;16;58,63;196;5073,-1;-1
1840,ICLR,2019,Minimal Images in Deep Neural Networks: Fragile Object Recognition in Natural Images,Sanjana Srivastava;Guy Ben-Yosef;Xavier Boix,sanjanas@mit.edu;gby@csail.mit.edu;xboix@mit.edu,7;7;6,4;4;4,Accept (Poster),0,5,0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2,5;5;5,4,9/27/18,2,0,0,0,3,1,13;108;1407,4;26;50,2;6;15,2;7;221,-1;-1
1841,ICLR,2019,Detecting Egregious Responses in Neural Sequence-to-sequence Models,Tianxing He;James Glass,tianxing@mit.edu;glass@mit.edu,7;7;8,4;3;2,Accept (Poster),0,8,0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2,5;5,4,9/11/18,14,7,2,1,7,1,141;10705,18;342,5;54,20;964,-1;-1
1842,ICLR,2019,Analyzing Inverse Problems with Invertible Neural Networks,Lynton Ardizzone;Jakob Kruse;Carsten Rother;Ullrich Köthe,lynton.ardizzone@iwr.uni-heidelberg.de;jakob.kruse@iwr.uni-heidelberg.de;carsten.rother@iwr.uni-heidelberg.de;ullrich.koethe@iwr.uni-heidelberg.de,7;7;6,5;2;3,Accept (Poster),2,16,0,yes,9/27/18,Heidelberg University;Heidelberg University;Heidelberg University;Heidelberg University,199;199;199;199,45;45;45;45,1,8/14/18,67,43,24,2,70,10,104;116;351;2444,13;12;33;98,4;4;6;23,17;20;36;209,-1;-1
1843,ICLR,2019,Initialized Equilibrium Propagation for Backprop-Free Training,Peter O'Connor;Efstratios Gavves;Max Welling,peter.ed.oconnor@gmail.com;egavves@uva.nl;m.welling@uva.nl,7;5;8,5;4;5,Accept (Poster),0,10,1,yes,9/27/18,;University of Amsterdam;University of Amsterdam,-1;169;169,-1;59;59,,9/27/18,3,1,1,0,0,0,1725;3268;26464,81;72;269,17;26;58,132;467;5073,-1;-1
1844,ICLR,2019,L2-Nonexpansive Neural Networks,Haifeng Qian;Mark N. Wegman,qianhaifeng@us.ibm.com;wegman@us.ibm.com,8;5;6,4;3;4,Accept (Poster),13,49,0,yes,9/27/18,International Business Machines;International Business Machines,-1;-1,-1;-1,4;8,2/22/18,29,17,9,4,17,5,5143;8058,361;68,35;25,400;869,-1;-1
1845,ICLR,2019,KnockoffGAN: Generating Knockoffs for Feature Selection using Generative Adversarial Networks,James Jordon;Jinsung Yoon;Mihaela van der Schaar,james.jordon@wolfson.ox.ac.uk;jsyoon0823@gmail.com;mihaela.vanderschaar@eng.ox.ac.uk,6;10;7,4;4;4,Accept (Oral),0,4,1,yes,9/27/18,"University of Oxford;University of California, Los Angeles;University of Oxford",50;20;50,1;15;1,5;4,9/27/18,11,4,4,1,0,1,240;536;8607,17;55;643,8;13;42,43;57;537,-1;-1
1846,ICLR,2019,INVASE: Instance-wise Variable Selection using Neural Networks,Jinsung Yoon;James Jordon;Mihaela van der Schaar,jsyoon0823@gmail.com;james.jordon@wolfson.ox.ac.uk;mihaela.vanderschaar@eng.ox.ac.uk,6;6;6,3;4;3,Accept (Poster),0,6,0,yes,9/27/18,"University of California, Los Angeles;University of Oxford;University of Oxford",20;50;50,15;1;1,,9/27/18,10,4,7,1,0,3,536;240;8607,55;17;643,13;8;42,57;43;537,-1;-1
1847,ICLR,2019,Gradient Descent Provably Optimizes Over-parameterized Neural Networks,Simon S. Du;Xiyu Zhai;Barnabas Poczos;Aarti Singh,ssdu@cs.cmu.edu;xiyuzhai@mit.edu;bapoczos@cs.cmu.edu;aartisingh@cmu.edu,8;8;3;7,4;4;5;4,Accept (Poster),18,18,1,yes,9/27/18,Carnegie Mellon University;Massachusetts Institute of Technology;Carnegie Mellon University;Carnegie Mellon University,1;2;1;1,24;5;24;24,1;9,9/27/18,333,203,65,29,196,57,1989;726;5679;2850,55;12;244;167,19;8;40;27,302;122;699;295,-1;-1
1848,ICLR,2019,Phase-Aware Speech Enhancement with Deep Complex U-Net,Hyeong-Seok Choi;Jang-Hyun Kim;Jaesung Huh;Adrian Kim;Jung-Woo Ha;Kyogu Lee,kekepa15@snu.ac.kr;blue378@snu.ac.kr;jaesung.huh@navercorp.com;adrian.kim@navercorp.com;jungwoo.ha@navercorp.com;kglee@snu.ac.kr,6;7;7,4;4;4,Accept (Poster),0,5,2,yes,9/27/18,Seoul National University;Seoul National University;NAVER;NAVER;NAVER;Seoul National University,41;41;-1;-1;-1;41,74;74;-1;-1;-1;74,,9/27/18,17,11,7,0,0,5,35;160;26;42;1907;1287,11;52;8;15;67;120,2;5;3;3;15;20,8;7;5;6;368;111,-1;-1
1849,ICLR,2019,Context-adaptive Entropy Model for End-to-end Optimized Image Compression,Jooyoung Lee;Seunghyun Cho;Seung-Kwon Beack,leejy1003@etri.re.kr;shcho@etri.re.kr;skbeack@etri.re.kr,7;7;6,5;4;3,Accept (Poster),3,9,0,yes,9/27/18,Electronics and Telecommunications Research Institute;Electronics and Telecommunications Research Institute;Electronics and Telecommunications Research Institute,-1;-1;-1,-1;-1;-1,,9/27/18,41,20,20,3,0,9,1479;60;39,165;18;4,19;3;1,182;8;8,-1;-1
1850,ICLR,2019,Beyond Pixel Norm-Balls: Parametric Adversaries using an Analytically Differentiable Renderer,Hsueh-Ti Derek Liu;Michael Tao;Chun-Liang Li;Derek Nowrouzezahrai;Alec Jacobson,hsuehtil@cs.toronto.edu;mtao@dgp.toronto.edu;chunlial@cs.cmu.edu;derek@cim.mcgill.ca;jacobson@cs.toronto.edu,7;7;6,4;4;3,Accept (Poster),0,8,0,yes,9/27/18,"Department of Computer Science, University of Toronto;University of Toronto;Carnegie Mellon University;McGill University;Department of Computer Science, University of Toronto",18;18;1;85;18,22;22;24;42;22,4,8/8/18,21,11,9,1,0,1,67;106;1203;1929;128,9;14;89;111;21,5;6;17;25;6,5;8;112;110;6,-1;-1
1851,ICLR,2019,Multilingual Neural Machine Translation With Soft Decoupled Encoding,Xinyi Wang;Hieu Pham;Philip Arthur;Graham Neubig,xinyiw1@cs.cmu.edu;hyhieu@cmu.edu;philip.arthur@monash.edu;gneubig@cs.cmu.edu,6;7;6,5;4;4,Accept (Poster),0,8,0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;Monash University;Carnegie Mellon University,1;1;123;1,24;24;80;24,3,9/27/18,16,9,5,2,4,3,187;4985;195;5424,25;18;17;441,7;11;6;39,16;765;10;569,-1;-1
1852,ICLR,2019,Unsupervised Speech Recognition via Segmental Empirical Output Distribution Matching,Chih-Kuan Yeh;Jianshu Chen;Chengzhu Yu;Dong Yu,cjyeh@cs.cmu.edu;chenjianshu@gmail.com;czyu@tencent.com;dyu@tencent.com,7;7;7,4;4;4,Accept (Poster),0,5,0,yes,9/27/18,Carnegie Mellon University;Tencent AI Lab;Tencent AI Lab;Tencent AI Lab,1;-1;-1;-1,24;-1;-1;-1,3;2,9/27/18,6,3,2,0,0,2,269;2810;616;24612,21;107;40;289,7;26;11;61,42;224;47;1802,-1;-1
1853,ICLR,2019,Understanding Composition of Word Embeddings via Tensor Decomposition,Abraham Frandsen;Rong Ge,abef@cs.duke.edu;rongge@cs.duke.edu,7;6;6,2;4;3,Accept (Poster),0,4,0,yes,9/27/18,Duke University;Duke University,44;44,17;17,3;5;1,9/27/18,2,2,0,0,0,0,9;5730,6;78,3;32,0;800,-1;-1
1854,ICLR,2019,Neural Message Passing for Multi-Label Classification,Jack Lanchantin;Arshdeep Sekhon;Yanjun Qi,jjl5sw@virginia.edu;as5cu@virginia.edu;yq2h@virginia.edu,4;6;5,4;4;2,Reject,0,17,0,yes,9/27/18,University of Virginia;University of Virginia;University of Virginia,65;65;65,113;113;113,,9/27/18,3,2,1,0,0,0,896;44;3076,24;12;114,8;3;25,33;2;261,-1;-1
1855,ICLR,2019,ARM: Augment-REINFORCE-Merge Gradient for Stochastic Binary Networks,Mingzhang Yin;Mingyuan Zhou,mzyin@utexas.edu;mingyuan.zhou@mccombs.utexas.edu,8;6;7,4;4;3,Accept (Poster),0,10,1,yes,9/27/18,"University of Texas, Austin;University of Texas, Austin",22;22,49;49,,9/27/18,21,11,13,1,0,5,107;1965,13;115,6;24,16;232,-1;-1
1856,ICLR,2019,An Empirical Study of Example Forgetting during Deep Neural Network Learning,Mariya Toneva*;Alessandro Sordoni*;Remi Tachet des Combes*;Adam Trischler;Yoshua Bengio;Geoffrey J. Gordon,mariya.k.toneva@gmail.com;alsordon@microsoft.com;retachet@microsoft.com;adtrisch@microsoft.com;yoshua.bengio@mila.quebec;geoff.gordon@microsoft.com,7;8;9,4;4;5,Accept (Poster),0,9,1,yes,9/27/18,Carnegie Mellon University;Microsoft;Microsoft;Microsoft;University of Montreal;Microsoft,1;-1;-1;-1;123;-1,24;-1;-1;-1;108;-1,8,9/27/18,36,26,11,3,8,5,257;3811;220;1547;201105;10541,15;54;14;47;807;186,5;19;8;17;147;48,16;541;20;287;23941;1211,-1;-1
1857,ICLR,2019,Distribution-Interpolation Trade off in Generative Models,Damian Leśniak;Igor Sieradzki;Igor Podolak,damian.lesniak@doctoral.uj.edu.pl;igor.sieradzki@doctoral.uj.edu.pl;igor.podolak@uj.edu.pl,6;5;7,4;4;3,Accept (Poster),0,9,0,yes,9/27/18,Jagiellonian University;Jagiellonian University;Jagiellonian University,478;478;478,695;695;695,5;1,9/27/18,5,5,1,1,0,1,91;10;145,24;7;37,3;2;6,6;1;9,-1;-1
1858,ICLR,2019,Practical lossless compression with latent variables using bits back coding,James Townsend;Thomas Bird;David Barber,james.townsend@cs.ucl.ac.uk;thomas.bird@cs.ucl.ac.uk;david.barber@ucl.ac.uk,6;6;8,4;3;5,Accept (Poster),0,7,1,yes,9/27/18,University College London;University College London;University College London,50;50;50,16;16;16,5,9/27/18,15,8,9,0,6,3,114;326;3779,9;14;200,5;7;27,7;22;405,-1;-1
1859,ICLR,2019,Double Viterbi: Weight Encoding for High Compression Ratio and Fast On-Chip Reconstruction for Deep Neural Network,Daehyun Ahn;Dongsoo Lee;Taesu Kim;Jae-Joon Kim,daehyun.ahn@postech.ac.kr;dslee3@gmail.com;taesukim@postech.ac.kr;jaejoon@postech.ac.kr,6;7;6,3;2;4,Accept (Poster),0,13,0,yes,9/27/18,POSTECH;Samsung;POSTECH;POSTECH,123;-1;123;123,137;-1;137;137,,9/27/18,5,5,2,0,0,2,20;330;1903;1345,7;33;106;112,3;10;20;17,7;39;173;114,-1;-1
1860,ICLR,2019,Zero-training Sentence Embedding via Orthogonal Basis,Ziyi Yang;Chenguang Zhu;Weizhu Chen,ziyi.yang@stanford.edu;chezhu@microsoft.com;wzchen@microsoft.com,5;4;5,4;4;4,Reject,6,6,0,yes,9/27/18,Stanford University;Microsoft;Microsoft,4;-1;-1,3;-1;-1,3,9/27/18,9,3,5,0,2,3,409;841;1988,42;47;87,10;11;21,16;95;245,-1;-1
1861,ICLR,2019,Guiding Policies with Language via Meta-Learning,John D. Co-Reyes;Abhishek Gupta;Suvansh Sanjeev;Nick Altieri;Jacob Andreas;John DeNero;Pieter Abbeel;Sergey Levine,jcoreyes@eecs.berkeley.edu;abhigupta@berkeley.edu;suvansh@berkeley.edu;naltieri@berkeley.edu;j.d.andreas@gmail.com;denero@berkeley.edu;pabbeel@cs.berkeley.edu;svlevine@eecs.berkeley.edu,6;6;6,4;4;3,Accept (Poster),0,7,0,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;Microsoft;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5;-1;5;5;5,18;18;18;18;-1;18;18;18,3;6,9/27/18,13,8,2,1,35,1,1395;1383;13;17;2611;134;36452;24235,52;89;2;3;42;8;433;309,20;18;1;2;25;4;94;73,172;141;1;1;347;17;4390;3140,-1;-1
1862,ICLR,2019,Towards the first adversarially robust neural network model on MNIST,Lukas Schott;Jonas Rauber;Matthias Bethge;Wieland Brendel,lukas.schott@bethgelab.org;jonas.rauber@bethgelab.org;matthias.bethge@bethgelab.org;wieland.brendel@bethgelab.org,7;7;6,4;3;3,Accept (Poster),7,18,1,yes,9/27/18,"Centre for Integrative Neuroscience, AG Bethge;Centre for Integrative Neuroscience, AG Bethge;Centre for Integrative Neuroscience, AG Bethge;Centre for Integrative Neuroscience, AG Bethge",153;153;153;153,94;94;94;94,4;2,5/23/18,123,63,37,8,173,21,345;1212;11274;1729,9;17;414;39,5;11;45;16,42;138;1251;211,-1;-1
1863,ICLR,2019,Neural network gradient-based learning of black-box function interfaces,Alon Jacovi;Guy Hadash;Einat Kermany;Boaz Carmeli;Ofer Lavi;George Kour;Jonathan Berant,alon.jacovi@il.ibm.com;guyh@il.ibm.com;einatke@il.ibm.com;boazc@il.ibm.com;oferl@il.ibm.com;gkour@ibm.com;joberant@cs.tau.ac.il,7;7;7,3;3;3,Accept (Poster),0,5,0,yes,9/27/18,International Business Machines;International Business Machines;International Business Machines;International Business Machines;International Business Machines;International Business Machines;Tel Aviv University,-1;-1;-1;-1;-1;-1;37,-1;-1;-1;-1;-1;-1;217,,9/27/18,5,2,0,0,32,0,73;57;355;285;390;37;3137,8;5;11;35;16;10;77,4;4;6;7;7;4;25,2;1;36;21;39;2;409,-1;-1
1864,ICLR,2019,Learning Multimodal Graph-to-Graph Translation for Molecule Optimization,Wengong Jin;Kevin Yang;Regina Barzilay;Tommi Jaakkola,wengong@csail.mit.edu;yangk@mit.edu;regina@csail.mit.edu;tommi@csail.mit.edu,7;7;6,5;4;4,Accept (Poster),0,9,0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2,5;5;5;5,4;10,9/27/18,36,12,9,0,11,1,514;142;11910;21902,24;28;233;292,7;5;55;69,61;6;1212;2317,-1;-1
1865,ICLR,2019,Lagging Inference Networks and Posterior Collapse in Variational Autoencoders,Junxian He;Daniel Spokoyny;Graham Neubig;Taylor Berg-Kirkpatrick,junxianh@cs.cmu.edu;dspokoyn@cs.cmu.edu;gneubig@cs.cmu.edu;tberg@cs.cmu.edu,8;7;8,4;4;4,Accept (Poster),3,12,2,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1,24;24;24;24,5;1,9/27/18,70,46,21,3,0,18,230;89;5424;1950,14;5;441;48,8;2;39;20,43;19;569;341,-1;-1
1866,ICLR,2019,Stable Opponent Shaping in Differentiable Games,Alistair Letcher;Jakob Foerster;David Balduzzi;Tim Rocktäschel;Shimon Whiteson,ahp.letcher@gmail.com;jakobfoerster@gmail.com;dbalduzzi@google.com;tim.rocktaeschel@gmail.com;shimon.whiteson@cs.ox.ac.uk,8;6;6,4;2;1,Accept (Poster),0,7,0,yes,9/27/18,University of Oxford;University of Oxford;Google;Facebook AI Research;University of Oxford,50;50;-1;-1;50,1;1;-1;-1;1,5;1,9/27/18,21,17,2,1,41,5,35;2049;2435;2243;5288,4;57;62;48;203,2;19;21;21;38,6;331;352;278;573,-1;-1
1867,ICLR,2019,DELTA: DEEP LEARNING TRANSFER USING FEATURE MAP WITH ATTENTION FOR CONVOLUTIONAL NETWORKS,Xingjian Li;Haoyi Xiong;Hanchao Wang;Yuxuan Rao;Liping Liu;Jun Huan,1762778193@qq.com;xhyccc@gmail.com;wanghanchao01@baidu.com;yrao4@illinois.edu;liuliping@baidu.com;huanjun@baidu.com,6;7;6,4;3;4,Accept (Poster),0,9,0,yes,9/27/18,";Baidu;Baidu;University of Illinois, Urbana Champaign;Baidu;Baidu",-1;-1;-1;3;-1;-1,-1;-1;-1;37;-1;-1,6,9/27/18,17,10,9,1,2,4,91;1473;107;21;2302;2630,14;91;42;3;214;194,5;21;6;2;24;23,13;76;17;4;157;240,-1;-1
1868,ICLR,2019,CAMOU: Learning Physical Vehicle Camouflages to Adversarially Attack Detectors in the Wild,Yang Zhang;Hassan Foroosh;Philip David;Boqing Gong,yangzhang4065@gmail.com;foroosh@cs.ucf.edu;philip.j.david4.civ@mail.mil;boqinggo@outlook.com,4;8;7,3;4;3,Accept (Poster),0,4,4,yes,9/27/18,University of Central Florida;University of Central Florida;Army Reserach laboratory;International Computer Science Institute,78;78;-1;-1,1103;1103;-1;-1,4,9/27/18,9,5,2,0,0,2,314;3388;21;3759,80;173;13;85,10;26;3;24,18;177;4;751,-1;-1
1869,ICLR,2019,Visual Reasoning by Progressive Module Networks,Seung Wook Kim;Makarand Tapaswi;Sanja Fidler,seung@cs.toronto.edu;makarand@cs.toronto.edu;fidler@cs.toronto.edu,6;6;7,4;4;5,Accept (Poster),0,12,0,yes,9/27/18,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",18;18;18,22;22;22,,6/6/18,4,2,0,0,2,0,2358;979;10491,360;46;160,23;16;48,149;125;1358,-1;-1
1870,ICLR,2019,Recurrent Experience Replay in Distributed Reinforcement Learning,Steven Kapturowski;Georg Ostrovski;John Quan;Remi Munos;Will Dabney,skapturowski@google.com;ostrovski@google.com;johnquan@google.com;munos@google.com;wdabney@google.com,7;8;7,2;4;3,Accept (Poster),7,11,0,yes,9/27/18,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,,9/27/18,82,47,36,6,0,22,97;10659;3225;9303;1733,8;16;16;190;25,4;10;11;53;13,23;1852;542;1313;331,-1;-1
1871,ICLR,2019,Dynamic Sparse Graph for Efficient Deep Learning,Liu Liu;Lei Deng;Xing Hu;Maohua Zhu;Guoqi Li;Yufei Ding;Yuan Xie,liu_liu@ucsb.edu;leideng@ucsb.edu;huxing@ece.ucsb.edu;maohuazhu@ucsb.edu;liguoqi@mail.tsinghua.edu.cn;yufeiding@cs.ucsb.edu;yuanxie@ucsb.edu,7;8;7,2;3;4,Accept (Poster),0,4,0,yes,9/27/18,UC Santa Barbara;UC Santa Barbara;UC Santa Barbara;UC Santa Barbara;Tsinghua University;UC Santa Barbara;UC Santa Barbara,37;37;37;37;8;37;37,53;53;53;53;30;53;53,10,9/27/18,10,4,4,1,3,2,1763;1560;515;105;1118;344;503,155;180;113;14;161;43;51,17;21;11;7;16;9;10,101;92;50;9;64;31;49,-1;-1
1872,ICLR,2019,Learning Recurrent Binary/Ternary Weights,Arash Ardakani;Zhengyun Ji;Sean C. Smithson;Brett H. Meyer;Warren J. Gross,arash.ardakani@mail.mcgill.ca;zhengyun.ji@mail.mcgill.ca;sean.smithson@mail.mcgill.ca;brett.meyer@mcgill.ca;warren.gross@mcgill.ca,7;6;8,4;3;3,Accept (Poster),0,11,0,yes,9/27/18,McGill University;McGill University;McGill University;McGill University;McGill University,85;85;85;85;85,42;42;42;42;42,3,9/27/18,10,7,4,0,11,2,161;14;108;490;4223,19;3;11;66;253,8;2;6;12;31,11;2;10;35;348,-1;-1
1873,ICLR,2019,Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning,Ying Wen;Yaodong Yang;Rui Luo;Jun Wang;Wei Pan,ying.wen@cs.ucl.ac.uk;yaodong.yang@cs.ucl.ac.uk;rui.luo@cs.ucl.ac.uk;jun.wang@cs.ucl.ac.uk;wei.pan@tudelft.nl,7;8;7,4;3;4,Accept (Poster),0,10,0,yes,9/27/18,University College London;University College London;University College London;University College London;Delft University of Technology,50;50;50;50;89,16;16;16;16;63,,9/27/18,24,12,2,1,7,1,922;491;1025;11309;11952,87;54;82;1061;727,12;8;14;43;51,87;44;115;845;848,-1;-1
1874,ICLR,2019,"Slalom: Fast, Verifiable and Private Execution of Neural Networks in Trusted Hardware",Florian Tramer;Dan Boneh,tramer@cs.stanford.edu;dabo@cs.stanford.edu,7;7;9,3;2;4,Accept (Oral),0,4,10,yes,9/27/18,Stanford University;Stanford University,4;4,3;3,,6/8/18,50,29,22,1,13,7,2330;50589,36;414,18;98,321;6296,-1;-1
1875,ICLR,2019,"A comprehensive, application-oriented study of catastrophic forgetting in DNNs",B. Pfülb;A. Gepperth,benedikt.pfuelb@cs.hs-fulda.de;alexander.gepperth@cs.hs-fulda.de,6;7;5,5;3;4,Accept (Poster),0,1,0,yes,9/27/18,HS Fulda;HS Fulda,-1;-1,-1;-1,,9/27/18,17,6,3,0,0,0,27;493,5;70,2;11,2;27,-1;-1
1876,ICLR,2019,Stable Recurrent Models,John Miller;Moritz Hardt,miller_john@berkeley.edu;hardt@berkeley.edu,6;7;6,4;2;4,Accept (Poster),0,10,2,yes,9/27/18,University of California Berkeley;University of California Berkeley,5;5,18;18,1,5/25/18,25,15,6,1,0,6,309;7643,68;88,11;33,38;949,-1;-1
1877,ICLR,2019,Large Scale Graph Learning From Smooth Signals,Vassilis Kalofolias;Nathanaël Perraudin,v.kalofolias@gmail.com;nathanael.perraudin@sdsc.ethz.ch,7;5;7,5;3;4,Accept (Poster),0,7,0,yes,9/27/18,;Swiss Federal Institute of Technology,-1;10,-1;10,10,10/16/17,11,6,5,2,3,3,605;709,11;38,9;12,80;71,-1;-1
1878,ICLR,2019,Accumulation Bit-Width Scaling For Ultra-Low Precision Training Of Deep Networks,Charbel Sakr;Naigang Wang;Chia-Yu Chen;Jungwook Choi;Ankur Agrawal;Naresh Shanbhag;Kailash Gopalakrishnan,sakr2@illinois.edu;nwang@us.ibm.com;cchen@us.ibm.com;choij@us.ibm.com;ankuragr@us.ibm.com;shanbhag@illinois.edu;kailash@us.ibm.com,7;6;6,4;4;3,Accept (Poster),0,7,0,yes,9/27/18,"University of Illinois, Urbana Champaign;International Business Machines;International Business Machines;International Business Machines;International Business Machines;University of Illinois, Urbana Champaign;International Business Machines",3;-1;-1;-1;-1;3;-1,37;-1;-1;-1;-1;37;-1,,9/27/18,6,2,3,0,4,1,140;873;539;990;1403;5288;2308,18;49;33;99;55;299;45,7;15;9;18;12;38;14,8;59;51;89;90;526;214,-1;-1
1879,ICLR,2019,Learning to Remember More with Less Memorization,Hung Le;Truyen Tran;Svetha Venkatesh,lethai@deakin.edu.au;truyen.tran@deakin.edu.au;svetha.venkatesh@deakin.edu.au,7;7;8,3;4;4,Accept (Oral),0,5,3,yes,9/27/18,Deakin University;Deakin University;Deakin University,478;478;478,334;334;334,1,9/27/18,10,5,2,0,3,0,730;1857;8523,49;133;561,10;24;43,90;124;540,-1;-1
1880,ICLR,2019,Bayesian Prediction of Future Street Scenes using Synthetic Likelihoods,Apratim Bhattacharyya;Mario Fritz;Bernt Schiele,abhattac@mpi-inf.mpg.de;mfritz@mpi-inf.mpg.de;schiele@mpi-inf.mpg.de,6;8;6,4;4;2,Accept (Poster),0,8,1,yes,9/27/18,"Saarland Informatics Campus, Max-Planck Institute;Saarland Informatics Campus, Max-Planck Institute;Saarland Informatics Campus, Max-Planck Institute",-1;-1;-1,-1;-1;-1,11,9/27/18,15,3,6,0,3,1,148;7610;40491,18;196;501,7;45;98,17;1001;5067,-1;-1
1881,ICLR,2019,Label super-resolution networks,Kolya Malkin;Caleb Robinson;Le Hou;Rachel Soobitsky;Jacob Czawlytko;Dimitris Samaras;Joel Saltz;Lucas Joppa;Nebojsa Jojic,kolya_malkin@hotmail.com;dcrobins@gatech.edu;le.hou@stonybrook.edu;rsoobitsky@chesapeakeconservancy.org;jczawlytko@chesapeakeconservancy.org;samaras@cs.stonybrook.edu;joel.saltz@stonybrookmedicine.edu;lujoppa@microsoft.com;jojic@microsoft.com,7;6;9,4;4;4,Accept (Poster),0,5,0,yes,9/27/18,"Yale University;Georgia Institute of Technology;State University of New York, Stony Brook;;;State University of New York, Stony Brook;Renaissance School of Medicine at Stony Brook University;Microsoft;Microsoft",62;13;41;-1;-1;41;41;-1;-1,12;33;258;-1;-1;258;258;-1;-1,2,9/27/18,6,3,0,1,0,0,13;175;238;12;12;4952;11782;6094;250,5;17;22;3;2;174;579;132;31,2;6;6;2;2;38;52;33;8,1;7;6;1;1;381;640;371;10,-1;-1
1882,ICLR,2019,Variational Smoothing in Recurrent Neural Network Language Models,Lingpeng Kong;Gabor Melis;Wang Ling;Lei Yu;Dani Yogatama,lingpenk@cs.cmu.edu;melisgl@google.com;lingwang@google.com;leiyu@google.com;dyogatama@google.com,7;6;2,4;4;5,Accept (Poster),0,3,0,yes,9/27/18,Carnegie Mellon University;Google;Google;Google;Google,1;-1;-1;-1;-1,24;-1;-1;-1;-1,3;11,9/27/18,1,0,1,0,0,1,1047;600;2547;2175;3499,31;14;756;281;41,14;8;21;23;21,106;97;266;136;402,-1;-1
1883,ICLR,2019,Hierarchical interpretations for neural network predictions,Chandan Singh;W. James Murdoch;Bin Yu,chandan_singh@berkeley.edu;jmurdoch@berkeley.edu;binyu@berkeley.edu,6;6;7,4;4;3,Accept (Poster),0,10,0,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,18;18;18,4,6/14/18,29,16,12,0,38,6,306;348;5268,29;10;268,6;6;35,23;29;380,-1;-1
1884,ICLR,2019,h-detach: Modifying the LSTM Gradient Towards Better Optimization,Bhargav Kanuparthi;Devansh Arpit;Giancarlo Kerg;Nan Rosemary Ke;Ioannis Mitliagkas;Yoshua Bengio,bhargavkanuparthi25@gmail.com;devansharpit@gmail.com;giancarlo.kerg@gmail.com;rosemary.nan.ke@gmail.com;ioannis@iro.umontreal.ca;yoshua.umontreal@gmail.com,7;5;6,5;4;3,Accept (Poster),0,4,5,yes,9/27/18,University of Montreal;University of Montreal;University of Montreal;Polytechnique Montreal;University of Montreal;University of Montreal,123;123;123;386;123;123,108;108;108;108;108;108,10;8,9/27/18,7,4,4,1,17,2,7;880;15;744;1164;201105,4;42;5;32;43;807,1;12;2;13;18;147,2;114;3;76;189;23941,-1;-1
1885,ICLR,2019,Neural Probabilistic Motor Primitives for Humanoid Control,Josh Merel;Leonard Hasenclever;Alexandre Galashov;Arun Ahuja;Vu Pham;Greg Wayne;Yee Whye Teh;Nicolas Heess,jsmerel@google.com;leonardh@google.com;agalashov@google.com;arahuja@google.com;vuph@google.com;gregwayne@google.com;ywteh@google.com;heess@google.com,3;6;4,4;4;4,Accept (Poster),0,8,0,yes,9/27/18,Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1,,9/27/18,22,16,7,2,114,0,1629;316;78;726;193;2562;23048;11329,29;17;7;37;21;32;249;104,15;10;5;11;7;15;51;37,127;25;2;49;15;260;3207;1616,-1;-1
1886,ICLR,2019,Hierarchical Visuomotor Control of Humanoids,Josh Merel;Arun Ahuja;Vu Pham;Saran Tunyasuvunakool;Siqi Liu;Dhruva Tirumala;Nicolas Heess;Greg Wayne,jsmerel@google.com;arahuja@google.com;vuph@google.com;stunya@google.com;liusiqi@google.com;dhruvat@google.com;heess@google.com;gregwayne@google.com,5;8;6,3;3;4,Accept (Poster),2,9,0,yes,9/27/18,Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1,,9/27/18,25,15,9,2,171,1,1629;726;193;190;4766;517;11329;2562,29;37;21;7;265;10;104;32,15;11;7;4;33;6;37;15,127;49;15;9;197;59;1616;260,-1;-1
1887,ICLR,2019,Adversarial Domain Adaptation for Stable Brain-Machine Interfaces,Ali Farshchian;Juan A. Gallego;Joseph P. Cohen;Yoshua Bengio;Lee E. Miller;Sara A. Solla,a-farshchiansadegh@northwestern.edu;juan.gallego@northwestern.edu;joseph@josephpcohen.com;yoshua.bengio@umontreal.ca;lm@northwestern.edu;solla@northwestern.edu,9;5;7,4;3;5,Accept (Poster),2,8,0,yes,9/27/18,Northwestern University;Northwestern University;University of Montreal;University of Montreal;Northwestern University;Northwestern University,44;44;123;123;44;44,20;20;108;108;20;20,4,9/27/18,8,4,3,0,36,2,28;569;479;201105;2856;4194,5;42;61;807;132;90,3;14;10;147;32;27,3;27;49;23941;185;274,-1;-1
1888,ICLR,2019,Emerging Disentanglement in Auto-Encoder Based Unsupervised Image Content Transfer,Ori Press;Tomer Galanti;Sagie Benaim;Lior Wolf,theoripress@gmail.com;tomer22g@gmail.com;sagiebenaim@gmail.com;wolf@fb.com,6;6;6,2;3;1,Accept (Poster),0,6,0,yes,9/27/18,Tel Aviv University;Tel Aviv University;Tel Aviv University;Facebook,37;37;37;-1,217;217;217;-1,,9/27/18,11,8,4,2,0,0,11;70;168;13809,1;18;15;199,1;5;5;45,0;1;26;1636,-1;-1
1889,ICLR,2019,Global-to-local Memory Pointer Networks for Task-Oriented Dialogue,Chien-Sheng Wu;Richard Socher;Caiming Xiong,jason.wu@connect.ust.hk;rsocher@salesforce.com;cxiong@salesforce.com,8;8;5,2;2;3,Accept (Poster),0,0,10,yes,9/27/18,The Hong Kong University of Science and Technology;SalesForce.com;SalesForce.com,39;-1;-1,44;-1;-1,,9/27/18,27,17,10,0,4,4,395;52263;6210,30;180;156,10;49;31,49;8807;1052,-1;-1
1890,ICLR,2019,"Improving Differentiable Neural Computers Through Memory Masking, De-allocation, and Link Distribution Sharpness Control",Robert Csordas;Juergen Schmidhuber,robert@idsia.ch;juergen@idsia.ch,7;8;7,5;5;5,Accept (Poster),0,5,0,yes,9/27/18,IDSIA;IDSIA,-1;-1,-1;-1,,9/27/18,3,1,0,0,3,0,7;64399,4;347,2;75,0;8263,-1;-1
1891,ICLR,2019,Learning Representations of Sets through Optimized Permutations,Yan Zhang;Jonathon Hare;Adam Prügel-Bennett,yz5n12@ecs.soton.ac.uk;jsh2@ecs.soton.ac.uk;apb@ecs.soton.ac.uk,3;6;6,2;4;4,Accept (Poster),0,25,0,yes,9/27/18,University of Southampton;University of Southampton;University of Southampton,169;169;169,126;126;126,,9/27/18,6,4,2,1,0,2,3488;1603;2097,699;117;137,28;22;25,203;149;135,-1;-1
1892,ICLR,2019,Kernel Change-point Detection with Auxiliary Deep Generative Models,Wei-Cheng Chang;Chun-Liang Li;Yiming Yang;Barnabás Póczos,wchang2@cs.cmu.edu;chunlial@cs.cmu.edu;yiming@cs.cmu.edu;bapoczos@cs.cmu.edu,7;8;8,4;3;4,Accept (Poster),0,5,0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1,24;24;24;24,5;1,9/27/18,8,2,5,0,6,0,1734;1203;21147;5679,43;89;272;244,17;17;49;40,195;112;2566;699,-1;-1
1893,ICLR,2019,Unsupervised Domain Adaptation for Distance Metric Learning,Kihyuk Sohn;Wenling Shang;Xiang Yu;Manmohan Chandraker,kihyuk.sohn@gmail.com;wendyshang1208@gmail.com;xiangyu@nec-labs.com;manu@nec-labs.com,8;5;8,5;4;4,Accept (Poster),0,5,0,yes,9/27/18,NEC-Labs;;NEC-Labs;NEC-Labs,-1;-1;-1;-1,-1;-1;-1;-1,7;2,9/27/18,15,10,2,0,0,2,3562;494;9867;3327,49;18;702;93,20;6;46;33,567;46;467;398,-1;-1
1894,ICLR,2019,Feature Intertwiner for Object Detection,Hongyang Li;Bo Dai;Shaoshuai Shi;Wanli Ouyang;Xiaogang Wang,yangli@ee.cuhk.edu.hk;db014@ie.cuhk.edu.hk;shaoss@link.cuhk.edu.hk;wanli.ouyang@gmail.com;xgwang@ee.cuhk.edu.hk,7;5;9,3;4;4,Accept (Poster),0,3,0,yes,9/27/18,The Chinese University of Hong Kong;The Chinese University of Hong Kong;The Chinese University of Hong Kong;University of Sydney;The Chinese University of Hong Kong,57;57;57;-1;57,40;40;40;-1;40,2,9/27/18,5,4,0,0,2,0,814;3479;858;10542;2483,59;397;70;149;136,15;30;14;52;21,65;275;85;1182;381,-1;-1
1895,ICLR,2019,On the Universal Approximability and Complexity Bounds of Quantized ReLU Neural Networks,Yukun Ding;Jinglan Liu;Jinjun Xiong;Yiyu Shi,yding5@nd.edu;jliu16@nd.edu;jinjun@us.ibm.com;yshi4@nd.edu,7;6;8,3;3;3,Accept (Poster),0,4,0,yes,9/27/18,University of Notre Dame;University of Notre Dame;International Business Machines;University of Notre Dame,115;115;-1;115,150;150;-1;150,1,2/10/18,6,3,0,0,27,0,121;76;1630;119,26;21;171;30,4;5;21;6,5;0;156;1,-1;-1
1896,ICLR,2019,SNAS: stochastic neural architecture search,Sirui Xie;Hehui Zheng;Chunxiao Liu;Liang Lin,xiesirui@sensetime.com;zhenghehui@sensetime.com;liuchunxiao@sensetime.com;linliang@ieee.org,7;6;7,4;4;4,Accept (Poster),2,19,1,yes,9/27/18,SenseTime Group Limited;SenseTime Group Limited;SenseTime Group Limited;SUN YAT-SEN UNIVERSITY,-1;-1;-1;478,-1;-1;-1;352,1,9/27/18,216,110,113,12,9,64,217;354;462;8625,7;9;48;294,2;4;10;49,62;69;80;889,-1;-1
1897,ICLR,2019,LayoutGAN: Generating Graphic Layouts with Wireframe Discriminators,Jianan Li;Jimei Yang;Aaron Hertzmann;Jianming Zhang;Tingfa Xu,lijianan15@gmail.com;jimyang@adobe.com;hertzman@adobe.com;jianmzha@adobe.com;ciom_xtf1@bit.edu.cn,7;7;6,3;4;4,Accept (Poster),0,6,0,yes,9/27/18,;Adobe Systems;Adobe Systems;Adobe Systems;BIT,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,5;4;10,9/27/18,22,12,8,1,0,2,1423;5379;13199;148;868,117;74;166;32;77,14;35;57;5;10,113;843;1347;8;80,-1;-1
1898,ICLR,2019,"Don't Settle for Average, Go for the Max: Fuzzy Sets and Max-Pooled Word Vectors",Vitalii Zhelezniak;Aleksandar Savkov;April Shen;Francesco Moramarco;Jack Flann;Nils Y. Hammerla,vitali.zhelezniak@babylonhealth.com;sasho.savkov@babylonhealth.com;april.shen@babylonhealth.com;francesco.moramarco@babylonhealth.com;jack.flann@babylonhealth.com;nils.hammerla@babylonhealth.com,8;8;5,3;4;3,Accept (Poster),3,12,1,yes,9/27/18,babylon health;babylon health;babylon health;babylon health;babylon health;babylon health,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,3,9/27/18,12,5,8,0,0,2,25;65;25;12;79;2290,9;15;7;1;4;43,3;5;3;1;3;19,2;4;2;2;3;214,-1;-1
1899,ICLR,2019,ADef: an Iterative Algorithm to Construct Adversarial Deformations,Rima Alaifari;Giovanni S. Alberti;Tandri Gauksson,rima.alaifari@sam.math.ethz.ch;alberti@dima.unige.it;tandrig@sam.math.ethz.ch,6;7;7,3;4;3,Accept (Poster),0,8,0,yes,9/27/18,Swiss Federal Institute of Technology;Università degli Studi di Genova;Swiss Federal Institute of Technology,10;-1;10,10;-1;10,4,4/1/18,27,12,4,0,15,1,135;252;28,15;40;2,7;10;1,5;9;1,-1;-1
1900,ICLR,2019,On the Turing Completeness of Modern Neural Network Architectures,Jorge Pérez;Javier Marinković;Pablo Barceló,jperez@dcc.uchile.cl;javier.marinkovic95@gmail.com;pbarcelo@dcc.uchile.cl,6;7;7,2;2;2,Accept (Poster),2,14,0,yes,9/27/18,Universidad de Chile;;Universidad de Chile,314;-1;314,660;-1;660,,9/27/18,12,10,3,2,2,2,50;12;121,14;2;21,4;1;6,2;2;18,-1;-1
1901,ICLR,2019,Spherical CNNs on Unstructured Grids,Chiyu Max Jiang;Jingwei Huang;Karthik Kashinath;Prabhat;Philip Marcus;Matthias Niessner,chiyu.jiang@berkeley.edu;jingweih@stanford.edu;kkashinath@lbl.gov;prabhat@lbl.gov;pmarcus@me.berkeley.edu;niessner@tum.de,7;6;7,3;3;5,Accept (Poster),2,3,0,yes,9/27/18,University of California Berkeley;Stanford University;Lawrence Berkeley National Lab;Lawrence Berkeley National Lab;University of California Berkeley;Technical University Munich,5;4;-1;-1;5;54,18;3;-1;-1;18;41,2,9/27/18,39,11,16,1,3,12,71;914;427;3065;2281;21351,13;59;88;151;39;140,4;17;12;27;10;38,12;92;31;184;233;1760,-1;-1
1902,ICLR,2019,Convolutional Neural Networks on Non-uniform Geometrical Signals Using Euclidean Spectral Transformation,Chiyu Max Jiang;Dequan Wang;Jingwei Huang;Philip Marcus;Matthias Niessner,chiyu.jiang@berkeley.edu;dqw@berkeley.edu;jingweih@stanford.edu;pmarcus@me.berkeley.edu;niessner@tum.de,5;7;4,3;4;3,Accept (Poster),0,3,1,yes,9/27/18,University of California Berkeley;University of California Berkeley;Stanford University;University of California Berkeley;Technical University Munich,5;5;4;5;54,18;18;3;18;41,10,9/27/18,7,3,2,0,7,0,71;981;914;2258;21583,13;23;59;39;140,4;11;17;10;39,12;193;92;232;1780,-1;-1
1903,ICLR,2019,Graph Wavelet Neural Network,Bingbing Xu;Huawei Shen;Qi Cao;Yunqi Qiu;Xueqi Cheng,xubingbing@ict.ac.cn;shenhuawei@ict.ac.cn;caoqi@ict.ac.cn;qiuyunqi@ict.ac.cn;cxq@ict.ac.cn,7;7;4,5;4;4,Accept (Poster),8,17,4,yes,9/27/18,"Institute of Computing Technology, Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences",62;62;62;62;62,1103;1103;1103;1103;1103,10,9/27/18,26,11,12,2,0,10,1035;1754;297;29;6944,92;99;77;5;395,20;22;7;2;42,57;165;23;11;758,-1;-1
1904,ICLR,2019,"A Closer Look at Deep Learning Heuristics: Learning rate restarts, Warmup and Distillation",Akhilesh Gotmare;Nitish Shirish Keskar;Caiming Xiong;Richard Socher,akhilesh.gotmare@epfl.ch;nkeskar@salesforce.com;cxiong@salesforce.com;rsocher@salesforce.com,4;7;6,4;5;4,Accept (Poster),0,5,0,yes,9/27/18,Swiss Federal Institute of Technology Lausanne;SalesForce.com;SalesForce.com;SalesForce.com,478;-1;-1;-1,38;-1;-1;-1,9,9/27/18,27,12,7,3,5,2,126;2137;6210;52263,6;28;156;180,5;13;31;49,7;329;1052;8807,-1;-1
1905,ICLR,2019,Revealing interpretable object representations from human behavior,Charles Y. Zheng;Francisco Pereira;Chris I. Baker;Martin N. Hebart,charles.zheng@nih.gov;francisco.pereira@nih.gov;bakerchris@mail.nih.gov;martin.hebart@nih.gov,5;7;7,4;4;4,Accept (Poster),0,6,0,yes,9/27/18,National Institutes of Health;National Institutes of Health;National Institutes of Health;National Institutes of Health,-1;-1;-1;-1,-1;-1;-1;-1,,9/27/18,5,2,1,0,18,1,47;2587;8929;1109,16;43;228;42,5;18;43;14,2;235;638;93,-1;-1
1906,ICLR,2019,Near-Optimal Representation Learning for Hierarchical Reinforcement Learning,Ofir Nachum;Shixiang Gu;Honglak Lee;Sergey Levine,ofirnachum@google.com;shanegu@google.com;honglak@google.com;svlevine@eecs.berkeley.edu,8;7;9,3;5;5,Accept (Poster),1,11,0,yes,9/27/18,Google;Google;Google;University of California Berkeley,-1;-1;-1;5,-1;-1;-1;18,1,9/27/18,42,24,11,0,19,7,1032;3765;23861;24235,41;39;166;309,15;21;60;73,152;467;2806;3140,-1;-1
1907,ICLR,2019,Quaternion Recurrent Neural Networks,Titouan Parcollet;Mirco Ravanelli;Mohamed Morchid;Georges Linarès;Chiheb Trabelsi;Renato De Mori;Yoshua Bengio,titouan.parcollet@alumni.univ-avignon.fr;mirco.ravanelli@gmail.com;mohamed.morchid@univ-avignon.fr;georges.linares@univ-avignon.fr;chiheb.trabelsi@polymtl.ca;rdemori@cs.mcgill.ca;yoshua.bengio@mila.quebec,7;7;8,5;5;4,Accept (Poster),0,8,0,yes,9/27/18,University of Avignon;University of Montreal;University of Avignon;University of Avignon;Polytechnique Montreal;McGill University;University of Montreal,199;123;199;199;386;85;123,267;108;267;267;108;42;108,,6/12/18,22,12,13,0,60,2,162;748;433;1212;275;3924;201105,22;37;79;198;11;303;807,7;17;11;15;4;23;147,11;70;15;52;42;202;23941,-1;-1
1908,ICLR,2019,Efficient Lifelong Learning with A-GEM,Arslan Chaudhry;Marc’Aurelio Ranzato;Marcus Rohrbach;Mohamed Elhoseiny,arslan.chaudhry@eng.ox.ac.uk;ranzato@fb.com;mrf@fb.com;elhoseiny@fb.com,7;7;6,4;4;4,Accept (Poster),0,0,7,yes,9/27/18,University of Oxford;Facebook;Facebook;Facebook,50;-1;-1;-1,1;-1;-1;-1,,9/27/18,95,54,40,8,2,22,298;20205;11319;1405,7;75;90;67,5;42;43;18,69;2056;1527;180,-1;-1
1909,ICLR,2019,Quasi-hyperbolic momentum and Adam for deep learning,Jerry Ma;Denis Yarats,maj@fb.com;denisy@fb.com,6;7;8,4;4;3,Accept (Poster),0,18,1,yes,9/27/18,Facebook;Facebook,-1;-1,-1;-1,,9/27/18,25,6,15,2,5,7,241;1629,7;12,4;7,47;249,-1;-1
1910,ICLR,2019,AdaShift: Decorrelation and Convergence of Adaptive Learning Rate Methods,Zhiming Zhou*;Qingru Zhang*;Guansong Lu;Hongwei Wang;Weinan Zhang;Yong Yu,heyohai@apex.sjtu.edu.cn;neverquit@sjtu.edu.cn;gslu@apex.sjtu.edu.cn;wanghongwei55@gmail.com;wnzhang@sjtu.edu.cn;yyu@apex.sjtu.edu.cn,6;6;9,4;4;4,Accept (Poster),3,5,5,yes,9/27/18,Shanghai Jiao Tong University;Shanghai Jiao Tong University;Shanghai Jiao Tong University;Shanghai Jiao Tong University;Shanghai Jiao Tong University;Shanghai Jiao Tong University,52;52;52;52;52;52,188;188;188;188;188;188,1;8,9/27/18,18,10,3,0,6,2,231;42;58;3234;4833;27700,29;11;5;405;205;1552,8;3;3;21;31;71,20;5;8;244;688;2744,-1;-1
1911,ICLR,2019,Diversity is All You Need: Learning Skills without a Reward Function,Benjamin Eysenbach;Abhishek Gupta;Julian Ibarz;Sergey Levine,beysenba@cs.cmu.edu;abhigupta@berkeley.edu;julianibarz@google.com;svlevine@eecs.berkeley.edu,8;7;7,4;3;4,Accept (Poster),0,3,0,yes,9/27/18,Carnegie Mellon University;University of California Berkeley;Google;University of California Berkeley,1;5;-1;5,24;18;-1;18,,2/16/18,182,114,58,8,0,32,347;1386;2860;24235,16;89;22;309,6;18;13;73,52;141;349;3140,-1;-1
1912,ICLR,2019,MAE: Mutual Posterior-Divergence Regularization for Variational AutoEncoders,Xuezhe Ma;Chunting Zhou;Eduard Hovy,xuezhem@cs.cmu.edu;ctzhou@cs.cmu.edu;ehovy@cs.cmu.edu,7;6;6,5;4;4,Accept (Poster),2,7,1,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1,24;24;24,5,9/27/18,9,5,4,1,2,1,1923;541;23510,37;20;580,14;9;76,243;61;2470,-1;-1
1913,ICLR,2019,Learning a SAT Solver from Single-Bit Supervision,"Daniel Selsam;Matthew Lamm;Benedikt B\{u}nz;Percy Liang;Leonardo de Moura;David L. Dill""",dselsam@cs.stanford.edu;mlamm@cs.stanford.edu;buenz@cs.stanford.edu;pliang@cs.stanford.edu;leonardo@microsoft.com;dill@cs.stanford.edu,7;7;7,3;4;3,Accept (Poster),1,14,0,yes,9/27/18,Stanford University;Stanford University;Stanford University;Stanford University;Microsoft;Stanford University,4;4;4;4;-1;4,3;3;3;3;-1;3,10,2/11/18,112,51,38,3,0,21,588;173;690;12558;284;21403,11;19;26;144;27;299,6;5;12;47;8;60,81;22;114;2038;34;2727,-1;-1
1914,ICLR,2019, Reasoning About Physical Interactions with Object-Oriented Prediction and Planning,Michael Janner;Sergey Levine;William T. Freeman;Joshua B. Tenenbaum;Chelsea Finn;Jiajun Wu,janner@berkeley.edu;svlevine@eecs.berkeley.edu;billf@mit.edu;jbt@mit.edu;cbfinn@eecs.berkeley.edu;jiajunwu@mit.edu,7;9;5,4;4;5,Accept (Poster),4,5,0,yes,9/27/18,University of California Berkeley;University of California Berkeley;Massachusetts Institute of Technology;Massachusetts Institute of Technology;University of California Berkeley;Massachusetts Institute of Technology,5;5;2;2;5;2,18;18;5;5;18;5,,9/27/18,41,27,11,0,4,2,191;26033;43345;30421;7689;3934,10;343;402;590;99;90,4;75;91;83;33;29,27;3357;4343;2658;1026;376,-1;-1
1915,ICLR,2019, The relativistic discriminator: a key element missing from standard GAN,Alexia Jolicoeur-Martineau,alexia.jolicoeur-martineau@mail.mcgill.ca,6;6;7,2;4;3,Accept (Poster),0,5,3,yes,9/27/18,McGill University,85,42,5;4,7/2/18,187,71,106,3,679,39,233,13,5,42,-1
1916,ICLR,2019,SOM-VAE: Interpretable Discrete Representation Learning on Time Series,Vincent Fortuin;Matthias Hüser;Francesco Locatello;Heiko Strathmann;Gunnar Rätsch,fortuin@inf.ethz.ch;mhueser@inf.ethz.ch;locatelf@inf.ethz.ch;heiko.strathmann@gmail.com;raetsch@inf.ethz.ch,6;9;6,2;4;4,Accept (Poster),0,1,2,yes,9/27/18,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10;10;10,10;10;10;10;10,5,6/6/18,29,10,4,0,16,2,79;176;438;739;9260,18;15;29;25;243,6;3;12;13;51,5;8;46;139;769,-1;-1
1917,ICLR,2019,The Limitations of Adversarial Training and the Blind-Spot Attack,Huan Zhang*;Hongge Chen*;Zhao Song;Duane Boning;Inderjit S. Dhillon;Cho-Jui Hsieh,huan@huan-zhang.com;chenhg@mit.edu;zhaos@utexas.edu;boning@mtl.mit.edu;inderjit@cs.utexas.edu;chohsieh@cs.ucla.edu,6;7;7,4;3;2,Accept (Poster),0,10,0,yes,9/27/18,"University of California, Los Angeles;Massachusetts Institute of Technology;University of Texas, Austin;Massachusetts Institute of Technology;University of Texas, Austin;University of California, Los Angeles",20;2;22;2;22;20,15;5;49;5;49;15,4,9/27/18,32,12,4,0,2,3,2006;497;919;2830;20517;12483,31;41;123;283;265;168,19;8;15;27;69;40,275;64;74;200;2752;1722,-1;-1
1918,ICLR,2019,Generating Multiple Objects at Spatially Distinct Locations,Tobias Hinz;Stefan Heinrich;Stefan Wermter,hinz@informatik.uni-hamburg.de;heinrich@informatik.uni-hamburg.de;wermter@informatik.uni-hamburg.de,6;7;8,4;4;4,Accept (Poster),0,8,1,yes,9/27/18,University of Hamburg;University of Hamburg;University of Hamburg,228;228;228,207;207;207,3;4;5,9/27/18,26,20,7,1,5,2,686;2563;3587,39;180;367,11;25;30,46;214;175,-1;-1
1919,ICLR,2019,Multi-class classification without multi-class labels,Yen-Chang Hsu;Zhaoyang Lv;Joel Schlosser;Phillip Odom;Zsolt Kira,yenchang.hsu@gatech.edu;zhaoyang.lv@gatech.edu;joel.schlosser@gtri.gatech.edu;phillip.odom@gtri.gatech.edu;zkira@gatech.edu,6;7;5,3;4;4,Accept (Poster),0,5,0,yes,9/27/18,Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology,13;13;13;13;13,33;33;33;33;33,10,9/27/18,16,12,12,1,25,9,189;119;128;141;1108,13;14;62;22;69,9;7;5;8;16,35;25;11;14;157,-1;-1
1920,ICLR,2019,Optimal Completion Distillation for Sequence Learning,Sara Sabour;William Chan;Mohammad Norouzi,sasabour@google.com;williamchan@google.com;mnorouzi@google.com,7;7;6,4;4;3,Accept (Poster),1,8,1,yes,9/27/18,Google;Google;Google,-1;-1;-1,-1;-1;-1,,9/27/18,23,6,11,0,25,1,1930;2081;7792,13;21;125,7;15;30,484;228;992,-1;-1
1921,ICLR,2019,Graph HyperNetworks for Neural Architecture Search,Chris Zhang;Mengye Ren;Raquel Urtasun,cjzhang@edu.uwaterloo.ca;mren@cs.toronto.edu;urtasun@cs.toronto.edu,7;6;7,4;4;4,Accept (Poster),5,7,0,yes,9/27/18,"University of Waterloo;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",26;18;18,207;22;22,10,9/27/18,62,23,20,6,20,11,235;1473;24327,16;18;245,8;12;73,18;207;3432,-1;-1
1922,ICLR,2019,GO Gradient for Expectation-Based Objectives,Yulai Cong;Miaoyun Zhao;Ke Bai;Lawrence Carin,yulaicong@gmail.com;miaoyun9zhao@gmail.com;ke.bai@duke.edu;lcarin@duke.edu,7;7;6,4;4;4,Accept (Poster),2,5,3,yes,9/27/18,Duke University;Duke University;Duke University;Duke University,44;44;44;44,17;17;17;17,,9/27/18,5,2,4,0,0,0,160;41;15;19178,17;10;6;819,8;4;3;65,8;0;1;1986,-1;-1
1923,ICLR,2019,InstaGAN: Instance-aware Image-to-Image Translation,Sangwoo Mo;Minsu Cho;Jinwoo Shin,swmo@kaist.ac.kr;mscho@postech.ac.kr;jinwoos@kaist.ac.kr,7;8;7,4;5;5,Accept (Poster),4,6,0,yes,9/27/18,Korea Advanced Institute of Science and Technology;POSTECH;Korea Advanced Institute of Science and Technology,20;123;20,95;137;95,5;4;2,9/27/18,56,30,19,0,0,7,53;2193;1681,7;63;184,2;25;18,8;439;219,-1;-1
1924,ICLR,2019,DialogWAE: Multimodal Response Generation with Conditional Wasserstein Auto-Encoder,Xiaodong Gu;Kyunghyun Cho;Jung-Woo Ha;Sunghun Kim,guxiaodong1987@126.com;kyunghyun.cho@nyu.edu;jungwoo.ha@navercorp.com;hunkim@cse.ust.hk,7;7;5,4;3;3,Accept (Poster),0,3,3,yes,9/27/18,126;New York University;NAVER;The Hong Kong University of Science and Technology,-1;26;-1;39,-1;27;-1;44,5,5/31/18,44,28,20,3,11,14,458;45318;1907;7733,7;272;67;129,5;52;15;44,64;6549;368;1001,-1;-1
1925,ICLR,2019,Supervised Policy Update for Deep Reinforcement Learning,Quan Vuong;Yiming Zhang;Keith W. Ross,quan.hovuong@gmail.com;yiming.zhang@nyu.edu;keithwross@nyu.edu,6;9;6,4;2;3,Accept (Poster),7,21,0,yes,9/27/18,"University of California, San Diego;New York University;New York University",11;26;26,31;27;27,,5/29/18,9,4,3,0,0,3,18;145;11976,4;69;295,3;7;55,4;11;1295,-1;-1
1926,ICLR,2019,Unsupervised Learning of the Set of Local Maxima,Lior Wolf;Sagie Benaim;Tomer Galanti,wolf@fb.com;sagiebenaim@gmail.com;tomer22g@gmail.com,8;8;8,3;4;3,Accept (Poster),0,10,0,yes,9/27/18,Facebook;Tel Aviv University;Tel Aviv University,-1;37;37,-1;217;217,8,9/27/18,2,0,0,0,0,0,13809;168;70,199;15;18,45;5;5,1636;26;1,-1;-1
1927,ICLR,2019,Per-Tensor Fixed-Point Quantization of the Back-Propagation Algorithm,Charbel Sakr;Naresh Shanbhag,sakr2@illinois.edu;shanbhag@illinois.edu,7;3;8,3;2;4,Accept (Poster),0,9,0,yes,9/27/18,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",3;3,37;37,,9/27/18,10,4,2,0,4,0,140;5288,18;299,7;38,8;526,-1;-1
1928,ICLR,2019,Generalized Tensor Models for Recurrent Neural Networks,Valentin Khrulkov;Oleksii Hrinchuk;Ivan Oseledets,khrulkov.v@gmail.com;oleksii.hrinchuk@skoltech.ru;i.oseledets@skoltech.ru,6;7;7,4;3;4,Accept (Poster),0,5,0,yes,9/27/18,Skolkovo Institute of Science and Technology;Skolkovo Institute of Science and Technology;Skolkovo Institute of Science and Technology,-1;-1;-1,-1;-1;-1,,9/27/18,7,5,0,1,0,0,162;63;4282,12;10;197,7;5;29,17;4;401,-1;-1
1929,ICLR,2019,Towards Metamerism via Foveated Style Transfer,Arturo Deza;Aditya Jonnalagadda;Miguel P. Eckstein,deza@dyns.ucsb.edu;aditya_jonnalagadda@ece.ucsb.edu;eckstein@psych.ucsb.edu,7;8;7,4;4;5,Accept (Poster),0,7,0,yes,9/27/18,UC Santa Barbara;UC Santa Barbara;UC Santa Barbara,37;37;37,53;53;53,5,5/29/17,4,3,1,1,10,0,83;3;4587,16;4;294,4;1;36,12;0;245,-1;-1
1930,ICLR,2019,Learning To Solve Circuit-SAT: An Unsupervised Differentiable Approach,Saeed Amizadeh;Sergiy Matusevych;Markus Weimer,saeed.amizadeh@gmail.com;sergiym@microsoft.com;markus.weimer@microsoft.com,6;8;7,5;4;3,Accept (Poster),0,7,0,yes,9/27/18,Microsoft;Microsoft;Microsoft,-1;-1;-1,-1;-1;-1,8,9/27/18,19,9,9,2,0,6,249;98;2345,23;9;79,5;5;19,30;14;271,-1;-1
1931,ICLR,2019,Deep Convolutional Networks as shallow Gaussian Processes,Adrià Garriga-Alonso;Carl Edward Rasmussen;Laurence Aitchison,ag919@cam.ac.uk;cer54@cam.ac.uk;laurence.aitchison@gmail.com,5;8;5,5;3;4,Accept (Poster),0,3,0,yes,9/27/18,University of Cambridge;University of Cambridge;HHMI Janelia Research Campus,71;71;-1,2;2;-1,,8/16/18,68,49,16,1,33,11,69;9632;411,2;83;28,1;42;10,11;1064;35,-1;-1
1932,ICLR,2019,"Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow",Xue Bin Peng;Angjoo Kanazawa;Sam Toyer;Pieter Abbeel;Sergey Levine,jasonpeng142@hotmail.com;kanazawa@eecs.berkeley.edu;sdt@berkeley.edu;pabbeel@cs.berkeley.edu;svlevine@eecs.berkeley.edu,6;10;8,3;4;3,Accept (Poster),0,6,0,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5;5,18;18;18;18;18,5;4,9/27/18,57,30,15,0,11,9,1116;1739;105;36452;24235,23;24;9;433;309,11;15;3;94;73,84;288;11;4390;3140,-1;-1
1933,ICLR,2019,Hierarchical Generative Modeling for Controllable Speech Synthesis,Wei-Ning Hsu;Yu Zhang;Ron J. Weiss;Heiga Zen;Yonghui Wu;Yuxuan Wang;Yuan Cao;Ye Jia;Zhifeng Chen;Jonathan Shen;Patrick Nguyen;Ruoming Pang,wnhsu@mit.edu;ngyuzh@google.com;ronw@google.com;heigazen@google.com;yonghui@google.com;logpie@gmail.com;yuancao@google.com;jiaye@google.com;zhifengc@google.com;jonathanasdf@google.com;drpng@google.com;rpang@google.com,8;6;5,4;5;4,Accept (Poster),0,25,0,yes,9/27/18,Massachusetts Institute of Technology;Google;Google;Google;Google;Bytedance;Google;Google;Google;Google;Google;Google,2;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,5;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,5,9/27/18,62,35,31,1,41,11,753;17549;4654;8556;8026;6495;2781;696;21538;62;5303;3426,34;1400;62;124;142;179;23;130;215;2;157;49,15;54;34;35;35;40;11;13;32;1;29;23,68;1169;616;994;891;537;247;63;2622;11;437;483,-1;-1
1934,ICLR,2019,Doubly Reparameterized Gradient Estimators for Monte Carlo Objectives,George Tucker;Dieterich Lawson;Shixiang Gu;Chris J. Maddison,gjt@google.com;jdl404@nyu.edu;shanegu@google.com;cmaddis@google.com,7;7;6,3;5;4,Accept (Poster),0,4,2,yes,9/27/18,Google;New York University;Google;Google,-1;26;-1;-1,-1;27;-1;-1,1,9/27/18,33,13,15,1,28,5,2594;192;3765;8199,74;14;39;30,21;6;21;14,291;24;467;516,-1;-1
1935,ICLR,2019,Reward Constrained Policy Optimization,Chen Tessler;Daniel J. Mankowitz;Shie Mannor,chen.tessler@gmail.com;daniel.mankowitz@gmail.com;shiemannor@gmail.com,6;6;7,2;4;2,Accept (Poster),0,5,0,yes,9/27/18,Technion;Google;Technion,25;-1;25,327;-1;327,1,5/28/18,38,19,18,1,12,7,247;541;11404,10;29;418,5;12;50,14;32;1225,-1;-1
1936,ICLR,2019,Coarse-grain Fine-grain Coattention Network for Multi-evidence Question Answering,Victor Zhong;Caiming Xiong;Nitish Shirish Keskar;Richard Socher,victor@victorzhong.com;cxiong@salesforce.com;nkeskar@salesforce.com;richard@socher.org,7;7;4,4;5;3,Accept (Poster),0,10,0,yes,9/27/18,University of Washington;SalesForce.com;SalesForce.com;SalesForce.com,6;-1;-1;-1,25;-1;-1;-1,,9/27/18,28,16,6,0,0,5,1847;6210;2137;52263,18;156;28;180,11;31;13;49,346;1052;329;8807,-1;-1
1937,ICLR,2019,Preconditioner on Matrix Lie Group for SGD,Xi-Lin Li,lixilinx@gmail.com,5;8;7,5;5;3,Accept (Poster),0,5,1,yes,9/27/18,,,,,9/26/18,1,0,1,0,0,0,1151,62,18,117,-1
1938,ICLR,2019,LeMoNADe: Learned Motif and Neuronal Assembly Detection in calcium imaging videos,Elke Kirschbaum;Manuel Haußmann;Steffen Wolf;Hannah Sonntag;Justus Schneider;Shehabeldin Elzoheiry;Oliver Kann;Daniel Durstewitz;Fred A Hamprecht,elke.kirschbaum@iwr.uni-heidelberg.de;manuel.haussmann@iwr.uni-heidelberg.de;steffen.wolf@iwr.uni-heidelberg.de;hannah.sonntag@mpimf-heidelberg.mpg.de;justus.schneider@physiologie.uni-heidelberg.de;shehab.elzoheiry@physiologie.uni-heidelberg.de;oliver.kann@physiologie.uni-heidelberg.de;daniel.durstewitz@zi-mannheim.de;fred.hamprecht@iwr.uni-heidelberg.de,5;8;8,4;4;5,Accept (Poster),0,7,0,yes,9/27/18,Heidelberg University;Heidelberg University;Heidelberg University;Max-Planck Institute;Heidelberg University;Heidelberg University;Heidelberg University;ZI Mannheim;Heidelberg University,199;199;199;-1;199;199;199;-1;199,45;45;45;-1;45;45;45;-1;45,5,6/26/18,3,1,0,0,6,0,9;29;374;1;158;30;360;1868;5738,7;9;37;4;102;7;11;82;195,2;4;9;1;6;3;6;20;39,1;2;36;0;8;1;41;129;457,-1;-1
1939,ICLR,2019,On the loss landscape of a class of deep neural networks with no bad local valleys,Quynh Nguyen;Mahesh Chandra Mukkamala;Matthias Hein,quynh@cs.uni-saarland.de;mmahesh.chandra873@gmail.com;matthias.hein@uni-tuebingen.de,7;6;8,4;5;4,Accept (Poster),0,10,0,yes,9/27/18,Saarland University;Saarland University;University of Tuebingen,89;89;153,1103;1103;94,,9/27/18,30,17,3,3,14,4,341;135;4730,24;6;119,8;4;39,29;14;527,-1;-1
1940,ICLR,2019,DHER: Hindsight Experience Replay for Dynamic Goals,Meng Fang;Cheng Zhou;Bei Shi;Boqing Gong;Jia Xu;Tong Zhang,moefang@gmail.com;chengzhmike@gmail.com;shibei00@gmail.com;boqinggo@outlook.com;jiaxu@cs.wisc.edu;tongzhang@tongzhang-ml.org,6;6;7,3;4;4,Accept (Poster),0,17,2,yes,9/27/18,Tencent AI Lab;Hong Kong University of Science and Technology;Tencent AI Lab;International Computer Science Institute;University of Southern California;,-1;39;-1;-1;30;-1,-1;44;-1;-1;66;-1,,9/27/18,7,3,2,0,0,0,722;114;412;3759;313;350,59;18;50;85;34;23,12;4;12;24;8;5,61;3;34;751;8;17,-1;-1
1941,ICLR,2019,SPIGAN: Privileged Adversarial Learning from Simulation,Kuan-Hui Lee;German Ros;Jie Li;Adrien Gaidon,kuan.lee@tri.global;germanros1987@gmail.com;jie.li@tri.global;adrien.gaidon@tri.global,7;6;7,5;5;4,Accept (Poster),0,5,0,yes,9/27/18,Toyota Research Institute;Intel;Toyota Research Institute;Toyota Research Institute,-1;-1;-1;-1,-1;-1;-1;-1,5;4;2,9/27/18,24,11,6,1,2,3,246;57;6196;1403,24;20;866;53,10;5;33;17,17;10;339;163,-1;-1
1942,ICLR,2019,Augmented Cyclic Adversarial Learning for Low Resource Domain Adaptation,Ehsan Hosseini-Asl;Yingbo Zhou;Caiming Xiong;Richard Socher,ehosseiniasl@salesforce.com;yingbo.zhou@salesforce.com;cxiong@salesforce.com;rsocher@salesforce.com,6;5;8,4;4;2,Accept (Poster),0,13,0,yes,9/27/18,SalesForce.com;SalesForce.com;SalesForce.com;SalesForce.com,-1;-1;-1;-1,-1;-1;-1;-1,4,7/1/18,27,10,7,1,43,4,556;1523;6210;52263,31;48;156;180,12;16;31;49,68;213;1052;8807,-1;-1
1943,ICLR,2019,GamePad: A Learning Environment for Theorem Proving,Daniel Huang;Prafulla Dhariwal;Dawn Song;Ilya Sutskever,dehuang@berkeley.edu;prafulla@openai.com;dawnsong@cs.berkeley.edu;ilyasu@openai.com,4;7;7,4;3;2,Accept (Poster),0,5,1,yes,9/27/18,University of California Berkeley;OpenAI;University of California Berkeley;OpenAI,5;-1;5;-1,18;-1;18;-1,1,6/2/18,32,6,10,3,10,2,226;3327;36706;130182,33;12;276;90,7;6;95;53,23;906;4088;16864,-1;-1
1944,ICLR,2019,Deep Graph Infomax,Petar Veličković;William Fedus;William L. Hamilton;Pietro Liò;Yoshua Bengio;R Devon Hjelm,petar.velickovic@cst.cam.ac.uk;liam.fedus@gmail.com;wleif@stanford.edu;pietro.lio@cst.cam.ac.uk;yoshua.umontreal@gmail.com;devon.hjelm@microsoft.com,7;9;5,3;4;4,Accept (Poster),2,13,0,yes,9/27/18,University of Cambridge;University of Montreal;Stanford University;University of Cambridge;University of Montreal;Microsoft,71;123;4;71;123;-1,2;108;3;2;108;-1,10,9/27/18,123,60,62,4,0,34,1562;670;4313;7075;201105;1636,31;25;45;381;807;43,9;10;19;36;147;13,406;84;900;726;23941;258,-1;-1
1945,ICLR,2019,Fluctuation-dissipation relations for stochastic gradient descent,Sho Yaida,shoyaida@fb.com,8;5;6,5;4;3,Accept (Poster),0,3,1,yes,9/27/18,Facebook,-1,-1,1,9/27/18,21,13,4,0,13,2,878,41,11,60,m
1946,ICLR,2019,A Kernel Random Matrix-Based Approach for Sparse PCA,Mohamed El Amine Seddik;Mohamed Tamaazousti;Romain Couillet,melaseddik@gmail.com;mohamed.tamaazousti@cea.fr;romain.couillet@gmail.com,6;5;7,4;5;2,Accept (Poster),0,4,0,yes,9/27/18,CEA;CEA;,228;228;-1,811;811;-1,,9/27/18,1,0,1,0,0,0,42;197;2848,8;38;223,4;6;25,2;14;251,-1;-1
1947,ICLR,2019,Sample Efficient Adaptive Text-to-Speech,Yutian Chen;Yannis Assael;Brendan Shillingford;David Budden;Scott Reed;Heiga Zen;Quan Wang;Luis C. Cobo;Andrew Trask;Ben Laurie;Caglar Gulcehre;Aäron van den Oord;Oriol Vinyals;Nando de Freitas,yutianc@google.com;yannisassael@google.com;shillingford@google.com;budden@google.com;reedscot@google.com;heigazen@google.com;quanw@google.com;luisca@google.com;atrask@google.com;benl@google.com;caglarg@google.com;avdnoord@google.com;vinyals@google.com;nandodefreitas@google.com,7;7;6,4;4;5,Accept (Poster),0,3,0,yes,9/27/18,Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,6,9/27/18,39,20,18,3,78,5,952;853;295;1249;10980;8556;2374;573;241;1224;19282;8527;52009;18886,87;14;13;55;28;124;285;8;16;49;36;40;121;184,16;9;5;15;16;35;22;8;5;19;26;24;55;54,105;93;34;129;2053;994;221;77;34;157;2974;1093;6497;1852,-1;-1
1948,ICLR,2019,ProMP: Proximal Meta-Policy Search,Jonas Rothfuss;Dennis Lee;Ignasi Clavera;Tamim Asfour;Pieter Abbeel,jonas.rothfuss@gmail.com;dennisl88@berkeley.edu;iclavera@berkeley.edu;asfour@kit.edu;pabbeel@cs.berkeley.edu,6;7;9,3;3;3,Accept (Poster),11,10,2,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley;Karlsruhe Institute of Technology;University of California Berkeley,5;5;5;153;5,18;18;18;133;18,6,9/27/18,40,22,21,2,110,13,121;1526;421;7170;36452,7;153;15;294;433,4;21;8;43;94,20;124;55;363;4390,-1;-1
1949,ICLR,2019,A Convergence Analysis of Gradient Descent for Deep Linear Neural Networks,Sanjeev Arora;Nadav Cohen;Noah Golowich;Wei Hu,arora@cs.princeton.edu;cohennadav@ias.edu;ngolowich@college.harvard.edu;huwei@cs.princeton.edu,7;7;7,4;4;5,Accept (Poster),0,10,0,yes,9/27/18,"Princeton University;Institue for Advanced Study, Princeton;Harvard University;Princeton University",30;-1;39;30,7;-1;6;7,,9/27/18,74,50,13,2,0,14,15903;1024;353;2834,349;63;25;235,61;15;8;25,1794;118;51;340,-1;-1
1950,ICLR,2019,Slimmable Neural Networks,Jiahui Yu;Linjie Yang;Ning Xu;Jianchao Yang;Thomas Huang,jyu79@illinois.edu;linjie.yang@snap.com;ning.xu@snap.com;jianchao.yang@bytedance.com;huang@ifp.uiuc.edu,8;9;7,4;5;4,Accept (Poster),1,15,0,yes,9/27/18,"University of Illinois, Urbana Champaign;Snap Inc.;Snap Inc.;Bytedance;University of Illinois, Urbana-Champaign",3;-1;-1;-1;3,37;-1;-1;-1;37,2,9/27/18,62,26,16,4,3,9,1607;847;6536;2204;67594,40;28;654;20;1501,13;11;35;8;117,244;170;445;431;6375,-1;-1
1951,ICLR,2019,Discrete flow posteriors for variational inference in discrete dynamical systems,Laurence Aitchison;Vincent Adam;Srinivas C. Turaga,laurence.aitchison@gmail.com;vincent.adam@prowler.io;turagas@janelia.hhmi.org,4;4;7,3;4;4,Reject,0,0,0,yes,9/27/18,HHMI Janelia Research Campus;Prowler.io;HHMI Janelia Research Campus,-1;-1;-1,-1;-1;-1,5,5/28/18,2,1,0,0,0,0,411;227;1679,28;19;41,10;5;14,35;16;87,-1;-1
1952,ICLR,2019,Variational Sparse Coding,Francesco Tonolini;Bjorn Sand Jensen;Roderick Murray-Smith,2402432t@student.gla.ac.uk;bjorn.jensen@glasgow.ac.uk;roderick.murray-smith@glasgow.ac.uk,4;5;5,4;5;4,Reject,3,6,2,yes,9/27/18,University of Glasgow;University of Glasgow;University of Glasgow,169;169;169,80;80;80,5;1,9/27/18,3,0,1,0,0,0,42;107;3736,6;35;142,3;6;32,4;8;232,-1;-1
1953,ICLR,2019,Detecting Memorization in ReLU Networks,Edo Collins;Siavash Arjomand Bigdeli;Sabine Süsstrunk,edo.collins@epfl.ch;siavash.bigdeli@epfl.ch;sabine.susstrunk@epfl.ch,6;5;9,4;4;5,Reject,0,11,0,yes,9/27/18,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne,478;478;478,38;38;38,8,9/27/18,3,3,1,0,3,0,70;160;12993,9;17;243,5;4;37,8;16;2093,-1;-1
1954,ICLR,2019,Reinforcement Learning with Perturbed Rewards,Jingkang Wang;Yang Liu;Bo Li,wangjksjtu_01@sjtu.edu.cn;yangliu@ucsc.edu;lxbosky@gmail.com,6;6;6,4;3;3,Reject,0,9,0,yes,9/27/18,Shanghai Jiao Tong University;University of Southern California;University of California Berkeley,52;30;5,188;66;18,1,9/27/18,7,3,1,0,4,0,55;10237;25967,7;1033;2410,3;45;64,3;736;2033,-1;-1
1955,ICLR,2019,Local Critic Training of Deep Neural Networks,Hojung Lee;Jong-Seok Lee,hjlee92@yonsei.ac.kr;jong-seok.lee@yonsei.ac.kr,6;7;6,3;5;4,Reject,0,7,0,yes,9/27/18,Yonsei University;Yonsei University,478;478,231;231,,5/3/18,0,0,0,0,0,0,181;3349,37;343,7;33,5;206,-1;-1
1956,ICLR,2019,Learning from Noisy Demonstration Sets via Meta-Learned Suitability Assessor,Te-Lin Wu;Jaedong Hwang;Jingyun Yang;Shaofan Lai;Carl Vondrick;Joseph J. Lim,telinwu@usc.edu;jd730@snu.ac.kr;jingyuny@usc.edu;shaofanl@usc.edu;vondrick@cs.columbia.edu;limjj@usc.edu,4;4;4,4;4;3,Reject,0,0,0,yes,9/27/18,University of Southern California;Seoul National University;University of Southern California;University of Southern California;Columbia University;University of Southern California,30;41;30;30;15;30,66;74;66;66;14;66,6,9/27/18,0,0,0,0,0,0,97;2;165;10;4034;2922,15;12;34;3;56;51,4;1;6;1;25;20,16;1;18;3;466;264,-1;-1
1957,ICLR,2019,Detecting Topological Defects in 2D Active Nematics Using Convolutional Neural Networks,Ruoshi Liu;Michael M. Norton;Seth Fraden;Pengyu Hong,ruoshiliu@brandeis.edu;mmnorton@brandeis.edu;fraden@brandeis.edu;hongpeng@brandeis.edu,4;4;2,4;4;5,Reject,0,0,0,yes,9/27/18,Brandeis University;Brandeis University;Brandeis University;Brandeis University,314;314;314;314,223;223;223;223,,9/27/18,0,0,0,0,0,0,122;236;1267;1764,22;98;110;92,6;8;20;21,9;10;43;97,-1;-1
1958,ICLR,2019,Rating Continuous Actions in Spatial Multi-Agent Problems,Uwe Dick;Maryam Tavakol;Ulf Brefeld,uwe.dick@leuphana.de;tavakol@leuphana.de;brefeld@leuphana.de,5;4;4,4;3;4,Reject,0,0,0,yes,9/27/18,Inst. of Information Systems / Machine Learning;Inst. of Information Systems / Machine Learning;Inst. of Information Systems / Machine Learning,-1;-1;-1,-1;-1;-1,4,9/27/18,0,0,0,0,0,0,64;49;1896,12;16;110,4;3;23,12;2;208,-1;-1
1959,ICLR,2019,SSoC: Learning Spontaneous and Self-Organizing Communication for Multi-Agent Collaboration,Xiangyu Kong;Jing Li;Bo Xin;Yizhou Wang,kong@pku.edu.cn;lijingg@pku.edu.cn;jimxinbo@gmail.com;yizhou.wang@pku.edu.cn,4;5;5,3;4;3,Reject,0,1,0,yes,9/27/18,Peking University;Peking University;Microsoft;Peking University,24;24;-1;24,27;27;-1;27,,9/27/18,0,0,0,0,0,0,511;112;126;1901,136;36;13;90,12;5;5;20,18;15;12;140,-1;-1
1960,ICLR,2019,Generating Realistic Stock Market Order Streams,Junyi Li;Xintong Wang;Yaoyang Lin;Arunesh Sinha;Michael P. Wellman,junyili@umich.edu;xintongw@umich.edu;yaoyang@umich.edu;arunesh@umich.edu;wellman@umich.edu,5;5;4,4;4;5,Reject,0,4,0,yes,9/27/18,University of Michigan;University of Michigan;University of Michigan;University of Michigan;University of Michigan,8;8;8;8;8,21;21;21;21;21,5;4,9/27/18,4,3,0,0,0,0,96;29;3;1025;10314,33;5;1;87;283,5;3;1;17;50,4;3;0;68;736,-1;-1
1961,ICLR,2019,Causal Reasoning from Meta-reinforcement learning,Ishita Dasgupta;Jane Wang;Silvia Chiappa;Jovana Mitrovic;Pedro Ortega;David Raposo;Edward Hughes;Peter Battaglia;Matthew Botvinick;Zeb Kurth-Nelson,ishitadasgupta@g.harvard.edu;wangjane@google.com;csilvia@google.com;mitrovic@google.com;pedroortega@google.com;draposo@google.com;edwardhughes@google.com;peterbattaglia@google.com;botvinick@google.com;zebk@google.com,5;4;4;7,4;3;4;4,Reject,0,10,0,yes,9/27/18,Harvard University;Google;Google;Google;Google;Google;Google;Google;Google;Google,39;-1;-1;-1;-1;-1;-1;-1;-1;-1,6;-1;-1;-1;-1;-1;-1;-1;-1;-1,6,9/27/18,15,6,2,0,29,1,119;671;583;74;822;1550;152;4482;13392;1595,17;16;45;8;59;15;20;88;147;62,5;9;12;4;15;8;7;29;45;19,7;70;40;8;36;212;13;417;1407;105,-1;-1
1962,ICLR,2019,MahiNet: A Neural Network for Many-Class Few-Shot Learning with Class Hierarchy,Lu Liu;Tianyi Zhou;Guodong Long;Jing Jiang;Chengqi Zhang,lu.liu.cs.uts@gmail.com;tianyizh@uw.edu;guodong.long@uts.edu.au;jing.jiang@uts.edu.au;chengqi.zhang@uts.edu.au,5;6;5,3;3;3,Reject,0,7,0,yes,9/27/18,"University of Technology Sydney;University of Washington, Seattle;University of Technology Sydney;University of Technology Sydney;University of Technology Sydney",106;6;106;106;106,216;25;216;216;216,6,9/27/18,0,0,0,0,0,0,2420;1447;1690;22;6900,224;81;79;26;468,24;14;16;3;39,102;142;151;1;471,-1;-1
1963,ICLR,2019,CoDraw: Collaborative Drawing as a Testbed for Grounded Goal-driven Communication,Nikita Kitaev;Jin-Hwa Kim;Xinlei Chen;Marcus Rohrbach;Yuandong Tian;Dhruv Batra;Devi Parikh,kitaev@cs.berkeley.edu;jnhwkim@gmail.com;xinleic@fb.com;maroffm@gmail.com;yuandong@fb.com;dbatra@gatech.edu;parikh@gatech.edu,4;6;7,4;4;4,Reject,0,3,0,yes,9/27/18,University of California Berkeley;SK T-Brain;Facebook;Facebook;Facebook;Georgia Institute of Technology;Georgia Institute of Technology,5;-1;-1;-1;-1;13;13,18;-1;-1;-1;-1;33;33,3,12/15/17,14,9,3,0,0,1,455;205;3506;11319;2435;8305;80,12;15;39;90;84;147;18,9;3;23;43;25;41;4,62;34;423;1527;285;1135;9,-1;-1
1964,ICLR,2019,I Know the Feeling: Learning to Converse with Empathy,Hannah Rashkin;Eric Michael Smith;Margaret Li;Y-Lan Boureau,hrashkin@cs.washington.edu;ems@fb.com;hadasah@gmail.com;ylan@fb.com,4;7;5,4;4;3,Reject,0,11,0,yes,9/27/18,University of Washington;Facebook;Facebook;Facebook,6;-1;-1;-1,25;-1;-1;-1,,9/27/18,12,10,5,1,0,4,635;124;74;4406,18;9;6;24,12;4;4;14,119;36;20;335,-1;-1
1965,ICLR,2019,Neural MMO: A massively multiplayer game environment for intelligent agents,Joseph Suarez;Yilun Du;Phillip Isola;Igor Mordatch,joseph15@stanford.edu;yilundu@gmail.com;phillipi@mit.edu;mordatch@openai.com,6;5;7,4;2;5,Reject,0,5,0,yes,9/27/18,Stanford University;Massachusetts Institute of Technology;Massachusetts Institute of Technology;OpenAI,4;2;2;-1,3;5;5;-1,,9/27/18,15,3,2,0,0,1,59;121;12266;3005,18;27;73;48,4;6;27;26,2;10;2143;344,-1;-1
1966,ICLR,2019,Temporal Gaussian Mixture Layer for Videos,AJ Piergiovanni;Michael S. Ryoo,ajpiergi@indiana.edu;mryoo@indiana.edu,6;6;7,5;3;5,Reject,0,6,1,yes,9/27/18,University of Arizona;University of Arizona,169;169,161;161,,3/16/18,10,4,3,0,0,1,168;2812,30;100,7;24,10;272,-1;-1
1967,ICLR,2019,LEARNING GENERATIVE MODELS FOR DEMIXING OF STRUCTURED SIGNALS FROM THEIR SUPERPOSITION USING GANS,Mohammadreza Soltani;Swayambhoo Jain;Abhinav V. Sambasivan,msoltani@iastate.edu;swayambhoo.jain@technicolor.com;samba014@umn.edu,7;4;5,4;5;4,Reject,0,3,0,yes,9/27/18,"Iowa State University;Technicolor;University of Minnesota, Minneapolis",169;-1;57,341;-1;56,5;4,9/27/18,1,1,0,0,0,0,111;131;6,34;28;8,6;5;1,7;5;0,-1;-1
1968,ICLR,2019,Invariant-equivariant representation learning for multi-class data,Ilya Feige,ilya@asidatascience.com,7;5;4,2;5;3,Reject,0,5,0,yes,9/27/18,University College London,50,16,5,9/27/18,2,0,0,0,2,0,197,20,5,5,-1
1969,ICLR,2019,Graph Matching Networks for Learning the Similarity of Graph Structured Objects,Yujia Li;Chenjie Gu;Thomas Dullien;Oriol Vinyals;Pushmeet Kohli,yujiali@google.com;gcj@google.com;thomasdullien@google.com;vinyals@google.com;pushmeet@google.com,6;5;6,4;4;4,Reject,0,3,0,yes,9/27/18,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,10,9/27/18,45,23,16,1,66,14,496;472;379;52009;22034,33;45;15;121;313,8;12;8;55;69,49;54;59;6497;2746,-1;-1
1970,ICLR,2019,Probabilistic Semantic Embedding,Yue Jiao;Jonathon Hare;Adam Prügel-Bennett,yj5y15@ecs.soton.ac.uk;jsh2@ecs.soton.ac.uk;apb@ecs.soton.ac.uk,7;4;4,3;4;4,Reject,0,7,0,yes,9/27/18,University of Southampton;University of Southampton;University of Southampton,169;169;169,126;126;126,,9/27/18,1,1,1,0,0,1,47;1603;2097,11;117;137,4;22;25,8;149;135,-1;-1
1971,ICLR,2019,SENSE: SEMANTICALLY ENHANCED NODE SEQUENCE EMBEDDING,Swati Rallapalli;Liang Ma;Mudhakar Srivatsa;Ananthram Swami;Heesung Kwon;Graham Bent;Christopher Simpkin,srallapalli@us.ibm.com;maliang@us.ibm.com;msrivats@us.ibm.com;ananthram.swami.civ@mail.mil;heesung.kwon.civ@mail.mil;gbent@uk.ibm.com;simpkin.chris@gmail.com,4;4;5,4;5;3,Reject,0,3,0,yes,9/27/18,International Business Machines;International Business Machines;International Business Machines;Army Reserach laboratory;Army Reserach laboratory;International Business Machines;,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,1;10,9/27/18,0,0,0,0,0,0,460;651;2584;12426;1627;52;15,37;59;194;418;128;29;7,11;13;27;43;18;4;3,45;50;224;1271;153;0;0,-1;-1
1972,ICLR,2019,Pix2Scene: Learning Implicit 3D Representations from Images,Sai Rajeswar;Fahim Mannan;Florian Golemo;David Vazquez;Derek Nowrouzezahrai;Aaron Courville,rajsai24@gmail.com;fmannan@gmail.com;florian.golemo@inria.fr;dvazquez@cvc.uab.es;dereknow@gmail.com;aaron.courville@gmail.com,6;5;6,1;4;4,Reject,0,15,2,yes,9/27/18,"University of Montreal;;INRIA;Computer Vision Center, Universitat Autònoma de Barcelona;;University of Montreal",123;-1;-1;478;-1;123,108;-1;-1;146;-1;108,5;4;2,9/27/18,4,4,0,0,0,2,634;62;115;5;1929;59549,16;21;17;11;111;203,7;5;4;1;25;64,116;6;10;2;110;7800,-1;-1
1973,ICLR,2019,COCO-GAN: Conditional Coordinate Generative Adversarial Network,Chieh Hubert Lin;Chia-Che Chang;Yu-Sheng Chen;Da-Cheng Juan;Wei Wei;Hwann-Tzong Chen,hubert052702@gmail.com;chang810249@gmail.com;nothinglo@cmlab.csie.ntu.edu.tw;dacheng@google.com;wewei@google.com;htchen@cs.nthu.edu.tw,6;4;6,4;5;4,Reject,0,27,0,yes,9/27/18,National Tsing Hua University;National Tsing Hua University;National Taiwan University;Google;Google;National Tsing Hua University,199;199;85;-1;-1;199,323;323;197;-1;-1;323,5;4,9/27/18,2,2,0,0,0,0,38;41;3954;617;4641;1513,7;8;212;47;494;75,3;4;24;17;31;16,6;6;273;76;182;217,-1;-1
1974,ICLR,2019,Implicit Autoencoders,Alireza Makhzani,a.makhzani@gmail.com,3;6;6,3;4;3,Reject,0,7,1,yes,9/27/18,,,,5;4,5/24/18,9,3,1,0,23,0,1426,13,8,210,-1
1975,ICLR,2019,Identifying Bias in AI using Simulation,Daniel McDuff;Roger Cheng;Ashish Kapoor,damcduff@microsoft.com;rocheng@microsoft.com;akapoor@microsoft.com,7;5;6,5;4;2,Reject,0,9,0,yes,9/27/18,Microsoft;Microsoft;Microsoft,-1;-1;-1,-1;-1;-1,11;7,9/27/18,5,5,0,0,2,1,4147;5377;5753,127;154;186,28;22;41,435;483;448,-1;-1
1976,ICLR,2019,Backplay: 'Man muss immer umkehren',Cinjon Resnick;Roberta Raileanu;Sanyam Kapoor;Alexander Peysakhovich;Kyunghyun Cho;Joan Bruna,cinjon.resnick@gmail.com;raileanu.roberta@gmail.com;sanyam@nyu.edu;alexpeys@fb.com;kyunghyun.cho@nyu.edu;bruna@cims.nyu.edu,5;5;5,4;4;3,Reject,0,8,0,yes,9/27/18,New York University;New York University;New York University;Facebook;New York University;New York University,26;26;26;-1;26;26,27;27;27;-1;27;27,,7/18/18,27,13,8,2,90,5,278;80;39;30;45318;11337,12;7;4;4;272;90,6;3;2;2;52;29,38;9;5;5;6549;1271,-1;-1
1977,ICLR,2019,Intrinsic Social Motivation via Causal Influence in Multi-Agent RL,Natasha Jaques;Angeliki Lazaridou;Edward Hughes;Caglar Gulcehre;Pedro A. Ortega;DJ Strouse;Joel Z. Leibo;Nando de Freitas,jaquesn@mit.edu;angeliki@google.com;edwardhughes@google.com;caglarg@google.com;pedroortega@google.com;danieljstrouse@gmail.com;jzl@google.com;nandodefreitas@google.com,5;4;6,3;5;3,Reject,0,9,1,yes,9/27/18,Massachusetts Institute of Technology;Google;Google;Google;Google;Princeton University;Google;Google,2;-1;-1;-1;-1;30;-1;-1,5;-1;-1;-1;-1;7;-1;-1,,9/27/18,32,22,3,2,374,2,889;1721;1547;19282;822;179;3451;18886,43;76;98;36;59;14;75;184,19;21;15;26;15;6;29;54,73;192;163;2974;36;18;346;1852,-1;-1
1978,ICLR,2019,Dynamic Planning Networks,Norman L. Tasfi;Miriam Capretz,ntasfi@gmail.com;mcapretz@uwo.ca,6;4;6,5;5;2,Reject,1,0,8,yes,9/27/18,University of Western Ontario;University of Western Ontario,478;478,1103;1103,8,9/27/18,1,1,0,0,53,0,9;1408,3;127,1;17,0;87,-1;-1
1979,ICLR,2019,Learning to Coordinate Multiple Reinforcement Learning Agents for Diverse Query Reformulation,Rodrigo Nogueira;Jannis Bulian;Massimiliano Ciaramita,rodrigonogueira@nyu.edu;jbulian@google.com;massi@google.com,4;7;5,3;4;4,Reject,2,0,4,yes,9/27/18,New York University;Google;Google,26;-1;-1,27;-1;-1,8,9/27/18,8,1,6,1,0,0,304;122;2895,24;11;76,8;4;28,38;14;266,-1;-1
1980,ICLR,2019,Recovering the Lowest Layer of Deep Networks with High Threshold Activations,Surbhi Goel;Rina Panigrahy,surbhi@cs.utexas.edu;rinap@google.com,4;5;4,4;3;4,Reject,0,3,0,yes,9/27/18,"University of Texas, Austin;Google",22;-1,49;-1,,9/27/18,0,0,0,0,0,0,259;6928,31;130,8;36,20;807,-1;-1
1981,ICLR,2019,Improved Learning of One-hidden-layer Convolutional Neural Networks with Overlaps,Simon S. Du;Surbhi Goel,ssdu@cs.cmu.edu;surbhi@cs.utexas.edu,6;5;6,3;1;4,Reject,0,3,0,yes,9/27/18,"Carnegie Mellon University;University of Texas, Austin",1;22,24;49,2,5/20/18,9,9,2,0,9,2,1989;259,55;31,19;8,302;20,-1;-1
1982,ICLR,2019,Using Ontologies To Improve Performance In Massively Multi-label Prediction,Ethan Steinberg;Peter J. Liu,ethan.steinberg@gmail.com;peterjliu@google.com,6;5;4,3;3;4,Reject,0,4,0,yes,9/27/18,Stanford University;Google,4;-1,3;-1,11,9/27/18,6,0,0,0,6,0,61;2515,7;24,1;12,1;489,-1;-1
1983,ICLR,2019,Overfitting Detection of Deep Neural Networks without a Hold Out Set,Konrad Groh,konrad.groh@de.bosch.com,4;5;3,4;3;4,Reject,0,1,0,yes,9/27/18,Bosch,-1,-1,,9/27/18,0,0,0,0,0,0,28,13,3,0,-1
1984,ICLR,2019,TarMAC: Targeted Multi-Agent Communication,Abhishek Das;Theophile Gervet;Joshua Romoff;Dhruv Batra;Devi Parikh;Mike Rabbat;Joelle Pineau,abhshkdz@gatech.edu;tgervet@andrew.cmu.edu;joshua.romoff@mail.mcgill.ca;dbatra@gatech.edu;parikh@gatech.edu;mikerabbat@fb.com;jpineau@cs.mcgill.ca,6;6;6,5;4;5,Reject,0,11,0,yes,9/27/18,Georgia Institute of Technology;Carnegie Mellon University;McGill University;Georgia Institute of Technology;Georgia Institute of Technology;Facebook;McGill University,13;1;85;13;13;-1;85,33;24;42;33;33;-1;42,,9/27/18,41,24,12,2,28,5,3067;40;199;8305;14930;5055;11114,20;1;13;147;184;180;264,12;1;7;41;54;36;45,421;5;17;1135;2366;352;1214,-1;-1
1985,ICLR,2019,DATNet: Dual Adversarial Transfer for Low-resource Named Entity Recognition,Joey Tianyi Zhou;Hao Zhang;Di Jin;Hongyuan Zhu;Rick Siow Mong Goh;Kenneth Kwok,joey.tianyi.zhou@gmail.com;isaac.changhau@gmail.com;jindi15@mit.edu;hongyuanzhu.cn@gmail.com;gohsm@ihpc.a-star.edu.sg;kenkwok@ihpc.a-star.edu.sg,6;6;6,4;5;4,Reject,0,3,0,yes,9/27/18,";A*STAR;Massachusetts Institute of Technology;Institute for Infocomm Research;Institute of High Performance Computing, Singapore, A*STAR;Institute of High Performance Computing, Singapore, A*STAR",-1;-1;2;-1;-1;-1,-1;-1;5;-1;-1;-1,4;8,9/27/18,1,1,0,0,0,0,932;395;172;442;812;562,83;75;17;46;125;30,18;10;7;12;17;13,88;22;17;19;63;33,-1;-1
1986,ICLR,2019,MILE: A Multi-Level Framework for Scalable Graph Embedding,Jiongqian Liang;Saket Gurukar;Srinivasan Parthasarathy,liang.albert@outlook.com;gurukar.1@osu.edu;srini@cse.ohio-state.edu,7;4;6,3;4;5,Reject,0,5,0,yes,9/27/18,Ohio State University;Ohio State University;Ohio State University,76;76;76,318;318;318,10,2/26/18,16,5,10,0,7,3,114;44;7379,13;7;264,6;3;45,18;4;629,-1;-1
1987,ICLR,2019,Adversarial Attacks on Node Embeddings,Aleksandar Bojchevski;Stephan Günnemann,a.bojchevski@in.tum.de;guennemann@in.tum.de,6;5;6,4;5;3,Reject,0,3,0,yes,9/27/18,Technical University Munich;Technical University Munich,54;54,41;41,4;10,9/4/18,56,27,9,2,9,12,576;2437,21;138,12;28,105;278,-1;-1
1988,ICLR,2019,DADAM: A consensus-based distributed adaptive gradient method for online optimization,Parvin Nazari;Davoud Ataee Tarzanagh;George Michailidis,p_nazari@aut.ac.ir;tarzanagh@ufl.edu;gmichail@ufl.edu,8;4;6,3;4;4,Reject,0,10,3,yes,9/27/18,University of Tehran;University of Florida;University of Florida,478;123;123,683;143;143,1;9,9/27/18,10,7,3,0,3,1,15;30;5450,4;8;311,2;4;38,1;2;386,-1;-1
1989,ICLR,2019,Training Hard-Threshold Networks with Combinatorial Search in a Discrete Target Propagation Setting,Lukas Nabergall;Justin Toth;Leah Cousins,lnaberga@uwaterloo.ca;wjtoth@uwaterloo.ca;lm2cousi@uwaterloo.ca,3;4;5,5;4;5,Reject,0,4,0,yes,9/27/18,University of Waterloo;University of Waterloo;University of Waterloo,26;26;26,207;207;207,,9/27/18,0,0,0,0,0,0,11;8;0,10;6;2,2;2;0,2;0;0,-1;-1
1990,ICLR,2019,Projective Subspace Networks For Few-Shot Learning,Christian Simon;Piotr Koniusz;Mehrtash Harandi,christian.simon@anu.edu.au;piotr.koniusz@data61.csiro.au;mehrtash.harandi@monash.edu,6;6;6,3;4;4,Reject,0,9,0,yes,9/27/18,"Australian National University;, CSIRO;Monash University",106;-1;123,48;-1;80,6;8,9/27/18,2,1,1,0,0,0,137;865;2944,42;50;100,6;16;26,8;81;338,-1;-1
1991,ICLR,2019,Alignment Based Mathching Networks for One-Shot Classification and Open-Set Recognition,Paresh Malalur;Tommi Jaakkola,pareshmg@csail.mit.edu;tommi@csail.mit.edu,7;6;7;4,3;2;4;4,Reject,0,3,0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2,5;5,,9/27/18,1,0,0,0,0,0,1;21902,7;292,1;69,0;2317,-1;-1
1992,ICLR,2019,Cramer-Wold AutoEncoder,Jacek Tabor;Szymon Knop;Przemysław Spurek;Igor Podolak;Marcin Mazur;Stanisław Jastrzębski,jacek.tabor@uj.edu.pl;szymon.knop@doctoral.uj.edu.pl;przemyslaw.spurek@uj.edu.pl;igor.podolak@uj.edu.pl;marcin.mazur@uj.edu.pl;staszek.jastrzebski@gmail.com,6;7;5,4;4;4,Reject,0,9,0,yes,9/27/18,Jagiellonian University;Jagiellonian University;Jagiellonian University;Jagiellonian University;Jagiellonian University;New York University,478;478;478;478;478;26,695;695;695;695;695;27,5,5/23/18,15,6,9,0,6,2,272;17;133;145;157;847,87;4;36;37;101;27,8;2;6;6;7;12,10;2;9;9;17;99,-1;-1
1993,ICLR,2019,Directional Analysis of Stochastic Gradient Descent via von Mises-Fisher Distributions in Deep Learning,Cheolhyoung Lee;Kyunghyun Cho;Wanmo Kang,bloodwass@kaist.ac.kr;kyunghyun.cho@nyu.edu;wanmo.kang@kaist.edu,6;5;4,3;3;3,Reject,0,7,0,yes,9/27/18,Korea Advanced Institute of Science and Technology;New York University;KAIST,20;26;20,95;27;95,,9/27/18,3,3,0,0,33,0,8;45318;363,2;272;49,2;52;11,2;6549;36,-1;-1
1994,ICLR,2019,Unsupervised Control Through Non-Parametric Discriminative Rewards,David Warde-Farley;Tom Van de Wiele;Tejas Kulkarni;Catalin Ionescu;Steven Hansen;Volodymyr Mnih,d.warde.farley@gmail.com;tomvandewiele@google.com;tejasdkulkarni@gmail.com;cdi@google.com;stevenhansen@google.com;vmnih@google.com,7;8;5,3;5;5,Accept (Poster),3,15,4,yes,9/27/18,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,,9/27/18,40,22,12,2,63,3,23985;1647;303;1359;445;20783,27;166;34;38;75;38,19;19;8;10;7;27,3838;104;18;168;27;3333,-1;-1
1995,ICLR,2019,Clean-Label Backdoor Attacks,Alexander Turner;Dimitris Tsipras;Aleksander Madry,turneram@mit.edu;tsipras@mit.edu;madry@mit.edu,6;7;4,2;2;4,Reject,0,11,0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2,5;5;5,5;4,9/27/18,20,6,0,0,0,1,443;3768;5359,13;33;84,4;16;29,59;906;1068,-1;-1
1996,ICLR,2019,Universal Successor Features for Transfer Reinforcement Learning,Chen Ma;Dylan R. Ashley;Junfeng Wen;Yoshua Bengio,chenchloem@gmail.com;dashley@ualberta.ca;junfengwen@gmail.com;yoshua.umontreal@gmail.com,6;4;7,5;5;5,Reject,0,15,3,yes,9/27/18,;University of Alberta;University of Alberta;University of Montreal,-1;99;99;123,-1;119;119;108,8,9/27/18,1,1,1,0,0,1,6;32;159;201105,7;8;16;807,1;4;7;147,2;7;17;23941,-1;-1
1997,ICLR,2019,Improved Language Modeling by Decoding the Past,Siddhartha Brahma,sidbrahma@gmail.com,3;6;7,5;3;5,Reject,8,6,0,yes,9/27/18,International Business Machines,-1,-1,3,8/14/18,3,2,0,0,5,0,264,43,10,18,-1
1998,ICLR,2019,ON BREIMAN’S DILEMMA IN NEURAL NETWORKS: SUCCESS AND FAILURE OF NORMALIZED MARGINS,Yifei HUANG;Yuan YAO;Weizhi ZHU,yhuangcc@ust.hk;yuany@ust.hk;wzhuai@connect.ust.hk,4;5;5,4;4;3,Reject,0,3,0,yes,9/27/18,The Hong Kong University of Science and Technology;The Hong Kong University of Science and Technology;The Hong Kong University of Science and Technology,39;39;39,44;44;44,1;8,9/27/18,0,0,0,0,0,0,0;436;17,11;60;7,0;10;2,0;27;2,-1;-1
1999,ICLR,2019,Decoupling Gating from Linearity,Yonathan Fiat;Eran Malach;Shai Shalev-Shwartz,jonathan.fiat@gmail.com;eran.malach@mail.huji.ac.il;shais@cs.huji.ac.il,3;2;3,4;5;5,Reject,0,0,0,yes,9/27/18,Hebrew University of Jerusalem;Hebrew University of Jerusalem;Hebrew University of Jerusalem,65;65;65,205;205;205,,9/27/18,5,3,0,2,0,0,5;255;13761,2;12;168,1;5;48,0;40;1808,-1;-1
2000,ICLR,2019,Learning Information Propagation in the Dynamical Systems via Information Bottleneck Hierarchy,Gaurav Gupta;Mohamed Ridha Znaidi;Paul Bogdan,ggaurav@usc.edu;znaidi@usc.edu;pbogdan@usc.edu,5;4;5,3;2;4,Reject,0,6,0,yes,9/27/18,University of Southern California;University of Southern California;University of Southern California,30;30;30,66;66;66,,9/27/18,0,0,0,0,0,0,199;0;1762,48;3;126,4;0;25,15;0;88,-1;-1
2001,ICLR,2019,Efficient Dictionary Learning with Gradient Descent,Dar Gilboa;Sam Buchanan;John Wright,dg2893@columbia.edu;sdb2157@columbia.edu;jw2966@columbia.edu,5;4;5,4;3;2,Reject,0,0,0,yes,9/27/18,Columbia University;Columbia University;Columbia University,15;15;15,14;14;14,9,9/27/18,18,6,3,0,0,1,60;20;236,7;2;27,4;2;7,6;1;18,-1;-1
2002,ICLR,2019,Learning Backpropagation-Free Deep Architectures with Kernels,Shiyu Duan;Shujian Yu;Yunmei Chen;Jose Principe,michaelshiyu3@gmail.com;yusjlcy9011@ufl.edu,6;6;5,4;4;3,Reject,0,25,2,yes,9/27/18,University of Florida;University of Florida,123;123,143;143,1,9/27/18,0,0,0,0,0,0,28;324;3978;16641,11;56;192;1022,3;9;32;57,2;12;288;1266,-1;-1
2003,ICLR,2019,Entropic GANs meet VAEs: A Statistical Approach to Compute Sample Likelihoods in GANs,Yogesh Balaji;Hamed Hasani;Rama Chellappa;Soheil Feizi,yogesh@cs.umd.edu;hassani@seas.upenn.edu;rama@umiacs.umd.edu;sfeizi@cs.umd.edu,6;5;5,5;3;4,Reject,0,4,0,yes,9/27/18,"University of Maryland, College Park;University of Pennsylvania;University of Maryland, College Park;University of Maryland, College Park",12;19;12;12,69;10;69;69,5;4;1,9/27/18,8,5,0,0,14,0,472;242;40309;501,18;43;1066;38,7;10;95;13,60;26;3158;55,-1;-1
2004,ICLR,2019,Incremental training of multi-generative adversarial networks,Qi Tan;Pingzhong Tang;Ke Xu;Weiran Shen;Song Zuo,thunderingtan@gmail.com;kenshinping@gmail.com;xuke@tsinghua.edu.cn;emersonswr@gmail.com;songzuo.z@gmail.com,5;6;6,3;4;3,Reject,0,3,0,yes,9/27/18,Tsinghua University;Tsinghua University;Tsinghua University;;Google,8;8;8;-1;-1,30;30;30;-1;-1,5;4,9/27/18,0,0,0,0,0,0,130;675;4687;543;344,37;108;467;48;69,8;14;34;13;9,7;33;201;42;14,-1;-1
2005,ICLR,2019,Transformer-XL: Language Modeling with Longer-Term Dependency,Zihang Dai*;Zhilin Yang*;Yiming Yang;William W. Cohen;Jaime Carbonell;Quoc V. Le;Ruslan Salakhutdinov,zander.dai@gmail.com;zhiliny@cs.cmu.edu;yiming@cs.cmu.edu;wcohen@google.com;jgc@cs.cmu.edu;qvl@google.com;rsalakhu@cs.cmu.edu,6;6;4,4;4;4,Reject,0,9,6,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Google;Carnegie Mellon University;Google;Carnegie Mellon University,1;1;1;-1;1;-1;1,24;24;24;-1;24;-1;24,3,9/27/18,33,15,11,0,0,9,2364;4894;21147;22370;15678;47470;66785,27;90;272;423;505;193;253,14;26;49;68;54;79;81,425;783;2566;2619;1625;5968;7750,-1;-1
2006,ICLR,2019,Formal Limitations on the Measurement of Mutual Information,David McAllester;Karl Stratos,mcallester@ttic.edu;stratos@ttic.edu,6;8;4,3;5;4,Reject,0,12,0,yes,9/27/18,Toyota Technological Institute at Chicago;Toyota Technological Institute at Chicago,123;123,1103;1103,1,9/27/18,31,18,4,6,4,2,435;1293,50;68,6;19,44;112,-1;-1
2007,ICLR,2019,Area Attention,Yang Li;Lukasz Kaiser;Samy Bengio;Si Si,liyang@google.com;lukaszkaiser@google.com;bengio@google.com;sisidaisy@google.com,6;5;5,4;5;4,Reject,12,10,0,yes,9/27/18,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,3,9/27/18,5,4,1,3,15,4,2196;22306;26187;950,37;75;332;52,19;24;67;15,235;3854;3450;99,-1;-1
2008,ICLR,2019,The Conditional Entropy Bottleneck,Ian Fischer,iansf@google.com,6;6;2,3;3;4,Reject,0,20,0,yes,9/27/18,Google,-1,-1,4;8,9/27/18,16,7,5,0,0,6,2635,16,12,356,-1
2009,ICLR,2019,State-Denoised Recurrent Neural Networks,Michael C. Mozer;Denis Kazakov;Robert V. Lindsey,mozer@colorado.edu;denis.kazakov@colorado.edu;rob@imagen.ai,6;5;5,4;3;4,Reject,0,3,0,yes,9/27/18,"University of Colorado, Boulder;University of Colorado, Boulder;",44;44;-1,100;100;-1,8,5/22/18,2,2,0,0,4,0,6803;17;85,237;12;23,43;2;5,514;3;0,-1;-1
2010,ICLR,2019,NETWORK COMPRESSION USING CORRELATION ANALYSIS OF LAYER RESPONSES,Xavier Suau;Luca Zappella;Nicholas Apostoloff,xsuaucuadros@apple.com;lzappella@apple.com;napostoloff@apple.com,6;5;5,3;4;5,Reject,0,7,1,yes,9/27/18,Apple;Apple;Apple,-1;-1;-1,-1;-1;-1,1,9/27/18,14,5,1,0,2,1,151;577;536,15;21;21,7;13;12,9;68;22,-1;-1
2011,ICLR,2019,LIT: Block-wise Intermediate Representation Training for Model Compression,Animesh Koratana*;Daniel Kang*;Peter Bailis;Matei Zaharia,koratana@stanford.edu;ddkang@stanford.edu;pbailis@cs.stanford.edu;matei@cs.stanford.edu,5;6;6,4;4;3,Reject,0,7,0,yes,9/27/18,Stanford University;Stanford University;Stanford University;Stanford University,4;4;4;4,3;3;3;3,5,9/27/18,13,11,2,0,13,3,21;622;2182;28093,5;33;119;133,3;11;24;37,2;66;227;3453,-1;-1
2012,ICLR,2019,NLProlog: Reasoning with Weak Unification for Natural Language Question Answering,Leon Weber;Pasquale Minervini;Ulf Leser;Tim Rocktäschel,leonweber@posteo.de;p.minervini@gmail.com;leser@informatik.hu-berlin.de;tim.rocktaeschel@gmail.com,5;7;7,3;3;4,Reject,0,5,0,yes,9/27/18,Humboldt Universität Berlin;University College London;Humboldt Universität Berlin;Facebook AI Research,261;50;261;-1,62;16;62;-1,3,9/27/18,10,2,4,0,0,2,242;581;5214;2243,7;41;283;48,3;9;34;21,22;167;508;278,-1;-1
2013,ICLR,2019,Revisiting Reweighted Wake-Sleep,Tuan Anh Le;Adam R. Kosiorek;N. Siddharth;Yee Whye Teh;Frank Wood,tuananh@robots.ox.ac.uk;adamk@robots.ox.ac.uk;nsid@robots.ox.ac.uk;y.w.teh@stats.ox.ac.uk;fwood@cs.ubc.ca,6;6;5,4;3;3,Reject,0,10,0,yes,9/27/18,University of Oxford;University of Oxford;University of Oxford;University of Oxford;University of British Columbia,50;50;50;50;36,1;1;1;1;34,5,9/27/18,10,4,4,0,0,0,1693;332;208;23048;510,141;18;21;249;38,18;9;9;51;10,109;45;15;3207;62,-1;-1
2014,ICLR,2019,Looking for ELMo's friends: Sentence-Level Pretraining Beyond Language Modeling,Samuel R. Bowman;Ellie Pavlick;Edouard Grave;Benjamin Van Durme;Alex Wang;Jan Hula;Patrick Xia;Raghavendra Pappagari;R. Thomas McCoy;Roma Patel;Najoung Kim;Ian Tenney;Yinghui Huang;Katherin Yu;Shuning Jin;Berlin Chen,bowman@nyu.edu;ellie_pavlick@brown.edu;egrave@fb.com;vandurme@cs.jhu.edu;alexwang@nyu.edu;jan.hula21@gmail.com;paxia@jhu.edu;raghu1991.p@gmail.com;tom.mccoy@jhu.edu;romapatel@brown.edu;n.kim@jhu.edu;iftenney@google.com;huangyi@us.ibm.com;yukatherin@fb.com;jinxx596@d.umn.edu;bchen6@swarthmore.edu,5;7;8,3;4;4,Reject,0,3,2,yes,9/27/18,"New York University;Brown University;Facebook;Johns Hopkins University;New York University;;Johns Hopkins University;Johns Hopkins University;Johns Hopkins University;Brown University;Johns Hopkins University;Google;International Business Machines;Facebook;University of Minnesota, Minneapolis;Swarthmore College",26;65;-1;72;26;-1;72;72;72;65;72;-1;-1;-1;57;478,27;50;-1;13;27;-1;13;13;13;50;13;-1;-1;-1;56;1103,3,9/27/18,15,4,1,0,0,1,5878;1274;7543;4928;1171;46;350;80;392;99;227;499;220;39;42;1674,80;61;55;193;27;28;15;8;17;10;16;28;21;3;5;238,27;15;23;38;10;3;8;5;9;5;4;8;6;2;3;22,1210;228;1128;548;227;7;28;10;51;11;20;43;12;7;7;86,-1;-1
2015,ICLR,2019,Estimating Information Flow in DNNs,Ziv Goldfeld;Ewout van den Berg;Kristjan Greenewald;Brian Kingsbury;Igor Melnyk;Nam Nguyen;Yury Polyanskiy,zivg@mit.edu;evandenberg@us.ibm.com;kristjan.h.greenewald@ibm.com;bedk@us.ibm.com;igor.melnyk@ibm.com;nnguyen@us.ibm.com;yp@mit.edu,7;7;4,4;4;5,Reject,0,9,0,yes,9/27/18,Massachusetts Institute of Technology;International Business Machines;International Business Machines;International Business Machines;International Business Machines;International Business Machines;Massachusetts Institute of Technology,2;-1;-1;-1;-1;-1;2,5;-1;-1;-1;-1;-1;5,,9/27/18,1,0,0,0,0,0,276;2451;160;13414;248;1279;3765,47;36;21;131;33;68;139,10;14;8;39;10;16;24,14;271;9;816;17;166;510,-1;-1
2016,ICLR,2019,Approximation and non-parametric estimation of ResNet-type convolutional neural networks via block-sparse fully-connected neural networks,Kenta Oono;Taiji Suzuki,k.oono.delta@gmail.com;taiji@mist.i.u-tokyo.ac.jp,4;6;4,4;3;3,Reject,0,6,0,yes,9/27/18,The University of Tokyo;The University of Tokyo,54;54,45;45,1,9/27/18,5,4,1,0,6,0,657;2256,11;177,5;25,68;235,-1;-1
2017,ICLR,2019,Learning to Reinforcement Learn by Imitation,Rosen Kralev;Russell Mendonca;Alvin Zhang;Tianhe Yu;Abhishek Gupta;Pieter Abbeel;Sergey Levine;Chelsea Finn,rdkralev@gmail.com;russellm@berkeley.edu;alvinz@berkeley.edu;tianheyu927@gmail.com;abhigupta@berkeley.edu;pabbeel@cs.berkeley.edu;svlevine@eecs.berkeley.edu;cbfinn@eecs.berkeley.edu,4;3;2;5,3;2;5;2,Reject,0,0,0,yes,9/27/18,;University of California Berkeley;University of California Berkeley;Stanford University;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,-1;5;5;4;5;5;5;5,-1;18;18;3;18;18;18;18,6,9/27/18,1,1,0,0,0,1,10;90;7;572;229;36452;24330;7689,2;9;4;18;116;433;309;99,1;2;1;8;7;94;73;33,1;6;1;68;22;4390;3155;1026,-1;-1
2018,ICLR,2019,Non-Synergistic Variational Autoencoders,Gonzalo Barrientos;Sten Sootla,gonzalo.ayquipa.16@ucl.ac.uk;sten.sootla.17@ucl.ac.uk,3;4;3,4;3;5,Reject,0,0,0,yes,9/27/18,University College London;University College London,50;50,16;16,5;1,9/27/18,0,0,0,0,0,0,128;5,16;2,2;1,16;0,-1;-1
2019,ICLR,2019,On the Trajectory of Stochastic Gradient Descent in the Information Plane,Emilio Rafael Balda;Arash Behboodi;Rudolf Mathar,emilio.balda@ti.rwth-aachen.de;arash.behboodi@ti.rwth-aachen.de;mathar@ti.rwth-aachen.de,4;6;2,3;4;4,Reject,0,5,0,yes,9/27/18,RWTH Aachen University;RWTH Aachen University;RWTH Aachen University,99;99;99,79;79;79,,9/27/18,1,0,0,0,0,0,40;309;3489,16;66;472,4;8;28,2;15;187,-1;-1
2020,ICLR,2019,Adversarial Vulnerability of Neural Networks Increases with Input Dimension,Carl-Johann Simon-Gabriel;Yann Ollivier;Léon Bottou;Bernhard Schölkopf;David Lopez-Paz,cjsimon@tuebingen.mpg.de;yol@fb.com;leon@bottou.org;bs@tuebingen.mpg.de;dlp@fb.com,6;4;9;5,4;5;4;5,Reject,0,20,2,yes,9/27/18,"Max Planck Institute for Intelligent Systems, Max-Planck Institute;Facebook;Facebook;Max Planck Institute for Intelligent Systems, Max-Planck Institute;Facebook",-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,4;1,2/5/18,36,20,6,2,12,4,314;1568;47558;75101;2393,11;131;167;860;45,7;20;58;119;19,37;191;7054;9901;413,-1;-1
2021,ICLR,2019,CoT: Cooperative Training for Generative Modeling of Discrete Data,Sidi Lu;Lantao Yu;Siyuan Feng;Yaoming Zhu;Weinan Zhang;Yong Yu,steve_lu@apex.sjtu.edu.cn;yulantao@apex.sjtu.edu.cn;siyuanfeng@apex.sjtu.edu;ymzhu@apex.sjtu.edu.cn;wnzhang@apex.sjtu.edu.cn;yyu@apex.sjtu.edu.cn,6;7;7,4;2;2,Reject,9,21,0,yes,9/27/18,Shanghai Jiao Tong University;Shanghai Jiao Tong University;Shanghai Jiao Tong University;Shanghai Jiao Tong University;Shanghai Jiao Tong University;Shanghai Jiao Tong University,52;52;52;52;52;52,188;188;188;188;188;188,5;8,4/11/18,6,1,2,0,21,1,323;1376;699;143;4833;34647,13;34;30;12;205;1553,6;8;13;4;31;86,67;210;30;29;688;2809,-1;-1
2022,ICLR,2019,Direct Optimization through $\arg \max$  for  Discrete Variational Auto-Encoder,Guy Lorberbom;Tamir Hazan,guy_lorber@campus.technion.ac.il;tamir.hazan@technion.ac.il,7;5;7,4;4;4,Reject,0,5,1,yes,9/27/18,Technion;Technion,25;25,327;327,,6/7/18,11,5,9,0,27,2,13;1953,3;72,2;21,2;183,-1;-1
2023,ICLR,2019,Sinkhorn AutoEncoders,Giorgio Patrini;Marcello Carioni;Patrick Forré;Samarth Bhargav;Max Welling;Rianne van den Berg;Tim Genewein;Frank Nielsen,patrinig@hotmail.com;marcello.carioni@uni-graz.at;patrickforre@gmail.com;samarth.bhargav@student.uva.nl;welling.max@gmail.com;riannevdberg@gmail.com;tim.genewein@de.bosch.com;nielsen@lix.polytechnique.fr,7;5;6;7,3;4;3;3,Reject,0,7,0,yes,9/27/18,"University of Amsterdam;Karl-Franzens University Graz;University of Amsterdam;University of Amsterdam;University of California - Irvine;University of Amsterdam;Bosch;Ecole Polytechnique, France",169;478;169;169;35;169;-1;478,59;1103;59;59;99;59;-1;115,5;1,9/27/18,9,0,1,0,47,0,541;78;95;6;26464;992;784;4258,22;14;13;3;269;26;27;330,9;3;6;1;58;7;9;33,103;6;13;0;5073;174;63;300,-1;-1
2024,ICLR,2019,Structured Prediction using cGANs with Fusion Discriminator,Faisal Mahmood;Wenhao Xu;Nicholas J. Durr;Jeremiah W. Johnson;Alan Yuille,faisalm@jhu.edu;wxu47@jhu.edu;ndurr@jhu.edu;jeremiah.johnson@unh.edu;alan.l.yuille@gmail.com,5;3;3,3;4;4,Reject,0,3,0,yes,9/27/18,Johns Hopkins University;Johns Hopkins University;Johns Hopkins University;University of New Hampshire;Johns Hopkins University,72;72;72;261;72,13;13;13;1103;13,5;4;2,9/27/18,2,1,1,0,0,0,269;33;1136;175;32598,48;21;95;31;494,7;4;15;5;80,13;1;40;11;3737,-1;-1
2025,ICLR,2019,Evaluating GANs via Duality,Paulina Grnarova;Kfir Y Levy;Aurelien Lucchi;Nathanael Perraudin;Thomas Hofmann;Andreas Krause,paulina.grnarova@inf.ethz.ch;yehuda.levy@inf.ethz.ch;aurelien.lucchi@inf.ethz.ch;nathanael.perraudin@sdsc.ethz.ch;thomas.hofmann@inf.ethz.ch;krausea@ethz.ch,4;5;3,3;3;4,Reject,0,12,0,yes,9/27/18,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10;10;10;10,10;10;10;10;10;10,5;4,9/27/18,7,4,2,0,8,0,150;402;6789;709;22653;16305,9;19;68;38;173;241,6;11;24;12;52;65,19;69;1108;71;3390;1759,-1;-1
2026,ICLR,2019,Hyper-Regularization: An Adaptive Choice for the Learning Rate in Gradient Descent,Guangzeng Xie;Hao Jin;Dachao Lin;Zhihua Zhang,smsxgz@pku.edu.cn;jin.hao@pku.edu.cn;lindachao@pku.edu.cn;zhzhang@math.pku.edu.cn,4;4;4,5;4;4,Reject,0,5,0,yes,9/27/18,Peking University;Peking University;Peking University;Peking University,24;24;24;24,27;27;27;27,9,9/27/18,0,0,0,0,0,0,6;984;4;5058,6;184;9;407,2;15;1;29,1;57;0;522,-1;-1
2027,ICLR,2019,Wasserstein proximal of GANs,Alex Tong Lin;Wuchen Li;Stanley Osher;Guido Montufar,atlin@math.ucla.edu;wcli@math.ucla.edu;sjo@math.ucla.edu;montufar@math.ucla.edu,3;6;4,5;3;3,Reject,0,5,0,yes,9/27/18,"University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles",20;20;20;20,15;15;15;15,5,9/27/18,14,7,2,0,0,0,151;423;57043;1156,55;57;560;60,7;11;94;14,5;16;5061;76,-1;-1
2028,ICLR,2019,On the Convergence and Robustness of Batch Normalization,Yongqiang Cai;Qianxiao Li;Zuowei Shen,matcyon@nus.edu.sg;liqix@ihpc.a-star.edu.sg;matzuows@nus.edu.sg,4;6;4,5;3;3,Reject,0,9,0,yes,9/27/18,"National University of Singapore;Institute of High Performance Computing, Singapore, A*STAR;National University of Singapore",16;-1;16,22;-1;22,4,9/27/18,5,0,0,0,5,0,117;393;10997,12;29;187,6;9;49,4;38;942,-1;-1
2029,ICLR,2019,Aligning Artificial Neural Networks to the Brain yields Shallow Recurrent Architectures,Jonas Kubilius;Martin Schrimpf;Ha Hong;Najib J. Majaj;Rishi Rajalingham;Elias B. Issa;Kohitij Kar;Pouya Bashivan;Jonathan Prescott-Roy;Kailyn Schmidt;Aran Nayebi;Daniel Bear;Daniel L. K. Yamins;James J. DiCarlo,qbilius@mit.edu;msch@mit.edu;dicarlo@mit.edu,7;7;5,4;4;4,Reject,0,11,0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2,5;5;5,,9/27/18,0,0,0,0,0,0,683;227;1;2401;362;590;562;689;79;299;369;156;582;8116,53;20;13;60;20;23;38;27;3;12;35;18;42;122,13;6;1;17;9;13;12;11;2;7;7;4;9;43,51;24;0;272;12;41;30;61;5;10;31;17;34;554,-1;-1
2030,ICLR,2019,Finding Mixed Nash Equilibria of Generative Adversarial Networks,Ya-Ping Hsieh;Chen Liu;Volkan Cevher,ya-ping.hsieh@epfl.ch;chen.liu@epfl.ch;volkan.cevher@epfl.ch,6;4;5,4;4;5,Reject,0,10,0,yes,9/27/18,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne,478;478;478,38;38;38,5;4;1;9,9/27/18,19,9,5,1,12,2,195;1851;6222,31;278;284,9;21;40,9;111;472,-1;-1
2031,ICLR,2019,Local Stability and Performance of Simple Gradient Penalty $\mu$-Wasserstein GAN,Cheolhyeong Kim;Seungtae Park;Hyung Ju Hwang,tyty4@postech.ac.kr;swash21@postech.ac.kr;hjhwang@postech.ac.kr,6;5;4,4;4;3,Reject,0,6,0,yes,9/27/18,POSTECH;POSTECH;POSTECH,123;123;123,137;137;137,5;1,9/27/18,1,0,1,0,5,0,2;25;446,2;11;75,1;2;13,0;0;39,-1;-1
2032,ICLR,2019,Learning Discriminators as Energy Networks in Adversarial Learning,Pingbo Pan;Yan Yan;Tianbao Yang;Yi Yang,pingbo.pan@student.uts.edu.au;yan.yan-3@student.uts.edu.au;tianbao-yang@uiowa.edu;yi.yang@uts.edu.au,5;5;5,4;5;4,Reject,5,5,0,yes,9/27/18,University of Technology Sydney;University of Technology Sydney;University of Iowa;University of Technology Sydney,106;106;153;106,216;216;223;216,4;2,9/27/18,1,0,0,0,2,0,303;796;3145;11753,12;52;187;321,5;13;28;55,59;93;343;1287,-1;-1
2033,ICLR,2019,Optimistic Acceleration for Optimization,Jun-Kun Wang;Xiaoyun Li;Ping Li,jimwang@gatech.edu;xl374@scarletmail.rutgers.edu;pingli98@gmail.com,5;6;5;4,4;2;4;4,Reject,0,7,0,yes,9/27/18,Georgia Institute of Technology;Rutgers University;Rutgers University New Brunswick,13;34;34,33;172;172,,9/27/18,1,1,0,0,2,0,48;53;5691,11;21;675,4;2;36,2;5;358,m;m
2034,ICLR,2019,Rethinking learning rate schedules for stochastic optimization,Rong Ge;Sham M. Kakade;Rahul Kidambi;Praneeth Netrapalli,rongge@cs.duke.edu;sham@cs.washington.edu;rkidambi@uw.edu;praneeth@microsoft.com,6;4;6,4;4;4,Reject,0,7,0,yes,9/27/18,"Duke University;University of Washington;University of Washington, Seattle;Microsoft",44;6;6;-1,17;25;25;-1,9,9/27/18,6,3,1,1,0,0,2049;13419;247;3241,77;196;22;72,20;57;9;27,137;1960;22;414,-1;-1
2035,ICLR,2019,On the Ineffectiveness of Variance Reduced Optimization for Deep Learning,Aaron Defazio,aaron.defazio@gmail.com,5;6;5,5;3;4,Reject,0,9,0,yes,9/27/18,Facebook,-1,-1,9,9/27/18,27,21,1,0,3,3,1288,28,9,265,-1
2036,ICLR,2019,EDDI: Efficient Dynamic Discovery of High-Value Information with Partial VAE,Chao Ma;Sebastian Tschiatschek;Konstantina Palla;Jose Miguel Hernandez Lobato;Sebastian Nowozin;Cheng Zhang,cm905@cam.ac.uk;sebastian.tschiatschek@microsoft.com;konstantina.palla@microsoft.com;jmh233@cam.ac.uk;sebastian.nowozin@microsoft.com;cheng.zhang@microsoft.com,6;5;6,2;4;4,Reject,0,5,0,yes,9/27/18,University of Cambridge;Microsoft;Microsoft;University of Cambridge;Microsoft;Microsoft,71;-1;-1;71;-1;-1,2;-1;-1;2;-1;-1,5;11,9/27/18,28,12,12,2,4,0,342;772;303;3737;6872;156,63;64;15;113;134;111,10;13;8;28;39;7,10;50;45;420;931;5,-1;-1
2037,ICLR,2019,"Search-Guided, Lightly-supervised Training of  Structured Prediction Energy Networks",Amirmohammad Rooshenas;Dongxu Zhang;Gopal Sharma;Andrew McCallum,pedram@cs.umass.edu;dongxuzhang@cs.umass.edu;gopalsharma@cs.umass.edu;mccallum@cs.umass.edu,5;7;4,4;4;4,Reject,0,3,0,yes,9/27/18,"University of Massachusetts, Amherst;University of Massachusetts, Amherst;University of Massachusetts, Amherst;University of Massachusetts, Amherst",30;30;30;30,191;191;191;191,,9/27/18,5,0,0,0,5,0,242;208;111;43661,12;14;28;434,5;6;5;96,34;32;9;4704,-1;-1
2038,ICLR,2019,Distinguishability of Adversarial Examples,Yi Qin;Ryan Hunt;Chuan Yue,yiqin@mines.edu;ryhunt@mines.edu;chuanyue@mines.edu,4;4;4,4;5;4,Reject,1,2,0,yes,9/27/18,Colorado School of Mines;Colorado School of Mines;Colorado School of Mines,169;169;169,268;268;268,4;2,9/27/18,0,0,0,0,0,0,1949;193;621,244;25;73,22;4;13,80;6;37,-1;-1
2039,ICLR,2019,On the Geometry of Adversarial Examples,Marc Khoury;Dylan Hadfield-Menell,khoury@eecs.berkeley.edu;dhm@berkeley.edu,5;3;6,3;4;4,Reject,0,13,0,yes,9/27/18,University of California Berkeley;University of California Berkeley,5;5,18;18,4;1,9/27/18,18,13,2,2,6,2,179;546,23;33,7;10,23;29,-1;-1
2040,ICLR,2019,Partially Mutual Exclusive Softmax for Positive and Unlabeled data,Ugo Tanielian;Flavian vasile;Mike Gartrell,u.tanielian@criteo.com;f.vasile@criteo.com;m.gartrell@criteo.com,5;4;5,4;4;4,Reject,0,2,0,yes,9/27/18,Criteo;Criteo;Criteo,-1;-1;-1,-1;-1;-1,3;4,9/27/18,0,0,0,0,0,0,15;296;709,10;28;47,2;7;11,3;23;49,-1;-1
2041,ICLR,2019,Learning Global Additive Explanations for Neural Nets Using Model Distillation,Sarah Tan;Rich Caruana;Giles Hooker;Paul Koch;Albert Gordo,ht395@cornell.edu;rcaruana@microsoft.com;gjh27@cornell.edu;paulkoch@microsoft.com;albert.gordo.s@gmail.com,6;4;6,5;5;4,Reject,0,7,0,yes,9/27/18,Cornell University;Microsoft;Cornell University;Microsoft;Facebook,7;-1;7;-1;-1,19;-1;19;-1;-1,,1/26/18,23,9,9,0,0,2,144;16740;502;8295;2702,20;147;20;201;44,6;48;7;49;19,11;1359;70;1038;577,-1;-1
2042,ICLR,2019,Predicted Variables in Programming,Victor Carbune;Thierry Coppey;Alexander Daryin;Thomas Deselaers;Nikhil Sarda;Jay Yagnik,victor.carbune@gmail.com;thierryc@google.com;shurick@google.com;deselaers@google.com;nikhilsarda@google.com;jyagnik@google.com,5;5;7,3;3;3,Reject,0,3,0,yes,9/27/18,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,,9/27/18,0,0,0,0,0,0,91;24;92;6671;196;1253,72;10;39;151;9;27,2;2;4;38;4;11,9;1;1;673;18;121,-1;-1
2043,ICLR,2019,Learning Abstract Models for Long-Horizon Exploration,Evan Zheran Liu;Ramtin Keramati;Sudarshan Seshadri;Kelvin Guu;Panupong Pasupat;Emma Brunskill;Percy Liang,evanliu@cs.stanford.edu;keramati@stanford.edu;ssesha@stanford.edu;kguu@stanford.edu;ppasupat@cs.stanford.edu;ebrun@cs.stanford.edu;pliang@cs.stanford.edu,6;5;4,2;4;4,Reject,0,10,0,yes,9/27/18,Stanford University;Stanford University;Stanford University;Stanford University;Stanford University;Stanford University;Stanford University,4;4;4;4;4;4;4,3;3;3;3;3;3;3,,9/27/18,4,2,1,1,0,1,122;27;101;565;664;3097;12558,9;11;10;12;22;150;144,4;3;4;8;14;28;47,23;3;4;89;106;293;2038,-1;-1
2044,ICLR,2019,Domain Adaptation for Structured Output via Disentangled Patch Representations,Yi-Hsuan Tsai;Kihyuk Sohn;Samuel Schulter;Manmohan Chandraker,wasidennis@gmail.com;kihyuk.sohn@gmail.com;samuel@nec-labs.com;manu@nec-labs.com,5;7;5,5;5;4,Reject,2,7,1,yes,9/27/18,NEC-Labs;NEC-Labs;NEC-Labs;NEC-Labs,-1;-1;-1;-1,-1;-1;-1;-1,4;2,9/27/18,10,4,2,1,0,1,1140;3562;1156;3327,60;49;34;93,13;20;15;33,220;567;183;398,-1;-1
2045,ICLR,2019,Meta-Learning Neural Bloom Filters,Jack W Rae;Sergey Bartunov;Timothy P Lillicrap,jwrae@google.com;bartunov@google.com;countzero@google.com,7;6;3,3;4;1,Reject,0,10,0,yes,9/27/18,Google;Google;Google,-1;-1;-1,-1;-1;-1,6,9/27/18,9,2,3,0,0,1,631;1111;849,19;13;31,10;8;12,74;118;75,-1;-1
2046,ICLR,2019,The Universal Approximation Power of Finite-Width Deep ReLU Networks,Dmytro Perekrestenko;Philipp Grohs;Dennis Elbrächter;Helmut Bölcskei,pdmytro@nari.ee.ethz.ch;philipp.grohs@univie.ac.at;dennis.elbraechter@univie.ac.at;boelcskei@nari.ee.ethz.ch,5;5;6,3;4;3,Reject,0,5,0,yes,9/27/18,Swiss Federal Institute of Technology;University of Vienna;University of Vienna;Swiss Federal Institute of Technology,10;199;199;10,10;164;164;10,1,6/5/18,14,6,1,0,3,0,140;1230;44;11809,5;105;5;189,4;20;3;41,10;69;1;1008,-1;-1
2047,ICLR,2019,Transfer Learning for Related Reinforcement Learning Tasks via Image-to-Image Translation,Shani Gamrian;Yoav Goldberg,gamrianshani@gmail.com;yoav.goldberg@gmail.com,4;7;7,4;3;3,Reject,0,5,0,yes,9/27/18,Bar Ilan University;Bar-Ilan University,95;95,456;456,5;4;6,5/31/18,21,13,6,1,30,1,21;9461,1;116,1;39,1;1013,-1;-1
2048,ICLR,2019,Guiding Physical Intuition with Neural Stethoscopes,Fabian Fuchs;Oliver Groth;Adam Kosiorek;Alex Bewley;Markus Wulfmeier;Andrea Vedaldi;Ingmar Posner,fabian@robots.ox.ac.uk;ogroth@robots.ox.ac.uk;adamk@robots.ox.ac.uk;alex.bewley@gmail.com;m.wulfmeier@gmail.com;vedaldi@robots.ox.ac.uk;ingmar@robots.ox.ac.uk,6;4;7,4;3;3,Reject,0,3,0,yes,9/27/18,University of Oxford;University of Oxford;University of Oxford;Wayve Technologies;Google;University of Oxford;University of Oxford,50;50;50;-1;-1;50;50,1;1;1;-1;-1;1;1,4,9/27/18,0,0,0,0,0,0,58;1634;332;1118;599;33608;2453,10;12;18;31;27;201;100,3;4;9;13;12;62;28,3;269;45;179;48;4622;157,-1;-1
2049,ICLR,2019,An Adversarial Learning Framework for a Persona-based Multi-turn Dialogue Model,Oluwatobi O. Olabiyi;Anish Khazane;Alan Salimov;Erik T.Mueller,oluwatobi.olabiyi@capitalone.com;anish.khazan@capitalone.com;alan.salimov@capitalone.com;erik.mueller@capitalone.com,5;4;6,4;4;3,Reject,0,6,0,yes,9/27/18,Capital One Bank;Capital One Bank;Capital One Bank;Capital One Bank,-1;-1;-1;-1,-1;-1;-1;-1,4,9/27/18,7,7,3,1,0,0,380;26;21;117,49;6;3;126,11;3;2;5,36;2;2;4,-1;-1
2050,ICLR,2019,Adaptive Sample-space & Adaptive Probability coding: a neural-network based approach for compression,Ken Nakanishi;Shin-ichi Maeda;Takeru Miyato;Masanori Koyama,ikyhn1.ken.n@gmail.com;ichi@preferred.jp;miyato@preferred.jp;masomatics@preferred.jp,5;7;5,4;3;4,Reject,0,5,0,yes,9/27/18,"The University of Tokyo;Preferred Networks, Inc.;Preferred Networks, Inc.;Preferred Networks, Inc.",54;-1;-1;-1,45;-1;-1;-1,,9/27/18,0,0,0,0,0,0,336;1908;2758;2374,35;99;17;35,8;21;10;12,34;253;578;482,-1;-1
2051,ICLR,2019,Seq2Slate: Re-ranking and Slate Optimization with RNNs,Irwan Bello;Sayali Kulkarni;Sagar Jain;Craig Boutilier;Ed Chi;Elad Eban;Xiyang Luo;Alan Mackey;Ofer Meshi,ibello@google.com;sayali@google.com;sagarj@google.com;cboutilier@google.com;edchi@google.com;elade@google.com;xyluo@google.com;mackeya@google.com;meshi@google.com,7;6;6,5;4;4,Reject,0,5,0,yes,9/27/18,Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1,,9/27/18,18,10,10,1,8,3,514;479;828;13828;10211;557;143;107;436,9;8;65;276;209;21;22;8;28,5;4;16;55;47;10;8;4;12,94;63;54;1715;901;74;14;11;48,-1;-1
2052,ICLR,2019,An Efficient and Margin-Approaching Zero-Confidence Adversarial Attack,Yang Zhang;Shiyu Chang;Mo Yu;Kaizhi Qian,yang.zhang2@ibm.com;shiyu.chang@ibm.com;yum@us.ibm.com;kqian3@illinois.edu,6;5;5,5;3;4,Reject,8,15,0,yes,9/27/18,"International Business Machines;International Business Machines;International Business Machines;University of Illinois, Urbana Champaign",-1;-1;-1;3,-1;-1;-1;37,4,9/27/18,0,0,0,0,0,0,532;2903;3506;95,113;111;71;11,10;28;26;4,54;386;450;9,-1;-1
2053,ICLR,2019,Neural Causal Discovery with Learnable Input Noise,Tailin Wu;Thomas Breuel;Jan Kautz,tailin@mit.edu;tbreuel@nvidia.com;jkautz@nvidia.com,4;4;8,5;4;4,Reject,0,3,0,yes,9/27/18,Massachusetts Institute of Technology;NVIDIA;NVIDIA,2;-1;-1,5;-1;-1,,9/27/18,0,0,0,0,0,0,189;1;13807,20;5;301,8;1;57,16;0;1866,-1;-1
2054,ICLR,2019,Dense Morphological Network: An Universal Function Approximator,Ranjan Mondal;Sanchayan Santra;Bhabatosh Chanda,ranjan.rev@gmail.com;sanchayan_r@isical.ac.in;chanda@isical.ac.in,5;5;5,3;4;5,Reject,2,9,0,yes,9/27/18,"Indian Statistical Institute, Kolkata;Indian Statistical Institute, Kolkata;Indian Statistical Institute, Kolkata",478;478;478,1103;1103;1103,,9/27/18,6,3,1,0,4,0,22;48;2280,7;12;223,3;4;24,0;4;135,-1;-1
2055,ICLR,2019,Filter Training and Maximum Response: Classification via Discerning,Lei Gu,gul2@uci.edu,2;3;6,1;4;3,Reject,0,0,0,yes,9/27/18,"University of California, Irvine",35,99,,9/27/18,0,0,0,0,0,0,248,50,8,13,-1
2056,ICLR,2019,BEHAVIOR MODULE IN NEURAL NETWORKS,Andrey Sakryukin;Yongkang Wong;Mohan S. Kankanhalli,asakryukin@u.nus.edu;yongkang.wong@nus.edu.sg;mohan@comp.nus.edu.sg,3;3;4,4;5;5,Reject,0,0,0,yes,9/27/18,National University of Singapore;National University of Singapore;National University of Singapore,16;16;16,22;22;22,,9/27/18,0,0,0,0,0,0,1;958;8124,3;68;449,1;17;43,0;88;469,-1;-1
2057,ICLR,2019,Surprising Negative Results for Generative  Adversarial Tree Search ,Kamyar Azizzadenesheli;Brandon Yang;Weitang Liu;Emma Brunskill;Zachary Lipton;Animashree Anandkumar,kazizzad@uci.edu;bcyang@stanford.edu;wetliu@ucdavis.edu;ebrun@cs.stanford.edu;zlipton@cmu.edu;anima@caltech.edu,5;5;6,3;4;2,Reject,0,4,0,yes,9/27/18,"University of California, Irvine;Stanford University;University of California, Davis;Stanford University;Carnegie Mellon University;California Institute of Technology",35;4;81;4;1;140,99;3;54;3;24;3,5;4,6/15/18,10,9,4,3,4,2,670;786;13;3097;4731;252,38;13;8;150;97;33,10;8;2;28;28;7,96;101;2;293;431;30,-1;-1
2058,ICLR,2019,Evaluation Methodology for Attacks Against Confidence Thresholding Models,Ian Goodfellow;Yao Qin;David Berthelot,goodfellow@google.com;yaoqin@google.com;dberth@google.com,2;3;4,4;3;4,Reject,0,1,0,yes,9/27/18,Google;Google;Google,-1;-1;-1,-1;-1;-1,4,9/27/18,13,7,5,0,0,4,55185;516;1393,90;27;30,56;2;10,9299;67;223,-1;-1
2059,ICLR,2019,Featurized Bidirectional GAN: Adversarial Defense via Adversarially Learned Semantic Inference,Ruying Bao;Sihang Liang;Qingcan Wang,rbao@princeton.edu;sihangl@princeton.edu;qingcanw@princeton.edu,4;3;3,5;4;4,Reject,0,4,0,yes,9/27/18,Princeton University;Princeton University;Princeton University,30;30;30,7;7;7,5;4,5/21/18,5,2,1,0,8,1,5;204;56,1;5;9,1;3;4,1;7;5,-1;-1
2060,ICLR,2019,ATTACK GRAPH CONVOLUTIONAL NETWORKS BY ADDING FAKE NODES,Xiaoyun Wang;Joe Eaton;Cho-Jui Hsieh;Felix Wu,xiywang@ucdavis.edu;featon@nvidia.com;chohsieh@ucdavis.edu;sfwu@ucdavis.edu,4;3;3,3;4;2,Reject,0,0,0,yes,9/27/18,"University of California, Davis;NVIDIA;University of California, Davis;University of California, Davis",81;-1;81;81,54;-1;54;54,4;10,9/27/18,12,9,3,0,13,1,4499;96;12483;3604,168;15;168;210,22;4;40;33,414;5;1722;230,-1;-1
2061,ICLR,2019,Stochastic Quantized Activation: To prevent Overfitting in Fast Adversarial Training,Wonjun Yoon;Jisuk Park;Daeshik Kim,wonjun.yoon@kaist.ac.kr;ssuk30@kaist.ac.kr;daeshik@kaist.ac.kr,4;5;4,3;5;4,Reject,0,0,0,yes,9/27/18,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,20;20;20,95;95;95,4,9/27/18,1,1,0,0,0,0,53;20;3924,13;6;159,3;2;28,3;0;314,-1;-1
2062,ICLR,2019,BlackMarks: Black-box Multi-bit Watermarking for Deep Neural Networks,Huili Chen;Bita Darvish Rouhani;Farinaz Koushanfar,huc044@ucsd.edu;bita@ucsd.edu;farinaz@ucsd.edu,5;4;4,3;4;4,Reject,0,0,0,yes,9/27/18,"University of California, San Diego;University of California, San Diego;University of California, San Diego",11;11;11,31;31;31,4,9/27/18,3,1,1,0,0,0,744;376;9682,68;47;265,14;10;46,55;28;777,-1;-1
2063,ICLR,2019,Purchase as Reward : Session-based  Recommendation by Imagination Reconstruction,Qibing Li;Xiaolin Zheng,qblee@zju.edu.cn;xlzheng@zju.edu.cn,5;6;5,3;2;5,Reject,0,3,0,yes,9/27/18,Zhejiang University;Zhejiang University,57;57,177;177,,9/27/18,0,0,0,0,0,0,364;714,57;106,11;14,17;55,-1;-1
2064,ICLR,2019,Optimal Attacks against Multiple Classifiers,Juan C. Perdomo;Yaron Singer,jcperdomo@berkeley.edu;yaron@seas.harvard.edu,5;6;4;6,4;3;4;4,Reject,0,5,0,yes,9/27/18,University of California Berkeley;Harvard University,5;39,18;6,4,9/27/18,2,2,0,0,0,1,13;1663,13;77,2;23,2;162,-1;-1
2065,ICLR,2019,Label Propagation Networks,Kojin Oshiba;Nir Rosenfeld;Amir Globerson,kojinoshiba@college.harvard.edu;nirr@g.harvard.edu;amir.globerson@gmail.com,5;5;6,4;2;4,Reject,5,4,0,yes,9/27/18,Harvard University;Harvard University;Tel Aviv University,39;39;37,6;6;217,10,9/27/18,1,0,1,0,0,0,3;80;4456,3;16;124,1;5;32,1;4;576,-1;-1
2066,ICLR,2019,Prob2Vec: Mathematical Semantic Embedding for Problem Retrieval in Adaptive Tutoring,Du Su;Ali Yekkehkhany;Yi Lu;Wenmiao Lu,dusu3@illinois.edu;yekkehk2@illinois.edu;yilu4@illinois.edu;wenmiao.lu@gmail.com,3;5;4,3;3;4,Reject,2,5,0,yes,9/27/18,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;",3;3;3;-1,37;37;37;-1,,9/27/18,0,0,0,0,0,0,8;103;374;609,13;11;57;31,2;5;10;11,0;11;23;65,-1;-1
2067,ICLR,2019,Visualizing and Understanding the Semantics of Embedding Spaces via Algebraic Formulae,Piero Molino;Yang Wang;Jiawei Zhang,piero@uber.com;gnavvy@uber.com;rivulet.zhang@gmail.com,3;3;4,4;3;3,Reject,0,6,0,yes,9/27/18,Uber;Uber;Purdue University,-1;-1;26,-1;-1;60,3,9/27/18,4,0,0,0,4,0,480;12988;122,33;1175;16,8;55;4,59;982;18,-1;-1
2068,ICLR,2019,Cross-Entropy Loss Leads To Poor Margins,Kamil Nar;Orhan Ocal;S. Shankar Sastry;Kannan Ramchandran,nar@berkeley.edu;ocal@eecs.berkeley.edu;sastry@eecs.berkeley.edu;kannanr@eecs.berkeley.edu,3;4;5;8;5,4;5;4;3;4,Reject,1,8,0,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5,18;18;18;18,4,9/27/18,3,2,0,1,0,1,36;6;12581;23791,11;6;230;617,4;2;34;78,5;1;1179;2312,-1;-1
2069,ICLR,2019,A Case for Object Compositionality in Deep Generative Models of Images,Sjoerd van Steenkiste;Karol Kurach;Sylvain Gelly,sjoerd@idsia.ch;kkurach@gmail.com;sylvain.gelly@gmail.com,5;4;6,5;5;4,Reject,0,6,0,yes,9/27/18,IDSIA;Google;Google,-1;-1;-1,-1;-1;-1,5,9/27/18,8,5,2,0,0,0,256;1160;3525,12;28;112,6;11;25,29;129;455,-1;-1
2070,ICLR,2019,Object-Oriented Model Learning through Multi-Level Abstraction,Guangxiang Zhu;Jianhao Wang;ZhiZhou Ren;Chongjie Zhang,guangxiangzhu@outlook.com;jh-wang15@mails.tsinghua.edu.cn;rzz16@mails.tsinghua.edu.cn;chongjie@tsinghua.edu.cn,4;4;6,4;3;3,Reject,0,9,0,yes,9/27/18,Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University,8;8;8;8,30;30;30;30,8,9/27/18,1,0,0,0,0,0,93;187;11;537,21;19;11;59,6;4;2;12,4;9;0;44,-1;-1
2071,ICLR,2019,IB-GAN: Disentangled Representation Learning with Information Bottleneck GAN,Insu Jeon;Wonkwang Lee;Gunhee Kim,isjeon@vision.snu.ac.kr;wonkwang.lee.94@gmail.com;gunhee@snu.ac.kr,7;7;4,3;4;4,Reject,0,7,1,yes,9/27/18,Seoul National University;Hanyang University;Seoul National University,41;228;41,74;377;74,5,9/27/18,10,8,4,1,0,2,376;13;1981,74;2;85,10;2;23,23;2;250,-1;-1
2072,ICLR,2019,ACE: Artificial Checkerboard Enhancer to Induce and Evade Adversarial Attacks,Jisung Hwang;Younghoon Kim;Sanghyuk Chun;Jaejun Yoo;Ji-Hoon Kim;Dongyoon Han;Jung-Woo Ha,jeshwang92@uchicago.edu;snu13dlx@snu.ac.kr;sanghyuk.c@navercorp.com;jaejun.yoo@navercorp.com;genesis.kim@navercorp.com;dongyoon.han@navercorp.com;jungwoo.ha@navercorp.com,4;4;6,2;3;1,Reject,0,6,0,yes,9/27/18,University of Chicago;Seoul National University;NAVER;NAVER;NAVER;NAVER;NAVER,48;41;-1;-1;-1;-1;-1,9;74;-1;-1;-1;-1;-1,4;2,9/27/18,0,0,0,0,0,0,32;150;139;76;639;703;1907,20;54;17;20;78;23;67,3;6;4;6;10;8;15,3;8;40;9;19;125;368,-1;-1
2073,ICLR,2019,Second-Order Adversarial Attack and Certifiable Robustness,Bai Li;Changyou Chen;Wenlin Wang;Lawrence Carin,bai.li@duke.edu;cchangyou@gmail.com;wenlin.wang@duke.edu;lcarin@duke.edu,4;5;3,5;3;5,Reject,0,6,0,yes,9/27/18,"Duke University;State University of New York, Buffalo;Duke University;Duke University",44;81;44;44,17;270;17;17,4;1,9/10/18,46,22,21,1,3,2,-1;1797;838;19178,-1;94;41;819,-1;24;15;65,0;244;72;1986,-1;-1
2074,ICLR,2019,Learning Heuristics for Automated Reasoning through Reinforcement Learning,Gil Lederman;Markus N. Rabe;Edward A. Lee;Sanjit A. Seshia,gilled@berkeley.edu;markus.norman.rabe@gmail.com;eal@berkeley.edu;sseshia@eecs.berkeley.edu,5;6;7,3;4;4,Reject,0,6,0,yes,9/27/18,University of California Berkeley;Google;University of California Berkeley;University of California Berkeley,5;-1;5;5,18;-1;18;18,,7/20/18,20,9,4,1,21,1,28;629;26424;9828,6;38;575;317,2;14;66;49,1;80;2007;816,-1;-1
2075,ICLR,2019,Strength in Numbers: Trading-off Robustness and Computation via Adversarially-Trained Ensembles,Edward Grefenstette;Robert Stanforth;Brendan O'Donoghue;Jonathan Uesato;Grzegorz Swirszcz;Pushmeet Kohli,etg@google.com;stanforth@google.com;bodonoghue@google.com;juesato@google.com;swirszcz@google.com;pushmeet@google.com,5;6;4,4;3;3,Reject,0,5,0,yes,9/27/18,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,4,9/27/18,3,0,3,0,34,0,6981;484;2370;895;629;22034,57;18;30;17;50;313,25;9;16;11;15;69,836;57;295;110;53;2746,-1;-1
2076,ICLR,2019,Explaining Adversarial Examples with Knowledge Representation,Xingyu Zhou;Tengyu Ma;Huahong Zhang,xingyu.zhou@vanderbilt.edu;tengyu.ma@vanderbilt.edu;huahong.zhang@vanderbilt.edu,3;3;2,4;2;5,Reject,0,0,0,yes,9/27/18,Vanderbilt University;Vanderbilt University;Vanderbilt University,228;228;228,105;105;105,4,9/27/18,1,0,0,0,0,0,62;3818;9,14;87;7,3;32;2,3;491;0,-1;-1
2077,ICLR,2019,Cutting Down Training Memory by Re-fowarding,Jianwei Feng;Dong Huang,jfeng1@andrew.cmu.edu;donghuang@cmu.edu,6;4;4;6,3;3;2;3,Reject,0,6,0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University,1;1,24;24,10,7/31/18,3,1,0,0,3,0,198;4283,24;445,8;30,7;261,-1;-1
2078,ICLR,2019,Neural Networks with Structural Resistance to Adversarial Attacks,Luca de Alfaro,luca@ucsc.edu,7;5;5,4;3;3,Reject,2,1,0,yes,9/27/18,University of Southern California,30,66,4,9/25/18,2,1,0,0,11,0,6467,162,39,667,-1
2079,ICLR,2019,Controlling Over-generalization and its Effect on Adversarial Examples Detection and Generation,Mahdieh Abbasi;Arezoo Rajabi;Azadeh Sadat Mozafari;Rakesh B. Bobba;Christian Gagné,mahdieh.abbasi.1@ulaval.ca;rajabia@oregonstate.edu;azadeh-sadat.mozafari.1@ulaval.ca;rakesh.bobba@oregonstate.edu;christian.gagne@gel.ulaval.ca,4;4;3,4;5;3,Reject,8,6,0,yes,9/27/18,Laval university;Oregon State University;Laval university;Oregon State University;Laval university,478;76;478;76;478,265;318;265;318;265,4;2;8,8/21/18,6,2,1,0,13,0,68;17;41;1355;1878,10;8;15;71;102,4;3;3;21;19,5;2;4;116;147,-1;-1
2080,ICLR,2019,Robust Determinantal Generative Classifier for Noisy Labels and Adversarial Attacks,Kimin Lee;Sukmin Yun;Kibok Lee;Honglak Lee;Bo Li;Jinwoo Shin,kiminlee@kaist.ac.kr;sm3199@kaist.ac.kr;kibok@umich.edu;honglak@eecs.umich.edu;lxbosky@gmail.com;jinwoos@kaist.ac.kr,7;3;4,5;4;4,Reject,0,8,0,yes,9/27/18,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;University of Michigan;University of Michigan;University of California Berkeley;Korea Advanced Institute of Science and Technology,20;20;8;8;5;20,95;95;21;21;18;95,5;4,9/27/18,5,2,4,0,0,0,480;22;713;23861;498;1681,26;5;35;166;196;184,7;2;12;60;10;18,102;3;107;2806;21;219,-1;-1
2081,ICLR,2019,Single Shot Neural Architecture Search Via Direct Sparse Optimization,Xinbang Zhang;Zehao Huang;Naiyan Wang,xinbang.zhang@nlpr.ia.ac.cn;zehaohuang18@gmail.com;winsty@gmail.com,7;6;6,3;4;3,Reject,0,18,1,yes,9/27/18,"Institute of automation, Chinese academy of science, Chinese Academy of Sciences;;",62;-1;-1,1103;-1;-1,,9/27/18,22,10,8,1,12,2,1332;358;6026,38;17;44,21;7;23,53;60;847,-1;-1
2082,ICLR,2019,How Training Data Affect the Accuracy and Robustness of Neural Networks for Image Classification,Suhua Lei;Huan Zhang;Ke Wang;Zhendong Su,sulei@ucdavis.edu;huan@huan-zhang.com;kewang@visa.com;zhendong.su@inf.ethz.ch,5;4;5,4;4;3,Reject,0,5,0,yes,9/27/18,"University of California, Davis;University of California, Los Angeles;VISA;Swiss Federal Institute of Technology",81;20;-1;10,54;15;-1;10,4,9/27/18,0,0,0,0,0,0,48;2006;596;6993,5;31;154;165,2;19;12;42,8;275;39;733,-1;-1
2083,ICLR,2019,EFFICIENT TWO-STEP ADVERSARIAL DEFENSE FOR DEEP NEURAL NETWORKS,Ting-Jui Chang;Yukun He;Peng Li,tingjui.chang@tamu.edu;dominiche@tamu.edu;pli@tamu.edu,5;6;7,4;3;3,Reject,2,1,0,yes,9/27/18,Texas A&M;Texas A&M;Texas A&M,44;44;44,160;160;160,4,9/27/18,4,0,3,0,8,0,25;150;7714,21;23;760,3;6;41,1;4;533,-1;-1
2084,ICLR,2019,Adaptive Neural Trees,Ryutaro Tanno;Kai Arulkumaran;Daniel C. Alexander;Antonio Criminisi;Aditya Nori,ryutaro.tanno.15@ucl.ac.uk;kailash.arulkumaran13@imperial.ac.uk;d.alexander@ucl.ac.uk;antcrim@microsoft.com;adityan@microsoft.com,6;6;4,4;3;4,Reject,0,7,2,yes,9/27/18,University College London;Imperial College London;University College London;Microsoft;Microsoft,50;72;50;-1;-1,16;8;16;-1;-1,,7/17/18,29,12,10,2,245,3,268;1366;12426;18289;2996,23;20;475;231;97,9;9;58;59;28,16;98;1158;2304;215,-1;-1
2085,ICLR,2019,Graph Convolutional Network with Sequential Attention For Goal-Oriented Dialogue Systems,Suman Banerjee;Mitesh M. Khapra,suman@cse.iitm.ac.in;miteshk@cse.iitm.ac.in,5;6;7,3;4;2,Reject,0,0,5,yes,9/27/18,Indian Institute of Technology Madras;Indian Institute of Technology Madras,153;153,625;625,3;10,9/27/18,2,0,0,0,0,0,10463;1327,313;91,48;18,1076;152,-1;-1
2086,ICLR,2019,Probabilistic Neural-Symbolic Models for Interpretable Visual Question Answering,Ramakrishna Vedantam;Stefan Lee;Marcus Rohrbach;Dhruv Batra;Devi Parikh,vrama@gatech.edu;steflee@gatech.edu;maroffm@gmail.com;dbatra@gatech.edu;parikh@gatech.edu,6;8;7,3;5;3,Reject,0,17,1,yes,9/27/18,Georgia Institute of Technology;Georgia Institute of Technology;Facebook;Georgia Institute of Technology;Georgia Institute of Technology,13;13;-1;13;13,33;33;-1;33;33,,9/27/18,15,6,3,0,4,1,4364;1760;11319;8305;14930,19;31;90;147;185,14;20;43;41;54,825;256;1527;1135;2366,-1;-1
2087,ICLR,2019,Neural separation of observed and unobserved distributions,Tavi Halperin;Ariel Ephrat;Yedid Hoshen,tavihalperin@gmail.com;ariel.ephrat@gmail.com;yedidh@fb.com,5;6;6,4;2;4,Reject,0,4,1,yes,9/27/18,Hebrew University of Jerusalem;Google;Facebook,65;-1;-1,205;-1;-1,,9/27/18,3,3,3,2,0,2,145;360;483,12;12;31,5;9;9,16;38;57,-1;-1
2088,ICLR,2019,BNN+: Improved Binary Network Training,Sajad Darabi;Mouloud Belbahri;Matthieu Courbariaux;Vahid Partovi Nia,sajad.darabi@cs.ucla.edu;belbahrim@dms.umontreal.ca;matthieu.courbariaux@gmail.com;vahid.partovinia@huawei.com,4;8;6,4;4;3,Reject,13,15,1,yes,9/27/18,"University of California, Los Angeles;University of Montreal;;Huawei Technologies Ltd.",20;123;-1;-1,15;108;-1;-1,,9/27/18,26,13,10,3,10,5,57;33;4845;127,13;10;14;37,4;2;10;6,8;5;667;7,-1;-1
2089,ICLR,2019,Dynamic Early Terminating of Multiply Accumulate Operations for Saving Computation Cost in Convolutional Neural Networks,Yu-Yi Su;Yung-Chih Chen;Xiang-Xiu Wu;Shih-Chieh Chang,wwball34@gmail.com;ycchen.phi@gmail.com;jaubau999@gmail.com;scchang@cs.nthu.edu.tw,5;6;6,3;5;3,Reject,0,8,0,yes,9/27/18,"National Tsing Hua University;Department of Computer Science and Engineering, Yuan Ze University;;National Tsing Hua University",199;478;-1;199,323;822;-1;323,,9/27/18,0,0,0,0,0,0,46;283;0;1967,5;17;3;244,2;4;0;24,0;22;0;215,-1;-1
2090,ICLR,2019,SEGEN: SAMPLE-ENSEMBLE GENETIC EVOLUTIONARY NETWORK MODEL,Jiawei Zhang;Limeng Cui;Fisher B. Gouza,jiawei@ifmlab.org;lmcui932@163.com;fisherbgouza@gmail.com,4;5;5,4;2;5,Reject,0,5,0,yes,9/27/18,SUN YAT-SEN UNIVERSITY;163;,478;-1;-1,352;-1;-1,,3/23/18,4,0,2,0,23,1,4564;392;50,228;39;9,36;10;4,454;62;10,-1;-1
2091,ICLR,2019,Local Binary Pattern Networks for Character Recognition,Jeng-Hau Lin;Yunfan Yang;Rajesh K. Gupta;Zhuowen Tu,jel252@ucsd.edu;yuy130@ucsd.edu;rgupta@ucsd.edu;ztu@ucsd.edu,5;6;5,5;4;4,Reject,0,3,0,yes,9/27/18,"University of California, San Diego;University of California, San Diego;University of California, San Diego;University of California, San Diego",11;11;11;11,31;31;31;31,,9/27/18,1,1,0,0,0,0,259;79;3135;15538,16;22;208;163,5;3;30;53,27;1;244;2147,-1;-1
2092,ICLR,2019,A  Model Cortical Network for Spatiotemporal Sequence Learning and Prediction,Jielin Qiu;Ge Huang;Tai Sing Lee,ternence1996@gmail.com;hgesummer@gmail.com;taislee@andrew.cmu.edu,7;7;3,3;3;3,Reject,0,13,0,yes,9/27/18,Shanghai Jiao Tong University;Carnegie Mellon University;Carnegie Mellon University,52;1;1,188;24;24,,9/27/18,0,0,0,0,0,0,4;52;4701,3;16;161,1;5;25,0;2;359,-1;-1
2093,ICLR,2019,Graph Transformer ,Yuan Li;Xiaodan Liang;Zhiting Hu;Yinbo Chen;Eric P. Xing,liyuanchristy@gmail.com;xiaodan1@cs.cmu.edu;zhitingh@cs.cmu.edu;cyb15@mails.tsinghua.edu.cn;epxing@cs.cmu.edu,6;6;6,5;5;3,Reject,3,8,0,yes,9/27/18,Duke University;Carnegie Mellon University;Carnegie Mellon University;Tsinghua University;Carnegie Mellon University,44;1;1;8;1,17;24;24;30;24,6;10,9/27/18,-1,-1,-1,-1,-1,-1,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,0;0;0;0;0,-1;-1
2094,ICLR,2019,Automata Guided Skill Composition,Xiao Li;Yao Ma;Calin Belta,xli87@bu.edu;yaoma@bu.edu;cbelta@bu.edu,5;7;6;5,2;3;4;2,Reject,0,5,1,yes,9/27/18,Boston University;Boston University;Boston University,65;65;65,70;70;70,,9/27/18,0,0,0,0,0,0,4545;489;6296,402;73;304,29;12;41,328;24;365,-1;-1
2095,ICLR,2019,Manifold Mixup: Learning Better Representations by Interpolating Hidden States,Vikas Verma;Alex Lamb;Christopher Beckham;Amir Najafi;Aaron Courville;Ioannis Mitliagkas;Yoshua Bengio,vikasverma.iitm@gmail.com;lambalex@iro.umontreal.ca;christopher.j.beckham@gmail.com;najafy@ce.sharif.edu;aaron.courville@gmail.com;imitliagkas@gmail.com;yoshua.umontreal@gmail.com,8;6;4,2;4;4,Reject,0,33,0,yes,9/27/18,;University of Montreal;Polytechnique Montreal;Sharif University of Technology;University of Montreal;University of Montreal;University of Montreal,-1;123;386;314;123;123;123,-1;108;108;603;108;108;108,4;1;8,6/13/18,18,5,1,0,0,2,250;1148;312;609;59549;1168;201719,45;21;38;87;203;43;807,8;9;9;13;64;18;147,14;182;44;36;7800;190;23989,-1;-1
2096,ICLR,2019,Convolutional Neural Networks combined with Runge-Kutta Methods,Mai Zhu;Bo Chang;Chong Fu,zhumai@stumail.neu.edu.cn;bchang@stat.ubc.ca;fuchong@mail.neu.edu.cn,4;5;6,3;4;3,Reject,0,3,0,yes,9/27/18,Northeastern University;University of British Columbia;Northeastern University,16;36;16,839;34;839,,2/24/18,14,6,7,0,28,1,33;446;1498,5;56;122,2;9;22,2;42;68,-1;-1
2097,ICLR,2019,Exploring and Enhancing the Transferability of Adversarial Examples,Lei Wu;Zhanxing Zhu;Cheng Tai,leiwu@pku.edu.cn;zhanxing.zhu@pku.edu.cn;chengtai@pku.edu.cn,4;6;6,2;3;3,Reject,0,5,0,yes,9/27/18,Peking University;Peking University;Peking University,24;24;24,27;27;27,4,2/27/18,20,9,6,1,5,2,3681;875;503,511;80;23,28;14;9,101;109;46,-1;-1
2098,ICLR,2019,Invariance and Inverse Stability under ReLU,Jens Behrmann;Sören Dittmer;Pascal Fernsel;Peter Maass,jensb@uni-bremen.de;sdittmer@math.uni-bremen.de;pfernsel@math.uni-bremen.de;pmaass@uni-bremen.de,6;6;7,3;4;3,Reject,0,8,0,yes,9/27/18,Universität Bremen;Universität Bremen;Universität Bremen;Universität Bremen,153;153;153;153,291;291;291;291,4,9/27/18,0,0,0,0,0,0,270;23;13;2378,13;9;5;187,5;3;2;28,43;0;0;146,-1;-1
2099,ICLR,2019,Improving Sentence Representations with Multi-view Frameworks,Shuai Tang;Virginia R. de Sa,shuaitang93@ucsd.edu;desa@ucsd.edu,6;5;7,5;4;4,Reject,0,8,0,yes,9/27/18,"University of California, San Diego;University of California, San Diego",11;11,31;31,5,9/27/18,4,0,0,0,4,0,48;1044,15;81,4;16,1;54,-1;-1
2100,ICLR,2019,Principled Deep Neural Network Training through Linear Programming,Daniel Bienstock;Gonzalo Muñoz;Sebastian Pokutta,dano@columbia.edu;gonzalo.munoz@polymtl.ca;sebastian.pokutta@isye.gatech.edu,6;6;8,4;3;3,Reject,0,9,0,yes,9/27/18,Columbia University;Polytechnique Montreal;Georgia Institute of Technology,15;386;13,14;108;33,,9/27/18,5,1,2,0,9,0,3700;137;993,175;24;106,32;7;16,449;5;54,-1;-1
2101,ICLR,2019,The Importance of Norm Regularization in Linear Graph Embedding: Theoretical Analysis and Empirical Demonstration,Yihan Gao;Chao Zhang;Jian Peng;Aditya Parameswaran,gaoyihan@gmail.com;czhang82@illinois.edu;jianpeng@illinois.edu;adityagp@illinois.edu,7;4;4,3;3;4,Reject,0,5,0,yes,9/27/18,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",3;3;3;3,37;37;37;37,10;1;8,2/10/18,0,0,0,0,0,0,144;2166;1316;155,20;255;151;28,6;22;19;5,12;36;45;9,-1;-1
2102,ICLR,2019,Learning to Decompose Compound Questions with Reinforcement Learning,Haihong Yang;Han Wang;Shuang Guo;Wei Zhang;Huajun Chen,capriceyhh@zju.edu.cn;wanghanwh@zju.edu.cn;guoshuang@zju.edu.cn;lantau.zw@alibaba-inc.com;huajunsir@zju.edu.cn,6;5;5,5;3;4,Reject,0,4,0,yes,9/27/18,Zhejiang University;Zhejiang University;Zhejiang University;Tel Aviv University;Zhejiang University,57;57;57;37;57,177;177;177;217;177,10,9/27/18,0,0,0,0,0,0,287;7;19;2814;65,41;19;9;497;24,10;2;2;25;3,11;0;1;191;3,-1;-1
2103,ICLR,2019,Dimension-Free Bounds for Low-Precision Training,Zheng Li;Christopher De Sa,lzlz19971997@gmail.com;cdesa@cs.cornell.edu,6;6;6,3;4;3,Reject,0,4,0,yes,9/27/18,Tsinghua University;Cornell University,8;7,30;19,1;9,9/27/18,0,0,0,0,0,0,203;1472,69;68,8;17,7;158,-1;-1
2104,ICLR,2019,Hint-based Training for Non-Autoregressive Translation,Zhuohan Li;Di He;Fei Tian;Tao Qin;Liwei Wang;Tie-Yan Liu,lizhuohan@pku.edu.cn;dihe@microsoft.com;fetia@microsoft.com;taoqin@microsoft.com;wanglw@cis.pku.edu.cn;tyliu@microsoft.com,6;6;4,4;3;4,Reject,0,8,0,yes,9/27/18,Peking University;Microsoft;Microsoft;Microsoft;Peking University;Microsoft,24;-1;-1;-1;24;-1,27;-1;-1;-1;27;-1,3,9/27/18,25,12,8,1,0,2,100;566;5217;2172;201;13439,13;29;370;43;31;366,7;6;32;8;6;51,10;65;269;319;20;1721,-1;-1
2105,ICLR,2019,Convergence Guarantees for RMSProp and ADAM in Non-Convex Optimization and an Empirical Comparison to Nesterov Acceleration,Soham De;Anirbit Mukherjee;Enayat Ullah,sohamde@cs.umd.edu;amukhe14@jhu.edu;enayat@jhu.edu,5;4;5,3;5;4,Reject,0,7,0,yes,9/27/18,"University of Maryland, College Park;Johns Hopkins University;Johns Hopkins University",12;72;72,69;13;13,1;8,7/18/18,23,7,6,2,6,2,422;244;68,24;11;10,12;4;3,35;34;6,-1;-1
2106,ICLR,2019,Globally Soft Filter Pruning For Efficient Convolutional Neural Networks,Ke Xu;Xiaoyun Wang;Qun Jia;Jianjing An;Dong Wang,17112071@bjtu.edu.cn;16120304@bjtu.edu.cn;16120347@bjtu.edu.cn;16112065@bjtu.edu.cn;wangdong@bjtu.edu.cn,6;5;4,4;4;3,Reject,0,5,0,yes,9/27/18,Beijing jiaotong univercity;Beijing jiaotong univercity;Beijing jiaotong univercity;Beijing jiaotong univercity;Beijing jiaotong univercity,478;478;478;478;478,854;854;854;854;854,,9/27/18,2,0,0,0,0,0,4687;250;10;20;122,467;48;8;4;40,34;8;2;1;5,201;21;0;3;11,-1;-1
2107,ICLR,2019,Quantization for Rapid Deployment of Deep Neural Networks,Jun Haeng Lee;Sangwon Ha;Saerom Choi;Won-Jo Lee;Seungwon Lee,junhaeng2.lee@samsung.com;sw815.ha@samsung.com;sincere.choi@samsung.com;w-j.lee@samsung.com;seungw.lee@samsung.com,5;5;5,3;4;4,Reject,0,5,0,yes,9/27/18,Samsung;Samsung;Samsung;Samsung;Samsung,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,1;2,9/27/18,17,9,4,0,3,3,131;19;17;82;425,47;3;2;7;139,6;2;1;5;10,6;3;3;4;23,-1;-1
2108,ICLR,2019,Discovering Low-Precision Networks Close to Full-Precision Networks for Efficient Embedded Inference,Jeffrey L. McKinstry;Steven K. Esser;Rathinakumar Appuswamy;Deepika Bablani;John V. Arthur;Izzet B. Yildiz;Dharmendra S. Modha,jlmckins@us.ibm.com;sesser@us.ibm.com;rappusw@us.ibm.com;deepika.bablani@ibm.com;arthurjo@us.ibm.com;izzet.burak.yildiz@gmail.com;dmodha@us.ibm.com,4;6;5,5;3;4,Reject,0,6,1,yes,9/27/18,International Business Machines;International Business Machines;International Business Machines;International Business Machines;International Business Machines;;International Business Machines,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,,9/11/18,34,20,6,1,4,5,757;3519;963;176;4647;270;9699,34;25;26;19;45;11;95,11;17;13;7;21;4;37,81;325;93;16;373;21;1063,-1;-1
2109,ICLR,2019,Can I trust you more? Model-Agnostic Hierarchical Explanations,Michael Tsang;Youbang Sun;Dongxu Ren;Beibei Xin;Yan Liu,tsangm@usc.edu;syb98@mail.ustc.edu.cn;rdx15@mails.tsinghua.edu.cn;bxin@usc.edu;yanliu.cs@usc.edu,5;6;6,4;4;4,Reject,0,9,0,yes,9/27/18,University of Southern California;University of Science and Technology of China;Tsinghua University;University of Southern California;University of Southern California,30;478;8;30;30,66;132;30;66;66,,9/27/18,7,2,1,0,0,0,2320;31;170;-1;1241,101;7;24;-1;83,24;2;5;-1;13,198;2;3;0;125,-1;-1
2110,ICLR,2019,Towards a better understanding of Vector Quantized Autoencoders,Aurko Roy;Ashish Vaswani;Niki Parmar;Arvind Neelakantan,aurkor@google.com;avaswani@google.com;nikip@google.com;aneelakantan@google.com,6;7;5;3,3;4;4;4,Reject,0,13,0,yes,9/27/18,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,3;5,5/28/18,32,11,21,0,142,1,1036;11450;9900;1455,21;52;20;24,10;22;12;14,114;2659;2398;152,-1;-1
2111,ICLR,2019,Human-Guided Column Networks: Augmenting Deep Learning with Advice,Mayukh Das;Yang Yu;Devendra Singh Dhami;Gautam Kunapuli;Sriraam Natarajan,mayukh.das1@utdallas.edu;yangyu@hlt.utdallas.edu;devendra.dhami@utdallas.edu;gautam.kunapuli@utdallas.edu;sriraam.natarajan@utdallas.edu,6;4;5,3;4;5,Reject,0,3,0,yes,9/27/18,"University of Texas, Dallas;University of Texas, Dallas;University of Texas, Dallas;University of Texas, Dallas;University of Texas, Dallas",85;85;85;85;85,239;239;239;239;239,,9/27/18,3,0,0,0,3,0,125;579;35;404;1747,32;195;19;49;123,6;13;4;11;21,5;17;4;28;116,-1;-1
2112,ICLR,2019,Weakly-supervised Knowledge Graph Alignment with Adversarial Learning,Meng Qu;Jian Tang;Yoshua Bengio,qumn123@gmail.com;tangjianpku@gmail.com;yoshua.bengio@mila.quebec,5;5;5,3;4;3,Reject,2,0,0,yes,9/25/19,University of Montreal;HEC Montreal;University of Montreal,123;-1;123,108;-1;108,4;1;10,7/6/19,2,0,1,0,0,0,2613;5024;201719,53;154;807,13;33;147,687;438;23989,m;m
2113,ICLR,2019,RelWalk -- A Latent Variable Model Approach to Knowledge Graph Embedding,Danushka Bollegala;Huda Hakami;Yuichi Yoshida;Ken-ichi Kawarabayashi,danushka@liverpool.ac.uk;h.a.hakami@liverpool.ac.uk;yyoshida@nii.ac.jp;k_keniti@nii.ac.jp,6;5;4,4;5;4,Reject,10,5,0,yes,9/27/18,University of Liverpool;University of Liverpool;Meiji University;Meiji University,123;123;478;478,177;177;334;334,3;5;10,9/27/18,0,0,0,0,0,0,2363;23;2982;4807,171;13;263;364,23;3;22;33,224;0;461;543,-1;-1
2114,ICLR,2019,Riemannian TransE: Multi-relational Graph Embedding in Non-Euclidean Space,Atsushi Suzuki;Yosuke Enokida;Kenji Yamanishi,atsushi-suzuki@g.ecc.u-tokyo.ac.jp;xenolay@g.ecc.u-tokyo.ac.jp;yamanishi@mist.i.u-tokyo.ac.jp,5;5;5,2;5;3,Reject,4,3,0,yes,9/27/18,The University of Tokyo;The University of Tokyo;The University of Tokyo,54;54;54,45;45;45,10,9/27/18,3,2,0,0,0,1,3925;12;2404,341;4;146,29;3;21,284;2;145,-1;-1
2115,ICLR,2019,Why Do Neural Response Generation Models Prefer Universal Replies?,Bowen Wu;Nan Jiang;Zhifeng Gao;Zongsheng Wang;Suke Li;Wenge Rong;Baoxun Wang,jasonwbw@yahoo.com;nanjiang@buaa.edu.cn;gao_zhifeng@pku.edu.cn;jasonwang0512@gmail.com;lisuke@ss.pku.edu.cn;w.rong@buaa.edu.cn;baoxun.wang@gmail.com,3;7;1,4;3;5,Reject,2,10,1,yes,9/27/18,Tencent AI Lab;Beihang University;Peking University;;Peking University;Beihang University;,-1;115;24;-1;24;115;-1,-1;658;27;-1;27;658;-1,5,8/28/18,2,1,0,0,0,1,103;-1;953;482;56;656;-1,33;-1;93;35;17;101;-1,6;-1;16;11;4;13;-1,3;0;61;30;4;48;0,-1;-1
2116,ICLR,2019,CGNF: Conditional Graph Neural Fields,Tengfei Ma;Cao Xiao;Junyuan Shang;Jimeng Sun,tengfei.ma1@ibm.com;cxiao@us.ibm.com;sjy1203@pku.edu.cn;jsun@cc.gatech.edu,4;5;5,5;4;5,Reject,0,4,0,yes,9/27/18,International Business Machines;International Business Machines;Peking University;Georgia Institute of Technology,-1;-1;24;13,-1;-1;27;33,10,9/27/18,2,0,0,0,0,0,730;15;109;10320,35;26;14;240,12;2;4;55,85;0;5;785,-1;-1
2117,ICLR,2019,Few-shot Classification on Graphs with Structural Regularized GCNs,Shengzhong Zhang;Ziang Zhou;Zengfeng Huang;Zhongyu Wei,17210980007@fudan.edu.cn;15300180085@fudan.edu.cn;huangzf@fudan.edu.cn;zywei@fudan.edu.cn,4;6;5,4;3;4,Reject,0,5,0,yes,9/27/18,Fudan University;Fudan University;Fudan University;Fudan University,78;78;78;78,116;116;116;116,10;6;8,9/27/18,2,2,0,0,0,0,234;47;359;753,23;8;36;80,7;3;11;14,7;0;32;82,-1;-1
2118,ICLR,2019,Optimization on Multiple Manifolds,Mingyang Yi;Huishuai Zhang;Wei Chen;Zhi-ming Ma;Tie-yan Liu,yimingyang17@mails.ucas.edu.cn;huishuai.zhang@microsoft.com;wche@microsoft.com;mazm@amt.ac.cn;tie-yan.liu@mircosoft.com,7;1;3,3;5;4,Reject,2,0,0,yes,9/27/18,University of Chinese Academy of Sciences;Microsoft;Microsoft;Chinese Academy of Sciences;Mircosoft,62;-1;-1;62;-1,1103;-1;-1;1103;-1,1,9/27/18,0,0,0,0,0,0,4;399;866;653;13257,6;42;165;79;366,1;10;13;14;51,0;47;56;40;1716,-1;-1
2119,ICLR,2019,On the Selection of Initialization and Activation Function for Deep Neural Networks,Soufiane Hayou;Arnaud Doucet;Judith Rousseau,soufiane.hayou@stats.ox.ac.uk;doucet@stats.ox.ac.uk;judith.rousseau@stats.ox.ac.uk,3;4;5,5;4;4,Reject,0,13,0,yes,9/27/18,University of Oxford;University of Oxford;University of Oxford,50;50;50,1;1;1,,5/21/18,33,17,4,0,27,3,68;20560;2461,7;246;147,3;51;25,6;2616;251,-1;-1
2120,ICLR,2019,Online Learning for Supervised Dimension Reduction,Ning Zhang;Qiang Wu,ningzhang0123@gmail.com;qwu@mtsu.edu,2;5;6,5;4;5,Reject,0,0,0,yes,9/27/18,;SUN YAT-SEN UNIVERSITY,-1;478,-1;352,,9/27/18,0,0,0,0,0,0,896;3427,171;328,15;27,30;301,-1;-1
2121,ICLR,2019,Accelerated Sparse Recovery Under Structured Measurements,Ke Li;Jitendra Malik,ke.li@eecs.berkeley.edu;malik@eecs.berkeley.edu,4;5;5,5;3;4,Reject,0,0,0,yes,9/27/18,University of California Berkeley;University of California Berkeley,5;5,18;18,,9/27/18,0,0,0,0,0,0,6137;68980,781;429,35;115,290;7755,-1;-1
2122,ICLR,2019,The Cakewalk Method,Uri Patish;Shimon Ullman,uri.patish@gmail.com;shimon.ullman@gmail.com,5;4;4,3;4;4,Reject,0,6,0,yes,9/27/18,Weizmann Institute;,106;-1,1103;-1,10,9/27/18,0,0,0,0,0,0,0;773,1;20,0;5,0;61,-1;-1
2123,ICLR,2019,A fast quasi-Newton-type method for large-scale stochastic optimisation,Adrian Wills;Thomas B. Schön;Carl Jidling,adrian.wills@newcastle.edu.au;thomas.schon@it.uu.se;carl.jidling@it.uu.se,4;5;5,4;5;5,Reject,0,6,0,yes,9/27/18,"University of Newcastle, Australia;Uppsala University;Uppsala University",386;153;153,291;86;86,,9/27/18,3,0,0,0,3,0,1841;4083;55,114;234;12,22;31;3,122;335;3,-1;-1
2124,ICLR,2019,Deep Denoising: Rate-Optimal Recovery of Structured Signals with a Deep Prior,Reinhard Heckel;Wen Huang;Paul Hand;Vladislav Voroninski,rh43@rice.edu;wen.huang@xmu.edu.cn;p.hand@northeastern.edu;vlad@helm.ai,5;6;6,3;3;4,Reject,0,3,0,yes,9/27/18,Rice University;Xiamen University;Northeastern University;,85;62;16;-1,86;12;839;-1,5,9/27/18,14,3,1,0,5,0,932;991;1584;2240,57;91;67;27,16;13;19;16,112;98;151;205,-1;-1
2125,ICLR,2019,Perception-Aware Point-Based Value Iteration for Partially Observable Markov Decision Processes,Mahsa Ghasemi;Ufuk Topcu,mahsa.ghasemi@utexas.edu;utopcu@utexas.edu,6;4;7,4;4;2,Reject,0,3,0,yes,9/27/18,"University of Texas, Austin;University of Texas, Austin",22;22,49;49,,9/27/18,1,1,0,0,0,0,30;5376,11;268,3;36,0;322,-1;-1
2126,ICLR,2019,Towards Consistent Performance on Atari using Expert Demonstrations,Tobias Pohlen;Bilal Piot;Todd Hester;Mohammad Gheshlaghi Azar;Dan Horgan;David Budden;Gabriel Barth-Maron;Hado van Hasselt;John Quan;Mel Večerík;Matteo Hessel;Rémi Munos;Olivier Pietquin,pohlen@google.com;piot@google.com;toddhester@google.com;mazar@google.com;horgan@google.com;budden@google.com;gabrielbm@google.com;hado@google.com;johnquan@google.com;vec@google.com;mtthss@google.com;munos@google.com;pietquin@google.com,6;5;7;7,4;4;1;4,Reject,0,3,0,yes,9/27/18,Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,,9/27/18,0,0,0,0,0,0,515;2038;1495;1571;1371;98;587;5488;3225;98;2258;9303;3361,7;70;51;32;9;17;14;50;16;7;26;190;195,4;18;20;15;8;5;8;21;11;5;13;53;30,52;265;134;282;226;7;72;906;542;13;364;1313;288,-1;-1
2127,ICLR,2019,Manifold Alignment via Feature Correspondence,Jay S. Stanley III;Guy Wolf;Smita Krishnaswamy,jay.stanley@yale.edu;guy.wolf@yale.edu;smita.krishnaswamy@yale.edu,5;5;4,4;4;3,Reject,0,0,0,yes,9/27/18,Yale University;Yale University;Yale University,62;62;62,12;12;12,10,9/27/18,4,0,1,0,0,0,70;391;926,13;65;75,3;10;15,3;22;81,-1;-1
2128,ICLR,2019,Consistent Jumpy Predictions for Videos and Scenes,Ananya Kumar;S. M. Ali Eslami;Danilo Rezende;Marta Garnelo;Fabio Viola;Edward Lockhart;Murray Shanahan,skywalker94@gmail.com;aeslami@google.com;danilor@google.com;garnelo@google.com;fviola@google.com;locked@google.com;mshanahan@google.com,7;4;5,2;4;4,Reject,0,7,0,yes,9/27/18,Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,,9/27/18,-1,-1,-1,-1,-1,-1,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,0;0;0;0;0;0;0,-1;-1
2129,ICLR,2019,LEARNING ADVERSARIAL EXAMPLES WITH RIEMANNIAN GEOMETRY,Shufei Zhang;Kaizhu Huang;Rui Zhang;Amir Hussain,shufei.zhang@xjtlu.edu.cn;kaizhu.huang@xjtlu.edu.cn;rui.zhang02@xjtlu.edu.cn;ahu@cs.stir.ac.uk,6;4;3,2;5;5,Reject,0,15,0,yes,9/27/18,Tsinghua University;Tsinghua University;Tsinghua University;University of Stirling,8;8;8;478,30;30;30;309,4,9/27/18,0,0,0,0,0,0,74;2455;654;5537,20;194;101;435,5;26;13;40,1;187;33;235,-1;-1
2130,ICLR,2019,"On Tighter Generalization Bounds for Deep Neural Networks: CNNs, ResNets, and Beyond",Xingguo Li;Junwei Lu;Zhaoran Wang;Jarvis Haupt;Tuo Zhao,xingguol@princeton.edu;junweilu@hsph.harvard.edu;zhaoranwang@gmail.com;jdhaupt@umn.edu;tourzhao@gatech.edu,7;7;5,4;4;3,Reject,0,22,0,yes,9/27/18,"Princeton University;Harvard University;Northwestern University;University of Minnesota, Minneapolis;Georgia Institute of Technology",30;39;44;57;13,7;6;20;56;33,1;8,6/13/18,35,17,7,3,2,7,576;2292;1158;3663;2323,76;302;77;105;109,12;26;19;24;19,64;96;131;260;199,-1;-1
2131,ICLR,2019,DEEP GEOMETRICAL GRAPH CLASSIFICATION,Mostafa Rahmani;Ping Li,rahmani.sut@gmail.com;pingli98@gmail.com,6;4;3,5;4;4,Reject,0,6,0,yes,9/27/18,Baidu;Rutgers University New Brunswick,-1;34,-1;172,10,9/27/18,0,0,0,0,0,0,370;450,47;163,9;11,23;34,m;m
2132,ICLR,2019,Hierarchically Clustered Representation Learning,Su-Jin Shin;Kyungwoo Song;Il-Chul Moon,sujin.shin@kaist.ac.kr;gtshs2@kaist.ac.kr;icmoon@kaist.ac.kr,5;5;6,4;4;3,Reject,0,4,0,yes,9/27/18,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,20;20;20,95;95;95,5;11,9/27/18,5,0,0,0,5,0,711;134;614,121;38;134,15;7;11,35;8;40,-1;-1
2133,ICLR,2019,A Deep Learning Approach for Dynamic Survival Analysis with Competing Risks,Changhee Lee;Mihaela van der Schaar,chl8856@gmail.com;mihaela@ee.ucla.edu,4;4;8,3;4;4,Reject,0,5,0,yes,9/27/18,"University of California, Los Angeles;University of California, Los Angeles",20;20,15;15,,9/27/18,0,0,0,0,0,0,4515;55,519;46,31;3,234;3,-1;-1
2134,ICLR,2019,Locally Linear Unsupervised Feature Selection,Guillaume DOQUET;Michèle SEBAG,doquet@lri.fr;sebag@lri.fr,4;6;3,5;2;5,Reject,2,3,0,yes,9/27/18,"CNRS, Université Paris-Saclay;CNRS, Université Paris-Saclay",-1;-1,-1;-1,,9/27/18,0,0,0,0,0,0,0;4044,4;250,0;36,0;380,-1;-1
2135,ICLR,2019,Scalable Neural Theorem Proving on Knowledge Bases and Natural Language,Pasquale Minervini;Matko Bosnjak;Tim Rocktäschel;Edward Grefenstette;Sebastian Riedel,p.minervini@gmail.com;matko.bosnjak@gmail.com;tim.rocktaeschel@gmail.com;etg@google.com;etg@google.com,5;4;5,3;3;3,Reject,0,13,0,yes,9/27/18,University College London;University College London;Facebook AI Research;Google;Google,50;50;-1;-1;-1,16;16;-1;-1;-1,1,9/27/18,0,0,0,0,0,0,581;3464;106;6981;390,41;24;21;57;60,9;11;5;25;10,167;476;9;836;21,-1;-1
2136,ICLR,2019,A Frank-Wolfe Framework for Efficient and Effective Adversarial Attacks,Jinghui Chen;Jinfeng Yi;Quanquan Gu,jc4zg@virginia.edu;yijinfeng@jd.com;qgu@cs.ucla.edu,5;7;5,4;4;4,Reject,2,12,0,yes,9/27/18,"University of Virginia;JD AI Research;University of California, Los Angeles",65;-1;20,113;-1;15,4;9,9/27/18,10,2,4,0,2,0,346;1967;3758,44;79;174,10;24;34,38;238;405,-1;-1
2137,ICLR,2019,ACTRCE: Augmenting Experience via Teacher’s Advice,Yuhuai Wu;Harris Chan;Jamie Kiros;Sanja Fidler;Jimmy Ba,ywu@cs.toronto.edu;hchan@cs.toronto.edu;kirosjamie@gmail.com;fidler@cs.toronto.edu;jba@cs.toronto.edu,5;5;7,4;4;5,Reject,0,14,0,yes,9/27/18,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Google;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",18;18;-1;18;18,22;22;-1;22;22,3,9/27/18,5,1,0,0,6,1,1163;22;1435;10510;51696,28;12;18;160;52,13;3;9;48;21,181;4;177;1360;8455,-1;-1
2138,ICLR,2019,Learning data-derived privacy preserving representations from information metrics,Martin Bertran;Natalia Martinez;Afroditi Papadaki;Qiang Qiu;Miguel Rodrigues;Guillermo Sapiro,martin.bertran@duke.edu;natalia.martinez@duke.edu;a.papadaki.17@ucl.ac.uk;qiuqiang@gmail.com;m.rodrigues@ucl.ac.uk;guillermo.sapiro@duke.edu,6;6;5,3;4;4,Reject,0,6,0,yes,9/27/18,Duke University;Duke University;University College London;;University College London;Duke University,44;44;50;-1;50;44,17;17;16;-1;16;17,7;1;2,9/27/18,0,0,0,0,0,0,13;20;51;1230;3935;42902,12;28;12;90;192;650,2;3;5;18;24;86,1;2;3;117;291;3903,-1;-1
2139,ICLR,2019,Probabilistic Federated Neural Matching,Mikhail Yurochkin;Mayank Agarwal;Soumya Ghosh;Kristjan Greenewald;Nghia Hoang;Yasaman Khazaeni,mikhail.yurochkin@ibm.com;mayank.agarwal@ibm.com;ghoshso@us.ibm.com;kristjan.h.greenewald@ibm.com;nghiaht@ibm.com;yasaman.khazaeni@us.ibm.com,4;6;6,3;4;4,Reject,0,4,0,yes,9/27/18,International Business Machines;International Business Machines;International Business Machines;International Business Machines;International Business Machines;International Business Machines,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,11,9/27/18,0,0,0,0,0,0,136;1358;496;72;0;184,27;240;59;14;5;29,6;19;12;5;0;9,7;76;34;2;0;5,-1;-1
2140,ICLR,2019,Learning to Refer to 3D Objects with Natural Language,Panos Achlioptas;Judy E. Fan;Robert X.D. Hawkins;Noah D. Goodman;Leo Guibas,optas@cs.stanford.edu;jefan@stanford.edu;rxdh@stanford.edu;ngoodman@stanford.edu;guibas@cs.stanford.edu,6;4;6,4;4;3,Reject,0,0,0,yes,9/27/18,Stanford University;Stanford University;Stanford University;Stanford University;Stanford University,4;4;4;4;4,3;3;3;3;3,8,9/27/18,1,0,1,0,0,0,327;76;286;8392;1,13;6;41;240;2,4;2;10;47;1,70;4;22;575;0,-1;-1
2141,ICLR,2019,Monge-Amp\`ere Flow for Generative Modeling,Linfeng Zhang;Weinan E;Lei Wang,linfengz@princeton.edu;weinan@math.princeton.edu;wanglei@iphy.ac.cn,6;6;7,4;3;3,Reject,0,10,2,yes,9/27/18,Princeton University;Princeton University;Chinese Academy of Sciences,30;30;62,7;7;1103,5,9/26/18,22,3,1,0,8,0,861;7064;14746,71;261;1100,17;47;55,39;513;597,-1;-1
2142,ICLR,2019,Negotiating Team Formation Using Deep Reinforcement Learning,Yoram Bachrach;Richard Everett;Edward Hughes;Angeliki Lazaridou;Joel Leibo;Marc Lanctot;Mike Johanson;Wojtek Czarnecki;Thore Graepel,yorambac@google.com;reverett@google.com;edwardhughes@google.com;jzl@google.com;angeliki@google.com;lanctot@google.com;mjohanson@google.com;lejlot@google.com;thore@google.com,5;6;5,3;2;3,Reject,0,8,0,yes,9/27/18,Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1,,9/27/18,2,2,0,0,0,0,2706;90;1547;1722;3451;10541;2;2;18691,136;23;98;76;75;70;1;1;161,29;5;15;21;29;23;1;1;45,195;4;163;192;346;732;0;0;1384,-1;-1
2143,ICLR,2019,Why do deep convolutional networks generalize so poorly to small image transformations?,Aharon Azulay;Yair Weiss,aharon.azulay@mail.huji.ac.il;yweiss@cs.huji.ac.il,7;7;5,5;4;4,Reject,0,8,1,yes,9/27/18,Hebrew University of Jerusalem;Hebrew University of Jerusalem,65;65,205;205,1;8,5/30/18,93,56,7,1,0,5,116;26557,6;104,3;55,5;3312,-1;-1
2144,ICLR,2019,Psychophysical vs. learnt texture representations in novelty detection,Michael Grunwald;Matthias Hermann;Fabian Freiberg;Matthias O. Franz,m.grunwald@htwg-konstanz.de;matthias.hermann@htwg-konstanz.de;f.freiberg@htwg-konstanz.de;mfanz@htwg-konstanz.de,3;3;3;1,3;3;4;3,Reject,0,0,0,yes,9/27/18,University of Tuebingen;;;,153;-1;-1;-1,94;-1;-1;-1,,9/27/18,0,0,0,0,0,0,134;252;78;2194,19;21;7;102,5;5;3;22,5;20;7;163,-1;-1
2145,ICLR,2019,A fully automated periodicity detection in time series,Tom Puech;Matthieu Boussard,tom.puech@craft.ai;matthieu.boussard@craft.ai,3;5;3,3;2;2,Reject,0,0,0,yes,9/27/18,;,-1;-1,-1;-1,,9/27/18,0,0,0,0,0,0,0;92,1;22,0;6,0;1,-1;-1
2146,ICLR,2019,Downsampling leads to Image Memorization in Convolutional Autoencoders,Adityanarayanan Radhakrishnan;Caroline Uhler;Mikhail Belkin,aradha@mit.edu;cuhler@mit.edu;mbelkin@cse.ohio-state.edu,3;5;5,3;2;2,Reject,0,4,0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Ohio State University,2;2;76,5;5;318,,9/27/18,5,3,0,0,4,0,38;1256;16824,14;65;121,4;17;39,2;109;2062,-1;-1
2147,ICLR,2019,CDeepEx: Contrastive Deep Explanations,Amir Feghahati;Christian R. Shelton;Michael J. Pazzani;Kevin Tang,sfegh001@ucr.edu;cshelton@cs.ucr.edu;pazzani@ucr.edu;ktang012@ucr.edu,5;5;6,4;4;5,Reject,0,13,0,yes,9/27/18,"University of California, Riverside;University of California, Riverside;University of California, Riverside;University of California, Riverside",57;57;57;57,197;197;197;197,2,9/27/18,1,0,0,0,0,0,1;0;19946;44,3;2;209;21,1;0;53;4,0;0;1790;4,-1;-1
2148,ICLR,2019,Pooling Is Neither Necessary nor Sufficient for Appropriate Deformation Stability in CNNs,Avraham Ruderman;Neil C. Rabinowitz;Ari S. Morcos;Daniel Zoran,aruderman@google.com;ncr@google.com;arimorcos@gmail.com;danielzoran@google.com,5;5;4;5,5;2;4;2,Reject,0,4,0,yes,9/27/18,Google;Google;Facebook;Google,-1;-1;-1;-1,-1;-1;-1;-1,,4/12/18,6,2,3,0,0,0,593;2969;1001;1867,13;37;31;29,8;16;12;16,63;384;111;318,-1;-1
2149,ICLR,2019,Meta-Learning to Guide Segmentation,Kate Rakelly*;Evan Shelhamer*;Trevor Darrell;Alexei A. Efros;Sergey Levine,rakelly@eecs.berkeley.edu;shelhamer@cs.berkeley.edu;trevor@eecs.berkeley.edu;efros@eecs.berkeley.edu;svlevine@eecs.berkeley.edu,7;3;3,4;4;5,Reject,0,5,0,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5;5,18;18;18;18;18,6;2;8,9/27/18,2,0,0,0,0,0,255;26532;88307;36507;24386,8;26;558;192;309,6;14;110;77;73,52;4107;11356;4565;3167,-1;-1
2150,ICLR,2019,A theoretical framework for deep and locally connected ReLU network,Yuandong Tian,yuandong@fb.com,5;3;7,3;4;4,Reject,2,5,0,yes,9/27/18,Facebook,-1,-1,10,9/27/18,4,2,0,0,8,0,2435,84,25,285,-1
2151,ICLR,2019,FAST OBJECT LOCALIZATION VIA SENSITIVITY ANALYSIS,Mohammad K. Ebrahimpour;David C. Noelle,mebrahimpour@ucmerced.edu;dnoelle@ucmerced.edu,4;3;6,3;4;5,Reject,0,1,0,yes,9/27/18,University of California at Merced;University of California at Merced,478;478,1103;1103,,9/27/18,2,2,0,0,0,0,85;1003,10;85,5;11,4;61,-1;-1
2152,ICLR,2019,Generative Feature Matching Networks,Cicero Nogueira dos Santos;Inkit Padhi;Pierre Dognin;Youssef Mroueh,cicerons@us.ibm.com;inkit.padhi@ibm.com;pdognin@us.ibm.com;mroueh@us.ibm.com,6;6;6;6,3;4;3;3,Reject,0,23,0,yes,9/27/18,International Business Machines;International Business Machines;International Business Machines;International Business Machines,-1;-1;-1;-1,-1;-1;-1;-1,5;4,9/27/18,0,0,0,0,0,0,4447;693;119;878,60;25;27;52,19;10;6;11,500;36;6;145,-1;-1
2153,ICLR,2019,Generative Adversarial Models for Learning Private and Fair Representations,Chong Huang;Xiao Chen;Peter Kairouz;Lalitha Sankar;Ram Rajagopal,chuang83@asu.edu;markcx@stanford.edu;kairouzp@stanford.edu;lsankar@asu.edu;ramr@stanford.edu,4;4;7,3;3;3,Reject,0,8,0,yes,9/27/18,Arizona State University;Stanford University;Stanford University;Arizona State University;Stanford University,95;4;4;95;4,126;3;3;126;3,5;4;7,9/27/18,1,0,0,0,0,0,1060;935;991;1862;2956,138;204;50;102;210,17;16;11;21;28,53;38;113;83;142,-1;-1
2154,ICLR,2019,Understanding & Generalizing AlphaGo Zero,Ravichandra Addanki;Mohammad Alizadeh;Shaileshh Bojja Venkatakrishnan;Devavrat Shah;Qiaomin Xie;Zhi Xu,addanki@mit.edu;alizadeh@csail.mit.edu;bjjvnkt@csail.mit.edu;devavrat@mit.edu;qxie@mit.edu;zhixu@mit.edu,5;5;7,3;5;4,Reject,0,3,0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2;2;2,5;5;5;5;5;5,1,9/27/18,1,1,0,0,0,0,0;0;0;0;0;0,0;0;0;0;0;0,0;0;0;0;0;0,0;0;0;0;0;0,-1;-1
2155,ICLR,2019,Link Prediction in Hypergraphs using Graph Convolutional Networks,Naganand Yadati;Vikram Nitin;Madhav Nimishakavi;Prateek Yadav;Anand Louis;Partha Talukdar,y.naganand@gmail.com;vikramnitin9@gmail.com;madhav@iisc.ac.in;prateekyadav@iisc.ac.in;anandl@iisc.ac.in;ppt@iisc.ac.in,6;5;4,2;4;5,Reject,0,9,0,yes,9/27/18,"Indian Institute of Science;BITS Pilani, BITS Pilani;Indian Institute of Science;Indian Institute of Science;Indian Institute of Science;Indian Institute of Science",478;-1;478;478;478;478,273;-1;273;273;273;273,10,9/27/18,2,1,0,0,0,0,54;23;75;56;325;2337,11;5;14;9;39;101,4;3;5;4;12;27,4;2;3;3;19;203,-1;-1
2156,ICLR,2019,RedSync : Reducing Synchronization Traffic for Distributed Deep Learning,Jiarui Fang;Cho-Jui Hsieh,fang_jiarui@163.com;rainfarmer@gmail.com,5;5;5,3;4;4,Reject,2,5,0,yes,9/27/18,Tsinghua;,8;-1,30;-1,3,8/13/18,3,2,1,0,13,1,144;12483,20;168,7;40,6;1722,-1;-1
2157,ICLR,2019,PRUNING IN TRAINING: LEARNING AND RANKING SPARSE CONNECTIONS IN DEEP CONVOLUTIONAL NETWORKS,Yanwei Fu;Shun Zhang;Donghao Li;Xinwei Sun;Xiangyang Xue;Yuan Yao,yanweifu@fudan.edu.cn;15300180012@fudan.edu.cn;15307100013@fudan.edu.cn;sxwxiaoxiaohehe@pku.edu.cn;xyxue@fudan.edu.cn;yuany@ust.hk,5;5;4,4;4;5,Reject,0,0,0,yes,9/27/18,Fudan University;Fudan University;Fudan University;Peking University;Fudan University;The Hong Kong University of Science and Technology,78;78;78;24;78;39,116;116;116;27;116;44,,9/27/18,0,0,0,0,0,0,87;3986;1012;64;5675;4161,22;359;122;16;281;421,5;32;16;4;38;30,2;198;28;3;581;290,-1;-1
2158,ICLR,2019,Integral Pruning on Activations and Weights for Efficient Neural Networks,Qing Yang;Wei Wen;Zuoguan Wang;Yiran Chen;Hai Li,qing.yang21@duke.edu;wei.wen@duke.edu;zuoguan.wang@blacksesame.com;yiran.chen@duke.edu;hai.li@duke.edu,4;5;5,3;4;4,Reject,0,4,0,yes,9/27/18,Duke University;Duke University;Blacksesame;Duke University;Duke University,44;44;-1;44;44,17;17;-1;17;17,,9/27/18,1,0,0,0,0,0,18475;2326;226;585;318,1767;98;18;74;54,55;19;7;12;8,932;260;14;46;21,-1;-1
2159,ICLR,2019,Penetrating the Fog: the Path to Efficient CNN Models,Kun Wan;Boyuan Feng;Shu Yang;Yufei Ding,kun@cs.ucsb.edu;boyuan@cs.ucsb.edu;shuyang1995@ucsb.edu;yufeiding@cs.ucsb.edu,5;5;4,3;3;3,Reject,0,0,0,yes,9/27/18,UC Santa Barbara;UC Santa Barbara;UC Santa Barbara;UC Santa Barbara,37;37;37;37,53;53;53;53,,9/27/18,2,0,0,0,2,0,72;4;431;344,23;8;108;43,3;1;12;9,4;0;26;31,-1;-1
2160,ICLR,2019,ACIQ: Analytical Clipping for Integer Quantization of neural networks,Ron Banner;Yury Nahshan;Elad Hoffer;Daniel Soudry,ron.banner@intel.com;yury.nahshan@intel.com;daniel.soudry@gmail.com;elad.hoffer@gmail.com,4;4;5,5;4;4,Reject,3,5,0,yes,9/27/18,Intel;Intel;Technion;Technion,-1;-1;25;25,-1;-1;327;327,,9/27/18,13,8,7,0,5,6,517;50;1445;4820,31;4;27;75,11;2;11;26,81;15;167;622,-1;-1
2161,ICLR,2019,PRUNING WITH HINTS: AN EFFICIENT FRAMEWORK FOR MODEL ACCELERATION,Wei Gao;Yi Wei;Quanquan Li;Hongwei Qin;Wanli Ouyang;Junjie Yan,weigao1996@outlook.com;wei-y15@mails.tsinghua.edu.cn;liquanquan@sensetime.com;qinghongwei@sensetime.com;wanli.ouyang@sydney.edu.cn;yanjunjie@outlook.com,4;5;4,3;4;4,Reject,0,0,0,yes,9/27/18,Beihang University;Tsinghua University;SenseTime Group Limited;SenseTime Group Limited;Tsinghua University;SenseTime Group Limited,115;8;-1;-1;8;-1,658;30;-1;-1;30;-1,2;8,9/27/18,0,0,0,0,0,0,197;10;302;343;10542;6828,72;12;25;22;149;166,7;2;5;8;52;44,22;0;39;25;1182;968,-1;-1
2162,ICLR,2019,Progressive Weight Pruning Of Deep Neural Networks Using ADMM,Shaokai Ye;Tianyun Zhang;Kaiqi Zhang;Jiayu Li;Kaidi Xu;Yunfei Yang;Fuxun Yu;Jian Tang;Makan Fardad;Sijia Liu;Xiang Chen;Xue Lin;Yanzhi Wang,sye106@syr.edu;tzhan120@syr.edu;kzhang17@syr.edu;jli221@syr.edu;xu.kaid@husky.neu.edu;yunfei.yang717@gmail.com;fyu@gmu.edu;jtang02@syr.edu;makan@syr.edu;sijia.liu@ibm.com;xchen26@gmu.edu;xue.lin@northeastern.edu;yanz.wang@northeastern.edu,5;5;4,4;3;4,Reject,0,3,0,yes,9/27/18,Syracuse University;Syracuse University;Syracuse University;Syracuse University;Northeastern University;;George Mason University;Syracuse University;Syracuse University;International Business Machines;George Mason University;Northeastern University;Northeastern University,261;261;261;261;16;-1;99;261;261;-1;99;16;16,275;275;275;275;839;-1;336;275;275;-1;336;839;839,9,9/27/18,11,4,5,0,10,0,263;218;202;159;246;207;445;5024;1505;690;13505;1897;3465,19;25;20;33;27;69;49;154;90;62;574;256;262,10;7;7;8;10;8;11;33;21;14;53;24;28,36;29;18;17;17;10;23;438;98;89;830;111;219,-1;-1
2163,ICLR,2019,Understanding Opportunities for Efficiency in Single-image Super Resolution Networks,Royson Lee;Nic Lane;Marko Stankovic;Sourav Bhattacharya,rs@roysonlee.com;nicholas.d.lane@gmail.com;marko.stankovic996@gmail.com;bsourav@gmail.com,4;5;3,5;4;5,Reject,0,3,0,yes,9/27/18,Samsung;University of Oxford;;,-1;50;-1;-1,-1;1;-1;-1,,9/27/18,0,0,0,0,0,0,4;17;1;2012,5;24;6;149,1;2;1;21,0;1;0;181,-1;-1
2164,ICLR,2019,Actor-Attention-Critic for Multi-Agent Reinforcement Learning,Shariq Iqbal;Fei Sha,shariqiqbal2810@gmail.com;feisha.work@gmail.com,7;6;4,3;3;4,Reject,2,3,0,yes,9/27/18,University of Southern California;,30;-1,66;-1,4,9/27/18,56,33,23,0,11,10,89;8928,12;118,4;41,13;1379,-1;-1
2165,ICLR,2019,On the Margin Theory of Feedforward Neural Networks,Colin Wei;Jason Lee;Qiang Liu;Tengyu Ma,colinwei@stanford.edu;jasonlee@marshall.usc.edu;lqiang@cs.texas.edu;tengyuma@cs.stanford.edu,6;7;5;5,4;3;4;4,Reject,2,16,0,yes,9/27/18,Stanford University;University of Southern California;;Stanford University,4;30;-1;4,3;66;-1;3,8,9/27/18,60,36,8,0,2,2,302;4629;4529;3818,16;118;553;87,8;36;31;32,34;604;338;491,-1;-1
2166,ICLR,2019,Are adversarial examples inevitable?,Ali Shafahi;W. Ronny Huang;Christoph Studer;Soheil Feizi;Tom Goldstein,ashafahi@gmail.com;w.ronny.huang@gmail.com;studer@cornell.edu;feizi.soheil@gmail.com;tomg@cs.umd.edu,6;7;8,4;4;4,Accept (Poster),3,12,2,yes,9/27/18,"University of Maryland, College Park;;Cornell University;;University of Maryland, College Park",12;-1;7;-1;12,69;-1;19;-1;69,4,9/6/18,85,53,5,2,22,5,433;315;5251;501;5750,33;21;192;38;98,8;7;38;13;27,45;23;635;55;721,-1;-1
2167,ICLR,2019,Open Loop Hyperparameter Optimization and Determinantal Point Processes,Jesse Dodge;Kevin Jamieson;Noah Smith,jessed@cs.cmu.edu;jamieson@cs.washington.edu;nasmith@cs.washington.edu,5;5;6,3;5;4,Reject,0,3,0,yes,9/27/18,Carnegie Mellon University;University of Washington;University of Washington,1;6;6,24;25;25,,2/15/18,2,1,0,0,0,0,1238;1785;17574,21;67;292,13;17;64,122;260;2088,-1;-1
2168,ICLR,2019,Cross-Task Knowledge Transfer for Visually-Grounded Navigation,Devendra Singh Chaplot;Lisa Lee;Ruslan Salakhutdinov;Devi Parikh;Dhruv Batra,chaplot@cs.cmu.edu;lslee@cs.cmu.edu;rsalakhu@cs.cmu.edu;parikh@gatech.edu;dbatra@gatech.edu,7;5;5,4;3;5,Reject,0,5,0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Georgia Institute of Technology;Georgia Institute of Technology,1;1;1;13;13,24;24;24;33;33,6,9/27/18,1,1,0,0,0,0,768;20;66785;80;8305,30;7;254;18;147,11;3;81;4;41,98;3;7750;9;1135,-1;-1
2169,ICLR,2019,Expanding the Reach of Federated Learning by Reducing Client Resource Requirements,Sebastian Caldas;Jakub Konečný;Brendan McMahan;Ameet Talwalkar,scaldas@cmu.edu;konkey@google.com;mcmahan@google.com;talwalkar@cmu.edu,4;5;5,5;3;4,Reject,0,6,0,yes,9/27/18,Carnegie Mellon University;Google;Google;Carnegie Mellon University,1;-1;-1;1,24;-1;-1;24,,9/27/18,47,19,15,0,0,4,131;2216;5801;6281,5;36;68;78,4;16;32;34,20;231;847;759,-1;-1
2170,ICLR,2019,Generative Adversarial Self-Imitation Learning,Junhyuk Oh;Yijie Guo;Satinder Singh;Honglak Lee,junhyuk@umich.edu;guoyijie@umich.edu;baveja@umich.edu;honglak@google.com,5;5;6,4;5;5,Reject,0,7,0,yes,9/27/18,University of Michigan;University of Michigan;University of Michigan;Google,8;8;8;-1,21;21;21;-1,5;4,9/27/18,16,8,5,0,0,2,1351;622;616;23861,24;30;71;166,13;10;12;60,134;100;57;2806,-1;-1
2171,ICLR,2019,Diverse Machine Translation with a Single Multinomial Latent Variable,Tianxiao Shen;Myle Ott;Michael Auli;Marc’Aurelio Ranzato,tianxiao@mit.edu;myleott@fb.com;michaelauli@fb.com;ranzato@fb.com,3;6;5;7,4;4;4;4,Reject,0,6,0,yes,9/27/18,Massachusetts Institute of Technology;Facebook;Facebook;Facebook,2;-1;-1;-1,5;-1;-1;-1,,9/27/18,1,1,0,0,0,0,335;3702;6784;51,13;39;71;17,3;20;30;4,94;755;1017;6,-1;-1
2172,ICLR,2019,Learning and Planning with a Semantic Model,Yi Wu;Yuxin Wu;Aviv Tamar;Stuart Russell;Georgia Gkioxari;Yuandong Tian,jxwuyi@gmail.com;yuxinwu@fb.com;avivt@berkeley.edu;russell@cs.berkeley.edu;gkioxari@fb.com;yuandong@fb.com,5;4;7,4;4;3,Reject,0,6,0,yes,9/27/18,University of California Berkeley;Facebook;University of California Berkeley;University of California Berkeley;Facebook;Facebook,5;-1;5;5;-1;-1,18;-1;18;18;-1;-1,11;8,9/27/18,4,4,0,0,7,0,1276;2056;2202;34714;7274;2435,101;13;50;274;28;84,11;10;20;59;18;25,196;321;331;3500;1465;285,-1;-1
2173,ICLR,2019,Visual Imitation with a Minimal Adversary,Scott Reed;Yusuf Aytar;Ziyu Wang;Tom Paine;Aäron van den Oord;Tobias Pfaff;Sergio Gomez;Alexander Novikov;David Budden;Oriol Vinyals,reedscot@google.com;yusufaytar@google.com;ziyu@google.com;tpaine@google.com;avdnoord@google.com;tpfaff@google.com;sergomez@google.com;anovikov@google.com;budden@google.com;vinyals@google.com,5;3;6,4;3;4,Reject,0,5,0,yes,9/27/18,Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,5;4,9/27/18,3,2,2,0,0,1,10980;1437;70;510;8527;658;4757;1549;98;52009,28;32;19;17;40;29;136;153;17;121,16;15;5;8;24;10;29;19;5;55,2053;183;2;63;1093;51;261;152;7;6497,-1;-1
2174,ICLR,2019,One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL,Tom Le Paine;Sergio Gomez;Ziyu Wang;Scott Reed;Yusuf Aytar;Tobias Pfaff;Matt Hoffman;Gabriel Barth-Maron;Serkan Cabi;David Budden;Nando de Freitas,tpaine@google.com;sergomez@google.com;ziyu@google.com;reedscot@google.com;yusufaytar@google.com;tpfaff@google.com;mwhoffman@google.com;gabrielbm@google.com;cabi@google.com;budden@google.com;nandodefreitas@google.com,4;4;5;5,4;4;3;3,Reject,0,10,0,yes,9/27/18,Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,,9/27/18,10,7,1,2,131,0,510;1271;4187;10980;1437;658;1781;587;179;1249;18886,17;15;51;28;32;29;34;14;12;55;184,8;10;21;16;15;10;17;8;6;15;54,63;141;484;2053;183;51;193;72;11;129;1852,-1;-1
2175,ICLR,2019,Mixed Precision Quantization of ConvNets via Differentiable Neural Architecture Search,Bichen Wu;Yanghan Wang;Peizhao Zhang;Yuandong Tian;Peter Vajda;Kurt Keutzer,bichen@berkeley.edu;yanghan@instagram.com;stzpz@fb.com;yuandong@fb.com;vajdap@fb.com,5;7;6;6,5;3;3;3,Reject,0,21,0,yes,9/27/18,University of California Berkeley;Instagram;Facebook;Facebook;Facebook,5;-1;-1;-1;-1,18;-1;-1;-1;-1,,9/27/18,41,20,24,4,28,5,1071;105;643;2435;655;16681,36;4;16;84;47;417,12;2;7;25;10;59,143;11;78;285;85;1575,-1;-1
2176,ICLR,2019,Language Modeling with Graph Temporal Convolutional Networks,Hongyin Luo;Yichen Li;Jie Fu;James Glass,hyluo@mit.edu;yl3506@nyu.edu;jie.fu@polymtl.ca;glass@mit.edu,4;4;4,5;3;5,Reject,0,3,0,yes,9/27/18,Massachusetts Institute of Technology;New York University;Polytechnique Montreal;Massachusetts Institute of Technology,2;26;386;2,5;27;108;5,3;10,9/27/18,0,0,0,0,0,0,86;167;673;10705,21;54;116;342,5;5;13;54,7;15;36;964,-1;-1
2177,ICLR,2019,Lipschitz regularized Deep Neural Networks generalize,Adam M. Oberman;Jeff Calder,adam.oberman@mcgill.ca;jcalder@umn.edu,4;6;7,3;2;4,Reject,0,12,1,yes,9/27/18,"McGill University;University of Minnesota, Minneapolis",85;57,42;56,1;9;8,8/28/18,11,2,1,0,0,0,1639;348,88;51,22;12,156;16,-1;-1
2178,ICLR,2019,Count-Based Exploration with the Successor Representation,Marlos C. Machado;Marc G. Bellemare;Michael Bowling,machado@ualberta.ca;bellemare@google.com;mbowling@ualberta.ca,5;5;4,4;2;3,Reject,0,7,0,yes,9/27/18,University of Alberta;Google;University of Alberta,99;-1;99,119;-1;119,8,7/31/18,23,15,11,2,22,1,620;3952;1133,31;57;44,11;24;17,79;625;109,-1;-1
2179,ICLR,2019,Precision Highway for Ultra Low-precision Quantization,Eunhyeok Park;Dongyoung Kim;Sungjoo Yoo;Peter Vajda,canusglow@gmail.com;dongyoungkim42@gmail.com;sungjoo.yoo@gmail.com;vajdap@fb.com,6;7;5,5;4;3,Reject,0,9,0,yes,9/27/18,Seoul National University;;Seoul National University;Facebook,41;-1;41;-1,74;-1;74;-1,3,9/27/18,8,4,3,0,0,0,656;197;3195;655,12;34;159;47,7;7;25;10,77;27;334;85,-1;-1
2180,ICLR,2019,Graph Neural Networks with Generated Parameters for Relation Extraction,Hao Zhu;Yankai Lin;Zhiyuan Liu;Jie Fu;Tat-seng Chua;Maosong Sun,prokilchu@gmail.com;linyk14@mails.tsinghua.edu.cn;liuzy@tsinghua.edu.cn;full.jeffrey@gmail.com;chuats@comp.nus.edu.sg;sms@tsinghua.edu.cn,4;6;6,4;3;4,Reject,0,4,0,yes,9/27/18,Tsinghua University;Tsinghua University;Tsinghua University;Polytechnique Montreal;National University of Singapore;Tsinghua University,8;8;8;386;16;8,30;30;30;108;22;30,3;10,9/27/18,11,5,2,0,0,0,156;2349;6835;159;20452;8349,13;33;134;15;634;264,7;14;36;6;72;42,34;493;1139;11;2049;1348,-1;-1
2181,ICLR,2019,Cohen Welling bases & SO(2)-Equivariant classifiers using Tensor nonlinearity.,Muthuvel Murugan;K Venkata Subrahmanyam,muthu@cmi.ac.in;kv@cmi.ac.in,6;3;7,2;2;4,Reject,0,9,0,yes,9/27/18,Chennai Mathematical Institute;Chennai Mathematical Institute,478;478,1103;1103,,9/27/18,0,0,0,0,0,0,123;208,90;50,5;8,3;24,-1;-1
2182,ICLR,2019,Out-of-Sample Extrapolation with Neuron Editing,Matthew Amodio;David van Dijk;Ruth Montgomery;Guy Wolf;Smita Krishnaswamy,matthew.amodio@yale.edu;davidvandijk@gmail.com;ruth.montgomery@yale.edu;guy.wolf@yale.edu;smita.krishnaswamy@yale.edu,5;5;6,3;3;4,Reject,0,4,0,yes,9/27/18,Yale University;;Yale University;Yale University;Yale University,62;-1;62;62;62,12;-1;12;12;12,5;4,5/30/18,4,4,2,0,0,0,118;745;143;391;926,20;50;12;65;75,5;11;5;10;15,7;44;16;22;81,-1;-1
2183,ICLR,2019,Security Analysis of Deep Neural Networks Operating in the Presence of Cache Side-Channel Attacks,Sanghyun Hong;Michael Davinroy;Yigitcan Kaya;Stuart Nevans Locke;Ian Rackow;Kevin Kulda;Dana Dachman-Soled;Tudor Dumitraș,shhong@cs.umd.edu;mdavinr1@swarthmore.edu;yigitcan@cs.umd.edu;stnevans@mail.rit.edu;ian.rackow@gmail.com;kevin_kulda1@baylor.edu;danadach@ece.umd.edu;tdumitra@umiacs.umd.edu,4;6;4,4;4;2,Reject,0,6,0,yes,9/27/18,"University of Maryland, College Park;Swarthmore College;University of Maryland, College Park;Rochester Institute of Technology;University of Maryland, College Park;Baylor University;University of Maryland, College Park;University of Maryland, College Park",12;478;12;123;12;478;12;12,69;1103;69;666;69;642;69;69,4;6,9/27/18,15,11,6,1,4,3,262;17;95;463;14;14;831;1723,34;3;12;35;1;1;67;100,11;2;5;11;1;1;17;19,18;3;17;45;3;3;47;136,-1;-1
2184,ICLR,2019,Analyzing Federated Learning through an Adversarial Lens,Arjun Nitin Bhagoji;Supriyo Chakraborty;Seraphin Calo;Prateek Mittal,abhagoji@princeton.edu;supriyo@us.ibm.com;scalo@us.ibm.com;pmittal@princeton.edu,5;4;6,4;5;4,Reject,0,4,0,yes,9/27/18,Princeton University;International Business Machines;International Business Machines;Princeton University,30;-1;-1;30,7;-1;-1;7,4,9/27/18,65,40,6,0,8,7,628;1001;1777;3233,19;69;120;146,11;16;22;29,52;78;120;281,-1;-1
2185,ICLR,2019,Knows When it Doesn’t Know: Deep Abstaining Classifiers,Sunil Thulasidasan;Tanmoy Bhattacharya;Jeffrey Bilmes;Gopinath Chennupati;Jamal Mohd-Yusof,sunil@lanl.gov;tanmoy@lanl.gov;bilmes@uw.edu;gchennupati@lanl.gov;jamal@lanl.gov,6;5;5,4;3;5,Reject,0,6,0,yes,9/27/18,"Los Alamos National Laboratory;Los Alamos National Laboratory;University of Washington, Seattle;Los Alamos National Laboratory;Los Alamos National Laboratory",-1;-1;6;-1;-1,-1;-1;25;-1;-1,,9/27/18,3,1,0,0,0,0,298;9800;13536;209;33,40;262;350;40;5,11;45;54;7;3,28;601;1271;9;2,-1;-1
2186,ICLR,2019,Simple Black-box Adversarial Attacks,Chuan Guo;Jacob R. Gardner;Yurong You;Andrew G. Wilson;Kilian Q. Weinberger,cg563@cornell.edu;jrg365@cornell.edu;yy785@cornell.edu;andrew@cornell.edu;kqw4@cornell.edu,4;6;6,5;3;3,Reject,0,6,0,yes,9/27/18,Cornell University;Cornell University;Cornell University;Cornell University;Cornell University,7;7;7;7;7,19;19;19;19;19,4,9/27/18,42,21,7,2,3,7,1325;708;342;2728;23564,23;27;22;102;165,8;14;7;27;54,240;64;26;331;3794,-1;-1
2187,ICLR,2019,Modulating transfer between tasks in gradient-based meta-learning,Erin Grant;Ghassen Jerfel;Katherine Heller;Thomas L. Griffiths,eringrant@berkeley.edu;gj47@duke.edu;kheller@stat.duke.edu;tomg@princeton.edu,5;4;4,2;3;4,Reject,0,21,0,yes,9/27/18,University of California Berkeley;Duke University;Duke University;Princeton University,5;44;44;30,18;17;17;7,11;6;8,9/27/18,3,0,0,0,0,0,803;68;2033;21294,38;9;91;439,9;6;26;70,51;4;208;2170,-1;-1
2188,ICLR,2019,Approximation capability of neural networks on sets of probability measures and tree-structured data,Tomáš Pevný;Vojtěch Kovařík,pevnak@gmail.com;vojta.kovarik@gmail.com,6;5;4,3;5;5,Reject,0,4,0,yes,9/27/18,Czech Technical University in Prague;Czech Technical University in Prague,314;314,740;740,1,9/27/18,5,1,1,0,0,0,3862;40,92;21,27;4,450;0,-1;-1
2189,ICLR,2019,Uncertainty in Multitask Transfer Learning,Alexandre Lacoste;Boris Oreshkin;Wonchang Chung;Thomas Boquet;Negar Rostamzadeh;David Krueger,alex.lacoste.shmu@gmail.com;boris@elementai.com;wonchang@elementai.com;thomas@elementai.com;negar.rostamzadeh@gmail.com;david.scott.krueger@gmail.com,4;3;2,4;5;5,Reject,0,4,0,yes,9/27/18,Element AI;Element AI;Element AI;Element AI;Element AI;University of Montreal,-1;-1;-1;-1;-1;123,-1;-1;-1;-1;-1;108,6,6/20/18,10,6,2,0,45,0,963;742;20;53;375;1460,38;46;3;7;31;48,14;12;2;3;10;12,127;86;0;4;51;214,-1;-1
2190,ICLR,2019,Compound Density Networks,Agustinus Kristiadi;Asja Fischer,kristiadi@protonmail.com;asja.fischer@gmail.com,4;5;5,4;4;4,Reject,0,10,0,yes,9/27/18,University of Bonn;Institute for Cognitive Neuroscience/ Inst. for Neuroinformatics,123;-1,100;-1,4;11,9/27/18,4,0,1,0,3,0,43;1713,10;53,4;16,9;199,-1;-1
2191,ICLR,2019,Adversarial Examples Are a Natural Consequence of Test Error in Noise,Nicolas Ford;Justin Gilmer;Ekin D. Cubuk,nicf@google.com;gilmer@google.com;cubuk@google.com,4;5;4,4;3;3,Reject,6,5,3,yes,9/27/18,Google;Google;Google,-1;-1;-1,-1;-1;-1,4;1,9/27/18,86,39,11,3,2,7,117;3306;1304,3;45;50,3;18;18,9;457;134,-1;-1
2192,ICLR,2019,Stochastic Adversarial Video Prediction,Alex X. Lee;Richard Zhang;Frederik Ebert;Pieter Abbeel;Chelsea Finn;Sergey Levine,alexlee_gk@cs.berkeley.edu;rich.zhang@eecs.berkeley.edu;febert@berkeley.edu;pabbeel@cs.berkeley.edu;cbfinn@eecs.berkeley.edu;svlevine@eecs.berkeley.edu,6;6;5,3;4;5,Reject,0,9,0,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5;5;5,18;18;18;18;18;18,4,4/4/18,143,78,65,8,11,30,1329;2799;449;36468;7689;24386,18;27;16;433;98;309,15;12;8;94;33;73,151;497;68;4391;1026;3167,-1;-1
2193,ICLR,2019,On Generalization Bounds of a Family of Recurrent Neural Networks,Minshuo Chen;Xingguo Li;Tuo Zhao,mchen393@gatech.edu;lixx1661@umn.edu;tourzhao@gatech.edu,3;4;6,4;4;4,Reject,4,7,0,yes,9/27/18,"Georgia Institute of Technology;University of Minnesota, Minneapolis;Georgia Institute of Technology",13;57;13,33;56;33,1;8,9/27/18,8,5,3,2,0,3,54;576;2323,21;76;109,4;12;19,6;64;199,-1;-1
2194,ICLR,2019,N-Ary Quantization for CNN Model Compression and Inference Acceleration,Günther Schindler;Wolfgang Roth;Franz Pernkopf;Holger Fröning,guenther.schindler@ziti.uni-heidelberg.de;roth@tugraz.at;pernkopf@tugraz.at;holger.froening@ziti.uni-heidelberg.de,4;4;7,4;4;5,Reject,1,5,0,yes,9/27/18,Heidelberg University;Graz University of Technology;Graz University of Technology;Heidelberg University,199;106;106;199,45;443;443;45,,9/27/18,0,0,0,0,0,0,12;196;1857;432,9;137;196;82,3;7;21;12,1;14;110;29,-1;-1
2195,ICLR,2019,AntMan: Sparse Low-Rank Compression To Accelerate RNN Inference,Samyam Rajbhandari;Harsh Shrivastava;Yuxiong He,samyamr@microsoft.com;hshrivastava3@gatech.edu;yuxhe@microsoft.com,6;5;5,5;2;4,Reject,0,7,0,yes,9/27/18,Microsoft;Georgia Institute of Technology;Microsoft,-1;13;-1,-1;33;-1,,9/27/18,1,0,1,0,0,0,241;27;1601,22;14;102,8;3;23,29;0;135,-1;-1
2196,ICLR,2019,Convolutional CRFs for Semantic Segmentation,Marvin Teichmann;Roberto Cipolla,mttt2@cam.ac.uk;cipolla@eng.cam.ac.uk,7;4;6,4;4;4,Reject,0,8,1,yes,9/27/18,University of Cambridge;University of Cambridge,71;71,2;2,2,5/12/18,36,14,23,0,4,5,36;25233,1;496,1;73,5;2892,-1;-1
2197,ICLR,2019,Holographic and other Point Set Distances for Machine Learning,Lukas Balles;Thomas Fischbacher,lukas.balles@tuebingen.mpg.de;tfish@google.com,3;7;4,4;3;3,Reject,0,0,0,yes,9/27/18,"Max Planck Institute for Intelligent Systems, Max-Planck Institute;Google",-1;-1,-1;-1,1;2,9/27/18,2,0,1,1,0,0,224;557,10;55,6;13,31;36,-1;-1
2198,ICLR,2019,Pixel Redrawn For A Robust Adversarial Defense,Jiacang Ho;Dae-Ki Kang,ho_jiacang@hotmail.com;dkkang@dongseo.ac.kr,4;6;3,3;3;4,Reject,22,3,0,yes,9/27/18,;Dongseo University,-1;478,-1;1103,4,9/27/18,0,0,0,0,0,0,22;588,7;75,4;11,1;35,-1;-1
2199,ICLR,2019,HIGHLY EFFICIENT 8-BIT LOW PRECISION INFERENCE OF CONVOLUTIONAL NEURAL NETWORKS,Haihao Shen;Jiong Gong;Xiaoli Liu;Guoming Zhang;Ge Jin;and Eric Lin,haihao.shen@intel.com;jiong.gong@intel.com;xiaoli.liu@intel.com;guoming.zhang@intel.com;ge.jin@intel.com;eric.lin@intel.com,6;4;4,4;4;4,Reject,0,0,0,yes,9/27/18,Intel;Intel;Intel;Intel;Intel;Intel,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,2,5/4/18,11,5,4,0,4,0,83;222;687;1254;95;-1,9;23;159;92;33;-1,5;8;12;16;6;-1,5;18;24;121;6;0,-1;-1
2200,ICLR,2019,Modulated Variational Auto-Encoders for Many-to-Many Musical Timbre Transfer,Adrien Bitton;Philippe Esling;Axel Chemla-Romeu-Santos,bitton@ircam.fr;philippe.esling@ircam.fr;axel.chemla-romeu-santos@ircam.fr,5;5;3,3;3;4,Reject,5,7,0,yes,9/27/18,Sorbonne Université;;,-1;-1;-1,-1;-1;-1,5;4,9/27/18,4,2,3,0,0,1,30;1138;46,5;41;9,4;15;5,3;82;3,-1;-1
2201,ICLR,2019,Geometry aware convolutional filters for omnidirectional images representation,Renata Khasanova;Pascal Frossard,renata.khasanova@epfl.ch;pascal.frossard@epfl.ch,4;6;4,4;4;5,Reject,2,4,0,yes,9/27/18,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne,478;478,38;38,2,9/27/18,0,0,0,0,0,0,66;8091,10;488,3;38,7;679,-1;-1
2202,ICLR,2019,A CASE STUDY ON OPTIMAL DEEP LEARNING MODEL FOR UAVS,Chandan Kumar;Subrahmanyam Vaddi;Aishwarya Sarkar,chandan@iastate.edu;svaddi@iastate.edu;asarkar1@iastate.edu,3;3;2,3;2;2,Reject,0,0,0,yes,9/27/18,Iowa State University;Iowa State University;Iowa State University,169;169;169,341;341;341,,9/27/18,0,0,0,0,0,0,119;2;0,36;3;1,6;1;0,11;0;0,-1;-1
2203,ICLR,2019,Deep Perm-Set Net: Learn to predict sets with unknown permutation and cardinality using deep neural networks,S. Hamid Rezatofighi;Roman Kaskman;Farbod T. Motlagh;Qinfeng Shi;Daniel Cremers;Laura Leal-Taixé;Ian Reid,hamid.rezatofighi@adelaide.edu.au;roman.kaskman@tum.de;farbod.motlagh@student.adelaide.edu.au;javen.shi@adelaide.edu.au;cremers@tum.de;leal.taixe@tum.de;ian.reid@adelaide.edu.au,7;3;3,3;3;4,Reject,0,7,0,yes,9/27/18,The University of Adelaide;Technical University Munich;The University of Adelaide;The University of Adelaide;Technical University Munich;Technical University Munich;The University of Adelaide,123;54;123;123;54;54;123,134;41;134;134;41;41;134,2,5/2/18,13,5,5,1,9,2,1092;17;24;3617;24880;2785;17300,39;3;4;101;494;73;345,16;2;2;26;76;21;64,142;4;2;373;2727;522;1994,-1;-1
2204,ICLR,2019,DEEP-TRIM: REVISITING L1 REGULARIZATION FOR CONNECTION PRUNING OF DEEP NETWORK,Chih-Kuan Yeh;Ian E.H. Yen;Hong-You Chen;Chun-Pei Yang;Shou-De Lin;Pradeep Ravikumar,cjyeh@cs.cmu.edu;eyan2@snapchat.com;applebasket70179@gmail.com;skylyyang@gmail.com;sdlin@csie.ntu.edu.tw;pradeep.ravikumar@gmail.com,4;6;4,4;3;3,Reject,0,3,0,yes,9/27/18,Carnegie Mellon University;Snap Inc.;Ohio State University;;National Taiwan University;Carnegie Mellon University,1;-1;76;-1;85;1,24;-1;318;-1;197;24,9,9/27/18,0,0,0,0,0,0,271;479;6;0;1693;8439,21;52;8;1;181;181,7;12;2;0;22;38,42;68;0;0;112;1194,-1;-1
2205,ICLR,2019,INTERPRETABLE CONVOLUTIONAL FILTER PRUNING,Zhuwei Qin;Fuxun Yu;Chenchen Liu;Xiang Chen,zqin@gmu.edu;fyu2@gmu.edu;chliu@clarkson.edu;xchen26@gmu.edu,4;4;3,4;3;4,Reject,0,16,0,yes,9/27/18,George Mason University;George Mason University;Clarkson University;George Mason University,99;99;478;99,336;336;1103;336,,9/27/18,4,1,2,0,5,1,72;445;430;13538,14;49;67;574,4;11;9;53,5;23;39;831,-1;-1
2206,ICLR,2019,Learning Internal Dense But External Sparse Structures of Deep Neural Network,Yiqun Duan,duanyiquncc@gmail.com,5;5;6,3;3;2,Reject,0,5,0,yes,9/27/18,University of British Columbia,36,34,,9/27/18,0,0,0,0,0,0,20,10,3,1,-1
2207,ICLR,2019,Understand the dynamics of GANs via Primal-Dual Optimization,Songtao Lu;Rahul Singh;Xiangyi Chen;Yongxin Chen;Mingyi Hong,lus@umn.edu;rasingh@gatech.edu;chen5719@umn.edu;yongchen@gatech.edu;mhong@umn.edu,4;5;6,4;3;3,Reject,3,4,0,yes,9/27/18,"University of Minnesota, Minneapolis;Georgia Institute of Technology;University of Minnesota, Minneapolis;Georgia Institute of Technology;University of Minnesota, Minneapolis",57;13;57;13;57,56;33;56;33;56,5;4,9/27/18,6,4,0,0,0,0,361;485;402;857;5121,83;136;23;106;205,10;10;8;14;33,14;27;70;38;528,-1;-1
2208,ICLR,2019,Do Language Models Have Common Sense?,Trieu H. Trinh;Quoc V. Le,thtrieu@google.com;qvl@google.com,5;4;4,4;4;4,Reject,6,1,0,yes,9/27/18,Google;Google,-1;-1,-1;-1,3,9/27/18,5,4,0,0,0,0,172;47503,5;192,4;79,18;5974,-1;-1
2209,ICLR,2019,"Prototypical Examples in Deep Learning: Metrics, Characteristics, and Utility",Nicholas Carlini;Ulfar Erlingsson;Nicolas Papernot,nicholas@carlini.com;ulfar@google.com;papernot@google.com,5;3;5,4;3;4,Reject,0,14,0,yes,9/27/18,University of California Berkeley;Google;Google,5;-1;-1,18;-1;-1,4,9/27/18,11,7,1,0,0,0,5765;538;9133,44;10;66,20;4;27,989;124;1049,-1;-1
2210,ICLR,2019,Learning From the Experience of Others: Approximate Empirical Bayes in Neural Networks,Han Zhao;Yao-Hung Hubert Tsai;Ruslan Salakhutdinov;Geoff Gordon,han.zhao@cs.cmu.edu;yaohungt@cs.cmu.edu;rsalakhu@cs.cmu.edu;ggordon@cs.cmu.edu,6;3;7,4;5;4,Reject,0,7,0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1,24;24;24;24,,9/27/18,0,0,0,0,0,0,1026;529;66968;203,125;63;254;33,18;12;81;8,66;71;7763;11,-1;-1
2211,ICLR,2019,Pearl: Prototype lEArning via Rule Lists,Tianfan Fu*;Tian Gao*;Cao Xiao*;Tengfei Ma*;Jimeng Sun,tfu42@gatech.edu;tgao@us.ibm.com;cxiao@us.ibm.com;tengfei.ma1@ibm.com;jsun@cc.gatech.edu,5;3;4,4;3;4,Reject,0,4,0,yes,9/27/18,Georgia Institute of Technology;International Business Machines;International Business Machines;International Business Machines;Georgia Institute of Technology,13;-1;-1;-1;13,33;-1;-1;-1;33,,9/27/18,3,1,1,0,0,0,179;1164;15;730;10320,16;193;26;35;240,5;18;2;12;55,13;40;0;85;785,-1;-1
2212,ICLR,2019,Optimal margin Distribution Network,Shen-Huan Lv;Lu Wang;Zhi-Hua Zhou,lvsh@lamda.nju.edu.cn;wangl@lamda.nju.edu.cn;zhouzh@lamda.nju.edu.cn,5;5;6,4;5;3,Reject,3,4,0,yes,9/27/18,Zhejiang University;Zhejiang University;Zhejiang University,57;57;57,177;177;177,11;8,9/27/18,3,1,1,0,6,1,3;997;30675,2;239;590,1;14;82,1;74;3003,-1;-1
2213,ICLR,2019,An Information-Theoretic Metric of Transferability for Task Transfer Learning,Yajie Bao;Yang Li;Shao-Lun Huang;Lin Zhang;Amir R. Zamir;Leonidas J. Guibas,byjem123@163.com;tori2011@gmail.com;shaolun.huang@sz.tsinghua.edu.cn;linzhang@tsinghua.edu.cn;zamir@cs.stanford.edu;guibas@cs.stanford.edu,5;6;6,3;3;4,Reject,0,5,0,yes,9/27/18,163;Tsinghua University;Tsinghua University;Tsinghua University;Stanford University;Stanford University,-1;8;8;8;4;4,-1;30;30;30;3;3,6,9/27/18,3,3,1,1,0,1,179;1222;261;561;5153;45340,15;529;59;223;50;699,8;14;10;12;21;98,6;82;20;26;1113;5553,-1;-1
2214,ICLR,2019,Selective Convolutional Units: Improving CNNs via Channel Selectivity,Jongheon Jeong;Jinwoo Shin,jongheonj@kaist.ac.kr;jinwoos@kaist.ac.kr,6;5;5,2;3;3,Reject,0,5,0,yes,9/27/18,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,20;20,95;95,,9/27/18,0,0,0,0,0,0,5;1681,7;184,1;18,0;219,-1;-1
2215,ICLR,2019,Sufficient Conditions for Robustness to Adversarial Examples: a Theoretical and Empirical Study with Bayesian Neural Networks,Yarin Gal;Lewis Smith,yarin@cs.ox.ac.uk;lsgs@robots.ox.ac.uk,5;5;4,4;3;4,Reject,0,16,1,yes,9/27/18,University of Oxford;University of Oxford,50;50,1;1,11;4;1,9/27/18,0,0,0,0,0,0,6448;129,90;17,20;5,960;12,-1;-1
2216,ICLR,2019,Empirical Bounds on Linear Regions of Deep Rectifier Networks,Thiago Serra;Srikumar Ramalingam,tserra@gmail.com;srikumar.ramalingam@gmail.com,6;7;6,4;4;4,Reject,0,5,0,yes,9/27/18,Carnegie Mellon University;University of Utah,1;52,24;200,1,9/27/18,8,6,1,1,15,0,110;2762,21;106,6;26,10;314,-1;-1
2217,ICLR,2019,Provable Defenses against Spatially Transformed Adversarial Inputs: Impossibility and Possibility Results,Xinyang Zhang;Yifan Huang;Chanh Nguyen;Shouling Ji;Ting Wang,xizc15@lehigh.edu;yih319@lehigh.edu;cpn217@lehigh.edu;sji@zju.edu.cn;inbox.ting@gmail.com,5;3;5,3;4;3,Reject,0,0,0,yes,9/27/18,Lehigh University;Lehigh University;Lehigh University;Zhejiang University;Lehigh University,261;261;261;57;261,533;533;533;177;533,4,9/27/18,0,0,0,0,0,0,578;75;491;1643;257,68;63;73;135;79,11;4;10;23;9,47;4;31;126;9,-1;-1
2218,ICLR,2019,On Meaning-Preserving Adversarial Perturbations for Sequence-to-Sequence Models,Paul Michel;Graham Neubig;Xian Li;Juan Miguel Pino,pmichel1@cs.cmu.edu;gneubig@cs.cmu.edu;xianl@fb.com;juancarabina@fb.com,4;6;4;3,4;3;4;4,Reject,0,11,0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;Facebook;Facebook,1;1;-1;-1,24;24;-1;-1,3;4,9/27/18,0,0,0,0,0,0,512;5424;1477;51,17;443;132;25,7;39;19;4,51;569;81;3,-1;-1
2219,ICLR,2019,Collapse of deep and narrow neural nets,Lu Lu;Yanhui Su;George Em Karniadakis,lu_lu_1@brown.edu;suyh@fzu.edu.cn;george_karniadakis@brown.edu,7;6;4,5;4;4,Reject,0,7,0,yes,9/27/18,Brown University;Tsinghua University;Brown University,65;8;65,50;30;50,,8/15/18,8,3,2,0,13,0,1238;50;23455,138;18;808,17;3;71,69;3;1621,-1;-1
2220,ICLR,2019,Diminishing Batch Normalization,Yintai Ma;Diego Klabjan,yintaima2020@u.northwestern.edu;d-klabjan@northwestern.edu,4;3;4,4;3;5,Reject,0,5,0,yes,9/27/18,Northwestern University;Northwestern University,44;44,20;20,9;8,9/27/18,1,0,0,0,0,0,22;2613,5;215,2;26,0;196,-1;-1
2221,ICLR,2019,The Nonlinearity Coefficient - Predicting Generalization in Deep Neural Networks,George Philipp;Jaime G. Carbonell,george.philipp@email.de;jgc@cs.cmu.edu,4;5;7,3;4;4,Reject,1,11,0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University,1;1,24;24,8,6/1/18,7,6,1,0,33,2,52;15711,13;507,5;55,8;1628,-1;-1
2222,ICLR,2019,Multi-way Encoding for Robustness to Adversarial Attacks,Donghyun Kim;Sarah Adel Bargal;Jianming Zhang;Stan Sclaroff,donhk@bu.edu;sbargal@bu.edu;jianmzha@adobe.com;sclaroff@bu.edu,4;6;6;6,4;3;2;2,Reject,3,14,1,yes,9/27/18,Boston University;Boston University;Adobe Systems;Boston University,65;65;-1;65,70;70;-1;70,4;2,9/27/18,1,1,1,0,0,0,1478;336;248;13761,181;20;84;300,18;7;8;60,133;60;15;1297,-1;-1
2223,ICLR,2019,Improved robustness to adversarial examples using Lipschitz regularization of the loss,Chris Finlay;Adam M. Oberman;Bilal Abbasi,christopher.finlay@mail.mcgill.ca;adam.oberman@mcgill.ca;bilal.abbasi@mail.mcgill.ca,4;6;6,3;1;3,Reject,1,10,0,yes,9/27/18,McGill University;McGill University;McGill University,85;85;85,42;42;42,4,9/27/18,11,5,3,0,7,0,146;1646;35,48;88;12,8;22;4,10;157;0,-1;-1
2224,ICLR,2019,CONTROLLING COVARIATE SHIFT USING EQUILIBRIUM NORMALIZATION OF WEIGHTS,Aaron Defazio,aaron.defazio@gmail.com,4;6;7,4;1;4,Reject,0,2,0,yes,9/27/18,Facebook,-1,-1,,9/27/18,0,0,0,0,0,0,1288,28,9,265,-1
2225,ICLR,2019,Implicit Maximum Likelihood Estimation,Ke Li;Jitendra Malik,ke.li@eecs.berkeley.edu;malik@eecs.berkeley.edu,3;4;5,4;4;4,Reject,2,2,0,yes,9/27/18,University of California Berkeley;University of California Berkeley,5;5,18;18,,9/24/18,25,10,14,1,0,3,6190;68980,781;429,36;115,290;7755,-1;-1
2226,ICLR,2019,Mental Fatigue Monitoring using Brain Dynamics Preferences,Yuangang Pan;Avinash K Singh;Ivor W. Tsang;Chin-teng Lin,yuangang.pan@student.uts.edu.au;avinashsingh@outlook.com;ivor.tsang@uts.edu.au;chin-teng.lin@uts.edu.au,7;4;2,3;3;5,Reject,0,6,0,yes,9/27/18,University of Technology Sydney;University of Technology Sydney;University of Technology Sydney;University of Technology Sydney,106;106;106;106,216;216;216;216,8,9/27/18,0,0,0,0,0,0,23;1092;10585;4128,15;173;253;233,3;16;50;29,0;41;1263;286,-1;-1
2227,ICLR,2019,Gradient-based learning for F-measure and other performance metrics,Yu Gai;Zheng Zhang;Kyunghyun Cho,yg1246@nyu.edu;zz@nyu.edu;kyunghyun.cho@nyu.edu,3;5;5,4;3;5,Reject,0,6,0,yes,9/27/18,New York University;New York University;New York University,26;26;26,27;27;27,,9/27/18,0,0,0,0,0,0,64;110;45405,7;55;273,2;6;52,6;5;6560,-1;-1
2228,ICLR,2019,MANIFOLDNET: A DEEP NEURAL NETWORK FOR MANIFOLD-VALUED DATA,Rudrasis Chakraborty;Jose Bouza;Jonathan Manton;Baba C. Vemuri,rudrasischa@gmail.com;josebouza@ufl.edu;jonathan.manton@ieee.org;baba.vemuri@gmail.com,5;4;4,3;4;4,Reject,0,12,0,yes,9/27/18,University of California Berkeley;University of Florida;;University of Florida,5;123;-1;123,18;143;-1;143,1;8,9/27/18,0,0,0,0,0,0,262;12;2728;10154,54;4;191;320,9;2;26;45,16;1;227;780,-1;-1
2229,ICLR,2019,REPRESENTATION COMPRESSION AND GENERALIZATION IN DEEP NEURAL NETWORKS,Ravid Shwartz-Ziv;Amichai Painsky;Naftali Tishby,ravid.ziv@mail.huji.ac.il;amichai.painsky@mail.huji.ac.il;tishby@cs.huji.ac.il,3;6;4,3;3;3,Reject,0,3,0,yes,9/27/18,Hebrew University of Jerusalem;Hebrew University of Jerusalem;Hebrew University of Jerusalem,65;65;65,205;205;205,1;8,9/27/18,3,2,0,0,0,0,530;150;12057,7;30;263,3;7;45,53;4;1230,-1;-1
2230,ICLR,2019,Distributionally Robust Optimization Leads to Better Generalization: on SGD and Beyond,Jikai Hou;Kaixuan Huang;Zhihua Zhang,houjikai@pku.edu.cn;hackyhuang@pku.edu.cn;zhzhang@math.pku.edu.cn,3;4;5,3;4;4,Reject,9,5,0,yes,9/27/18,Peking University;Peking University;Peking University,24;24;24,27;27;27,1;9;8,9/27/18,1,0,0,0,0,0,13;153;5090,4;16;407,2;7;29,1;21;521,-1;-1
2231,ICLR,2019,A Priori Estimates  of the Generalization Error for Two-layer Neural Networks,Lei Wu;Chao Ma;Weinan E,leiwu@pku.edu.cn;chaom@princeton.edu;weinan@math.princeton.edu,4;4;4;5,3;3;4;3,Reject,0,0,0,yes,9/27/18,Peking University;Princeton University;Princeton University,24;30;30,27;7;7,8,9/27/18,18,4,1,1,3,1,2915;1337;7064,183;164;261,29;19;47,202;79;513,-1;-1
2232,ICLR,2019,Online Hyperparameter Adaptation via Amortized Proximal Optimization,Paul Vicol;Jeffery Z. HaoChen;Roger Grosse,pvicol@cs.toronto.edu;zhc15@mails.tsinghua.edu.cn;rgrosse@cs.toronto.edu,7;6;5,3;4;4,Reject,0,11,0,yes,9/27/18,"Department of Computer Science, University of Toronto;Tsinghua University;Department of Computer Science, University of Toronto",18;8;18,22;30;22,9,9/27/18,0,0,0,0,0,0,143;18;189,19;3;50,6;1;8,16;5;12,-1;-1
2233,ICLR,2019,Adversarial Exploration Strategy for Self-Supervised Imitation Learning,Zhang-Wei Hong;Tsu-Jui Fu;Tzu-Yun Shann;Yi-Hsiang Chang;Chun-Yi Lee,williamd4112@gapp.nthu.edu.tw;yesray0216@gmail.com;ariel@shann.net;shawn420@gapp.nthu.edu.tw;cylee@cs.nthu.edu.tw,7;5;5,3;3;3,Reject,0,12,0,yes,9/27/18,National Tsing Hua University;;University of British Columbia;National Tsing Hua University;National Tsing Hua University,199;-1;36;199;199,323;-1;34;323;323,4,9/27/18,1,0,0,0,6,0,183;61;66;67;758,12;16;6;15;73,4;4;3;4;18,16;15;7;10;55,-1;-1
2234,ICLR,2019,On Learning Heteroscedastic Noise Models within Differentiable Bayes Filters,Alina Kloss;Jeannette Bohg,alina.kloss@tuebingen.mpg.de;bohg@stanford.edu,6;6;4,4;5;4,Reject,0,6,0,yes,9/27/18,"Max Planck Institute for Intelligent Systems, Max-Planck Institute;Stanford University",-1;4,-1;3,11,9/27/18,1,0,0,0,0,0,78;1860,8;87,3;21,4;98,-1;-1
2235,ICLR,2019,Manifold regularization with GANs for semi-supervised learning,Bruno Lecouat;Chuan-Sheng Foo;Houssam Zenati;Vijay Chandrasekhar,bruno_lecouat@i2r.a-star.edu.sg;foo_chuan_sheng@i2r.a-star.edu.sg;houssam.zenati@student.ecp.fr;vijay@i2r.a-star.edu.sg,7;5;5,4;4;4,Reject,0,4,0,yes,9/27/18,A*STAR;A*STAR;Ecole Centrale Paris;A*STAR,-1;-1;478;-1,-1;-1;452;-1,5;4,7/11/18,9,4,6,2,12,2,258;259;258;2976,11;11;11;109,6;6;6;28,53;59;54;225,-1;-1
2236,ICLR,2019,SOSELETO: A Unified Approach to Transfer Learning and Training with Noisy Labels,Or Litany;Daniel Freedman,orlitany@gmail.com;danielfreedman@google.com,7;5;5,4;4;4,Reject,0,4,0,yes,9/27/18,Facebook;Google,-1;-1,-1;-1,6,5/24/18,4,2,1,0,12,0,579;1716,34;64,12;17,57;149,-1;-1
2237,ICLR,2019,SOLAR: Deep Structured Representations for Model-Based Reinforcement Learning,Marvin Zhang*;Sharad Vikram*;Laura Smith;Pieter Abbeel;Matthew Johnson;Sergey Levine,marvin@cs.berkeley.edu;svikram@cs.ucsd.edu;smithlaura@berkeley.edu;pabbeel@cs.berkeley.edu;mattjj@google.com;svlevine@cs.berkeley.edu,5;5;5,4;4;3,Reject,0,10,0,yes,9/27/18,"University of California Berkeley;University of California, San Diego;University of California Berkeley;University of California Berkeley;Google;University of California Berkeley",5;11;5;5;-1;5,18;31;18;18;-1;18,,8/28/18,60,27,20,1,97,7,290;231;32;36532;1447;24386,11;21;4;433;52;309,7;8;1;94;15;73,31;21;3;4402;144;3167,-1;-1
2238,ICLR,2019,Selective Self-Training for semi-supervised Learning,Jisoo Jeong;Seungeui Lee;Nojun Kwak,soo3553@snu.ac.kr;dehlix@snu.ac.kr;nojunk@snu.ac.kr,5;4;4,4;5;4,Reject,0,5,0,yes,9/27/18,Seoul National University;Seoul National University;Seoul National University,41;41;41,74;74;74,,9/27/18,1,1,0,0,0,0,109;43;2471,8;6;135,3;3;20,16;3;293,-1;-1
2239,ICLR,2019,Dissecting an Adversarial framework for Information Retrieval,Ameet Deshpande;Mitesh M.Khapra,cs15b001@cse.iitm.ac.in;miteshk@cse.iitm.ac.in,6;5;4,4;3;4,Reject,0,3,0,yes,9/27/18,Indian Institute of Technology Madras;Indian Institute of Technology Madras,153;153,625;625,5;4,9/27/18,0,0,0,0,0,0,15;1329,10;91,2;18,1;152,-1;-1
2240,ICLR,2019,Learning Latent Semantic Representation from Pre-defined Generative Model,Jin-Young Kim;Sung-Bae Cho,seago0828@yonsei.ac.kr;sbcho@yonsei.ac.kr,5;3;4,2;5;3,Reject,0,0,0,yes,9/27/18,Yonsei University;Yonsei University,478;478,231;231,5;1,9/27/18,0,0,0,0,0,0,1304;6545,200;583,21;37,70;371,-1;-1
2241,ICLR,2019,Adversarial Information Factorization,Antonia Creswell;Yumnah Mohamied;Biswa Sengupta;Anil Bharath,ac2211@ic.ac.uk;ym1008@ic.ac.uk;biswasengupta@gmail.com;aab01@ic.ac.uk,6;6;6,4;4;4,Reject,2,27,0,yes,9/27/18,Imperial College London;Imperial College London;;Imperial College London,72;72;-1;72,8;8;-1;8,5,11/14/17,6,4,2,0,0,3,460;392;721;2780,15;9;42;128,7;6;13;20,34;16;49;167,-1;-1
2242,ICLR,2019,TequilaGAN: How To Easily Identify GAN Samples,Rafael Valle;Wilson Cai;Anish P. Doshi,rafaelvalle@berkeley.edu;wcai@berkeley.edu;apdoshi@berkeley.edu,5;4;6,4;4;5,Reject,0,5,0,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,18;18;18,5;4,7/13/18,6,4,1,0,17,1,244;18;18,18;3;2,6;2;2,44;2;2,-1;-1
2243,ICLR,2019,Guided Evolutionary Strategies: Escaping the curse of dimensionality in random search,Niru Maheswaranathan;Luke Metz;George Tucker;Dami Choi;Jascha Sohl-Dickstein,nirum@google.com;lmetz@google.com;gjt@google.com;damichoi@google.com;jaschasd@google.com,5;4;6,3;3;4,Reject,0,5,0,yes,9/27/18,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,6,6/26/18,26,17,10,1,0,5,729;7178;174;169;4921,38;25;4;6;101,12;10;3;3;33,54;1200;28;32;684,-1;-1
2244,ICLR,2019,DL2: Training and Querying Neural Networks with Logic,Marc Fischer;Mislav Balunovic;Dana Drachsler-Cohen;Timon Gehr;Ce Zhang;Martin Vechev,marcfisc@student.ethz.ch;bmislav@student.ethz.ch;dana.drachsler@inf.ethz.ch;timon.gehr@inf.ethz.ch;ce.zhang@inf.ethz.ch;martin.vechev@inf.ethz.ch,6;7;5,2;4;4,Reject,0,8,0,yes,9/27/18,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10;10;10;10,10;10;10;10;10;10,,9/27/18,16,8,4,0,0,1,137;68;496;720;866;4180,25;8;14;22;65;153,7;5;7;9;14;35,6;2;59;99;81;467,-1;-1
2245,ICLR,2019,Déjà Vu: An Empirical Evaluation of the Memorization Properties of Convnets,Alexandre Sablayrolles;Matthijs Douze;Cordelia Schmid;Hervé Jégou,asablayrolles@fb.com;matthijs@fb.com;cordelia.schmid@inria.fr;rvj@fb.com,6;4;5,2;2;4,Reject,0,4,0,yes,9/27/18,Facebook;Facebook;INRIA;Facebook,-1;-1;-1;-1,-1;-1;-1;-1,,9/17/18,7,5,0,0,42,0,132;10267;72250;13686,8;98;430;165,6;31;113;40,14;1793;10110;2368,-1;-1
2246,ICLR,2019,iRDA Method for Sparse Convolutional Neural Networks,Xiaodong Jia;Liang Zhao;Lian Zhang;Juncai He;Jinchao Xu,jiaxiaodong1994@gmail.com;zhaoliang14@lsec.cc.ac.cn;lzhangay@ust.hk;juncaihe@pku.edu.cn;xu@math.psu.edu,3;3;3,5;4;5,Reject,0,0,0,yes,9/27/18,Peking University;Chinese Academy of Sciences;The Hong Kong University of Science and Technology;Peking University;Pennsylvania State University,24;62;39;24;41,27;1103;44;27;77,,9/27/18,0,0,0,0,0,0,402;637;334;69;7278,54;119;73;14;289,13;13;9;4;41,15;37;24;3;569,-1;-1
2247,ICLR,2019,Variation Network: Learning High-level Attributes for Controlled Input Manipulation,Gaëtan Hadjeres,hadjeres.g@gmail.com,3;6;4,4;2;3,Reject,0,3,0,yes,9/27/18,,,,5,9/27/18,2,1,0,0,5,0,217,15,7,10,-1
2248,ICLR,2019,Consistency-based anomaly detection with adaptive multiple-hypotheses predictions,Duc Tam Nguyen;Zhongyu Lou;Michael Klar;Thomas Brox,nguyen@cs.uni-freiburg.de;zhongyu.lou@de.bosch.com;michael.klar2@de.bosch.com;brox@cs.uni-freiburg.de,4;5;5,4;3;4,Reject,0,6,0,yes,9/27/18,Universität Freiburg;Bosch;Bosch;Universität Freiburg,123;-1;-1;123,82;-1;-1;82,5;4,9/27/18,4,0,0,0,4,0,35;236;64;37731,16;31;22;256,4;8;5;72,3;16;14;5890,-1;-1
2249,ICLR,2019,"VARIATIONAL SGD: DROPOUT , GENERALIZATION AND CRITICAL POINT AT THE END OF CONVEXITY",Michael Tetelman,michael.tetelman@gmail.com,4;2;2,3;4;5,Reject,0,1,2,yes,9/27/18,"ARRAIY, INC",-1,-1,11,9/27/18,0,0,0,0,0,0,44,7,2,1,-1
2250,ICLR,2019,Learning to remember: Dynamic Generative Memory for Continual Learning,Oleksiy Ostapenko;Mihai Puscas;Tassilo Klein;Moin Nabi,oleksiy.ostapenko@sap.com;mihai.puscas@sap.com;mihaimarian.puscas@unitn.it;tassilo.klein@sap.com;m.nabi@sap.com,4;3;8,5;5;5,Reject,0,7,0,yes,9/27/18,SAP;SAP;University of Trento;SAP;SAP,314;314;18;314;314,300;300;258;300;300,5;4,9/27/18,1,0,1,0,0,0,26;526;592;682,15;10;43;43,2;6;10;12,3;34;65;64,-1;-1
2251,ICLR,2019,Perfect Match: A Simple Method for Learning Representations For Counterfactual Inference With Neural Networks,Patrick Schwab;Lorenz Linhardt;Walter Karlen,patrick.schwab@hest.ethz.ch;llorenz@student.ethz.ch;walter.karlen@hest.ethz.ch,5;5;6,3;4;4,Reject,0,11,0,yes,9/27/18,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10,10;10;10,,9/27/18,15,5,10,0,9,1,145;53;1281,26;7;140,8;4;19,12;4;115,-1;-1
2252,ICLR,2019,Pyramid Recurrent Neural Networks for Multi-Scale Change-Point Detection,Zahra Ebrahimzadeh;Min Zheng;Selcuk Karakas;Samantha Kleinberg,shina.ebiz@gmail.com;mzheng3@stevens.edu;fkarakas@stevens.edu;samantha.kleinberg@stevens.edu,7;4;6,4;5;3,Reject,0,3,0,yes,9/27/18,Stevens Institute of Technology;Stevens Institute of Technology;Stevens Institute of Technology;Stevens Institute of Technology,153;153;153;153,512;512;512;512,,9/27/18,1,0,0,0,0,0,44;85;1;422,7;28;2;44,2;4;1;10,2;1;0;28,-1;-1
2253,ICLR,2019,BIGSAGE: unsupervised inductive representation learning of graph via bi-attended sampling and global-biased aggregating,Xin Luo;Hankz Hankui Zhuo,luox35@mail2.sysu.edu.cn;zhuohank@mail.sysu.edu.cn,2;4;4,4;3;4,Reject,0,0,0,yes,9/27/18,SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY,478;478,352;352,6;10,9/27/18,0,0,0,0,0,0,155;48,27;14,4;3,1;3,-1;-1
2254,ICLR,2019,Stochastic Learning of Additive Second-Order Penalties with  Applications to Fairness,Heinrich Jiang;Yifan Wu;Ofir Nachum,heinrichj@google.com;yw4@andrew.cmu.edu;ofirnachum@google.com,5;5;4,3;4;3,Reject,0,3,0,yes,9/27/18,Google;Carnegie Mellon University;Google,-1;1;-1,-1;24;-1,7,9/27/18,0,0,0,0,0,0,265;516;1038,28;112;41,9;13;15,27;43;155,-1;-1
2255,ICLR,2019,Learning to Augment Influential Data,Donghoon Lee;Chang D. Yoo,iamdh@kaist.ac.kr;cd_yoo@kaist.ac.kr,6;6;5,4;4;4,Reject,0,3,1,yes,9/27/18,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,20;20,95;95,8,9/27/18,1,1,0,0,0,0,1028;1935,93;162,13;22,98;189,-1;-1
2256,ICLR,2019,Generative Adversarial Network Training is a Continual Learning Problem,Kevin J Liang;Chunyuan Li;Guoyin Wang;Lawrence Carin,kevin.liang@duke.edu;chunyuan.li@duke.edu;guoyin.wang@duke.edu;lcarin@duke.edu,7;5;3,4;4;5,Reject,6,13,0,yes,9/27/18,Duke University;Duke University;Duke University;Duke University,44;44;44;44,17;17;17;17,5;4,9/27/18,16,9,1,1,2,0,159;2016;2952;19178,19;80;324;819,7;25;28;65,4;243;173;1986,-1;-1
2257,ICLR,2019,Multi-objective training of Generative Adversarial Networks with multiple discriminators,Isabela Albuquerque;João Monteiro;Thang Doan;Breandan Considine;Tiago Falk;Ioannis Mitliagkas,isabelamcalbuquerque@gmail.com;joaomonteirof@gmail.com;thang.doan@mail.mcgill.ca;breandan.considine@gmail.com;falk@emt.inrs.ca;ioannis@iro.umontreal.ca,6;6;5,4;3;3,Reject,1,13,0,yes,9/27/18,Institut national de la recherche scientifique;Institut national de la recherche scientifique;McGill University;University of Montreal;Institut national de la recherche scientifique;University of Montreal,-1;-1;85;123;-1;123,-1;-1;42;108;-1;108,5;4,9/27/18,10,4,3,0,5,0,103;160;59;10;2989;1176,24;44;14;5;203;43,3;6;6;1;28;18,6;6;8;0;237;193,-1;-1
2258,ICLR,2019,Few-Shot Intent Inference via Meta-Inverse Reinforcement Learning,Kelvin Xu;Ellis Ratner;Anca Dragan;Sergey Levine;Chelsea Finn,kelvinxu@eecs.berkeley.edu;eratner@berkeley.edu;anca@berkeley.edu;svlevine@eecs.berkeley.edu;cbfinn@eecs.berkeley.edu,3;4;4,5;3;4,Reject,0,8,0,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5;5,18;18;18;18;18,6,9/27/18,4,1,2,0,0,0,7326;51;26;24386;7689,19;7;2;309;98,11;4;2;73;33,756;4;3;3167;1026,-1;-1
2259,ICLR,2019,Latent Domain Transfer: Crossing modalities with Bridging Autoencoders,Yingtao Tian;Jesse Engel,yittian@cs.stonybrook.edu;jesseengel@google.com,4;4;4,4;4;4,Reject,1,3,0,yes,9/27/18,"State University of New York, Stony Brook;Google",41;-1,258;-1,5;4,9/27/18,0,0,0,0,0,0,556;2183,44;36,11;12,61;245,-1;-1
2260,ICLR,2019,TENSOR RING NETS ADAPTED DEEP MULTI-TASK LEARNING,Xinqi Chen;Ming Hou;Guoxu Zhou;Qibin Zhao,xinqicham@gmail.com;ming.hou@riken.jp;gx.zhou@gdut.edu.cn;qibin.zhao@riken.jp,5;6;4,4;4;4,Reject,0,7,0,yes,9/27/18,South China University of Technology;RIKEN;South China University of Technology;RIKEN,478;-1;478;-1,576;-1;576;-1,,9/27/18,0,0,0,0,0,0,1930;487;2583;81,71;19;64;10,21;10;23;4,59;55;185;1,-1;-1
2261,ICLR,2019,Overcoming Multi-model Forgetting,Yassine Benyahia*;Kaicheng Yu*;Kamil Bennani-Smires;Martin Jaggi;Anthony Davison;Mathieu Salzmann;Claudiu Musat,yassine.benyahia1@gmail.com;kaicheng.yu@epfl.ch;kamil.bennani-smires@swisscom.com;martin.jaggi@epfl.ch;anthony.davison@epfl.ch;mathieu.salzmann@epfl.ch;claudiu.musat@swisscom.com,6;5;6,2;5;4,Reject,2,4,0,yes,9/27/18,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swisscom;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swisscom,478;478;-1;478;478;478;-1,38;38;-1;38;38;38;-1,3;2,9/27/18,5,5,4,0,0,1,5;142;36;3742;19;5531;198,1;9;6;114;14;198;37,1;5;3;27;2;42;6,1;13;9;553;2;617;28,-1;-1
2262,ICLR,2019,Learning to Separate Domains in Generalized Zero-Shot and Open Set Learning: a probabilistic perspective,Hanze Dong;Yanwei Fu;Leonid Sigal;SungJu Hwang;Yu-Gang Jiang;Xiangyang Xue,hzdong15@fudan.edu.cn;yanweifu@fudan.edu.cn;lsigal@cs.ubc.ca;sjhwang82@kaist.ac.kr;ygj@fudan.edu.cn;xyxue@fudan.edu.cn,5;5;6,4;3;3,Reject,1,10,0,yes,9/27/18,Fudan University;Fudan University;University of British Columbia;Korea Advanced Institute of Science and Technology;Fudan University;Fudan University,78;78;36;20;78;78,116;116;34;95;116;116,6,9/27/18,5,0,2,0,5,0,18;1861;6616;1101;8140;5675,6;90;166;71;204;281,3;22;39;16;44;38,1;247;666;124;969;581,-1;-1
2263,ICLR,2019,Uncovering Surprising Behaviors in Reinforcement Learning via Worst-case Analysis,Avraham Ruderman;Richard Everett;Bristy Sikder;Hubert Soyer;Jonathan Uesato;Ananya Kumar;Charlie Beattie;Pushmeet Kohli,aruderman@google.com;reverett@google.com;bristy@google.com;soyer@google.com;juesato@google.com;skywalker94@gmail.com;cbeattie@google.com;pushmeet@google.com,7;5;6,4;3;2,Reject,0,5,0,yes,9/27/18,Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1,8,9/27/18,10,4,1,1,0,0,593;90;10;2101;895;68;195;22034,13;23;2;22;17;8;3;313,8;5;1;11;11;4;2;69,63;4;0;258;110;8;13;2746,-1;-1
2264,ICLR,2019,Efficient Codebook and Factorization for Second Order Representation Learning,Pierre jacob;David Picard;Aymeric Histace;Edouard Klein,pierre.jacob@ensea.fr;picard@ensea.fr;aymeric.histace@ensea.fr;edouard.klein@gendarmerie.interieur.gouv.fr,4;5;6,5;2;4,Reject,0,8,0,yes,9/27/18,ETIS;ETIS;ETIS;,-1;-1;-1;-1,-1;-1;-1;-1,,9/27/18,1,0,0,0,0,0,1113;1150;499;125,193;100;146;17,20;19;11;4,92;100;32;12,-1;-1
2265,ICLR,2019,Metric-Optimized Example Weights,Sen Zhao;Mahdi Milani Fard;Maya Gupta,senzhao@google.com;mmilanifard@google.com;mayagupta@google.com,4;4;7,4;4;3,Reject,0,0,0,yes,9/27/18,Google;Google;Google,-1;-1;-1,-1;-1;-1,7,5/27/18,5,1,1,1,7,0,185;195;2898,16;20;146,5;9;27,17;20;239,-1;-1
2266,ICLR,2019,Unsupervised classification into unknown number of classes,Sungyeob Han;Daeyoung Kim;Jungwoo Lee,syhan@cml.snu.ac.kr;kimdy7@snu.ac.kr;junglee@snu.ac.kr,4;4;5,2;4;4,Reject,0,5,0,yes,9/27/18,Seoul National University;Seoul National University;Seoul National University,41;41;41,74;74;74,5;4;10,9/27/18,0,0,0,0,0,0,68;159;711,3;68;180,1;7;12,4;3;50,-1;-1
2267,ICLR,2019,Fortified Networks: Improving the Robustness of Deep Networks by Modeling the Manifold of Hidden Representations,Alex Lamb;Jonathan Binas;Anirudh Goyal;Dmitriy Serdyuk;Sandeep Subramanian;Ioannis Mitliagkas;Yoshua Bengio,lambalex@iro.umontreal.ca;jonathan.binas@umontreal.ca;anirudhgoyal9119@gmail.com;serdyuk.dmitriy@gmail.com;sandeep.subramanian@gmail.com;ioannis@iro.umontreal.ca;yoshua.bengio@mila.quebec,4;5;9;6,5;3;4;3,Reject,0,27,0,yes,9/27/18,University of Montreal;University of Montreal;University of Montreal;Element AI;University of Montreal;University of Montreal;University of Montreal,123;123;123;-1;123;123;123,108;108;108;-1;108;108;108,4,4/7/18,22,11,9,0,33,4,1149;658;1110;3839;2516;1176;201719,21;29;46;15;17;43;807,9;10;12;11;11;18;147,182;91;127;303;496;193;23989,-1;-1
2268,ICLR,2019,Better Accuracy with Quantified Privacy: Representations Learned via Reconstructive Adversarial Network,Sicong Liu;Anshumali Shrivastava;Junzhao Du;Lin Zhong,scliu007@gmail.com;anshumali@rice.edu;dujz@xidian.edu.cn;lzhong@rice.edu,4;5;3,4;4;4,Reject,0,3,0,yes,9/27/18,Rice University;Rice University;Tsinghua University;Rice University,85;85;8;85,86;86;30;86,4,9/27/18,2,1,0,0,0,0,1744;1097;310;2237,147;100;39;425,22;16;10;25,95;110;26;102,-1;-1
2269,ICLR,2019,Sample-efficient policy learning in multi-agent Reinforcement Learning via meta-learning,Jialian Li;Hang Su;Jun Zhu,lijialian7@163.com;suhangss@mail.tsinghua.edu.cn;dcszj@mail.tsinghua.edu.cn,4;4;4,3;4;4,Reject,0,0,0,yes,9/27/18,Tsinghua University;Tsinghua University;Tsinghua University,8;8;8,30;30;30,6,9/27/18,0,0,0,0,0,0,53;6735;4508,23;414;204,4;33;35,1;589;524,-1;-1
2270,ICLR,2019,UaiNets: From Unsupervised to Active Deep Anomaly Detection,Tiago Pimentel;Marianne Monteiro;Juliano Viana;Adriano Veloso;Nivio Ziviani,tiago.pimentel@kunumi.com;marianne@kunumi.com;juliano@kunumi.com;adrianov@dcc.ufmg.br;nivio@dcc.ufmg.br,4;5;3,4;2;4,Reject,3,4,0,yes,9/27/18,Universidade Federal de Minas Gerais;Kunumi;Kunumi;Universidade Federal de Minas Gerais;Universidade Federal de Minas Gerais,478;-1;-1;478;478,715;-1;-1;715;715,,9/27/18,0,0,0,0,0,0,39;8;9;1736;2964,14;3;6;124;169,4;2;2;23;30,3;0;0;89;203,-1;-1
2271,ICLR,2019,Differentially Private Federated Learning: A Client Level Perspective,Robin C. Geyer;Tassilo J. Klein;Moin Nabi,geyerr@ethz.ch;tassilo.klein@sap.com;moin.nabi@sap.com,4;4;4,3;4;4,Reject,0,6,0,yes,9/27/18,Swiss Federal Institute of Technology;SAP;SAP,10;314;314,10;300;300,4,12/20/17,157,95,40,4,7,20,157;592;682,1;43;43,1;10;12,20;65;64,-1;-1
2272,ICLR,2019,LSH Microbatches for Stochastic Gradients:  Value in Rearrangement,Eliav Buchnik;Edith Cohen;Avinatan Hassidim;Yossi Matias,edith@cohenwang.com;eliavbuh@gmail.com,4;4;4;3,4;3;4;2,Reject,0,5,0,yes,9/27/18,Tel Aviv University;Tel Aviv University,37;37,217;217,,3/14/18,3,0,0,0,3,0,22;7424;2427;7237,8;213;122;180,3;38;24;35,3;842;225;896,-1;-1
2273,ICLR,2019,Learning to Control Visual Abstractions for Structured Exploration in Deep Reinforcement Learning,catalin ionescu;tejas kulkarni;aaron van de oord;andriy mnih;vlad mnih,cdi@google.com;tkulkarni@google.com;avdnoord@google.com;amnih@google.com;vmnih@google.com,4;4;5,3;3;3,Reject,0,6,0,yes,9/27/18,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,,9/27/18,2,1,0,0,0,0,1359;1937;1;9659;284,38;23;1;37;2,10;13;1;21;2,168;151;0;1553;38,-1;-1
2274,ICLR,2019,Large-Scale Study of Curiosity-Driven Learning,Yuri Burda;Harri Edwards;Deepak Pathak;Amos Storkey;Trevor Darrell;Alexei A. Efros,yburda@openai.com;harri@openai.com;pathak@berkeley.edu;a.storkey@ed.ac.uk;trevor@eecs.berkeley.edu;efros@eecs.berkeley.edu,6;9;7,4;5;3,Accept (Poster),0,4,0,yes,9/27/18,OpenAI;OpenAI;University of California Berkeley;University of Edinburgh;University of California Berkeley;University of California Berkeley,-1;-1;5;33;5;5,-1;-1;18;27;18;18,,8/13/18,186,97,65,13,0,33,1317;1158;4130;3794;88648;36507,21;18;40;198;558;192,8;8;14;31;111;77,277;190;546;434;11396;4565,-1;-1
2275,ICLR,2019,Generative Models from the perspective of Continual Learning,Timothée Lesort;Hugo Caselles-Dupré;Michael Garcia-Ortiz;Jean-François Goudou;David Filliat,timothee.lesort@thalesgroup.com;caselles@ensta.fr;mgarciaortiz@softbankrobotics.com;jean-francois.goudou@thalesgroup.com;david.filliat@ensta.fr,4;4;5,4;4;3,Reject,0,4,0,yes,9/27/18,ENSTA ParisTech;ENSTA ParisTech;SoftBank Robotics Europe;Thalesgroup;ENSTA ParisTech,478;478;-1;-1;478,1103;1103;-1;-1;1103,5,9/27/18,32,19,13,0,15,5,226;101;158;92;2325,18;10;26;4;134,8;6;7;2;23,16;11;10;5;158,-1;-1
2276,ICLR,2019,Graph Classification with Geometric Scattering,Feng Gao;Guy Wolf;Matthew Hirn,gaofeng2@msu.edu;guy.wolf@yale.edu;mhirn@msu.edu,5;6;5,4;3;4,Reject,0,6,0,yes,9/27/18,SUN YAT-SEN UNIVERSITY;Yale University;SUN YAT-SEN UNIVERSITY,478;62;478,352;12;352,10,9/27/18,2,1,0,0,5,0,4290;391;343,526;65;28,31;10;11,261;22;26,-1;-1
2277,ICLR,2019,An investigation of model-free planning,Arthur Guez;Mehdi Mirza;Karol Gregor;Rishabh Kabra;Sébastien Racanière;Théophane Weber;David Raposo;Adam Santoro;Laurent Orseau;Tom Eccles;Greg Wayne;David Silver;Timothy Lillicrap,aguez@google.com;mmirza@google.com;karolg@google.com;rkabra@google.com;sracaniere@google.com;theophane@google.com;draposo@google.com;adamsantoro@google.com;lorseau@google.com;eccles@google.com;gregwayne@google.com;davidsilver@google.com;countzero@google.com,5;5;4,4;3;5,Reject,0,4,0,yes,9/27/18,Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,,9/27/18,18,3,8,0,14,0,12984;27921;4712;137;778;244;1550;2977;532;109;2564;42155;23451,32;118;44;3;32;20;15;35;40;28;32;158;74,17;23;19;3;12;7;8;20;13;6;15;56;39,917;5037;528;15;75;31;212;339;42;9;261;5862;2870,-1;-1
2278,ICLR,2019,Learning What to Remember: Long-term Episodic Memory Networks for Learning from Streaming Data,Hyunwoo Jung;Moonsu Han;Minki Kang;Sungju Hwang,hyunwooj@kaist.ac.kr;mshan92@kaist.ac.kr;zzxc1133@kaist.ac.kr;sjhwang82@kaist.ac.kr,5;4;4,5;4;4,Reject,0,0,3,yes,9/27/18,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,20;20;20;20,95;95;95;95,,9/27/18,3,2,0,0,2,0,175;6;6;1101,20;2;4;71,4;2;2;16,23;1;1;124,-1;-1
2279,ICLR,2019,Multi-Task Learning for Semantic Parsing with Cross-Domain Sketch,Huan Wang;Yuxiang Hu;Li Dong;Feijun Jiang;Zaiqing Nie,odile.wh@alibaba-inc.com;yuxiang.hyx@alibaba-inc.com;li.dong@ed.ac.uk;feijun.jiangfj@alibaba-inc.com;zaiqing.nzq@alibaba-inc.com,3;4;5,4;4;4,Reject,0,0,4,yes,9/27/18,Alibaba Group;Alibaba Group;University of Edinburgh;Alibaba Group;Alibaba Group,-1;-1;33;-1;-1,-1;-1;27;-1;-1,3,9/27/18,1,1,0,0,0,0,1998;168;172;52;1809,317;51;155;10;86,20;7;7;4;21,133;4;12;1;159,-1;-1
2280,ICLR,2019,GEOMETRIC AUGMENTATION FOR ROBUST NEURAL NETWORK CLASSIFIERS,Robert M. Taylor;Yusong Tan,rtaylor@mitre.org;ytan@mitre.org,4;4;3,3;4;5,Reject,0,0,0,yes,9/27/18,Virginia Tech;,81;-1,300;-1,4;11,9/27/18,0,0,0,0,0,0,353;284,90;77,6;7,20;12,-1;-1
2281,ICLR,2019,Overlapping Community Detection with Graph Neural Networks,Oleksandr Shchur;Stephan Günnemann,shchur@in.tum.de;guennemann@in.tum.de,5;3;4,4;5;5,Reject,0,0,0,yes,9/27/18,Technical University Munich;Technical University Munich,54;54,41;41,10,9/27/18,3,0,0,0,0,0,247;2437,12;138,4;28,36;278,-1;-1
2282,ICLR,2019,A Guider Network for Multi-Dual Learning,Wenpeng Hu;Zhengwei Tao;Zhanxing Zhu;Bing Liu;Zhou Lin;Jinwen Ma;Dongyan Zhao;Rui Yan,wenpeng.hu@pku.edu.cn;tttzw@pku.edu.cn;zhanxing.zhu@pku.edu.cn;liub@uic.edu;jokerlin@pku.edu.cn;jwma@math.pku.edu.cn;zhaody@pku.edu.cn;ruiyan@pku.edu.cn,4;5;4,3;2;5,Reject,0,0,0,yes,9/27/18,"Peking University;Peking University;Peking University;University of Illinois, Chicago;Peking University;Peking University;Peking University;Peking University",24;24;24;57;24;24;24;24,27;27;27;255;27;27;27;27,3,9/27/18,0,0,0,0,0,0,141;20;841;105;128;1990;2577;6288,19;3;81;35;66;192;163;640,7;1;14;5;6;21;27;37,15;2;105;8;10;124;305;424,-1;-1
2283,ICLR,2019,Dual Learning: Theoretical Study and Algorithmic Extensions,Zhibing Zhao;Yingce Xia;Tao Qin;Tie-Yan Liu,zhaoz6@rpi.edu;yingce.xia@gmail.com;taoqin@microsoft.com;tyliu@microsoft.com,6;2;5,3;4;3,Reject,0,1,0,yes,9/27/18,Rensselaer Polytechnic Institute;Microsoft;Microsoft;Microsoft,169;-1;-1;-1,304;-1;-1;-1,3;1,9/27/18,0,0,0,0,0,0,59;1267;2172;13277,14;48;43;366,4;14;8;51,1;138;319;1718,-1;-1
2284,ICLR,2019,Real-time Neural-based Input Method,Jiali Yao;Raphael Shu;Xinjian Li;Katsutoshi Ohtsuki;Hideki Nakayama,jiayao@microsoft.com;shu@nlab.ci.i.u-tokyo.ac.jp;xinjianl@andrew.cmu.edu;katsutoshi.ohtsuki@microsoft.com;nakayama@ci.i.u-tokyo.ac.jp,3;3;3,3;4;3,Reject,0,3,0,yes,9/27/18,Microsoft;The University of Tokyo;Carnegie Mellon University;Microsoft;The University of Tokyo,-1;54;1;-1;54,-1;45;24;-1;45,3,9/27/18,2,0,0,0,2,0,57;95;1557;187;625,12;14;74;41;87,3;4;17;9;15,1;20;66;5;67,-1;-1
2285,ICLR,2019,CHEMICAL NAMES STANDARDIZATION USING NEURAL SEQUENCE TO SEQUENCE MODEL,Junlang Zhan;Hai Zhao,longmr.zhan@sjtu.edu.cn;zhaohai@cs.sjtu.edu.cn,4;3;7,4;5;3,Reject,0,9,0,yes,9/27/18,Shanghai Jiao Tong University;Shanghai Jiao Tong University,52;52,188;188,,9/27/18,1,0,0,0,0,0,2;1250,5;262,1;17,0;77,-1;-1
2286,ICLR,2019,TabNN: A Universal Neural Network Solution for Tabular Data,Guolin Ke;Jia Zhang;Zhenhui Xu;Jiang Bian;Tie-Yan Liu,guolin.ke@microsoft.com;jia.zhang@microsoft.com;zhenhui.xu@pku.edu.cn;jiang.bian@microsoft.com;tyliu@microsoft.com,5;4;5,4;5;2,Reject,0,6,0,yes,9/27/18,Microsoft;Microsoft;Peking University;Microsoft;Microsoft,-1;-1;24;-1;-1,-1;-1;27;-1;-1,,9/27/18,5,4,2,0,0,1,903;55;11;1023;13277,11;26;7;191;366,5;5;1;17;51,197;4;1;60;1718,-1;-1
2287,ICLR,2019,Learning Representations of Categorical Feature Combinations via Self-Attention,Chen Xu;Chengzhen Fu;Peng Jiang;Wenwu Ou,chaos.xc@alibaba-inc.com;fuchengzhen@pku.edu.cn;jiangpeng.jp@alibaba-inc.com;wenwu.ou@alibaba-inc.com,5;5;5,4;3;4,Reject,0,0,0,yes,9/27/18,Alibaba Group;Peking University;Alibaba Group;Alibaba Group,-1;24;-1;-1,-1;27;-1;-1,,9/27/18,1,0,1,0,0,0,574;7;5345;245,30;4;317;29,7;1;35;8,36;0;337;39,-1;-1
2288,ICLR,2019,Spatial-Winograd Pruning Enabling Sparse Winograd Convolution,Jiecao Yu;Jongsoo Park;Maxim Naumov,jiecaoyu@umich.edu;jongsoo@fb.com;mnaumov@fb.com,5;4;6,3;3;3,Reject,0,8,0,yes,9/27/18,University of Michigan;Facebook;Facebook,8;-1;-1,21;-1;-1,,9/27/18,1,0,0,0,4,0,180;875;484,6;49;49,2;17;12,16;101;34,-1;-1
2289,ICLR,2019,Phrase-Based Attentions,Phi Xuan Nguyen;Shafiq Joty,xuanphi001@e.ntu.edu.sg;srjoty@ntu.edu.sg,5;5;5,4;5;5,Reject,0,13,0,yes,9/27/18,National Taiwan University;National Taiwan University,85;85,197;197,3,9/27/18,4,4,0,0,0,0,4;1986,2;130,1;25,0;202,-1;-1
2290,ICLR,2019,Adaptive Pruning of Neural Language Models for Mobile Devices,Raphael Tang;Jimmy Lin,r33tang@uwaterloo.ca;jimmylin@uwaterloo.ca,6;5;6,4;3;4,Reject,0,5,0,yes,9/27/18,University of Waterloo;University of Waterloo,26;26,207;207,3,9/27/18,3,0,0,1,0,0,225;1993,26;102,6;23,39;223,-1;-1
2291,ICLR,2019,Total Style Transfer with a Single Feed-Forward Network,Minseong Kim;Hyun-Chul Choi,tyui592@ynu.ac.kr;pogary@ynu.ac.kr,4;5;4,5;5;3,Reject,2,0,0,yes,9/27/18,Yeungnam University.;Yeungnam University.,478;478,749;749,,9/27/18,0,0,0,0,0,0,360;526,49;88,10;10,19;14,-1;-1
2292,ICLR,2019,Mixture of Pre-processing Experts Model for Noise Robust Deep Learning on Resource Constrained Platforms,Taesik Na;Minah Lee;Burhan A. Mudassar;Priyabrata Saha;Jong Hwan Ko;Saibal Mukhopadhyay,taesik.na@gatech.edu;minah.lee@gatech.edu;burhan.mudassar@gatech.edu;priyabratasaha@gatech.edu;jonghwan.ko@gatech.edu;smukhopadhyay6@gatech.edu,3;4;4,4;5;4,Reject,0,3,0,yes,9/27/18,Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology,13;13;13;13;13;13,33;33;33;33;33;33,4;2,9/27/18,5,4,1,0,0,0,274;663;110;25;238;4175,31;33;19;15;45;281,9;11;5;3;8;29,21;5;5;1;14;348,-1;-1
2293,ICLR,2019,Multi-Scale Stacked Hourglass Network for Human Pose Estimation,Chunsheng Guo;Wenlong Du;Na Ying,guo.chsh@gmail.com;dwl1993@hdu.edu.cn;yingna@hdu.edu.cn,3;4;3,5;4;5,Reject,3,0,0,yes,9/27/18,Shandong University;Shandong University;Shandong University,4;4;4,3;3;3,2,9/27/18,1,0,0,0,0,0,15;61;29,19;14;11,2;5;2,0;2;0,-1;-1
2294,ICLR,2019,A quantifiable testing of global translational invariance in Convolutional and Capsule Networks,Weikai Qi,wikaiqi@gmail.com,3;4;3,5;4;5,Reject,0,0,0,yes,9/27/18,university of guelph,261,1103,,9/27/18,1,0,0,0,0,0,10,7,2,1,-1
2295,ICLR,2019,Multiple Encoder-Decoders Net for Lane Detection,Yuetong Du;Xiaodong Gu;Junqin Liu;Liwen He,1239832590@qq.com;gu3xuan@qq.com;65581134@qq.com;helw@njupt.edu.cn,2;2;4,4;5;4,Reject,0,0,0,yes,9/27/18,Tsinghua University;;;Tsinghua University,8;-1;-1;8,30;-1;-1;30,2,9/27/18,0,0,0,0,0,0,0;471;103;463,1;7;6;68,0;5;4;14,0;63;0;27,-1;-1
2296,ICLR,2019,Unsupervised Video-to-Video Translation,Dina Bashkirova;Ben Usman;Kate Saenko,dbash@bu.edu;usmn@bu.edu;saenko@bu.edu,3;4;4,4;5;5,Reject,0,0,0,yes,9/27/18,Boston University;Boston University;Boston University,65;65;65,70;70;70,2,6/10/18,8,3,4,0,3,1,16;144;16930,5;12;177,3;4;56,2;26;2363,-1;-1
2297,ICLR,2019,Generative model based on minimizing exact empirical Wasserstein distance,Akihiro Iohara;Takahito Ogawa;Toshiyuki Tanaka,iohara@sys.i.kyoto-u.ac.jp;takahito.ogawa@datagrid.co.jp;tt@i.kyoto-u.ac.jp,5;2;3,2;5;4,Reject,0,0,1,yes,9/27/18,Meiji University;DataGrid Inc.;Meiji University,478;-1;478,334;-1;334,5;4,9/27/18,4,2,1,0,0,1,4;4;2367,1;2;154,1;1;24,1;1;229,-1;-1
2298,ICLR,2019,Deli-Fisher GAN: Stable and Efficient Image Generation With Structured Latent Generative Space,Boli Fang;Chuck Jia;Miao Jiang;Dhawal Chaturvedi,bfang@iu.edu;jiac@iu.edu;miajiang@iu.edu;dhchat@iu.edu,2;2;3,4;5;5,Reject,0,1,0,yes,9/27/18,"Indiana University, Bloomington;Indiana University, Bloomington;Indiana University, Bloomington;Indiana University, Bloomington",72;72;72;72,117;117;117;117,5;4,9/27/18,0,0,0,0,0,0,2;0;165;0,5;2;28;1,1;0;7;0,0;0;10;0,-1;-1
2299,ICLR,2019,Stacking for Transfer Learning,Peng Yuankai,pyk3350266@163.com,3;4;2,5;5;5,Reject,0,0,0,yes,9/27/18,Xi'an Jiaotong University,478,565,6;8,9/27/18,0,0,0,0,0,0,0,2,0,0,-1
2300,ICLR,2019,Graph Spectral Regularization For Neural Network Interpretability,Alexander Tong;David van Dijk;Jay Stanley;Guy Wolf;Smita Krishnaswamy,alexander.tong@yale.edu;david.vandijk@yale.edu;jay.stanley@yale.edu;guy.wolf@yale.edu;smita.krishnaswamy@yale.edu,4;3;4,4;5;3,Reject,0,0,0,yes,9/27/18,Yale University;Yale University;Yale University;Yale University;Yale University,62;62;62;62;62,12;12;12;12;12,10,9/27/18,1,0,1,0,2,0,20;745;70;391;926,6;50;13;65;75,3;11;3;10;15,1;44;3;22;81,-1;-1
2301,ICLR,2019,RETHINKING SELF-DRIVING : MULTI -TASK KNOWLEDGE FOR BETTER GENERALIZATION AND ACCIDENT EXPLANATION ABILITY,Zhihao LI;Toshiyuki MOTOYOSHI;Kazuma SASAKI;Tetsuya OGATA;Shigeki SUGANO,mr.zhihao.li@gmail.com;motoyoshi@idr.ias.sci.waseda.ac.jp;ssk.sasaki@suou.waseda.jp;ogata@waseda.jp;sugano@waseda.jp,4;4;3,4;5;4,Reject,2,0,0,yes,9/27/18,Waseda University;Meiji University;Waseda University;Waseda University;Waseda University,261;478;261;261;261,642;334;642;642;642,2;8,9/27/18,12,9,5,2,4,3,2048;14;244;3211;3058,95;4;34;329;697,22;2;7;29;28,104;3;13;129;109,-1;-1
2302,ICLR,2019,Normalization Gradients are Least-squares Residuals,Yi Liu,liu.yi.pei@gmail.com,4;4;3,5;4;4,Reject,0,3,0,yes,9/27/18,,,,,9/27/18,0,0,0,0,0,0,2004,348,23,128,-1
2303,ICLR,2019,Task-GAN for Improved GAN based Image Restoration,Jiahong Ouyang;Guanhua Wang;Enhao Gong;Kevin Chen;John Pauly;Greg Zaharchuk,jiahongo@stanford.edu;guanhua@stanford.edu;enhaog@stanford.edu;ktchen@stanford.edu;pauly@stanford.edu;gregz@stanford.edu,4;5;4,5;4;5,Reject,0,0,0,yes,9/27/18,Stanford University;Stanford University;Stanford University;Stanford University;Stanford University;Stanford University,4;4;4;4;4;4,3;3;3;3;3;3,5;4;2,9/27/18,0,0,0,0,0,0,15;246;634;376;13524;6422,11;22;40;31;316;263,2;6;12;8;54;41,3;8;40;16;979;432,-1;-1
2304,ICLR,2019,Morpho-MNIST: Quantitative Assessment and Diagnostics for Representation Learning,Daniel C. Castro;Jeremy Tan;Bernhard Kainz;Ender Konukoglu;Ben Glocker,d.coelho-de-castro15@imperial.ac.uk;j.tan17@imperial.ac.uk;b.kainz@imperial.ac.uk;kender@vision.ee.ethz.ch;b.glocker@imperial.ac.uk,3;5;4,4;3;3,Reject,0,5,0,yes,9/27/18,Imperial College London;Imperial College London;Imperial College London;Swiss Federal Institute of Technology;Imperial College London,72;72;72;10;72,8;8;8;10;8,5;4,9/27/18,6,0,4,0,14,1,89;26;1985;5473;6061,17;12;127;114;167,5;4;21;31;37,11;1;165;525;617,-1;-1
2305,ICLR,2019,Small steps and giant leaps: Minimal Newton solvers for Deep Learning,Joao Henriques;Sebastien Ehrhardt;Samuel Albanie;Andrea Vedaldi,joao@robots.ox.ac.uk;hyenal@robots.ox.ac.uk;albanie@robots.ox.ac.uk;vedali@robots.ox.ac.uk,7;3;7,5;5;4,Reject,0,11,1,yes,9/27/18,University of Oxford;University of Oxford;University of Oxford;University of Oxford,50;50;50;50,1;1;1;1,,5/21/18,3,1,2,0,22,0,7152;95;312;33732,44;10;19;201,19;6;8;62,1781;2;45;4640,-1;-1
2306,ICLR,2019,Characterizing the Accuracy/Complexity Landscape of Explanations of Deep Networks through Knowledge Extraction,Simon Odense;Artur d'Avila Garcez,simon.odense@city.ac.uk;a.garcez@city.ac.uk,4;4;4;5,2;5;4;3,Reject,0,4,0,yes,9/27/18,"City, University of London;City, University of London",314;314,373;373,,9/27/18,0,0,0,0,0,0,6;1589,7;144,2;21,0;102,-1;-1
2307,ICLR,2019,When Will Gradient Methods Converge to Max-margin Classifier under ReLU Models?,Tengyu Xu;Yi Zhou;Kaiyi Ji;Yingbin Liang,xu.3260@osu.edu;zhou.1172@osu.edu;ji.367@osu.edu;liang.889@osu.edu,5;4;5,5;3;4,Reject,0,0,0,yes,9/27/18,Ohio State University;Ohio State University;Ohio State University;Ohio State University,76;76;76;76,318;318;318;318,,6/12/18,7,4,0,1,0,0,75;686;104;4975,14;113;16;212,6;15;6;31,7;41;23;363,-1;-1
2308,ICLR,2019,A NON-LINEAR  THEORY FOR SENTENCE EMBEDDING,Hichem Mezaoui;Isar Nejadgholi,hichem@imrsv.ai;isar@imrsv.ai,3;3;3,3;3;4,Reject,0,0,1,yes,9/27/18,;,-1;-1,-1;-1,,9/27/18,0,0,0,0,0,0,19;106,8;26,2;6,1;4,-1;-1
2309,ICLR,2019,The loss landscape of overparameterized neural networks,Y. Cooper,yaim@math.ias.edu,5;7;5,4;3;4,Reject,0,16,0,yes,9/27/18,"Institue for Advanced Study, Princeton",-1,-1,1,4/26/18,17,10,0,1,12,0,69,12,5,7,-1
2310,ICLR,2019,Stop memorizing: A data-dependent regularization framework for intrinsic pattern learning,Wei Zhu;Qiang Qiu;Bao Wang;Jianfeng Lu;Guillermo Sapiro;Ingrid Daubechies,zhu@math.duke.edu;qiang.qiu@duke.edu;wangbao@math.ucla.edu;jianfeng@math.duke.edu;guillermo.sapiro@duke.edu;ingrid@math.duke.edu,7;4;4,4;3;4,Reject,0,4,0,yes,9/27/18,"Duke University;Duke University;University of California, Los Angeles;Duke University;Duke University;Duke University",44;44;20;44;44;44,17;17;15;17;17;17,,5/18/18,2,1,0,0,14,0,5572;1244;552;2050;43361;27051,442;90;68;149;650;250,36;18;13;19;86;56,263;119;14;189;3891;2437,-1;-1
2311,ICLR,2019,Neural Variational Inference For Embedding Knowledge Graphs,Alexander I. Cowen-Rivers;Pasquale Minervini,mc_rivers@icloud.com;p.minervini@ucl.ac.uk,5;5;4,3;5;3,Reject,0,7,0,yes,9/27/18,University College London;University College London,50;50,16;16,5;10,9/27/18,0,0,0,0,0,0,2;581,8;41,1;9,0;167,-1;-1
2312,ICLR,2019,Learning Implicit Generative Models by Teaching Explicit Ones,Chao Du;Kun Xu;Chongxuan Li;Jun Zhu;Bo Zhang,duchao0726@gmail.com;kunxu.thu@gmail.com;chongxuanli1991@gmail.com;dcszj@tsinghua.edu.cn;dcszb@tsinghua.edu.cn,6;7;5,4;3;4,Reject,0,6,0,yes,9/27/18,Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University,8;8;8;8;8,30;30;30;30;30,5;4,7/10/18,2,1,0,0,7,0,1253;2713;495;4508;10502,107;306;18;204;1042,16;25;7;35;46,96;165;58;524;762,-1;-1
2313,ICLR,2019,"The GAN Landscape: Losses, Architectures, Regularization, and Normalization",Karol Kurach;Mario Lucic;Xiaohua Zhai;Marcin Michalski;Sylvain Gelly,kkurach@gmail.com;lucic@google.com;xzhai@google.com;michalski@google.com;sylvain.gelly@gmail.com,4;4;7,3;2;4,Reject,0,3,0,yes,9/27/18,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,5;4,6/5/18,120,62,34,6,944,14,1162;1579;813;585;3531,28;48;35;57;112,11;20;14;8;25,129;189;104;83;456,-1;-1
2314,ICLR,2019,PAIRWISE AUGMENTED GANS WITH ADVERSARIAL RECONSTRUCTION LOSS,Aibek Alanov;Max Kochurov;Daniil Yashkov;Dmitry Vetrov,alanov.aibek@gmail.com;maxim.v.kochurov@gmail.com;daniil.yashkov@phystech.edu;vetrodim@gmail.com,6;4;5,4;3;4,Reject,0,5,0,yes,9/27/18,Higher School of Economics;Skolkovo Institute of Science and Technology;Moscow Institute of Physics and Technology;Higher School of Economics,478;-1;478;478,377;-1;254;377,5;4,9/27/18,2,1,0,0,13,0,2;11;22;2071,4;8;6;124,1;2;2;16,0;2;3;283,-1;-1
2315,ICLR,2019,Improving Sample-based Evaluation for Generative Adversarial Networks,Shaohui Liu*;Yi Wei*;Jiwen Lu;Jie Zhou,b1ueber2y@gmail.com;wei-y15@mails.tsinghua.edu.cn;lujiwen@tsinghua.edu.cn;jzhou@tsinghua.edu.cn,5;5;3,3;4;5,Reject,0,1,0,yes,9/27/18,Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University,8;8;8;8,30;30;30;30,5;4,9/27/18,0,0,0,0,0,0,170;36;8822;426,27;24;280;131,6;2;48;12,30;5;839;36,-1;-1
2316,ICLR,2019,Tinkering with black boxes: counterfactuals uncover modularity in generative models,Michel Besserve;Remy Sun;Bernhard Schoelkopf,michel.besserve@tuebingen.mpg.de;remy.sun@ens-rennes.fr;bs@tuebingen.mpg.de,4;6;4,5;3;4,Reject,0,3,0,yes,9/27/18,"Max Planck Institute for Intelligent Systems, Max-Planck Institute;Ecole Normale Superieure de Rennes;Max Planck Institute for Intelligent Systems, Max-Planck Institute",-1;478;-1,-1;1103;-1,5;4,9/27/18,0,0,0,0,0,0,123;17;75101,14;7;860,4;2;119,7;1;9901,-1;-1
2317,ICLR,2019,Deep Generative Models for learning Coherent Latent Representations from Multi-Modal Data,Timo Korthals;Marc Hesse;Jürgen Leitner,korthals.timo@gmail.com;mhesse@cit-ec.uni-bielefeld.de;juxi.leitner@gmail.com,4;4;4,2;3;4,Reject,0,6,0,yes,9/27/18,"Bielefeld University, CITEC;Bielefeld University;South China University of Technology",314;314;478,288;288;576,5,9/27/18,0,0,0,0,0,0,97;63;932,28;30;93,5;5;17,5;4;40,-1;-1
2318,ICLR,2019,Mol-CycleGAN - a generative model for molecular optimization,Łukasz Maziarka;Agnieszka Pocha;Jan Kaczmarczyk;Michał Warchoł,l.maziarka@gmail.com;lamiane.chan@gmail.com;jan.kaczmarczyk@ardigen.com;michal.warchol@ardigen.com,4;4;4,5;4;3,Reject,0,0,0,yes,9/27/18,Ardigen;;Ardigen;Ardigen,-1;-1;-1;-1,-1;-1;-1;-1,5,9/27/18,5,1,2,0,0,0,16;12;188;64,9;5;51;20,3;3;8;4,1;0;3;9,-1;-1
2319,ICLR,2019,Effective and Efficient Batch Normalization Using Few Uncorrelated Data for Statistics' Estimation,Zhaodong Chen;Lei Deng;Guoqi Li;Jiawei Sun;Xing Hu;Ling Liang;YufeiDing;Yuan Xie,chenzd15@mails.tsinghua.edu.cn;leideng@ucsb.edu;liguoqi@mail.tsinghua.edu.cn;sunjw15@mails.tsinghua.edu.cn;xinghu@ucsb.edu;lingliang@ucsb.edu;yufeiding@cs.ucsb.edu;yuanxie@ucsb.edu,5;4;5,5;3;3,Reject,0,5,0,yes,9/27/18,Tsinghua University;UC Santa Barbara;Tsinghua University;Tsinghua University;UC Santa Barbara;UC Santa Barbara;UC Santa Barbara;UC Santa Barbara,8;37;8;8;37;37;37;37,30;53;30;30;53;53;53;53,9,9/27/18,1,1,1,0,0,0,62;1560;1118;4;515;152;14;184,28;180;161;6;114;26;12;18,4;21;16;1;11;8;2;6,4;92;64;1;50;2;0;6,-1;-1
2320,ICLR,2019,Contextual Recurrent Convolutional Model for Robust Visual Learning,Siming Yan*;Bowen Xiao*;Yimeng Zhang;Tai Sing Lee,simingyan@pku.edu.cn;mike.xiao@pku.edu.cn;zym1010@gmail.com;taislee@andrew.cmu.edu,4;3;4,4;5;5,Reject,0,5,0,yes,9/27/18,Peking University;Peking University;;Carnegie Mellon University,24;24;-1;1,27;27;-1;24,2,9/27/18,0,0,0,0,0,0,24;307;50;4701,15;33;15;161,2;11;3;25,0;11;7;359,-1;-1
2321,ICLR,2019,A unified theory of adaptive stochastic gradient descent as Bayesian filtering,Laurence Aitchison,laurence.aitchison@gmail.com,5;5;7,3;4;4,Reject,0,29,0,yes,9/27/18,HHMI Janelia Research Campus,-1,-1,11,9/27/18,4,4,1,0,4,1,411,28,10,35,-1
2322,ICLR,2019,MEAN-FIELD ANALYSIS OF BATCH NORMALIZATION,Mingwei Wei;James Stokes;David J Schwab,m.wei@u.northwestern.edu;james@tunnel.tech;dschwab@gc.cuny.edu,7;6;5,3;3;3,Reject,0,6,0,yes,9/27/18,Northwestern University;;The City College of New York,44;-1;199,20;-1;1103,,9/27/18,3,1,2,0,6,0,126;355;3662,14;65;157,7;9;36,5;36;347,-1;-1
2323,ICLR,2019,Reconciling Feature-Reuse and Overfitting in DenseNet with Specialized Dropout,Kun Wan;Boyuan Feng;Lingwei Xie;Yufei Ding,kun@cs.ucsb.edu;boyuan@cs.ucsb.edu;xielingwei@stu.xmu.edu.cn;yufeiding@cs.ucsb.edu,5;3;4,4;3;4,Reject,0,0,0,yes,9/27/18,UC Santa Barbara;UC Santa Barbara;Xiamen University;UC Santa Barbara,37;37;62;37,53;53;12;53,,9/27/18,0,0,0,0,0,0,72;4;30;352,23;8;6;43,3;1;3;9,4;0;0;31,-1;-1
2324,ICLR,2019,PCNN: Environment Adaptive Model Without Finetuning,Boyuan Feng;Kun Wan;Shu Yang;Yufei Ding,boyuan@cs.ucsb.edu;kun@cs.ucsb.edu;shuyang1995@ucsb.edu;yufeiding@cs.ucsb.edu,4;3;4,4;4;4,Reject,1,3,0,yes,9/27/18,UC Santa Barbara;UC Santa Barbara;UC Santa Barbara;UC Santa Barbara,37;37;37;37,53;53;53;53,6;2,9/27/18,0,0,0,0,0,0,4;5;185;14,8;9;70;12,1;1;6;2,0;1;12;0,-1;-1
2325,ICLR,2019,ATTENTION INCORPORATE NETWORK: A NETWORK CAN ADAPT VARIOUS DATA SIZE,Liangbo He;Hao Sun,heliangbo@tsinghua.edu.cn;sh759811581@tsinghua.edu.cn,3;4;2,5;4;4,Reject,0,0,0,yes,9/27/18,Tsinghua University;Tsinghua University,8;8,30;30,,6/6/18,0,0,0,0,0,0,340;926,7;174,5;14,0;64,-1;-1
2326,ICLR,2019,What a difference a pixel makes: An empirical examination of features used by CNNs for categorisation,Gaurav Malhotra;Jeffrey Bowers,gaurav.malhotra@bristol.ac.uk;j.bowers@bristol.ac.uk,4;4;7,4;4;5,Reject,1,12,0,yes,9/27/18,University of Bristol;University of Bristol,123;123,76;76,,9/27/18,2,2,0,0,0,0,73;78,16;22,4;5,6;3,-1;-1
2327,ICLR,2019,Trace-back along capsules and its application on semantic segmentation  		,Tao Sun;Zhewei Wang;C. D. Smith;Jundong Liu,zw340113@ohio.edu;ts202115@ohio.edu;cdsmith.uk@gmail.com;liuj1@ohio.edu,6;6;5,4;3;4,Reject,0,12,0,yes,9/27/18,Ohio University;Ohio University;;Ohio University,386;386;-1;386,627;627;-1;627,2,9/27/18,2,0,0,0,2,0,1846;115;7799;392,208;30;182;57,23;7;44;8,125;5;425;27,-1;-1
2328,ICLR,2019,"Neural Network Regression with Beta, Dirichlet, and Dirichlet-Multinomial Outputs",Peter Sadowski;Pierre Baldi,peter.sadowski@hawaii.edu;pfbaldi@ics.uci.edu,3;3;4,4;5;4,Reject,1,1,0,yes,9/27/18,"University of Hawaii, Manoa;University of California, Irvine",386;35,204;99,,9/27/18,4,2,2,0,0,0,59;4649,7;154,4;34,3;283,-1;-1
2329,ICLR,2019,Conditional Inference in Pre-trained Variational Autoencoders via Cross-coding,Ga Wu;Justin Domke;Scott Sanner,wuga@mie.utoronto.ca;domke@cs.umass.edu;ssanner@mie.utoronto.ca,4;4;4,4;4;5,Reject,0,2,0,yes,9/27/18,"Toronto University;University of Massachusetts, Amherst;Toronto University",18;30;18,22;191;22,5,5/20/18,3,1,3,0,3,0,57;769;2552,19;45;164,4;13;28,9;87;294,-1;-1
2330,ICLR,2019,Neural Rendering Model: Joint Generation and Prediction for Semi-Supervised Learning,Nhat Ho;Tan Nguyen;Ankit B. Patel;Anima Anandkumar;Michael I. Jordan;Richard G. Baraniuk,minhnhat@berkeley.edu;mn15@rice.edu;ankit.patel@bcm.edu;anima@caltech.edu;jordan@cs.berkeley.edu;richb@rice.edu,5;5;3,3;3;4,Reject,0,8,0,yes,9/27/18,University of California Berkeley;Rice University;Baylor College of Medicine;California Institute of Technology;University of California Berkeley;Rice University,5;85;-1;140;5;85,18;86;-1;3;18;86,5;8,9/27/18,7,4,2,0,0,0,119;256;366;5337;115388;29471,10;37;30;187;846;659,5;9;6;38;138;84,8;24;39;744;15955;2746,-1;-1
2331,ICLR,2019,Modular Deep Probabilistic Programming,Zhenwen Dai;Eric Meissner;Neil D. Lawrence,zhenwend@amazon.com;erimeiss@amazon.com;lawrennd@amazon.com,5;3;4,3;3;4,Reject,1,4,0,yes,9/27/18,Amazon;Amazon;Amazon,-1;-1;-1,-1;-1;-1,11,9/27/18,8,7,1,0,0,0,676;61;10929,132;13;264,12;5;52,81;2;1377,-1;-1
2332,ICLR,2019,Asynchronous SGD without gradient delay for efficient distributed training,Roman Talyansky;Pavel Kisilev;Zach Melamed;Natan Peterfreund;Uri Verner,roma.talyansky@gmail.com;pavel.kisilev@huawei.com;zach.melamed@huawei.com;natan.peterfreund@gmail.com;uri.verner@gmail.com,5;4;4,4;4;5,Reject,0,0,0,yes,9/27/18,Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.;Amazon;,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,9,9/27/18,0,0,0,0,0,0,87;516;0;445;121,11;51;1;28;8,5;12;0;10;6,2;41;0;18;3,-1;-1
2333,ICLR,2019,EnGAN: Latent Space MCMC and Maximum Entropy Generators for Energy-based Models,Rithesh Kumar;Anirudh Goyal;Aaron Courville;Yoshua Bengio,ritheshkumar.95@gmail.com;anirudhgoyal9119@gmail.com;aaron.courville@gmail.com;yoshua.umontreal@gmail.com,6;5;5,4;5;4,Reject,4,13,0,yes,9/27/18,University of Montreal;University of Montreal;University of Montreal;University of Montreal,123;123;123;123,108;108;108;108,5;4,9/27/18,0,0,0,0,0,0,348;1110;59549;201719,9;46;203;807,4;12;64;147,73;127;7800;23989,-1;-1
2334,ICLR,2019,Variational recurrent models for representation learning,Qingming Tang;Mingda Chen;Weiran Wang;Karen Livescu,qmtang@ttic.edu;mchen@ttic.edu;weiranw@amazon.com;klivescu@ttic.edu,5;5;3,3;3;5,Reject,0,3,2,yes,9/27/18,Toyota Technological Institute at Chicago;Toyota Technological Institute at Chicago;Amazon;Toyota Technological Institute at Chicago,123;123;-1;123,1103;1103;-1;1103,5,9/27/18,0,0,0,0,0,0,40;362;1457;5290,16;11;63;128,2;4;18;35,3;78;172;685,-1;-1
2335,ICLR,2019,HR-TD: A Regularized TD Method to Avoid Over-Generalization,Ishan Durugkar;Bo Liu;Peter Stone,ishand@cs.utexas.edu;liubo19831214@gmail.com;pstone@cs.utexas.edu,4;3;2,4;4;5,Reject,0,0,0,yes,9/27/18,"University of Texas, Austin;University of California, San Diego;University of Texas, Austin",22;11;22,49;31;49,8,9/27/18,0,0,0,0,0,0,335;628;18831,15;92;682,5;15;67,47;43;1480,-1;-1
2336,ICLR,2019,Discovering General-Purpose Active Learning Strategies,Ksenia Konyushkova;Raphael Sznitman;Pascal Fua,ksenia.konyushkova@epfl.ch;raphael.sznitman@artorg.unibe.ch;pascal.fua@epfl.ch,5;4;4;4,4;5;5;4,Reject,0,4,0,yes,9/27/18,Swiss Federal Institute of Technology Lausanne;University of Bern;Swiss Federal Institute of Technology Lausanne,478;386;478,38;105;38,,9/27/18,3,2,1,0,3,1,360;1009;29619,22;90;476,8;20;77,26;94;3581,-1;-1
2337,ICLR,2019,Multi-Objective Value Iteration with Parameterized Threshold-Based Safety Constraints,Hussein Sibai;Sayan Mitra,sibai2@illinois.edu;mitras@illinois.edu,5;5;3,4;2;4,Reject,0,5,0,yes,9/27/18,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",3;3,37;37,,9/27/18,0,0,0,0,0,0,234;1839,17;147,4;23,10;90,-1;-1
2338,ICLR,2019,Optimizing for Generalization in Machine Learning with Cross-Validation Gradients,Barratt;Shane;Sharma;Rishi,sbarratt@stanford.edu;rsh@stanford.edu,5;2;4,5;4;4,Reject,1,0,0,yes,9/27/18,Stanford University;Stanford University,4;4,3;3,8,5/18/18,1,0,1,0,9,0,-1;320;-1;345,-1;34;-1;40,-1;7;-1;9,0;13;0;14,-1;-1
2339,ICLR,2019,FAVAE: SEQUENCE DISENTANGLEMENT USING IN- FORMATION BOTTLENECK PRINCIPLE,Masanori Yamada;Kim Heecheol;Kosuke Miyoshi;Hiroshi Yamakawa,yamada0224@gmail.com;h-kim@isi.imi.i.u-tokyo.ac.jp;miyoshi@narr.jp;hiroshi_yamakawa@dwango.co.jp,5;6;4,4;5;4,Reject,0,7,0,yes,9/27/18,NTT;The University of Tokyo;;,-1;54;-1;-1,-1;45;-1;-1,5,9/27/18,1,1,1,0,0,0,540;3;258;659,101;2;5;290,13;1;2;12,33;0;7;40,-1;-1
2340,ICLR,2019,Learning Partially Observed PDE Dynamics with Neural Networks,Ibrahim Ayed;Emmanuel De Bézenac;Arthur Pajot;Patrick Gallinari,ayedibrahim@gmail.com;emmanuel.de-bezenac@lip6.fr;arthur.pajot@lip6.fr;patrick.gallinari@lip6.fr,6;5;5,3;5;3,Reject,0,6,0,yes,9/27/18,LIP6;LIP6;LIP6;LIP6,-1;-1;-1;-1,-1;-1;-1;-1,,9/27/18,0,0,0,0,0,0,33;84;78;4777,7;9;9;450,3;3;3;33,0;9;9;378,-1;-1
2341,ICLR,2019,Weak contraction mapping and optimization,Siwei Luo,siuluosiwei@gmail.com,3;1;4,2;5;5,Reject,0,4,0,yes,9/27/18,"University of Illinois, Chicago",57,255,9,9/27/18,0,0,0,0,0,0,969,180,16,53,-1
2342,ICLR,2019,Riemannian Stochastic Gradient Descent for Tensor-Train Recurrent Neural Networks,Jun Qi;Chin-Hui Lee;Javier Tejedor,jqi41@gatech.edu;qij13@uw.edu;javiertejedornoguerales@gmail.com,4;4;3,4;4;3,Reject,0,0,0,yes,9/27/18,"Georgia Institute of Technology;University of Washington, Seattle;",13;6;-1,33;25;-1,3,9/27/18,0,0,0,0,0,0,566;10351;497,84;453;61,14;50;13,18;732;24,-1;-1
2343,ICLR,2019,Unseen Action Recognition with Unpaired Adversarial Multimodal Learning,AJ Piergiovanni;Michael S. Ryoo,ajpiergi@indiana.edu;mryoo@indiana.edu,7;5;4,4;4;5,Reject,0,3,0,yes,9/27/18,University of Arizona;University of Arizona,169;169,161;161,4;6,9/27/18,0,0,0,0,0,0,179;2856,31;101,8;24,11;272,-1;-1
2344,ICLR,2019,Unlabeled Disentangling of GANs with Guided Siamese Networks,Gökhan Yildirim;Nikolay Jetchev;Urs Bergmann,gokhan.yildirim@zalando.de;nikolay.jetchev@zalando.de;urs.bergmann@zalando.de,5;6;6;5,4;4;3;4,Reject,0,11,1,yes,9/27/18,Zalando Research;Zalando Research;Zalando Research,-1;-1;-1,-1;-1;-1,5,9/27/18,0,0,0,0,0,0,208;443;231,68;25;21,7;10;6,21;39;30,-1;-1
2345,ICLR,2019,Local Image-to-Image Translation via Pixel-wise Highway Adaptive Instance Normalization,Wonwoong Cho;Seunghwan Choi;Junwoo Park;David Keetae Park;Tao Qin;Jaegul Choo,tyflehd21@korea.ac.kr;shadow2496@korea.ac.kr;skp.1003874@sk.com;heykeetae@gmail.com;taotsin@msn.com;jchoo@korea.ac.kr,5;6;5,4;4;5,Reject,0,6,2,yes,9/27/18,Korea University;Korea University;Sk;;;Korea University,314;314;314;-1;-1;314,244;244;38;-1;-1;244,2,9/27/18,0,0,0,0,0,0,27;40;640;47;4922;2388,8;20;50;20;289;124,2;4;10;5;33;22,3;1;13;7;588;386,-1;-1
2346,ICLR,2019,Empirically Characterizing Overparameterization Impact on Convergence,Newsha Ardalani;Joel Hestness;Gregory Diamos,newsha@baidu.com;joel@baidu.com;gregdiamos@baidu.com,5;4;3,3;5;4,Reject,0,0,0,yes,9/27/18,Baidu;Baidu;Baidu,-1;-1;-1,-1;-1;-1,,9/27/18,1,1,0,0,0,0,207;3599;1955,15;24;42,5;11;19,25;384;192,-1;-1
2347,ICLR,2019,A Biologically Inspired Visual Working Memory for Deep Networks,Ethan Harris;Mahesan Niranjan;Jonathon Hare,ewah1g13@ecs.soton.ac.uk;mn@ecs.soton.ac.uk;jsh2@ecs.soton.ac.uk,9;4;5,5;4;4,Reject,0,5,0,yes,9/27/18,University of Southampton;University of Southampton;University of Southampton,169;169;169,126;126;126,,9/27/18,1,0,0,0,0,0,7;1916;1629,11;152;118,2;20;22,0;160;150,-1;-1
2348,ICLR,2019,Using GANs for Generation of Realistic City-Scale Ride Sharing/Hailing Data Sets,Abhinav Jauhri;Brad Stocks;Jian Hui Li;Koichi Yamada;John Paul Shen,ajauhri@cmu.edu;brad.stocks@sv.cmu.edu;jian.hui.li@intel.com;koichi.yamada@intel.com;jpshen@cmu.edu,4;5;5,4;4;4,Reject,0,4,0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;Intel;Intel;Carnegie Mellon University,1;1;-1;-1;1,24;24;-1;-1;24,5;4,9/27/18,0,0,0,0,0,0,24;0;164;540;5076,11;2;46;153;175,3;0;5;11;33,2;0;4;38;419,-1;-1
2349,ICLR,2019,Pushing the bounds of dropout,Gábor Melis;Charles Blundell;Tomáš Kočiský;Karl Moritz Hermann;Chris Dyer;Phil Blunsom,melisgl@google.com;cblundell@google.com;tkocisky@google.com;kmh@google.com;cdyer@google.com;pblunsom@google.com,5;5;4,3;2;3,Reject,0,5,0,yes,9/25/19,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,3;1,5/23/18,8,2,4,1,19,0,602;6016;2729;5082;21134;11400,14;50;16;41;231;144,8;22;10;21;59;47,99;1059;413;686;3164;1326,m;m
2350,ICLR,2019,Traditional and Heavy Tailed Self Regularization in Neural Network Models,Charles H. Martin;Michael W. Mahoney,charles@calculationconsulting.com;mmahoney@stat.berkeley.edu,4;4;6,4;1;5,Reject,0,11,0,yes,9/27/18,Calculationconsulting;University of California Berkeley,-1;5,-1;18,8,9/27/18,25,10,4,4,4,2,334;11793,89;230,7;45,18;1114,-1;-1
2351,ICLR,2019,ISA-VAE: Independent Subspace Analysis with Variational Autoencoders,Jan Stühmer;Richard Turner;Sebastian Nowozin,t-jastuh@microsoft.com;ret26@cam.ac.uk;senowozi@microsoft.com,4;7;4,3;4;5,Reject,0,3,0,yes,9/27/18,Microsoft;University of Cambridge;Microsoft,-1;71;-1,-1;2;-1,5,9/27/18,2,1,1,0,0,1,431;2936;6872,18;174;134,8;30;39,17;308;931,-1;-1
2352,ICLR,2019,Guaranteed Recovery of One-Hidden-Layer Neural Networks via Cross Entropy,Haoyu Fu;Yuejie Chi;Yingbin Liang,fu.436@osu.edu;yuejiechi@cmu.edu;liang.889@osu.edu,3;4;5,4;4;4,Reject,0,0,0,yes,9/27/18,Ohio State University;Carnegie Mellon University;Ohio State University,76;1;76,318;24;318,1;9,2/18/18,9,1,1,0,2,3,67;2541;4975,13;113;212,4;24;31,10;243;363,-1;-1
2353,ICLR,2019,Metropolis-Hastings view on variational inference and adversarial training,Kirill Neklyudov;Dmitry Vetrov,k.necludov@gmail.com;vetrodim@gmail.com,6;9;5,4;4;3,Reject,0,10,0,yes,9/27/18,Higher School of Economics;Higher School of Economics,478;478,377;377,11;5;4;1,9/27/18,4,2,2,1,7,0,143;2071,10;124,4;16,26;283,-1;-1
2354,ICLR,2019,Open-Ended Content-Style Recombination Via Leakage Filtering,Karl Ridgeway;Michael C. Mozer,karl.ridgeway@colorado.edu;mozer@colorado.edu,5;7;5,4;4;3,Reject,0,3,0,yes,9/27/18,"University of Colorado, Boulder;University of Colorado, Boulder",44;44,100;100,5;6,9/27/18,2,0,0,0,2,0,160;6805,13;237,5;43,15;514,-1;-1
2355,ICLR,2019,VHEGAN: Variational Hetero-Encoder Randomized GAN for Zero-Shot Learning,Hao Zhang;Bo Chen;Long Tian;Zhengjue Wang;Mingyuan Zhou,zhanghao_xidian@163.com;bchen@mail.xidian.edu.cn;zhengjuewang@163.com;tianlong_xidian@163.com;mingyuan.zhou@mccombs.utexas.edu,5;5;5,4;5;5,Reject,1,4,0,yes,9/27/18,"Xidian University;Tsinghua University;Xidian University;163;University of Texas, Austin",478;8;478;-1;22,917;30;917;-1;49,5;4;6,9/27/18,0,0,0,0,0,0,1447;8188;340;31;1969,176;980;66;8;115,19;37;10;4;24,64;289;16;0;233,m;m
2356,ICLR,2019,$A^*$ sampling with probability matching,Yichi Zhou;Jun Zhu,vofhqn@gmail.com;dcszj@mail.tsinghua.edu.cn,5;6;3,5;2;5,Reject,0,0,0,yes,9/27/18,Tsinghua University;Tsinghua University,8;8,30;30,1,9/27/18,0,0,0,0,0,0,10;11233,9;322,2;43,0;721,-1;-1
2357,ICLR,2019,Maximum a Posteriori on a Submanifold: a General Image Restoration Method with GAN,Fangzhou Luo;Xiaolin Wu,fluo1993@gmail.com;xwu510@gmail.com,4;4;6,5;5;4,Reject,0,4,0,yes,9/27/18,;,-1;-1,-1;-1,5;4,9/27/18,0,0,0,0,0,0,16;10455,10;421,2;47,0;1164,-1;-1
2358,ICLR,2019,Ada-Boundary: Accelerating the DNN Training via Adaptive Boundary Batch Selection,Hwanjun Song;Sundong Kim;Minseok Kim;Jae-Gil Lee,songhwanjun@kaist.ac.kr;sundong.kim@kaist.ac.kr;minseokkim@kaist.ac.kr;jaegil@kaist.ac.kr,5;5;5,4;3;4,Reject,0,8,0,yes,9/27/18,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,20;20;20;20,95;95;95;95,,9/27/18,0,0,0,0,0,0,39;11;48;34,11;11;46;27,3;2;3;2,8;1;1;2,-1;-1
2359,ICLR,2019,A Resizable Mini-batch Gradient Descent based on a Multi-Armed Bandit,Seong Jin Cho;Sunghun Kang;Chang D. Yoo,ipcng00@kaist.ac.kr;sunghun.kang@kaist.ac.kr;cd_yoo@kaist.ac.kr,6;7;4,3;4;4,Reject,0,4,1,yes,9/27/18,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,20;20;20,95;95;95,,11/17/17,21,1,0,0,20,0,1055;81;1935,194;12;162,15;4;22,67;5;189,-1;-1
2360,ICLR,2019,Neural Network Cost Landscapes as Quantum States,Abdulah Fawaz;Sebastien Piat;Paul Klein;Peter Mountney;Simone Severini,abdulah.fawaz.14@ucl.ac.uk;sebastien.piat@siemens-healthineers.com;klein.paul@siemens-healthineers.com;peter.mountney@siemens-healthineers.com;s.severini@ucl.ac.uk,5;3;4,5;4;4,Reject,0,4,0,yes,9/27/18,University College London;Siemens Healthineers;Siemens Healthineers;Siemens Healthineers;University College London,50;-1;-1;-1;50,16;-1;-1;-1;16,,9/27/18,0,0,0,0,0,0,2;22;1415;972;2870,2;4;220;59;273,1;2;14;16;30,0;4;193;53;226,-1;-1
2361,ICLR,2019,Over-parameterization Improves Generalization in the XOR Detection Problem,Alon Brutzkus;Amir Globerson,brutzkus@gmail.com;amir.globerson@gmail.com,5;4;5,4;4;4,Reject,0,5,0,yes,9/27/18,Tel Aviv University;Tel Aviv University,37;37,217;217,1;8,9/27/18,5,2,1,1,5,0,376;4471,10;125,5;32,40;578,-1;-1
2362,ICLR,2019,On the effect of the activation function on the distribution of hidden nodes in a deep network,Philip M. Long;Hanie Sedghi,plong@google.com;hsedghi@google.com,4;4;5,3;3;3,Reject,0,3,0,yes,9/27/18,Google;Google,-1;-1,-1;-1,1,9/27/18,3,0,0,0,9,0,5172;569,156;30,35;12,400;52,-1;-1
2363,ICLR,2019,INFORMATION MAXIMIZATION AUTO-ENCODING,Dejiao Zhang;Tianchen Zhao;Laura Balzano,dejiao@umich.edu;ericolon@umich.edu;girasole@umich.edu,5;6;4,5;4;4,Reject,0,17,0,yes,9/27/18,University of Michigan;University of Michigan;University of Michigan,8;8;8,21;21;21,5,9/27/18,0,0,0,0,0,0,141;71;2650,15;14;88,7;4;23,17;2;425,-1;-1
2364,ICLR,2019,Infinitely Deep Infinite-Width Networks,Jovana Mitrovic;Peter Wirnsberger;Charles Blundell;Dino Sejdinovic;Yee Whye Teh,jovana.mitrovic@spc.ox.ac.uk;pewi@google.com;cblundell@google.com;dino.sejdinovic@stats.ox.ac.uk;ywteh@google.com,6;5;6,4;4;2,Reject,0,6,0,yes,9/27/18,University of Oxford;Google;Google;University of Oxford;Google,50;-1;-1;50;-1,1;-1;-1;1;-1,,9/27/18,0,0,0,0,0,0,74;58;6016;1810;23146,8;16;50;94;249,4;4;22;21;52,8;0;1059;236;3217,-1;-1
2365,ICLR,2019,Training Variational Auto Encoders with Discrete Latent Representations using Importance Sampling,Alexander Bartler;Felix Wiewel;Bin Yang;Lukas Mauch,alexander.bartler@iss.uni-stuttgart.de;felix.wiewel@iss.uni-stuttgart.de;bin.yang@iss.uni-stuttgart.de;lukas.mauch@iss.uni-stuttgart.de,3;1;3,5;5;5,Reject,0,0,0,yes,9/27/18,University of Stuttgart;University of Stuttgart;University of Stuttgart;University of Stuttgart,95;95;95;95,219;219;219;219,5;1,9/27/18,0,0,0,0,0,0,3;4;529;169,4;6;100;22,1;2;11;8,0;0;32;15,-1;-1
2366,ICLR,2019,An Exhaustive Analysis of Lazy vs. Eager Learning Methods for Real-Estate Property Investment,Setareh Rafatirad;Maryam Heidari,srafatir@gmu.edu;mheidari@gmu.edu,3;4;2,5;4;4,Reject,0,0,0,yes,9/27/18,George Mason University;George Mason University,99;99,336;336,,9/27/18,0,0,0,0,0,0,318;85,60;27,10;6,14;2,-1;-1
2367,ICLR,2019,Graph2Seq: Graph to Sequence Learning with Attention-Based Neural Networks,Kun Xu;Lingfei Wu;Zhiguo Wang;Yansong Feng;Michael Witbrock;Vadim Sheinin,xukun@pku.edu.cn;lwu@email.wm.edu;zhigwang@us.ibm.com;fengyansong@pku.edu.cn;witbrock@us.ibm.com;vadims@us.ibm.com,4;6;6,5;4;4,Reject,0,17,0,yes,9/27/18,Peking University;College of William and Mary;International Business Machines;Peking University;International Business Machines;International Business Machines,24;169;-1;24;-1;-1,27;261;-1;27;-1;-1,3;10,4/3/18,56,30,26,1,0,9,577;645;1969;1602;2335;221,21;62;75;95;100;40,11;15;24;22;23;8,90;62;304;200;156;23,-1;-1
2368,ICLR,2019,Scaling shared model governance via model splitting,Miljan Martic;Jan Leike;Andrew Trask;Matteo Hessel;Shane Legg;Pushmeet Kohli,miljanm@google.com;leike@google.com;atrask@google.com;mtthss@google.com;legg@google.com;pushmeet@google.com,5;4;9,4;3;4,Reject,0,10,0,yes,9/27/18,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,,9/27/18,1,0,1,0,81,0,423;800;241;2258;11032;22034,7;47;16;26;58;313,5;14;5;13;21;69,38;63;34;364;1793;2746,-1;-1
2369,ICLR,2019,An Alarm System for Segmentation Algorithm Based on Shape Model,Fengze Liu;Yingda Xia;Dong Yang;Alan Yuille;Daguang Xu,liufz13@gmail.com;yxia25@jhu.edu;don.yang.mech@gmail.com;alan.l.yuille@gmail.com;cathy.xudg@gmail.com,3;6;7;5,5;4;3;4,Reject,0,4,0,yes,9/27/18,Johns Hopkins University;Johns Hopkins University;NVIDIA;Johns Hopkins University;,72;72;-1;72;-1,13;13;-1;13;-1,5;2,9/27/18,3,1,1,2,0,0,73;149;1538;32754;305,12;18;179;494;43,5;6;21;80;10,7;15;104;3744;18,-1;-1
2370,ICLR,2019,Interpretable Continual Learning,Tameem Adel;Cuong V. Nguyen;Richard E. Turner;Zoubin Ghahramani;Adrian Weller,tah47@cam.ac.uk;nvcuong92@gmail.com;ret26@cam.ac.uk;zoubin@eng.cam.ac.uk;aw665@cam.ac.uk,4;5;6,4;3;3,Reject,0,2,1,yes,9/27/18,University of Cambridge;Amazon;University of Cambridge;University of Cambridge;University of Cambridge,71;-1;71;71;71,2;-1;2;2;2,,9/27/18,0,0,0,0,0,0,431;65;2936;42621;944,25;33;174;463;74,9;5;30;91;17,50;6;308;5079;57,-1;-1
2371,ICLR,2019,On the Spectral Bias of Neural Networks,Nasim Rahaman;Aristide Baratin;Devansh Arpit;Felix Draxler;Min Lin;Fred Hamprecht;Yoshua Bengio;Aaron Courville,nasim.rahaman@iwr.uni-heidelberg.de;aristidebaratin@hotmail.com;devansharpit@gmail.com;felix.draxler@iwr.uni-heidelberg.de;mavenlin@gmail.com;yoshua.umontreal@gmail.com;aaron.courville@umontreal.ca,6;4;6;5,3;4;3;3,Reject,0,12,1,yes,9/27/18,Heidelberg University;University of Montreal;University of Montreal;Heidelberg University;University of Montreal;University of Montreal;University of Montreal,199;123;123;199;123;123;123,45;108;108;45;108;108;108,,6/22/18,59,31,6,2,0,9,226;920;886;106;5929;5785;201719;59549,15;31;42;4;161;195;807;203,6;17;12;3;25;39;147;64,26;122;116;23;631;462;23989;7800,-1;-1
2372,ICLR,2019,REVISTING NEGATIVE TRANSFER USING ADVERSARIAL LEARNING,Saneem Ahmed Chemmengath;Samarth Bharadwaj;Suranjana Samanta;Karthik Sankaranarayanan,saneem.cg@in.ibm.com;samarth.b@in.ibm.com;suransam@in.ibm.com;kartsank@in.ibm.com,4;2;6,4;4;4,Reject,0,1,0,yes,9/27/18,International Business Machines;International Business Machines;International Business Machines;International Business Machines,-1;-1;-1;-1,-1;-1;-1;-1,4,9/27/18,0,0,0,0,0,0,23;1075;280;17,5;34;27;20,2;16;5;2,1;107;16;3,-1;-1
2373,ICLR,2019,PIE: Pseudo-Invertible Encoder,Jan Jetze Beitler;Ivan Sosnovik;Arnold Smeulders,j.j.beitler@uva.nl;i.sosnovik@uva.nl;a.w.m.smeulders@uva.nl,3;5;5,4;5;4,Reject,2,3,12,yes,9/27/18,University of Amsterdam;University of Amsterdam;University of Amsterdam,169;169;169,59;59;59,5,9/27/18,1,0,0,0,0,0,11;48;21949,2;4;469,1;3;50,0;3;1919,-1;-1
2374,ICLR,2019,Variational Domain Adaptation,Hirono Okamoto;Shohei Ohsawa;Itto Higuchi;Haruka Murakami;Mizuki Sango;Zhenghang Cui;Masahiro Suzuki;Hiroshi Kajino;Yutaka Matsuo,ohsawa@weblab.t.u-tokyo.ac.jp,4;4;5,3;5;3,Reject,0,6,1,yes,9/27/18,The University of Tokyo,54,45,5;11,9/27/18,0,0,0,0,0,0,4;26;0;1099;1;8;902;232;5464,5;14;4;140;2;6;316;24;492,1;3;0;15;1;2;14;8;36,0;3;0;62;0;0;78;23;199,-1;-1
2375,ICLR,2019,Siamese Capsule Networks,James O' Neill,james.o-neill@liverpool.ac.uk,5;6;3,4;4;4,Reject,0,2,0,yes,9/27/18,University of Liverpool,123,177,6;2,5/18/18,15,6,3,0,7,2,134,29,4,7,-1
2376,ICLR,2019,Unsupervised Multi-Target Domain Adaptation: An Information Theoretic Approach,Behnam Gholami;Pritish Sahu;Ognjen (Oggi) Rudovic;Konstantinos Bousmalis;Vladimir Pavlovic,bb510@cs.rutgers.edu;ps851@cs.rutgers.edu;orudovic@mit.edu;konstantinos@google.com;vladimir@cs.rutgers.edu,4;6;5,4;5;4,Reject,0,4,0,yes,9/27/18,Rutgers University;Rutgers University;Massachusetts Institute of Technology;Google;Rutgers University,34;34;2;-1;34,172;172;5;-1;172,,9/27/18,17,6,8,2,7,3,88;99;1350;1943;6364,14;36;61;22;300,5;7;20;12;35,6;11;126;191;562,-1;-1
2377,ICLR,2019,Flow++: Improving Flow-Based Generative Models  with  Variational Dequantization and Architecture Design  ,Jonathan Ho;Xi Chen;Aravind Srinivas;Yan Duan;Pieter Abbeel,jonathanho@berkeley.edu;peter@covariant.ai;aravind_srinivas@berkeley.edu;dementrock@gmail.com;pabbeel@cs.berkeley.edu,6;6;5,4;3;5,Reject,0,4,0,yes,9/27/18,University of California Berkeley;covariant.ai;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;-1;5;5;5,18;-1;18;18;18,5,9/27/18,65,18,24,4,2,12,2679;13363;287;5479;36532,31;444;12;48;433,12;40;5;19;94,396;1541;32;638;4402,-1;-1
2378,ICLR,2019,End-to-end learning of pharmacological assays from high-resolution microscopy images,Markus Hofmarcher;Elisabeth Rumetshofer;Sepp Hochreiter;Günter Klambauer,hofmarcher@ml.jku.at;rumetshofer@ml.jku.at;hochreit@ml.jku.at;klambauer@ml.jku.at,3;6;5,3;5;4,Reject,0,4,0,yes,9/27/18,Johannes Kepler University Linz;Johannes Kepler University Linz;Johannes Kepler University Linz;Johannes Kepler University Linz,228;228;228;228,538;538;538;538,2,9/27/18,0,0,0,0,0,0,136;24;1572;2057,11;5;20;51,4;3;8;17,3;1;94;252,-1;-1
2379,ICLR,2019,Meta-Learning with Domain Adaptation for Few-Shot Learning under Domain Shift,Doyen Sahoo;Hung Le;Chenghao Liu;Steven C. H. Hoi,doyens@smu.edu.sg;hungle.2018@phdis.smu.edu.sg;chliu@smu.edu.sg;chhoi@smu.edu.sg,5;6;6,3;3;3,Reject,2,5,0,yes,9/27/18,Singapore Management University;Singapore Management University;Singapore Management University;Singapore Management University,89;89;89;89,1103;1103;1103;1103,4;6,9/27/18,2,1,0,0,0,0,361;730;820;53,27;49;49;10,10;10;12;3,47;90;72;5,-1;-1
2380,ICLR,2019,On-Policy Trust Region Policy Optimisation with Replay Buffers,Dmitry Kangin;Nicolas Pugeault,d.kangin@exeter.ac.uk;n.pugeault@exeter.ac.uk,7;6;5,5;3;4,Reject,0,4,0,yes,9/27/18,University of Exeter;University of Exeter,386;386,130;130,,9/27/18,1,1,0,1,6,1,155;1330,23;91,8;19,10;113,-1;-1
2381,ICLR,2019,Stochastic Gradient Descent Learns State Equations with Nonlinear Activations,Samet Oymak,sametoymak@gmail.com,7;7;5,3;3;5,Reject,0,3,0,yes,9/27/18,"University of California, Riverside",57,197,1,9/9/18,19,7,4,2,4,2,1525,60,21,148,-1
2382,ICLR,2019,Identifying Generalization Properties in Neural Networks,Huan Wang;Nitish Shirish Keskar;Caiming Xiong;Richard Socher,huan.wang@salesforce.com;nkeskar@salesforce.com;cxiong@salesforce.com;rsocher@salesforce.com,5;6;6,4;4;4,Reject,0,5,0,yes,9/27/18,SalesForce.com;SalesForce.com;SalesForce.com;SalesForce.com,-1;-1;-1;-1,-1;-1;-1;-1,1;8,9/19/18,19,12,5,2,158,2,2147;2137;6230;52392,317;28;156;180,21;13;31;49,133;329;1054;8829,-1;-1
2383,ICLR,2019,Stackelberg GAN: Towards Provable Minimax Equilibrium via Multi-Generator Architectures,Hongyang Zhang;Susu Xu;Jiantao Jiao;Pengtao Xie;Ruslan Salakhutdinov;Eric P. Xing,hongyanz@cs.cmu.edu;susux@andrew.cmu.edu;jiantao@eecs.berkeley.edu;pengtao.xie@petuum.com;rsalakhu@cs.cmu.edu;epxing@cs.cmu.edu,4;5;7,4;3;3,Reject,0,10,0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;University of California Berkeley;Petuum Inc.;Carnegie Mellon University;Carnegie Mellon University,1;1;5;-1;1;1,24;24;18;-1;24;24,5,9/27/18,6,3,1,0,5,0,519;75;1092;1322;66968;24050,47;23;60;70;254;603,11;6;17;17;81;75,84;0;149;153;7763;2652,-1;-1
2384,ICLR,2019,Stability of Stochastic Gradient Method with Momentum for Strongly Convex Loss Functions,Ali Ramezani-Kebrya;Ashish Khisti;and Ben Liang,aramezani@ece.utoronto.ca;akhisti@ece.utoronto.ca;liang@ece.utoronto.ca,4;6;4,5;4;4,Reject,0,16,1,yes,9/27/18,Toronto University;Toronto University;Toronto University,18;18;18,22;22;22,1;8,9/27/18,0,0,0,0,0,0,79;6600;0,12;195;1,4;25;0,3;608;0,-1;-1
2385,ICLR,2019,A Variational Dirichlet Framework for Out-of-Distribution Detection,Wenhu Chen;Yilin Shen;William Wang;Hongxia Jin,wenhuchen@ucsb.edu;yilin.shen@samsung.com;william@cs.ucsb.edu;hongxia.jin@samsung.com,6;5;6,4;5;3,Reject,4,7,0,yes,9/27/18,UC Santa Barbara;Samsung;UC Santa Barbara;Samsung,37;-1;37;-1,53;-1;53;-1,,9/27/18,3,2,1,0,0,1,468;947;10;1493,42;101;23;179,10;18;2;22,65;73;3;116,-1;-1
2386,ICLR,2019,Optimized Gated Deep Learning Architectures for Sensor Fusion,Myung Seok Shim;Peng Li,mrshim1101@tamu.edu;pli@tamu.edu,4;4;3,5;4;4,Reject,0,0,0,yes,9/27/18,Texas A&M;Texas A&M,44;44,160;160,,9/27/18,2,2,0,0,3,0,11;7732,4;760,2;41,0;533,-1;-1
2387,ICLR,2019,RESIDUAL NETWORKS CLASSIFY INPUTS BASED ON THEIR NEURAL TRANSIENT DYNAMICS,Fereshteh Lagzi,lagzi@informatik.uni-freiburg.de,4;2;5,4;4;5,Reject,0,4,0,yes,9/27/18,Universität Freiburg,123,82,,9/27/18,0,0,0,0,0,0,3,3,1,0,-1
2388,ICLR,2019,Better Generalization with On-the-fly Dataset Denoising,Jiaming Song;Tengyu Ma;Michael Auli;Yann Dauphin,jiaming.tsong@gmail.com;tengyuma@cs.stanford.edu;michael.auli@gmail.com;yann@dauphin.io,5;6;6,4;3;5,Reject,0,9,0,yes,9/27/18,Stanford University;Stanford University;Facebook;Facebook,4;4;-1;-1,3;3;-1;-1,8,9/27/18,5,3,0,1,0,0,779;3818;6784;8515,44;87;71;43,13;32;30;27,118;491;1017;1021,-1;-1
2389,ICLR,2019,Generative Adversarial Networks for Extreme Learned Image Compression,Eirikur Agustsson;Michael Tschannen;Fabian Mentzer;Radu Timofte;Luc van Gool,aeirikur@vision.ee.ethz.ch;michaelt@nari.ee.ethz.ch;mentzerf@vision.ee.ethz.ch;radu.timofte@vision.ee.ethz.ch;vangool@vision.ee.ethz.ch,6;6;4,3;3;4,Reject,0,5,0,yes,9/27/18,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10;10;10,10;10;10;10;10,5;4,4/9/18,119,59,41,2,495,15,1669;1229;515;7850;80600,36;39;13;180;1263,14;18;8;36;121,232;160;91;1014;10366,-1;-1
2390,ICLR,2019,A Synaptic Neural Network and Synapse Learning,Chang Li,changli@neatware.com,2;3;2;2,4;3;3;3,Reject,0,17,0,yes,9/27/18,Neatware,-1,-1,1;10,9/27/18,0,0,0,0,0,0,55,39,3,5,-1
2391,ICLR,2019,Reducing Overconfident Errors outside the Known Distribution,Zhizhong Li;Derek Hoiem,zli115@illinois.edu;dhoiem@illinois.edu,6;4;6,4;4;3,Reject,0,8,0,yes,9/27/18,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",3;3,37;37,7,9/27/18,4,2,0,0,0,0,888;12939,116;110,14;43,56;1705,-1;-1
2392,ICLR,2019,Adversarial Sampling for Active Learning,Christoph Mayer;Radu Timofte,chmayer@vision.ee.ethz.ch;radu.timofte@vision.ee.ethz.ch,6;5;5,2;4;5,Reject,0,4,0,yes,9/27/18,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10,10;10,4,8/20/18,9,5,4,0,12,1,1242;7469,61;180,17;36,93;994,-1;-1
2393,ICLR,2019,"Classification from Positive, Unlabeled and Biased Negative Data",Yu-Guan Hsieh;Gang Niu;Masashi Sugiyama,yu-guan.hsieh@ens.fr;gang.niu@riken.jp;sugi@k.u-tokyo.ac.jp,5;6;5,5;3;3,Reject,0,8,0,yes,9/27/18,Ecole Normale Superieure;RIKEN;The University of Tokyo,99;-1;54,603;-1;45,1,9/27/18,9,7,4,0,0,0,24;1146;11582,3;78;712,2;16;52,2;143;1299,-1;-1
2394,ICLR,2019,Learning Diverse Generations using Determinantal Point Processes,Mohamed Elfeki;Camille Couprie;Mohamed Elhoseiny,m.elfeki11@gmail.com;coupriec@fb.com;elhoseiny@fb.com,5;5;5,4;5;4,Reject,0,7,1,yes,9/27/18,University of Central Florida;Facebook;Facebook,78;-1;-1,1103;-1;-1,5;4,9/27/18,21,9,3,0,1,3,53;4460;1405,11;44;67,4;16;18,6;374;180,-1;-1
2395,ICLR,2019,Probabilistic Binary Neural Networks,Jorn W.T. Peters;Tim Genewein;Max Welling,jornpeters@gmail.com;tim.genewein@gmail.com;welling.max@gmail.com,6;5;3,4;2;3,Reject,0,6,0,yes,9/27/18,University of Amsterdam;Google;University of California - Irvine,169;-1;35,59;-1;99,,9/10/18,17,8,11,0,32,1,48;1123;26565,3;27;269,3;10;58,8;68;5085,-1;-1
2396,ICLR,2019,Geomstats: a Python Package for Riemannian Geometry in Machine Learning,Nina Miolane;Johan Mathe;Claire Donnat;Mikael Jorda;Xavier Pennec,nmiolane@stanford.edu;johan@froglabs.ai;cdonnat@stanford.edu;mjorda@stanford.edu;xavier.pennec@inria.fr,4;4;3;8,4;5;5;2,Reject,1,1,0,yes,9/27/18,Stanford University;;Stanford University;Stanford University;INRIA,4;-1;4;4;-1,3;-1;3;3;-1,,5/21/18,9,4,3,0,134,0,47;15;158;85;12851,22;4;13;7;373,5;2;7;3;55,1;2;14;3;1032,-1;-1
2397,ICLR,2019,Outlier Detection from Image Data,Lei Cao;Yizhou Yan;Samuel Madden;Elke Rundensteiner,lcao@csail.mit.edu;yyan2@wpi.edu;madden@csail.mit.edu;rundenst@cs.wpi.edu,4;5;5,4;3;4,Reject,1,11,0,yes,9/27/18,Massachusetts Institute of Technology;Worcester Polytechnic Institute;Massachusetts Institute of Technology;Worcester Polytechnic Institute,2;169;2;169,5;1103;5;1103,8,9/27/18,1,1,0,0,0,0,1608;141;33629;7758,319;37;366;596,16;6;80;44,127;7;3725;515,-1;-1
2398,ICLR,2019,Investigating CNNs' Learning Representation under label noise,Ryuichiro Hataya;Hideki Nakayama,hataya@nlab.ci.i.u-tokyo.ac.jp;nakayama@nlab.ci.i.u-tokyo.aco.jp,5;4;5,4;5;5,Reject,0,3,0,yes,9/27/18,The University of Tokyo;,54;-1,45;-1,,9/27/18,1,1,0,0,0,0,33;625,9;87,3;15,3;67,-1;-1
2399,ICLR,2019,Discriminative out-of-distribution detection for semantic segmentation,Petra Bevandić;Siniša Šegvić;Ivan Krešo;Marin Oršić,petra.bevandic@fer.hr;sinisa.segvic@fer.hr;ivan.kreso@fer.hr;marin.orsic@fer.hr,4;7;3,4;5;3,Reject,0,5,0,yes,9/27/18,"Faculty of Electrical Engineering and Computing, University of Zagreb;Faculty of Electrical Engineering and Computing, University of Zagreb;Faculty of Electrical Engineering and Computing, University of Zagreb;Faculty of Electrical Engineering and Computing, University of Zagreb",478;478;478;478,894;894;894;894,1;2,8/23/18,14,5,4,1,12,1,51;603;120;54,6;71;11;15,4;15;5;4,5;27;5;5,-1;-1
2400,ICLR,2019,DppNet: Approximating Determinantal Point Processes with Deep Networks,Zelda Mariet;Jasper Snoek;Yaniv Ovadia,zelda@csail.mit.edu;jsnoek@google.com;yovadia@google.com,3;5;5,5;4;3,Reject,0,5,0,yes,9/27/18,Massachusetts Institute of Technology;Google;Google,2;-1;-1,5;-1;-1,5,9/27/18,2,2,0,0,13,0,142;4862;547,16;61;37,5;18;10,11;508;63,-1;-1
2401,ICLR,2019,Difference-Seeking Generative Adversarial Network,Yi-Lin Sung;Sung-Hsien Hsieh;Soo-Chang Pei;Chun-Shien Lu,r06942076@ntu.edu.tw;parvaty316@hotmail.com;peisc@ntu.edu.tw;lcs@iis.sinica.edu.tw,5;4;3,4;3;4,Reject,0,4,0,yes,9/27/18,National Taiwan University;;National Taiwan University;Academia Sinica,85;-1;85;-1,197;-1;197;-1,5;4,9/27/18,0,0,0,0,0,0,0;168;74;3116,1;30;22;170,0;5;5;30,0;25;4;217,-1;-1
2402,ICLR,2019,Learning to Search Efficient DenseNet with Layer-wise Pruning,Xuanyang Zhang;Hao liu;Zhanxing Zhu;Zenglin Xu,xuanyang91.zhang@gmail.com;uestcliuhao@gmail.com;zhanxing.zhu@pku.edu.cn;zenglin@gmail.com,4;4;4,5;4;4,Reject,0,0,0,yes,9/27/18,University of Electronic Science and Technology of China;University of California Berkeley;Peking University;University of Electronic Science and Technology of China,169;5;24;169,843;18;27;843,,9/27/18,0,0,0,0,0,0,0;373;841;2045,2;76;81;138,0;11;14;24,0;16;105;141,-1;-1
2403,ICLR,2019,Unsupervised  one-to-many image translation,Samuel Lavoie-Marchildon;Sebastien Lachapelle;Mikołaj Bińkowski;Aaron Courville;Yoshua Bengio;R Devon Hjelm,samuel.lavoie-marchildon@umontreal.ca;sebastien.lachapelle@umontreal.ca;mikbinkowski@gmail.com;aaron.courville@gmail.com;yoshua.umontreal@gmail.com;devon.hjelm@microsoft.com,3;4;4,4;4;4,Reject,0,5,0,yes,9/27/18,University of Montreal;University of Montreal;Imperial College London;University of Montreal;University of Montreal;Microsoft,123;123;72;123;123;-1,108;108;8;108;108;-1,,9/27/18,2,1,0,0,0,0,281;12;61116;1693;272;205096,7;3;203;43;4;807,4;2;65;13;2;147,65;3;7900;263;61;24136,-1;-1
2404,ICLR,2019,Calibration of neural network logit vectors to combat adversarial attacks,Oliver Goldstein,og14775@my.bristol.ac.uk,3;2;4,4;4;5,Reject,2,3,0,yes,9/27/18,University of Bristol,123,76,4,9/27/18,0,0,0,0,0,0,0,2,0,0,-1
2405,ICLR,2019,The Expressive Power of Gated Recurrent Units as a Continuous Dynamical System,Ian D. Jordan;Piotr Aleksander Sokol;Il Memming Park,ian.jordan@stonybrook.edu;piotr.sokol@stonybrook.edu;memming.park@stonybrook.edu,6;5;5,4;4;4,Reject,0,6,0,yes,9/27/18,"State University of New York, Stony Brook;State University of New York, Stony Brook;State University of New York, Stony Brook",41;41;41,258;258;258,,9/27/18,0,0,0,0,0,0,33;16;678,7;10;40,3;2;12,4;0;59,-1;-1
2406,ICLR,2019,A Self-Supervised Method for Mapping Human Instructions to Robot Policies,Hsin-Wei Yu;Po-Yu Wu;Chih-An Tsao;You-An Shen;Shih-Hsuan Lin;Zhang-Wei Hong;Yi-Hsiang Chang;Chun-Yi Lee,hsinweiyo@gmail.com;bwoyu85928@gmai.com;hl6540@gmail.com;jerrylin1121@gmail.com;williamd4112@gapp.nthu.edu.tw;shawn420@gapp.nthu.edu.tw;cylee@gapp.nthu.edu.tw,4;3;2,5;5;4,Reject,0,0,0,yes,9/27/18,National Tsing Hua University;Gmai;National Tsing Hua University;National Tsing Hua University;National Tsing Hua University;National Tsing Hua University;National Tsing Hua University,199;-1;199;199;199;199;199,323;-1;323;323;323;323;323,,9/27/18,0,0,0,0,0,0,40;0;0;0;58;183;67;758,5;2;1;1;11;12;15;73,4;0;0;0;4;4;4;18,9;0;0;0;2;16;10;55,-1;-1
2407,ICLR,2019,Detecting Out-Of-Distribution Samples Using Low-Order Deep Features Statistics,Igor M. Quintanilha;Roberto de M. E. Filho;José Lezama;Mauricio Delbracio;Leonardo O. Nunes,igormq@poli.ufrj.br;robertomest@poli.ufrj.br;jlezama@fing.edu.uy;mdelbra@fing.edu.uy;lnunes@microsoft.com,7;5;5,4;4;4,Reject,3,16,1,yes,9/27/18,Federal University of Rio de Janeiro - UFRJ;Federal University of Rio de Janeiro - UFRJ;Facultad de Ingeniería;Facultad de Ingeniería;Microsoft,386;386;478;478;-1,715;715;631;631;-1,,9/27/18,5,0,1,0,0,0,9;6;238;514;129,3;2;51;37;25,2;1;5;12;6,0;0;20;66;4,-1;-1
2408,ICLR,2019,Context Dependent Modulation of Activation Function,Long Sha;Jonathan Schwarcz;Pengyu Hong,longsha@brandeis.edu;johnschwarcz@brandeis.edu;hongpeng@brandeis.edu,4;4;4;4;6,5;3;4;5;4,Reject,0,5,0,yes,9/27/18,Brandeis University;Brandeis University;Brandeis University,314;314;314,223;223;223,,9/27/18,0,0,0,0,0,0,301;0;1768,32;1;92,10;0;21,17;0;97,-1;-1
2409,ICLR,2019,Reliable Uncertainty Estimates in Deep Neural Networks using Noise Contrastive Priors,Danijar Hafner;Dustin Tran;Timothy Lillicrap;Alex Irpan;James Davidson,mail@danijar.com;trandustin@google.com;countzero@google.com;alexirpan@google.com;james@electric-thought.com,7;4;6,3;4;4,Reject,1,4,0,yes,9/27/18,"Department of Computer Science, University of Toronto;Google;Google;Google;Electric-thought",18;-1;-1;-1;-1,22;-1;-1;-1;-1,11,7/24/18,41,21,10,0,29,3,624;1740;23537;611;4907,24;50;74;13;205,9;20;39;6;28,65;198;2880;37;812,-1;-1
2410,ICLR,2019,DON’T JUDGE A BOOK BY ITS COVER - ON THE DYNAMICS OF RECURRENT NEURAL NETWORKS,Doron Haviv;Alexander Rivkind;Omri Barak,doron.haviv12@gmail.com;sashkarivkind@gmail.com;omri.barak@gmail.com,6;5;7,3;4;4,Reject,0,6,0,yes,9/27/18,Technion;Technion;Technion,25;25;25,327;327;327,,9/27/18,0,0,0,0,0,0,26;47;2349,9;7;41,3;3;16,2;5;167,-1;-1
2411,ICLR,2019,Volumetric Convolution: Automatic Representation Learning in Unit Ball,Sameera Ramasinghe;Salman Khan;Nick Barnes,sameera.ramasinghe@anu.edu.au;salman.khan@anu.edu.au;nick.barnes@data61.csiro.au,6;5;5,2;5;3,Reject,0,7,0,yes,9/27/18,"Australian National University;Australian National University;, CSIRO",106;106;-1,48;48;-1,,9/27/18,3,2,1,0,2,0,23;449;1639,13;60;198,3;12;21,0;31;112,-1;-1
2412,ICLR,2019,Classification in the dark using tactile exploration,Mayur Mudigonda;Blake Tickell;Pulkit Agrawal,mudigonda@berkeley.edu;btickell@berkeley.edu;pulkitag@berkeley.edu,4;3;2,3;5;5,Reject,0,0,0,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,18;18;18,,9/27/18,0,0,0,0,0,0,205;0;2961,25;1;52,6;0;16,13;0;242,-1;-1
2413,ICLR,2019,"Deep Imitative Models for Flexible Inference, Planning, and Control",Nicholas Rhinehart;Rowan McAllister;Sergey Levine,nrhineha@cs.cmu.edu;rmcallister@berkeley.edu;svlevine@eecs.berkeley.edu,6;5;6,3;5;1,Reject,0,5,0,yes,9/25/19,Carnegie Mellon University;University of California Berkeley;University of California Berkeley,1;5;5,24;18;18,,10/15/18,30,18,12,2,126,2,378;556;24386,21;29;309,9;10;73,42;83;3167,m;m
2414,ICLR,2019,SHE2: Stochastic Hamiltonian Exploration and Exploitation for Derivative-Free Optimization,Haoyi Xiong;Wenqing Hu;Zhanxing Zhu;Xinjian Li;Yunchao Zhang;Jun Huan,xhyccc@gmail.com;huwenqing.pku@gmail.com;zhanxing.zhu@pku.edu.cn;lixingjian@baidu.com;yzgv7@mst.edu;huanjun@baidu.com,4;3;3,4;3;5,Reject,0,0,0,yes,9/27/18,Baidu;Missouri University of Science and Technology;Peking University;Baidu;Missouri University of Science and Technology;Baidu,-1;477;24;-1;477;-1,-1;538;27;-1;538;-1,,9/27/18,0,0,0,0,0,0,1473;105;841;126;165;2630,91;27;81;33;53;194,21;4;14;6;6;23,76;1;105;5;17;240,-1;-1
2415,ICLR,2019,Universal discriminative quantum neural networks,Hongxiang Chen;Leonard Wossnig;Hartmut Neven;Simone Severini;Masoud Mohseni,we.taper@gmail.com;leonard.wossnig.17@ucl.ac.uk;neven@google.com;s.severini@ucl.ac.uk;mohseni@google.com,5;5;2,3;2;2,Reject,0,0,0,yes,9/27/18,;University College London;Google;University College London;Google,-1;50;-1;50;-1,-1;16;-1;16;-1,8,5/22/18,15,4,2,0,4,0,428;303;5005;2870;1095,67;20;130;273;38,12;8;32;30;12,16;8;305;226;52,-1;-1
2416,ICLR,2019,Bias Also Matters: Bias Attribution for Deep Neural Network Explanation,Shengjie Wang;Tianyi Zhou;Jeff Bilmes,wangsj@cs.washington.edu;tianyi.david.zhou@gmail.com;bilmes@uw.edu,5;5;5,5;5;4,Reject,0,0,0,yes,9/27/18,"University of Washington;University of Washington;University of Washington, Seattle",6;6;6,25;25;25,,9/27/18,4,0,1,0,0,0,901;1447;13581,63;81;350,15;14;54,51;142;1275,-1;-1
2417,ICLR,2019,Dirichlet Variational Autoencoder,Weonyoung Joo;Wonsung Lee;Sungrae Park;and Il-Chul Moon,weonyoungjoo@gmail.com;aporia@kaist.ac.kr;sungraepark@kaist.ac.kr;icmoon@kaist.ac.kr,6;5;7,4;5;3,Reject,0,10,0,yes,9/27/18,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,20;20;20;20,95;95;95;95,5,9/27/18,5,3,3,0,0,2,13;87;168;614,7;15;19;134,2;5;6;11,4;9;21;40,-1;-1
2418,ICLR,2019,DANA: Scalable Out-of-the-box Distributed ASGD Without Retuning,Ido Hakimi;Saar Barkai;Moshe Gabel;Assaf Schuster,idohakimi@gmail.com;saarbarkai@gmail.com;mgabel@cs.toronto.edu;assaf@cs.technion.ac.il,5;7;5,3;4;4,Reject,0,8,0,yes,9/27/18,"Technion;Technion;Department of Computer Science, University of Toronto;Technion",25;25;18;25,327;327;22;327,,9/27/18,1,0,1,0,0,0,4;2;357;4033,3;3;23;254,1;1;8;34,0;0;11;348,-1;-1
2419,ICLR,2019,Bayesian Deep Learning via Stochastic Gradient MCMC with a Stochastic Approximation Adaptation,Wei Deng;Xiao Zhang;Faming Liang;Guang Lin,deng106@purdue.edu;zhang923@purdue.edu;fmliang@purdue.edu;guanglin@purdue.edu,5;4;6,4;2;5,Reject,0,0,1,yes,9/27/18,Purdue University;Purdue University;Purdue University;Purdue University,26;26;26;26,60;60;60;60,4;11,9/27/18,0,0,0,0,0,0,62;463;2416;1268,36;184;117;152,4;10;23;19,5;29;286;49,-1;-1
2420,ICLR,2019,Physiological Signal Embeddings (PHASE) via Interpretable Stacked Models,Hugh Chen;Scott Lundberg;Gabe Erion;Su-In Lee,hughchen@cs.washington.edu;slund1@cs.washington.edu;erion@cs.washington.edu;suinlee@cs.washington.edu,6;5;4,5;4;4,Reject,0,10,0,yes,9/27/18,University of Washington;University of Washington;University of Washington;University of Washington,6;6;6;6,25;25;25;25,3;1;2,9/27/18,0,0,0,0,0,0,100;1191;0;3477,12;37;2;105,4;7;0;22,18;157;0;351,-1;-1
2421,ICLR,2019,ReNeg and Backseat Driver: Learning from demonstration with continuous human feedback,Zoe Papakipos;Jacob Beck;Michael Littman,zoe_papakipos@alumni.brown.edu;jacob_beck@alumni.brown.edu;mlittman@cs.brown.edu,3;4;2,4;4;5,Reject,0,12,0,yes,9/27/18,Brown University;Brown University;Brown University,65;65;65,50;50;50,10,9/27/18,0,0,0,0,0,0,0;1323;27157,3;78;368,0;16;68,0;93;2808,-1;-1
2422,ICLR,2019,Learning  agents with prioritization and parameter noise in continuous state and action space,Rajesh Devaraddi;G. Srinivasaraghavan,rajesh.dm@iiitb.ac.in;gsr@iiitb.ac.in,3;4;4,4;3;4,Reject,0,0,0,yes,9/27/18,Indian Institute of Technology Bombay;Indian Institute of Technology Bombay,115;115,367;367,,9/27/18,2,1,0,0,0,0,2;71,1;20,1;3,0;11,-1;-1
2423,ICLR,2019,Gradient Descent Happens in a Tiny Subspace,Guy Gur-Ari;Daniel A. Roberts;Ethan Dyer,guyg@ias.edu;danr@fb.com;edyer@google.com,4;6;4,3;4;4,Reject,0,4,1,yes,9/27/18,"Institue for Advanced Study, Princeton;Facebook;Google",-1;-1;-1,-1;-1;-1,,9/27/18,31,17,4,2,14,4,1102;1469;442,21;26;33,14;13;12,104;118;31,-1;-1
2424,ICLR,2019,Nested Dithered Quantization for Communication Reduction in Distributed Training,Afshin Abdi;Faramarz Fekri,abdi@ece.gatech.edu;fekri@ece.gatech.edu,5;5;7,4;3;4,Reject,0,5,0,yes,9/27/18,Georgia Institute of Technology;Georgia Institute of Technology,13;13,33;33,1,9/27/18,1,0,0,0,0,0,116;3136,33;244,4;30,9;204,-1;-1
2425,ICLR,2019,Learned optimizers that outperform on wall-clock and validation loss,Luke Metz;Niru Maheswaranathan;Jeremy Nixon;Daniel Freeman;Jascha Sohl-dickstein,lmetz@google.com;nirum@google.com;jeremynixon@google.com;cdfreeman@google.com;jaschasd@google.com,5;4;5,3;5;4,Reject,0,10,1,yes,9/27/18,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,,9/27/18,11,6,4,1,85,0,7230;693;59;160;4921,25;38;10;13;101,10;12;5;6;33,1206;51;3;10;684,-1;-1
2426,ICLR,2019,A Walk with SGD: How SGD Explores Regions of Deep Network Loss?,Chen Xing;Devansh Arpit;Christos Tsirigotis;Yoshua Bengio,xingchen1113@gmail.com;devansharpit@gmail.com;tsirif@gmail.com;yoshua.umontreal@gmail.com,4;4;3,5;3;4,Reject,1,0,0,yes,9/27/18,Nankai University;University of Montreal;;University of Montreal,478;123;-1;123,1049;108;-1;108,,9/27/18,0,0,0,0,0,0,273;886;39;201719,148;42;4;807,7;12;2;147,23;116;2;23989,-1;-1
2427,ICLR,2019,Combining adaptive algorithms and hypergradient method: a performance and robustness study,Akram Erraqabi;Nicolas Le Roux,akram.er-raqabi@umontreal.ca;nicolas@le-roux.name,3;3;4,4;2;4,Reject,0,4,0,yes,9/27/18,University of Montreal;Google,123;-1,108;-1,,9/27/18,1,0,0,0,0,0,64;4896,10;172,5;23,6;536,-1;-1
2428,ICLR,2019,The Expressive Power of Deep Neural Networks with Circulant Matrices,Alexandre Araujo;Benjamin Negrevergne;Yann Chevaleyre;Jamal Atif,alexandre.araujo@dauphine.eu;benjamin.negrevergne@dauphine.fr;yann.chevaleyre@lamsade.dauphine.fr;jamal.atif@lamsade.dauphine.fr,6;4;7,5;4;4,Reject,0,3,0,yes,9/27/18,Univeristé Paris-Dauphine;Univeristé Paris-Dauphine;Univeristé Paris-Dauphine;Univeristé Paris-Dauphine,478;478;478;478,1103;1103;1103;1103,,9/27/18,0,0,0,0,0,0,35;256;2069;888,9;30;102;114,3;9;25;13,6;22;194;36,-1;-1
2429,ICLR,2019,The Anisotropic Noise in Stochastic Gradient Descent: Its Behavior of Escaping from Minima and Regularization Effects,Zhanxing Zhu;Jingfeng Wu;Bing Yu;Lei Wu;Jinwen Ma,zhanxing.zhu@pku.edu.cn;pkuwjf@pku.edu.cn;byu@pku.edu.cn;leiwu@pku.edu.cn;jwma@math.pku.edu.cn,5;4;6,4;5;3,Reject,0,0,0,yes,9/27/18,Peking University;Peking University;Peking University;Peking University;Peking University,24;24;24;24;24,27;27;27;27;27,,3/1/18,45,23,4,1,0,7,841;46;1177;752;1990,80;2;102;135;192,14;2;15;15;21,105;7;90;38;124,-1;-1
2430,ICLR,2019,Stochastic Gradient Push for Distributed Deep Learning,Mahmoud Assran;Nicolas Loizou;Nicolas Ballas;Mike Rabbat,massran@fb.com;n.loizou@sms.ed.ac.uk;ballasn@fb.com;mikerabbat@fb.com,6;6;6,3;4;3,Reject,0,9,2,yes,9/27/18,Facebook;University of Edinburgh;Facebook;Facebook,-1;33;-1;-1,-1;27;-1;-1,1;9,9/27/18,58,36,21,8,12,10,88;271;4922;5071,8;20;54;180,3;8;20;36,13;23;577;353,-1;-1
2431,ICLR,2019,Towards Language Agnostic Universal Representations,Armen Aghajanyan;Xia Song;Saurabh Tiwary,araghaja@microsoft.com;xiaso@microsoft.com;satiwary@microsoft.com,5;4;6,4;4;3,Reject,0,7,0,yes,9/27/18,Microsoft;Microsoft;Microsoft,-1;-1;-1,-1;-1;-1,6,9/23/18,19,0,0,0,19,0,42;595;504,7;41;14,3;6;5,2;101;97,-1;-1
2432,ICLR,2019,Mapping the hyponymy relation of wordnet onto vector Spaces,Jean-Philippe Bernardy;Aleksandre Maskharashvili,jean-philippe.bernardy@gu.se;aleksandre.maskharashvili@gu.se,3;3;3,4;3;5,Reject,0,1,0,yes,9/27/18,Gothenburg University;Gothenburg University,478;478,197;197,,9/27/18,0,0,0,0,0,0,461;9,56;13,12;2,51;0,-1;-1
2433,ICLR,2019,Model Compression with Generative Adversarial Networks,Ruishan Liu;Nicolo Fusi;Lester Mackey,ruishan@stanford.edu;fusi@microsoft.com;lmackey@microsoft.com,6;5;5,4;4;4,Reject,0,7,0,yes,9/27/18,Stanford University;Microsoft;Microsoft,4;-1;-1,3;-1;-1,5;4,9/27/18,9,5,3,0,8,2,82;891;484,17;31;60,5;10;10,2;143;43,-1;-1
2434,ICLR,2019,Efficient Convolutional Neural Network Training with Direct Feedback Alignment,Donghyeon Han;Hoi-jun Yoo,hdh4797@kaist.ac.kr;hjyoo@kaist.ac.kr,4;4;5,4;4;3,Reject,0,4,0,yes,9/27/18,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,20;20,95;95,,9/27/18,3,2,1,0,12,0,76;6292,16;567,5;36,4;423,-1;-1
2435,ICLR,2019, Large-Scale Visual Speech Recognition,Brendan Shillingford;Yannis Assael;Matthew W. Hoffman;Thomas Paine;Cían Hughes;Utsav Prabhu;Hank Liao;Hasim Sak;Kanishka Rao;Lorrayne Bennett;Marie Mulville;Ben Coppin;Ben Laurie;Andrew Senior;Nando de Freitas,shillingford@google.com;assael@google.com;mwhoffman@google.com;tpaine@google.com;cianh@google.com;utsavprabhu@google.com;hankliao@google.com;hasim@google.com;kanishkarao@google.com;lorrayne@google.com;mariecharlotte@google.com;coppin@google.com;benl@google.com;andrewsenior@google.com;nandodefreitas@google.com,9;4;3,4;4;5,Reject,0,17,0,yes,9/27/18,Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,,7/13/18,28,10,11,2,0,2,295;853;1781;482;437;228;988;4643;1879;28;28;217;1224;21523;18934,13;14;34;197;29;10;24;45;33;1;1;4;49;150;184,5;9;17;11;9;6;13;25;17;1;1;4;19;53;54,34;93;193;22;19;22;64;397;134;2;2;32;157;1863;1853,-1;-1
2436,ICLR,2019,The effectiveness of layer-by-layer training using the information bottleneck principle,Adar Elad;Doron Haviv;Yochai Blau;Tomer Michaeli,adarelad@campus.technion.ac.il;doron.haviv12@gmail.com;yochai@campus.technion.ac.il;tomer.m@ee.technion.ac.il,5;5;2,5;4;4,Reject,0,5,0,yes,9/27/18,Technion;Technion;Technion;Technion,25;25;25;25,327;327;327;327,8,9/27/18,7,1,0,0,0,0,22;26;336;1320,6;9;16;76,3;3;6;17,1;2;50;131,-1;-1
2437,ICLR,2019,The Case for Full-Matrix Adaptive Regularization,Naman Agarwal;Brian Bullins;Xinyi Chen;Elad Hazan;Karan Singh;Cyril Zhang;Yi Zhang,namanagarwal@google.com;bbullins@cs.princeton.edu;xinyic@google.com;ehazan@cs.princeton.edu;karans@cs.princeton.edu;cyril.zhang@cs.princeton.edu;y.zhang@cs.princeton.edu,5;6;5,3;3;3,Reject,0,3,0,yes,9/27/18,Google;Princeton University;Google;Princeton University;Princeton University;Princeton University;Princeton University,-1;30;-1;30;30;30;30,-1;7;-1;7;7;7;7,9,6/8/18,12,5,5,0,0,2,588;381;697;11911;4841;198;-1,31;21;83;148;261;18;-1,12;8;16;43;39;6;-1,86;63;20;1998;346;20;0,-1;-1
2438,ICLR,2019,What Information Does a ResNet Compress?,Luke Nicholas Darlow;Amos Storkey,l.n.darlow@sms.ed.ac.uk;a.storkey@ed.ac.uk,4;4;6,4;3;5,Reject,0,11,0,yes,9/27/18,University of Edinburgh;University of Edinburgh,33;33,27;27,,9/27/18,1,0,0,0,0,0,123;3809,15;198,6;31,16;437,-1;-1
2439,ICLR,2019,Information Regularized Neural Networks,Tianchen Zhao;Dejiao Zhang;Zeyu Sun;Honglak Lee,ericolon@umich.edu;dejiao@umich.edu;zeyusun@umich.edu;honglak@eecs.umich.edu,6;5;6,4;3;3,Reject,0,7,0,yes,9/27/18,University of Michigan;University of Michigan;University of Michigan;University of Michigan,8;8;8;8,21;21;21;21,1,9/27/18,1,0,0,0,0,0,71;141;417;23861,14;15;112;166,4;7;10;60,2;17;14;2806,-1;-1
2440,ICLR,2019,NICE: noise injection and clamping estimation for neural network quantization,Chaim Baskin;Natan Liss;Yoav Chai;Evgenii Zheltonozhskii;Eli Schwartz;Raja Girayes;Avi Mendelson;Alexander M.Bronstein,chaimbaskin@cs.technion.ac.il;lissnatan@campus.technion.ac.il;yoavchai1@mail.tau.ac.il;evgeniizh@campus.technion.ac.il;eli.shw@gmail.com;raja@tauex.tau.ac.il;avi.mendelson@tce.technion.ac.il;bron@cs.technion.ac.il,4;5;4,4;3;3,Reject,1,0,0,yes,9/27/18,Technion;Technion;Tel Aviv University;Technion;Tel Aviv University;Tel Aviv University;Technion;Technion,25;25;37;25;37;37;25;25,327;327;217;327;217;217;327;327,3;2,9/27/18,22,8,13,2,7,2,88;72;27;78;169;1514;1652;8219,14;6;4;10;13;113;138;264,5;4;2;4;6;22;23;48,10;8;2;8;17;132;121;693,-1;-1
2441,ICLR,2019,Mean Replacement Pruning  ,Utku Evci;Nicolas Le Roux;Pablo Castro;Leon Bottou,evcu@google.com;nicolas@le-roux.name;psc@google.com;leon@bottou.org,5;5;4,3;3;4,Reject,0,9,0,yes,9/27/18,Google;Google;Google;Facebook,-1;-1;-1;-1,-1;-1;-1;-1,,9/27/18,1,0,1,0,0,0,147;4896;810;47674,8;172;30;167,3;23;14;58,17;536;53;7065,-1;-1
2442,ICLR,2019,The Effectiveness of Pre-Trained Code Embeddings,Ben Trevett;Donald Reay;N. K. Taylor,bbt1@hw.ac.uk;n.k.taylor@hw.ac.uk;d.s.reay@hw.ac.uk,6;4;5,3;4;4,Reject,0,5,0,yes,9/27/18,Heriot-Watt University;Heriot-Watt University;Heriot-Watt University,261;261;261,363;363;363,3,9/27/18,1,1,0,0,0,0,0;337;129,3;53;54,0;9;6,0;15;8,-1;-1
2443,ICLR,2019,Laplacian Networks: Bounding Indicator Function Smoothness for Neural Networks Robustness,Carlos Eduardo Rosar Kos Lassance;Vincent Gripon;Antonio Ortega,carlos.rosarkoslassance@imt-atlantique.fr;vincent.gripon@imt-atlantique.fr;antonio.ortega@ee.usc.edu,9;5;5,5;3;4,Reject,0,11,0,yes,9/27/18,IMT Atlantique;IMT Atlantique;University of Southern California,-1;-1;30,-1;-1;66,4;10,5/24/18,4,4,2,1,5,0,19;724;11960,16;93;468,3;15;48,2;59;947,-1;-1
2444,ICLR,2019,Jumpout: Improved Dropout for Deep Neural Networks with Rectified Linear Units,Shengjie Wang;Tianyi Zhou;Jeff Bilmes,tianyi.david.zhou@gmail.com,5;4;4,4;3;5,Reject,0,5,0,yes,9/27/18,University of Washington,6,25,8,9/27/18,0,0,0,0,0,0,901;1447;13581,63;81;350,15;14;54,51;142;1275,-1;-1
2445,ICLR,2019,Causal importance of orientation selectivity for generalization in image recognition,Jumpei Ukita,i.love.ny517@gmail.com,5;4;7,4;4;2,Reject,0,5,1,yes,9/27/18,The University of Tokyo,54,45,8,9/27/18,2,1,0,0,0,1,11,6,2,2,-1
2446,ICLR,2019,Backprop with Approximate Activations for Memory-efficient Network Training,Ayan Chakrabarti;Benjamin Moseley,ayan@wustl.edu;moseleyb@andrew.cmu.edu,5;5;7,5;3;4,Reject,0,9,0,yes,9/27/18,"Washington University, St. Louis;Carnegie Mellon University",99;1,50;24,,9/27/18,1,0,0,0,0,0,1358;1759,47;128,17;19,155;133,-1;-1
2447,ICLR,2019,Experience replay for continual learning,David Rolnick;Arun Ahuja;Jonathan Schwarz;Timothy P. Lillicrap;Greg Wayne,drolnick@mit.edu;arahuja@google.com;schwarzjn@google.com;countzero@google.com;gregwayne@google.com,5;5;5,5;4;5,Reject,0,4,0,yes,9/27/18,Massachusetts Institute of Technology;Google;Google;Google;Google,2;-1;-1;-1;-1,5;-1;-1;-1;-1,,9/27/18,44,23,9,0,11,2,615;759;551;23537;2564,37;37;27;74;32,10;11;8;39;15,39;49;92;2880;261,-1;-1
2448,ICLR,2019,Doubly Sparse: Sparse Mixture of Sparse Experts for Efficient Softmax Inference,Shun Liao;Ting Chen;Tian Lin;Chong Wang;Dengyong Zhou,sliao3@cs.toronto.edu;tingchen@cs.ucla.edu;tianlin@google.com;dennyzhou@google.com;chongw@google.com,6;4;7,3;3;3,Reject,0,4,0,yes,9/27/18,"Department of Computer Science, University of Toronto;University of California, Los Angeles;Google;Google;Google",18;20;-1;-1;-1,22;15;-1;-1;-1,,9/27/18,0,0,0,0,0,0,270;3654;489;18144;65,11;154;53;1045;10,3;29;11;55;4,43;322;36;1634;7,-1;-1
2449,ICLR,2019,LARGE BATCH SIZE TRAINING OF NEURAL NETWORKS WITH ADVERSARIAL TRAINING AND SECOND-ORDER INFORMATION,Zhewei Yao;Amir Gholami;Kurt Keutzer;Michael Mahoney,zheweiy@berkeley.edu;amirgh@berkeley.edu;keutzer@berkeley.edu;mmahoney@stat.berkeley.edu,7;4;4,4;5;4,Reject,0,7,1,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5,18;18;18;18,4;8,9/27/18,12,7,6,1,7,0,266;577;16722;11825,27;50;417;230,9;12;59;45,20;38;1581;1118,-1;-1
2450,ICLR,2019,Double Neural Counterfactual Regret Minimization,Hui Li;Kailiang Hu;Zhibang Ge;Tao Jiang;Yuan Qi;Le Song,ken.lh@antfin.com;hkl163251@antfin.com;zhibang.zg@antfin.com;lvshan.jt@antfin.com;yuan.qi@antfin.com;lsong@cc.gatech.edu,5;6;4,4;2;5,Reject,5,8,0,yes,9/25/19,Alibaba Group and Ant Financial Services Group;Antfin;Antfin;Antfin;Antfin;Georgia Institute of Technology,-1;-1;-1;-1;-1;13,-1;-1;-1;-1;-1;33,,12/27/18,9,5,3,0,4,2,14710;11;9;12260;6119;9290,1304;4;2;803;253;329,54;2;1;50;32;52,938;2;2;1063;659;1102,u;m
2451,ICLR,2019,Escaping Flat Areas via Function-Preserving Structural Network Modifications,Yannic Kilcher;Gary Bécigneul;Thomas Hofmann,yannic.kilcher@inf.ethz.ch;garybecigneul06@gmail.com;thomas.hofmann@inf.ethz.ch,6;6;4,4;3;4,Reject,0,2,0,yes,9/27/18,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10,10;10;10,,9/27/18,0,0,0,0,0,0,113;234;22675,14;17;173,4;5;52,26;47;3389,-1;-1
2452,ICLR,2019,DEFactor: Differentiable Edge Factorization-based Probabilistic Graph Generation,Rim Assouel;Mohamed Ahmed;Marwin Segler;Amir Saffari;Yoshua Bengio,rim.assouel@hotmail.fr;mohamed.ahmed@benevolent.ai;marwin.segler@benevolent.ai;amir.saffari@benevolent.ai;yoshua.bengio@mila.quebec,4;3;5,4;5;3,Reject,4,4,0,yes,9/27/18,University of Montreal;BenevolentAI;BenevolentAI;BenevolentAI;University of Montreal,123;-1;-1;-1;123,108;-1;-1;-1;108,5;10,9/27/18,14,5,3,0,4,1,14;2061;1125;3924;201719,3;70;23;44;807,1;14;12;23;147,1;232;37;708;23989,-1;-1
2453,ICLR,2019,Neural Random Projections for Language Modelling,Davide Nunes;Luis Antunes,nunesd@campus.ul.pt;xarax@ciencias.ulisboa.pt,3;4;3,4;3;4,Reject,0,7,0,yes,9/27/18,"Faculdade de Ciências, Universidade de Lisboa, Portugal;University of Lisbon",99;99,509;509,3,7/2/18,2,0,1,0,12,0,17;268,20;84,2;10,2;17,-1;-1
2454,ICLR,2019,NATTACK: A STRONG AND UNIVERSAL GAUSSIAN BLACK-BOX ADVERSARIAL ATTACK,Yandong Li;Lijun Li;Liqiang Wang;Tong Zhang;Boqing Gong,lyndon.leeseu@outlook.com;lilijun1990@buaa.edu.cn;lwang@cs.ucf.edu;bradymzhang@tencent.com;boqinggo@outlook.com,7;4;4,3;3;5,Reject,4,25,0,yes,9/27/18,University of Central Florida;Beihang University;University of Central Florida;Tencent AI Lab;International Computer Science Institute,78;115;78;-1;-1,1103;658;1103;-1;-1,4,9/27/18,3,1,2,0,0,2,239;27;1798;16999;3772,34;37;142;184;85,9;2;24;66;24,12;2;162;2239;751,-1;-1
2455,ICLR,2019,Improved resistance of neural networks to adversarial images through generative pre-training,Joachim Wabnig,joachim.wabnig@nokia-bell-labs.com,4;4;6,4;3;4,Reject,0,6,0,yes,9/27/18,Nokia Bell Labs,-1,-1,5;4,9/27/18,0,0,0,0,0,0,149,22,6,5,-1
2456,ICLR,2019,Select Via Proxy: Efficient Data Selection For Training Deep Networks,Cody Coleman;Stephen Mussmann;Baharan Mirzasoleiman;Peter Bailis;Percy Liang;Jure Leskovec;Matei Zaharia,cody@cs.stanford.edu;mussmann@stanford.edu;baharanm@stanford.edu;pbailis@stanford.edu;pliang@cs.stanford.edu;jure@cs.stanford.edu;mzaharia@stanford.edu,4;4;5,4;2;4,Reject,2,5,0,yes,9/27/18,Stanford University;Stanford University;Stanford University;Stanford University;Stanford University;Stanford University;Stanford University,4;4;4;4;4;4;4,3;3;3;3;3;3;3,3,9/27/18,8,5,1,0,0,0,531;115;924;2185;12588;46911;28136,16;14;28;120;144;298;134,9;7;12;24;47;92;37,52;6;141;227;2043;6012;3457,-1;-1
2457,ICLR,2019,Generalized Capsule Networks with Trainable Routing Procedure,Zhenhua Chen;Chuhua Wang;Tiancong Zhao;David Crandall,chen478@iu.edu;cw234@iu.edu;tz11@iu.edu;djcran@iu.edu,4;5;3,5;3;5,Reject,0,6,0,yes,9/27/18,"Indiana University, Bloomington;Indiana University, Bloomington;Indiana University, Bloomington;Indiana University, Bloomington",72;72;72;72,117;117;117;117,5;4;8,8/27/18,13,7,5,0,6,2,1354;3;-1;5383,123;3;-1;176,17;1;-1;31,59;0;0;461,-1;-1
2458,ICLR,2019,Padam: Closing the Generalization Gap of Adaptive Gradient Methods in Training Deep Neural Networks,Jinghui Chen;Quanquan Gu,jc4zg@virginia.edu;qgu@cs.ucla.edu,6;6;9,4;4;3,Reject,0,5,0,yes,9/27/18,"University of Virginia;University of California, Los Angeles",65;20,113;15,9;8,6/18/18,46,22,15,0,32,10,378;4067,45;175,11;34,39;419,-1;-1
2459,ICLR,2019,Text Embeddings for Retrieval from a Large Knowledge Base,Tolgahan Cakaloglu;Christian Szegedy;Xiaowei Xu,txcakaloglu@ualr.edu;szegedy@google.com;xwxu@ualr.edu,3;5;3,4;4;5,Reject,0,0,0,yes,9/27/18,"University of Arkansas, Little Rock;Google;University of Arkansas, Little Rock",314;-1;314,585;-1;585,3,9/27/18,4,1,2,0,3,1,12;65456;7721,11;37;187,2;17;43,1;9131;485,-1;-1
2460,ICLR,2019,Combining Learned Representations for Combinatorial Optimization,Saavan Patel;Sayeef Salahuddin,saavan@berkeley.edu;sayeef@berkeley.edu,4;4;5,3;5;3,Reject,0,4,0,yes,9/27/18,University of California Berkeley;University of California Berkeley,5;5,18;18,,9/27/18,0,0,0,0,0,0,0;7920,8;280,0;39,0;328,-1;-1
2461,ICLR,2019,Intriguing Properties of Learned Representations,Amartya Sanyal;Varun Kanade;Philip H. Torr,amartya.sanyal@cs.ox.ac.uk;varunk@cs.ox.ac.uk;philip.torr@eng.ox.ac.uk,3;6;5,4;2;2,Reject,0,5,0,yes,9/27/18,University of Oxford;University of Oxford;University of Oxford,50;50;50,1;1;1,4;6,9/27/18,-1,-1,-1,-1,-1,-1,-1;-1;-1,-1;-1;-1,-1;-1;-1,0;0;0,-1;-1
2462,ICLR,2019,Predictive Local Smoothness for Stochastic Gradient Methods,Jun Li;Hongfu Liu;Bineng Zhong;Yue Wu;Yun Fu,junl.mldl@gmail.com;hongfuliu@brandeis.edu;bnzhong@gmail.com;wuyuebupt@gmail.com;yunfu@ece.neu.edu,2;3;2;4,4;3;5;5,Reject,0,0,0,yes,9/27/18,Massachusetts Institute of Technology;Brandeis University;Tsinghua University;;Northeastern University,2;314;8;-1;16,5;223;30;-1;839,1;9,5/23/18,2,2,0,0,7,0,22018;928;1929;2689;10556,2354;61;77;288;328,58;17;18;24;52,1450;71;323;173;1162,-1;-1
2463,ICLR,2019,Accelerating first order optimization algorithms,Ange tato;Roger nkambou,nyamen_tato.ange_adrienne@courrier.uqam.ca;nkambou.roger@uqam.ca,3;4;4,3;3;5,Reject,0,0,0,yes,9/27/18,université du Québec à Montreal;université du Québec à Montreal,123;123,108;108,,9/27/18,0,0,0,0,0,0,47;1600,21;209,4;20,1;96,-1;-1
2464,ICLR,2019,An Analysis of Composite Neural Network Performance from Function Composition Perspective,Ming-Chuan Yang;Meng Chang Chen,mingchuan@iis.sinica.edu.tw;mcc@iis.sinica.edu.tw,3;3;3,2;3;4,Reject,0,3,0,yes,9/27/18,Academia Sinica;Academia Sinica,-1;-1,-1;-1,1;10,9/27/18,1,1,0,0,0,1,97;59,29;24,6;4,13;4,-1;-1
2465,ICLR,2019,Complexity of Training ReLU Neural Networks,Digvijay Boob;Santanu S. Dey;Guanghui Lan,digvijaybb40@gatech.edu;santanu.dey@isye.gatech.edu;george.lan@isye.gatech.edu,3;5;4,5;5;3,Reject,0,0,0,yes,9/27/18,Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology,13;13;13,33;33;33,,9/27/18,18,5,0,1,10,0,59;1533;4472,10;122;90,4;22;27,3;142;614,-1;-1
2466,ICLR,2019,Backdrop: Stochastic Backpropagation,Siavash Golkar;Kyle Cranmer,siavash.golkar@gmail.com;kyle.cranmer@nyu.edu,5;5;3,3;3;3,Reject,0,0,0,yes,9/27/18,New York University;New York University,26;26,27;27,8,6/4/18,1,0,0,0,129,0,190;1112,19;58,7;16,5;75,-1;-1
2467,ICLR,2019,A preconditioned accelerated stochastic gradient descent algorithm,Alexandru Onose;Seyed Iman Mossavat;Henk-Jan H. Smilde,alexandru.onose@asml.com;iman.mossavat@asml.com;henk-jan.smilde@asml.com,4;4;5,3;5;3,Reject,0,0,0,yes,9/27/18,Asml;National University of Singapore;Asml,-1;16;-1,-1;22;-1,1,9/27/18,0,0,0,0,0,0,156;9;109,21;7;9,6;1;5,3;0;0,-1;-1
2468,ICLR,2019,Attentive Task-Agnostic Meta-Learning for Few-Shot Text Classification,Xiang Jiang;Mohammad Havaei;Gabriel Chartrand;Hassan Chouaib;Thomas Vincent;Andrew Jesson;Nicolas Chapados;Stan Matwin,xiang.jiang@dal.ca;mohammad@imagia.com;gabriel@imagia.com;hassan.chouaib@imagia.com;thomas.vincent@imagia.com;andrew.jesson@imagia.com;nic@imagia.com;stan@cs.dal.ca,5;5;7,4;3;3,Reject,0,0,0,yes,9/27/18,Dalhousie University;Imagia;Imagia;Imagia;Imagia;Imagia;Imagia;Dalhousie University,314;-1;-1;-1;-1;-1;-1;314,289;-1;-1;-1;-1;-1;-1;289,3;6;8,9/27/18,7,5,2,0,0,0,68;1534;874;72;634;94;479;7150,12;27;24;14;91;12;44;386,5;9;12;5;14;6;12;32,3;82;36;1;29;6;33;519,-1;-1
2469,ICLR,2019,Open Vocabulary Learning on Source Code with a Graph-Structured Cache,Milan Cvitkovic;Badal Singh;Anima Anandkumar,mcvitkov@caltech.edu;sbadal@amazon.com;anima@caltech.edu,4;4;6,4;4;5,Reject,0,13,0,yes,9/27/18,California Institute of Technology;Amazon;California Institute of Technology,140;-1;140,3;-1;3,3;10,9/27/18,8,2,5,0,4,1,45;146;5337,11;26;187,4;5;38,3;12;744,-1;-1
2470,ICLR,2019,The Natural Language Decathlon: Multitask Learning as Question Answering,Bryan McCann;Nitish Shirish Keskar;Caiming Xiong;Richard Socher,bmccann@salesforce.com;nkeskar@salesforce.com;cxiong@salesforce.com;rsocher@salesforce.com,5;5;3,3;4;4,Reject,2,18,0,yes,9/27/18,SalesForce.com;SalesForce.com;SalesForce.com;SalesForce.com,-1;-1;-1;-1,-1;-1;-1;-1,3;6,6/20/18,172,115,40,6,30,27,927;2147;6230;52392,73;28;156;180,10;13;31;49,136;331;1054;8829,-1;-1
2471,ICLR,2019,NEURAL MALWARE CONTROL WITH DEEP REINFORCEMENT LEARNING,Yu Wang;Jack W. Stokes;Mady Marinescu,yu.wang@yale.edu;jstokes@microsoft.com;mady@microsoft.com,5;4;5,2;3;2,Reject,0,3,0,yes,9/27/18,Yale University;Microsoft;Microsoft,62;-1;-1,12;-1;-1,4,9/27/18,0,0,0,0,0,0,1526;963;214,59;59;10,10;12;5,221;66;11,-1;-1
2472,ICLR,2019,Beyond Games: Bringing Exploration to Robots in Real-world,Deepak Pathak;Dhiraj Gandhi;Abhinav Gupta,pathak@berkeley.edu;dgandhi@andrew.cmu.edu;abhinavg@cs.cmu.edu,5;3;3,3;5;4,Reject,0,18,0,yes,9/27/18,University of California Berkeley;Carnegie Mellon University;Carnegie Mellon University,5;1;1,18;24;24,,9/27/18,1,0,1,0,0,0,4138;387;17325,40;20;233,14;7;64,546;22;1929,-1;-1
2473,ICLR,2019,Overcoming catastrophic forgetting through weight consolidation and long-term memory,Shixian Wen;Laurent Itti,shixianwen1993@gmail.com;itti@usc.edu,4;4;4,4;4;5,Reject,2,0,0,yes,9/27/18,University of Southern California;University of Southern California,30;30,66;66,4,5/18/18,7,3,0,1,11,0,19;26491,7;266,2;55,0;2973,-1;-1
2474,ICLR,2019,Localized random projections challenge benchmarks for bio-plausible deep learning,Bernd Illing;Wulfram Gerstner;Johanni Brea,bernd.illing@epfl.ch;wulfram.gerstner@epfl.ch;johanni.brea@epfl.ch,5;3;3,3;4;5,Reject,0,4,0,yes,9/27/18,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne,478;478;478,38;38;38,,9/27/18,1,0,1,0,0,0,81;16661;320,7;373;26,3;63;9,7;1708;30,-1;-1
2475,ICLR,2019,Learning Goal-Conditioned Value Functions with one-step Path rewards rather than Goal-Rewards,Vikas Dhiman;Shurjo Banerjee;Jeffrey M Siskind;Jason J Corso,dhiman@umich.edu;shurjo@umich.edu;qobi@purdue.edu;jjcorso@umich.edu,4;1;3,3;4;4,Reject,2,6,1,yes,9/27/18,University of Michigan;University of Michigan;Purdue University;University of Michigan,8;8;26;8,21;21;60;21,,9/27/18,0,0,0,0,0,0,83;20;3447;5764,20;8;114;231,5;2;27;32,0;0;268;584,-1;-1
2476,ICLR,2019,Unsupervised Hyper-alignment for Multilingual Word Embeddings,Jean Alaux;Edouard Grave;Marco Cuturi;Armand Joulin,jean.alaux--lorain@ens.fr;egrave@fb.com;marco.cuturi.cameto@gmail.com;ajoulin@fb.com,5;6;7,3;4;3,Accept (Poster),0,1,0,yes,9/27/18,Ecole Normale Superieure;Facebook;Google;Facebook,99;-1;-1;-1,603;-1;-1;-1,,9/27/18,25,19,8,3,2,6,28;7588;4457;10425,29;56;96;74,2;23;29;32,6;1132;578;1521,-1;-1
2477,ICLR,2019,Synthnet: Learning synthesizers end-to-end,Florin Schimbinschi;Christian Walder;Sarah Erfani;James Bailey,florinsch@student.unimelb.edu.au;christian.walder@data61.csiro.au;sarah.erfani@unimelb.edu.au;baileyj@unimelb.edu.au,4;4;3,3;4;5,Reject,0,21,0,yes,9/27/18,"The University of Melbourne;, CSIRO;The University of Melbourne;The University of Melbourne",123;-1;123;123,32;-1;32;32,5,9/27/18,0,0,0,0,0,0,45;272;898;5525,8;37;57;329,3;8;11;34,2;10;81;660,-1;-1
2478,ICLR,2019,Relational Graph Attention Networks,Dan Busbridge;Dane Sherburn;Pietro Cavallo;Nils Y. Hammerla,dan.busbridge@gmail.com;danesherbs@gmail.com;p.cavallo85@gmail.com;nils.hammerla@babylonhealth.com,4;4;4,5;4;5,Reject,6,4,0,yes,9/27/18,babylon health;babylon health;;babylon health,-1;-1;-1;-1,-1;-1;-1;-1,10,9/27/18,8,2,4,0,5,1,12;8;37;2297,4;1;28;43,2;1;4;19,1;1;3;216,-1;-1
2479,ICLR,2019,IEA: Inner Ensemble Average within a convolutional neural network,Abduallah Mohamed;Xinrui Hua;Xianda Zhou;Christian Claudel,abduallah.mohamed@utexas.edu;xinruihua@utexas.edu;xianda@utexas.edu;christian.claudel@utexas.edu,4;2;4,3;4;5,Reject,0,6,0,yes,9/27/18,"University of Texas, Austin;University of Texas, Austin;University of Texas, Austin;University of Texas, Austin",22;22;22;22,49;49;49;49,,8/30/18,1,0,1,0,2,0,11;-1;-1;856,7;-1;-1;94,2;-1;-1;14,2;0;0;55,-1;-1
2480,ICLR,2019,ChainGAN: A sequential approach to GANs,Safwan Hossain;Kiarash Jamali;Yuchen Li;Frank Rudzicz,safwan.hossain@mail.utoronto.ca;kiarash.jamali@mail.utoronto.ca;ychnlgy.li@utoronto.ca;frank@spoclab.com,4;4;4,4;4;4,Reject,0,0,0,yes,9/27/18,Toronto University;Toronto University;Toronto University;University of Toronto,18;18;18;18,22;22;22;22,5;4,9/27/18,1,1,0,0,2,0,12;3;812;1592,9;3;86;171,2;1;14;23,0;0;47;123,-1;-1
2481,ICLR,2019,Adaptive Convolutional Neural Networks,Julio Cesar Zamora;Jesus Adan Cruz Vargas;Omesh Tickoo,julio.c.zamora.esquivel@intel.com;jesus.a.cruz.vargas@intel.com;omesh.tickoo@intel.com,5;4;4,3;3;4,Reject,0,3,0,yes,9/27/18,Intel;Intel;Intel,-1;-1;-1,-1;-1;-1,,9/27/18,0,0,0,0,0,0,0;0;1139,1;1;57,0;0;15,0;0;98,-1;-1
2482,ICLR,2019,Explicit Recall for Efficient Exploration,Honghua Dong;Jiayuan Mao;Xinyue Cui;Lihong Li,dhh19951@gmail.com;maojiayuan@gmail.com;rogar2233cxy@gmail.com;lihongli.cs@gmail.com,7;4;3,3;4;4,Reject,0,6,0,yes,9/27/18,Tsinghua University;Tsinghua University;;Google,8;8;-1;-1,30;30;-1;-1,,9/27/18,2,1,1,0,0,0,63;473;45;10603,5;19;8;242,3;7;3;46,3;47;5;1190,-1;-1
2483,ICLR,2019,A Multi-modal one-class generative adversarial network for anomaly detection in manufacturing,Shuhui Qu;Janghwan Lee;Wei Xiong;Wonhyouk Jang;Jie Wang,shuhuiq@stanford.edu;jake.ee@samsung.com;w.xiong@samsung.com;damian.jang@samsung.com;jiewang@stanford.edu,3;4;5,4;5;4,Reject,0,0,0,yes,9/27/18,Stanford University;Samsung;Samsung;Samsung;Stanford University,4;-1;-1;-1;4,3;-1;-1;-1;3,5;4,9/27/18,0,0,0,0,0,0,244;32;76;6;1529,25;14;41;2;279,7;3;4;1;21,25;4;2;0;43,-1;-1
2484,ICLR,2019,Learning to encode spatial relations from natural language,Tiago Ramalho;Tomas Kocisky‎;Frederic Besse;S. M. Ali Eslami;Gabor Melis;Fabio Viola;Phil Blunsom;Karl Moritz Hermann,tiago.mpramalho@gmail.com;tkocisky@google.com;fbesse@google.com;aeslami@google.com;melisgl@google.com;fviola@google.com;pblunsom@google.com;kmh@google.com,6;5;5,5;4;4,Reject,0,3,0,yes,9/27/18,Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1,3,9/27/18,0,0,0,0,0,0,5;2729;208;4034;605;190;11400;5082,5;16;5;38;14;6;143;41,1;10;2;19;8;1;47;21,0;413;34;514;99;32;1326;686,-1;-1
2485,ICLR,2019,D-GAN: Divergent generative adversarial network for positive unlabeled learning and counter-examples generation,Florent CHIARONI. Mohamed-Cherif RAHAL. Nicolas HUEBER. Frédéric DUFAUX.,florent.chiaroni@vedecom.fr;mohamed.rahal@vedecom.fr;nicolas.hueber@isl.eu;frederic.dufaux@l2s.centralesupelec.fr,3;5;3,4;1;5,Reject,1,11,0,yes,9/27/18,CentraleSupelec;;;CentraleSupelec,478;-1;-1;478,452;-1;-1;452,5;4,9/27/18,0,0,0,0,0,0,0,1,0,0,-1
2486,ICLR,2019,Assessing Generalization in Deep Reinforcement Learning,Charles Packer*;Katelyn Gao*;Jernej Kos;Philipp Krahenbuhl;Vladlen Koltun;Dawn Song,cpacker@berkeley.edu;katelyn.gao@intel.com;jernej@kos.mx;philkr@cs.utexas.edu;vladlen.koltun@intel.com;dawnsong@berkeley.edu,5;3;5,2;5;3,Reject,0,5,0,yes,9/27/18,"University of California Berkeley;Intel;National University of Singapore;University of Texas, Austin;Intel;University of California Berkeley",5;-1;16;22;-1;5,18;-1;22;49;-1;18,8,9/27/18,44,34,4,3,12,7,138;63;584;8737;17569;36766,11;11;23;43;189;276,4;3;8;26;62;95,19;9;53;1544;2500;4089,-1;-1
2487,ICLR,2019,Where and when to look? Spatial-temporal attention for action recognition in videos,Lili Meng;Bo Zhao;Bo Chang;Gao Huang;Frederick Tung;Leonid Sigal,lilimeng1103@gmail.com;bzhao03@cs.ubc.ca;bchang@stat.ubc.ca;gh349@cornell.edu;ftung@sfu.ca;lsigal@cs.ubc.ca,6;6;3,4;4;5,Reject,0,8,0,yes,9/27/18,University of British Columbia;University of British Columbia;University of British Columbia;Cornell University;Simon Fraser University;University of British Columbia,36;36;36;7;62;36,34;34;34;19;253;34,,9/27/18,4,0,1,0,4,0,473;1741;421;12231;701;6616,53;202;56;60;65;166,12;20;8;22;14;39,34;119;42;2056;48;666,-1;-1
2488,ICLR,2019,Integrated Steganography and Steganalysis with Generative Adversarial Networks,Chong Yu,dxxzdxxz@126.com,5;6;5,5;4;2,Reject,0,2,0,yes,9/27/18,NVIDIA,-1,-1,5;4;2,9/27/18,1,0,0,0,0,0,65,16,6,2,-1
2489,ICLR,2019,Heated-Up Softmax Embedding,Xu Zhang;Felix Xinnan Yu;Svebor Karaman;Wei Zhang;Shih-Fu Chang,xu.zhang@columbia.edu;felixyu@google.com;svebor.karaman@gmail.com;wz2363@columbia.edu;sc250@columbia.edu,8;3;5,4;5;4,Reject,0,3,0,yes,9/27/18,Columbia University;Google;Columbia University;Columbia University;Columbia University,15;-1;15;15;15,14;-1;14;14;14,8,9/11/18,11,7,4,0,11,1,2253;2673;480;43288;31245,303;66;46;4567;584,22;25;12;77;86,136;312;28;2377;3169,-1;-1
2490,ICLR,2019,"S3TA: A Soft, Spatial, Sequential, Top-Down Attention Model",Alex Mott;Daniel Zoran;Mike Chrzanowski;Daan Wierstra;Danilo J. Rezende,alexmott@google.com;danielzoran@google.com;chrzanowskim@google.com;wierstra@google.com;danilor@google.com,5;5;5,4;4;4,Reject,0,4,0,yes,9/27/18,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,,9/27/18,2,2,2,0,0,0,75;1867;1820;27095;8628,10;29;17;64;63,3;16;7;40;27,3;318;194;4733;1117,-1;-1
2491,ICLR,2019,ON THE USE OF CONVOLUTIONAL AUTO-ENCODER FOR INCREMENTAL CLASSIFIER LEARNING IN CONTEXT AWARE ADVERTISEMENT,Tin Lay Nwe;Shudong Xie;Balaji Nataraj;Yiqun Li;Joo-Hwee Lim,tlnma@i2r.a-star.edu.sg;xie_shudong@i2r.a-star.edu.sg;e0267605@u.nus.edu;yqli@i2r.a-star.edu.sg;joohwee@i2r.a-star.edu.sg,5;4;3,5;4;4,Reject,0,0,0,yes,9/27/18,A*STAR;A*STAR;National University of Singapore;A*STAR;A*STAR,-1;-1;16;-1;-1,-1;-1;22;-1;-1,,9/27/18,0,0,0,0,0,0,788;4;57;406;2359,54;8;7;44;244,15;2;3;10;23,48;0;3;23;140,-1;-1
2492,ICLR,2019,"Look Ma, No GANs! Image Transformation with ModifAE",Chad Atalla;Bartholomew Tam;Amanda Song;Gary Cottrell,chada@ucsd.edu;b4tam@ucsd.edu,3;4;5,4;4;3,Reject,0,0,0,yes,9/27/18,"University of California, San Diego;University of California, San Diego",11;11,31;31,5,9/27/18,1,1,0,0,0,0,8;1;46;18,6;2;14;12,2;1;3;2,0;0;3;4,-1;-1
2493,ICLR,2019,Object detection deep learning networks for Optical Character Recognition,Christopher Bourez;Aurelien Coquard,christopher.bourez@gmail.com;acq@ivalua.com,2;1;2;1,5;5;5;5,Reject,0,0,0,yes,9/27/18,;Ivalua Inc,-1;-1,-1;-1,2,9/27/18,0,0,0,0,0,0,0;65,1;3,0;1,0;1,-1;-1
2494,ICLR,2019,Learning to Drive by Observing the Best and Synthesizing the Worst,Mayank Bansal;Alex Krizhevsky;Abhijit Ogale,mayban@waymo.com;akrizhevsky@gmail.com;ogale@waymo.com,3;6;5,4;4;4,Reject,0,8,0,yes,9/27/18,Waymo;;Waymo,-1;-1;-1,-1;-1;-1,,9/27/18,133,91,41,6,0,17,511;79674;1256,46;19;37,11;16;13,47;12120;90,-1;-1
2495,ICLR,2019,An adaptive homeostatic algorithm for the unsupervised learning of visual features,Victor Boutin;Angelo Franciosini;Laurent Perrinet,victor.boutin@univ-amu.fr;angelo.franciosini@univ-amu.fr;laurent.perrinet@univ-amu.fr,5;4;9,5;4;4,Reject,0,5,0,yes,9/27/18,Aix Marseille Univ;Aix Marseille Univ;Aix Marseille Univ,478;478;478,297;297;297,,9/27/18,2,0,0,0,0,0,11;10;1382,9;11;68,2;2;17,0;0;123,-1;-1
2496,ICLR,2019,Efficient Exploration through Bayesian Deep Q-Networks,Kamyar Azizzadenesheli;Animashree Anandkumar,kazizzad@uci.edu;anima@caltech.edu,4;6;2;4,2;2;5;4,Reject,2,16,3,yes,9/27/18,"University of California, Irvine;California Institute of Technology",35;140,99;3,11;1,2/13/18,57,28,24,1,35,6,675;5337,38;187,10;38,96;744,-1;-1
2497,ICLR,2019,Radial Basis Feature Transformation to Arm CNNs Against Adversarial Attacks,Saeid Asgari Taghanaki;Shekoofeh Azizi;Ghassan Hamarneh,sasgarit@sfu.ca;shazizi@ece.ubc.ca;hamarneh@sfu.ca,4;4;3,4;3;4,Reject,0,0,0,yes,9/27/18,Simon Fraser University;University of British Columbia;Simon Fraser University,62;36;62,253;34;253,4;2,9/27/18,0,0,0,0,0,0,89;12;5236,15;6;369,6;2;35,3;1;334,-1;-1
2498,ICLR,2019,Machine Translation With Weakly Paired Bilingual Documents,Lijun Wu;Jinhua Zhu;Di He;Fei Gao;Xu Tan;Tao Qin;Tie-Yan Liu,wulijun3@mail2.sysu.edu.cn;teslazhu@mail.ustc.edu.cn;di_he@pku.edu.cn;feiga@microsoft.com;xuta@microsoft.com;taoqin@microsoft.com;tyliu@microsoft.com,7;6;5,5;3;5,Reject,1,14,0,yes,9/27/18,SUN YAT-SEN UNIVERSITY;University of Science and Technology of China;Peking University;Microsoft;Microsoft;Microsoft;Microsoft,478;478;24;-1;-1;-1;-1,352;132;27;-1;-1;-1;-1,3,9/27/18,1,0,0,0,0,0,454;326;2539;5534;4868;4895;13277,23;38;258;507;154;289;366,8;9;27;38;24;33;51,44;16;109;321;429;587;1718,-1;-1
2499,ICLR,2019,DVOLVER: Efficient Pareto-Optimal Neural Network Architecture Search,Guillaume Michel;Mohammed Amine Alaoui;Alice Lebois;Amal Feriani;Mehdi Felhi,guillaume.michel@netatmo.com;mohammed-amine.alaoui@netatmo.com;alice.lebois@netatmo.com;amal.feriani@netatmo.com;mehdi.felhi@netatmo.com,4;5;4,4;3;4,Reject,0,3,0,yes,9/27/18,Netatmo;Netatmo;Netatmo;Georgia Institute of Technology;Netatmo,-1;-1;-1;13;-1,-1;-1;-1;33;-1,,9/27/18,5,1,2,0,0,0,304;4;22;8;17,13;1;14;2;11,8;1;3;2;3,9;0;0;0;1,-1;-1
2500,ICLR,2019,Complementary-label learning for arbitrary losses and models,Takashi Ishida;Gang Niu;Aditya Krishna Menon;Masashi Sugiyama,ishida@ms.k.u-tokyo.ac.jp;gang.niu@riken.jp;aditya.menon@anu.edu.au;sugi@k.u-tokyo.ac.jp,5;5;6,3;4;4,Reject,0,3,0,yes,9/27/18,The University of Tokyo;RIKEN;Australian National University;The University of Tokyo,54;-1;106;54,45;-1;48;45,,9/27/18,10,6,3,2,2,2,2407;1149;2297;11582,266;78;77;712,24;16;23;52,164;144;285;1299,-1;-1
2501,ICLR,2019,Feature Transformers: A Unified Representation Learning Framework for Lifelong Learning,Hariharan Ravishankar;Rahul Venkataramani;Saihareesh Anamandra;Prasad Sudhakar,hariharan.ravishankar@ge.com;rahul.venkataramani@ge.com;saihareesh.anamandra@ge.com;prasad.sudhakar@ge.com,4;3;4,3;4;5,Reject,0,4,0,yes,9/27/18,General Electric;General Electric;General Electric;General Electric,-1;-1;-1;-1,-1;-1;-1;-1,,9/27/18,0,0,0,0,0,0,174;162;4;192,19;10;4;30,5;4;1;6,6;8;0;7,-1;-1
2502,ICLR,2019,Functional Bayesian Neural Networks for Model Uncertainty Quantification,Nanyang Ye;Zhanxing Zhu,yn272@cam.ac.uk;zhanxing.zhu@pku.edu.cn,3;4;5,3;4;2,Reject,0,0,0,yes,9/27/18,University of Cambridge;Peking University,71;24,2;27,11,9/27/18,1,1,1,0,0,1,46;861,17;80,4;14,2;107,-1;-1
2503,ICLR,2019,Learning shared manifold representation of images and attributes for generalized zero-shot learning,Masahiro Suzuki;Yusuke Iwasawa;Yutaka Matsuo,masa@weblab.t.u-tokyo.ac.jp;iwasawa@weblab.t.u-tokyo.ac.jp;matsuo@weblab.t.u-tokyo.ac.jp,5;5;4,4;4;5,Reject,0,6,0,yes,9/27/18,The University of Tokyo;The University of Tokyo;The University of Tokyo,54;54;54,45;45;45,5;6,9/27/18,0,0,0,0,0,0,761;114;3493,316;39;492,12;6;28,77;10;193,-1;-1
2504,ICLR,2019,Continual Learning via Explicit Structure Learning,Xilai Li;Yingbo Zhou;Tianfu Wu;Richard Socher;Caiming Xiong,xli47@ncsu.edu;yingbo.zhou@salesforce.com;tianfu_wu@ncsu.edu;rsocher@salesforce.com;cxiong@salesforce.com,4;4;4,4;4;5,Reject,0,8,0,yes,9/27/18,North Carolina State University;SalesForce.com;North Carolina State University;SalesForce.com;SalesForce.com,89;-1;89;-1;-1,275;-1;275;-1;-1,,9/27/18,0,0,0,0,0,0,225;1523;1125;52392;6230,18;48;90;180;156,6;16;17;49;31,21;213;54;8829;1054,-1;-1
2505,ICLR,2019,In Your Pace: Learning the Right Example at the Right Time,Guy Hacohen;Daphna Weinshall,guy.hacohen@mail.huji.ac.il;daphna@cs.huji.ac.il,5;4;4,4;4;4,Reject,3,1,0,yes,9/27/18,Hebrew University of Jerusalem;Hebrew University of Jerusalem,65;65,205;205,6,9/27/18,0,0,0,0,0,0,29;5081,4;215,1;35,3;542,-1;-1
2506,ICLR,2019,Polar Prototype Networks,Pascal Mettes;Elise van der Pol;Cees G. M. Snoek,p.s.m.mettes@uva.nl;e.e.vanderpol@uva.nl;cgmsnoek@uva.nl,5;3;4,3;4;5,Reject,0,4,0,yes,9/27/18,University of Amsterdam;University of Amsterdam;University of Amsterdam,169;169;169,59;59;59,,9/27/18,0,0,0,0,0,0,402;157;9868,32;15;261,10;6;44,40;22;866,-1;-1
2507,ICLR,2019,Meta-Learning for Contextual Bandit Exploration,Amr Sharaf;Hal Daumé III,amr@cs.umd.edu;hal@umiacs.umd.edu,7;6;3,4;4;4,Reject,0,3,0,yes,9/27/18,"University of Maryland, College Park;University of Maryland, College Park",12;12,69;69,6,9/27/18,6,2,1,0,3,0,70;10548,20;198,4;46,3;1073,m;m
2508,ICLR,2019,FEED: Feature-level Ensemble Effect for knowledge Distillation,SeongUk Park;Nojun Kwak,swpark0703@snu.ac.kr;nojunk@snu.ac.kr,5;4;4,3;3;4,Reject,0,5,0,yes,9/27/18,Seoul National University;Seoul National University,41;41,74;74,,9/27/18,3,1,2,0,0,1,38;2517,5;135,3;20,8;296,-1;-1
2509,ICLR,2019,Like What You Like: Knowledge Distill via Neuron Selectivity Transfer,Zehao Huang;Naiyan Wang,zehaohuang18@gmail.com;winsty@gmail.com,4;4;6,4;4;5,Reject,2,0,0,yes,9/27/18,;,-1;-1,-1;-1,2,7/5/17,69,42,25,0,13,16,358;6044,17;44,7;23,60;848,-1;-1
2510,ICLR,2019,Activity Regularization for Continual Learning,Quang H. Pham;Steven C. H. Hoi,hqpham.2017@smu.edu.sg;chhoi@smu.edu.sg,4;4;4,5;5;4,Reject,0,0,0,yes,9/27/18,Singapore Management University;Singapore Management University,89;89,1103;1103,,9/27/18,0,0,0,0,0,0,4;8811,4;295,1;49,0;841,-1;-1
2511,ICLR,2019,Empirical Study of Easy and Hard Examples in CNN Training,Ikki Kishida;Hideki Nakayama,kishida@nlab.ci.i.u-tokyo.ac.jp;nakayama@nlab.ci.i.u-tokyo.ac.jp,3;4;3,4;5;4,Reject,0,0,0,yes,9/27/18,The University of Tokyo;The University of Tokyo,54;54,45;45,8,9/27/18,0,0,0,0,0,0,0;625,2;87,0;15,0;67,-1;-1
2512,ICLR,2019,Improving machine classification using human uncertainty measurements,Ruairidh M. Battleday;Joshua C. Peterson;Thomas L. Griffiths,ruairidh.battleday@gmail.com;peterson.c.joshua@gmail.com;tomg@princeton.edu,6;3;3,4;5;2,Reject,3,0,0,yes,9/27/18,Princeton University;University of California Berkeley;Princeton University,30;5;30,7;18;7,4;8,9/27/18,0,0,0,0,0,0,133;264;21330,13;30;439,5;9;70,5;18;2169,-1;-1
2513,ICLR,2019,Policy Optimization via Stochastic Recursive Gradient Algorithm,Huizhuo Yuan;Chris Junchi Li;Yuhao Tang;Yuren Zhou,yuanhz@pku.edu.cn;junchi.li.duke@gmail.com;yuhaotang97@gmail.com;yuren.zhou@duke.edu,5;6;5,3;2;3,Reject,0,1,0,yes,9/27/18,Peking University;Tencent AI Lab;University of Nottingham;Duke University,24;-1;228;44,27;-1;146;17,,9/27/18,3,1,2,0,0,1,15;250;67;1435,10;26;12;127,3;7;5;19,3;59;1;133,-1;-1
2514,ICLR,2019,Discriminative Active Learning,Daniel Gissin;Shai Shalev-Shwartz,daniel.gissin@mail.huji.ac.il;shais@cs.huji.ac.il,6;8;4,4;4;4,Reject,0,15,0,yes,9/27/18,Hebrew University of Jerusalem;Hebrew University of Jerusalem,65;65,205;205,,9/27/18,17,13,4,2,0,3,19;46,2;23,2;3,3;8,-1;-1
2515,ICLR,2019,Robustness and Equivariance of Neural Networks,Amit Deshpande;Sandesh Kamath;K.V.Subrahmanyam,amitdesh@microsoft.com;ksandeshk@cmi.ac.in;kv@cmi.ac.in,3;4;5,5;4;3,Reject,0,3,0,yes,9/27/18,Microsoft;Chennai Mathematical Institute;Chennai Mathematical Institute,-1;478;478,-1;1103;1103,4,9/27/18,0,0,0,0,0,0,280;17;9,25;16;10,7;2;2,22;2;0,-1;-1
2516,ICLR,2019,Knowledge Distillation from Few Samples,Tianhong Li;Jianguo Li;Zhuang Liu;Changshui Zhang,tianhong@mit.edu;jianguo.li@intel.com;zhuangl@berkeley.edu;zcs@mail.tsinghua.edu.cn,4;6;6,4;4;3,Reject,2,12,0,yes,9/27/18,Massachusetts Institute of Technology;Intel;University of California Berkeley;Tsinghua University,2;-1;5;8,5;-1;18;30,1,9/27/18,7,3,2,0,0,0,2452;4613;77;10358,119;393;20;385,29;30;4;53,124;391;10;1175,-1;-1
2517,ICLR,2019,An Automatic Operation Batching Strategy for the Backward Propagation of Neural Networks Having Dynamic Computation Graphs,Yuchen Qiao;Kenjiro Taura,qiao@eidos.ic.i.u-tokyo.ac.jp;tau@eidos.ic.i.u-tokyo.ac.jp,5;6;4,3;5;4,Reject,0,1,0,yes,9/27/18,The University of Tokyo;The University of Tokyo,54;54,45;45,10,9/27/18,0,0,0,0,0,0,5;1503,14;151,1;22,0;129,-1;-1
2518,ICLR,2019,Graph Learning Network: A Structure Learning Algorithm,Darwin Danilo Saire Pilco;Adín Ramírez Rivera,darwin.pilco@ic.unicamp.br;adin@ic.unicamp.br,4;3;4,5;4;4,Reject,0,4,0,yes,9/27/18,University of Campinas;University of Campinas,386;386,441;441,10,9/27/18,1,0,1,0,0,0,1;601,2;24,1;9,0;68,-1;-1
2519,ICLR,2019,Remember and Forget for Experience Replay,Guido Novati;Petros Koumoutsakos,novatig@ethz.ch;petros@ethz.ch,7;6;6,3;3;3,Reject,0,6,0,yes,9/27/18,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10,10;10,,7/16/18,12,1,6,1,4,1,132;10772,21;384,7;52,2;887,-1;-1
2520,ICLR,2019,Probabilistic Model-Based Dynamic Architecture Search,Nozomu Yoshinari;Kento Uchida;Shota Saito;Shinichi Shirakawa;Youhei Akimoto,yoshinari-nozomu-ry@ynu.jp;uchida-kento-nc@ynu.jp;saito-shota-bt@ynu.jp;shirakawa-shinichi-bg@ynu.ac.jp;akimoto@cs.tsukuba.ac.jp,5;6;5,4;4;4,Reject,0,4,0,yes,9/27/18,Yokohama National University;Yokohama National University;Yokohama National University;Yokohama National University;University of Tsukuba,478;478;478;478;478,827;827;827;827;468,2,9/27/18,0,0,0,0,0,0,18;609;92;466;463,3;32;10;53;86,1;7;3;10;13,4;20;2;45;38,-1;-1
2521,ICLR,2019,Talk The Walk: Navigating Grids in New York City through Grounded Dialogue,Harm de Vries;Kurt Shuster;Dhruv Batra;Devi Parikh;Jason Weston;Douwe Kiela,mail@harmdevries.com;kshuster@fb.com;dbatra@gatech.edu;parikh@gatech.edu;jase@fb.com;dkiela@fb.com,6;7;4,4;4;3,Reject,2,0,3,yes,9/27/18,University of Montreal;Facebook;Georgia Institute of Technology;Georgia Institute of Technology;Facebook;Facebook,123;-1;13;13;-1;-1,108;-1;33;33;-1;-1,3,7/9/18,54,33,8,0,16,2,2969;300;1823;14973;44559;3399,31;16;18;185;243;79,16;7;5;54;76;29,300;49;436;2371;5821;580,-1;-1
2522,ICLR,2019,Large-scale classification of structured objects using a CRF with deep class embedding,Eran Goldman;Jacob Goldberger,eg4000@gmail.com;jacob.goldberger@biu.ac.il,4;3;3,4;3;5,Reject,0,0,0,yes,9/27/18,;Bar Ilan University,-1;95,-1;456,,5/21/17,1,0,0,0,3,0,28;5191,4;180,2;35,6;547,-1;-1
2523,ICLR,2019,Deep Ensemble Bayesian Active Learning : Adressing the Mode Collapse issue in Monte Carlo dropout via Ensembles,Remus Pop;Patric Fulop,remus.p.pop@gmail.com;patric.fulop@ed.ac.uk,4;4;5,4;4;4,Reject,0,9,0,yes,9/27/18,;University of Edinburgh,-1;33,-1;27,11,9/27/18,5,3,0,0,4,0,8;5,5;3,2;1,0;0,-1;-1
2524,ICLR,2019,MixFeat: Mix Feature in Latent Space Learns Discriminative Space,Yoichi Yaguchi;Fumiyuki Shiratani;Hidekazu Iwaki,yoichi_yaguchi@ot.olympus.co.jp;f_shiratani@ot.olympus.co.jp;h_iwaki@ot.olympus.co.jp,6;4;4,4;3;4,Reject,0,10,0,yes,9/27/18,Olympus Corporation;Olympus Corporation;Olympus Corporation,-1;-1;-1,-1;-1;-1,,9/27/18,3,0,1,0,0,0,161;1;75,11;1;21,3;1;4,7;0;6,-1;-1
2525,ICLR,2019,Question Generation using a Scratchpad Encoder,Ryan Y Benmalek;Madian Khabsa;Suma Desu;Claire Cardie;Michele Banko,ryanai3@cs.cornell.edu;me@madiankhabsa.com;desuma24@gmail.com;cardie@cs.cornell.edu;mbanko@apple.com,4;3;4,4;5;5,Reject,0,3,0,yes,9/27/18,Cornell University;Madiankhabsa;;Cornell University;Apple,7;-1;-1;7;-1,19;-1;-1;19;-1,3,9/27/18,0,0,0,0,0,0,3;850;319;14362;4374,4;55;14;226;33,1;15;6;55;18,0;64;12;1463;503,-1;-1
2526,ICLR,2019,Interpreting Adversarial Robustness: A View from Decision Surface in Input Space,Fuxun Yu;Chenchen Liu;Yanzhi Wang;Xiang Chen,fyu2@gmu.edu;chliu@clarkson.edu;yanz.wang@northeastern.edu;xchen26@gmu.com,3;6;5,5;4;5,Reject,1,8,1,yes,9/27/18,George Mason University;Clarkson University;Northeastern University;Gmu,99;478;16;99,336;1103;839;336,4;8,9/27/18,5,3,0,0,13,1,445;358;3484;13538,49;55;262;574,11;9;28;53,23;28;221;831,-1;-1
2527,ICLR,2019,Sequenced-Replacement Sampling for Deep Learning,Chiu Man Ho;Dae Hoon Park;Wei Yang;Yi Chang,chiuman100@gmail.com;pdhvip@gmail.com;wei.yang2@huawei.com;yichang@acm.org,3;5;4,4;5;4,Reject,0,0,0,yes,9/27/18,;Huawei Technologies Ltd.;Huawei Technologies Ltd.;,-1;-1;-1;-1,-1;-1;-1;-1,8,9/27/18,20,0,0,0,20,0,309;326;-1;4343,59;36;-1;269,10;10;-1;33,5;19;0;363,-1;-1
2528,ICLR,2019,Representation-Constrained Autoencoders and an Application to Wireless Positioning,Pengzhi Huang;Emre Gonultas;Said Medjkouh;Oscar Castaneda;Olav Tirkkonen;Tom Goldstein;Christoph Studer,ph448@cornell.edu;eg566@cornell.edu;sm2685@cornell.edu;oc66@cornell.edu;olav.tirkkonen@aalto.fi;tomg@cs.umd.edu;studer@cornell.edu,5;4;6,4;4;2,Reject,0,4,0,yes,9/27/18,"Cornell University;Cornell University;Cornell University;Cornell University;Aalto University;University of Maryland, College Park;Cornell University",7;7;7;7;140;12;7,19;19;19;19;190;69;19,,9/27/18,1,0,1,0,0,1,6;1;11;58;3765;5769;5301,2;2;5;6;259;98;193,1;1;2;2;27;27;38,1;0;2;9;283;724;641,-1;-1
2529,ICLR,2019,Learn From Neighbour: A Curriculum That Train Low Weighted Samples By Imitating,Benyuan Sun;Yizhou Wang,sunbenyuan@pku.edu.cn;yizhou.wang@pku.edu.cn,2;3;4,5;3;4,Reject,0,0,0,yes,9/27/18,Peking University;Peking University,24;24,27;27,,9/27/18,0,0,0,0,0,0,204;807,16;47,5;16,8;86,-1;-1
2530,ICLR,2019,On the Statistical and Information Theoretical Characteristics of DNN Representations,Daeyoung Choi;Wonjong Rhee;Kyungeun Lee;Changho Shin,choid@snu.ac.kr;wrhee@snu.ac.kr;ruddms0415@snu.ac.kr;chshin@encoredtech.com,5;4;3,3;4;3,Reject,0,8,0,yes,9/27/18,Seoul National University;Seoul National University;Seoul National University;Encoredtech,41;41;41;-1,74;74;74;-1,8,9/27/18,0,0,0,0,0,0,27;2334;219;121,11;57;66;42,2;14;7;6,5;273;15;8,-1;-1
2531,ICLR,2019,Trajectory VAE for multi-modal imitation,Xiaoyu Lu;Jan Stuehmer;Katja Hofmann,xiaoyu.lu@stats.ox.ac.uk;t-jastuh@microsoft.com;katja.hofmann@microsoft.com,4;4;4,4;4;4,Reject,0,3,0,yes,9/27/18,University of Oxford;Microsoft;Microsoft,50;-1;-1,1;-1;-1,5,9/27/18,0,0,0,0,0,0,7;432;1774,8;18;112,1;8;24,1;17;150,-1;-1
2532,ICLR,2019,SIMILE: Introducing Sequential Information towards More Effective Imitation Learning,Yutong Bai;Lingxi Xie,ytongbai@gmail.com;198808xc@gmail.com,6;4;4,3;5;4,Reject,0,3,0,yes,9/27/18,;Huawei Technologies Ltd.,-1;-1,-1;-1,,9/27/18,0,0,0,0,0,0,25;2140,9;106,3;23,4;223,-1;-1
2533,ICLR,2019,A   RECURRENT NEURAL CASCADE-BASED MODEL FOR CONTINUOUS-TIME DIFFUSION PROCESS,Sylvain Lamprier,sylvain.lamprier@lip6.fr,7;4;4,4;4;4,Reject,0,4,0,yes,9/27/18,LIP6,-1,-1,10,9/27/18,3,1,0,0,2,0,251,57,7,35,-1
2534,ICLR,2019,Multi-task Learning with Gradient Communication,Pengfei Liu;Xuanjing Huang,pfliu14@fudan.edu.cn;xjhuang@fudan.edu.cn,5;4;7,4;4;3,Reject,0,3,0,yes,9/27/18,Fudan University;Fudan University,78;78,116;116,,9/27/18,0,0,0,0,0,0,2131;3715,189;223,20;30,229;437,-1;-1
2535,ICLR,2019,Improving Composition of Sentence Embeddings through the Lens of Statistical Relational Learning,Damien Sileo;Tim Van de Cruys;Camille Pradel;Philippe Muller,damien.sileo@synapse-fr.com;tim.van-de-cruys@irit.fr;camille.pradel@synapse-fr.com;philippe.muller@irit.fr,5;5;6,3;3;4,Reject,0,4,0,yes,9/27/18,"Synapse-fr;IRIT, University of Toulouse;Synapse-fr;IRIT, University of Toulouse",-1;-1;-1;-1,-1;-1;-1;-1,3,9/27/18,0,0,0,0,0,0,11;577;123;98,9;60;30;22,2;13;8;5,3;30;7;7,-1;-1
2536,ICLR,2019,A Variational Autoencoder for Probabilistic Non-Negative Matrix Factorisation,Steven Squires;Adam Prugel-Bennett;Mahesan Niranjan,ses2g14@ecs.soton.ac.uk;apb@ecs.soton.ac.uk;mn@ecs.soton.ac.uk,4;4;7,5;3;5,Reject,0,5,0,yes,9/27/18,University of Southampton;University of Southampton;University of Southampton,169;169;169,126;126;126,5,9/27/18,3,1,1,0,0,1,21;2097;1916,7;137;152,3;25;20,2;135;160,-1;-1
2537,ICLR,2019,Inferring Reward Functions from Demonstrators with Unknown Biases,Rohin Shah;Noah Gundotra;Pieter Abbeel;Anca Dragan,rohinmshah@berkeley.edu;noah.gundotra@berkeley.edu;pabbeel@cs.berkeley.edu;anca@berkeley.edu,5;5;5,4;3;4,Reject,0,0,0,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5,18;18;18;18,,9/27/18,1,0,0,0,0,0,77;1;36532;3627,7;2;433;126,4;1;94;31,5;0;4402;285,-1;-1
2538,ICLR,2019,Low Latency Privacy Preserving Inference,Alon Brutzkus;Oren Elisha;Ran Gilad-Bachrach,brutzkus@gmail.com;oren.elisha@microsoft.com;rani.gb@gmail.com,5;6;5,3;2;4,Reject,5,4,0,yes,9/27/18,Tel Aviv University;Microsoft;Microsoft,37;-1;-1,217;-1;-1,,9/27/18,16,8,5,1,3,3,379;35;2400,10;10;64,5;3;19,41;5;296,-1;-1
2539,ICLR,2019,Ain't Nobody Got Time for Coding: Structure-Aware Program Synthesis from Natural Language,Jakub Bednarek;Karol Piaskowski;Krzysztof Krawiec,jakub.bednarek@put.poznan.pl;kar.piaskowski@gmail.com;krawiec@cs.put.poznan.pl,4;4;4,4;3;5,Reject,0,9,0,yes,9/27/18,Poznan University of Technology;Poznan University of Technology;Poznan University of Technology,478;478;478,1103;1103;1103,3,9/27/18,4,1,4,0,10,2,17;4;2527,10;8;217,3;1;22,2;2;218,-1;-1
2540,ICLR,2019,Likelihood-based Permutation Invariant Loss Function for Probability Distributions,Masataro Asai,masataro.asai@ibm.com,5;6;4,4;3;4,Reject,0,11,0,yes,9/27/18,International Business Machines,-1,-1,,9/27/18,3,0,0,0,3,0,107,18,5,8,-1
2541,ICLR,2019,Learning to Progressively Plan,Xinyun Chen;Yuandong Tian,xinyun.chen@berkeley.edu;yuandong@fb.com,5;5;5,3;3;3,Reject,0,6,0,yes,9/27/18,University of California Berkeley;Facebook,5;-1,18;-1,,9/27/18,5,1,2,0,19,0,1657;2455,51;85,14;25,141;284,-1;-1
2542,ICLR,2019,"CNNSAT: Fast, Accurate Boolean Satisfiability using Convolutional Neural Networks",Yu Wang;Fengjuan Gao;Amin Alipour;Linzhang Wang;Xuandong Li;Zhendong Su,yuwang@seg.nju.edu.cn;fjgao@seg.nju.edu.cn;alipour@cs.uh.edu;lzwang@nju.edu.cn;lxd@nju.edu.cn;zhendong.su@inf.ethz.ch,5;6;5,4;2;4,Reject,1,20,0,yes,9/27/18,Zhejiang University;Zhejiang University;University of Houston;Zhejiang University;Zhejiang University;Swiss Federal Institute of Technology,57;57;169;57;57;10,177;177;330;177;177;10,,9/27/18,0,0,0,0,0,0,3112;69;78;838;1123;6999,637;20;27;92;154;165,22;5;5;15;16;42,234;7;3;75;72;734,-1;-1
2543,ICLR,2019,Transferring SLU Models in Novel Domains,Yaohua Tang;Kaixiang Mo;Qian Xu;Chao Zhang;Qiang Yang,yaohuatang@webank.com;kxmo@connect.ust.hk;fleurxq@outlook.com;carlzzhang@webank.com;qyang@cse.ust.hk,6;5;4,4;3;3,Reject,0,0,5,yes,9/27/18,webank;The Hong Kong University of Science and Technology;;webank;The Hong Kong University of Science and Technology,-1;39;-1;-1;39,-1;44;-1;-1;44,3;6,9/27/18,0,0,0,0,0,0,74;179;277;264;926,18;14;25;148;106,3;7;6;8;13,4;11;20;10;61,-1;-1
2544,ICLR,2019,Fake Sentence Detection as a Training Task for Sentence Encoding,Viresh Ranjan;Heeyoung Kwon;Niranjan Balasubramanian;Minh Hoai,vranjan@cs.stonybrook.edu;heekwon@cs.stonybrook.edu;niranjan@cs.stonybrook.edu;minhhoai@cs.stonybrook.edu,5;3;3,3;4;5,Reject,0,0,0,yes,9/27/18,"State University of New York, Stony Brook;State University of New York, Stony Brook;State University of New York, Stony Brook;State University of New York, Stony Brook",41;41;41;41,258;258;258;258,3;5,8/11/18,3,1,1,1,27,0,293;29;1580;735,14;9;63;50,4;4;16;15,45;5;191;99,-1;-1
2545,ICLR,2019,Q-map: a Convolutional Approach for Goal-Oriented Reinforcement Learning,Fabio Pardo;Vitaly Levdik;Petar Kormushev,f.pardo@imperial.ac.uk;v.levdik@imperial.ac.uk;p.kormushev@imperial.ac.uk,4;5;4,3;4;5,Reject,0,7,1,yes,9/27/18,Imperial College London;Imperial College London;Imperial College London,72;72;72,8;8;8,8,9/27/18,2,1,0,0,153,0,53;30;1265,6;6;102,2;2;19,5;1;60,-1;-1
2546,ICLR,2019,Inter-BMV: Interpolation with Block Motion Vectors for Fast Semantic Segmentation on Video,Samvit Jain;Joseph Gonzalez,samvit@eecs.berkeley.edu;jegonzal@cs.berkeley.edu,5;3;5,4;5;4,Reject,0,4,0,yes,9/27/18,University of California Berkeley;University of California Berkeley,5;5,18;18,2,9/27/18,1,1,1,0,0,1,40;6296,14;72,4;26,11;1002,-1;-1
2547,ICLR,2019,3D-RelNet: Joint Object and Relational Network for 3D Prediction,Nilesh Kulkarni;Ishan Misra;Shubham Tulsiani;Abhinav Gupta,nileshk@cs.cmu.edu;ishan@cmu.edu;shubhtuls@fb.com;abhinavg@cs.cmu.edu,6;5;3,4;5;5,Reject,0,6,0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;Facebook;Carnegie Mellon University,1;1;-1;1,24;24;-1;24,,9/27/18,5,3,0,0,0,0,705;1508;1818;18151,49;33;35;233,10;14;18;65,67;197;225;1966,-1;-1
2548,ICLR,2019,Image Score: how to select useful samples,Simiao Zuo;Jialin Wu,zsmx1996@utexas.edu;jialinwu@utexas.edu,4;4;3,3;4;3,Reject,0,0,0,yes,9/27/18,"University of Texas, Austin;University of Texas, Austin",22;22,49;49,,9/27/18,1,0,0,0,0,0,2;138,3;23,1;8,0;6,-1;-1
2549,ICLR,2019,Learning Neuron Non-Linearities with Kernel-Based Deep Neural Networks,Giuseppe Marra;Dario Zanca;Alessandro Betti;Marco Gori,g.marra@unifi.it;dario.zanca@unifi.it;alessandro.betti@unifi.it;marco.gori@unisi.it,5;4;6,3;3;4,Reject,0,0,0,yes,9/27/18,University of Florence;University of Florence;University of Florence;University of Siena,478;478;478;169,489;489;489;161,1,7/17/18,2,0,1,0,5,0,432;32;84;7006,88;14;33;414,11;4;5;36,16;0;1;483,-1;-1
2550,ICLR,2019,DelibGAN: Coarse-to-Fine Text Generation via Adversarial Network,Ke Wang;Xiaojun Wan,wangke17@pku.edu.cn;wanxiaojun@pku.edu.cn,4;3;4,4;4;4,Reject,0,0,0,yes,9/27/18,Peking University;Peking University,24;24,27;27,4,9/27/18,0,0,0,0,0,0,596;4195,154;206,12;31,39;385,-1;-1
2551,ICLR,2019,High Resolution and Fast Face Completion via Progressively Attentive GANs,Zeyuan Chen;Shaoliang Nie;Tianfu Wu;Christopher G. Healey,zchen23@ncsu.edu;snie@ncsu.edu;tianfu_wu@ncsu.edu;healey@ncsu.edu,5;5;5,5;2;5,Reject,0,4,0,yes,9/27/18,North Carolina State University;North Carolina State University;North Carolina State University;North Carolina State University,89;89;89;89,275;275;275;275,5;4,9/27/18,0,0,0,0,0,0,34;16;1125;2311,14;7;90;112,4;2;17;23,3;1;54;115,-1;-1
2552,ICLR,2019,Unsupervised Disentangling Structure and Appearance,Wayne Wu;Kaidi Cao;Cheng Li;Chen Qian;Chen Change Loy,wuwenyan@sensetime.com;kaidicao@cs.stanford.edu;chengli@sensetime.com;qianchen@sensetime.com;ccloy225@gmail.com,6;5;3,4;4;4,Reject,0,0,0,yes,9/27/18,SenseTime Group Limited;Stanford University;SenseTime Group Limited;SenseTime Group Limited;the Chinese University of Hong Kong,-1;4;-1;-1;57,-1;3;-1;-1;58,5,9/27/18,0,0,0,0,0,0,55;186;28;211;16532,11;17;28;41;189,4;6;2;8;57,4;32;1;14;2855,-1;-1
2553,ICLR,2019,Language Model Pre-training for Hierarchical Document Representations,Ming-Wei Chang;Kristina Toutanova;Kenton Lee;Jacob Devlin,mingweichang@google.com;kristout@google.com;kentonl@google.com;jacobdevlin@google.com,6;6;6,4;4;4,Reject,0,6,0,yes,9/27/18,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,3;2,9/27/18,8,4,1,1,0,0,10860;14168;12190;9211,90;105;36;45,30;36;20;20,2859;3180;3473;2687,-1;-1
2554,ICLR,2019,Human Action Recognition Based on Spatial-Temporal Attention,Wensong Chan;Zhiqiang Tian;Xuguang Lan,2489925838@qq.com;zhiqiangtian@xjtu.edu.cn;xglan@xjtu.edu.cn,4;3;3,4;5;4,Reject,0,0,0,yes,9/27/18,Xi'an Jiaotong University;Xi'an Jiaotong University;Xi'an Jiaotong University,478;478;478,565;565;565,,9/27/18,0,0,0,0,0,0,0;0;603,3;11;93,0;0;10,0;0;37,-1;-1
2555,ICLR,2019,Adversarial Attacks for Optical Flow-Based Action Recognition Classifiers,Nathan Inkawhich;Matthew Inkawhich;Hai Li;Yiran Chen,nathan.inkawhich@duke.edu;matthew.inkawhich@duke.edu;hai.li@duke.edu;yiran.chen@duke.edu,4;3;5,5;4;4,Reject,0,4,0,yes,9/27/18,Duke University;Duke University;Duke University;Duke University,44;44;44;44,17;17;17;17,4,9/27/18,6,2,0,0,0,0,24;6;162;585,5;3;26;74,2;1;6;12,0;0;17;46,-1;-1
2556,ICLR,2019,Computing committor functions for the study of rare events using deep learning with importance sampling,Qianxiao Li;Bo Lin;Weiqing Ren,liqix@ihpc.a-star.edu.sg;linbo94@u.nus.edu;matrw@nus.edu.sg,6;6;5;7,4;4;4;4,Reject,0,6,0,yes,9/27/18,"Institute of High Performance Computing, Singapore, A*STAR;National University of Singapore;National University of Singapore",-1;16;16,-1;22;22,,9/27/18,1,1,0,0,0,0,397;1856;1203,29;232;32,9;21;14,38;83;75,-1;-1
2557,ICLR,2019,(Unconstrained) Beam Search is Sensitive to Large Search Discrepancies,Eldan Cohen;J. Christopher Beck,ecohen@mie.utoronto.ca;jcb@mie.utoronto.ca,5;5;7,4;5;5,Reject,0,10,0,yes,9/27/18,Toronto University;Toronto University,18;18,22;22,,9/27/18,1,1,0,0,0,0,24;2757,12;227,3;29,1;203,-1;-1
2558,ICLR,2019,Multi-turn Dialogue Response Generation in an Adversarial Learning Framework,Oluwatobi O. Olabiyi;Alan Salimov;Anish Khazane;Erik T. Mueller,oluwatobi.olabiyi@capitalone.com;alan.salimov@capitalone.com;anish.khazan@capitalone.com;erik.mueller@capitalone.com,4;4;6;5,4;4;4;5,Reject,0,5,0,yes,9/27/18,Capital One Bank;Capital One Bank;Capital One Bank;Capital One Bank,-1;-1;-1;-1,-1;-1;-1;-1,5;4,5/30/18,14,8,6,1,10,2,380;21;26;1351,49;4;6;124,11;2;3;15,36;2;2;109,-1;-1
2559,ICLR,2019,Zero-Resource Multilingual Model Transfer: Learning What to Share,Xilun Chen;Ahmed Hassan Awadallah;Hany Hassan;Wei Wang;Claire Cardie,xlchen@cs.cornell.edu;hassanam@microsoft.com;hanyh@microsoft.com;wei.wang@microsoft.com;cardie@cs.cornell.edu,6;5;6,4;5;4,Reject,0,6,0,yes,9/27/18,Cornell University;Microsoft;Microsoft;Microsoft;Cornell University,7;-1;-1;-1;7,19;-1;-1;-1;19,3;4;6,9/27/18,10,5,1,0,4,1,292;2147;1421;18728;14362,10;120;69;1165;226,7;25;19;43;55,35;172;111;1551;1463,-1;-1
2560,ICLR,2019,Inducing Cooperation via Learning to reshape rewards in semi-cooperative multi-agent reinforcement learning,David Earl Hostallero;Daewoo Kim;Kyunghwan Son;Yung Yi,ddhostallero@kaist.ac.kr;kdw2139@gmail.com;khson@lanada.kaist.ac.kr;yiyung@kaist.edu,5;5;5,3;4;4,Reject,0,4,0,yes,9/27/18,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;KAIST,20;20;20;20,95;95;95;95,,9/27/18,0,0,0,0,0,0,101;126;70;4774,9;28;9;224,4;6;3;29,17;8;12;506,-1;-1
2561,ICLR,2019,VECTORIZATION METHODS IN RECOMMENDER SYSTEM,Qiang Sun;Bin Wang;Zizhou Gu;Yanwei Fu,sunqiang85@gmail.com;vborisw@gmail.com;2470569@qq.com;yanweifu@fudan.edu.cn,2;2;3,5;5;4,Reject,0,0,0,yes,9/27/18,Fudan University;;;Fudan University,78;-1;-1;78,116;-1;-1;116,3,9/27/18,0,0,0,0,0,0,4655;23431;0;87,498;2497;1;22,30;58;0;5,103;1195;0;2,-1;-1
2562,ICLR,2019,The Forward-Backward Embedding of Directed Graphs,Thomas Bonald;Nathan De Lara,thomas.bonald@telecom-paristech.fr;nathan.delara@telecom-paristech.fr,5;3;4,5;5;4,Reject,2,4,0,yes,9/27/18,Télécom ParisTech;Télécom ParisTech,478;478,188;188,1;10,9/27/18,0,0,0,0,0,0,3890;17,177;9,30;2,405;2,-1;-1
2563,ICLR,2019,Provable Guarantees on Learning Hierarchical Generative Models with Deep CNNs,Eran Malach;Shai Shalev-Shwartz,eran.malach@mail.huji.ac.il;shais@cs.huji.ac.il,6;4;6,3;4;3,Reject,0,3,0,yes,9/27/18,Hebrew University of Jerusalem;Hebrew University of Jerusalem,65;65,205;205,5;9,9/27/18,0,0,0,0,0,0,258;13818,12;168,5;48,40;1814,-1;-1
2564,ICLR,2019,Contextualized Role Interaction for Neural Machine Translation,Dirk Weissenborn;Douwe Kiela;Jason Weston;Kyunghyun Cho,dirk.weissenborn@gmail.com;dkiela@fb.com;jase@fb.com;kyunghyun.cho@nyu.edu,4;5;4,5;4;4,Reject,0,5,0,yes,9/27/18,German Research Center for AI;Facebook;Facebook;New York University,-1;-1;-1;26,-1;-1;-1;27,3,9/27/18,2,0,0,0,0,0,601;3415;44559;45405,28;79;243;273,10;29;76;52,61;583;5821;6560,-1;-1
2565,ICLR,2019,Dual Skew Divergence Loss for Neural Machine Translation,Yingting Wu;Hai Zhao;Rui Wang,wuyingting@sjtu.edu.cn;zhaohai@cs.sjtu.edu.cn;wangrui.nlp@gmail.com,3;6;5,4;4;4,Reject,0,0,0,yes,9/27/18,Shanghai Jiao Tong University;Shanghai Jiao Tong University;,52;52;-1,188;188;-1,3;8,9/27/18,2,2,1,0,0,1,35;53;582,8;24;54,4;4;14,1;5;47,-1;-1
2566,ICLR,2019,On Accurate Evaluation of GANs for Language Generation,Stanislau Semeniuta;Aliaksei Severyn;Sylvain Gelly,stas@inb.uni-luebeck.de;severyn@google.com;sylvaingelly@google.com,6;5;3,4;4;4,Reject,0,4,0,yes,9/27/18,University of Luebeck;Google;Google,261;-1;-1,1103;-1;-1,3;4;5,6/13/18,34,24,12,3,0,9,363;2590;3531,13;44;112,6;22;25,42;286;456,-1;-1
2567,ICLR,2019,Zero-shot Dual Machine Translation,Lierni Sestorain;Massimiliano Ciaramita;Christian Buck;Thomas Hofmann,lierni@google.com;massi@google.com;cbuck@google.com;thomas.hofmann@inf.ethz.ch,5;4;6,4;5;3,Reject,0,6,0,yes,9/27/18,Google;Google;Google;Swiss Federal Institute of Technology,-1;-1;-1;10,-1;-1;-1;10,3;6,5/25/18,10,5,4,2,22,2,10;2895;1466;22675,2;76;80;173,1;28;19;52,2;266;153;3389,-1;-1
2568,ICLR,2019,Towards the Latent Transcriptome,Assya Trofimov;Francis Dutil;Claude Perreault;Sebastien Lemieux;Yoshua Bengio;Joseph Paul Cohen,trofimov.assya@gmail.com;frdutil@gmail.com;claude.perreault@umontreal.ca;s.lemieux@umontreal.ca;yoshua.bengio@mila.quebec;joseph@josephpcohen.com,4;2;5,4;5;4,Reject,0,4,1,yes,9/27/18,University of Montreal;;University of Montreal;University of Montreal;University of Montreal;University of Montreal,123;-1;123;123;123;123,108;-1;108;108;108;108,,9/27/18,5,0,0,0,5,0,69;358;3587;3311;201719;480,8;17;239;120;807;61,4;6;33;30;147;10,10;32;170;247;23989;49,-1;-1
2569,ICLR,2019,On the Relationship between Neural Machine Translation and Word Alignment,Xintong Li;Lemao Liu;Guanlin Li;Max Meng;Shuming Shi,znculee@gmail.com;redmondliu@tencent.com;epsilonlee.green@gmail.com;max.meng@ieee.org;shumingshi@tencent.com,4;5;6,4;4;4,Reject,0,9,0,yes,9/27/18,The Chinese University of Hong Kong;Tencent AI Lab;Harbin Institute of Technology;;Tencent AI Lab,57;-1;199;-1;-1,40;-1;522;-1;-1,3,9/27/18,0,0,0,0,0,0,57;501;17;9;1505,23;46;8;8;118,4;13;3;1;24,4;42;0;0;172,-1;-1
2570,ICLR,2019,"FROM DEEP LEARNING TO DEEP DEDUCING: AUTOMATICALLY TRACKING DOWN NASH EQUILIBRIUM THROUGH AUTONOMOUS NEURAL AGENT, A POSSIBLE MISSING STEP TOWARD GENERAL A.I.",Brown Wang,brownwang0426@gmail.com,3;2;4,3;4;5,Reject,0,1,0,yes,9/27/18,National Taiwan University,85,197,,9/27/18,0,0,0,0,0,0,0,2,0,0,-1
2571,ICLR,2019,Neural Predictive Belief Representations,Zhaohan Daniel Guo;Mohammad Gheshlaghi Azar;Bilal Piot;Bernardo Avila Pires;Rémi Munos,z.daniel.guo@gmail.com;mazar@google.com;piot@google.com;bavilapires@google.com;munos@google.com,4;7;5,3;4;3,Reject,0,4,0,yes,9/27/18,Carnegie Mellon University;Google;Google;Google;Google,1;-1;-1;-1;-1,24;-1;-1;-1;-1,,9/27/18,13,9,5,0,8,3,30;1571;2040;25;9377,5;32;70;5;190,3;15;18;3;53,5;282;266;3;1319,-1;-1
2572,ICLR,2019,Accidental exploration through value predictors,Tomasz Kisielewski;Damian Leśniak;Maia Pasek,tymorl@gmail.com;damian.lesniak@doctoral.uj.edu.pl;maiapasek@gmail.com,3;5;4,4;4;4,Reject,0,6,0,yes,9/27/18,Jagiellonian University;Jagiellonian University;,478;478;-1,695;695;-1,,9/27/18,0,0,0,0,0,0,3;91;0,4;24;1,1;3;0,1;6;0,-1;-1
2573,ICLR,2019,Neural Model-Based Reinforcement Learning for Recommendation,Xinshi Chen;Shuang Li;Hui Li;Shaohua Jiang;Le Song,xinshi.chen@gatech.edu;sli370@gatech.edu;ken.lh@alibaba-inc.com;shaohua.jsh@alipay.com;lsong@cc.gatech.edu,5;6;5,4;3;5,Reject,0,5,0,yes,9/27/18,Georgia Institute of Technology;Georgia Institute of Technology;Alibaba Group;Alipay;Georgia Institute of Technology,13;13;-1;-1;13,33;33;-1;-1;33,5;4,9/27/18,17,10,7,0,2,1,36;17329;1187;102;9290,14;1711;147;35;329,3;56;18;4;52,1;1005;28;9;1102,-1;-1
2574,ICLR,2019,PPO-CMA: Proximal Policy Optimization with Covariance Matrix Adaptation,Perttu Hämäläinen;Amin Babadi;Xiaoxiao Ma;Jaakko Lehtinen,perttu.hamalainen@aalto.fi;amin.babadi@aalto.fi;xiaoxiao.ma@aalto.fi;jaakko.lehtinen@aalto.fi,4;9;4,4;3;2,Reject,0,6,0,yes,9/27/18,Aalto University;Aalto University;Aalto University;Aalto University,140;140;140;140,190;190;190;190,,9/27/18,12,3,4,0,8,2,1093;20;419;3945,68;8;53;90,18;2;12;26,75;1;33;558,-1;-1
2575,ICLR,2019,Unsupervised Meta-Learning for Reinforcement Learning,Abhishek Gupta;Benjamin Eysenbach;Chelsea Finn;Sergey Levine,abhigupta@berkeley.edu;eysenbachbe@gmail.com;cbfinn@eecs.berkeley.edu;svlevine@eecs.berkeley.edu,3;6;4,4;3;2,Reject,0,4,0,yes,9/25/19,University of California Berkeley;Carnegie Mellon University;University of California Berkeley;University of California Berkeley,5;1;5;5,18;24;18;18,6,6/12/18,35,27,10,1,0,4,1394;351;7706;24386,89;16;98;309,18;6;33;73,142;52;1033;3167,m;m
2576,ICLR,2019,Set Transformer,Juho Lee;Yoonho Lee;Jungtaek Kim;Adam R. Kosiorek;Seungjin Choi;Yee Whye Teh,juho.lee@stats.ox.ac.uk;einet89@gmail.com;jtkim@postech.ac.kr;adamk@robots.ox.ac.uk;seungjin@postech.ac.kr;y.w.teh@stats.ox.ac.uk,6;5;6,5;4;3,Reject,0,5,0,yes,9/27/18,University of Oxford;POSTECH;POSTECH;University of Oxford;POSTECH;University of Oxford,50;123;123;50;123;50,1;137;137;1;137;1,6,9/27/18,56,30,27,2,55,14,1516;255;78;332;6441;23146,108;34;16;19;321;249,20;8;4;9;37;52,104;30;15;45;636;3217,-1;-1
2577,ICLR,2019,Deep Neuroevolution: Genetic Algorithms are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning,Felipe Petroski Such;Vashisht Madhavan;Edoardo Conti;Joel Lehman;Kenneth O. Stanley;Jeff Clune,felipe.such@uber.com;vashisht@uber.com;edoardo@uber.com;joel.lehman@uber.com;kstanley@uber.com;jeffclune@uber.com,6;7;6;4;3,4;5;2;4;4,Reject,2,10,0,yes,9/27/18,Uber;Uber;Uber;Uber;Uber;Uber,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,,12/18/17,282,122,85,10,0,22,711;917;463;2900;11999;10742,14;17;11;69;260;113,7;7;4;23;52;36,86;118;44;370;1524;768,-1;-1
2578,ICLR,2019,A Rotation and a Translation Suffice: Fooling CNNs with Simple Transformations,Logan Engstrom;Brandon Tran;Dimitris Tsipras;Ludwig Schmidt;Aleksander Madry,engstrom@mit.edu;btran115@mit.edu;tsipras@mit.edu;ludwigs@mit.edu;madry@mit.edu,8;6;5,3;2;4,Reject,0,6,0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2;2,5;5;5;5;5,4,12/7/17,162,105,33,8,0,18,1998;-1;3774;3730;5419,27;-1;33;198;84,15;-1;16;21;29,264;0;908;896;1082,-1;-1
2579,ICLR,2019,Deep Recurrent Gaussian Process with Variational Sparse Spectrum Approximation,Roman Föll;Bernard Haasdonk;Markus Hanselmann;Holger Ulmer,foell@mathematik.uni-stuttgart.de;haasdonk@mathematik.uni-stuttgart.de;markus.hanselmann@etas.com;holger.ulmer@etas.com,7;5;5,2;4;4,Reject,0,5,0,yes,9/27/18,University of Stuttgart;University of Stuttgart;ETAS GmbH;ETAS GmbH,95;95;-1;-1,219;219;-1;-1,,11/2/17,3,3,1,0,2,1,3;2544;63;703,1;128;8;40,1;25;3;15,1;206;5;51,-1;-1
2580,ICLR,2019,Analysis of Memory Organization for Dynamic Neural Networks,Ying Ma;Jose Principe,mayingbit2011@gmail.com;principe@cnel.ufl.edu,7;5;3,3;5;5,Reject,0,14,0,yes,9/27/18,University of Florida;University of Florida,123;123,143;143,3,9/27/18,0,0,0,0,0,0,2255;16641,297;1022,22;57,113;1266,-1;-1
2581,ICLR,2019,Generative Ensembles for Robust Anomaly Detection,Hyunsun Choi;Eric Jang,hyunsunchoi@kaist.ac.kr;ejang@google.com,5;4;6,4;5;3,Reject,0,8,0,yes,9/27/18,Korea Advanced Institute of Science and Technology;Google,20;-1,95;-1,3;5,9/27/18,38,22,12,3,137,3,113;1936,21;15,3;11,19;291,-1;-1
2582,ICLR,2019,SHAMANN: Shared Memory Augmented Neural Networks,Cosmin I. Bercea;Olivier Pauly;Andreas K. Maier;Florin C. Ghesu,cosmin.bercea@fau.de;olivier.pauly@gmail.com;andreas.maier@fau.de;florin.ghesu@siemens-healthineers.com,4;5;4,5;3;5,Reject,0,0,0,yes,9/27/18,University of Erlangen-Nuremberg;;University of Erlangen-Nuremberg;Siemens Healthineers,199;-1;199;-1,162;-1;162;-1,2,9/27/18,0,0,0,0,0,0,0;475;2648;325,1;40;428;19,0;12;25;7,0;24;126;16,-1;-1
2583,ICLR,2019,Novel positional encodings to enable tree-structured transformers,Vighnesh Leonardo Shiv;Chris Quirk,vishiv@microsoft.com;chrisq@microsoft.com,4;6;5,3;3;3,Reject,0,7,0,yes,9/27/18,Microsoft;Microsoft,-1;-1,-1;-1,,9/27/18,8,3,3,0,0,0,11;3508,4;94,3;28,0;379,-1;-1
2584,ICLR,2019,Learning Joint Wasserstein Auto-Encoders for Joint Distribution Matching,Jiezhang Cao;Yong Guo;Langyuan Mo;Peilin Zhao;Junzhou Huang;Mingkui Tan,secaojiezhang@mail.scut.edu.cn;guoyongcs@gmail.com;selangyuanmo@mail.scut.edu.cn;peilinzhao@hotmail.com;jzhuang@uta.edu;mingkuitan@scut.edu.cn,6;4;5,4;4;4,Reject,0,4,0,yes,9/27/18,"South China University of Technology;South China University of Technology;South China University of Technology;;University of Texas, Arlington;South China University of Technology",478;478;478;-1;115;478,576;576;576;-1;601;576,1;8,9/27/18,1,1,0,0,0,0,87;224;15;3483;5546;1805,17;40;3;133;189;112,5;6;1;31;36;24,2;19;0;390;511;164,-1;-1
2585,ICLR,2019,Meta Learning with Fast/Slow Learners,zhuoyuan@fb.com,chengzhuoyuan07@gmail.com,5;5;6,3;4;3,Reject,0,0,0,yes,9/27/18,,,,6,9/27/18,-1,-1,-1,-1,-1,-1,-1,-1,-1,0,-1
2586,ICLR,2019,Domain Generalization via Invariant Representation under Domain-Class Dependency,Kei Akuzawa;Yusuke Iwasawa;Yutaka Matsuo,akuzawa-kei@weblab.t.u-tokyo.ac.jp;iwasawa@weblab.t.u-tokyo.ac.jp;matsuo@weblab.t.u-tokyo.ac.jp,4;7;5,5;5;4,Reject,0,8,0,yes,9/27/18,The University of Tokyo;The University of Tokyo;The University of Tokyo,54;54;54,45;45;45,8,9/27/18,2,1,0,0,0,0,52;114;3493,10;39;492,2;6;28,5;10;193,-1;-1
2587,ICLR,2019,Hierarchical Attention: What Really Counts in Various NLP Tasks,Zehao Dou;Zhihua Zhang,zehaodou@pku.edu.cn;zhzhang@math.pku.edu.cn,4;3;4,4;5;3,Reject,1,0,0,yes,9/27/18,Peking University;Peking University,24;24,27;27,3;1;8,8/10/18,23,0,0,0,23,0,12;5090,5;407,2;29,0;521,-1;-1
2588,ICLR,2019,Learning with Reflective Likelihoods,Adji B. Dieng;Kyunghyun Cho;David M. Blei;Yann LeCun,abd2141@columbia.edu;kyunghyun.cho@nyu.edu;david.blei@columbia.edu;yann@fb.com,4;2;3,4;4;4,Reject,2,13,0,yes,9/27/18,Columbia University;New York University;Columbia University;Facebook,15;26;15;-1,14;27;14;-1,5,9/27/18,2,1,0,0,0,0,466;45405;52408;91479,14;273;306;345,7;52;76;107,60;6560;9236;10337,-1;-1
2589,ICLR,2019,Adapting Auxiliary Losses Using Gradient Similarity,Yunshu Du;Wojciech M. Czarnecki;Siddhant M. Jayakumar;Razvan Pascanu;Balaji Lakshminarayanan,yunshu.du@wsu.edu;lejlot@google.com;sidmj@google.com;razp@google.com;balajiln@google.com,4;6;6,5;4;3,Reject,0,11,0,yes,9/27/18,SUN YAT-SEN UNIVERSITY;Google;Google;Google;Google,478;-1;-1;-1;-1,352;-1;-1;-1;-1,,9/27/18,24,10,9,0,114,3,67;2275;181;16705;2904,9;44;14;101;43,4;20;8;46;22,4;244;23;1671;378,-1;-1
2590,ICLR,2019,Model Comparison for Semantic Grouping,Francisco Vargas;Kamen Brestnichki;Nils Hammerla,francisco.vargas@babylonhealth.com;kamen.brestnichki@babylonhealth.com;nils.hammerla@babylonhealth.com,5;5;5,3;3;1,Reject,0,8,0,yes,9/27/18,babylon health;babylon health;babylon health,-1;-1;-1,-1;-1;-1,5;11,9/27/18,1,0,1,0,0,0,1;1;62,1;3;6,1;1;1,0;0;2,-1;-1
2591,ICLR,2019,Connecting the Dots Between MLE and RL for Sequence Generation,Bowen Tan*;Zhiting Hu*;Zichao Yang;Ruslan Salakhutdinov;Eric P. Xing,tanbowen@sjtu.edu.cn;zhitinghu@gmail.com;yangtze2301@gmail.com;rsalakhu@cs.cmu.edu;epxing@cs.cmu.edu,5;6;5,5;3;4,Reject,0,5,0,yes,9/27/18,Shanghai Jiao Tong University;Carnegie Mellon University;;Carnegie Mellon University;Carnegie Mellon University,52;1;-1;1;1,188;24;-1;24;24,3,9/27/18,13,8,6,0,8,2,94;3202;5699;67470;24050,22;64;56;254;604,6;29;18;82;75,5;366;628;7779;2652,-1;-1
2592,ICLR,2019,DeepTwist: Learning Model Compression via Occasional Weight Distortion,Dongsoo Lee;Parichay Kapoor;Byeongwook Kim,dslee3@gmail.com;kparichay@gmail.com;quddnr145@gmail.com,4;5;4,4;3;3,Reject,0,17,0,yes,9/27/18,Samsung;;Samsung,-1;-1;-1,-1;-1;-1,,9/27/18,9,4,3,0,2,0,331;15;23,33;6;10,10;2;3,39;0;1,-1;-1
2593,ICLR,2019,Excitation Dropout: Encouraging Plasticity in Deep Neural Networks,Andrea Zunino;Sarah Adel Bargal;Pietro Morerio;Jianming Zhang;Stan Sclaroff;Vittorio Murino,andrea.zunino@iit.it;sbargal@bu.edu;pietro.morerio@iit.it;jianmzha@adobe.com;sclaroff@bu.edu;vittorio.murino@iit.it,5;5;5,4;3;4,Reject,0,9,0,yes,9/27/18,Istituto Italiano di Tecnologia;Boston University;Istituto Italiano di Tecnologia;Adobe Systems;Boston University;Istituto Italiano di Tecnologia,478;65;478;-1;65;478,1103;70;1103;-1;70;1103,8,5/23/18,6,3,1,0,11,0,114;336;612;4172;13761;8965,21;20;45;196;300;492,6;7;13;31;60;44,9;60;51;551;1297;879,-1;-1
2594,ICLR,2019,Measuring Density and Similarity of Task Relevant Information in Neural Representations,Danish Pruthi;Mansi Gupta;Nitish Kumar Kulkarni;Graham Neubig;Eduard Hovy,ddanish@cs.cmu.edu;mansig1@cs.cmu.edu;nitishkk@andrew.cmu.edu;gneubig@cs.cmu.edu;hovy@cmu.edu,4;5;5,4;4;3,Reject,0,7,0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1;1,24;24;24;24;24,3;6,9/27/18,0,0,0,0,0,0,-1;37;9;5467;23539,-1;16;7;443;582,-1;4;2;39;76,0;2;2;575;2475,-1;-1
2595,ICLR,2019,ATTENTIVE EXPLAINABILITY FOR PATIENT TEMPORAL EMBEDDING,Daby Sow;Mohamed Ghalwash;Zach Shahn;Sanjoy Dey;Moulay Draidia;Li-wei Lehmann,sowdaby@us.ibm.com;mohamed.ghalwash@ibm.com;zach.shahn@ibm.com;deysa@us.ibm.com;mzdraidia@berkeley.edu;lilehman@mit.edu,4;3;2,4;4;3,Reject,0,3,1,yes,9/27/18,International Business Machines;International Business Machines;International Business Machines;International Business Machines;University of California Berkeley;Massachusetts Institute of Technology,-1;-1;-1;-1;5;2,-1;-1;-1;-1;18;5,,9/27/18,0,0,0,0,0,0,5;16;100;146;0;0,11;10;10;27;1;1,1;2;5;6;0;0,0;1;6;6;0;0,-1;-1
2596,ICLR,2019,Feature prioritization and regularization improve standard accuracy and adversarial robustness,Chihuang Liu;Joseph JaJa,chliu@umd.edu;joseph@umiacs.umd.edu,5;5;4,5;2;3,Reject,0,4,0,yes,9/27/18,"University of Maryland, College Park;University of Maryland, College Park",12;12,69;69,4,9/27/18,1,0,0,0,10,0,10;2717,3;234,1;24,0;271,-1;-1
2597,ICLR,2019,A Study of Robustness of Neural Nets Using Approximate Feature Collisions,Ke Li*;Tianhao Zhang*;Jitendra Malik,ke.li@eecs.berkeley.edu;bryanzhang@berkeley.edu;malik@eecs.berkeley.edu,6;4;4,3;4;4,Reject,0,4,0,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,18;18;18,4,9/27/18,1,0,1,0,0,0,165;1835;69075,53;77;429,8;19;115,8;184;7768,-1;-1
2598,ICLR,2019,DEEP HIERARCHICAL MODEL FOR HIERARCHICAL SELECTIVE CLASSIFICATION AND ZERO SHOT LEARNING,Eliyahu Sason;Koby Crammer,sasonil@gmail.com;koby@ee.technion.ac.il,4;5;2,4;3;4,Reject,0,5,0,yes,9/27/18,Technion;Technion,25;25,327;327,6;8,9/27/18,0,0,0,0,0,0,0;11156,1;139,0;37,0;1634,-1;-1
2599,ICLR,2019,Advocacy Learning,Ian Fox;Jenna Wiens,ifox@umich.edu;wiensj@umich.edu,4;4;8,4;4;2,Reject,0,4,0,yes,9/27/18,University of Michigan;University of Michigan,8;8,21;21,,9/27/18,0,0,0,0,0,0,31;337,17;37,4;9,4;22,-1;-1
2600,ICLR,2019,Learning with Random Learning Rates.,Léonard Blier;Pierre Wolinski;Yann Ollivier,leonardb@fb.com;pierre.wolinski@u-psud.fr;yol@fb.com,4;6;5,4;4;4,Reject,0,5,0,yes,9/27/18,Facebook;UPSud/INRIA University Paris-Saclay;Facebook,-1;478;-1,-1;1103;-1,,9/27/18,4,2,1,0,55,1,31;6;1576,6;7;131,3;2;20,5;1;191,-1;-1
2601,ICLR,2019,BLISS in Non-Isometric Embedding Spaces,Barun Patra;Joel Ruben Antony Moniz;Sarthak Garg;Matthew R Gormley;Graham Neubig,bpatra@andrew.cmu.edu;jrmoniz@andrew.cmu.edu;sarthakg@andrew.cmu.edu;mgormley@andrew.cmu.edu;gneubig@andrew.cmu.edu,4;6;6,5;5;4,Reject,0,6,0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1;1,24;24;24;24;24,3,9/27/18,1,1,1,0,0,0,33;85;30;512;5467,11;18;13;28;443,2;5;2;12;39,6;15;11;57;575,-1;-1
2602,ICLR,2019,Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels,Bo Han;Gang Niu;Jiangchao Yao;Xingrui Yu;Miao Xu;Ivor Tsang;Masashi Sugiyama,bo.han@riken.jp;gang.niu@riken.jp;jiangchao.yao@student.uts.edu.au;xingrui.yu@student.uts.edu.au;miao.xu@riken.jp;ivor.tsang@uts.edu.au;sugi@k.u-tokyo.ac.jp,6;5;3,3;5;4,Reject,12,11,0,yes,9/27/18,RIKEN;RIKEN;University of Technology Sydney;University of Technology Sydney;RIKEN;University of Technology Sydney;The University of Tokyo,-1;-1;106;106;-1;106;54,-1;-1;216;216;-1;216;45,8,9/27/18,9,6,5,1,6,3,1318;1149;212;336;779;10711;11582,130;78;58;17;85;253;712,19;16;9;7;12;50;52,128;144;21;56;109;1268;1299,-1;-1
2603,ICLR,2019,HAPPIER: Hierarchical Polyphonic Music Generative RNN,Tianyang Zhao;Xiaoxuan Ma;Honglin Ma;Yizhou Wang,zhaotianyang@pku.edu.cn;maxiaoxuan@pku.edu.cn;mahonglin_pku@outlook.com;yizhou.wang@pku.edu.cn,2;3;3,4;4;5,Reject,1,0,0,yes,9/27/18,Peking University;Peking University;;Peking University,24;24;-1;24,27;27;-1;27,5,9/27/18,0,0,0,0,0,0,283;54;1;807,73;25;3;47,7;5;1;16,21;3;1;86,-1;-1
2604,ICLR,2019,A Modern Take on the Bias-Variance Tradeoff in Neural Networks,Brady Neal;Sarthak Mittal;Aristide Baratin;Vinayak Tantia;Matthew Scicluna;Simon Lacoste-Julien;Ioannis Mitliagkas,bradyneal11@gmail.com;sarthmit@gmail.com;aristidebaratin@hotmail.com;tantia.vinayak1@gmail.com;mattcscicluna@gmail.com;slacoste@iro.umontreal.ca;ioannis@iro.umontreal.ca,5;7;4,3;4;4,Reject,0,11,0,yes,9/27/18,University of Montreal;IIT Kanpur;University of Montreal;;;University of Montreal;University of Montreal,123;123;123;-1;-1;123;123,108;578;108;-1;-1;108;108,8,9/27/18,29,15,1,5,7,1,30;30;924;37;29;3744;1176,5;6;31;4;5;73;43,1;1;17;2;1;26;18,1;1;121;1;1;606;193,-1;-1
2605,ICLR,2019,Learning a Neural-network-based Representation for Open Set Recognition,Mehadi Hassen;Philip K. Chan,mhassen2005@my.fit.edu;pkc@cs.fit.edu,4;4;5,4;4;4,Reject,0,5,0,yes,9/27/18,Florida Institute of Technology;Florida Institute of Technology,478;478,750;750,,2/12/18,14,6,1,0,6,2,56;4781,5;80,3;26,6;323,-1;-1
2606,ICLR,2019,Faster Training by Selecting Samples Using Embeddings,Santiago Gonzalez;Joshua Landgraf;Risto Miikkulainen,slgonzalez@utexas.edu;jland@cs.utexas.edu;risto@cs.utexas.edu,3;3;2,5;3;5,Reject,0,0,0,yes,9/27/18,"University of Texas, Austin;University of Texas, Austin;University of Texas, Austin",22;22;22,49;49;49,,9/27/18,3,1,1,0,0,0,24;112;12696,8;9;441,3;3;49,4;8;1318,-1;-1
2607,ICLR,2019,Effective Path: Know the Unknowns of Neural Network,Yuxian Qiu;Jingwen Leng;Yuhao Zhu;Quan Chen;Chao Li;Minyi Guo,qiuyuxian@sjtu.edu.cn;leng-jw@sjtu.edu.cn;yzhu@rochester.edu;chen-quan@sjtu.edu.cn;lichao@cs.sjtu.edu.cn;guo-my@cs.sjtu.edu.cn,4;4;6,4;3;5,Reject,0,0,0,yes,9/27/18,Shanghai Jiao Tong University;Shanghai Jiao Tong University;University of Rochester;Shanghai Jiao Tong University;Shanghai Jiao Tong University;Shanghai Jiao Tong University,52;52;106;52;52;52,188;188;153;188;188;188,4,9/27/18,0,0,0,0,0,0,5;665;723;111;13072;5545,4;34;59;40;1816;544,2;8;15;7;46;38,0;92;57;4;477;467,-1;-1
2608,ICLR,2019,Pseudosaccades: A simple ensemble scheme for improving classification performance of deep nets,Jin Sean Lim;Robert John Durrant,me@nicklim.com;bobd@waikato.ac.nz,5;4;4,4;4;5,Reject,0,0,0,yes,9/27/18,The University of Waikato;The University of Waikato,314;314,393;393,,9/27/18,0,0,0,0,0,0,0;310,4;35,0;10,0;24,-1;-1
2609,ICLR,2019,ADAPTIVE NETWORK SPARSIFICATION VIA DEPENDENT VARIATIONAL BETA-BERNOULLI DROPOUT,Juho Lee;Saehoon Kim;Jaehong Yoon;Hae Beom Lee;Eunho Yang;Sung Ju Hwang,juho.lee@stats.ox.ac.uk;shkim@aitrics.com;jaehong.yoon@kaist.ac.kr;haebeom.lee@kaist.ac.kr;eunhoy@kaist.ac.kr;sjhwang82@kaist.ac.kr,5;5;7,4;4;4,Reject,0,6,0,yes,9/27/18,University of Oxford;AITRICS;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,50;-1;20;20;20;20,1;-1;95;95;95;95,11,5/28/18,5,1,2,0,2,0,1516;413;248;48;1038;1101,108;35;14;88;76;71,20;10;4;4;16;16,104;65;38;3;166;124,-1;-1
2610,ICLR,2019,Curiosity-Driven Experience Prioritization via Density Estimation,Rui Zhao;Volker Tresp,zhaorui.in.germany@gmail.com;volker.tresp@siemens.com,6;4;6,3;4;4,Reject,0,4,0,yes,9/27/18,Siemens Corporate Research;Siemens Corporate Research,-1;-1,-1;-1,,9/27/18,16,4,7,0,6,0,8050;8140,608;287,38;44,502;794,-1;-1
2611,ICLR,2019,SEQUENCE MODELLING WITH AUTO-ADDRESSING AND RECURRENT MEMORY INTEGRATING NETWORKS,Zhangheng Li;Jia-Xing Zhong;Jingjia Huang;Tao Zhang;Thomas Li;Ge Li,zhanghengli@pku.edu.cn;jxzhong@pku.edu.cn;jjhuang@pku.edu.cn;t_zhang@pku.edu.cn;thomasli@pkusz.edu.cn;geli@ece.pku.edu.cn,5;4;4,4;5;4,Reject,0,6,0,yes,9/27/18,Peking University;Peking University;Peking University;Peking University;Tsinghua University;Peking University,24;24;24;24;8;24,27;27;27;27;30;27,,9/27/18,0,0,0,0,0,0,2;28;27;777;104;82,3;10;11;74;26;59,1;3;3;9;7;5,0;1;1;44;16;6,-1;-1
2612,ICLR,2019,Log Hyperbolic Cosine Loss Improves Variational Auto-Encoder,Pengfei Chen;Guangyong Chen;Shengyu Zhang,chenpf.cuhk@gmail.com;gycchen@tencent.com;shengyuzhang@gmail.com,4;4;5,4;4;4,Reject,0,2,0,yes,9/27/18,The Chinese University of Hong Kong;Tencent AI Lab;Chinese University of Hong Kong,57;-1;57,40;-1;58,5,9/27/18,2,0,0,0,0,0,1788;396;1175,153;40;107,20;7;19,120;47;80,-1;-1
2613,ICLR,2019,Computation-Efficient Quantization Method for Deep Neural Networks,Parichay Kapoor;Dongsoo Lee;Byeongwook Kim;Saehyung Lee,kparichay@gmail.com;dslee3@gmail.com;guddnr145@gmail.com;halo8218@gmail.com,4;5;5,4;4;4,Reject,0,8,1,yes,9/27/18,;Samsung;;Samsung,-1;-1;-1;-1,-1;-1;-1;-1,,9/27/18,2,0,1,0,0,0,15;331;23;2,6;33;10;2,2;10;3;1,0;39;1;0,-1;-1
2614,ICLR,2019,EFFICIENT SEQUENCE LABELING WITH ACTOR-CRITIC TRAINING,Saeed Najafi;Colin Cherry;Greg Kondrak,snajafi@ualberta.ca;colin.a.cherry@gmail.com;gkondrak@ualberta.ca,5;4;4,4;3;5,Reject,0,4,0,yes,9/27/18,University of Alberta;Google;University of Alberta,99;-1;99,119;-1;119,,9/27/18,2,0,1,0,3,0,59;4295;2604,19;139;107,5;31;29,4;519;254,-1;-1
2615,ICLR,2019,Unsupervised Exploration with Deep Model-Based Reinforcement Learning,Kurtland Chua;Rowan McAllister;Roberto Calandra;Sergey Levine,kchua@berkeley.edu;rmcallister@berkeley.edu;roberto.calandra@berkeley.edu;svlevine@eecs.berkeley.edu,4;4;4,4;3;4,Reject,0,4,0,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5,18;18;18;18,,9/27/18,1,1,1,0,0,0,195;556;1001;24386,4;29;57;309,1;10;15;73,56;83;100;3167,-1;-1
2616,ICLR,2019,Unification of  Recurrent   Neural Network Architectures and Quantum Inspired Stable Design ,Murphy Yuezhen Niu;Lior Horesh;Michael O'Keeffe;Isaac Chuang,yzniu@mit.edu;lhoresh@us.ibm.com;michael.okeeffe@ll.mit.edu;ichuang@mit.edu,5;4;4;5,2;2;3;2,Reject,0,0,0,yes,9/27/18,Massachusetts Institute of Technology;International Business Machines;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;-1;2;2,5;-1;5;5,1,9/27/18,0,0,0,0,0,0,585;1024;15;12101,31;81;11;86,10;17;3;18,34;83;1;1962,-1;-1
2617,ICLR,2019,Text Infilling,Wanrong Zhu;Zhiting Hu;Eric P. Xing,zhuwr56@gmail.com;zhitinghu@gmail.com;epxing@cs.cmu.edu,3;5;6,4;4;4,Reject,0,0,0,yes,9/27/18,Peking University;Carnegie Mellon University;Carnegie Mellon University,24;1;1,27;24;24,,9/27/18,14,8,7,2,8,4,41;3202;24050,3;64;604,2;29;75,5;366;2652,-1;-1
2618,ICLR,2019,Skip-gram word embeddings in hyperbolic space,Matthias Leimeister;Benjamin J. Wilson,matthias@lateral.io;benjamin@lateral.io,6;5;5,3;3;3,Reject,0,3,0,yes,9/27/18,Lateral GmbH;Lateral GmbH,-1;-1,-1;-1,3;10,8/30/18,13,5,3,0,10,1,38;294,6;27,4;11,3;21,-1;-1
2619,ICLR,2019,EXPLORATION OF EFFICIENT ON-DEVICE ACOUSTIC MODELING WITH NEURAL NETWORKS,Wonyong Sung;Lukas Lee;Jinwhan Park,wysung@snu.ac.kr;proboscis@snu.ac.kr;bnoo@snu.ac.kr,4;4;4,5;4;4,Reject,0,0,0,yes,9/27/18,Seoul National University;Seoul National University;Seoul National University,41;41;41,74;74;74,,9/27/18,0,0,0,0,0,0,154;0;0,23;4;1,6;0;0,14;0;0,-1;-1
2620,ICLR,2019,The Variational Deficiency Bottleneck,Pradeep Kr. Banerjee;Guido Montufar,pradeep@mis.mpg.de;montufar@math.ucla.edu,5;7;6,5;2;2,Reject,0,5,0,yes,9/27/18,"Max-Planck Institute;University of California, Los Angeles",-1;20,-1;15,1;8,9/27/18,4,2,1,0,0,1,651;1159,43;60,11;14,51;76,-1;-1
2621,ICLR,2019,Dataset Distillation,Tongzhou Wang;Jun-Yan Zhu;Antonio Torralba;Alexei A. Efros,tongzhou.wang.1994@gmail.com;junyanz@mit.edu;torralba@mit.edu;efros@eecs.berkeley.edu,6;5;5,4;4;4,Reject,4,4,0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;University of California Berkeley,2;2;2;5,5;5;5;18,,11/27/18,26,8,8,0,115,2,118;15972;49875;38166,7;51;281;194,3;27;90;78,18;3079;6413;4665,m;m
2622,ICLR,2019,DecayNet: A Study on the Cell States of Long Short Term Memories,Nicholas I.H. Kuo;Mehrtash T. Harandi;Hanna Suominen;Nicolas Fourrier;Christian Walder;Gabriela Ferraro,u6424547@anu.edu.au;mehrtash.harandi@monash.edu;hanna.suominen@anu.edu.au;nicolas.fourrier@devinci.fr;christian.walder@data61.csiro.au;gabriela.ferraro@csiro.au,8;4;4,3;4;4,Reject,0,10,2,yes,9/27/18,"Australian National University;Monash University;Australian National University;Ecole Superieur d'Ingenieurs Leonard de Vinci;, CSIRO;CSIRO",106;123;106;-1;-1;-1,48;80;48;-1;-1;-1,,9/27/18,0,0,0,0,0,0,0;2952;28;206;272;252,1;100;9;15;37;39,0;27;3;6;8;9,0;339;1;24;10;18,-1;-1
2623,ICLR,2019,k-Nearest Neighbors by Means of Sequence to Sequence Deep Neural Networks and Memory Networks,Yiming Xu;Diego Klabjan,yimingxu2020@u.northwestern.edu;d-klabjan@northwestern.edu,6;5;4,4;4;4,Reject,0,7,0,yes,9/27/18,Northwestern University;Northwestern University,44;44,20;20,,4/27/18,7,0,0,0,7,0,483;2625,63;215,14;26,14;196,-1;-1
2624,ICLR,2019,Towards More Theoretically-Grounded Particle Optimization Sampling for Deep Learning,Jianyi Zhang;Ruiyi Zhang;Changyou Chen,15300180019@fudan.edu.cn;rz68@duke.edu;cchangyou@gmail.com,5;3;4,4;4;3,Reject,0,12,0,yes,9/27/18,"Fudan University;Duke University;State University of New York, Buffalo",78;44;81,116;17;270,11,9/27/18,0,0,0,0,0,0,109;232;211,38;30;39,6;10;8,8;32;25,-1;-1
2625,ICLR,2019,Gaussian-gated LSTM: Improved convergence by reducing state updates,Matthew Thornton;Jithendar Anumula;Shih-Chii Liu,mattsthornton@gmail.com;anumula@ini.uzh.ch;shih@ini.uzh.ch,5;5;6,5;4;4,Reject,0,8,0,yes,9/27/18,Swiss Federal Institute of Technology;University of Zurich;University of Zurich,10;140;140,10;136;136,,9/27/18,2,1,1,0,0,0,520;67;4790,73;14;181,10;4;29,29;6;409,-1;-1
2626,ICLR,2019,Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data,Puyudi Yang;Jianbo Chen;Cho-Jui Hsieh;Jane-Ling Wang;Michael I. Jordan,pydyang@ucdavis.edu;jianbochen@berkeley.edu;chohsieh@ucdavis.edu;janelwang@ucdavis.edu;jordan@cs.berkeley.edu,3;6;8;7,4;4;2;4,Reject,0,22,5,yes,9/27/18,"University of California, Davis;University of California Berkeley;University of California, Davis;University of California, Davis;University of California Berkeley",81;5;81;81;5,54;18;54;54;18,4,5/31/18,19,12,9,0,14,4,30;856;12522;2592;115388,6;71;168;117;847,3;16;41;25;138,7;97;1726;245;15955,-1;-1
2627,ICLR,2019,Characterizing Malicious Edges targeting on Graph Neural Networks,Xiaojun Xu;Yue Yu;Bo Li;Le Song;Chengfeng Liu;Carl Gunter,xuxiaojun1005@gmail.com;yue9yu@gmail.com;lxbosky@gmail.com;lsong@cc.gatech.edu;windsonliu@tencent.com;cgunter@illinois.edu,5;5;5,5;3;3,Reject,0,5,0,yes,9/27/18,";Tsinghua University;University of California Berkeley;Georgia Institute of Technology;Tencent AI Lab;University of Illinois, Urbana Champaign",-1;8;5;13;-1;3,-1;30;18;33;-1;37,4;10,9/27/18,3,1,1,0,0,2,2821;2066;498;9290;10;6626,207;219;196;329;8;270,29;20;10;52;2;43,151;115;21;1102;2;552,-1;-1
2628,ICLR,2019,Graph2Seq: Scalable Learning Dynamics for Graphs,Shaileshh Bojja Venkatakrishnan;Mohammad Alizadeh;Pramod Viswanath,bjjvnkt@csail.mit.edu;alizadeh@csail.mit.edu;pramodv@illinois.edu,6;5;4,3;5;4,Reject,0,17,2,yes,9/27/18,"Massachusetts Institute of Technology;Massachusetts Institute of Technology;University of Illinois, Urbana Champaign",2;2;3,5;5;37,10;8,2/14/18,7,6,1,0,3,1,272;5199;16592,27;109;180,11;33;39,36;998;2155,-1;-1
2629,ICLR,2019,Unsupervised Word Discovery with Segmental Neural Language Models,Kazuya Kawakami;Chris Dyer;Phil Blunsom,kawakamik@google.com;cdyer@google.com;pblunsom@google.com,6;4;3,4;3;5,Reject,8,5,0,yes,9/27/18,Google;Google;Google,-1;-1;-1,-1;-1;-1,3;11;2;8,9/27/18,7,6,1,2,13,1,1832;21134;11400,21;231;143,7;59;47,380;3164;1326,-1;-1
2630,ICLR,2019,"A bird's eye view on coherence, and a worm's eye view on cohesion",Woon Sang Cho;Pengchuan Zhang;Yizhe Zhang;Xiujun Li;Mengdi Wang;Jianfeng Gao,woonsang@princeton.edu;penzhan@microsoft.com;yizhe.zhang@microsoft.com;xiul@microsoft.com;mengdiw@princeton.edu;jfgao@microsoft.com,2;2;4,4;4;4,Reject,0,5,0,yes,9/27/18,Princeton University;Microsoft;Microsoft;Microsoft;Princeton University;Microsoft,30;-1;-1;-1;30;-1,7;-1;-1;-1;7;-1,3,9/27/18,1,0,1,0,2,0,7;580;12;389;1008;18639,5;29;52;28;100;353,2;9;2;8;18;61,0;96;0;40;150;2650,-1;-1
2631,ICLR,2019,State-Regularized Recurrent Networks,Cheng Wang;Mathias Niepert,dr.rer.nat.chengwang@gmail.com;mathias.niepert@neclab.eu,6;6;5,4;5;5,Reject,0,4,0,yes,9/27/18,NEC Labs Europe;NEC Labs Europe,-1;-1,-1;-1,,9/27/18,7,2,1,0,3,0,339;2370,130;91,9;22,33;239,-1;-1
2632,ICLR,2019,DEEP GRAPH TRANSLATION,Xiaojie Guo;Lingfei Wu;Liang Zhao,xguo7@gmu.edu;lwu@email.wm.edu;lzhao9@gmu.edu,5;5;6,2;4;4,Reject,0,14,0,yes,9/27/18,George Mason University;College of William and Mary;George Mason University,99;169;99,336;261;336,5;4;10,5/25/18,8,4,3,0,9,0,2553;645;551,181;62;79,26;15;9,354;62;42,m;m
2633,ICLR,2019,Canonical Correlation Analysis with Implicit Distributions,Yaxin Shi;Donna Xu;Yuangang Pan;Ivor Tsang,yaxin.shi@student.uts.edu.au;donna.xu@student.uts.edu.au;yuangang.pan@student.uts.edu.au;ivor.tsang@uts.edu.au,5;6;4,5;4;5,Reject,0,12,0,yes,9/27/18,University of Technology Sydney;University of Technology Sydney;University of Technology Sydney;University of Technology Sydney,106;106;106;106,216;216;216;216,5;4,9/27/18,0,0,0,0,0,0,37;102;23;10711,11;15;15;253,4;6;3;50,0;9;0;1268,-1;-1
2634,ICLR,2019,Neural Distribution Learning for generalized time-to-event prediction,Egil Martinsson;Adrian Kim;Jaesung Huh;Jaegul Choo;Jung-Woo Ha,egil.martinsson@gmail.com;adrian.kim@navercorp.com;jaesung.huh@navercorp.com;jchoo@korea.ac.kr;jungwoo.ha@navercorp.com,4;3;3,5;4;3,Reject,0,7,0,yes,9/27/18,Chalmers University;NAVER;NAVER;Korea University;NAVER,169;-1;-1;314;-1,240;-1;-1;244;-1,,9/27/18,0,0,0,0,0,0,22;42;26;2376;1907,3;15;8;124;67,1;3;3;22;15,6;6;5;386;368,-1;-1
2635,ICLR,2019,Inference of unobserved event streams with neural Hawkes particle smoothing,Hongyuan Mei;Guanghui Qin;Jason Eisner,hmei@cs.jhu.edu;ghq@pku.edu.cn;jason@cs.jhu.edu,5;4;5,4;5;3,Reject,0,11,0,yes,9/27/18,Johns Hopkins University;Peking University;Johns Hopkins University,72;24;72,13;27;13,,9/27/18,0,0,0,0,0,0,518;14;6095,21;6;204,5;3;41,55;0;557,-1;-1
2636,ICLR,2019,Countdown Regression: Sharp and Calibrated Survival Predictions,Anand Avati;Tony Duan;Sharon Zhou;Kenneth Jung;Nigam Shah;Andrew Ng,avati@cs.stanford.edu;tonyduan@cs.stanford.edu;sharonz@cs.stanford.edu,4;4;5;4,5;4;3;4,Reject,0,4,0,yes,9/27/18,Stanford University;Stanford University;Stanford University,4;4;4,3;3;3,8,6/21/18,7,3,1,0,17,1,254;985;-1;928;7601;268,13;29;-1;32;277;19,7;10;-1;13;44;3,24;108;0;38;454;30,-1;-1
2637,ICLR,2019,Discrete Structural Planning for Generating Diverse Translations,Raphael Shu;Hideki Nakayama,shu@nlab.ci.i.u-tokyo.ac.jp;nakayama@ci.i.u-tokyo.ac.jp,4;5;2,5;3;5,Reject,0,7,0,yes,9/27/18,The University of Tokyo;The University of Tokyo,54;54,45;45,3,9/27/18,0,0,0,0,0,0,95;632,14;87,4;15,20;70,-1;-1
2638,ICLR,2019,Feature quantization for parsimonious and interpretable predictive models,Adrien EHRHARDT;Vincent VANDEWALLE;Christophe BIERNACKI;Philippe HEINRICH,adrien.ehrhardt@inria.fr;vincent.vandewalle@inria.fr;christophe.biernacki@inria.fr;philippe.heinrich@univ-lille.fr,2;3;4,4;3;2,Reject,0,4,0,yes,9/27/18,INRIA;INRIA;INRIA;Université de Lille,-1;-1;-1;478,-1;-1;-1;468,,9/27/18,1,0,0,0,0,0,2;136;11;595,6;42;15;31,1;7;2;10,0;4;1;45,-1;-1
2639,ICLR,2019,Hierarchical Bayesian Modeling for Clustering Sparse Sequences in the Context of Group Profiling,Ishani Chakraborty,ishani.chakrab@gmail.com,2;2;1;3;2,5;5;5;4;4,Reject,0,0,0,yes,9/27/18,University of Southern California,30,66,11,9/27/18,0,0,0,0,0,0,112,24,4,5,-1
2640,ICLR,2019,Clinical Risk: wavelet reconstruction networks for marked point processes,Jeremy C. Weiss,jeremy.weiss@gmail.com,7;4;5,4;4;4,Reject,0,11,0,yes,9/27/18,Carnegie Mellon University,1,24,,9/27/18,1,1,1,0,0,0,251,24,7,15,-1
2641,ICLR,2019,Interpreting Layered Neural Networks via Hierarchical Modular Representation,Chihiro Watanabe,watanabe.chihiro@lab.ntt.co.jp,4;3;3,3;4;4,Reject,0,0,0,yes,9/27/18,NTT,-1,-1,,9/27/18,1,1,0,0,4,0,245,50,8,13,-1
2642,ICLR,2019,Learning Graph Representations by Dendrograms,Thomas Bonald;Bertrand Charpentier,thomas.bonald@telecom-paristech.fr;bertrand.charpentier@live.fr,4;5;5,4;4;3,Reject,0,3,0,yes,9/27/18,Télécom ParisTech;,478;-1,188;-1,10,7/13/18,4,0,0,0,4,0,3890;11,177;9,30;2,405;0,-1;-1
2643,ICLR,2019,Fast Binary Functional Search on Graph,Shulong Tan;Zhixin Zhou;Zhaozhuo Xu;Ping Li,laos1984@gmail.com;zhixin0825@gmail.com;zhaozhuoxu@gmail.com;pingli98@gmail.com,5;4,5;4,Reject,0,6,0,yes,9/27/18,";University of California, Los Angeles;;Rutgers University New Brunswick",-1;20;-1;34,-1;15;-1;172,10,9/27/18,0,0,0,0,0,0,17;1195;63;1040,10;173;14;86,2;16;4;20,0;37;2;78,-1;-1
2644,ICLR,2019,HyperGAN:  Exploring the Manifold of Neural Networks,Neale Ratzlaff;Li  Fuxin,ratzlafn@oregonstate.edu;lif@oregonstate.edu,5;6;4,4;5;3,Reject,2,5,0,yes,9/27/18,Oregon State University;Oregon State University,76;76,318;318,5;4,9/27/18,0,0,0,0,0,0,6;4,5;25,1;1,1;0,-1;-1
2645,ICLR,2019,Correction Networks: Meta-Learning for Zero-Shot Learning,R. Lily Hu;Caiming Xiong;Richard Socher,rlilyhu@gmail.com;cxiong@salesforce.com;rsocher@salesforce.com,7;4;4,4;5;4,Reject,0,7,1,yes,9/27/18,SalesForce.com;SalesForce.com;SalesForce.com,-1;-1;-1,-1;-1;-1,3;6,9/27/18,2,0,2,0,0,0,105;6230;52392,14;156;180,6;31;49,2;1054;8829,-1;-1
2646,ICLR,2019,NUTS: Network for Unsupervised Telegraphic Summarization,Chanakya Malireddy;Tirth Maniar;Sajal Maheshwari;Manish Shrivastava,chanakya.malireddy@gmail.com;tirthmaniar1998@gmail.com;sajalmaheshwari624@gmail.com;m.shrivastava@iiit.ac.in,4;4;4,4;4;4,Reject,0,3,0,yes,9/27/18,International Institute of Information Technology Hyderabad;;;International Institute of Information Technology Hyderabad,199;-1;-1;199,1103;-1;-1;1103,,9/27/18,0,0,0,0,0,0,1;21;8;1339,3;4;6;46,1;1;2;17,0;2;0;104,-1;-1
2647,ICLR,2019,The wisdom of the crowd: reliable deep reinforcement learning through ensembles of Q-functions,Daniel Elliott;Charles Anderson,daniel.elliott18@alumni.colostate.edu;chuck.anderson@colostate.edu,4;5;3,5;3;4,Reject,0,0,0,yes,9/27/18,Colorado State University;Colorado State University,314;314,356;356,,9/27/18,1,0,0,0,0,0,39;4527,9;139,3;27,3;395,-1;-1
2648,ICLR,2019,Co-manifold learning with missing data,Gal Mishne;Eric C. Chi;Ronald R. Coifman,gal.mishne@yale.edu;eric_chi@ncsu.edu;coifman.ronald@yale.edu,7;4;4,4;4;3,Reject,0,8,0,yes,9/27/18,Yale University;North Carolina State University;Yale University,62;89;62,12;275;12,,9/27/18,1,1,0,0,3,0,137;662;11057,20;40;209,8;12;44,12;82;951,-1;-1
2649,ICLR,2019,Training generative latent models  by variational f-divergence minimization,Mingtian Zhang;Thomas Bird;Raza Habib;Tianlin Xu;David Barber,mingtian.zhang.17@ucl.ac.uk;thomas.bird@cs.ucl.ac.uk;raza.habib@cs.ucl.ac.uk;t.xu12@lse.ac.uk;david.barber@ucl.ac.uk,6;5;5,3;4;3,Reject,0,3,0,yes,9/27/18,University College London;University College London;University College London;London School of Economics;University College London,50;50;50;478;50,16;16;16;26;16,5;1,9/27/18,3,2,0,1,0,1,115;540;517;33;3832,37;46;9;9;200,6;10;3;3;27,15;38;39;3;411,-1;-1
2650,ICLR,2019,Improved Gradient Estimators for Stochastic Discrete Variables,Evgeny Andriyash;Arash Vahdat;Bill Macready,eandriyash@dwavesys.com;avahdat@dwavesys.com;wgm@dwavesys.com,7;6;6,4;3;4,Reject,0,3,0,yes,9/27/18,D-Wave Systems;D-Wave Systems;D-Wave Systems,-1;-1;-1,-1;-1;-1,,9/27/18,0,0,0,0,0,0,462;897;103,28;37;3,10;15;1,23;104;5,-1;-1
2651,ICLR,2019,AutoLoss: Learning Discrete Schedule for Alternate Optimization,Haowen Xu;Hao Zhang;Zhiting Hu;Xiaodan Liang;Ruslan Salakhutdinov;Eric Xing,haowen.will.xu@gmail.com;hao@cs.cmu.edu;zhitingh@cs.cmu.edu;xiaodan1@cs.cmu.edu;rsalakhu@cs.cmu.edu;eric.xing@petuum.com,6;7;7,3;4;3,Accept (Poster),0,9,2,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Petuum Inc.,1;1;1;1;1;-1,24;24;24;24;24;-1,3;5;6,9/27/18,8,3,5,1,10,0,159;17619;3202;5919;67470;24050,18;1218;64;137;254;604,4;63;29;41;82;75,10;1029;366;693;7779;2652,-1;-1
2652,ICLR,2019,Spread Divergences,David Barber;Mingtian Zhang;Raza Habib;Thomas Bird,d.barber@cs.ucl.ac.uk;mingtian.zhang.17@ucl.ac.uk;raza.habib.15@ucl.ac.uk;thomas.bird.17@ucl.ac.uk,5;4;6,4;4;4,Reject,0,3,0,yes,9/27/18,University College London;University College London;University College London;University College London,50;50;50;50,16;16;16;16,5,9/27/18,1,0,0,0,4,0,3832;115;517;540,200;37;9;46,27;6;3;10,411;15;39;38,-1;-1
2653,ICLR,2019,Explicit Information Placement on Latent Variables using Auxiliary Generative Modelling Task,Nat Dilokthanakul;Nick Pawlowski;Murray Shanahan,n.dilokthanakul14@imperial.ac.uk;n.pawlowski16@imperial.ac.uk;m.shanahan@imperial.ac.uk,5;6;7,4;4;4,Reject,0,7,0,yes,9/27/18,Imperial College London;Imperial College London;Imperial College London,72;72;72,8;8;8,5,9/27/18,1,1,0,0,0,0,255;474;4811,8;26;166,4;12;38,24;41;455,-1;-1
2654,ICLR,2019,Convergence Properties of Deep Neural Networks on Separable Data,Remi Tachet des Combes;Mohammad Pezeshki;Samira Shabanian;Aaron Courville;Yoshua Bengio,remi.tachet@microsoft.com;mohammad.pezeshki@umontreal.ca;s.shabanian@gmail.com;aaron.courville@gmail.com;yoshua.umontreal@gmail.com,5;5;5,4;4;3,Reject,0,5,0,yes,9/27/18,Microsoft;University of Montreal;Microsoft;University of Montreal;University of Montreal,-1;123;-1;123;123,-1;108;-1;108;108,5;4;1;8,9/18/18,15,11,0,0,34,2,228;365;1666;60124;201719,14;20;11;203;807,8;5;4;64;147,24;34;129;7848;23989,-1;-1
2655,ICLR,2019,Meta-Learning with Individualized Feature Space for Few-Shot Classification,Chunrui Han;Shiguang Shan;Meina Kan;Shuzhe Wu;Xilin Chen,chunrui.han@vipl.ict.ac.cn;sgshan@ict.ac.cn;kanmeina@ict.ac.cn;shuzhe.wu@vipl.ict.ac.cn;xlchen@ict.ac.cn,5;5;3,4;4;3,Reject,0,0,0,yes,9/27/18,"Institute of Computing Technology, Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences",62;62;62;62;62,1103;1103;1103;1103;1103,6,9/27/18,1,1,0,0,0,1,101;14333;1990;210;14999,11;379;56;12;511,6;58;21;5;57,2;1721;272;17;1782,-1;-1
2656,ICLR,2019,Neural Regression Tree,Wenbo Zhao;Shahan Ali Memon;Bhiksha Raj;Rita Singh,wzhao1@andrew.cmu.ecu;samemon@cs.cmu.edu;bhikshar@cs.cmu.edu;rsingh@cs.cmu.edu,5;3;4,3;4;5,Reject,0,0,0,yes,9/27/18,;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,-1;1;1;1,-1;24;24;24,,9/27/18,3,1,1,0,1,0,13;6;8586;2225,5;8;316;157,3;1;46;22,0;0;794;171,-1;-1
2657,ICLR,2019,Learning Corresponded Rationales for Text Matching,Mo Yu;Shiyu Chang;Tommi S Jaakkola,shiyu.chang@ibm.com;yum@us.ibm.com;tommi@csail.mit.edu,6;4;3,4;4;5,Reject,0,3,0,yes,9/27/18,International Business Machines;International Business Machines;Massachusetts Institute of Technology,-1;-1;2,-1;-1;5,3,9/27/18,7,3,3,0,0,0,3529;3033;21955,71;111;292,26;28;69,452;404;2320,-1;-1
2658,ICLR,2019,Differentiable Expected BLEU for Text Generation,Wentao Wang;Zhiting Hu;Zichao Yang;Haoran Shi;Eric P. Xing,wwt10@pku.edu.cn;zhitinghu@gmail.com;yangtze2301@gmail.com;shr970423@gmail.com;epxing@cs.cmu.edu,4;4;6,4;5;4,Reject,2,0,0,yes,9/27/18,Peking University;Carnegie Mellon University;;;Carnegie Mellon University,24;1;-1;-1;1,27;24;-1;-1;24,3,9/27/18,1,1,1,1,0,1,144;3202;4293;201;24050,47;64;40;23;604,6;29;18;8;75,2;366;601;18;2652,-1;-1
2659,ICLR,2019,Cautious Deep Learning,Yotam Hechtlinger;Barnabas Poczos;Larry Wasserman,yhechtli@andrew.cmu.edu;bapoczos@cs.cmu.edu;larry@cmu.edu,4;7;4,3;2;5,Reject,0,5,0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1,24;24;24,,5/24/18,13,4,3,0,28,1,96;5728;8761,8;243;262,4;40;50,7;703;999,-1;-1
2660,ICLR,2019,Success at any cost: value constrained model-free continuous control,Steven Bohez;Abbas Abdolmaleki;Michael Neunert;Jonas Buchli;Nicolas Heess;Raia Hadsell,sbohez@google.com;aabdolmaleki@google.com;neunertm@google.com;buchli@google.com;heess@google.com;raia@google.com,6;7;5,4;4;4,Reject,0,11,0,yes,9/27/18,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,1,9/27/18,0,0,0,0,0,0,432;529;661;4142;11371;8138,32;45;33;129;104;63,11;10;14;34;37;25,31;56;39;245;1617;781,-1;-1
2661,ICLR,2019,Learning Hash Codes via Hamming Distance Targets,Martin Loncaric;Ryan Weber;Bowei Liu,martin@thehive.ai;ryan@thehive.ai;liubowei@gmail.com,6;4;4,3;3;5,Reject,4,6,0,yes,9/27/18,;;,-1;-1;-1,-1;-1;-1,,9/27/18,1,0,1,0,2,0,89;328;61,29;44;20,5;6;3,2;29;5,-1;-1
2662,ICLR,2019,Interactive Parallel Exploration for Reinforcement Learning in Continuous Action Spaces,Whiyoung Jung;Giseung Park;Youngchul Sung,wy.jung@kaist.ac.kr;gs.park@kaist.ac.kr;ycsung@kaist.ac.kr,7;6;4,4;4;4,Reject,0,5,0,yes,9/27/18,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,20;20;20,95;95;95,,9/27/18,0,0,0,0,0,0,1;1;1427,4;5;121,1;1;21,0;0;117,m;m
2663,ICLR,2019,A Proposed Hierarchy of Deep Learning Tasks,Joel Hestness;Sharan Narang;Newsha Ardalani;Heewoo Jun;Hassan Kianinejad;Md. Mostofa Ali Patwary;Yang Yang;Yanqi Zhou;Gregory Diamos;Kenneth Church,joel@baidu.com;sharan@baidu.com;ardalaninewsha@baidu.com;junheewoo@baidu.com;hassankianinejad@baidu.com;patwarymostofa@baidu.com;yangyang62@baidu.com;zhouyanqi@baidu.com;gregdiamos@baidu.com;kennethchurch@baidu.com,6;4;4,3;5;2,Reject,0,0,0,yes,9/27/18,Baidu;Baidu;Baidu;Baidu;Baidu;Baidu;Baidu;Baidu;Baidu;Baidu,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,3,9/27/18,0,0,0,0,0,0,3607;2527;210;224;92;1817;581;858;1955;12107,24;15;15;10;3;46;103;33;42;208,11;10;5;5;2;19;10;13;19;46,384;271;25;18;8;180;33;86;192;1069,-1;-1
2664,ICLR,2019,Multi-Grained Entity Proposal Network for Named Entity Recognition,Congying Xia;Chenwei Zhang;Tao Yang;Yaliang Li;Nan Du;Xian Wu;Wei Fan;Fenglong Ma;Philip S. Yu,cxia8@uic.edu;czhang99@uic.edu;tytaoyang@tencent.com;yaliangli@tencent.com;ndu@tencent.com;kevinxwu@tencent.com;davidwfan@tencent.com;fenglong@buffalo.edu;psyu@uic.edu,5;5;4,4;3;4,Reject,0,3,0,yes,9/27/18,"University of Illinois, Chicago;University of Illinois, Chicago;Tencent AI Lab;Tencent AI Lab;Tencent AI Lab;Tencent AI Lab;Tencent AI Lab;State University of New York, Buffalo;University of Illinois, Chicago",57;57;-1;-1;-1;-1;-1;81;57,255;255;-1;-1;-1;-1;-1;270;255,,9/27/18,1,0,0,0,0,0,90;573;269;1596;2136;2242;11864;820;60435,11;80;80;114;136;215;476;59;1578,4;12;8;18;24;23;47;16;111,10;34;28;164;179;160;1038;67;5773,-1;-1
2665,ICLR,2019,Massively Parallel Hyperparameter Tuning,Liam Li;Kevin Jamieson;Afshin Rostamizadeh;Ekaterina Gonina;Moritz Hardt;Ben Recht;Ameet Talwalkar,jamieson@cs.washington.edu;rostami@google.com;kgonina@google.com;hardt@berkeley.edu;brecht@berkeley.edu;talwalkar@cmu.edu,5;6;5,4;4;4,Reject,0,5,0,yes,9/27/18,University of Washington;Google;Google;University of California Berkeley;University of California Berkeley;Carnegie Mellon University,6;-1;-1;5;5;1,25;-1;-1;18;18;24,,2/15/18,39,17,18,2,54,8,195;1605;4296;335;7679;20165;6323,7;47;61;26;88;141;78,3;17;23;11;33;55;34,35;242;608;38;955;2651;762,-1;-1
2666,ICLR,2019,Dynamic Graph Representation Learning via Self-Attention Networks,Aravind Sankar;Yanhong Wu;Liang Gou;Wei Zhang;Hao Yang,asankar3@illinois.edu;yanwu@visa.com;ligou@visa.com;wzhan@visa.com;haoyang@visa.com,4;6;5,5;4;4,Reject,2,7,0,yes,9/27/18,"University of Illinois, Urbana Champaign;VISA;VISA;VISA;VISA",3;-1;-1;-1;-1,37;-1;-1;-1;-1,10,9/27/18,22,13,6,1,2,4,106;471;656;43288;6287,21;83;58;4567;629,6;10;13;77;36,9;39;45;2377;463,-1;-1
2667,ICLR,2019,Denoise while Aggregating: Collaborative Learning in Open-Domain Question Answering,Haozhe Ji;Yankai Lin;Zhiyuan Liu;Maosong Sun,jihaozhe@gmail.com;mrlyk423@gmail.com;liuzy@tsinghua.edu.cn;sms@tsinghua.edu.cn,4;6;5,4;4;4,Reject,0,0,0,yes,9/27/18,Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University,8;8;8;8,30;30;30;30,,9/27/18,0,0,0,0,0,0,75;2361;6842;8349,4;34;134;264,2;14;36;42,16;492;1140;1348,-1;-1
2668,ICLR,2019,Are Generative Classifiers More Robust to Adversarial Attacks?,Yingzhen Li;John Bradshaw;Yash Sharma,yl494@cam.ac.uk;jab255@cam.ac.uk;ysharma1126@gmail.com,4;6;4;8,4;3;5;3,Reject,0,13,2,yes,9/27/18,University of Cambridge;University of Cambridge;The Cooper Union,71;71;-1,2;2;-1,5;4,2/19/18,18,8,6,3,13,3,984;-1;-1,43;-1;-1,13;-1;-1,137;0;0,-1;-1
2669,ICLR,2019,Sentence Encoding with Tree-Constrained Relation Networks,Lei Yu;Cyprien de Masson d'Autume;Chris Dyer;Phil Blunsom;Lingpeng Kong;Wang Ling,leiyu@google.com;cyprien@google.com;cdyer@google.com;pblunsom@google.com;lingpenk@google.com;lingwang@google.com,5;5;3,4;4;4,Reject,0,4,0,yes,9/27/18,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,3,9/27/18,1,0,0,0,15,0,167;74;21134;11400;1047;15,88;6;231;143;31;8,8;4;59;47;14;2,6;10;3164;1326;106;0,-1;-1
2670,ICLR,2019,Exploiting Cross-Lingual Subword Similarities in Low-Resource Document Classification,Mozhi Zhang;Yoshinari Fujinuma;Jordan Boyd-Graber,mozhi@cs.umd.edu;yoshinari.fujinuma@colorado.edu;jbg@umiacs.umd.edu,6;4;6,3;3;4,Reject,0,4,0,yes,9/27/18,"University of Maryland, College Park;University of Colorado, Boulder;University of Maryland, College Park",12;44;12,69;100;69,6,9/27/18,2,2,0,0,0,0,42;12;1676,5;7;74,2;2;18,6;1;166,-1;-1
2671,ICLR,2019,Exploiting Environmental Variation to Improve Policy Robustness in  Reinforcement Learning,Siddharth Mysore;Robert Platt;Kate Saenko,sidmys@bu.edu;rplatt@ccs.neu.edu;saenko@bu.edu,5;3;6,3;4;4,Reject,0,3,0,yes,9/27/18,Boston University;Northeastern University;Boston University,65;16;65,70;839;70,8,9/27/18,0,0,0,0,0,0,3;3668;17040,3;169;177,1;34;56,0;242;2368,-1;-1
2672,ICLR,2019,DOMAIN ADAPTATION VIA DISTRIBUTION AND REPRESENTATION MATCHING: A CASE STUDY ON TRAINING DATA SELECTION VIA REINFORCEMENT LEARNING,Miaofeng Liu;Yan Song;Hongbin Zou;Tong Zhang,water3er@gmail.com;clksong@gmail.com;hbzou@xdu.edu.cn;bradymzhang@tencent.com,4;7;5,2;3;4,Reject,0,1,1,yes,9/27/18,;Tencent AI Lab;Shandong University;Tencent AI Lab,-1;-1;4;-1,-1;-1;3;-1,3,9/27/18,0,0,0,0,0,0,15;36;236;3142,7;16;49;216,3;4;9;30,2;0;5;312,-1;-1
2673,ICLR,2019,Assumption Questioning: Latent Copying and Reward Exploitation in Question Generation,Tom Hosking;Sebastian Riedel,thomas.hosking.17@ucl.ac.uk;sebastian.riedel@gmail.com,3;4;5,4;4;4,Reject,0,3,0,yes,9/27/18,University College London;University College London,50;50,16;16,3;4,9/27/18,0,0,0,0,0,0,9;5809,3;230,1;35,1;890,-1;-1
2674,ICLR,2019,Using Word Embeddings to Explore the Learned Representations of Convolutional Neural Networks,Dhanush Dharmaretnam;Chris Foster;Alona Fyshe,dhanush987@gmail.com;chris.james.foster@gmail.com;alona@ualberta.ca,4;3;4,4;4;2,Reject,0,0,0,yes,9/27/18,University of Victoria;University of Victoria;University of Alberta,169;169;99,346;346;119,3;4,9/27/18,0,0,0,0,0,0,11;387;829,8;43;47,2;6;13,0;32;87,-1;-1
2675,ICLR,2019,Learning to control self-assembling morphologies: a study of generalization via modularity,Deepak Pathak;Chris Lu;Trevor Darrell;Philip Isola;Alexei A. Efros,pathak@berkeley.edu;chris.lu@berkeley.edu;trevor@eecs.berkeley.edu;phillip.isola@gmail.com;efros@eecs.berkeley.edu,4;4;7,4;3;3,Reject,2,8,0,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley;;University of California Berkeley,5;5;5;-1;5,18;18;18;-1;18,10,9/27/18,8,3,2,0,14,0,4138;1277;88648;12385;36776,40;20;558;73;192,14;8;111;27;77,546;130;11396;2159;4579,-1;-1
2676,ICLR,2019,End-to-End Hierarchical Text Classification with Label Assignment Policy,Yuning Mao;Jingjing Tian;Jiawei Han;Xiang Ren,yuningm2@illinois.edu;tianjj97@pku.edu.cn;hanj@illinois.edu;xiangren@usc.edu,5;4;4,4;5;4,Reject,1,8,0,yes,9/27/18,"University of Illinois, Urbana Champaign;Peking University;University of Illinois, Urbana Champaign;University of Southern California",3;24;3;30,37;27;37;66,,9/27/18,3,2,1,0,0,0,70;91;79511;2368,17;26;1083;165,5;4;132;24,2;4;8766;307,-1;-1
2677,ICLR,2019,SynonymNet: Multi-context Bilateral Matching for Entity Synonyms,Chenwei Zhang;Yaliang Li;Nan Du;Wei Fan;Philip S. Yu,czhang99@uic.edu;yaliangli@tencent.com;ndu@tencent.com;davidwfan@tencent.com;psyu@uic.edu,5;7;4,4;5;4,Reject,0,4,0,yes,9/27/18,"University of Illinois, Chicago;Tencent AI Lab;Tencent AI Lab;Tencent AI Lab;University of Illinois, Chicago",57;-1;-1;-1;57,255;-1;-1;-1;255,,9/27/18,1,1,1,0,8,1,573;1596;2136;11864;60435,80;114;136;476;1578,12;18;24;47;111,34;164;179;1038;5773,-1;-1
2678,ICLR,2019,TopicGAN: Unsupervised Text Generation from Explainable Latent Topics,Yau-Shian Wang;Yun-Nung Chen;Hung-Yi Lee,king6101@gmail.com;y.v.chen@ieee.org;tlkagkb93901106@gmail.com,4;4;5,2;4;4,Reject,0,3,0,yes,9/27/18,National Taiwan University;National Taiwan University;,85;85;-1,197;197;-1,3;4;5,9/27/18,0,0,0,0,0,0,14;1725;1094,4;101;134,1;22;17,0;151;73,-1;-1
2679,ICLR,2019,ON THE EFFECTIVENESS OF TASK GRANULARITY FOR TRANSFER LEARNING,Farzaneh Mahdisoltani;Guillaume Berger;Waseem Gharbieh;David Fleet;Roland Memisevic,farzaneh@cs.toronto.edu;guillaume.berger@twentybn.com;waseem.gharbieh@twentybn.com;fleet@cs.toronto.edu;roland.memisevic@twentybn.com,5;5;5,4;4;4,Reject,0,0,0,yes,9/27/18,"Department of Computer Science, University of Toronto;Twenty Billion Neurons;Twenty Billion Neurons;Department of Computer Science, University of Toronto;Twenty Billion Neurons",18;-1;-1;18;-1,22;-1;-1;22;-1,6,4/24/18,12,4,0,0,0,1,418;57;57;1329;5353,12;10;11;76;105,5;3;4;15;30,86;20;14;64;493,-1;-1
2680,ICLR,2019,Neural Networks for Modeling Source Code Edits,Rui Zhao;David Bieber;Kevin Swersky;Daniel Tarlow,oahziur@gmail.com;dbieber@google.com;kswersky@google.com;dtarlow@google.com,5;6;6;6,4;2;4;4,Reject,0,5,0,yes,9/27/18,;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,5;8,9/27/18,5,3,0,0,0,0,8050;27;5656;2516,608;4;52;68,38;2;23;23,502;3;866;303,-1;-1
2681,ICLR,2019,COMPOSITION AND DECOMPOSITION OF GANS,Yeu-Chern Harn;Zhenghao Chen;Vladimir Jojic,ycharn@cs.unc.edu;chen.zhenghao@gmail.com;vjojic@gmail.com,4;5;4,5;5;4,Reject,0,5,0,yes,9/27/18,"University of North Carolina, Chapel Hill;Calico Labs;University of North Carolina, Chapel Hill",76;-1;76,56;-1;56,5;4,9/27/18,0,0,0,0,4,0,17;1230;2378,7;21;60,2;7;20,0;111;131,-1;-1
2682,ICLR,2019,"Unicorn: Continual learning with a universal, off-policy agent",Daniel J. Mankowitz;Augustin Žídek;André Barreto;Dan Horgan;Matteo Hessel;John Quan;Junhyuk Oh;Hado van Hasselt;David Silver;Tom Schaul,dmankowitz@google.com;augustinzidek@google.com;andrebarreto@google.com;horgan@google.com;mtthss@google.com;johnquan@google.com;junhyuk@google.com;hado@google.com;davidsilver@google.com;schaul@google.com,4;5;6,5;4;4,Reject,0,3,0,yes,9/27/18,Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,,2/22/18,27,15,9,1,34,2,547;123;398;1371;2264;3225;1357;5507;42155;8406,29;8;17;9;26;16;24;50;158;83,12;4;6;8;13;11;13;21;56;30,32;8;37;226;365;542;134;910;5862;1123,-1;-1
2683,ICLR,2019,Understanding the Asymptotic Performance of Model-Based RL Methods,William Whitney;Rob Fergus,wfwhitney@gmail.com;fergus@cs.nyu.edu,5;6;4;2,3;4;3;4,Reject,0,5,0,yes,9/27/18,New York University;New York University,26;26,27;27,,9/27/18,2,2,0,0,0,0,695;52077,10;127,5;61,48;6432,-1;-1
2684,ICLR,2019,Dynamic Pricing on E-commerce Platform with Deep Reinforcement Learning,Jiaxi Liu;Yidong Zhang;Xiaoqing Wang;Yuming Deng;Xingyu Wu;Miaolan Xie,galiliu.ljx@alibaba-inc.com;tanfu.zyd@alibaba-inc.com;robin.wxq@alibaba-inc.com;yuming.dym@alibaba-inc.com;zhuyang.wxy@alibaba-inc.com;miaolan.xml@alibaba-inc.com,4;4;4,4;5;3,Reject,2,4,0,yes,9/27/18,Alibaba Group;Alibaba Group;Alibaba Group;Alibaba Group;Alibaba Group;Alibaba Group,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,,9/27/18,1,0,0,0,0,0,45;267;2133;25;76;-1,43;25;151;8;16;-1,4;6;17;2;5;-1,3;11;160;2;0;0,-1;-1
2685,ICLR,2019,Coupled Recurrent Models for Polyphonic Music Composition,John Thickstun;Zaid Harchaoui;Dean P. Foster;Sham M. Kakade,thickstn@cs.washington.edu;zaid@uw.edu;sham@cs.washington.edu;dean@foster.net,7;4;3,3;4;4,Reject,1,4,0,yes,9/27/18,"University of Washington;University of Washington, Seattle;University of Washington;",6;6;6;-1,25;25;25;-1,5,9/27/18,6,0,0,0,6,0,133;4823;6183;13520,16;57;197;197,3;35;41;57,26;680;685;1968,-1;-1
2686,ICLR,2019,From Nodes to Networks: Evolving Recurrent Neural Networks,Aditya Rawal;Jason Liang;Risto Miikkulainen,aditya@cs.utexas.edu;jasonzliang@utexas.edu;risto@cs.utexas.edu,5;4;4,4;4;4,Reject,0,0,0,yes,9/27/18,"University of Texas, Austin;University of Texas, Austin;University of Texas, Austin",22;22;22,49;49;49,3,3/12/18,28,9,6,1,18,4,671;-1;12696,48;-1;441,13;-1;49,37;0;1318,-1;-1
2687,ICLR,2019,Lyapunov-based Safe Policy Optimization,Yinlam Chow;Ofir Nachum;Mohammad Ghavamzadeh;Edgar Guzman-Duenez,yinlamchow@google.com;ofirnachum@google.com;mohammad.ghavamzadeh@inria.fr;duenez@google.com,6;5;6;8,2;3;2;3,Reject,0,6,0,yes,9/27/18,Google;Google;INRIA;Google,-1;-1;-1;-1,-1;-1;-1;-1,,9/27/18,20,4,2,1,9,1,745;1040;2981;411,50;41;113;27,14;15;31;12,86;155;331;24,-1;-1
2688,ICLR,2019,Composing Entropic Policies using Divergence Correction,Jonathan J Hunt;Andre Barreto;Timothy P Lillicrap;Nicolas Heess,jjhunt@google.com;andrebarreto@google.com;countzero@google.com;heess@google.com,4;5;7,3;4;3,Reject,0,9,0,yes,9/27/18,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,,9/27/18,1,1,1,0,0,1,3447;282;23537;11371,22;36;74;104,9;10;39;37,800;23;2880;1617,-1;-1
2689,ICLR,2019,Accelerated Gradient Flow for Probability Distributions,Amirhossein Taghvaei;Prashant G. Mehta,amirhoseintghv@gmail.com;mehtapg@illinois.edu,4;5;6,4;3;4,Reject,0,7,0,yes,9/27/18,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",3;3,37;37,9,9/27/18,9,6,4,2,3,1,122;1667,30;138,7;22,9;110,-1;-1
2690,ICLR,2019,Constraining Action Sequences with Formal Languages for Deep Reinforcement Learning,Dong Xu;Eleanor Quint;Zeynep Hakguder;Haluk Dogan;Stephen Scott;Matthew Dwyer,dx@virginia.edu;pquint@cse.unl.edu;zeynep.hakguder@huskers.unl.edu;haluk.dogan@huskers.unl.edu;sscott@cse.unl.edu;matthewbdwyer@virginia.edu,5;4;3,4;3;4,Reject,0,1,0,yes,9/27/18,"University of Virginia;University of Nebraska, Lincoln;University of Nebraska, Lincoln;University of Nebraska, Lincoln;University of Nebraska, Lincoln;University of Virginia",65;228;228;228;228;65,113;337;337;337;337;113,,9/27/18,0,0,0,0,0,0,2696;1;12;96;2172;11,408;2;6;9;245;11,22;1;2;4;22;2,110;0;0;9;170;0,-1;-1
2691,ICLR,2019,What Would pi* Do?: Imitation Learning via Off-Policy Reinforcement Learning,Siddharth Reddy;Anca D. Dragan;Sergey Levine,sgr@berkeley.edu;anca@berkeley.edu;svlevine@eecs.berkeley.edu,5;6;5,3;4;4,Reject,3,12,0,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,18;18;18,4,9/27/18,0,0,0,0,0,0,99;3627;24386,16;126;309,6;31;73,8;285;3167,-1;-1
2692,ICLR,2019,Boosting Trust Region Policy Optimization by Normalizing flows Policy,Yunhao Tang;Shipra Agrawal,yt2541@columbia.edu;sa3305@columbia.edu,4;6;4,4;4;4,Reject,0,3,0,yes,9/27/18,Columbia University;Columbia University,15;15,14;14,,9/27/18,8,5,3,0,5,0,93;2650,29;83,7;24,4;391,-1;-1
2693,ICLR,2019,Architecture Compression,Anubhav Ashok,anubhava@alumni.cmu.edu,4;6;4,3;4;4,Reject,0,5,0,yes,9/27/18,Carnegie Mellon University,1,24,,9/27/18,0,0,0,0,2,0,104,7,3,15,-1
2694,ICLR,2019,Super-Resolution via Conditional Implicit Maximum Likelihood Estimation,Ke Li*;Shichong Peng*;Jitendra Malik,ke.li@eecs.berkeley.edu;shichong.peng@mail.utoronto.ca;malik@eecs.berkeley.edu,5;6;6,3;5;5,Reject,6,5,0,yes,9/27/18,University of California Berkeley;Toronto University;University of California Berkeley,5;18;5,18;22;18,,9/27/18,2,0,2,0,0,0,6190;9;69075,781;4;429,36;2;115,290;0;7768,-1;-1
2695,ICLR,2019,A Better Baseline for Second Order Gradient Estimation in Stochastic Computation Graphs,Jingkai Mao;Jakob Foerster;Tim Rocktäschel;Gregory Farquhar;Maruan Al-Shedivat;Shimon Whiteson,jingkai.mao@gmail.com;jakobfoerster@gmail.com;tim.rocktaeschel@gmail.com;gregory.farquhar@cs.ox.ac.uk;alshedivat@cs.cmu.edu;shimon.whitesone@cs.ox.ac.uk,3;6;5;6,4;3;4;3,Reject,0,5,0,yes,9/27/18,University of Oxford;University of Oxford;Facebook AI Research;University of Oxford;Carnegie Mellon University;University of Oxford,50;50;-1;50;1;50,1;1;-1;1;24;1,6;10,9/27/18,1,0,1,0,0,0,5;2059;2249;923;739;5304,3;58;48;19;35;203,1;19;22;9;13;38,0;332;278;168;79;573,-1;-1
2696,ICLR,2019,Reinforced Pipeline Optimization: Behaving Optimally with Non-Differentiabilities,Aijun Bai;Dongdong Chen;Gang Hua;Lu Yuan,aijunbai@gmail.com;cd722522@mail.ustc.edu.cn;ganghua@gmail.com;luyuan@microsoft.com,4;5;3,5;2;4,Reject,0,0,0,yes,9/27/18,Microsoft;University of Science and Technology of China;Wormpex AI Research;Microsoft,-1;478;-1;-1,-1;132;-1;-1,2,9/27/18,0,0,0,0,0,0,164;42;7737;3040,27;12;172;136,8;4;48;25,10;6;866;322,-1;-1
2697,ICLR,2019,An Active Learning Framework for Efficient Robust Policy Search,Sai Kiran Narayanaswami;Nandan Sudarsanam;Balaraman Ravindran,saikirann94@gmail.com;nandan@iitm.ac.in;ravi@cse.iitm.ac.in,5;6;5,3;3;4,Reject,0,4,0,yes,9/27/18,;Indian Institute of Technology Madras;Indian Institute of Technology Madras,-1;153;153,-1;625;625,,9/27/18,6,0,0,0,6,0,0;83;2434,2;20;234,0;3;27,0;8;197,-1;-1
2698,ICLR,2019,Model-Agnostic Meta-Learning for Multimodal Task Distributions,Risto Vuorio;Shao-Hua Sun;Hexiang Hu;Joseph J. Lim,vuoristo@gmail.com;shaohuas@usc.edu;hexiangh@usc.edu;limjj@usc.edu,5;3;5,3;5;4,Reject,0,7,0,yes,9/27/18,;University of Southern California;University of Southern California;University of Southern California,-1;30;30;30,-1;66;66;66,6,9/27/18,1,0,0,0,0,0,32;164;508;2922,6;36;27;51,3;6;11;20,4;15;64;264,-1;-1
2699,ICLR,2019,Teaching to Teach by Structured Dark Knowledge,Ziliang Chen;Keze Wang;Liang Lin,c.ziliang@yahoo.com;kezewang@gmail.com;linliang@ieee.org,4;3;6,1;4;5,Reject,0,0,0,yes,9/27/18,"SUN YAT-SEN UNIVERSITY;University of California, Los Angeles;SUN YAT-SEN UNIVERSITY",478;20;478,352;15;352,,9/27/18,0,0,0,0,0,0,190;809;606,28;29;13,6;13;3,22;62;38,-1;-1
2700,ICLR,2019,Constrained Bayesian Optimization for Automatic Chemical Design,Ryan-Rhys Griffiths;José Miguel Hernández-Lobato,rrg27@cam.ac.uk;jmh233@cam.ac.uk,3;4;5,4;3;4,Reject,0,0,0,yes,9/27/18,University of Cambridge;University of Cambridge,71;71,2;2,11,9/16/17,30,12,6,0,0,2,47;3763,9;114,3;28,3;423,-1;-1
2701,ICLR,2019,Transfer Value or Policy? A Value-centric Framework Towards Transferrable Continuous Reinforcement Learning,Xingchao Liu;Tongzhou Mu;Hao Su,liuxc1996@gmail.com;t3mu@eng.ucsd.edu;haosu@eng.ucsd.edu,5;4;5,3;4;2,Reject,0,10,0,yes,9/27/18,"Beihang University;University of California, San Diego;University of California, San Diego",115;11;11,658;31;31,6,9/27/18,0,0,0,0,0,0,17;15;68,11;5;37,2;3;5,2;0;2,-1;-1
2702,ICLR,2019,Understanding GANs via Generalization Analysis for Disconnected Support,Masaaki Imaizumi;Kenji Fukumizu,insou11@hotmail.com;fukumizu@ism.ac.jp,5;6;6,4;4;3,Reject,0,4,0,yes,9/27/18,"The Institute of Statistical Mathematics, Japan;The Institute of Statistical Mathematics, Japan",-1;-1,-1;-1,5;4;8,9/27/18,0,0,0,0,0,0,83;5697,21;194,5;35,11;758,-1;-1
2703,ICLR,2019,Ergodic Measure Preserving Flows,Yichuan Zhang;José Miguel Hernández-Lobato;Zoubin Ghahramani,yichuan.zhang@eng.cam.ac.uk;jmh233@cam.ac.uk;zoubin@eng.cam.ac.uk,5;5;4,4;3;5,Reject,4,16,0,yes,9/27/18,University of Cambridge;University of Cambridge;University of Cambridge,71;71;71,2;2;2,5;11,9/27/18,3,3,2,0,0,1,89;26;42621,31;5;463,4;2;91,3;5;5079,-1;-1
2704,ICLR,2019,Sparse Binary Compression: Towards Distributed Deep Learning with minimal Communication,Felix Sattler;Simon Wiedemann;Klaus-Robert Müller;Wojciech Samek,felix.sattler@hhi.fraunhofer.de;simon.wiedemann@hhi.fraunhofer.de;klaus-robert.mueller@tu-berlin.de;wojciech.samek@hhi.fraunhofer.de,6;3;5,4;4;4,Reject,0,2,0,yes,9/27/18,Fraunhofer IIS;Fraunhofer IIS;TU Berlin;Fraunhofer IIS,-1;-1;106;-1,-1;-1;92;-1,,5/22/18,37,18,18,0,10,7,123;311;38100;5368,12;43;522;163,4;11;82;28,14;21;3869;504,-1;-1
2705,ICLR,2019,"S-System, Geometry, Learning, and Optimization: A Theory of Neural Networks",Shuai Li;Kui Jia,lishuai918@gmail.com;kuijia@scut.edu.cn,4;4,2;1,Reject,0,0,0,yes,9/27/18,;South China University of Technology,-1;478,-1;576,1,9/27/18,0,0,0,0,0,0,7660;2777,515;75,47;25,282;397,-1;-1
2706,ICLR,2019,Lorentzian Distance Learning,Marc T Law;Jake Snell;Richard S Zemel,law@cs.toronto.edu;jsnell@cs.toronto.edu;zemel@cs.toronto.edu,6;5;5,4;4;4,Reject,0,9,0,yes,9/27/18,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",18;18;18,22;22;22,,9/27/18,7,3,2,0,0,1,305;1505;21440,34;12;208,8;6;52,24;382;2485,-1;-1
2707,ICLR,2019,W2GAN: RECOVERING AN OPTIMAL TRANSPORT MAP WITH A GAN,Leygonie Jacob*;Jennifer She*;Amjad Almahairi;Sai Rajeswar;Aaron Courville,jacob.leygonie@gmail.com;jennifershe123@gmail.com;amjadmahayri@gmail.com;rajsai24@gmail.com;aaron.courville@gmail.com,6;3;4,3;4;3,Reject,0,9,0,yes,9/27/18,University of Oxford;Stanford University;Element AI;University of Montreal;University of Montreal,50;4;-1;123;123,1;3;-1;108;108,5;4,9/27/18,2,0,0,0,0,0,2;91;1991;634;60124,1;11;11;16;203,1;5;7;7;64,0;9;170;116;7848,-1;-1
2708,ICLR,2019,A Differentiable Self-disambiguated Sense Embedding Model via Scaled Gumbel Softmax,Fenfei Guo;Mohit Iyyer;Leah Findlater;Jordan Boyd-Graber,fenfeigo@cs.umd.edu;miyyer@cs.umass.edu;leahkf@uw.edu;jbg@umiacs.umd.edu,6;7;6,5;3;4,Reject,0,8,0,yes,9/27/18,"University of Maryland, College Park;University of Massachusetts, Amherst;University of Washington, Seattle;University of Maryland, College Park",12;30;6;12,69;191;25;69,,9/27/18,1,0,1,0,0,0,85;5718;4168;1973,9;46;129;75,5;15;31;19,2;1041;424;149,-1;-1
2709,ICLR,2019,On the Computational Inefficiency of Large Batch Sizes for Stochastic Gradient Descent,Noah Golmant;Nikita Vemuri;Zhewei Yao;Vladimir Feinberg;Amir Gholami;Kai Rothauge;Michael Mahoney;Joseph Gonzalez,noah.golmant@berkeley.edu;nikitavemuri@berkeley.edu;zheweiy@berkeley.edu;vladf@berkeley.edu;amirgh@berkeley.edu;kai.rothauge@berkeley.edu;mmahoney@stat.berkeley.edu;jegonzal@cs.berkeley.edu,5;8;5,3;4;3,Reject,0,6,0,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5;5;5;5;5,18;18;18;18;18;18;18;18,3;2,9/27/18,24,15,2,2,0,1,115;72;274;89;588;68;11825;6356,7;6;27;6;50;13;230;72,2;3;10;3;13;5;45;26,10;8;21;11;41;5;1118;1005,-1;-1
2710,ICLR,2019,Three continual learning scenarios and a case for generative replay,Gido M. van de Ven;Andreas S. Tolias,gidovandeven@gmail.com;astolias@bcm.edu,4;4;6,4;4;5,Reject,0,6,2,yes,9/27/18,Baylor College of Medicine;Baylor College of Medicine,-1;-1,-1;-1,5,9/27/18,0,0,0,0,0,0,307;5699,8;167,6;39,26;490,-1;-1
2711,ICLR,2019,A More Globally Accurate Dimensionality Reduction Method Using Triplets,Ehsan Amid;Manfred K. Warmuth,eamid@ucsc.edu;manfred@ucsc.edu,5;6;6,4;5;3,Reject,0,8,0,yes,9/27/18,University of Southern California;University of Southern California,30;30,66;66,,3/1/18,7,0,2,0,0,0,134;11143,24;251,7;52,13;1216,m;m
2712,ICLR,2019,Learning Kolmogorov Models for Binary Random Variables,Hadi Ghauch;Hossein S. Ghadikolaei;Mikael Skoglund;Carlo Fischione,ghauch@kth.se;hshokri@kth.se;skoglund@kth.se;carlofi@kth.se,5;5;8,4;2;4,Reject,0,5,0,yes,9/27/18,"KTH Royal Institute of Technology, Stockholm, Sweden;KTH Royal Institute of Technology, Stockholm, Sweden;KTH Royal Institute of Technology, Stockholm, Sweden;KTH Royal Institute of Technology, Stockholm, Sweden",140;140;140;140,173;173;173;173,,6/6/18,1,0,1,0,5,0,302;813;5555;3822,39;46;584;283,8;13;34;30,18;51;420;244,-1;-1
2713,ICLR,2019,Laplacian Smoothing Gradient Descent,Stanley J. Osher;Bao Wang;Penghang Yin;Xiyang Luo;Minh Pham;Alex T. Lin,sjo@math.ucla.edu;wangbaonj@gmail.com;yph@g.ucla.edu;xylmath@gmail.com;minhrose@ucla.edu;atlin@math.ucla.edu,5;6;6,4;4;4,Reject,4,7,1,yes,9/27/18,"University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles;;University of California, Los Angeles;University of California, Los Angeles",20;20;20;-1;20;20,15;15;15;-1;15;15,9;8,6/17/18,15,7,6,0,37,3,57177;547;540;145;270;151,560;68;34;22;37;55,94;13;14;8;9;7,5065;14;36;14;21;5,-1;-1
2714,ICLR,2019,Language Modeling Teaches You More Syntax than Translation Does: Lessons Learned Through Auxiliary Task Analysis,Kelly W. Zhang;Samuel R. Bowman,kellywzhang@seas.harvard.edu;bowman@nyu.edu,6;5;7,4;4;4,Reject,0,7,0,yes,9/27/18,Harvard University;New York University,39;26,6;27,3;6,9/26/18,26,9,4,1,14,1,516;5889,28;80,12;27,24;1209,-1;-1
2715,ICLR,2019,Conscious Inference for Object Detection,Jiahuan Zhou;Nikolaos Karianakis;Ying Wu;Gang Hua,zhoujh09@gmail.com;nikolaos.karianakis@microsoft.com;yingwu@eecs.northwestern.edu;ganghua@gmail.com,4;6;4,4;4;5,Reject,0,3,0,yes,9/27/18,Northwestern University;Microsoft;Northwestern University;Wormpex AI Research,44;-1;44;-1,20;-1;20;-1,2,9/27/18,0,0,0,0,0,0,121;89;22555;46,23;15;1552;26,5;6;67;3,10;4;1447;4,-1;-1
2716,ICLR,2019,Learning Gibbs-regularized GANs with variational discriminator reparameterization,Nicholas Rhinehart;Anqi Liu;Kihyuk Sohn;Paul Vernaza,nrhineha@cs.cmu.edu;anqiliu@caltech.edu;ksohn@nec-labs.com;pvernaza@nec-labs.com,5;5;4,5;3;4,Reject,0,4,0,yes,9/27/18,Carnegie Mellon University;California Institute of Technology;NEC-Labs;NEC-Labs,1;140;-1;-1,24;3;-1;-1,5;4;10,9/27/18,0,0,0,0,0,0,378;279;3569;812,21;46;49;30,9;9;20;13,42;16;568;97,-1;-1
2717,ICLR,2019,DynCNN: An Effective Dynamic Architecture on Convolutional Neural Network for Surveillance Videos,De-Qin Gao;Ping-Chen Tsai;Shanq-Jang Ruan,b10113120@gmail.com;pctsainb@gmail.com;sjruan@mail.ntust.edu.tw,3;4;4,4;4;3,Reject,2,0,0,yes,9/27/18,Facebook;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology,-1;39;39,-1;44;44,,9/27/18,0,0,0,0,0,0,0;6;1037,2;4;150,0;2;16,0;0;81,-1;-1
2718,ICLR,2019,Expressiveness in Deep Reinforcement Learning,Xufang Luo;Qi Meng;Di He;Wei Chen;Yunhong Wang;Tie-Yan Liu,luoxufang@buaa.edu.cn;meq@microsoft.com;dihe@microsoft.com;wche@microsoft.com;yhwang@buaa.edu.cn;tyliu@microsoft.com,6;4;4,4;3;4,Reject,0,7,0,yes,9/27/18,Beihang University;Microsoft;Microsoft;Microsoft;Beihang University;Microsoft,115;-1;-1;-1;115;-1,658;-1;-1;-1;658;-1,,9/27/18,0,0,0,0,0,0,3;280;2539;722;8758;13277,5;57;258;80;387;366,1;11;27;15;41;51,0;7;109;40;646;1718,-1;-1
2719,ICLR,2019,Exploring Curvature Noise in Large-Batch Stochastic Optimization,Yeming Wen;Kevin Luk;Maxime Gazeau;Guodong Zhang;Harris Chan;Jimmy Ba,ywen@cs.toronto.edu;kevin.luk@borealisai.com;maxime.gazeau@borealisai.com;gdzhang.cs@gmail.com;hchan@cs.toronto.edu;jba@cs.toronto.edu,5;6;5,4;4;5,Reject,0,20,0,yes,9/27/18,"Department of Computer Science, University of Toronto;Borealis AI;Borealis AI;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",18;-1;-1;18;18;18,22;-1;-1;22;22;22,8,9/27/18,0,0,0,0,0,0,135;200;59;1291;22;51870,10;36;19;16;12;52,4;10;5;11;3;21,10;11;4;214;4;8479,-1;-1
2720,ICLR,2019,Layerwise Recurrent Autoencoder for General Real-world Traffic Flow Forecasting,Peize Zhao;Danfeng Cai;Shaokun Zhang;Feng Chen;Zhemin Zhang;Cheng Wang;Jonathan Li,zhaopeize@sensetime.com;caidanfeng@sensetime.com;zhangshaokun@sensetime.com;chenfeng@xmu.edu.cn;zhangzhemin@xmu.edu.cn;cwang@xmu.edu.cn;junli@xmu.edu.cn,4;5;3,3;3;4,Reject,1,3,0,yes,9/27/18,SenseTime Group Limited;SenseTime Group Limited;SenseTime Group Limited;Xiamen University;Xiamen University;Xiamen University;Xiamen University,-1;-1;-1;62;62;62;62,-1;-1;-1;12;12;12;12,1,9/27/18,1,0,0,0,0,0,4;162;52;7992;122;1754;6237,4;7;19;350;22;339;382,1;6;4;49;7;18;41,0;10;2;442;5;148;351,-1;-1
2721,ICLR,2019,An experimental study of layer-level training speed and its impact on generalization,Simon Carbonnelle;Christophe De Vleeschouwer,simon.carbonnelle@uclouvain.be;christophe.devleeschouwer@uclouvain.be,6;5;5,3;4;2,Reject,0,10,0,yes,9/27/18,UCL;UCL,261;261,16;16,8,9/27/18,0,0,0,0,0,0,5;2372,7;168,1;26,1;184,-1;-1
2722,ICLR,2019,DEEP ADVERSARIAL FORWARD MODEL,Morgan Funtowicz;Tomi Silander;Arnaud Sors;Julien Perez,morgan.funtowicz@naverlabs.com;tomi.silander@naverlabs.com;arnaud.sors@naverlabs.com;julien.perez@naverlabs.com,4;4;4,5;5;4,Reject,0,0,0,yes,9/27/18,Naver Labs Europe;Naver Labs Europe;Naver Labs Europe;Naver Labs Europe,-1;-1;-1;-1,-1;-1;-1;-1,4,9/27/18,0,0,0,0,0,0,346;1424;72;379,4;108;7;46,3;18;2;13,33;94;6;24,-1;-1
2723,ICLR,2019,Learning models for visual 3D localization with implicit mapping,Dan Rosenbaum;Frederic Besse;Fabio Viola;Danilo J. Rezende;S. M. Ali Eslami,danro@google.com;fbesse@google.com;fviola@google.com;danilor@google.com;aeslami@google.com,6;7;5,3;4;4,Reject,0,7,0,yes,9/27/18,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,5,7/4/18,15,7,9,0,0,2,499;553;1751;8628;4062,17;22;49;63;38,8;9;15;27;19,81;59;206;1117;516,-1;-1
2724,ICLR,2019,End-to-End Learning of Video Compression Using Spatio-Temporal Autoencoders,Jorge Pessoa;Helena Aidos;Pedro Tomás;Mário A. T. Figueiredo,jorge.pessoa@tecnico.ulisboa.pt;haidos@lx.it.pt;pedro.tomas@inesc-id.pt;mario.figueiredo@lx.it.pt,3;3;2,3;4;5,Reject,0,3,0,yes,9/27/18,"Instituto Superior Técnico;Instituto de Telecomunicações, Portugal;INESC-ID;Instituto de Telecomunicações, Portugal",478;478;-1;478,1103;1103;-1;1103,,9/27/18,4,2,0,0,0,0,5;344;383;18034,3;37;86;299,1;6;11;52,0;39;30;1899,-1;-1
2725,ICLR,2019,Augment your batch: better training with larger batches,Elad Hoffer;Itay Hubara;Niv Giladi;Daniel Soudry,elad.hoffer@gmail.com;itayhubara@gmail.com;giladiniv@gmail.com;daniel.soudry@gmail.com,4;4;8,4;4;3,Reject,9,4,0,yes,9/27/18,Technion;;Technion;Technion,25;-1;25;25,327;-1;327;327,8,9/27/18,11,3,2,2,2,1,1456;2995;23;4846,27;21;5;76,12;11;2;26,170;429;2;625,-1;-1
2726,ICLR,2019,Unsupervised Neural Multi-Document Abstractive Summarization of Reviews,Eric Chu;Peter J. Liu,echu@mit.edu;peterjliu@google.com,4;5;9,4;4;4,Reject,1,6,0,yes,9/27/18,Massachusetts Institute of Technology;Google,2;-1,5;-1,,9/27/18,0,0,0,0,0,0,11739;2541,74;24,22;12,2681;493,-1;-1
2727,ICLR,2019,Automatic generation of object shapes with desired functionalities,Mihai Andries;Atabak Dehban;Jose Santos-Victor,mandries@isr.tecnico.ulisboa.pt;adehban@isr.tecnico.ulisboa.pt;jasv@isr.tecnico.ulisboa.pt,5;3;3,3;4;4,Reject,0,4,0,yes,9/27/18,Instituto Superior Técnico;Instituto Superior Técnico;Instituto Superior Técnico,478;478;478,1103;1103;1103,,5/30/18,8,0,0,0,8,0,66;86;4396,14;19;212,5;5;32,1;5;190,-1;-1
2728,ICLR,2019,Hybrid Policies Using Inverse Rewards for Reinforcement Learning,Yao Shi;Tian Xia;Guanjun Zhao;Xin Gao,yao.shi@huawei.com;xiatian14@huawei.com;zhaoguanjun1@huawei.com;gaoxin17@huawei.com,3;2;4,4;5;5,Reject,0,0,0,yes,9/27/18,Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.,-1;-1;-1;-1,-1;-1;-1;-1,,9/27/18,0,0,0,0,0,0,833;4037;0;547,99;203;5;23,15;28;0;7,48;405;0;62,-1;-1
2729,ICLR,2019,Sample Efficient Deep Neuroevolution in Low Dimensional Latent Space,Bin Zhou;Jiashi Feng,bin.zhou@u.nus.edu;elefjia@u.nus.edu,4;5;4,4;5;4,Reject,0,0,0,yes,9/27/18,National University of Singapore;National University of Singapore,16;16,22;22,5,9/27/18,0,0,0,0,0,0,3072;9305,643;328,24;51,105;1208,-1;-1
2730,ICLR,2019,Semi-supervised Learning with Multi-Domain Sentiment Word Embeddings,Ran Tian;Yash Agrawal;Kento Watanabe;Hiroya Takamura,robin.tianran@gmail.com;yashagrawal@iitkgp.ac.in;kento.watanabe@aist.go.jp;takamura.hiroya@aist.go.jp,6;6;6,3;3;3,Reject,0,3,0,yes,9/27/18,AIST;Indian Institute of Technology Kharagpur;AIST;AIST,-1;261;-1;-1,-1;506;-1;-1,3,9/27/18,0,0,0,0,0,0,19;105;2;1546,29;44;11;119,3;7;1;20,2;3;0;135,-1;-1
2731,ICLR,2019,Teacher Guided Architecture Search,Pouya Bashivan;Mark Tensen;James J DiCarlo,bashivan@mit.edu;mark.tensen@student.uva.nl;dicarlo@mit.edu,6;6;5,4;4;4,Reject,0,8,0,yes,9/27/18,Massachusetts Institute of Technology;University of Amsterdam;Massachusetts Institute of Technology,2;169;2,5;59;5,,8/4/18,6,2,2,0,7,1,702;6;8187,27;1;122,11;1;43,62;1;565,-1;-1
2732,ICLR,2019,A Convergent Variant of the Boltzmann Softmax Operator in Reinforcement Learning,Ling Pan;Qingpeng Cai;Qi Meng;Wei Chen;Tie-Yan Liu,v-lip@microsoft.com;cqp14@mails.tsinghua.edu.cn;v-qimeng@microsoft.com;wche@microsoft.com;tie-yan.liu@microsoft.com,4;4;5,5;4;4,Reject,0,6,2,yes,9/27/18,Microsoft;Tsinghua University;Microsoft;Microsoft;Microsoft,-1;8;-1;-1;-1,-1;30;-1;-1;-1,1;9,9/27/18,0,0,0,0,0,0,29;27;1166;47062;13277,16;3;39;3662;366,2;2;10;88;51,4;1;221;2821;1718,-1;-1
2733,ICLR,2019,GraphSeq2Seq: Graph-Sequence-to-Sequence for Neural Machine Translation,Guoshuai Zhao;Jun Li;Lu Wang;Xueming Qian;Yun Fu,zgs2012@stu.xjtu.edu.cn;junl.mldl@gmail.com;luwang@ccs.neu.edu;qianxm@mail.xjtu.edu.cn;yunfu@ece.neu.edu,6;6;6,5;4;3,Reject,0,5,0,yes,9/27/18,Xi'an Jiaotong University;Massachusetts Institute of Technology;Northeastern University;Xi'an Jiaotong University;Northeastern University,478;2;16;478;16,565;5;839;565;839,3;10,9/27/18,1,1,0,0,0,0,461;22018;997;19;10556,24;2354;239;16;328,10;58;14;2;52,20;1450;74;1;1162,-1;-1
2734,ICLR,2019,Understanding the Effectiveness of Lipschitz-Continuity in Generative Adversarial Nets,Zhiming Zhou;Yuxuan Song;Lantao Yu;Hongwei Wang;Weinan Zhang;Zhihua Zhang;Yong Yu,heyohai@apex.sjtu.edu.cn;songyuxuan@apex.sjtu.edu.cn;yulantao@apex.sjtu.edu.cn;wanghongwei55@gmail.com;wnzhang@sjtu.edu.cn;zhzhang@math.pku.edu.cn;yyu@apex.sjtu.edu.cn,6;4;5,4;4;4,Reject,0,21,1,yes,9/27/18,Shanghai Jiao Tong University;Shanghai Jiao Tong University;Shanghai Jiao Tong University;Shanghai Jiao Tong University;Shanghai Jiao Tong University;Peking University;Shanghai Jiao Tong University,52;52;52;52;52;24;52,188;188;188;188;188;27;188,5,7/2/18,7,3,0,0,21,0,231;68;1389;36;4864;16;283,29;18;33;23;206;11;24,8;5;8;4;31;2;6,20;1;212;0;689;0;32,-1;-1
2735,ICLR,2019,Hierarchically-Structured Variational Autoencoders for Long Text Generation,Dinghan Shen;Asli Celikyilmaz;Yizhe Zhang;Liqun Chen;Xin Wang;Lawrence Carin,dinghan.shen@duke.edu;asli@ieee.org;yizhe.zhang@microsoft.com;liqun.chen@duke.edu;xwang@cs.ucsb.edu;lcarin@duke.edu,5;5;7,4;4;4,Reject,0,8,0,yes,9/27/18,Duke University;Microsoft;Microsoft;Duke University;UC Santa Barbara;Duke University,44;-1;-1;44;37;44,17;-1;-1;17;53;17,3;5,9/27/18,1,1,0,0,0,0,857;2107;10;161;547;19178,34;122;9;53;171;819,13;24;2;8;13;65,103;183;0;12;22;1986,-1;-1
2736,ICLR,2019,Learning Representations in Model-Free Hierarchical Reinforcement Learning,Jacob Rafati;David Noelle,jrafatiheravi@ucmerced.edu;dnoelle@ucmerced.edu,5;4;3,4;4;5,Reject,0,0,0,yes,9/27/18,University of California at Merced;University of California at Merced,478;478,1103;1103,,9/27/18,14,5,5,0,25,0,53;1003,15;85,6;11,0;61,-1;-1
2737,ICLR,2019,Towards Decomposed Linguistic Representation with Holographic Reduced Representation,Jiaming Luo;Yuan Cao;Yonghui Wu,j_luo@csail.mit.edu;yuancao@google.com;yonghui@google.com,5;5;6,4;4;3,Reject,0,12,1,yes,9/27/18,Massachusetts Institute of Technology;Google;Google,2;-1;-1,5;-1;-1,3,9/27/18,2,0,1,0,0,0,9;3719;363,3;187;41,2;28;8,0;141;28,-1;-1
2738,ICLR,2019,Quality Evaluation of GANs Using Cross Local Intrinsic Dimensionality,Sukarna Barua;Xingjun Ma;Sarah Monazam Erfani;Michael Houle;James Bailey,sukarnab@student.unimelb.edu.au;xingjun.ma@unimelb.edu.au;sarah.erfani@unimelb.edu.au;meh@nii.ac.jp;baileyj@unimelb.edu.au,6;4;6,5;3;4,Reject,0,9,0,yes,9/27/18,The University of Melbourne;The University of Melbourne;The University of Melbourne;Meiji University;The University of Melbourne,123;123;123;478;123,32;32;32;334;32,5;4,9/27/18,2,1,0,0,0,0,421;549;898;2138;5525,15;30;57;122;329,7;8;11;24;34,48;81;81;201;660,-1;-1
2739,ICLR,2019,Deep processing of structured data,Łukasz Maziarka;Marek Śmieja;Aleksandra Nowak;Jacek Tabor;Łukasz Struski;Przemysław Spurek,l.maziarka@gmail.com;marek.smieja@uj.edu.pl;aknoow@gmail.com;jacek.tabor@uj.edu.pl;lukasz.struski@uj.edu.pl;przemyslaw.spurek@uj.edu.pl,4;4;4,3;4;3,Reject,0,1,0,yes,9/27/18,Ardigen;Jagiellonian University;;Jagiellonian University;Jagiellonian University;Jagiellonian University,-1;478;-1;478;478;478,-1;695;-1;695;695;695,10,9/27/18,1,0,0,0,0,0,16;565;168;272;36;134,9;72;23;87;18;36,3;10;4;8;4;6,1;19;5;10;2;9,-1;-1
2740,ICLR,2019,Predictive Uncertainty through Quantization,Bastiaan S. Veeling;Rianne van den Berg;Max Welling,basveeling@gmail.com;welling.max@gmail.com,5;4;5,3;4;4,Reject,0,3,0,yes,9/27/18,University of Amsterdam;University of California - Irvine,169;35,59;99,,9/27/18,39,0,0,0,39,0,93;997;26565,14;26;269,5;7;58,17;175;5085,-1;-1
2741,ICLR,2019,TherML: The Thermodynamics of Machine Learning,Alexander A. Alemi;Ian Fischer,alemi@google.com;iansf@google.com,7;3;5,3;4;3,Reject,0,3,0,yes,9/27/18,Google;Google,-1;-1,-1;-1,,7/11/18,9,4,1,0,118,0,1270;365,53;25,14;8,186;23,-1;-1
2742,ICLR,2019,Countering Language Drift via Grounding,Jason Lee;Kyunghyun Cho;Douwe Kiela,jason@cs.nyu.edu;kyunghyun.cho@nyu.edu;dkiela@fb.com,6;6;6,3;4;4,Reject,0,10,9,yes,9/27/18,New York University;New York University;Facebook,26;26;-1,27;27;-1,3,9/27/18,11,10,5,0,0,3,150;45940;3430,33;273;80,4;52;29,7;6588;584,-1;-1
2743,ICLR,2019,Amortized Context Vector Inference for Sequence-to-Sequence Networks,Sotirios Chatzis;Kyriacos Tolias;Aristotelis Charalampous,sotirios.chatzis@cut.ac.cy;k.v.tolias@edu.cut.ac.cy;aristotelis.charalampous@edu.cut.ac.cy,6;6;5,3;3;4,Reject,0,4,0,yes,9/27/18,Cyprus University of Technology;Cyprus University of Technology;Cyprus University of Technology,261;261;261,354;354;354,3;8,5/23/18,2,0,0,0,2,0,1074;0;21,113;1;7,17;0;2,88;0;6,-1;-1
2744,ICLR,2019,Coverage and Quality Driven Training of Generative Image Models,Thomas LUCAS;Konstantin SHMELKOV;Karteek ALAHARI;Cordelia SCHMID;Jakob VERBEEK,thomas.lucas@inria.fr;konstantin.shmelkov@inria.fr;karteek.alahari@inria.fr;cordelia.schmid@inria.fr,7;5;4,4;4;5,Reject,0,19,0,yes,9/27/18,INRIA;INRIA;INRIA;INRIA,-1;-1;-1;-1,-1;-1;-1;-1,5;4;8,9/27/18,5,0,2,1,0,0,192;248;2883;72419;1521,30;8;76;430;43,6;5;26;113;16,6;29;382;10135;131,-1;-1
2745,ICLR,2019,Switching Linear Dynamics for Variational Bayes Filtering,Philip Becker-Ehmck;Jan Peters;Patrick van der Smagt,philip.becker-ehmck@volkswagen.de;peters@ias.tu-darmstadt.de;smagt@volkswagen.de,6;4;7,3;4;5,Reject,0,5,0,yes,9/27/18,"Data Lab, Volkswagen Group;TU Darmstadt;Data Lab, Volkswagen Group",-1;65;-1,-1;244;-1,5;11,9/27/18,4,1,1,0,0,0,24;312;3921,4;34;122,2;6;26,1;27;414,-1;-1
2746,ICLR,2019,SupportNet: solving catastrophic forgetting in class incremental learning with support data,Yu Li;Zhongxiao Li;Lizhong Ding;Yijie Pan;Chao Huang;Yuhui Hu;Wei Chen;Xin Gao,yu.li@kaust.edu.sa;zhongxiao.li@kaust.edu.sa;lizhong.ding@inceptioniai.org;pyj@nbicc.com;chuang@ict.ac.cn;huyh@sustc.edu.cn;chenw@sustc.edu.cn;xin.gao@kaust.edu.sa,5;6;4,4;4;4,Reject,0,12,0,yes,9/27/18,"KAUST;KAUST;Inception Institute of Artificial Intelligence;Nbicc;Institute of Computing Technology, Chinese Academy of Sciences;University of Science and Technology of China;University of Science and Technology of China;KAUST",123;123;-1;-1;62;478;478;123,1103;1103;-1;-1;1103;132;132;1103,,6/8/18,14,9,7,0,11,1,6363;142;226;92;4663;82;47062;15607,734;34;35;10;411;17;3662;1582,39;7;8;3;34;5;88;55,498;3;10;0;260;6;2821;751,-1;-1
2747,ICLR,2019,Unsupervised Learning  of Sentence Representations Using Sequence Consistency,Siddhartha Brahma,sidbrahma@gmail.com,7;5;5,4;4;4,Reject,0,8,0,yes,9/27/18,International Business Machines,-1,-1,3;6,8/10/18,3,0,0,0,0,0,264,43,10,18,-1
2748,ICLR,2019,Recurrent Kalman Networks: Factorized Inference in High-Dimensional Deep Feature Spaces,Philipp Becker;Harit Pandya;Gregor H.W. Gebhardt;Cheng Zhao;Gerhard Neumann,philippbecker93@googlemail.com;hpandya@lincoln.ac.uk;gebhardt@ias.tu-darmstadt.de;irobotcheng@gmail.com;gneumann@lincoln.ac.uk,6;6;6,4;4;3,Reject,0,3,0,yes,9/27/18,TU Darmstadt;University of Lincoln;TU Darmstadt;Birmingham University;University of Lincoln,65;478;65;123;478,244;757;244;141;757,5,9/27/18,5,0,1,0,0,1,114;70;30;341;3281,30;15;15;58;163,6;4;3;5;29,6;4;2;20;246,-1;-1
2749,ICLR,2019,Shallow Learning For Deep Networks,Eugene Belilovsky;Michael Eickenberg;Edouard Oyallon,belilove@iro.umontreal.ca;michael.eickenberg@berkeley.edu;edouard.oyallon@centralesupelec.fr,6;5;7,4;4;4,Reject,5,20,1,yes,9/27/18,University of Montreal;University of California Berkeley;CentraleSupelec,123;5;478,108;18;452,,9/27/18,2,0,0,0,0,0,289;848;601,30;35;25,10;11;10,29;59;53,-1;-1
2750,ICLR,2019,A Solution to China Competitive Poker Using Deep Learning,Zhenxing Liu;Maoyu Hu;Zhangfei Zhang,liuzx@smzy.cc;humaoyu@smzy.cc;zzf@smzy.cc,3;2,4;3,Reject,28,7,1,yes,9/27/18,;;,-1;-1;-1,-1;-1;-1,,9/27/18,0,0,0,0,0,0,88;0;1,19;1;2,5;0;1,5;0;0,-1;-1
2751,ICLR,2019,A NOVEL VARIATIONAL FAMILY FOR HIDDEN NON-LINEAR MARKOV MODELS,Daniel Hernandez Diaz;Antonio Khalil Moretti;Ziqiang Wei;Shreya Saxena;John Cunningham;Liam Paninski,dh2832@columbia.edu;amoretti@cs.columbia.edu;weiz@janelia.hhmi.org;ss5513@columbia.edu;jpcunni@gmail.com;liam.paninski@gmail.com,5;8;6,3;5;3,Reject,0,6,0,yes,9/27/18,Columbia University;Columbia University;HHMI Janelia Research Campus;Columbia University;;,15;15;-1;15;-1;-1,14;14;-1;14;-1;-1,,9/27/18,7,2,4,0,7,0,107;7;72;83;4939;10724,23;2;8;21;99;251,4;2;4;5;29;49,8;0;3;2;424;978,-1;-1
2752,ICLR,2019,GenEval: A Benchmark Suite for Evaluating Generative Models,Anton Bakhtin;Arthur Szlam;Marc'Aurelio Ranzato,yolo@fb.com;aszlam@fb.com;ranzato@fb.com,5;5;6,3;4;4,Reject,0,11,0,yes,9/27/18,Facebook;Facebook;Facebook,-1;-1;-1,-1;-1;-1,5,9/27/18,0,0,0,0,0,0,174;8722;20394,13;86;75,6;32;42,26;921;2066,-1;-1
2753,ICLR,2019,Making Convolutional Networks Shift-Invariant Again,Richard Zhang,rich.zhang@eecs.berkeley.edu,6;5;5,4;4;4,Reject,1,7,1,yes,9/27/18,University of California Berkeley,5,18,,9/27/18,60,32,22,0,11,8,2799,27,12,497,-1
2754,ICLR,2019,Deconfounding Reinforcement Learning in Observational Settings,Chaochao Lu;José Miguel Hernández Lobato,cl641@cam.ac.uk;jmh233@cam.ac.uk,4;4;2,3;4;4,Reject,19,18,0,yes,9/27/18,University of Cambridge;University of Cambridge,71;71,2;2,,9/27/18,13,9,4,0,4,2,286;3763,11;114,6;28,10;423,-1;-1
2755,ICLR,2019,Incremental Hierarchical Reinforcement Learning with Multitask LMDPs,Adam C Earle;Andrew M Saxe;Benjamin Rosman,adamchristopherearle@gmail.com;andrew.saxe@psy.ox.ac.uk;benjros@gmail.com,3;4;5,4;4;4,Reject,0,0,0,yes,9/27/18,;University of Oxford;University of the Witwatersrand,-1;50;478,-1;1;293,,9/27/18,0,0,0,0,0,0,26;1968;442,10;32;78,3;12;12,1;162;22,-1;-1
2756,ICLR,2019,Exploration by Uncertainty in Reward Space,Wei-Yang Qu;Yang Yu;Tang-Jie Lv;Ying-Feng Chen;Chang-Jie Fan,nju_qwy@163.com;yuy@nju.edu.cn;hzlvtangjie@corp.netease.com;chenyingfeng1@corp.netease.com;fanchangjie@corp.netease.com,5;5;3,3;2;5,Reject,0,0,0,yes,9/27/18,Zhejiang University;Zhejiang University;Corp.netease;University of Science and Technology of China;Corp.netease,57;57;-1;478;-1,177;177;-1;132;-1,,9/27/18,0,0,0,0,0,0,20;579;17;125;99,2;195;6;31;27,1;13;2;7;6,1;17;1;4;11,-1;-1
2757,ICLR,2019,Shrinkage-based Bias-Variance Trade-off for Deep Reinforcement Learning,Yihao Feng;Hao Liu;Jian Peng;Qiang Liu,yihao@cs.utexas.edu;uestcliuhao@gmail.com;jianpeng@illinois.edu;lqiang@cs.utexas.edu,5;4;4,3;2;4,Reject,0,4,0,yes,9/27/18,"University of Texas, Austin;University of California Berkeley;University of Illinois, Urbana Champaign;University of Texas, Austin",22;5;3;22,49;18;37;49,,9/27/18,2,2,0,0,0,0,138;1109;1362;5014,12;262;151;645,6;16;19;30,12;67;45;241,-1;-1
2758,ICLR,2019,Improving On-policy Learning with Statistical Reward Accumulation,Yubin Deng;Ke Yu;Dahua Lin;Xiaoou Tang;Chen Change Loy,dy015@ie.cuhk.edu.hk;yk017@ie.cuhk.edu.hk;dhlin@ie.cuhk.edu.hk;xtang@ie.cuhk.edu.hk;ccloy@ieee.org,4;5,3;3,Reject,0,5,0,yes,9/27/18,The Chinese University of Hong Kong;The Chinese University of Hong Kong;The Chinese University of Hong Kong;The Chinese University of Hong Kong;,57;57;57;57;-1,40;40;40;40;-1,,9/7/18,11,0,0,0,11,0,705;2997;6018;51342;16733,11;181;145;496;189,6;24;37;102;57,158;287;1061;8056;2881,-1;-1
2759,ICLR,2019,Guided Exploration in Deep Reinforcement Learning,Sahisnu Mazumder;Bing Liu;Shuai Wang;Yingxuan Zhu;Xiaotian Yin;Lifeng Liu;Jian Li;Yongbing Huang,sahisnumazumder@gmail.com;liub@cs.uic.edu;gshuaishuai@gmail.com;yingxuan.zhu@huawei.com;xiaotian.yin@huawei.com;lifeng.liu1@huawei.com;jian.li1@huawei.com;huangyongbing@huawei.com,7;5;3,5;4;3,Reject,0,4,0,yes,9/27/18,"University of Illinois, Chicago;University of Illinois, Chicago;;Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.",57;57;-1;-1;-1;-1;-1;-1,255;255;-1;-1;-1;-1;-1;-1,,9/27/18,0,0,0,0,0,0,114;440;4004;83;465;119;1833;143,21;34;160;20;47;27;57;25,6;12;33;5;11;5;12;6,10;36;250;4;19;5;153;16,-1;-1
2760,ICLR,2019,Probabilistic Knowledge Graph Embeddings,Farnood Salehi;Robert Bamler;Stephan Mandt,farnood.salehi@epfl.ch;robert.bamler@gmail.com;stephan.mandt@gmail.com,5;6;5,2;3;3,Reject,0,3,0,yes,9/27/18,"Swiss Federal Institute of Technology Lausanne;University of California, Irvine;University of California, Irvine",478;35;35,38;99;99,11;1;10,9/27/18,3,2,1,0,0,0,55;315;1263,12;31;66,4;8;17,9;17;118,-1;-1
2761,ICLR,2019,Accelerated Value Iteration via Anderson Mixing,Yujun Li;Chengzhuo Ni;Guangzeng Xie;Wenhao Yang;Shuchang Zhou;Zhihua Zhang,liyujun145@gmail.com;hzxsncz@pku.edu.cn;smsxgz@pku.edu.cn;yangwenhaosms@pku.edu.cn;zsc@megvii.com;zhzhang@math.pku.edu.cn,7;4;4,4;4;3,Reject,0,8,0,yes,9/27/18,Shanghai Jiao Tong University;Peking University;Peking University;Peking University;Megvii Technology Inc.;Peking University,52;24;24;24;-1;24,188;27;27;27;-1;27,,9/27/18,0,0,0,0,0,0,103;8;6;181;1591;5090,31;3;6;27;32;407,7;1;2;8;10;29,5;1;1;19;318;521,-1;-1
2762,ICLR,2019,Safe Policy Learning from Observations,Elad Sarafian;Aviv Tamar;Sarit Kraus,elad.sarafian@gmail.com;avivt@berkeley.edu;sarit@cs.biu.ac.il,5;5;5,4;4;3,Reject,0,5,0,yes,9/27/18,Bar Ilan University;University of California Berkeley;Bar Ilan University,95;5;95,456;18;456,,5/20/18,4,1,1,0,10,0,4;2215;12173,5;50;353,1;20;51,0;331;993,-1;-1
2763,ICLR,2019,Importance Resampling for Off-policy Policy Evaluation,Matthew Schlegel;Wesley Chung;Daniel Graves;Martha White,mkschleg@ualberta.ca;wchung@ualberta.ca;daniel.graves@huawei.com;whitem@ualberta.ca,6;5;5,4;3;3,Reject,0,10,0,yes,9/27/18,University of Alberta;University of Alberta;Huawei Technologies Ltd.;University of Alberta,99;99;-1;99,119;119;-1;119,,9/27/18,0,0,0,0,0,0,61;17;277;1458,12;8;19;66,3;3;5;17,4;0;24;167,-1;-1
2764,ICLR,2019,Mimicking actions is a good strategy for beginners: Fast Reinforcement Learning with Expert Action Sequences,Tharun Medini;Anshumali Shrivastava,tharun.medini@rice.edu;anshumali@rice.edu,5;5;6,3;2;4,Reject,0,5,0,yes,9/27/18,Rice University;Rice University,85;85,86;86,,9/27/18,0,0,0,0,0,0,6;1110,13;101,2;16,0;111,-1;-1
2765,ICLR,2019,Convergent Reinforcement Learning with Function Approximation: A Bilevel Optimization Perspective,Zhuoran Yang;Zuyue Fu;Kaiqing Zhang;Zhaoran Wang,zy6@princeton.edu;zuyuefu2022@u.northwestern.edu;kzhang66@illinois.edu;zhaoranwang@gmail.com,6;5;5;6,4;3;4;4,Reject,0,10,0,yes,9/27/18,"Princeton University;Northwestern University;University of Illinois, Urbana Champaign;Northwestern University",30;44;3;44,7;20;37;20,,9/27/18,4,3,0,0,0,1,524;11;481;108,48;6;65;26,12;2;14;6,70;3;34;16,-1;-1
2766,ICLR,2019,Unsupervised Emergence of Spatial Structure from Sensorimotor Prediction,Alban Laflaquière;Michael Garcia Ortiz,alban.laflaquiere@gmail.com;mgarciaortiz@softbankrobotics.com,6;7;4,3;3;4,Reject,0,21,2,yes,9/27/18,SoftBank Robotics Europe;SoftBank Robotics Europe,-1;-1,-1;-1,,9/27/18,1,1,0,0,0,0,97;-1,37;-1,5;-1,2;0,-1;-1
2767,ICLR,2019,Where Off-Policy Deep Reinforcement Learning Fails,Scott Fujimoto;David Meger;Doina Precup,scott.fujimoto@mail.mcgill.ca;david.meger@mcgill.ca;dprecup@cs.mcgill.ca,7;5;5,4;4;3,Reject,5,10,0,yes,9/27/18,McGill University;McGill University;McGill University,85;85;85,42;42;42,,9/27/18,2,2,0,0,0,0,516;1445;10034,9;60;325,6;16;38,171;130;1096,-1;-1
2768,ICLR,2019,Exploration by random network distillation,Yuri Burda;Harrison Edwards;Amos Storkey;Oleg Klimov,yburda@openai.com;h.l.edwards@sms.ed.ac.uk;a.storkey@ed.ac.uk;oleg@openai.com,7;4;9;10,4;4;5;4,Accept (Poster),0,16,0,yes,9/27/18,OpenAI;University of Edinburgh;University of Edinburgh;OpenAI,-1;33;33;-1,-1;27;27;-1,,9/27/18,191,118,86,10,0,57,1317;1162;3809;2576,21;18;198;43,8;8;31;7,277;190;437;763,-1;-1
2769,ICLR,2019,Learning State Representations in Complex Systems with Multimodal Data,Pavel Solovev;Vladimir Aliev;Pavel Ostyakov;Gleb Sterkin;Elizaveta Logacheva;Stepan Troeshestov;Roman Suvorov;Anton Mashikhin;Oleg Khomenko;Sergey I. Nikolenko,pavel.solovev.ilich@gmail.com;vldr.aliev@gmail.com;pavelosta@gmail.com;sterkin.gleb@gmail.com;elimohl@gmail.com;troeshust96@gmail.com;windj007@gmail.com;antonagoo@gmail.com;olegkhomenkoru@gmail.com;snikolenko@gmail.com,6;6;5,3;3;4,Reject,0,5,0,yes,9/27/18,;;Samsung;Samsung;;Lomonosov Moscow State University;Samsung;;;,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,,9/27/18,1,0,1,0,8,0,38;42;21;14;20;1;87;2;30;9443,16;11;6;3;4;1;19;3;6;157,4;4;2;1;2;1;5;1;3;19,1;1;1;2;1;0;4;0;2;1152,-1;-1
2770,ICLR,2019,EMI: Exploration with Mutual Information Maximizing State and Action Embeddings,Hyoungseok Kim;Jaekyeom Kim;Yeonwoo Jeong;Sergey Levine;Hyun Oh Song,harry2636@mllab.snu.ac.kr;jaekyeom@mllab.snu.ac.kr;yeonwoo@mllab.snu.ac.kr;svlevine@eecs.berkeley.edu;hyunoh@snu.ac.kr,5;7;7,4;4;3,Reject,1,9,0,yes,9/27/18,Seoul National University;Seoul National University;Seoul National University;University of California Berkeley;Seoul National University,41;41;41;5;41,74;74;74;18;74,5,9/27/18,4,4,0,0,0,0,22;16;34;24386;1313,11;4;13;309;28,3;2;4;73;14,0;2;6;3167;186,-1;-1
2771,ICLR,2019,P^2IR: Universal Deep Node Representation via Partial Permutation Invariant Set Functions,Shupeng Gui;Xiangliang Zhang;Shuang Qiu;Mingrui Wu;Jieping Ye;Ji Liu,sgui2@ur.rochester.edu;xiangliang.zhang@kaust.edu.sa;qiush@umich.edu;mingrui.wu@alibaba-inc.com;jieping@gmail.com;ji.liu.uwisc@gmail.com,4;5;7;5,4;3;4;5,Reject,0,0,0,yes,9/27/18,University of Rochester;KAUST;University of Michigan;Alibaba Group;;University of Rochester,106;123;8;-1;-1;106,153;1103;21;-1;-1;153,10,9/27/18,0,0,0,0,0,0,22;2268;51;1202;561;473,14;185;15;32;17;76,3;26;4;14;5;11,0;170;1;164;78;32,-1;-1
2772,ICLR,2019,Auto-Encoding Knockoff Generator for FDR  Controlled Variable Selection,Ying Liu;Cheng Zheng,summeryingl@gmail.com;zzhengccheng@gmail.com,3;4;6,4;4;3,Reject,0,9,0,yes,9/27/18,Medical College of Wisconsin;,-1;-1,-1;-1,,9/27/18,6,6,0,1,0,0,464;1563,49;204,9;21,32;57,-1;-1
2773,ICLR,2019,Learning Cross-Lingual Sentence Representations via a Multi-task Dual-Encoder Model,Muthuraman Chidambaram;Yinfei Yang;Daniel Cer;Steve Yuan;Yun-Hsuan Sung;Brian Strope;Ray Kurzweil,mc4xf@virginia.edu;yinfeiy@google.com;cer@google.com;steveyuan@google.com;yhsung@google.com;bps@google.com;raykurzweil@google.com,7;4;6,4;4;5,Reject,0,5,0,yes,9/27/18,University of Virginia;Google;Google;Google;Google;Google;Google,65;-1;-1;-1;-1;-1;-1,113;-1;-1;-1;-1;-1;-1,3;6,9/27/18,22,11,8,1,0,3,122;987;3591;605;802;1524;931,15;39;44;6;21;52;52,5;14;23;6;13;17;12,13;146;450;103;111;187;133,-1;-1
2774,ICLR,2019,Graph Generation via Scattering,Dongmian Zou;Gilad Lerman,dzou@umn.edu;lerman@umn.edu,4;4;4,4;4;3,Reject,0,6,0,yes,9/27/18,"University of Minnesota, Minneapolis;University of Minnesota, Minneapolis",57;57,56;56,5;10,9/27/18,1,0,0,0,0,0,90;1734,14;78,5;24,13;189,-1;-1
2775,ICLR,2019,Context Mover's Distance & Barycenters: Optimal transport of contexts for building representations,Sidak Pal Singh;Andreas Hug;Aymeric Dieuleveut;Martin Jaggi,sidak.singh@epfl.ch;andreas.hug@epfl.ch;aymeric.dieuleveut@epfl.ch;martin.jaggi@epfl.ch,7;4;6,4;4;4,Reject,0,14,1,yes,9/27/18,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne,478;478;478;478,38;38;38;38,,8/29/18,7,2,4,0,0,0,17;949;261;3754,8;59;17;114,3;14;7;27,0;37;29;553,-1;-1
2776,ICLR,2019,Pixel Chem: A Representation for Predicting Material Properties with Neural Network,Shuqian Ye;Yanheng Xu;Jiechun Liang;Hao Xu;Shuhong Cai;Shixin Liu;Xi Zhu,115010269@link.cuhk.edu.cn;115010252@link.cuhk.edu.cn;116010125@link.cuhk.edu.cn;115010250@link.cuhk.edu.cn;115010111@link.cuhk.edu.cn;115010194@link.cuhk.edu.cn;zhuxi@cuhk.edu.cn,3;1;3,3;5;5,Reject,0,7,0,yes,9/27/18,Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University,8;8;8;8;8;8;8,30;30;30;30;30;30;30,,9/27/18,0,0,0,0,0,0,7;24;10;543;76;112;1568,3;9;7;89;10;36;207,1;3;2;13;5;6;21,1;1;0;29;3;2;40,-1;-1
2777,ICLR,2019,Using Deep Siamese Neural Networks to Speed up Natural Products Research,Nicholas Roberts;Poornav S. Purushothama;Vishal T. Vasudevan;Siddarth Ravichandran;Chen Zhang;William H. Gerwick;Garrison W. Cottrell,n3robert@ucsd.edu;poornavsargoor@gmail.com;vthanvan@eng.ucsd.edu;s2ravich@eng.ucsd.edu;beowulf.zc@gmail.com;wgerwick@ucsd.edu;gary@ucsd.edu,4;3;4,4;2;4,Reject,0,0,0,yes,9/27/18,"University of California, San Diego;;University of California, San Diego;University of California, San Diego;;University of California, San Diego;University of California, San Diego",11;-1;11;11;-1;11;11,31;-1;31;31;-1;31;31,,9/27/18,0,0,0,0,0,0,10;0;10;10;184;7944;10968,11;1;4;6;26;312;312,2;0;2;2;7;42;43,1;0;0;0;3;499;1135,-1;-1
2778,ICLR,2019,Modeling Dynamics of Biological Systems with Deep Generative Neural Networks,Scott Gigante;David van Dijk;Kevin R. Moon;Alexander Strzalkowski;Katie Ferguson;Guy Wolf;Smita Krishnaswamy,scott.gigante@yale.edu;david.vandijk@yale.edu;kevin.moon@usu.edu;alexander.strzalkowski@yale.edu;katie.ferguson@yale.edu;jess.cardin@yale.edu;guy.wolf@yale.edu;smita.krishnaswamy@yale.edu,6;4;3,2;5;5,Reject,0,0,0,yes,9/27/18,Yale University;Yale University;SUN YAT-SEN UNIVERSITY;Yale University;Yale University;Yale University;Yale University;Yale University,62;62;478;62;62;62;62;62,12;12;352;12;12;12;12;12,5,2/10/18,2,0,0,0,23,0,38;745;541;1;-1;397;932,7;50;47;4;-1;65;75,2;11;14;1;-1;10;15,4;44;20;0;0;23;81,-1;-1
2779,ICLR,2019,Zero-shot Learning for Speech Recognition with Universal Phonetic Model,Xinjian Li;Siddharth Dalmia;David R. Mortensen;Florian Metze;Alan W Black,xinjianl@andrew.cmu.edu;sdalmia@cs.cmu.edu;dmortens@cs.cmu.edu;fmetze@cs.cmu.edu;awb@cs.cmu.edu,7;5;4,4;4;4,Reject,0,9,0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1;1,24;24;24;24;24,6,9/27/18,2,1,0,0,0,0,128;139;300;3839;13438,33;19;33;255;448,6;5;8;33;53,5;11;36;207;1148,-1;-1
2780,ICLR,2019,CAML: Fast Context Adaptation via Meta-Learning,Luisa M Zintgraf;Kyriacos Shiarlis;Vitaly Kurin;Katja Hofmann;Shimon Whiteson,lmzintgraf@gmail.com;kyriacos@latentlogic.com;vitaly.kurin@eng.ox.ac.uk;katja.hofmann@microsoft.com;shimon.whiteson@cs.ox.ac.uk,6;4;6;6,2;5;4;2,Reject,2,7,0,yes,9/27/18,University of Oxford;University of Amsterdam;University of Oxford;Microsoft;University of Oxford,50;169;50;-1;50,1;59;1;-1;1,6,9/27/18,60,25,21,1,23,4,467;235;180;1774;5304,16;11;11;112;203,8;7;6;24;38,51;16;11;150;573,-1;-1
2781,ICLR,2019,Interactive Agent Modeling by Learning to Probe,Tianmin Shu;Caiming Xiong;Ying Nian Wu;Song-Chun Zhu,tianmin.shu@ucla.edu;cxiong@salesforce.com;ywu@stat.ucla.edu;sczhu@stat.ucla.edu,6;6;6;6,4;4;3;4,Reject,0,9,0,yes,9/27/18,"University of California, Los Angeles;SalesForce.com;University of California, Los Angeles;University of California, Los Angeles",20;-1;20;20,15;-1;15;15,,9/27/18,1,0,0,0,4,0,260;6230;5722;13897,20;156;276;449,9;31;38;62,19;1054;598;986,-1;-1
2782,ICLR,2019,Unified recurrent network for many feature types,Alexander Stec;Diego Klabjan;Jean Utke,stec@u.northwestern.edu;d-klabjan@northwestern.edu;jutke@allstate.com,4;6;4;7,4;3;4;2,Reject,0,8,0,yes,9/25/19,Northwestern University;Northwestern University;Allstate,44;44;-1,20;20;-1,,9/24/18,0,0,0,0,2,0,12;2625;1024,7;215;66,1;26;14,1;196;74,m;m
2783,ICLR,2019,Differential Equation Networks,MohamadAli Torkamani;Phillip Wallis,torkamani@gmail.com;wallis.phillip@gmail.com,4;5;5,4;3;4,Reject,0,0,0,yes,9/27/18,;Oregon Health and Science University,-1;478,-1;272,,9/27/18,0,0,0,0,0,0,54;2,8;6,3;1,5;0,-1;-1
2784,ICLR,2019,Adversarial Audio Super-Resolution with Unsupervised Feature Losses,Sung Kim;Visvesh Sathe,sungmk@umich.edu;sathe@uw.edu,4;5;6,4;4;4,Reject,0,6,1,yes,9/27/18,"University of Michigan;University of Washington, Seattle",8;6,21;25,5;4,9/27/18,1,1,1,0,0,0,239;80,88;14,7;4,14;7,-1;-1
2785,ICLR,2019,Learning powerful policies and better dynamics models by encouraging consistency,Shagun Sodhani;Anirudh Goyal;Tristan Deleu;Yoshua Bengio;Jian Tang,sshagunsodhani@gmail.com;anirudhgoyal9119@gmail.com;tristan.deleu@gmail.com;yoshua.bengio@mila.quebec;tangjianpku@gmail.com,2;5;3,4;3;5,Reject,0,23,0,yes,9/27/18,University of Montreal;University of Montreal;University of Montreal;University of Montreal;HEC Montreal,123;123;123;123;-1,108;108;108;108;-1,,9/27/18,0,0,0,0,0,0,70;1115;82;201719;2707,23;46;13;807;71,5;12;5;147;13,7;127;6;23989;727,-1;-1
2786,ICLR,2019,Selectivity metrics can overestimate the selectivity of units: a case study on AlexNet,Ella M. Gale;Anh Nguyen;Ryan Blything;Nicholas Martin;Jeffrey S. Bowers,ella.gale@gmail.com;anhnguyen@auburn.edu;ryan.blything@bristol.ac.uk;nm13850@bristol.ac.uk;j.bowers@bristol.ac.uk,5;6;3,3;3;5,Reject,0,6,0,yes,9/27/18,University of Bristol;Auburn University;University of Bristol;University of Bristol;University of Bristol,123;386;123;123;123,76;652;76;76;76,,9/27/18,1,1,0,0,0,0,424;58;65;49;2423,72;16;12;29;123,12;4;4;4;27,6;1;8;0;238,-1;-1
2787,ICLR,2019,Decoupling feature extraction from policy learning: assessing benefits of state representation learning in goal based robotics,Antonin Raffin;Ashley Hill;René Traoré;Timothée Lesort;Natalia Díaz-Rodríguez;David Filliat,antonin.raffin@ensta-paristech.fr;ashley.hill@u-psud.fr;krb.traore@protonmail.com;timothee.lesort@ensta-paristech.fr;diaz.rodriguez.natalia@gmail.com;david.filliat@ensta-paristech.fr,5;3;4,4;4;4,Reject,0,4,0,yes,9/27/18,"ENSTA ParisTech;UPSud/INRIA University Paris-Saclay;Computer Science Lab  - Pierre and Marie Curie University, Paris, France;ENSTA ParisTech;ENSTA ParisTech;ENSTA ParisTech",478;478;478;478;478;478,1103;1103;123;1103;1103;1103,,9/27/18,12,7,1,0,8,0,46;413;37;241;584;2357,6;32;4;18;45;134,4;10;4;8;13;23,0;33;0;16;23;160,-1;-1
2788,ICLR,2019,Transferrable End-to-End Learning for Protein Interface Prediction,Raphael J. L. Townshend;Rishi Bedi;Ron O. Dror,raphael@cs.stanford.edu;rbedi@cs.stanford.edu;rondror@cs.stanford.edu,5;5;5,4;3;3,Reject,0,4,0,yes,9/27/18,Stanford University;Stanford University;Stanford University,4;4;4,3;3;3,6,7/3/18,5,1,2,0,0,0,67;135;11534,4;11;144,3;5;46,0;6;581,-1;-1
2789,ICLR,2019,Visual Imitation Learning with Recurrent Siamese Networks,Glen Berseth;Christopher J. Pal,gberseth@gmail.com;christopher.pal@polymtl.ca,4;4;5,4;3;4,Reject,0,6,0,yes,9/27/18,University of British Columbia;Polytechnique Montreal,36;386,34;108,,9/27/18,1,0,0,0,0,0,626;8257,44;120,11;33,21;764,-1;-1
2790,ICLR,2019,Self-Supervised Generalisation with Meta Auxiliary Learning,Shikun Liu;Edward Johns;Andrew Davison,shikun.liu17@imperial.ac.uk;e.johns@imperial.ac.uk;a.davison@imperial.ac.uk,4;4;6,3;4;4,Reject,0,3,0,yes,9/27/18,Imperial College London;Imperial College London;Imperial College London,72;72;72,8;8;8,,9/27/18,15,8,5,1,5,2,94;1001;30447,4;30;447,4;16;83,22;94;3111,-1;-1
2791,ICLR,2019,GRAPH TRANSFORMATION POLICY NETWORK FOR CHEMICAL REACTION PREDICTION,Kien Do;Truyen Tran;Svetha Venkatesh,dkdo@deakin.edu.au;truyen.tran@deakin.edu.au;svetha.venkatesh@deakin.edu.au,6;5;5,5;4;4,Reject,0,11,0,yes,9/27/18,Deakin University;Deakin University;Deakin University,478;478;478,334;334;334,10,9/27/18,18,5,3,0,0,1,65;1875;8559,11;133;561,5;24;43,7;124;540,-1;-1
2792,ICLR,2019,DiffraNet: Automatic Classification of Serial Crystallography Diffraction Patterns,Artur Souza;Leonardo B. Oliveira;Sabine Hollatz;Matt Feldman;Kunle Olukotun;James M. Holton;Aina E. Cohen;Luigi Nardi,arturluis@dcc.ufmg.br;leob@dcc.ufmg.br;shollatz@slac.stanford.edu;mattfel@stanford.edu;kunle@stanford.edu;jmholton@slac.stanford.edu;acohen@slac.stanford.edu;lnardi@stanford.edu,5;3;8,4;5;4,Reject,0,9,0,yes,9/27/18,Universidade Federal de Minas Gerais;Universidade Federal de Minas Gerais;Stanford University;Stanford University;Stanford University;Stanford University;Stanford University;Stanford University,478;478;4;4;4;4;4;4,715;715;3;3;3;3;3;3,2,9/27/18,1,0,0,0,0,0,51;1617;1;9;11466;4043;1204;383,10;67;2;11;213;129;82;27,2;21;1;1;53;32;16;10,5;172;0;0;1400;308;83;36,-1;-1
2793,ICLR,2019,Exploring the interpretability of LSTM neural networks over multi-variable data,Tian Guo;Tao Lin,tian.guo@gess.ethz.ch;tao.lin@epfl.ch,6;6;5,5;5;3,Reject,0,5,0,yes,9/27/18,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology Lausanne,10;478,10;38,,9/27/18,19,10,6,0,0,2,311;430,58;111,11;11,18;24,-1;-1
2794,ICLR,2019,Parameter efficient training of deep convolutional neural networks by dynamic sparse reparameterization,Hesham Mostafa;Xin Wang,hesham.mostafa@intel.com;xin3.wang@intel.com,4;4;6,4;4;4,Reject,16,11,0,yes,9/27/18,Intel;Intel,-1;-1,-1;-1,8,9/27/18,29,17,11,1,0,8,628;268,27;85,10;10,62;24,-1;-1
2795,ICLR,2019,An Energy-Based Framework for Arbitrary Label Noise Correction,Jaspreet Sahota;Divya Shanmugam;Janahan Ramanan;Sepehr Eghbali;Marcus Brubaker,sahotaj1@gmail.com;divyas@mit.edu;janahan.ramanan@borealisai.com;sepehr3pehr@gmail.com;mbrubake@cs.toronto.edu,5;5;5,4;4;5,Reject,0,2,0,yes,9/27/18,";Massachusetts Institute of Technology;Borealis AI;University of Waterloo;Department of Computer Science, University of Toronto",-1;2;-1;26;18,-1;5;-1;207;22,5;1,9/27/18,0,0,0,0,0,0,44;7;9;51;3850,9;5;3;16;55,4;1;2;4;17,3;0;0;4;465,-1;-1
2796,ICLR,2019,Incremental Few-Shot Learning with Attention Attractor Networks,Mengye Ren;Renjie Liao;Ethan Fetaya;Richard S. Zemel,mren@cs.toronto.edu;rjliao@cs.toronto.edu;ethanf@cs.toronto.edu;zemel@cs.toronto.edu,5;5;5,4;5;3,Reject,4,9,0,yes,9/27/18,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",18;18;18;18,22;22;22;22,6,9/27/18,29,14,7,0,6,3,1486;1593;657;21440,18;63;25;208,12;22;12;52,210;205;69;2485,-1;-1
2797,ICLR,2019,Targeted Adversarial Examples for Black Box Audio Systems,Rohan Taori;Amog Kamsetty;Brenton Chu;Nikita Vemuri,rohantaori@berkeley.edu;amogkamsetty@berkeley.edu;brentonlongchu@berkeley.edu;nikitavemuri@berkeley.edu,3;4;6,4;3;4,Reject,0,3,0,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5,18;18;18;18,4,5/20/18,39,22,12,1,64,6,40;59;39;72,4;5;1;6,1;3;1;3,6;9;6;8,-1;-1
2798,ICLR,2019,End-to-End Multi-Lingual Multi-Speaker Speech Recognition,Hiroshi Seki;Takaaki Hori;Shinji Watanabe;Jonathan Le Roux;John R. Hershey,seki@slp.cs.tut.ac.jp;thori@merl.com;shinjiw@ieee.org;leilujp@gmail.com;johnhershey@google.com,3;3;3,4;5;4,Reject,0,0,0,yes,9/27/18,"Toyohashi University of Technology,;Mitsubishi Electric Research Labs;Johns Hopkins University;;Google",-1;-1;72;-1;-1,-1;-1;13;-1;-1,,9/27/18,1,1,0,0,0,0,22;2792;6797;4702;5631,22;132;312;134;140,2;28;39;34;37,1;203;628;437;594,-1;-1
2799,ICLR,2019,Policy Generalization In Capacity-Limited Reinforcement Learning,Rachel A. Lerch;Chris R. Sims,lerchr2@rpi.edu;simsc3@rpi.edu,7;7;5,4;3;4,Reject,0,4,0,yes,9/27/18,Rensselaer Polytechnic Institute;Rensselaer Polytechnic Institute,169;169,304;304,8,9/27/18,2,2,0,0,0,1,41;652,21;47,4;13,1;68,-1;-1
2800,ICLR,2019,Learning Neural Random Fields with Inclusive Auxiliary Generators,Yunfu Song;Zhijian Ou,769414284@qq.com;ozjthu@gmail.com,5;6;6,3;3;2,Reject,0,7,1,yes,9/27/18,Tsinghua University;Tsinghua University,8;8,30;30,5,6/1/18,10,4,2,2,4,0,12;227,5;63,2;9,0;15,-1;-1
2801,ICLR,2019,Learning Disentangled Representations with Reference-Based Variational Autoencoders,Adria Ruiz;Oriol Martinez;Xavier Binefa;Jakob Verbeek,adria.ruiz-ovejero@inria.fr;oriol.martinez@upf.edu;xavier.binefa@upf.edu;jakob.verbeek@inria.fr,7;6;6,4;4;3,Reject,0,8,1,yes,9/27/18,INRIA;Universitat Pompeu Fabra;Universitat Pompeu Fabra;INRIA,-1;-1;-1;-1,-1;-1;-1;-1,5;4;2,9/27/18,10,3,3,1,3,0,104;61;1100;10549,16;13;106;145,6;5;15;38,10;1;91;1241,-1;-1
2802,ICLR,2019,Recycling the discriminator for improving the inference mapping of GAN,Duhyeon Bang;Hyunjung Shim,duhyeonbang@yonsei.ac.kr;kateshim@yonsei.ac.kr,3;3;7,5;4;4,Reject,0,4,0,yes,9/27/18,Yonsei University;Yonsei University,478;478,231;231,5;4,9/27/18,0,0,0,0,0,0,37;263,8;60,3;8,4;22,-1;-1
2803,ICLR,2019,On Difficulties of Probability Distillation,Chin-Wei Huang;Faruk Ahmed;Kundan Kumar;Alexandre Lacoste;Aaron Courville,chin-wei.huang@umontreal.ca;faruk.ahmed.91@gmail.com;kundankumar2510@gmail.com;allac@elementai.com;aaron.courville@gmail.com,5;7;5,5;2;4,Reject,0,3,0,yes,9/27/18,University of Montreal;;University of Montreal;Element AI;University of Montreal,123;-1;123;-1;123,108;-1;108;-1;108,,9/27/18,0,0,0,0,0,0,618;3574;540;963;60124,53;62;56;38;203,13;12;13;14;64,77;770;36;127;7848,-1;-1
2804,ICLR,2019,AIM: Adversarial Inference by Matching Priors and Conditionals,Hanbo Li;Yaqing Wang;Changyou Chen;Jing Gao,alexanderhanboli@gmail.com;yaqingwa@buffalo.edu;cchangyou@gmail.com;jing@buffalo.edu,6;7;4,4;4;5,Reject,0,4,1,yes,9/27/18,"Amazon;State University of New York, Buffalo;State University of New York, Buffalo;State University of New York, Buffalo",-1;81;81;81,-1;270;270;270,5;4,9/27/18,0,0,0,0,0,0,5;20;211;5115,14;8;39;515,1;3;8;33,0;1;25;260,-1;-1
2805,ICLR,2019,Deep Reinforcement Learning of Universal Policies with Diverse Environment Summaries,Felix Berkenkamp;Debadeepta Dey;Ashish Kapoor,befelix@inf.ethz.ch;dedey@microsoft.com;akapoor@microsoft.com,4;6;5,5;4;4,Reject,0,0,0,yes,9/27/18,Swiss Federal Institute of Technology;Microsoft;Microsoft,10;-1;-1,10;-1;-1,,9/27/18,0,0,0,0,0,0,821;1227;5759,25;48;186,12;16;41,47;113;448,-1;-1
2806,ICLR,2019,Gradient Acceleration in Activation Functions,Sangchul Hahn;Heeyoul Choi,s.hahn@handong.edu;hchoi@handong.edu,3;5;2,4;3;5,Reject,0,4,0,yes,9/27/18,Handong Global University;Handong Global University,478;478,1103;1103,,9/27/18,2,1,0,0,222,0,8;426,6;35,2;8,1;34,-1;-1
2807,ICLR,2019,COLLABORATIVE MULTIAGENT REINFORCEMENT LEARNING  IN HOMOGENEOUS SWARMS,Arbaaz Khan;Clark Zhang;Vijay Kumar;Alejandro Ribeiro,arbaazk@seas.upenn.edu;vijay.kumar@seas.upenn.edu;aribeiro@seas.upenn.edu,6;4;5,3;4;4,Reject,0,0,0,yes,9/27/18,University of Pennsylvania;University of Pennsylvania;University of Pennsylvania,19;19;19,10;10;10,,9/27/18,3,2,0,0,0,0,30;409;962;7675,7;35;155;313,3;11;16;44,2;20;52;586,-1;-1
2808,ICLR,2019,Generative adversarial interpolative autoencoding: adversarial training on latent space interpolations encourages convex latent distributions,Tim Sainburg;Marvin Thielk;Brad Thielman;Benjamin Migliori;Timothy Gentner,tsainbur@ucsd.edu;marvin.thielk@gmail.com;ben.migliori@lanl.gov;tgentner@ucsd.edu,4;5;4,4;4;5,Reject,2,12,0,yes,9/27/18,"University of California, San Diego;University of California, San Diego;Los Alamos National Laboratory;University of California, San Diego",11;11;-1;11,31;31;-1;31,5;4,7/17/18,15,10,4,0,20,1,32;36;25;113;1956,8;10;3;16;81,3;3;2;6;23,3;3;2;5;133,-1;-1
2809,ICLR,2019,Beyond Winning and Losing: Modeling Human Motivations and Behaviors with Vector-valued Inverse Reinforcement Learning,Baoxiang Wang;Tongfang Sun;Xianjun Sam Zheng,wangbx66@gmail.com;tongfs@uw.edu;sam.zheng@deephow.com,5;4;4,3;4;4,Reject,0,3,0,yes,9/27/18,"The Chinese University of Hong Kong;University of Washington, Seattle;Deephow",57;6;-1,40;25;-1,,9/27/18,0,0,0,0,0,0,999;10;658,105;4;56,17;2;8,83;0;35,-1;-1
2810,ICLR,2019,MLPrune: Multi-Layer Pruning for Automated Neural Network Compression,Wenyuan Zeng;Raquel Urtasun,zengwenyuan1995@gmail.com;urtasun@uber.com,5;6;4,5;4;4,Reject,0,5,0,yes,9/27/18,Uber;Uber,-1;-1,-1;-1,,9/27/18,7,4,4,0,0,2,402;24327,12;245,7;73,51;3432,-1;-1
2811,ICLR,2019,Offline Deep models calibration with bayesian neural networks,Juan Maroñas;Roberto Paredes;Daniel Ramos,jmaronasm@gmail.com;rparedes@dsic.upv.es;daniel.ramos@uam.es,4;3;3,4;4;4,Reject,1,12,0,yes,9/27/18,Universidad Politecnica de Valencia;Universidad Politecnica de Valencia;Universidad Autónoma de Madrid,478;478;-1,561;561;-1,11;2,9/27/18,0,0,0,0,0,0,3;1511;968,5;134;105,1;21;15,0;105;72,-1;-1
2812,ICLR,2019,Successor Uncertainties: exploration and uncertainty in temporal difference learning,David Janz;Jiri Hron;José Miguel Hernández-Lobato;Katja Hofmann;Sebastian Tschiatschek,david.janz93@gmail.com;jh2084@cam.ac.uk;jmh233@cam.ac.uk;katja.hofmann@microsoft.com;sebastian.tschiatschek@microsoft.com,4;5;4,3;4;5,Reject,0,8,0,yes,9/27/18,University of Cambridge;University of Cambridge;University of Cambridge;Microsoft;Microsoft,71;71;71;-1;-1,2;2;2;-1;-1,,9/27/18,6,3,2,0,9,0,6;454;3763;168;777,1;28;114;36;64,1;7;28;8;13,0;62;423;11;49,-1;-1
2813,ICLR,2019,Adversarially Robust Training through Structured Gradient Regularization,Kevin Roth;Aurelien Lucchi;Sebastian Nowozin;Thomas Hofmann,kevin.roth@inf.ethz.ch;aurelien.lucchi@inf.ethz.ch;sebastian.nowozin@microsoft.com;thomas.hofmann@inf.ethz.ch,4;3;4,4;4;4,Reject,10,10,0,yes,9/27/18,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Microsoft;Swiss Federal Institute of Technology,10;10;-1;10,10;10;-1;10,4,5/22/18,11,3,6,0,6,3,506;6806;6885;22675,28;68;134;173,10;24;39;52,71;1110;932;3389,-1;-1
2814,ICLR,2019,MERCI: A NEW METRIC TO EVALUATE THE CORRELATION BETWEEN PREDICTIVE UNCERTAINTY AND TRUE ERROR,michel moukari;loïc simon;sylvaine picard;frédéric jurie,michel.moukari@unicaen.fr;loic.simon@ensicaen.fr;sylvaine.picard@safrangroup.com;frederic.jurie@unicaen.fr,4;5;3,4;3;4,Reject,0,5,0,yes,9/27/18,University of Caen Normandie;ENSICAEN;SAFRAN;University of Caen Normandie,-1;-1;-1;-1,-1;-1;-1;-1,11,9/27/18,0,0,0,0,0,0,12;4;51;72,3;9;15;48,1;1;3;5,2;0;8;3,-1;-1
2815,ICLR,2019,Unsupervised Document Representation using Partition Word-Vectors Averaging,Vivek Gupta;Ankit Kumar Saw;Partha Pratim Talukdar;Praneeth Netrapalli,vgupta@cs.utah.edu;ankit.kgpian@gmail.com;ppt@iisc.ac.in;praneeth@microsoft.com,7;6;4,4;3;4,Reject,0,12,1,yes,9/27/18,University of Utah;;Indian Institute of Science;Microsoft,52;-1;478;-1,200;-1;273;-1,3,9/27/18,2,2,1,0,0,1,71;4;2351;3279,32;4;102;72,3;2;28;27,12;1;202;416,-1;-1
2816,ICLR,2019,Graph U-Net,Hongyang Gao;Shuiwang Ji,hongyang.gao@tamu.edu;sji@tamu.edu,7;4;7,5;4;4,Reject,6,12,2,yes,9/27/18,Texas A&M;Texas A&M,44;44,160;160,2;10,9/27/18,58,27,34,4,5,15,300;9389,21;136,8;36,41;738,-1;-1
2817,ICLR,2019,Fast Exploration with Simplified Models and Approximately Optimistic Planning in Model Based Reinforcement Learning,Ramtin Keramati;Jay Whang;Patrick Cho;Emma Brunskill,keramati@stanford.edu;jaywhang@cs.stanford.edu;patcho@cs.stanford.edu;ebrun@cs.stanford.edu,5;4,4;4,Reject,0,0,0,yes,9/27/18,Stanford University;Stanford University;Stanford University;Stanford University,4;4;4;4,3;3;3;3,,6/1/18,3,2,1,0,0,1,27;17;1;3097,11;7;4;150,3;2;1;28,3;2;1;293,-1;-1
2818,ICLR,2019,Evolutionary-Neural Hybrid Agents for Architecture Search,Krzysztof Maziarz;Andrey Khorlin;Quentin de Laroussilhe;Andrea Gesmundo,kmaziarz@google.com;akhorlin@google.com;underflow@google.com;agesmundo@google.com,4;5;4,4;2;4,Reject,0,1,0,yes,9/27/18,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,,9/27/18,8,1,7,0,5,0,445;643;60;267,8;17;4;26,2;5;2;8,33;91;10;29,-1;-1
2819,ICLR,2019,Deep Curiosity Search: Intra-Life Exploration Can Improve Performance on Challenging Deep Reinforcement Learning Problems,Christopher Stanton;Jeff Clune,cstanto3@uwyo.edu;jeffclune@uwyo.edu,5;5;5,3;1;3,Reject,0,4,0,yes,9/27/18,University of Wyoming;University of Wyoming,386;386,1103;1103,,6/1/18,17,10,2,0,65,0,363;10742,49;113,8;36,30;768,-1;-1
2820,ICLR,2019,Deep Probabilistic Video Compression,Jun Han;Salvator Lombardo;Christopher Schroers;Stephan Mandt,jun.han.gr@dartmouth.edu;sal.lombardo@disneyresearch.com;christopher.schroers@disneyresearch.com;stephan.mandt@gmail.com,6;5;6,5;4;5,Reject,0,5,0,yes,9/27/18,"Dartmouth College;Disney Research, Disney;Disney Research, Disney;University of California, Irvine",153;-1;-1;35,89;-1;-1;99,5,9/27/18,13,6,4,0,0,1,436;312;478;1263,76;33;28;66,12;10;11;17,42;19;58;118,-1;-1
2821,ICLR,2019,Reduced-Gate Convolutional LSTM Design Using Predictive Coding for Next-Frame Video Prediction,Nelly Elsayed;Anthony S. Maida;Magdy Bayoumi,nelly.elsayed5@gmail.com;maida@louisiana.edu;mab0778@louisiana.edu,3;5;7,5;4;4,Reject,0,7,0,yes,9/27/18,University of Arizona;University of Arizona;University of Arizona,169;169;169,161;161;161,,9/27/18,7,1,3,1,15,1,68;645;4456,15;98;579,4;13;29,2;32;311,-1;-1
2822,ICLR,2019,Playing the Game of Universal Adversarial Perturbations,Julien Perolet;Mateusz Malinowski;Bilal Piot;Olivier Pietquin,perolat@google.com;mateuszm@google.com;piot@google.com;pietquin@google.com,6;5;5,1;4;3,Reject,3,5,0,yes,9/27/18,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,4,9/20/18,13,9,4,0,3,1,678;2614;2040;3361,37;33;70;195,16;14;18;30,43;330;266;288,-1;-1
2823,ICLR,2019,Unsupervised Expectation Learning for Multisensory Binding,Pablo Barros;German I. Parisi;Manfred Eppe;Stefan Wermter,barros@informatik.uni-hamburg.de;parisi@informatik.uni-hamburg.de;eppe@informatik.uni-hamburg.de;wermter@informatik.uni-hamburg.de,4;5;5,4;3;2,Reject,0,5,0,yes,9/27/18,University of Hamburg;University of Hamburg;University of Hamburg;University of Hamburg,228;228;228;228,207;207;207;207,,9/27/18,0,0,0,0,0,0,584;761;234;3595,61;62;38;367,14;13;9;31,33;37;8;175,-1;-1
2824,ICLR,2019,"The meaning of most"" for visual question answering models""",Alexander Kuhnle;Ann Copestake,aok25@cam.ac.uk;aac10@cam.ac.uk,7;5;5,4;5;4,Reject,0,8,0,yes,9/27/18,University of Cambridge;University of Cambridge,71;71,2;2,,9/27/18,2,0,0,0,7,0,125;4144,20;78,7;27,5;517,-1;-1
2825,ICLR,2019,Unsupervised Image to Sequence Translation with Canvas-Drawer Networks,Kevin Frans;Chin-Yi Cheng,kevinfrans2@gmail.com;chin-yi.cheng@autodesk.com,4;4;6,4;4;5,Reject,0,9,0,yes,9/27/18,OpenAI;Autodesk Inc,-1;-1,-1;-1,2,9/21/18,8,4,1,0,27,0,182;261,4;21,3;6,23;11,-1;-1
2826,ICLR,2019,NSGA-Net: A Multi-Objective Genetic Algorithm for Neural Architecture Search,Zhichao Lu;Ian Whalen;Vishnu Boddeti;Yashesh Dhebar;Kalyanmoy Deb;Erik Goodman;Wolfgang Banzhaf,mikelzc1990@gmail.com;whalenia@msu.edu;vishnu@msu.edu;dhebarya@egr.msu.edu;kdeb@egr.msu.edu;goodman@egr.msu.edu;banzhafw@msu.edu,5;6;5,4;4;3,Reject,0,5,0,yes,9/27/18,SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY,478;478;478;478;478;478;478,352;352;352;352;352;352;352,11,9/27/18,31,9,11,0,7,1,270;52;894;82;55064;3230;7450,48;5;56;9;612;282;342,9;2;17;4;77;27;41,11;2;86;3;8474;191;663,-1;-1
2827,ICLR,2019,PA-GAN: Improving GAN Training by Progressive Augmentation,Dan Zhang;Anna Khoreva,dan.zhang2@de.bosch.com;anna.khoreva@de.bosch.com,5;4;5,5;2;4,Reject,0,9,3,yes,9/27/18,Bosch;Bosch,-1;-1,-1;-1,5;4,9/27/18,3,3,1,0,0,1,4959;860,427;24,33;12,335;158,-1;-1
2828,ICLR,2019,Point Cloud GAN,Chun-Liang Li;Manzil Zaheer;Yang Zhang;Barnabás Póczos;Ruslan Salakhutdinov,chunlial@cs.cmu.edu;manzilz@cs.cmu.edu;yz6@andrew.cmu.edu;bapoczos@cs.cmu.edu;rsalakhu@cs.cmu.edu,5;5;6,4;4;4,Reject,4,7,0,yes,9/27/18,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1;1,24;24;24;24;24,5;4;11;8,9/27/18,41,20,10,2,9,6,1372;1647;14586;5945;71264,90;63;611;244;256,18;17;42;40;83,118;275;1257;719;7998,-1;-1
2829,ICLR,2019,Uncertainty-guided Lifelong Learning in Bayesian Networks,Sayna Ebrahimi;Mohamed Elhoseiny;Trevor Darrell;Marcus Rohrbach,sayna@eecs.berkeley.edu;elhoseiny@fb.com;trevor@eecs.berkeley.edu;maroffm@gmail.com,4;4;4,4;4;4,Reject,0,4,0,yes,9/27/18,University of California Berkeley;Facebook;University of California Berkeley;Facebook,5;-1;5;-1,18;-1;18;-1,11,9/27/18,1,0,0,0,0,0,174;1426;88648;11384,19;67;558;90,7;18;111;43,24;186;11396;1532,-1;-1
2830,ICLR,2019,Rectified Gradient: Layer-wise Thresholding for Sharp and Coherent Attribution Maps,Beomsu Kim;Junghoon Seo;Jeongyeol Choe;Jamyoung Koo;Seunghyeon Jeon;Taegyun Jeon,1202kbs@gmail.com;sjh@satreci.com;cjy@si-analytics.ai;jmkoo@si-analytics.ai;jsh@satreci.com;tgjeon@si-analytics.ai,5;5;4,5;4;4,Reject,0,15,0,yes,9/27/18,KAIST;Satrec Initiative co. Itd;SI Analytics;SI Analytics;Satrec Initiative co. Itd;SI Analytics,20;-1;-1;-1;-1;-1,95;-1;-1;-1;-1;-1,,9/27/18,0,0,0,0,0,0,238;30;51;17;5;6,61;18;7;5;2;3,9;4;4;3;1;1,15;2;2;2;1;0,-1;-1
2831,ICLR,2019,Successor Options : An Option Discovery Algorithm for Reinforcement Learning,Manan Tomar*;Rahul Ramesh*;Balaraman Ravindran,manan.tomar@gmail.com;rahul13ramesh@gmail.com;ravi@cse.iitm.ac.in,4;5;6;4,5;4;4;5,Reject,3,8,0,yes,9/27/18,Indian Institute of Technology Madras;Indian Institute of Technology Madras;Indian Institute of Technology Madras,153;153;153,625;625;625,,9/27/18,2,0,1,0,0,0,6;166;2434,6;40;234,2;8;27,0;6;197,-1;-1
2832,ICLR,2019,SnapQuant: A Probabilistic and Nested Parameterization for Binary Networks,Kuan Wang;Hao Zhao;Anbang Yao;Aojun Zhou;Dawei Sun;Yurong Chen,wangkuan15@mails.tsinghua.edu.cn;hao.zhao@intel.com;anbang.yao@intel.com;aojun.zhou@intel.com;dawei.sun@intel.com;yurong.chen@intel.com,4;6;5,5;3;4,Reject,0,3,0,yes,9/27/18,Tsinghua University;Intel;Intel;Intel;Intel;Intel,8;-1;-1;-1;-1;-1,30;-1;-1;-1;-1;-1,11,9/27/18,0,0,0,0,0,0,306;205;1828;548;621;2951,45;34;33;11;62;102,7;7;14;5;8;22,39;3;238;63;34;332,-1;-1
2833,ICLR,2019,Learning Physics Priors for Deep Reinforcement Learing,Yilun Du;Karthik Narasimhan,yilundu@openai.com;karthikn@cs.princeton.edu,4;5;5,3;4;5,Reject,0,10,0,yes,9/27/18,OpenAI;Princeton University,-1;30,-1;7,6;8,9/27/18,0,0,0,0,0,0,121;1165,27;34,6;12,10;126,-1;-1
2834,ICLR,2019,HC-Net: Memory-based Incremental Dual-Network System for Continual learning,Jangho Kim;Jeesoo Kim;Nojun Kwak,kjh91@snu.ac.kr;kimjiss0305@snu.ac.kr;nojunk@snu.ac.kr,4;4;4,3;4;5,Reject,0,5,0,yes,9/27/18,Seoul National University;Seoul National University;Seoul National University,41;41;41,74;74;74,,9/7/18,6,0,0,0,6,0,209;50;2471,80;14;135,9;4;20,22;8;293,-1;-1
2835,ICLR,2019,Feature Attribution As Feature Selection,Satoshi Hara;Koichi Ikeno;Tasuku Soma;Takanori Maehara,satohara@ar.sanken.osaka-u.ac.jp;k1keno@ar.sanken.osaka-u.ac.jp;tasuku_soma@mist.i.u-tokyo.ac.jp;takanori.maehara@riken.jp,4;4;3,2;4;3,Reject,0,3,0,yes,9/27/18,Osaka University;Osaka University;The University of Tokyo;RIKEN,478;478;54;-1,236;236;45;-1,,9/27/18,1,1,0,0,0,0,511;157;234;652,71;27;24;97,13;4;8;15,25;9;34;84,-1;-1
2836,ICLR,2019,NECST: Neural Joint Source-Channel Coding,Kristy Choi;Kedar Tatwawadi;Tsachy Weissman;Stefano Ermon,kechoi@cs.stanford.edu;kedart@stanford.edu;tsachy@stanford.edu;ermon@cs.stanford.edu,6;4;7,5;3;4,Reject,0,8,0,yes,9/27/18,Stanford University;Stanford University;Stanford University;Stanford University,4;4;4;4,3;3;3;3,,9/27/18,26,14,6,2,0,4,423;130;3776;4911,8;28;288;203,5;7;29;31,36;13;362;661,-1;-1
2837,ICLR,2019,Generating Images from Sounds Using Multimodal Features and GANs,Jeonghyun Lyu;Takashi Shinozaki;Kaoru Amano,app@live.jp;tshino@nict.go.jp;kaoruamano@nict.go.jp,3;4;4,4;4;5,Reject,0,0,0,yes,9/27/18,"Osaka University;National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology;National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology",478;-1;-1,236;-1;-1,5;4,9/27/18,0,0,0,0,0,0,0;63;914,1;40;85,0;5;12,0;2;89,-1;-1
2838,ICLR,2019,Improving Generative Adversarial Imitation Learning with Non-expert Demonstrations,Voot Tangkaratt;Masashi Sugiyama,voot.tangkaratt@riken.jp;sugi@k.u-tokyo.ac.jp,5;5;7;4,3;4;3;5,Reject,0,9,0,yes,9/27/18,RIKEN;The University of Tokyo,-1;54,-1;45,5;4,9/27/18,0,0,0,0,0,0,204;11582,25;712,8;52,19;1299,-1;-1
2839,ICLR,2019,Step-wise Sensitivity Analysis: Identifying Partially Distributed Representations for Interpretable Deep Learning,Botty Dimanov;Mateja Jamnik,botty.dimanov@cl.cam.ac.uk;mateja.jamnik@cl.cam.ac.uk,3;4;3,5;4;4,Reject,0,2,0,yes,9/27/18,University of Cambridge;University of Cambridge,71;71,2;2,10,9/27/18,0,0,0,0,0,0,1;436,2;104,1;11,0;12,-1;-1
2840,ICLR,2019,Noisy Information Bottlenecks for Generalization,Julius Kunze;Louis Kirsch;Hippolyt Ritter;David Barber,juliuskunze@gmail.com;mail@louiskirsch.com;j.ritter@cs.ucl.ac.uk;d.barber@cs.ucl.ac.uk,7;5;3,2;3;4,Reject,0,7,0,yes,9/27/18,University College London;IDSIA;University College London;University College London,50;-1;50;50,16;-1;16;16,5;8,9/27/18,0,0,0,0,0,0,60;57;176;3832,8;7;6;200,3;3;3;27,6;6;24;411,-1;-1
2841,ICLR,2019,GENERALIZED ADAPTIVE MOMENT ESTIMATION,Guoqiang Zhang;Kenta Niwa;W. Bastiaan Kleijn,guoqiang.zhang@uts.edu.au;niwa.kenta@lab.ntt.co.jp;bastiaan.kleijn@ecs.vuw.ac.nz,3;4;7,4;3;4,Reject,0,6,0,yes,9/27/18,University of Technology Sydney;NTT;Victoria University Wellington,106;-1;314,216;-1;346,9,9/27/18,0,0,0,0,0,0,569;317;4957,82;79;348,12;9;37,44;11;423,-1;-1
2842,ICLR,2019,q-Neurons: Neuron Activations based on Stochastic Jackson's Derivative Operators,Frank Nielsen;Ke Sun,frank.nielsen@acm.org;sunk.edu@gmail.com,2;6;5,5;3;3,Reject,0,1,0,yes,9/27/18,"Ecole Polytechnique;University of Nebraska, Kearney",478;-1,115;-1,8,6/1/18,8,0,0,0,8,0,4264;283,330;29,33;7,302;17,-1;-1
2843,ICLR,2019,Gradient-based Training of Slow Feature Analysis by Differentiable Approximate Whitening,Merlin Schüler;Hlynur Davíð Hlynsson;Laurenz Wiskott,merlin.schueler@ini.rub.de;hlynur.hlynsson@ini.rub.de;laurenz.wiskott@ini.rub.de,5;6;6,2;4;4,Reject,0,3,0,yes,9/27/18,Ruhr-Universtät Bochum;Ruhr-Universtät Bochum;Ruhr-Universtät Bochum,261;261;261,248;248;248,,8/27/18,3,1,1,0,2,0,5;4;7065,4;4;134,2;1;31,0;0;639,-1;-1
2844,ICLR,2019,Unsupervised Conditional Generation using noise engineered mode matching GAN,Deepak Mishra;Prathosh AP;Aravind J;Prashant Pandey;Santanu Chaudhury,deemishra21@gmail.com;prathoshap@gmail.com;maxaravind@gmail.com;getprashant57@gmail.com;santanuc@ee.iitd.ac.in,5;5;6,3;3;4,Reject,0,7,0,yes,9/27/18,Indian Institute of Technology Delhi;Indian Institute of Technology Delhi;Indian Institute of Technology Delhi;Indian Institute of Technology Delhi;Indian Institute of Technology Delhi,123;123;123;123;123,529;529;529;529;529,5;4,9/27/18,0,0,0,0,0,0,120;2463;17;2761;440;2442,40;75;11;75;70;419,4;14;1;15;10;21,15;174;4;173;19;114,-1;-1
2845,ICLR,2019,Variadic Learning by Bayesian Nonparametric Deep Embedding,Kelsey R Allen;Hanul Shin;Evan Shelhamer;Josh B. Tenenbaum,krallen@mit.edu;skyshin@mit.edu;shelhamer@cs.berkeley.edu;jbt@mit.edu,5;4;4,4;2;4,Reject,0,13,0,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology;University of California Berkeley;Massachusetts Institute of Technology,2;2;5;2,5;5;18;5,11;6;8,9/27/18,3,2,0,0,0,0,895;305;26587;250,28;6;26;28,9;3;14;6,94;43;4110;28,-1;-1
2846,ICLR,2019,Fast adversarial training for semi-supervised learning,Dongha Kim;Yongchan Choi;Jae-Joon Han;Changkyu Choi;Yongdai Kim,dongha0718@hanmail.net;pminer32@gmail.com;jae-joon.han@samsung.com;changkyu_choi@samsung.com;ydkim0903@gmail.com,7;5;5,4;4;4,Reject,0,6,0,yes,9/27/18,Seoul National University;Seoul National University;Samsung;Samsung;,41;41;-1;-1;-1,74;74;-1;-1;-1,5;4,9/27/18,0,0,0,0,0,0,65;0;381;863;805,39;2;50;78;84,4;0;11;14;14,9;0;37;88;80,-1;-1
2847,ICLR,2019,Adversarially Learned Mixture Model,Andrew Jesson;Cécile Low-Kam;Tanya Nair;Florian Soudan;Florent Chandelier;Nicolas Chapados,andrew.jesson@imagia.com;cecile.low-kam@imagia.com;tanya.nair@imagia.com;fsoudan21@gmail.com;florent.chandelier@imagia.com;nicolas.chapados@imagia.com,6;5;6,1;4;2,Reject,0,1,0,yes,9/27/18,Imagia;Imagia;Imagia;;Imagia;Imagia,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,5;4,7/14/18,1,0,0,0,6,0,94;-1;-1;150;479;455,12;-1;-1;6;44;7,6;-1;-1;2;12;4,6;0;0;5;33;44,-1;-1
2848,ICLR,2019,Theoretical and Empirical Study of Adversarial Examples,Fuchen Liu;Hongwei Shang;Hong Zhang,fuchenl@andrew.cmu.edu;shanghongwei@oath.com;hongz@oath.com,5;5;4,2;4;4,Reject,5,2,0,yes,9/27/18,Carnegie Mellon University;Oath;Oath,1;-1;-1,24;-1;-1,4,9/27/18,0,0,0,0,0,0,46;54;273,9;10;153,2;5;8,3;3;17,-1;-1
2849,ICLR,2019,microGAN: Promoting Variety through Microbatch Discrimination,Goncalo Mordido;Haojin Yang;Christoph Meinel,goncalo.mordido@hpi.de;haojin.yang@hpi.de;christoph.meinel@hpi.de,3;3;6,3;3;3,Reject,0,1,0,yes,9/27/18,Hasso Plattner Institute;Hasso Plattner Institute;Hasso Plattner Institute,261;261;261,1103;1103;1103,5;4,9/27/18,0,0,0,0,0,0,14;1255;7870,7;74;1150,2;16;37,0;61;520,-1;-1
2850,ICLR,2019,Unifying Bilateral Filtering and Adversarial Training for Robust Neural Networks,Neale Ratzlaff;Li Fuxin,ratzlafn@oregonstate.edu;lif@oregonstate.edu,4;5;5,5;5;3,Reject,10,9,0,yes,9/27/18,Oregon State University;Oregon State University,76;76,318;318,4,4/5/18,1,1,1,0,10,1,6;2365,5;83,1;24,1;316,-1;-1
2851,ICLR,2019,PPD: Permutation Phase Defense Against Adversarial Examples in Deep Learning,Mehdi Jafarnia-Jahromi;Tasmin Chowdhury;Hsin-Tai Wu;Sayandev Mukherjee,mjafarni@usc.edu;chowdt1@unlv.nevada.edu;hwu@docomoinnovations.com;sayandev.mukherjee@huawei.com,6;7;4,3;4;5,Reject,24,6,0,yes,9/27/18,University of Southern California;Nevada System of Higher Education;Docomoinnovations;Huawei Technologies Ltd.,30;-1;-1;-1,66;-1;-1;-1,4,9/27/18,3,1,0,0,4,0,10;9;2;2858,5;4;1;92,2;2;1;21,3;1;0;145,-1;-1
2852,ICLR,2019,Universal Attacks on Equivariant Networks,Amit Deshpande;Sandesh Kamath;K V Subrahmanyam,amitdesh@microsoft.com;ksandeshk@cmi.ac.in;kv@cmi.ac.in,5;4;4,4;5;5,Reject,0,3,0,yes,9/27/18,Microsoft;Chennai Mathematical Institute;Chennai Mathematical Institute,-1;478;478,-1;1103;1103,4,9/27/18,0,0,0,0,0,0,280;17;178,25;16;63,7;2;8,22;2;15,-1;-1
2853,ICLR,2019,Detecting Adversarial Examples Via Neural Fingerprinting,Sumanth Dathathri;Stephan Zheng;Yisong Yue;Richard M. Murray,sdathath@caltech.edu;st.t.zheng@gmail.com;yyue@caltech.edu;murray@cds.caltech.edu,6;5;9,3;4;4,Reject,5,38,0,yes,9/27/18,California Institute of Technology;SalesForce.com;California Institute of Technology;California Institute of Technology,140;-1;140;140,3;-1;3;3,4,3/11/18,3,1,1,0,16,1,82;467;3225;31789,17;29;121;588,4;8;29;68,12;32;391;2370,-1;-1
2854,ICLR,2019,Sorting out Lipschitz function approximation,Cem Anil;James Lucas;Roger B. Grosse,cem.anil@mail.utoronto.ca;jlucas@cs.toronto.edu;rgrosse@cs.toronto.edu,7;5;4,3;4;4,Reject,0,6,4,yes,9/27/18,"Toronto University;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",18;18;18,22;22;22,4;8,9/27/18,47,25,21,2,100,5,248;303;5653,6;114;48,4;9;28,20;39;803,-1;-1
2855,ICLR,2019,Adaptive Mixture of Low-Rank Factorizations for Compact Neural Modeling,Ting Chen;Ji Lin;Tian Lin;Song Han;Chong Wang;Denny Zhou,iamtingchen@gmail.com;lin-j14@mails.tsinghua.edu.cn;tianlin@google.com;chongw@google.com;dennyzhou@google.com;hansong8811@gmail.com,7;4;6,5;5;4,Reject,0,12,0,yes,9/27/18,"University of California, Los Angeles;Tsinghua University;Google;Google;Google;",20;8;-1;-1;-1;-1,15;30;-1;-1;-1;-1,3,9/27/18,9,7,3,1,0,2,156;1594;156;96;18372;8908,51;52;55;12;1045;78,7;18;5;4;55;34,5;179;6;6;1638;1341,-1;-1
2856,ICLR,2019,ChoiceNet: Robust Learning by  Revealing Output Correlations,Sungjoon Choi;Sanghoon Hong;Kyungjae Lee;Sungbin Lim,sungjoon.s.choi@gmail.com;sanghoon.hong@kakaobrain.com;kyungjae.lee@cpslab.snu.ac.kr;sungbin.lim@kakaobrain.com,4;6;5,4;4;5,Reject,0,7,0,yes,9/27/18,"Disney Research, Disney;Kakao Brain;Seoul National University;Kakao Brain",-1;-1;41;-1,-1;-1;74;-1,,9/27/18,6,3,1,0,8,1,708;197;-1;196,70;12;-1;28,12;4;-1;7,90;34;0;24,-1;-1
2857,ICLR,2019,Iteratively Learning from the Best,Yanyao Shen;Sujay Sanghavi,shenyanyao@utexas.edu;sanghavi@mail.utexas.edu,6;3;6,3;5;4,Reject,0,3,0,yes,9/27/18,"University of Texas, Austin;University of Texas, Austin",22;22,49;49,1,9/27/18,3,0,0,0,3,0,312;5349,16;100,8;32,27;699,-1;-1
2858,ICLR,2019,Distilled Agent DQN for Provable Adversarial Robustness,Matthew Mirman;Marc Fischer;Martin Vechev,matthew.mirman@inf.ethz.ch;marcfisc@student.ethz.ch;martin.vechev@inf.ethz.ch,5;3;4,4;2;2,Reject,0,5,0,yes,9/27/18,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10,10;10;10,4,9/27/18,2,2,1,0,0,0,481;83;4198,15;83;153,4;4;35,63;5;465,-1;-1
2859,ICLR,2019,On Regularization and Robustness of Deep Neural Networks,Alberto Bietti*;Grégoire Mialon*;Julien Mairal,alberto.bietti@inria.fr;gregoire.mialon@inria.fr;julien.mairal@inria.fr,5;4;6,3;4;2,Reject,0,8,0,yes,9/27/18,INRIA;INRIA;INRIA,-1;-1;-1,-1;-1;-1,5;4;8,9/27/18,10,6,0,0,11,0,167;23;15184,18;4;120,9;2;38,13;0;1436,-1;-1
2860,ICLR,2019,Prior Networks for Detection of Adversarial Attacks,Andrey Malinin;Mark Gales,am969@cam.ac.uk;mjfg@eng.cam.ac.uk,3;4;4,4;4;5,Reject,4,0,0,yes,9/27/18,University of Cambridge;University of Cambridge,71;71,2;2,4,9/27/18,2,2,1,0,0,0,252;10460,23;369,8;48,30;1043,-1;-1
2861,ICLR,2019,A Rate-Distortion Theory of Adversarial Examples,Angus Galloway;Anna Golubeva;Graham W. Taylor,gallowaa@uoguelph.ca;agolubeva@perimeterinstitute.ca;gwtaylor@uoguelph.ca,4;3;2,4;3;3,Reject,0,1,0,yes,9/27/18,University of Guelph;Perimeter Institute;University of Guelph,261;-1;261,1103;-1;1103,4;8,9/27/18,0,0,0,0,0,0,84;69;5833,12;20;143,5;4;31,10;1;493,-1;-1
2862,ICLR,2019,Generalization and Regularization in DQN,Jesse Farebrother;Marlos C. Machado;Michael Bowling,jfarebro@ualberta.ca;machado@ualberta.ca;mbowling@ualberta.ca,6;5;5,3;5;5,Reject,0,3,0,yes,9/27/18,University of Alberta;University of Alberta;University of Alberta,99;99;99,119;119;119,8,9/27/18,31,21,9,0,25,4,32;620;1147,2;31;44,1;11;17,4;79;109,-1;-1
2863,ICLR,2019,Multi-agent Deep Reinforcement Learning with Extremely Noisy Observations,Ozsel Kilinc;Giovanni Montana,ozsel.kilinc@warwick.ac.uk;g.montana@warwick.ac.uk,6;7;3,3;2;4,Reject,0,9,0,yes,9/27/18,The university of Warwick;The university of Warwick,115;115,90;90,,9/27/18,7,1,3,1,0,3,58;1857,11;95,5;21,5;153,-1;-1
2864,ICLR,2019,Characterizing Attacks on Deep Reinforcement Learning,Chaowei Xiao;Xinlei Pan;Warren He;Bo Li;Jian Peng;Mingjie Sun;Jinfeng Yi;Mingyan Liu;Dawn Song.,xiaocw@umich.edu;xinleipan@berkeley.edu;_w@eecs.berkeley.edu;lxbosky@gmail.com;jianpeng@illinois.edu;sunmj15@mails.tsinghua.com;jinfengyi.ustc@gmail.com;mingyan@umich.edu;dawnsong@gmail.com,5;6;5,4;3;4,Reject,0,8,3,yes,9/27/18,"University of Michigan;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of Illinois, Urbana Champaign;Mails.tsinghua;JD AI Research;University of Michigan;University of California Berkeley",8;5;5;5;3;8;-1;8;5,21;18;18;18;37;30;-1;21;18,4,9/27/18,6,4,1,0,0,1,1337;186;1226;2412;994;417;2002;209;36930,31;16;22;80;74;34;79;29;276,12;6;14;23;16;9;24;5;95,147;8;129;273;66;51;241;12;4079,-1;-1
2865,ICLR,2019,Dopamine: A Research Framework for Deep Reinforcement Learning,Pablo Samuel Castro;Subhodeep Moitra;Carles Gelada;Saurabh Kumar;Marc G. Bellemare,psc@google.com;smoitra@google.com;cgel@google.com;kumasaurabh@google.com;bellemare@google.com,3;3;3,4;2;3,Reject,0,2,0,yes,9/27/18,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,,9/27/18,77,26,40,2,63,11,820;413;224;913;3989,30;12;6;114;57,14;6;5;15;24,53;58;38;72;628,-1;-1
2866,ICLR,2019,Probabilistic Program Induction for Intuitive Physics Game Play,Fahad Alhasoun,fha@mit.edu,3;4;2,4;2;4,Reject,0,0,0,yes,9/27/18,Massachusetts Institute of Technology,2,5,,9/27/18,0,0,0,0,0,0,53,15,5,3,-1
2867,ICLR,2019,Hallucinations in Neural Machine Translation,Katherine Lee;Orhan Firat;Ashish Agarwal;Clara Fannjiang;David Sussillo,katherinelee@google.com;orhanf@google.com;agarwal@google.com;clarafy@berkeley.edu;sussillo@google.com,6;4;7,5;4;4,Reject,0,12,0,yes,9/27/18,Google;Google;Google;University of California Berkeley;Google,-1;-1;-1;5;-1,-1;-1;-1;18;-1,3,9/27/18,14,12,2,0,0,0,266;3430;11426;37;3260,4;61;210;15;45,3;18;26;4;25,39;363;1252;1;266,-1;-1
2868,ICLR,2019,On Inductive Biases in Deep Reinforcement Learning,Matteo Hessel;Hado van Hasselt;Joseph Modayil;David Silver,mtthss@google.com;hado@google.com;modayil@google.com;davidsilver@google.com,3;3;7,4;4;2,Reject,0,4,0,yes,9/27/18,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,,9/27/18,6,3,2,0,0,0,2264;5507;1827;42470,26;50;40;158,13;21;17;56,365;910;191;5895,-1;-1
2869,ICLR,2019,Exponentially Decaying Flows for Optimization in Deep Learning,Mitsuharu Takeori;Kenta Nakamura,takeori.mitsuharu.d5s@jp.nssol.nssmc.com;nakamura.kenta.4n4@jp.nssol.nssmc.com,3;3;2,5;3;5,Withdrawn,0,0,,yes,9/27/18,NS Solutions Corporation;NS Solutions Corporation,-1;-1,-1;-1,8,9/27/18,0,0,0,0,0,0,9;833,3;83,1;17,0;45,-1;-1
2870,ICLR,2019,In search of theoretically grounded pruning,Filip Svoboda;Edgar Liberis;Nicholas D. Lane,filip.svoboda@stx.ox.ac.uk;edgar.liberis@chch.ox.ac.uk;nicholas.lane@cs.ox.ac.uk,4;3;5,3;4;3,Withdrawn,0,3,,yes,9/27/18,University of Oxford;University of Oxford;University of Oxford,50;50;50,1;1;1,,9/27/18,0,0,0,0,0,0,5;38;11518,13;8;171,1;3;46,0;2;683,-1;-1
2871,ICLR,2019,Structured Content Preservation for Unsupervised Text Style Transfer,Youzhi Tian;Zhiting Hu;Zhou Yu,yztian@ucdavis.edu;zhitingh@cs.cmu.edu;joyu@ucdavis.edu,5;6;4,4;3;5,Withdrawn,0,0,,yes,9/27/18,"University of California, Davis;Carnegie Mellon University;University of California, Davis",81;1;81,54;24;54,3,9/27/18,17,0,0,0,0,0,22;3398;2473,2;64;501,2;29;22,5;371;195,-1;-1
2872,ICLR,2019,Advanced Neuroevolution: A gradient-free algorithm to train Deep Neural Networks,Ahmed Aly;David Weikersdorfer;Claire Delaunay,aaa2cn@virginia.edu;dweikersdorfer@nvidia.com;cdelaunay@nvidia.com,1;1;5,5;5;4,Withdrawn,2,11,,yes,9/27/18,University of Virginia;NVIDIA;NVIDIA,65;-1;-1,113;-1;-1,,9/27/18,0,0,0,0,0,0,32;382;0,29;15;7,3;9;0,2;29;0,-1;-1
2873,ICLR,2019,Bridging HMMs and RNNs through Architectural Transformations,Jan Buys;Yonatan Bisk;Yejin Choi,jbuys@cs.washington.edu;ybisk@yonatanbisk.com;yejin@cs.washington.edu,3;5;5,3;4;4,Withdrawn,0,7,,yes,9/27/18,University of Washington;University of Washington;University of Washington,6;6;6,25;25;25,3,9/27/18,2,0,0,0,0,0,407;1175;8744,19;43;139,8;17;44,60;160;1040,-1;-1
2874,ICLR,2019,Learning with Little Data: Evaluation of Deep Learning Algorithms,Andreas Look;Stefan Riedelbauch,andreas.look@ihs.uni-stuttgart.de;stefan.riedelbauch@ihs.uni-stuttgart.de,6;4;4,4;3;5,Withdrawn,0,3,,yes,9/27/18,University of Stuttgart;University of Stuttgart,95;95,219;219,5;4;6;8,9/27/18,1,0,0,0,0,0,3;84,7;46,1;6,0;2,-1;-1
2875,ICLR,2019,Hierarchical Deep Reinforcement Learning Agent with Counter Self-play  on Competitive Games ,Huazhe Xu;Keiran Paster;Qibin Chen;Haoran Tang;Pieter Abbeel;Trevor Darrell;Sergey Levine,huazhe_xu@berkeley.edu;keirp@berkeley.edu;cqb@tsinghua.edu.cn;hrtang@math.berkeley.edu;pabbeel@cs.berkeley.edu;trevor@eecs.berkeley.edu;svlevine@eecs.berkeley.edu,3;2;2,3;4;3,Withdrawn,0,0,,yes,9/27/18,University of California Berkeley;University of California Berkeley;Tsinghua University;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;8;5;5;5;5,18;18;30;18;18;18;18,,9/27/18,1,0,0,0,0,0,822;1;57;652;38980;95486;25630,14;1;5;17;441;561;310,7;1;3;3;98;114;74,102;0;14;95;4590;11728;3323,-1;-1
2876,ICLR,2019,Label Smoothing and Logit Squeezing: A Replacement for Adversarial Training?,Ali Shafahi;Amin Ghiasi;Furong Huang;Tom Goldstein,ashafahi@cs.umd.edu;amin@cs.umd.edu;furongh@cs.umd.edu;tomg@cs.umd.edu,7;4;2,5;3;5,Withdrawn,13,9,,yes,9/27/18,"University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park",12;12;12;12,69;69;69;69,4,9/27/18,7,0,0,0,0,0,470;120;1879;6632,32;6;130;98,9;4;21;29,49;16;192;741,-1;-1
2877,ICLR,2019,Rotation Equivariant Networks via Conic Convolution and the DFT,Benjamin Chidester;Minh N. Do;Jian Ma,bchidest@andrew.cmu.edu;minhdo@illinois.edu;jianma@cs.cmu.edu,4;7;6,4;2;3,Withdrawn,0,1,,yes,9/27/18,"Carnegie Mellon University;University of Illinois, Urbana Champaign;Carnegie Mellon University",1;3;1,24;37;24,,9/27/18,0,0,0,0,0,0,40;246;384,13;29;34,3;6;9,4;7;4,-1;-1
2878,ICLR,2019,GradMix: Multi-source Transfer across Domains and Tasks,Junnan Li;Ziwei Xu;Yongkang Wong;Qi Zhao;Mohan S. Kankanhalli,lijunnan@u.nus.edu;ziwei-xu@comp.nus.edu.sg;yongkang.wong@nus.edu.sg;qzhao@cs.umn.edu;mohan@comp.nus.edu.sg,3;5;3,5;4;5,Withdrawn,0,4,,yes,9/27/18,"National University of Singapore;National University of Singapore;National University of Singapore;University of Minnesota, Minneapolis;National University of Singapore",16;16;16;57;16,22;22;22;56;22,6;2,9/27/18,1,0,0,0,0,0,81;464;1074;18264;9219,5;44;69;451;449,3;12;17;44;46,3;19;93;929;486,-1;-1
2879,ICLR,2019,Understanding and Improving Sequence-Labeling NER with Self-Attentive LSTMs,Peng-Hsuan Li;Wei-Yun Ma,jacobvsdanniel@iis.sinica.edu.tw;ma@iis.sinica.edu.tw,4;3;3,4;5;4,Withdrawn,0,4,,yes,9/27/18,Academia Sinica;Academia Sinica,-1;-1,-1;-1,,9/27/18,0,0,0,0,0,0,32;498,11;53,2;9,8;32,-1;-1
2880,ICLR,2019,Differentiable Greedy Networks,Thomas Powers;Rasool Fakoor;Siamak Shakeri;Abhinav Sethy;Amanjit Kainth;Abdel-rahman Mohamed;Ruhi Sarikaya,tcpowers@uw.edu;rasool.fakoor@mavs.uta.edu;siamaks@amazon.com;sethya@amazon.com;amanjitsingh.kainth@mail.utoronto.ca;asamir@cs.toronto.edu;rsarikay@amazon.com,5;2;4,4;5;4,Withdrawn,0,4,,yes,9/27/18,"University of Washington, Seattle;University of Texas, Arlington;Amazon;Amazon;Toronto University;Department of Computer Science, University of Toronto;Amazon",6;115;-1;-1;18;18;-1,25;601;-1;-1;22;22;-1,10,9/27/18,3,0,0,0,0,0,636;167;9;1733;4;20771;2644,31;16;9;100;2;56;121,9;5;2;25;1;28;27,36;7;2;107;2;1198;171,-1;-1
2881,ICLR,2019,Efficient Federated Learning via Variational Dropout,Wei Du;Xiao Zeng;Ming Yan;Mi Zhang,duwei1@msu.edu;zengxia6@msu.edu;myan@msu.edu;mizhang@msu.edu,4;4;3,4;3;4,Withdrawn,0,1,,yes,9/27/18,SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY,478;478;478;478,352;352;352;352,,9/27/18,6,0,0,0,0,0,1051;750;471;955,82;65;49;32,13;12;10;9,64;48;14;80,-1;-1
2882,ICLR,2019,Applications of Gaussian Processes in Finance,Rajbir S. Nirwan;Nils Bertschinger,nirwan@fias.uni-frankfurt.de;bertschinger@fias.uni-frankfurt.de,4;5;3,5;4;4,Withdrawn,0,2,,yes,9/27/18,Goethe University;Goethe University,65;65,70;70,11,9/27/18,0,0,0,0,0,0,1;1372,2;53,1;17,0;127,-1;-1
2883,ICLR,2019,An Attention-Based Model for Learning Dynamic Interaction Networks,Sandro Cavallari;Vincent W Zheng;Hongyun Cai;Erik Cambria,sandro001@e.ntu.edu.sg;vincent.zheng@adsc-create.edu.sg;hongyun.c@adsc.com.sg;cambria@ntu.edu.sg,4;3;4,3;5;4,Withdrawn,0,0,,yes,9/27/18,National Taiwan University;ADSC;Advanced Digital Sciences Center;National Taiwan University,85;-1;-1;85,197;-1;-1;197,10,9/27/18,0,0,0,0,0,0,187;3267;890;13793,10;85;33;330,4;23;8;65,15;237;68;765,-1;-1
2884,ICLR,2019,Modeling Evolution of Language Through Time with Neural Networks,Edouard Delasalles;Sylvain Lamprier;Ludovic Denoyer,edouard.delasalles@lip6.fr;sylvain.lamprier@lip6.fr;ludovic.denoyer@lip6.fr,3;4;4,5;5;4,Withdrawn,0,0,,yes,9/27/18,LIP6;LIP6;LIP6,-1;-1;-1,-1;-1;-1,3,9/27/18,0,0,0,0,0,0,20;275;3309,7;58;129,1;7;23,1;35;550,-1;-1
2885,ICLR,2019,Knowledge Representation for Reinforcement Learning using General Value Functions,Gheorghe Comanici;Doina Precup;Andre Barreto;Daniel Kenji Toyama;Eser Aygün;Philippe Hamel;Sasha Vezhnevets;Shaobo Hou;Shibl Mourad,gcomanici@google.com;doinap@google.com;andrebarreto@google.com;kenjitoyama@google.com;eser@google.com;hamelphi@google.com;vezhnick@google.com;shaobohou@google.com;shibl@google.com,6;7;4,3;3;4,Withdrawn,0,0,,yes,9/27/18,Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1,,9/27/18,0,0,0,0,0,0,68;10842;26;1;4;1875;0;305;46,10;327;5;2;7;28;2;14;13,4;39;3;1;1;6;0;8;2,5;1140;3;0;0;134;0;8;4,-1;-1
2886,ICLR,2019,,,vladymyrov@gmail.com,5;4;4,4;4;4,Withdrawn,0,0,,yes,9/27/18,,,,,9/27/18,-1,-1,-1,-1,-1,-1,-1,-1,-1,0,-1
2887,ICLR,2019,Geometric Operator Convolutional Neural Network,Yangling Ma;Yixin Luo;Zhouwang Yang,yangma@mail.ustc.edu.cn;seeing@mail.ustc.edu.cn;yangzw@ustc.edu.cn,2;5;3,5;5;4,Withdrawn,2,0,,yes,9/27/18,University of Science and Technology of China;University of Science and Technology of China;University of Science and Technology of China,478;478;478,132;132;132,4;1;8,9/4/18,2,0,0,0,0,0,2;1227;1040,2;38;69,1;16;13,1;91;50,-1;-1
2888,ICLR,2019,Online Bellman Residue Minimization via Saddle Point Optimization,Zhuoran Yang;Cheng Zhou;Tong Zhang;Han Liu,zy6@princeton.edu;mikechzhou@tencent.com;tongzhang@tongzhang-ml.org;hanliu.cmu@gmail.com,5;5;4,4;4;4,Withdrawn,0,3,,yes,9/27/18,Princeton University;Tencent AI Lab;;,30;-1;-1;-1,7;-1;-1;-1,,9/27/18,0,0,0,0,0,0,557;619;3396;6678,49;99;216;447,12;12;32;40,73;28;325;426,-1;-1
2889,ICLR,2019,Dual Importance Weight GAN,Gahye Lee;Seungkyu Lee,waldstein94@gmail.com;seungkyu@khu.ac.kr,4;3;5,4;5;4,Withdrawn,0,0,,yes,9/27/18,KyungHee univ.;Kyung Hee University,-1;-1,-1;-1,5;4,9/27/18,0,0,0,0,0,0,0;7,3;10,0;2,0;1,-1;-1
2890,ICLR,2019,Explainable Adversarial Learning: Implicit Generative Modeling of Random Noise during Training for Adversarial Robustness,Priyadarshini Panda;Kaushik Roy,pandap@purdue.edu;kaushik@purdue.edu,3;5;5,4;4;4,Withdrawn,0,7,,yes,9/27/18,Purdue University;Purdue University,26;26,60;60,5;4;1,7/5/18,7,0,0,0,0,0,674;22262,66;765,15;76,45;1551,-1;-1
2891,ICLR,2019,Nonlinear Channels Aggregation Networks for Deep Action Recognition,Zhigang Zhu;Hongbing Ji;Wenbo Zhang;Cheng Ouyang,zgzhu_xidian@163.com;hbji@xidian.edu.cn;zwbsoul@163.com;ouoyc@aliyun.com,3;3;3,3;4;5,Withdrawn,0,0,,yes,9/27/18,Tsinghua University;Tsinghua University;163;Aliyun,8;8;-1;-1,30;30;-1;-1,8,9/27/18,0,0,0,0,0,0,2128;4722;3640;486,182;381;349;52,22;33;33;11,93;69;139;29,-1;-1
2892,ICLR,2019,A SINGLE SHOT PCA-DRIVEN ANALYSIS OF NETWORK STRUCTURE TO REMOVE REDUNDANCY,Isha Garg;Priyadarshini Panda;Kaushik Roy,gargi@purdue.edu;pandap@purdue.edu;kaushik@purdue.edu,4;4;5,5;4;5,Withdrawn,0,0,,yes,9/27/18,Purdue University;Purdue University;Purdue University,26;26;26,60;60;60,3;2,9/27/18,0,0,0,0,0,0,38;674;1125,12;66;135,4;15;20,4;45;55,-1;-1
2893,ICLR,2019,D2KE: From Distance to Kernel and Embedding via Random Features For Structured Inputs,Lingfei Wu;Ian E.H. Yen;Fangli Xu;Pradeep Ravikumar;Michael J. Witbrock,lwu@email.wm.edu;eyan@cs.cmu.edu;fxu02@email.wm.edu;pradeepr@cs.cmu.edu;witbrock@us.ibm.com,4;3;5,4;4;4,Withdrawn,0,0,,yes,9/27/18,College of William and Mary;Carnegie Mellon University;College of William and Mary;Carnegie Mellon University;International Business Machines,169;1;169;1;-1,261;24;261;24;-1,10,9/27/18,0,0,0,0,0,0,102;531;411;9484;2381,33;52;14;183;100,6;12;8;40;24,10;72;31;1254;158,-1;-1
2894,ICLR,2019,Latent Transformations for Object  View Points Synthesis,Sangpil Kim;Nick Winovich;Hyung-gun Chi;Guang Lin;Karthik Ramani,kim2030@purdue.edu;nwinovic@purdue.edu;chi45@purdue.edu;guanglin@purdue.edu;ramani@purdue.edu,4;2;5,4;4;2,Withdrawn,0,2,,yes,9/27/18,Purdue University;Purdue University;Purdue University;Purdue University;Purdue University,26;26;26;26;26,60;60;60;60;60,5;4,7/12/18,1,0,0,0,0,0,776;12;-1;1661;4410,22;5;-1;152;159,6;1;-1;23;28,160;0;0;51;245,-1;-1
2895,ICLR,2019,Network Reparameterization for Unseen Class Categorization,Kai Li;Martin Renqiang Min;Bing Bai;Yun Fu;Hans Peter Graf,li.kai.gml@gmail.com;renqiang@nec-labs.com;bbai@nec-labs.com;yunfu@ece.neu.edu;hpg@nec-labs.com,5;3;5,5;5;3,Withdrawn,7,2,,yes,9/27/18,Northeastern University;NEC-Labs;NEC-Labs;Northeastern University;NEC-Labs,16;-1;-1;16;-1,839;-1;-1;839;-1,6,9/27/18,3,0,0,0,0,0,29813;947;216;11652;3797,1399;58;33;328;113,74;15;6;56;28,2734;105;15;1176;424,-1;-1
2896,ICLR,2019,Explaining Neural Networks Semantically and Quantitatively,Hao Chen;Runjin Chen;Quanshi Zhang,bridgechen@hust.edu.cn;chenrunjin@sjtu.edu.cn;zqs1022@sjtu.edu.cn,4;4;4,4;5;4,Withdrawn,0,4,,yes,9/27/18,Hong Kong University of Science and Technology;Shanghai Jiao Tong University;Shanghai Jiao Tong University,39;52;52,44;188;188,,9/27/18,8,0,0,0,0,0,4787;20;1226,460;2;77,34;2;17,352;1;44,-1;-1
2897,ICLR,2019,Explaining AlphaGo: Interpreting Contextual Effects in Neural Networks,Zenan Ling;Haotian Ma;Yu Yang;Robert C. Qiu;Song-Chun Zhu;Quanshi Zhang,lingzenan@sjtu.edu.cn;11612807@mail.sustc.edu.cn;yy19970901@ucla.edu;rqiu@tntech.edu;sczhu@stat.ucla.edu;zqs1022@sjtu.edu.cn,3;4;4,5;4;5,Withdrawn,1,3,,yes,9/27/18,"Shanghai Jiao Tong University;University of Science and Technology of China;University of California, Los Angeles;Tennessee Technological University;University of California, Los Angeles;Shanghai Jiao Tong University",52;478;20;-1;20;52,188;132;15;-1;15;188,,9/27/18,1,0,0,0,0,0,115;65;2626;3954;15118;1226,22;22;386;230;449;77,6;5;24;33;64;17,9;1;102;199;1009;44,-1;-1
2898,ICLR,2019,Deepström Networks,Luc Giffon;Hachem Kadri;Stéphane Ayache;Thierry Artières,luc.giffon@lis-lab.fr;hachem.kadri@lis-lab.fr;stephane.ayache@lis-lab.fr;thierry.artieres@lis-lab.fr,4;5;3,4;4;5,Withdrawn,0,1,,yes,9/27/18,Aix Marseille Université;Aix Marseille Université;Aix Marseille Université;Aix Marseille Université,478;478;478;478,297;297;297;297,,9/27/18,-1,-1,-1,-1,-1,-1,-1;-1;-1;-1,-1;-1;-1;-1,-1;-1;-1;-1,0;0;0;0,-1;-1
2899,ICLR,2019,One Bit Matters: Understanding Adversarial Examples as the Abuse of Redundancy,Jingkang Wang;Ruoxi Jia;Gerald Friedland;Bo Li;Costas Spanos,wangjksjtu_01@sjtu.edu.cn;ruoxijia@berkeley.edu;fractor@eecs.berkeley.edu;lxbosky@gmail.com;spanos@berkeley.edu,3;3;3,4;4;4,Withdrawn,2,1,,yes,9/27/18,Shanghai Jiao Tong University;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,52;5;5;5;5,188;18;18;18;18,4;1,9/27/18,2,0,0,0,0,0,58;424;9700;32234;4502,7;36;403;2411;328,3;13;46;75;35,3;27;534;2062;185,-1;-1
2900,ICLR,2019,,,samitha.herath@data61.csiro.au;u5505348@anu.edu.au;mehrtash.harandi@monash.edu,4;3;5,4;4;5,Withdrawn,0,3,,yes,9/27/18,", CSIRO;Australian National University;Monash University",-1;106;123,-1;48;80,,9/27/18,-1,-1,-1,-1,-1,-1,-1,-1,-1,0,-1
2901,ICLR,2019,Context-aware Forecasting for Multivariate Stationary Time-series,Valentin Guiguet;Nicolas Baskiotis;Vincent Guigue;Patrick Gallinari,guiguetvalentin@gmail.com;nicolas.baskiotis@lip6.fr;vincent.guigue@lip6.fr;patrick.gallinari@lip6.fr,5;5;4,3;5;4,Withdrawn,0,1,,yes,9/27/18,LIP6;LIP6;LIP6;LIP6,-1;-1;-1;-1,-1;-1;-1;-1,,9/27/18,0,0,0,0,0,0,0;552;1076;5287,3;46;57;451,0;9;9;35,0;45;117;383,-1;-1
2902,ICLR,2019,HANDLING CONCEPT DRIFT  IN WIFI-BASED INDOOR LOCALIZATION USING REPRESENTATION LEARNING,Raihan Seraj;Negar Ghourchian;Michel Allegue-Martinez,raihan.seraj@mail.mcgill.ca;negar.gh@aerial.ai;michel.allegue@aerial.ai,2;3;4,1;4;4,Withdrawn,0,0,,yes,9/27/18,McGill University;Aerial Technologies Inc.;Aerial Technologies Inc.,85;-1;-1,42;-1;-1,,9/27/18,0,0,0,0,0,0,1;22;145,4;9;21,1;2;7,0;1;12,-1;-1
2903,ICLR,2019,Transfer Learning for Estimating Causal Effects Using Neural Networks,Sören R. Künzel;Bradly C. Stadie;Nikita Vemuri;Varsha Ramakrishnan;Jasjeet S. Sekhon;Pieter Abbeel,srk@berkeley.edu;bstadie@berkeley.edu;nikitavemuri@berkeley.edu;vio@berkeley.edu;sekhon@berkeley.edu;pabbeel@cs.berkeley.edu,7;5;3,3;4;3,Withdrawn,0,0,,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5;5;5,18;18;18;18;18;18,6,8/23/18,11,0,0,0,0,0,170;792;72;496;4101;38980,6;18;6;37;78;441,4;7;3;9;21;98,25;50;8;28;215;4590,-1;-1
2904,ICLR,2019,Variational Autoencoders for Text Modeling without Weakening the Decoder,Ryo Kamoi;Hiroyasu Fukutomi,ryo_kamoi_st@keio.jp;hiroyasu.fukutomi@datasection.co.jp,4;4;1,3;5;4,Withdrawn,6,1,,yes,9/27/18,Keio University;,62;-1,12;-1,5,9/27/18,0,0,0,0,0,0,2;0,3;1,1;0,0;0,-1;-1
2905,ICLR,2019,A PRIVACY-PRESERVING IMAGE CLASSIFICATION FRAMEWORK WITH A LEARNABLE OBFUSCATOR,Xiangyi Meng;Zixuan Huang;Yuefeng Du;Antoni Chan;Cong Wang,xy.meng@my.cityu.edu.hk;zixuhuang3-c@my.cityu.edu.hk;yf.du@my.cityu.edu.hk;abchan@cityu.edu.hk;congwang@cityu.edu.hk,5;5;5,4;4;5,Withdrawn,2,0,,yes,9/27/18,City University of Hong Kong;City University of Hong Kong;City University of Hong Kong;City University of Hong Kong;City University of Hong Kong,89;89;89;89;89,40;40;40;40;40,4,9/27/18,0,0,0,0,0,0,216;81;187;344;22537,22;27;44;34;978,8;5;6;6;55,6;7;2;18;1113,-1;-1
2906,ICLR,2019,ODIN: Outlier Detection In Neural Networks,Rickard Sjögren;Johan Trygg,rickard.sjoegren@sartorius-stedim.com;johan.trygg@sartorius-stedim.com,5;4;4,4;4;4,Withdrawn,1,4,,yes,9/27/18,Computational Life Science Cluster;Sartorius-stedim,-1;-1,-1;-1,,9/27/18,1,0,0,0,0,0,134;10487,16;194,6;42,6;647,-1;-1
2907,ICLR,2019,Improving latent variable descriptiveness by modelling rather than ad-hoc factors,Alex Mansbridge;Roberto Fierimonte;Ilya Feige;David Barber,amansbridge@turing.ac.uk;roberto.fierimonte@gmail.com;ilya@asidatascience.com;david.barber@ucl.ac.uk,4;4;6,4;4;3,Withdrawn,0,3,,yes,9/27/18,Alan Turing Institute;;University College London;University College London,-1;-1;50;50,-1;-1;16;16,3;5;1,9/27/18,0,0,0,0,0,0,1;91;240;4250,2;9;20;199,1;4;6;28,0;0;8;426,-1;-1
2908,ICLR,2019,Capacity of Deep Neural Networks under Parameter Quantization,Yoonho Boo;Sungho Shin;and Wonyong Sung,dnsgh337@snu.ac.kr;ssh9919@snu.ac.kr;wysung@snu.ac.kr,5;5;5,3;4;3,Withdrawn,0,0,,yes,9/27/18,Seoul National University;Seoul National University;Seoul National University,41;41;41,74;74;74,3,9/27/18,4,0,0,0,0,0,70;274;4252,14;61;195,4;7;30,12;22;385,-1;-1
2909,ICLR,2019,Learning of Sophisticated Curriculums by viewing them as Graphs over Tasks,Lucas Willems;Yoshua Bengio,lcswillems@gmail.com;yoshua.bengio@umontreal.ca,3;2;4,1;2;4,Withdrawn,0,3,,yes,9/27/18,Ecole Normale Superieure;University of Montreal,99;123,603;108,,9/27/18,0,0,0,0,0,0,286;218416,18;809,6;152,23;24837,-1;-1
2910,ICLR,2019,RNNs with Private and Shared Representations for Semi-Supervised Sequence Learning,Ge Ya Luo;Jie Fu;Pengfei Liu;Zhi Hao Luo;Chris Pal,olga.xu@umontreal.ca;jie.fu@polymtl.ca;pfliu14@fudan.edu.cn;zhi-hao.luo@polymtl.ca;christopher.pal@polymtl.ca,3;5;4,5;5;4,Withdrawn,0,3,,yes,9/27/18,University of Montreal;Polytechnique Montreal;Fudan University;Polytechnique Montreal;Polytechnique Montreal,123;386;78;386;386,108;108;116;108;108,,9/27/18,0,0,0,0,0,0,330;4001;2252;0;862,16;306;189;3;58,9;32;21;0;12,23;125;239;0;72,-1;-1
2911,ICLR,2019,MAJOR-MINOR LSTMS FOR WORD-LEVEL LANGUAGE MODEL,Kai Shuang;Rui Li;Mengyu Gu;Qianqian Yang;Jonathan;Sen Su,shuangk@bupt.edu.cn;lirui@bupt.edu.cn;pattygu0622@bupt.edu.cn;echo_yang@bupt.edu.cn;jonathan.loo@uwl.ac.uk;susen@bupt.edu.cn,4;3;3,5;4;5,Withdrawn,2,4,,yes,9/27/18,Beijing University of Post and Telecommunication;Beijing University of Post and Telecommunication;Beijing University of Post and Telecommunication;Beijing University of Post and Telecommunication;;Beijing University of Post and Telecommunication,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,3,9/27/18,0,0,0,0,0,0,954;5765;0;1262;381;329,116;504;4;108;536;38,14;32;0;19;9;7,78;180;0;25;25;26,-1;-1
2912,ICLR,2019,SALSA-TEXT : SELF ATTENTIVE LATENT SPACE BASED ADVERSARIAL TEXT GENERATION,Jules Gagnon-Marchand;Hamed Sadeghi;Mehdi Rezagholizadeh;Md. Akmal Haider,jgagnonmarchand@gmail.com;haamed.sadeghi@gmail.com;mehdi.rezagholizadeh@gmail.com;md.akmal.haidar@huawei.com,4;4;5,3;4;4,Withdrawn,0,0,,yes,9/27/18,Huawei Technologies Ltd.;Huawei Technologies Ltd.;;Huawei Technologies Ltd.,-1;-1;-1;-1,-1;-1;-1;-1,5;4,9/27/18,7,0,0,0,0,0,9;76;155;93,2;14;32;25,2;6;6;6,0;4;5;7,-1;-1
2913,ICLR,2019,,Qingpeng Cai;Ling Pan;Pingzhong Tang,cqpcurry@gmail.com;penny.ling.pan@gmail.com;kenshinping@gmail.com,4;5;1,4;3;4,Withdrawn,0,2,,yes,9/27/18,Tsinghua University;Tsinghua University;Tsinghua University,8;8;8,30;30;30,,9/27/18,-1,-1,-1,-1,-1,-1,-1;-1;-1,-1;-1;-1,-1;-1;-1,0;0;0,-1;-1
2914,ICLR,2019,Neuron Hierarchical Networks,Han Yue;De-An Wu;Lei Wu;Ji Xie,johnhany@163.com;wudean.cn@uestc.edu.cn;wulei@uestc.edu.cn;zonghengxs@163.com,5;4;4,4;4;4,Withdrawn,0,0,,yes,9/27/18,University of Electronic Science and Technology of China;University of Electronic Science and Technology of China;University of Electronic Science and Technology of China;163,169;169;169;-1,843;843;843;-1,10,9/27/18,0,0,0,0,0,0,25;217;268;35,29;31;31;17,3;5;8;4,0;13;12;1,-1;-1
2915,ICLR,2019,Linearizing Visual Processes with Deep Generative Models,Alexander Sagel;Hao Shen,a.sagel@tum.de;shen@fortiss.org,3;3;4,4;4;3,Withdrawn,0,3,,yes,9/27/18,Technical University Munich;Fortiss,54;-1,41;-1,5;4,9/27/18,0,0,0,0,0,0,6;352,14;44,2;8,0;3,-1;-1
2916,ICLR,2019,Inhibited Softmax for Uncertainty Estimation in Neural Networks,Marcin Możejko;Mateusz Susik;Rafał Karczewski,marcin@sigmoidal.io;msusik@sigmoidal.io;rafal@sigmoidal.io,4;4;3,4;3;4,Withdrawn,0,1,,yes,9/27/18,;;,-1;-1;-1,-1;-1;-1,,9/27/18,5,0,0,0,0,0,32;39;11,10;14;3,3;3;2,0;6;1,-1;-1
2917,ICLR,2019,Improving Gaussian mixture latent variable model convergence with Optimal Transport,Benoit Gaujac;Ilya Feige;David Barber,benoit.gaujac.16@ucl.ac.uk;ilya@asidatascience.com;david.barber@ucl.ac.uk,5;5;5,3;4;4,Withdrawn,0,3,,yes,9/27/18,University College London;University College London;University College London,50;50;50,16;16;16,5,9/27/18,0,0,0,0,0,0,5;240;4250,3;20;199,1;6;28,1;8;426,-1;-1
2918,ICLR,2019,From Amortised to Memoised Inference: Combining Wake-Sleep and Variational-Bayes for Unsupervised Few-Shot Program Learning,Luke B. Hewitt;Joshua B. Tenenbaum,lbh@mit.edu;jbt@mit.edu,3;3;3,5;5;4,Withdrawn,0,1,,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2,5;5,5;11,9/27/18,0,0,0,0,0,0,41;33543,7;599,2;88,4;2752,-1;-1
2919,ICLR,2019,,,v-ziclin@microsoft.com;lizo@microsoft.com,6;2;4,2;5;3,Withdrawn,3,0,,yes,9/27/18,Microsoft;Microsoft,-1;-1,-1;-1,,9/27/18,-1,-1,-1,-1,-1,-1,-1,-1,-1,0,-1
2920,ICLR,2019,Encoder Discriminator Networks for Unsupervised Representation Learning,Nils Wandel,nils.wandel@ais.uni-bonn.de,3;4;3,4;4;5,Withdrawn,0,4,,yes,9/27/18,University of Bonn,123,100,,9/27/18,0,0,0,0,0,0,0,4,0,0,-1
2921,ICLR,2019,Geometry of Deep Convolutional Networks,Stefan Carlsson,stefanc@kth.se,2;4;3,5;4;2,Withdrawn,0,0,,yes,9/27/18,"KTH Royal Institute of Technology, Stockholm, Sweden",140,173,,9/27/18,2,0,0,0,0,0,8856,308,41,536,-1
2922,ICLR,2019,Learning and Data Selection in Big Datasets,Hossein S. Ghadikolaei;Hadi Ghauch;Carlo Fischione;Mikael Skoglund,hshokri@kth.se;ghauch@kth.se;carlofi@kth.se;skoglund@kth.se,4;3;3,3;4;5,Withdrawn,0,0,,yes,9/27/18,"KTH Royal Institute of Technology, Stockholm, Sweden;KTH Royal Institute of Technology, Stockholm, Sweden;KTH Royal Institute of Technology, Stockholm, Sweden;KTH Royal Institute of Technology, Stockholm, Sweden",140;140;140;140,173;173;173;173,8,9/27/18,1,0,0,0,0,0,888;329;4249;6154,46;40;283;583,14;8;33;36,52;18;247;419,-1;-1
2923,ICLR,2019,Data Poisoning Attack against Unsupervised Node Embedding Methods,Mingjie Sun;Jian Tang;Huichen Li;Bo Li;Chaowei Xiao;Yao Chen;Dawn Song,sunmj15@gmail.com;tangjianpku@gmail.com;huichen3@illinois.edu;lxbosky@gmail.com;xiaocw@umich.edu;antoniechen@tencent.com;dawnsong@gmail.com,4;4;4,5;4;3,Withdrawn,0,0,,yes,9/27/18,"Tsinghua University;HEC Montreal;University of Illinois, Urbana Champaign;University of California Berkeley;University of Michigan;Tencent AI Lab;University of California Berkeley",8;-1;3;5;8;-1;5,30;-1;37;18;21;-1;18,4;10,9/27/18,17,0,0,0,0,0,458;1522;35;2478;1449;2776;40714,34;119;4;80;31;293;279,9;18;3;23;13;25;100,55;81;4;282;151;163;3986,-1;-1
2924,ICLR,2019,Shaping representations through communication,Olivier Tieleman;Angeliki Lazaridou;Shibl Mourad;Charles Blundell;Doina Precup,tieleman@google.com;angeliki@google.com;shibl@google.com;cblundell@google.com;doinap@google.com,5;4;5,4;4;4,Withdrawn,0,1,,yes,9/27/18,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,6,9/27/18,6,0,0,0,0,0,250;1821;46;6392;10842,23;77;13;50;327,5;22;2;23;39,3;194;4;1132;1140,-1;-1
2925,ICLR,2019,,,hongyang.gao@wsu.edu,5;4;4,4;5;4,Withdrawn,6,3,,yes,9/27/18,SUN YAT-SEN UNIVERSITY,478,352,,9/27/18,-1,-1,-1,-1,-1,-1,-1,-1,-1,0,-1
2926,ICLR,2019,,,na@na.edu,3;5;5,4;4;4,Withdrawn,0,3,,yes,9/27/18,University of Arizona,169,161,,9/27/18,-1,-1,-1,-1,-1,-1,-1,-1,-1,0,-1
2927,ICLR,2019,Exploiting Invariant Structures for Compression in Neural Networks,Jiahao Su;Jingling Li;Bobby Bhattacharjee;Furong Huang,jiahaosu@terpmail.umd.edu;jingling@cs.umd.edu;bobby@cs.umd.edu;furongh@cs.umd.edu,4;4;4,4;4;4,Withdrawn,0,1,,yes,9/27/18,"University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park",12;12;12;12,69;69;69;69,9,9/27/18,1,0,0,0,0,0,20;49;8537;1879,13;8;147;130,3;4;32;21,3;8;880;192,-1;-1
2928,ICLR,2019,Evading Defenses to Transferable Adversarial Examples by Mitigating Attention Shift,Yinpeng Dong;Tianyu Pang;Hang Su;Jun Zhu,dyp17@mails.tsinghua.edu.cn;pty17@mails.tsinghua.edu.cn;suhangss@mail.tsinghua.edu.cn;dcszj@mail.tsinghua.edu.cn,4;4;4,3;4;3,Withdrawn,0,5,,yes,9/27/18,Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University,8;8;8;8,30;30;30;30,4,9/27/18,0,0,0,0,0,0,1422;998;7151;5025,28;26;412;203,15;10;34;37,212;186;518;530,-1;-1
2929,ICLR,2019,Classification of Building Noise Type/Position via Supervised Learning,Hwiyong Choi;Haesang Yang;Seungjun Lee;Woojae Seong,its_me_chy@snu.ac.kr;coupon3@snu.ac.kr;tl7qns7ch@snu.ac.kr;wseong@snu.ac.kr,4;4;4,2;4;4,Withdrawn,0,0,,yes,9/27/18,Seoul National University;Seoul National University;Seoul National University;Seoul National University,41;41;41;41,74;74;74;74,,9/27/18,0,0,0,0,0,0,2;11;411;440,5;18;114;136,1;2;13;12,0;0;8;13,-1;-1
2930,ICLR,2019,Nesterov's method is the discretization of a differential equation with Hessian damping,Adam M. Oberman;Maxime Laborde,adam.oberman@mcgill.ca;maxime.laborde@mcgill.ca,4;5;6,5;4;5,Withdrawn,5,0,,yes,9/27/18,McGill University;McGill University,85;85,42;42,1,9/27/18,0,0,0,0,0,0,1851;140,88;19,22;7,160;7,-1;-1
2931,ICLR,2019,"A Forensic Representation to Detect Non-Trivial Image Duplicates, and How it Applies to Semantic Segmentation",M. Cicconet;H. Elliott;D.L. Richmond;D. Wainstock;M. Walsh,cicconet@gmail.com;elliott.hunter@gmail.com;daverichmond@gmail.com;daniel_wainstock@hms.harvard.edu;mary_walsh@hms.harvard.edu,4;3;2,4;5;5,Withdrawn,0,0,,yes,9/27/18,Harvard University;;;Harvard University;Harvard University,39;-1;-1;39;39,6;-1;-1;6;6,2,9/27/18,0,0,0,0,0,0,509;1588;150;3;244,55;29;11;3;62,10;12;8;1;7,34;90;3;0;12,-1;-1
2932,ICLR,2019,End-to-end Learning of a Convolutional Neural Network via Deep Tensor Decomposition,Samet Oymak;Mahdi Soltanolkotabi,sametoymak@gmail.com;soltanol@usc.edu,5;5;5,3;3;3,Withdrawn,0,0,,yes,9/27/18,"University of California, Riverside;University of Southern California",57;30,197;66,,5/16/18,6,0,0,0,0,0,1692;3017,60;56,22;20,158;359,-1;-1
2933,ICLR,2019,Domain Adaptive Transfer Learning,Jiquan Ngiam;Daiyi Peng;Vijay Vasudevan;Simon Kornblith;Quoc Le;Ruoming Pang,jngiam@google.com;daiyip@google.com;vrv@google.com;skornblith@google.com;qvl@google.com;rpang@google.com,3;4;7,5;4;4,Withdrawn,0,3,,yes,9/27/18,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,6;2,9/27/18,1,0,0,0,0,0,3871;22;18926;1123;51065;3751,30;3;55;50;193;49,14;1;18;12;83;23,410;4;2363;145;6266;516,-1;-1
2934,ICLR,2019,Deep clustering based on a mixture of autoencoders,Shlomo E. Chazan;Sharon Gannot;Jacob Goldberger,shlomi.chazan@biu.ac.il;sharon.gannot@biu.ac.il;jacob.goldberger@biu.ac.il,6;4;5,3;3;5,Withdrawn,0,1,,yes,9/27/18,Bar Ilan University;Bar Ilan University;Bar Ilan University,95;95;95,456;456;456,,9/27/18,2,0,0,0,0,0,91;5372;5740,16;275;180,6;37;36,11;315;559,-1;-1
2935,ICLR,2019,Live Face De-Identification in Video,Oran Gafni;Lior Wolf;Yaniv Taigman,oran@fb.com;wolf@fb.com;yaniv@fb.com,6;4;6,4;4;4,Withdrawn,0,4,,yes,9/27/18,Facebook;Facebook;Facebook,-1;-1;-1,-1;-1;-1,,9/27/18,7,0,0,0,0,0,12;15230;6303,3;199;26,2;47;16,1;1643;558,-1;-1
2936,ICLR,2019,IMAGE DEFORMATION META-NETWORK FOR ONE-SHOT LEARNING,Zitian Chen;Yanwei Fu;Yu-Xiong Wang;Lin Ma;Wei Liu;Martial Hebert,tankche2@gmail.com;yanweifu@fudan.edu.cn;yuxiongw@cs.cmu.edu;forest.linma@gmail.com;wl2223@columbia.edu;hebert@ri.cmu.edu,5;7;6,4;1;4,Withdrawn,0,3,,yes,9/27/18,Fudan University;Fudan University;Carnegie Mellon University;Tencent AI Lab;Columbia University;Carnegie Mellon University,78;78;1;-1;15;1,116;116;24;-1;14;24,6,9/27/18,23,0,0,0,0,0,549;94;1438;2031;188;32486,25;22;34;107;53;521,13;6;16;27;8;95,16;2;119;192;9;2586,-1;-1
2937,ICLR,2019,Towards Resisting Large Data Variations via Introspective Learning,Yunhan Zhao;Ye Tian;Wei Shen;Alan Yuille,yzhao83@jhu.edu;tytian@outlook.com;shenwei1231@gmail.com;alan.l.yuille@gmail.com,4;5;6,4;4;3,Withdrawn,0,7,,yes,9/27/18,Johns Hopkins University;Verb Surgical;Johns Hopkins University;Johns Hopkins University,72;-1;72;72,13;-1;13;13,5,9/27/18,1,0,0,0,0,0,23;579;1229;35215,7;93;47;495,2;13;13;83,1;51;92;3815,-1;-1
2938,ICLR,2019,Realistic Adversarial Examples in 3D Meshes,Chaowei Xiao;Dawei Yang;Bo Li;Jia Deng;Mingyan Liu,xiaocw@umich.edu;ydawei@umich.edu;lxbosky@gmail.com;jiadeng@cs.princeton.edu;mingyan@umich.edu,5;3;5,3;3;3,Withdrawn,0,0,,yes,9/27/18,University of Michigan;University of Michigan;University of California Berkeley;Princeton University;University of Michigan,8;8;5;30;8,21;21;18;7;21,4,9/27/18,15,0,0,0,0,0,1449;1205;32234;16115;13402,31;86;2411;81;263,13;19;75;29;41,151;71;2062;2730;1927,-1;-1
2939,ICLR,2019,Representation Flow for Action Recognition,AJ Piergiovanni;Michael S. Ryoo,ajpiergi@indiana.edu;mryoo@indiana.edu,3;5;5,5;5;4,Withdrawn,2,10,,yes,9/27/18,University of Arizona;University of Arizona,169;169,161;161,,9/27/18,23,0,0,0,0,0,190;3161,33;103,8;25,12;275,-1;-1
2940,ICLR,2019,PointGrow: Autoregressively Learned Point Cloud Generation with Self-Attention,Yongbin Sun;Yue Wang;Ziwei Liu;Joshua E. Siegel;Sanjay Sarma,yb_sun@mit.edu;yuewang@csail.mit.edu;zwliu.hust@gmail.com;j_siegel@mit.edu;sesarma@mit.edu,3;6;6,4;4;5,Withdrawn,0,0,,yes,9/27/18,Massachusetts Institute of Technology;Massachusetts Institute of Technology;The Chinese University of Hong Kong;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;57;2;2,5;5;40;5;5,,9/27/18,21,0,0,0,0,0,1315;-1;5682;239;6735,47;-1;88;33;181,15;-1;21;7;37,188;0;1266;8;590,-1;-1
2941,ICLR,2019,Stacked U-Nets: A No-Frills Approach to Natural Image Segmentation,Sohil Shah;Pallabi Ghosh;Larry S Davis;Tom Goldstein,sohilas@umd.edu;tomg@cs.umd.edu;pallabig@umd.edu;lsd@umiacs.umd.edu,5;3;5,5;5;5,Withdrawn,0,1,,yes,9/27/18,"University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park",12;12;12;12,69;69;69;69,2,4/27/18,25,0,0,0,0,0,134;62;41840;6632,19;20;752;98,6;5;104;29,13;4;3267;741,-1;-1
2942,ICLR,2019,UNSUPERVISED CONVOLUTIONAL NEURAL NETWORKS FOR ACCURATE VIDEO FRAME INTERPOLATION WITH INTEGRATION OF MOTION COMPONENTS,Thang Van Nguyen;Kyu-Joong Lee;Hyuk-Jae Lee,itmanhieu@snu.ac.kr;kyujoonglee@sunmoon.ac.kr;hjlee@capp.snu.ac.kr,3;5;4,4;4;5,Withdrawn,0,0,,yes,9/27/18,Seoul National University;Kyung Hee;Seoul National University,41;-1;41,74;-1;74,10,9/27/18,0,0,0,0,0,0,212;74;913,35;13;217,8;3;17,14;8;47,-1;-1
2943,ICLR,2019,Compositional GAN: Learning Conditional Image Composition,Samaneh Azadi;Deepak Pathak;Sayna Ebrahimi;Trevor Darrell,sazadi@berkeley.edu;pathak@berkeley.edu;sayna@berkeley.edu;trevor@eecs.berkeley.edu,4;4;5,5;4;4,Withdrawn,0,5,,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5,18;18;18;18,5;4,7/19/18,21,0,0,0,0,0,281;4339;189;95486,12;42;19;561,7;14;7;114,38;575;28;11728,-1;-1
2944,ICLR,2019,PolyCNN: Learning Seed Convolutional Filters,Felix Juefei-Xu;Vishnu Naresh Boddeti;Marios Savvides,juefei.xu@gmail.com;vishnu@msu.edu;msavvide@ri.cmu.edu,3;4;4,4;2;3,Withdrawn,0,0,,yes,9/27/18,Alibaba Group;SUN YAT-SEN UNIVERSITY;Carnegie Mellon University,-1;478;1,-1;352;24,,9/27/18,0,0,0,0,0,0,315;971;6320,13;58;286,8;18;40,16;87;603,-1;-1
2945,ICLR,2019,A Main/Subsidiary Network Framework for Simplifying Binary Neural Networks,Yinghao Xu;Xin Dong;Yudian Li;Hao Su,justimyhxu@zju.edu.cn;xindong@g.harvard.edu;daniellee2519@gmail.com;haosu@eng.ucsd.edu,5,4,Withdrawn,0,0,,yes,9/27/18,"Zhejiang University;Harvard University;University of Electronic Science and Technology of China;University of California, San Diego",57;39;169;11,177;6;843;31,,9/27/18,4,0,0,0,0,0,125;-1;19;9437,21;-1;5;114,8;-1;3;30,5;0;1;2109,-1;-1
2946,ICLR,2019,A Teacher Student Network For Faster Video Classification,Shweta Bhardwaj;Mukundhan Srinivasan;Mitesh M. Khapra,cs16s003@cse.iitm.ac.in;msrinivasan@nvidia.com;miteshk@cse.iitm.ac.in,4;4;4,4;5;5,Withdrawn,0,0,,yes,9/27/18,Indian Institute of Technology Madras;NVIDIA;Indian Institute of Technology Madras,153;-1;153,625;-1;625,,9/27/18,0,0,0,0,0,0,115;60;1415,24;23;92,5;4;19,10;2;155,-1;-1
2947,ICLR,2019,Data Interpretation and Reasoning Over Scientific Plots,Pritha Ganguly;Nitesh Methani;Mitesh M. Khapra,prithag@cse.iitm.ac.in;nmethani@cse.iitm.ac.in,6;6;3,4;4;4,Withdrawn,0,0,,yes,9/27/18,Indian Institute of Technology Madras;Indian Institute of Technology Madras,153;153,625;625,10,9/27/18,0,0,0,0,0,0,6;1;1415,6;4;92,2;1;19,0;0;155,-1;-1
2948,ICLR,2019,Logit Regularization Methods for Adversarial Robustness,Cecilia Summers;Michael J. Dinneen,ceciliasummers07@gmail.com;mjd@cs.auckland.ac.nz,3;5;2,5;5;5,Withdrawn,10,5,,yes,9/27/18,University of Auckland;University of Auckland,261;261,191;191,4,9/27/18,1,0,0,0,0,0,20;60,4;22,2;5,1;3,f;m
2949,ICLR,2019,Feature Matters: A Stage-by-Stage Approach for Task Independent Knowledge Transfer,Mengya Gao;Yujun Shen;Quanquan Li;Liang Wan;Xiaoou Tang,daisy@tju.edu.cn;sy116@ie.cuhk.edu.hk;liquanquan@sensetime.com;lwan@tju.edu.cn;xtang@ie.cuhk.edu.hk,5;4;4,4;4;4,Withdrawn,0,0,,yes,9/27/18,Zhejiang University;The Chinese University of Hong Kong;SenseTime Group Limited;Zhejiang University;The Chinese University of Hong Kong,57;57;-1;57;57,177;40;-1;177;40,,9/27/18,0,0,0,0,0,0,115;1376;325;10;56860,10;68;25;8;496,4;19;5;1;107,1;98;40;0;8118,-1;-1
2950,ICLR,2019,Parametrizing Fully Convolutional Nets with a Single High-Order Tensor,Jean Kossaifi;Adrian Bulat;Georgios Tzimiropoulos;Maja Pantic,jean.kossaifi@gmail.com;bulat.adrian@gmail.com;yorgos.tzimiropoulos@nottingham.ac.uk;maja.pantic@gmail.com,4;3;4,4;4;5,Withdrawn,0,4,,yes,9/27/18,Imperial College London;Samsung;The University of Nottingham;Imperial College London,72;-1;228;72,8;-1;146;8,2,9/27/18,10,0,0,0,0,0,1254;1418;4977;25716,32;29;102;393,10;12;33;70,128;179;576;2042,-1;-1
2951,ICLR,2019,Associate Normalization,Song-Hao Jia;Ding-Jie Chen;Hwann-Tzong Chen,gasoonjia@icloud.com;djchen.tw@gmail.com;htchen@cs.nthu.edu.tw,3;5;2,5;4;5,Withdrawn,0,0,,yes,9/27/18,National Tsing Hua University;Academia Sinica;National Tsing Hua University,199;-1;199,323;-1;323,,9/27/18,-1,-1,-1,-1,-1,-1,-1;-1;-1,-1;-1;-1,-1;-1;-1,0;0;0,-1;-1
2952,ICLR,2019,Online abstraction with MDP homomorphisms for Deep Learning,Ondrej Biza;Robert Platt,bizaondr@fit.cvut.cz;rplatt@ccs.neu.edu,4;5,3;3,Withdrawn,0,0,,yes,9/27/18,Czech Technical University in Prague;Northeastern University,314;16,740;839,,9/27/18,2,0,0,0,0,0,2;4100,2;169,1;36,0;252,-1;-1
2953,ICLR,2019,Generalized Label Propagation Methods for Semi-Supervised Learning,Qimai Li;Xiao-Ming Wu;Zhichao Guan.,csqmli@comp.polyu.edu.hk;xiao-ming.wu@polyu.edu.hk;zcguan@zju.edu.cn,4;3;6,4;4;5,Withdrawn,2,3,,yes,9/27/18,The Hong Kong Polytechnic University;The Hong Kong Polytechnic University;Zhejiang University,169;169;57,182;182;177,10,9/27/18,1,0,0,0,0,0,336;1016;21,11;47;10,4;14;2,52;142;1,-1;-1
2954,ICLR,2019,Rethinking Knowledge Graph Propagation for Zero-Shot Learning,Michael Kampffmeyer;Yinbo Chen;Xiaodan Liang;Hao Wang;Yujia Zhang;Eric P. Xing,michael.c.kampffmeyer@uit.no;cyvius96@gmail.com;xdliang328@gmail.com;hwang87@mit.edu;zhangyujia2014@ia.ac.cn;epxing@cs.cmu.edu,7;5;5,4;3;4,Withdrawn,0,3,,yes,9/27/18,"UiT The Arctic University of Norway;Tsinghua University;SUN YAT-SEN UNIVERSITY;Massachusetts Institute of Technology;Institute of automation, Chinese academy of science, Chinese Academy of Sciences;Carnegie Mellon University",-1;8;478;2;62;1,-1;30;352;5;1103;24,10;6;8,5/29/18,53,0,0,0,0,0,444;77;6260;-1;319;26508,39;5;139;-1;58;606,10;3;41;-1;10;79,38;14;732;0;26;2722,-1;-1
2955,ICLR,2019,A Unified View of Deep Metric Learning via Gradient Analysis,Xun Wang;Xintong Han;Weilin Huang;Dengke Dong;Matthew R. Scott,xunwang@malong.com;xinhan@malong.com;whuang@malong.com,3;6;5,4;4;4,Withdrawn,2,3,,yes,9/27/18,Malong Technologies;Malong Technologies;Malong Technologies,-1;-1;-1,-1;-1;-1,,9/27/18,0,0,0,0,0,0,779;927;2439;206;532,117;51;77;9;55,13;13;22;3;11,48;142;209;39;52,-1;-1
2956,ICLR,2019,Learning Spatio-Temporal Representations Using Spike-Based Backpropagation,Deboleena Roy;Priyadarshini Panda;Kaushik Roy,roy77@purdue.edu;pandap@purdue.edu;kaushik@purdue.edu,3;4;3,5;5;4,Withdrawn,0,0,,yes,9/27/18,Purdue University;Purdue University;Purdue University,26;26;26,60;60;60,5,9/27/18,0,0,0,0,0,0,96;674;1125,15;66;135,6;15;20,6;45;55,-1;-1
2957,ICLR,2019,withdrawn,withdrawn,aaron.chadha.14@ucl.ac.uk;i.andreopoulos@ucl.ac.uk,4;4;3,5;4;5,Withdrawn,0,0,,yes,9/27/18,University College London;University College London,50;50,16;16,,9/27/18,-1,-1,-1,-1,-1,-1,-1,-1,-1,0,-1
2958,ICLR,2019,Cosine similarity-based Adversarial process,Hee-Soo Heo;Hye-Jin Shim;Jee-Weon Jung;IL-Ho Yang;Sung-Hyun Yoon;Ha-Jin Yu,zhasgone@naver.com;shimhyejin930615@gmail.com;aberforth19@naver.com;heisco@hanmail.net;ysh901108@naver.com;hjyu@uos.ac.kr,4;3;5,3;5;4,Withdrawn,0,0,,yes,9/27/18,"School of Computer Science, University of Seoul;;Naver;;Naver;School of Computer Science, University of Seoul",478;-1;-1;-1;-1;478,798;-1;-1;-1;-1;798,4,9/27/18,1,0,0,0,0,0,138;127;131;185;-1;189,26;31;24;92;-1;63,8;8;8;6;-1;9,5;5;5;5;0;8,-1;-1
2959,ICLR,2019,Low-Cost Parameterizations of Deep Convolutional Neural Networks,Eran Treister;Lars Ruthotto;Michal Sharoni;Sapir Zafrani;Eldad Haber,erant@bgu.ac.il;lruthotto@emory.edu;sharmic@post.bgu.ac.il;sapirza@post.bgu.ac.il;ehaber@eos.ubc.ca,4;4;5,3;4;5,Withdrawn,0,0,,yes,9/27/18,Ben Gurion University of the Negev;Emory University;Ben Gurion University of the Negev;Ben Gurion University of the Negev;University of British Columbia,-1;65;-1;-1;36,-1;50;-1;-1;34,,5/20/18,6,0,0,0,0,0,207;852;99;6;3725,31;48;7;1;164,10;13;5;1;35,13;72;9;2;258,-1;-1
2960,ICLR,2019,Engaging Image Captioning Via Personality,Kurt Shuster;Samuel Humeau;Hexiang Hu;Antoine Bordes;Jason Weston,kshuster@fb.com;samuelhumeau@fb.com;hexianghu@fb.com;abordes@fb.com;jaseweston@gmail.com,5;5;5,5;5;5,Withdrawn,0,1,,yes,9/27/18,Facebook;Facebook;Facebook;Facebook;,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,,9/27/18,27,0,0,0,0,0,317;355;539;16888;49154,16;15;28;76;242,8;10;11;32;79,50;31;67;2470;5933,-1;-1
2961,ICLR,2019,Spectral Convolutional Networks on Hierarchical Multigraphs,Boris Knyazev;Xiao Lin;Mohamed R. Amer;Graham W. Taylor,bknyazev@uoguelph.ca;xiao.lin@sri.com;mohamed.amer@sri.com;gwtaylor@uoguelph.ca,4;3;4,4;5;4,Withdrawn,0,0,,yes,9/27/18,University of Guelph;SRI International;SRI International;University of Guelph,261;-1;-1;261,1103;-1;-1;1103,10;8,9/27/18,0,0,0,0,0,0,96;735;983;6310,16;105;67;144,6;13;15;31,9;48;67;526,-1;-1
2962,ICLR,2019,,,youngjoon.yoo@navercorp.com,4;4;6,4;5;5,Withdrawn,0,0,,yes,9/27/18,NAVER,-1,-1,,9/27/18,-1,-1,-1,-1,-1,-1,-1,-1,-1,0,-1
2963,ICLR,2019,,Dai Quoc Nguyen;Tu Dinh Nguyen;Dinh Phung,dai.nguyen@monash.edu;tu.dinh.nguyen@monash.edu;dinh.phung@monash.edu,4;5;5,4;3;4,Withdrawn,0,3,,yes,9/27/18,Monash University;Monash University;Monash University,123;123;123,80;80;80,,9/27/18,-1,-1,-1,-1,-1,-1,-1;-1;-1,-1;-1;-1,-1;-1;-1,0;0;0,-1;-1
2964,ICLR,2019,MCTSBug: Generating Adversarial Text Sequences via Monte Carlo Tree Search and Homoglyph Attack,Ji Gao;Jack Lanchantin;Yanjun Qi,jg6yd@virginia.edu;jjl5sw@virginia.edu;yanjun@virginia.edu,3;4,4;3,Withdrawn,0,1,,yes,9/27/18,University of Virginia;University of Virginia;University of Virginia,65;65;65,113;113;113,4,9/27/18,1,0,0,0,0,0,914;930;3392,179;25;113,13;8;27,70;35;283,-1;-1
2965,ICLR,2019,Bamboo: Ball-Shape Data Augmentation Against Adversarial Attacks from All Directions,Huanrui Yang;Jingchi Zhang;Hsin-Pai Cheng;Wenhan Wang;Yiran Chen;Hai Li,huanrui.yang@duke.edu;jingchi.zhang@duke.edu;hc218@duke.edu;wenhanw@microsoft.com;yiran.chen@duke.edu;hai.li@duke.edu,4;3,3;5,Withdrawn,3,0,,yes,9/27/18,Duke University;Duke University;Duke University;Microsoft;Duke University;Duke University,44;44;44;-1;44;44,17;17;17;-1;17;17,4,9/27/18,2,0,0,0,0,0,77;21;80;18;677;1452,16;9;24;6;77;91,6;2;6;2;12;18,10;1;6;5;51;117,-1;-1
2966,ICLR,2019,Learning Grounded Sentence Representations by Jointly Using Video and Text Information,Patrick Bordes;Eloi Zablocki;Laure Soulier;Benjamin Piwowarski;Patrick Gallinari,patrick.bordes@lip6.fr;eloi.zablocki@gmail.com;laure.soulier@lip6.fr;benjamin.piwowarski@lip6.fr;patrick.gallinari@lip6.fr,4;3;6,4;5;4,Withdrawn,0,1,,yes,9/27/18,LIP6;LIP6;LIP6;LIP6;LIP6,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,,9/27/18,0,0,0,0,0,0,8;15;277;2034;5287,6;7;63;106;451,2;2;9;21;35,1;1;24;192;383,-1;-1
2967,ICLR,2019,Isolating effects of age with fair representation learning when assessing dementia,Zining Zhu;Jekaterina Novikova;Frank Rudzicz,zining.zhu@mail.utoronto.ca;jekaterina@winterlightlabs.com;frank@spoclab.com,4;4;5,3;3;4,Withdrawn,0,4,,yes,9/27/18,Toronto University;Winterlight Labs;University of Toronto,18;-1;18,22;-1;22,7,7/19/18,2,0,0,0,0,0,41;512;1771,6;38;172,2;10;24,0;90;134,-1;-1
2968,ICLR,2019,Diagnosing Language Inconsistency in Cross-Lingual Word Embeddings,Yoshinari Fujinuma;Jordan Boyd-Graber;Michael J. Paul,yoshinari.fujinuma@colorado.edu;jbg@umiacs.umd.edu;michael.j.paul@colorado.edu,6;4;4,4;4;5,Withdrawn,0,0,,yes,9/27/18,"University of Colorado, Boulder;University of Maryland, College Park;University of Colorado, Boulder",44;12;44,100;69;100,10,9/27/18,0,0,0,0,0,0,13;14;7476,7;17;133,3;2;32,1;0;565,-1;-1
2969,ICLR,2019,Bilingual-GAN: Neural Text Generation and Neural Machine Translation as Two Sides of the Same Coin,Ahmad Rashid;Alan Do-Omri;Mehdi Rezagholizadeh;Md. Akmal Haidar;Hamed Sadeghi,ahmadrash@gmail.com;alan.do-omri@mail.mcgill.ca;mehdi.rezagholizadeh@gmail.com;md.akmal.haidar@huawei.com;haamed.sadeghi@gmail.com,3;4;4,5;3;5,Withdrawn,0,0,,yes,9/27/18,Huawei Technologies Ltd.;McGill University;;Huawei Technologies Ltd.;Huawei Technologies Ltd.,-1;85;-1;-1;-1,-1;42;-1;-1;-1,5;4,9/27/18,0,0,0,0,0,0,613;0;155;93;-1,62;1;32;25;-1,13;0;6;6;-1,11;0;5;7;0,-1;-1
2970,ICLR,2019,"Learning Robust, Transferable Sentence Representations for Text Classification",Wasi Uddin Ahmad;Xueying Bai;Nanyun Peng;Kai-Wei Chang,wasiahmad@cs.ucla.edu;xubai@cs.stonybrook.edu;npeng@isi.edu;kwchang@cs.ucla.edu,4;3;4,4;2;4,Withdrawn,0,2,,yes,9/27/18,"University of California, Los Angeles;State University of New York, Stony Brook;USC/ISI;University of California, Los Angeles",20;41;-1;20,15;258;-1;15,6,9/27/18,0,0,0,0,0,0,83;57;1047;11019,19;9;58;84,6;3;14;26,7;3;156;1401,-1;-1
2971,ICLR,2019,,Masoud Faraki;Mahsa Baktashmotlagh;Tom Drummond;Mathieu Salzmann,masoud.faraki@monash.edu;m.baktashmotlagh@qut.edu.au;tom.drummond@monash.edu;mathieu.salzmann@epfl.ch,4;4;3,4;4;5,Withdrawn,2,0,,yes,9/27/18,Monash University;South China University of Technology;Monash University;Swiss Federal Institute of Technology Lausanne,123;478;123;478,80;576;80;38,,9/27/18,-1,-1,-1,-1,-1,-1,-1;-1;-1;-1,-1;-1;-1;-1,-1;-1;-1;-1,0;0;0;0,-1;-1
2972,ICLR,2019,Empirical observations on the instability of aligning word vector spaces with GANs,Mareike Hartmann;Yova Kementchedjhieva;Anders Søgaard,hartmann@di.ku.dk;yova@di.ku.dk;soegaard@di.ku.dk,4;6;5,4;3;4,Withdrawn,0,0,,yes,9/27/18,University of Copenhagen;University of Copenhagen;University of Copenhagen,99;99;99,109;109;109,3;4;5,9/27/18,0,0,0,0,0,0,52;102;3892,13;18;215,5;6;32,2;8;356,-1;-1
2973,ICLR,2019,Low-Rank Matrix Factorization of LSTM as Effective Model Compression,Genta Indra Winata;Andrea Madotto;Jamin Shin;Elham J. Barezi,giwinata@connect.ust.hk;amadotto@connect.ust.hk;jay.shin@connect.ust.hk;ejs@connect.ust.hk,5;5;4,4;2;4,Withdrawn,2,3,,yes,9/27/18,The Hong Kong University of Science and Technology;The Hong Kong University of Science and Technology;The Hong Kong University of Science and Technology;The Hong Kong University of Science and Technology,39;39;39;39,44;44;44;44,3,9/27/18,3,0,0,0,0,0,177;398;102;25,32;42;15;8,8;10;5;3,13;53;9;2,-1;-1
2974,ICLR,2019,Combining Global Sparse Gradients with Local Gradients,Alham Fikri Aji;Kenneth Heafield,a.fikri@ed.ac.uk;kheafiel@inf.ed.ac.uk,5;5;3,4;3;4,Withdrawn,0,0,,yes,9/27/18,University of Edinburgh;University of Edinburgh,33;33,27;27,3,9/27/18,0,0,0,0,0,0,395;2643,14;67,6;17,45;247,-1;-1
2975,ICLR,2019,KNOWLEDGE DISTILL VIA LEARNING NEURON MANIFOLD,Zeyi Tao;Qi Xia;Qun Li,ztao@email.wm.edu;qxia01@email.wm.edu;liqun@cs.wm.edu,5;1;3,3;5;4,Withdrawn,0,0,,yes,9/27/18,College of William and Mary;College of William and Mary;College of William and Mary,169;169;169,261;261;261,6,9/27/18,0,0,0,0,0,0,27;49;655,5;16;111,2;3;13,2;1;27,-1;-1
2976,ICLR,2019,Adversarial Decomposition of Text Representation,Alexey Romanov;Anna Rumshisky;Anna Rogers;David Donahue,jgc128@outlook.com;arum@cs.uml.edu;arogers@cs.uml.edu;david_donahue@student.uml.edu,3;6;4,4;3;3,Withdrawn,0,4,,yes,9/27/18,"University of Massachusetts, Lowell;University of Massachusetts, Lowell;University of Massachusetts, Lowell;University of Massachusetts, Lowell",-1;-1;-1;-1,-1;-1;-1;-1,4,8/27/18,9,0,0,0,0,0,327;1664;381;38,27;91;16;8,10;19;9;4,44;131;41;5,-1;-1
2977,ICLR,2019,How to learn (and how not to learn) multi-hop reasoning with memory networks,Jifan Chen;Greg Durrett,jf_chen@utexas.edu;gdurrett@cs.utexas.edu,3;5;5,5;5;4,Withdrawn,0,0,,yes,9/27/18,"University of Texas, Austin;University of Texas, Austin",22;22,49;49,,9/27/18,1,0,0,0,0,0,214;1306,15;49,6;17,21;162,-1;-1
2978,ICLR,2019,Learning to Attend On Essential Terms: An Enhanced Retriever-Reader Model for Open-domain Question Answering,Jianmo Ni;Chenguang Zhu;Weizhu Chen;Julian McAuley,jin018@ucsd.edu;chezhu@microsoft.com;wzchen@microsoft.com;jmcauley@cs.ucsd.edu,4;5;5,4;4;4,Withdrawn,2,0,,yes,9/27/18,"University of California, San Diego;Microsoft;Microsoft;University of California, San Diego",11;-1;-1;11,31;-1;-1;31,,8/28/18,20,0,0,0,0,0,230;937;2016;8035,12;47;62;133,8;11;21;32,24;91;248;1053,-1;-1
2979,ICLR,2019,The Missing Ingredient in Zero-Shot Neural Machine Translation,Naveen Arivazhagan;Ankur Bapna;Orhan Firat;Roee Aharoni;Melvin Johnson;Wolfgang Macherey,naveenariva@gmail.com;ankurbpn@google.com;orhanf@google.com;roee.aharoni@gmail.com;melvinp@google.com;wmach@google.com,5;4;3,5;3;3,Withdrawn,0,6,,yes,9/27/18,Google;Google;Google;Bar Ilan University;Google;Google,-1;-1;-1;95;-1;-1,-1;-1;-1;456;-1;-1,3;6;8,9/27/18,22,0,0,0,0,0,276;576;3673;356;3549;4201,30;22;61;11;16;39,9;12;19;8;8;22,45;64;373;41;328;395,-1;-1
2980,ICLR,2019,Iterative Binary Decisions,Stephan Alaniz;Zeynep Akata,s.alaniz@uva.nl;z.akata@uva.nl,4;4;4,4;4;3,Withdrawn,0,1,,yes,9/27/18,University of Amsterdam;University of Amsterdam,169;169,59;59,6,9/27/18,0,0,0,0,0,0,66;5923,10;68,5;25,6;843,-1;-1
2981,ICLR,2019,What Is in a Translation Unit?  Comparing Character and Subword Representations Beyond Translation,Nadir Durrani;Fahim Dalvi;Hassan Sajjad;Yonatan Belinkov;Preslav Nakov,ndurrani@qf.org.qa;faimaduddin@qf.org.qa;hsajjad@qf.org.qa;belinkov@mit.edu;pnakov@hbku.edu.qa,5;5;5,4;4;3,Withdrawn,0,0,,yes,9/27/18,QCRI;QCRI;QCRI;Massachusetts Institute of Technology;Peking University,199;199;199;2;24,1103;1103;1103;5;27,3;2,9/27/18,4,0,0,0,0,0,1446;498;1245;1643;7924,66;35;62;63;262,21;11;19;21;47,105;37;111;176;787,-1;-1
2982,ICLR,2019,Robust Text Classifier on Test-Time Budgets,Md Rizwan Parvez;Tolga Bolukbasi;Kai-Wei Chang;Venkatesh Saligrama,rizwan@cs.ucla.edu;tolgab@bu.edu;kwchang@cs.ucla.edu;srv@bu.edu,4;4;5,4;4;3,Withdrawn,0,0,,yes,9/27/18,"University of California, Los Angeles;Boston University;University of California, Los Angeles;Boston University",20;65;20;65,15;70;15;70,,8/24/18,0,0,0,0,0,0,1;904;11019;5501,4;19;84;293,1;7;26;36,0;137;1401;526,-1;-1
2983,ICLR,2019,Hiding Objects from Detectors: Exploring Transferrable Adversarial Patterns,Shangbang Long;Jie Fu;Chris Pal,longlongsb@pku.edu.cn;jie.fu@polymtl.ca;christopher.pal@polymtl.ca,6;4;3,4;4;3,Withdrawn,0,3,,yes,9/27/18,Peking University;Polytechnique Montreal;Polytechnique Montreal,24;386;386,27;108;108,4;1,9/27/18,0,0,0,0,0,0,185;796;862,12;117;58,4;14;12,23;40;72,-1;-1
2984,ICLR,2019,Unsupervised Latent Tree Induction with Deep Inside-Outside Recursive Auto-Encoders ,Andrew Drozdov;Patrick Verga;Mohit Yadev;Mohit Iyyer;Andrew McCallum,adrozdov@cs.umass.edu;pat@cs.umass.edu;ymohit@cs.umass.edu;miyyer@cs.umass.edu;mccallum@cs.umass.edu,5;6;2,4;3;4,Withdrawn,0,5,,yes,9/27/18,"University of Massachusetts, Amherst;University of Massachusetts, Amherst;University of Massachusetts, Amherst;University of Massachusetts, Amherst;University of Massachusetts, Amherst",30;30;30;30;30,191;191;191;191;191,2,9/27/18,23,0,0,0,0,0,116;821;60;5980;52577,7;22;20;44;434,3;13;3;15;98,12;109;8;1082;5357,-1;-1
2985,ICLR,2019,Tangent-Normal Adversarial Regularization for Semi-supervised Learning,Bing Yu;Jingfeng Wu;Jinwen Ma;Zhanxing Zhu,byu@pku.edu.cn;pkuwjf@pku.edu.cn;jwma@math.pku.edu.cn;zhanxing.zhu@pku.edu.cn,5;4;7,3;5;4,Withdrawn,0,1,,yes,9/27/18,Peking University;Peking University;Peking University;Peking University,24;24;24;24,27;27;27;27,4,8/18/18,10,0,0,0,0,0,1803;597;2380;937,102;37;193;80,21;10;22;15,110;40;130;115,-1;-1
2986,ICLR,2019,Answer-based Adversarial Training for Generating Clarification Questions,Sudha Rao;Hal Daumé III,raosudha@cs.umd.edu;hal@umiacs.umd.edu,4;4;6,4;5;4,Withdrawn,0,0,,yes,9/27/18,"University of Maryland, College Park;University of Maryland, College Park",12;12,69;69,5;4,9/27/18,16,0,0,0,0,0,244;11467,16;198,8;48,27;1087,-1;-1
2987,ICLR,2019,IncSQL: Training Incremental Text-to-SQL Parsers with Non-Deterministic Oracles,Tianze Shi;Kedar Tatwawadi;Kaushik Chakrabarti;Yi Mao;Oleksandr Polozov;Weizhu Chen,tianze@cs.cornell.edu;kedart@stanford.edu;kaushik@microsoft.com;maoyi@microsoft.com;polozov@microsoft.com;wzchen@microsoft.com,4;6;3,4;3;5,Withdrawn,0,0,,yes,9/27/18,Cornell University;Stanford University;Microsoft;Microsoft;Microsoft;Microsoft,7;4;-1;-1;-1;-1,19;3;-1;-1;-1;-1,3,9/13/18,27,0,0,0,0,0,181;132;5488;1344;577;2016,14;28;97;104;22;62,8;7;31;19;9;21,25;13;618;88;47;248,-1;-1
2988,ICLR,2019,Mitigating Bias in Natural Language Inference Using Adversarial Learning,Yonatan Belinkov;Adam Poliak;Stuart M. Shieber;Benjamin Van Durme,belinkov@seas.harvard.edu;azpoliak@cs.jhu.edu;shieber@seas.harvard.edu;vandurme@cs.jhu.edu,4;4;8,5;4;4,Withdrawn,0,5,,yes,9/27/18,Harvard University;Johns Hopkins University;Harvard University;Johns Hopkins University,39;72;39;72,6;13;6;13,3;4,9/27/18,1,0,0,0,0,0,1643;506;5236;5276,63;21;233;196,21;10;37;39,176;60;457;557,-1;-1
2989,ICLR,2019,Multi-Modal Generative Adversarial Networks for Diverse Datasets,Matan Ben-Yosef;Daphna Weinshall,matan.benyosef@mail.huji.ac.il;daphna@cs.huji.ac.il,4;6,4;4,Withdrawn,0,0,,yes,9/27/18,Hebrew University of Jerusalem;Hebrew University of Jerusalem,65;65,205;205,5;4,9/27/18,0,0,0,0,0,0,19;5527,3;216,1;37,2;529,-1;-1
2990,ICLR,2019,Few-Shot Learning by Exploiting Object Relation,Liangqu Long;Wei Wang;Jun Wen;Meihui  Zhang;Qian  Lin,liangqu.long@gmail.com;wangwei@comp.nus.edu.sg;jungel2star@gmail.com;meihui_zhang@bit.edu.cn;linqian@comp.nus.edu.sg,6;4;4,4;4;3,Withdrawn,0,0,,yes,9/27/18,;National University of Singapore;;BIT;National University of Singapore,-1;16;-1;-1;16,-1;22;-1;-1;22,6,9/27/18,1,0,0,0,0,0,15;653;64;1707;1861,3;101;22;68;312,2;10;3;18;23,2;27;2;124;91,-1;-1
2991,ICLR,2019,CrystalGAN: Learning to Discover Crystallographic Structures with Generative Adversarial Networks,Asma Nouira;Nataliya Sokolovska;Jean-Claude Crivello,asma.nouira.91@gmail.com;nataliya.sokolovska@upmc.fr;jccrivello@icmpe.cnrs.fr,3;7;4,4;2;2,Withdrawn,0,0,,yes,9/27/18,";Computer Science Lab  - Pierre and Marie Curie University, Paris, France;CNRS",-1;478;-1,-1;123;-1,5;4,9/27/18,10,0,0,0,0,0,52;1056;802,6;50;71,4;11;13,2;54;10,-1;-1
2992,ICLR,2019,Exploration using Distributional RL and UCB,Borislav Mavrin;Hengshuai Yao;Linglong Kong;ShangtongZhang,mavrin@ualberta.ca;hengshuai.yao@huawei.com;lkong@ualberta.ca;zhangshangtong.cpp@gmail.com,4;4;4,3;5;4,Withdrawn,0,3,,yes,9/27/18,University of Alberta;Huawei Technologies Ltd.;University of Alberta;University of Oxford,99;-1;99;50,119;-1;119;1,1,9/27/18,0,0,0,0,0,0,27;116;486;0,12;44;58;1,3;6;11;0,1;9;34;0,-1;-1
2993,ICLR,2019,The Body is not a Given: Joint Agent Policy Learning and Morphology Evolution,Dylan Banarse;Yoram Bachrach;Siqi Liu;Chrisantha Fernando;Nicolas Heess;Pushmeet Kohli;Guy Lever;Thore Graepel,dylski@google.com;yorambac@google.com;guylever@google.com;heess@google.com;pushmeet@google.com;liusiqi@google.com;chrisantha@google.com;thore@google.com,4;4;3;4,4;4;4;3,Withdrawn,0,1,,yes,9/27/18,Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1,,9/27/18,3,0,0,0,0,0,336;3058;1509;1987;12013;24235;2126;20220,4;137;98;66;104;315;30;161,3;30;17;18;37;71;14;47,25;211;91;162;1689;2804;330;1423,-1;-1
2994,ICLR,2019,From Adversarial Training to Generative Adversarial Networks,Xuanqing Liu;Cho-Jui Hsieh,xqliu@cs.ucla.edu;chohsieh@cs.ucla.edu,3;6;4,3;3;4,Withdrawn,0,1,,yes,9/27/18,"University of California, Los Angeles;University of California, Los Angeles",20;20,15;15,5;4,7/27/18,4,0,0,0,0,0,262;13775,20;168,6;42,38;1753,-1;-1
2995,ICLR,2019,An Efficient Network for Predicting Time-Varying Distributions,Connie Kou;Hwee Kuan Lee;Teck Khim Ng;Jorge Sanz,koukl@comp.nus.edu.sg;leehk@bii.a-star.edu.sg;ngtk@comp.nus.edu.sg;jorges@nus.edu.sg,5;4;5,4;3;4,Withdrawn,0,0,,yes,9/27/18,National University of Singapore;A*STAR;National University of Singapore;National University of Singapore,16;-1;16;16,22;-1;22;22,,9/27/18,1,0,0,0,0,0,10;890;95;32,6;109;21;32,2;17;5;3,1;44;5;3,-1;-1
2996,ICLR,2019,Quantile Regression Reinforcement Learning with State Aligned Vector Rewards,Oliver Richter;Roger Wattenhofer,richtero@ethz.ch;wattenhofer@ethz.ch,4;3;4,4;3;4,Withdrawn,0,8,,yes,9/27/18,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10,10;10,,9/27/18,0,0,0,0,0,0,55;20716,15;574,4;73,0;1864,-1;-1
2997,ICLR,2019,Fatty and Skinny: A Joint Training Method of Watermark Encoder and Decoder,Sanghyun Hong;Mahmoud Mohammadi;Noseong Park,shhong@cs.umd.edu;mmoham12@uncc.edu;npark9@gmu.edu,4;4;4,4;4;4,Withdrawn,0,0,,yes,9/27/18,"University of Maryland, College Park;University of North Carolina, Charlotte;George Mason University",12;314;99,69;1103;336,5;4;1,9/27/18,0,0,0,0,0,0,416;46;134,38;8;8,10;3;3,9;7;6,-1;-1
2998,ICLR,2019,Transfer Learning via Unsupervised Task Discovery for Visual Question Answering,Hyeonwoo Noh;Taehoon Kim;Jonghwan Mun;Bohyung Han,shgusdngogo@postech.ac.kr;carpedm20@gmail.com;choco1916@postech.ac.kr;bhhan@snu.ac.kr,4;5;8,5;5;5,Withdrawn,0,0,,yes,9/27/18,POSTECH;OpenAI;POSTECH;Seoul National University,123;-1;123;41,137;-1;137;74,,9/27/18,4,0,0,0,0,0,2075;2534;159;7527,12;173;11;121,9;15;5;40,170;120;17;896,-1;-1
2999,ICLR,2019,Confidence Calibration in Deep Neural Networks through Stochastic Inferences,Seonguk Seo;Paul Hongsuck Seo;Bohyung Han,seonguk@snu.ac.kr;hsseo@postech.ac.kr;bhhan@snu.ac.kr,5;3;5,4;2;4,Withdrawn,0,0,,yes,9/27/18,Seoul National University;POSTECH;Seoul National University,41;123;41,74;137;74,11,9/27/18,21,0,0,0,0,0,60;486;7527,8;13;121,3;8;40,5;48;896,-1;-1
3000,ICLR,2019,Noise-Tempered Generative Adversarial Networks,Simon Jenni;Paolo Favaro,jenni@inf.unibe.ch;paolo.favaro@inf.unibe.ch,4;5;5,5;4;4,Withdrawn,0,4,,yes,9/27/18,University of Bern;University of Bern,386;386,105;105,5;4,9/27/18,2,0,0,0,0,0,475;4549,25;107,9;34,40;432,-1;-1
3001,ICLR,2019,SpaMHMM: Sparse Mixture of Hidden Markov Models for Graph Connected Entities,Diogo Pernes;Jaime S. Cardoso,dpc@inesctec.pt;jaime.cardoso@inesctec.pt,3;3;3,4;4;4,Withdrawn,0,3,,yes,9/27/18,University of Porto;University of Porto,18;18,22;22,5;10,9/27/18,1,0,0,0,0,0,7;3273,7;249,2;27,0;213,-1;-1
3002,ICLR,2019,Self-Binarizing Networks,Fayez Lahoud;Radhakrishna Achanta;Pablo Márquez-Neila;Sabine Süsstrunk,fayez.lahoud@epfl.ch;radhakrishna.achanta@epfl.ch;pablo.marquez@artorg.unibe.ch;sabine.susstrunk@epfl.ch,5;5;5,4;4;4,Withdrawn,4,1,,yes,9/27/18,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;University of Bern;Swiss Federal Institute of Technology Lausanne,478;478;386;478,38;38;105;38,,9/27/18,5,0,0,0,0,0,54;10581;633;14924,12;28;32;244,5;14;13;38,8;1740;64;2116,-1;-1
3003,ICLR,2019,UNSUPERVISED MONOCULAR DEPTH ESTIMATION WITH CLEAR BOUNDARIES,Yihan Hu;Heng Luo;Yifeng Geng,y4hu@eng.ucsd.edu;heng.luo@horizon.ai;yifeng.geng@horizon.ai,4;4;3,3;5;4,Withdrawn,0,0,,yes,9/27/18,"University of California, San Diego;Horizon Robotics;Horizon Robotics",11;-1;-1,31;-1;-1,,9/27/18,0,0,0,0,0,0,10;633;328,8;18;10,1;3;3,1;33;40,-1;-1
3004,ICLR,2019,Object-Contrastive Networks: Unsupervised Object Representations,Soeren Pirk;Mohi Khansari;Yunfei Bai;Corey Lynch;Pierre Sermanet,pirk@google.com;khansari@google.com;yunfeibai@google.com;coreylynch@google.com;sermanet@google.com,3;3;5,5;5;4,Withdrawn,0,0,,yes,9/27/18,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,,9/27/18,0,0,0,0,0,0,744;59;904;346;25267,35;12;46;14;42,12;5;12;8;19,34;1;51;27;3010,-1;-1
3005,ICLR,2019,TFGAN: Improving Conditioning for Text-to-Video Synthesis,Yogesh Balaji;Martin Renqiang Min;Bing Bai;Rama Chellappa;Hans Peter Graf,yogesh@cs.umd.edu;renqiang@nec-labs.com;bbai@nec-labs.com;rama@umiacs.umd.edu;hpg@nec-labs.com,6;3;5,3;5;4,Withdrawn,0,0,,yes,9/27/18,"University of Maryland, College Park;NEC-Labs;NEC-Labs;University of Maryland, College Park;NEC-Labs",12;-1;-1;12;-1,69;-1;-1;69;-1,5;4,9/27/18,2,0,0,0,0,0,517;947;216;44999;3797,19;58;33;1067;113,7;15;6;101;28,64;105;15;3200;424,-1;-1
3006,ICLR,2019,Learning Graph Decomposition,Jie Song;Bjoern Andres;Michael Black;Otmar Hilliges;Siyu Tang,jsong@inf.ethz.ch;bjoern.andres@de.bosch.com;black@tuebingen.mpg.de;otmar.hilliges@inf.ethz.ch;stang@tuebingen.mpg.de,7;4;5,4;4;4,Withdrawn,0,0,,yes,9/27/18,"Swiss Federal Institute of Technology;Bosch;Max Planck Institute for Intelligent Systems, Max-Planck Institute;Swiss Federal Institute of Technology;Max Planck Institute for Intelligent Systems, Max-Planck Institute",10;-1;-1;10;-1,10;-1;-1;10;-1,2;10,9/27/18,0,0,0,0,0,0,2239;3371;237;8404;2022,98;80;14;137;109,20;27;5;34;17,106;300;25;857;167,-1;-1
3007,ICLR,2019,Logically-Constrained Neural Fitted Q-iteration,Mohammadhosein Hasanbeig;Alessandro Abate;Daniel Kroening,hosein.hasanbeig@cs.ox.ac.uk;aabate@cs.ox.ac.uk;kroening@cs.ox.ac.uk,5;4;5,2;5;4,Withdrawn,0,5,,yes,9/27/18,University of Oxford;University of Oxford;University of Oxford,50;50;50,1;1;1,,9/20/18,11,0,0,0,0,0,101;3022;8256,17;197;368,6;27;44,7;159;758,-1;-1
3008,ICLR,2019,Distributed Deep Policy Gradient for Competitive Adversarial Environment,Denis Osipychev;Girish Chowdhary,deniso2@illinois.edu;girishc@illinois.edu,4;4;3,4;3;5,Withdrawn,0,0,,yes,9/27/18,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",3;3,37;37,,9/27/18,0,0,0,0,0,0,18;2950,9;208,3;24,0;167,-1;-1
3009,ICLR,2019,Found by NEMO: Unsupervised Object Detection from Negative Examples and Motion,Rico Jonschkowski,rjon@google.com,5;3;4,4;4;4,Withdrawn,0,1,,yes,9/27/18,Google,-1,-1,2,9/27/18,1,0,0,0,0,0,502,30,11,50,-1
3010,ICLR,2019,Visualizing and Discovering Behavioural Weaknesses in Deep Reinforcement Learning,Christian Rupprecht;Cyril Ibrahim;Chris Pal,christian.rupprecht@in.tum.de;cyril.ibrahim@elementai.com;christopher.pal@polymtl.ca,5;5;4,4;4;5,Withdrawn,0,2,,yes,9/27/18,Technical University Munich;Element AI;Polytechnique Montreal,54;-1;386,41;-1;108,5,9/27/18,2,0,0,0,0,0,1312;13;862,45;3;58,13;2;12,214;3;72,-1;-1
3011,ICLR,2019,Estimating Heterogeneous Treatment Effects Using Neural Networks With The Y-Learner,Bradly C. Stadie;Sören R. Künzel;Nikita Vemuri;Jasjeet S. Sekhon,bstadie@berkeley.edu;srk@berkeley.edu;nikitavemuri@berkeley.edu;sekhon@berkeley.edu,5;5;4,3;4;4,Withdrawn,0,0,,yes,9/27/18,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5,18;18;18;18,,9/27/18,1,0,0,0,0,0,794;172;72;1079,18;6;6;32,7;4;3;11,51;27;8;67,-1;-1
3012,ICLR,2019,Generative Model For Material Irradiation Experiments Based On Prior Knowledge And Attention Mechanism,MinCong Luo;Li Liu,luomincong@foxmail.com;1920148271@qq.com,3;3,4;4,Withdrawn,0,0,,yes,9/27/18,Chinese Academy of Sciences;,62;-1,1103;-1,5;4,9/27/18,0,0,0,0,0,0,4;491,6;80,1;14,0;18,-1;-1
3013,ICLR,2019, Generating Text through Adversarial Training using Skip-Thought Vectors,Afroz Ahamad,afrozsahamad@gmail.com,3;2;2,5;5;5,Withdrawn,0,0,,yes,9/27/18,"BITS Pilani, BITS Pilani",-1,-1,3;4;5,8/27/18,2,0,0,0,0,0,2,2,1,0,-1
3014,ICLR,2019,Evolving intrinsic motivations for altruistic behavior,Jane X. Wang;Edward Hughes;Chrisantha Fernando;Wojciech M. Czarnecki;Edgar A. Duenez-Guzman;Joel Z. Leibo,wangjane@google.com;edwardhughes@google.com;chrisantha@google.com;lejlot@google.com;duenez@google.com;jzl@google.com,5;6;3,3;2;4,Withdrawn,0,0,,yes,9/27/18,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,,9/27/18,16,0,0,0,0,0,722;152;1987;2419;467;3693,16;20;66;44;27;75,9;7;18;21;14;30,74;13;162;254;25;365,-1;-1
3015,ICLR,2020,Pitfalls of In-Domain Uncertainty Estimation and Ensembling in Deep Learning,Arsenii Ashukha;Alexander Lyzhov;Dmitry Molchanov;Dmitry Vetrov,ars.ashuha@gmail.com;alex.grig.lyzhov@gmail.com;dmolch111@gmail.com;vetrovd@yandex.ru,6;8;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,1,yes,9/25/19,Samsung;Skolkovo Institute of Science and Technology;Samsung;Higher School of Economics,-1;-1;-1;481,-1;-1;-1;251,,9/25/19,17,10,5,1,0,3,488;12;481;2097,14;2;17;124,8;1;7;16,84;2;82;285,m;m
3016,ICLR,2020,A Theory of Usable Information under Computational Constraints,Yilun Xu;Shengjia Zhao;Jiaming Song;Russell Stewart;Stefano Ermon,xuyilun@pku.edu.cn;sjzhao@stanford.edu;tsong@cs.stanford.edu;russell.sb.nebel@gmail.com;ermon@cs.stanford.edu,8;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,3,0,yes,9/25/19,Peking University;Stanford University;Stanford University;;Stanford University,22;4;4;-1;4,24;4;4;-1;4,,9/25/19,3,1,0,0,0,0,22;592;811;2333;4975,5;29;44;96;203,3;13;14;26;31,3;98;127;134;664,m;m
3017,ICLR,2020,An Inductive Bias for Distances: Neural Nets that Respect the Triangle Inequality,Silviu Pitis;Harris Chan;Kiarash Jamali;Jimmy Ba,spitis@cs.toronto.edu;hchan@cs.toronto.edu;kiarash.jamali@mail.utoronto.ca;jba@cs.toronto.edu,8;3;8;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0,yes,9/25/19,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Toronto University;Department of Computer Science, University of Toronto",18;18;18;18,18;18;18;18,1;10,9/25/19,2,1,1,0,0,0,15;23;3;52924,9;13;3;56,2;3;1;22,0;4;0;8625,m;m
3018,ICLR,2020,Stochastic AUC Maximization with Deep Neural Networks,Mingrui Liu;Zhuoning Yuan;Yiming Ying;Tianbao Yang,mingrui-liu@uiowa.edu;zhuoning-yuan@uiowa.edu;yying@albany.edu;tianbao-yang@uiowa.edu,6;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,6,0,yes,9/25/19,"University of Iowa;University of Iowa;State University of New York, Albany;University of Iowa",154;154;266;154,227;227;350;227,9,8/28/19,4,4,2,1,0,0,238;112;2144;3236,29;12;64;187,9;4;24;29,10;9;235;351,m;m
3019,ICLR,2020,Convolutional Conditional Neural Processes,Jonathan Gordon;Wessel P. Bruinsma;Andrew Y. K. Foong;James Requeima;Yann Dubois;Richard E. Turner,jg801@cam.ac.uk;wpb23@cam.ac.uk;ykf21@cam.ac.uk;jrr41@cam.ac.uk;yanndubois96@gmail.com;ret26@cam.ac.uk,8;8;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,7,0,yes,9/25/19,University of Cambridge;University of Cambridge;University of Cambridge;University of Cambridge;Facebook;University of Cambridge,71;71;71;71;-1;71,3;3;3;3;-1;3,6;8,9/25/19,6,2,2,1,0,1,293;12;23;85;6;3055,22;12;5;9;3;175,8;3;3;4;2;30,22;0;6;10;0;315,m;m
3020,ICLR,2020,On the interaction between supervision and self-play in emergent communication,Ryan Lowe*;Abhinav Gupta*;Jakob Foerster;Douwe Kiela;Joelle Pineau,rlowe1@cs.mcgill.ca;abhinav.gupta@umontreal.ca;jakobfoerster@gmail.com;dkiela@fb.com;jpineau@cs.mcgill.ca,6;8;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0,yes,9/25/19,McGill University;University of Montreal;Facebook;Facebook;McGill University,86;128;-1;-1;86,42;85;-1;-1;42,3,9/25/19,4,2,1,0,0,0,3343;36;2107;3454;11328,53;10;58;80;267,17;3;19;29;46,518;5;340;588;1235,m;f
3021,ICLR,2020,Towards Better Understanding of Adaptive Gradient Algorithms in Generative Adversarial Nets,Mingrui Liu;Youssef Mroueh;Jerret Ross;Wei Zhang;Xiaodong Cui;Payel Das;Tianbao Yang,mingrui-liu@uiowa.edu;mroueh@us.ibm.com;rossja@us.ibm.com;weiz@us.ibm.com;cuix@us.ibm.com;daspa@us.ibm.com;tianbao-yang@uiowa.edu,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Accept (Poster),0,7,0,yes,9/25/19,University of Iowa;International Business Machines;International Business Machines;International Business Machines;International Business Machines;International Business Machines;University of Iowa,154;-1;-1;-1;-1;-1;154,227;-1;-1;-1;-1;-1;227,5;9,9/25/19,7,3,1,1,0,0,238;916;392;108;241;848;3236,29;53;12;46;30;86;187,9;11;4;6;6;17;29,10;151;86;3;20;50;351,m;m
3022,ICLR,2020,TabFact: A Large-scale Dataset for Table-based Fact Verification,Wenhu Chen;Hongmin Wang;Jianshu Chen;Yunkai Zhang;Hong Wang;Shiyang Li;Xiyou Zhou;William Yang Wang,wenhuchen@ucsb.edu;hongmin@ucsb.edu;chenjianshu@gmail.com;yunkai_zhang@ucsb.edu;hongwang600@ucsb.edu;shiyangli@ucsb.edu;xiyou@ucsb.edu;william@cs.ucsb.edu,6;6;8,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:N/A:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Poster),0,6,1,yes,9/25/19,UC Santa Barbara;UC Santa Barbara;Tencent AI Lab;UC Santa Barbara;UC Santa Barbara;UC Santa Barbara;UC Santa Barbara;UC Santa Barbara,38;38;-1;38;38;38;38;38,57;57;-1;57;57;57;57;57,3;10,9/5/19,10,6,4,0,0,3,479;31;2903;14;31;33;28;2333,42;8;107;5;8;7;5;128,10;4;26;1;4;3;3;27,68;7;233;4;7;7;5;281,m;m
3023,ICLR,2020,Keep Doing What Worked: Behavior Modelling Priors for Offline Reinforcement Learning,Noah Siegel;Jost Tobias Springenberg;Felix Berkenkamp;Abbas Abdolmaleki;Michael Neunert;Thomas Lampe;Roland Hafner;Nicolas Heess;Martin Riedmiller,siegeln@google.com;springenberg@google.com;befelix@inf.ethz.ch;aabdolmaleki@google.com;neunertm@google.com;thomaslampe@google.com;rhafner@google.com;heess@google.com;riedmiller@google.com,6;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,Google;Google;Swiss Federal Institute of Technology;Google;Google;Google;Google;Google;Google,-1;-1;10;-1;-1;-1;-1;-1;-1,-1;-1;13;-1;-1;-1;-1;-1;-1,,9/25/19,10,5,7,2,0,2,10;7449;845;549;681;1165;733;-1;25961,3;54;25;45;33;85;24;-1;190,1;28;12;10;14;16;10;-1;40,2;798;49;59;39;51;33;0;3578,m;m
3024,ICLR,2020,Scale-Equivariant Steerable Networks,Ivan Sosnovik;Michał Szmaja;Arnold Smeulders,sosnovikivan@gmail.com;szmajamichal@gmail.com;a.w.m.smeulders@uva.nl,8;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,5,0,yes,9/25/19,University of Amsterdam;University of Amsterdam;University of Amsterdam,172;172;172,62;62;62,,9/25/19,9,2,4,0,0,1,56;7;22435,4;1;469,3;1;50,4;1;1926,m;m
3025,ICLR,2020,Learning Hierarchical Discrete Linguistic Units from Visually-Grounded Speech,David Harwath*;Wei-Ning Hsu*;James Glass,dharwath@csail.mit.edu;wnhsu@mit.edu;glass@mit.edu,8;8;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,6,0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2,5;5;5,,9/25/19,7,4,3,3,0,1,247;764;10922,16;34;342,6;15;55,15;68;967,m;m
3026,ICLR,2020,Inductive and Unsupervised Representation Learning on Graph Structured Objects,Lichen Wang;Bo Zong;Qianqian Ma;Wei Cheng;Jingchao Ni;Wenchao Yu;Yanchi Liu;Dongjin Song;Haifeng Chen;Yun Fu,wanglichenxj@gmail.com;bzong@nec-labs.com;maqq@bu.edu;weicheng@nec-labs.com;jni@nec-labs.com;wyu@nec-labs.com;yanchi@nec-labs.com;dsong@nec-labs.com;haifeng@nec-labs.com;yunfu@ece.neu.edu,6;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,7,0,yes,9/25/19,Northeastern University;NEC-Labs;Boston University;NEC-Labs;NEC-Labs;NEC-Labs;NEC-Labs;NEC-Labs;NEC-Labs;Northeastern University,16;-1;67;-1;-1;-1;-1;-1;-1;16,906;-1;61;-1;-1;-1;-1;-1;-1;906,10,9/25/19,1,0,1,0,0,0,4;611;36;48;229;418;1838;789;2635;1057,8;53;17;39;31;37;72;40;156;164,1;11;3;4;8;11;15;13;30;20,0;64;1;2;15;31;109;77;231;45,m;m
3027,ICLR,2020,Online and stochastic optimization beyond Lipschitz continuity: A Riemannian approach,Kimon Antonakopoulos;E. Veronica Belmega;Panayotis Mertikopoulos,kimon.antonakopoulos@inria.fr;veronica.belmega@ensea.fr;panayotis.mertikopoulos@imag.fr,8;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,3,0,yes,9/25/19,INRIA;ETIS;Imag Montpellier Université,-1;-1;-1,-1;-1;-1,9,9/25/19,1,1,0,0,0,0,3;771;1231,3;74;132,1;16;20,0;36;88,u;u
3028,ICLR,2020,HOPPITY: LEARNING GRAPH TRANSFORMATIONS TO DETECT AND FIX BUGS IN PROGRAMS,Elizabeth Dinella;Hanjun Dai;Ziyang Li;Mayur Naik;Le Song;Ke Wang,edinella@seas.upenn.edu;hadai@google.com;liby99@seas.upenn.edu;mhnaik@cis.upenn.edu;lsong@cc.gatech.edu;kewang@visa.com,6;8;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Spotlight),0,5,0,yes,9/25/19,University of Pennsylvania;Google;University of Pennsylvania;University of Pennsylvania;Georgia Institute of Technology;VISA,19;-1;19;19;13;-1,11;-1;11;11;38;-1,10,9/25/19,9,6,4,0,0,2,10;1879;74;5722;9519;581,2;57;19;87;329;104,1;17;6;28;54;7,2;283;3;636;1114;32,f;m
3029,ICLR,2020,Learning to Retrieve Reasoning Paths over Wikipedia Graph for Question Answering,Akari Asai;Kazuma Hashimoto;Hannaneh Hajishirzi;Richard Socher;Caiming Xiong,akari@cs.washington.edu;k.hashimoto@salesforce.com;hannaneh@washington.edu;richard@socher.org;cxiong@salesforce.com,6;8;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),1,8,3,yes,9/25/19,University of Washington;SalesForce.com;University of Washington;SalesForce.com;SalesForce.com,6;-1;6;-1;-1,26;-1;26;-1;-1,10,9/25/19,25,10,14,0,0,3,102;860;2907;53531;6301,10;49;96;180;156,4;11;25;49;31,18;96;599;8917;1045,f;m
3030,ICLR,2020,Drawing Early-Bird Tickets: Toward More Efficient Training of Deep Networks,Haoran You;Chaojian Li;Pengfei Xu;Yonggan Fu;Yue Wang;Xiaohan Chen;Richard G. Baraniuk;Zhangyang Wang;Yingyan Lin,hy34@rice.edu;cl114@rice.edu;px5@rice.edu;yf22@rice.edu;yw68@rice.edu;chernxh@tamu.edu;richb@rice.edu;atlaswang@tamu.edu;yingyan.lin@rice.edu,8;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,6,0,yes,9/25/19,Rice University;Rice University;Rice University;Rice University;Rice University;Texas A&M;Rice University;Texas A&M;Rice University,84;84;84;84;84;44;84;44;84,105;105;105;105;105;177;105;177;105,,9/25/19,6,6,2,0,0,1,7;19;1886;10;2947;85;30222;-1;218,4;8;228;10;167;30;661;-1;30,1;3;19;2;28;6;84;-1;8,1;1;108;1;383;1;2757;0;16,m;f
3031,ICLR,2020,Learning from Rules Generalizing Labeled Exemplars,Abhijeet Awasthi;Sabyasachi Ghosh;Rasna Goyal;Sunita Sarawagi,awasthi@cse.iitb.ac.in;sghosh@cse.iitb.ac.in;rasna.goyal66@gmail.com;sunita@iitb.ac.in,6;8;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),1,5,0,yes,9/25/19,Indian Institute of Technology Bombay;Indian Institute of Technology Bombay;;Indian Institute of Technology Bombay,118;118;-1;118,480;480;-1;480,,9/25/19,0,0,0,0,0,0,10;84;7;7914,7;45;2;151,2;6;1;40,2;5;2;695,m;f
3032,ICLR,2020,Mirror-Generative Neural Machine Translation,Zaixiang Zheng;Hao Zhou;Shujian Huang;Lei Li;Xin-Yu Dai;Jiajun Chen,zhengzx.142857@gmail.com;zhouhao.nlp@bytedance.com;huangsj@nju.edu.cn;lilei.02@bytedance.com;daixinyu@nju.edu.cn;chenjj@nju.edu.cn,8;8;8,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,7,1,yes,9/25/19,Zhejiang University;Bytedance;Zhejiang University;Bytedance;Zhejiang University;Zhejiang University,56;-1;56;-1;56;56,107;-1;107;-1;107;107,3;5,9/25/19,1,0,0,0,0,0,60;174;900;346;215;4,11;73;80;78;43;11,4;8;12;8;8;1,3;26;85;47;19;2,u;u
3033,ICLR,2020,Phase Transitions for the Information Bottleneck in Representation Learning,Tailin Wu;Ian Fischer,tailin@cs.stanford.edu;iansf@google.com,6;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,Stanford University;Google,4;-1,4;-1,,9/25/19,4,1,1,1,0,1,189;2706,20;16,8;12,16;364,m;m
3034,ICLR,2020,Model-based reinforcement learning for biological sequence design,Christof Angermueller;David Dohan;David Belanger;Ramya Deshpande;Kevin Murphy;Lucy Colwell,christofa@google.com;ddohan@google.com;dbelanger@google.com;ramyadeshpande@google.com;lcolwell@google.com;kpmurphy@google.com,6;3;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,5,9/25/19,4,3,2,0,0,1,1768;1101;1164;0;3210;1626,9;11;92;3;93;44,3;5;16;0;18;15,126;158;137;0;441;119,m;f
3035,ICLR,2020,GAT: Generative Adversarial Training for Adversarial Example Detection and Robust Classification,Xuwang Yin;Soheil Kolouri;Gustavo K Rohde,xy4cm@virginia.edu;skolouri@hrl.com;gustavo@virginia.edu,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),1,5,0,yes,9/25/19,University of Virginia;HRL Labs;University of Virginia,59;-1;59,107;-1;107,5;4,9/25/19,1,0,0,0,0,0,618;836;2916,22;61;139,8;18;26,55;42;190,m;m
3036,ICLR,2020,StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding,Wei Wang;Bin Bi;Ming Yan;Chen Wu;Jiangnan Xia;Zuyi Bao;Liwei Peng;Luo Si,hebian.ww@alibaba-inc.com;b.bi@alibaba-inc.com;ym119608@alibaba-inc.com;wuchen.wc@alibaba-inc.com;jiangnan.xjn@alibaba-inc.com;zuyi.bzy@alibaba-inc.com;liwei.peng@alibaba-inc.com;luo.si@alibaba-inc.com,3;8;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,3,0,yes,9/25/19,Alibaba Group;Alibaba Group;Alibaba Group;Alibaba Group;Alibaba Group;Alibaba Group;Alibaba Group;Alibaba Group,-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1,3,8/1/19,18,12,10,1,0,3,95;487;415;173;-1;34;36;4944,52;32;46;33;-1;13;7;267,5;11;11;6;-1;2;3;34,7;41;39;23;0;3;3;493,u;m
3037,ICLR,2020,Restricting the Flow: Information Bottlenecks for Attribution,Karl Schulz;Leon Sixt;Federico Tombari;Tim Landgraf,karl.schulz@tum.de;leon.sixt@fu-berlin.de;tombari@in.tum.de;tim.landgraf@fu-berlin.de,8;8;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Talk),1,4,2,yes,9/25/19,Technical University Munich;Freie Universität Berlin;Technical University Munich;Freie Universität Berlin,53;-1;53;-1,43;-1;43;-1,,9/25/19,11,4,5,0,0,1,362;113;6702;522,183;6;178;44,10;4;37;13,9;5;953;22,m;m
3038,ICLR,2020,Oblique Decision Trees from Derivatives of ReLU Networks,Guang-He Lee;Tommi S. Jaakkola,guanghe@csail.mit.edu;tommi@csail.mit.edu,3;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),0,7,0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2,5;5,,9/25/19,0,0,0,0,0,0,103;22299,18;293,6;69,15;2326,m;m
3039,ICLR,2020,DBA: Distributed Backdoor Attacks against Federated Learning,Chulin Xie;Keli Huang;Pin-Yu Chen;Bo Li,chulinxie@zju.edu.cn;nick_cooper@sjtu.edu.cn;pin-yu.chen@ibm.com;lbo@illinois.edu,6;6;8,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Accept (Poster),0,3,0,yes,9/25/19,"Zhejiang University;Shanghai Jiao Tong University;International Business Machines;University of Illinois, Urbana Champaign",56;53;-1;3,107;157;-1;48,4,9/25/19,8,5,0,1,0,1,11;11;194;2461,4;4;44;80,2;2;6;23,1;0;22;280,f;f
3040,ICLR,2020,Understanding and Improving Information Transfer in Multi-Task Learning,Sen Wu;Hongyang R. Zhang;Christopher Ré,senwu@cs.stanford.edu;hongyang@cs.stanford.edu;chrismre@stanford.edu,6;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0,yes,9/25/19,Stanford University;Stanford University;Stanford University,4;4;4,4;4;4,6,9/25/19,1,1,0,0,0,0,214;36;2785,65;23;70,9;3;20,5;1;415,m;m
3041,ICLR,2020,FSPool: Learning Set Representations with Featurewise Sort Pooling,Yan Zhang;Jonathon Hare;Adam Prügel-Bennett,yz5n12@ecs.soton.ac.uk;jsh2@ecs.soton.ac.uk;apb@ecs.soton.ac.uk,6;8;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0,yes,9/25/19,University of Southampton;University of Southampton;University of Southampton,172;172;172,122;122;122,,6/6/19,6,3,4,0,0,1,3711;1662;2170,701;118;137,30;22;26,206;153;135,u;m
3042,ICLR,2020,Cyclical Stochastic Gradient MCMC for Bayesian Deep Learning,Ruqi Zhang;Chunyuan Li;Jianyi Zhang;Changyou Chen;Andrew Gordon Wilson,rz297@cornell.edu;chunyuan.li@duke.edu;jz318@duke.edu;cchangyou@gmail.com;andrewgw@cims.nyu.edu,8;8;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,5,0,yes,9/25/19,"Cornell University;Duke University;Duke University;State University of New York, Buffalo;New York University",7;47;47;84;25,19;20;20;263;29,11;1,2/11/19,25,10,11,1,18,2,42;2089;117;223;2814,13;81;38;40;102,3;25;6;8;27,2;242;8;26;340,f;m
3043,ICLR,2020,RIDE: Rewarding Impact-Driven Exploration for Procedurally-Generated Environments,Roberta Raileanu;Tim Rocktäschel,raileanu@cs.nyu.edu;tim.rocktaeschel@gmail.com,6;8;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,New York University;Facebook AI Research,25;-1,29;-1,,9/25/19,4,4,1,0,0,0,82;2316,8;48,3;22,9;289,f;m
3044,ICLR,2020,Counterfactuals uncover the modular structure of deep generative models,Michel Besserve;Arash Mehrjou;Rémy Sun;Bernhard Schölkopf,michel.besserve@tuebingen.mpg.de;mehrjou.arash@gmail.com;remy.sun@ens-rennes.fr;bs@tuebingen.mpg.de,8;8;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,"Max Planck Institute for Intelligent Systems, Max-Planck Institute;Max Planck Institute for Intelligent Systems, Max-Planck Institute;Ecole Normale Superieure de Rennes;Max Planck Institute for Intelligent Systems, Max-Planck Institute",-1;-1;100;-1,-1;-1;1397;-1,5,12/8/18,11,9,3,0,17,0,264;-1;17;77134,28;-1;7;860,9;-1;2;119,18;0;1;9933,m;m
3045,ICLR,2020,Neural Machine Translation with Universal Visual Representation,Zhuosheng Zhang;Kehai Chen;Rui Wang;Masao Utiyama;Eiichiro Sumita;Zuchao Li;Hai Zhao,zhangzs@sjtu.edu.cn;khchen@nict.go.jp;wangrui@nict.go.jp;mutiyama@nict.go.jp;eiichiro.sumita@nict.go.jp;charlee@sjtu.edu.cn;zhaohai@cs.sjtu.edu.cn,6;8;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A,Accept (Spotlight),1,4,0,yes,9/25/19,"Shanghai Jiao Tong University;National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology;National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology;National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology;National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology;Shanghai Jiao Tong University;Shanghai Jiao Tong University",53;-1;-1;-1;-1;53;53,157;-1;-1;-1;-1;157;157,3,9/25/19,1,1,1,0,0,0,391;263;591;2553;4424;339;1300,32;39;54;229;378;29;262,13;8;14;24;31;9;18,45;15;47;237;341;34;76,m;m
3046,ICLR,2020,Once-for-All: Train One Network and Specialize it for Efficient Deployment,Han Cai;Chuang Gan;Tianzhe Wang;Zhekai Zhang;Song Han,hancai@mit.edu;ganchuang1990@gmail.com;usedtobe@mit.edu;zhangzk@mit.edu;songhan@mit.edu,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,7,0,yes,9/25/19,Massachusetts Institute of Technology;International Business Machines;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;-1;2;2;2,5;-1;5;5;5,2,8/26/19,44,32,20,0,0,11,1197;2233;-1;-1;64,16;81;-1;-1;7,10;26;-1;-1;2,262;230;0;0;12,m;m
3047,ICLR,2020,Learning Nearly Decomposable Value Functions Via Communication Minimization,Tonghan Wang*;Jianhao Wang*;Chongyi Zheng;Chongjie Zhang,tonghanwang1996@gmail.com;1040594377@qq.com;chongyeezheng@gmail.com;chongjie@tsinghua.edu.cn,3;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0,yes,9/25/19,Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University,8;8;8;8,23;23;23;23,,9/25/19,8,4,3,0,0,2,15;192;5;551,6;19;1;59,2;4;1;12,4;9;2;44,m;m
3048,ICLR,2020,Bridging Mode Connectivity in Loss Landscapes and Adversarial Robustness,Pu Zhao;Pin-Yu Chen;Payel Das;Karthikeyan Natesan Ramamurthy;Xue Lin,zhao.pu@husky.neu.edu;pin-yu.chen@ibm.com;daspa@us.ibm.com;knatesa@us.ibm.com;xue.lin@northeastern.edu,8;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,Northeastern University;International Business Machines;International Business Machines;International Business Machines;Northeastern University,16;-1;-1;-1;16,906;-1;-1;-1;906,4,9/25/19,0,0,0,0,0,0,221;194;545;1171;47,41;44;67;131;51,8;6;11;15;4,9;22;19;91;4,m;f
3049,ICLR,2020,The Break-Even Point on Optimization Trajectories of Deep Neural Networks,Stanislaw Jastrzebski;Maciej Szymczak;Stanislav Fort;Devansh Arpit;Jacek Tabor;Kyunghyun Cho*;Krzysztof Geras*,staszek.jastrzebski@gmail.com;msz93@o2.pl;stanislav.fort@gmail.com;devansharpit@gmail.com;jcktbr@gmail.com;kyunghyun.cho@nyu.edu;k.j.geras@nyu.edu,6;8;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Spotlight),1,7,0,yes,9/25/19,New York University;Jagiellonian University;Google;SalesForce.com;Jagiellonian University;New York University;New York University,25;481;-1;-1;481;25;25,29;610;-1;-1;610;29;29,8,9/25/19,8,5,1,0,0,1,886;92;107;921;8;46450;556,27;39;11;42;4;272;35,13;5;7;12;1;52;12,104;7;7;118;1;6610;51,m;m
3050,ICLR,2020,Empirical Studies on the Properties of Linear Regions in Deep Neural Networks,Xiao Zhang;Dongrui Wu,xiao_zhang@hust.edu.cn;drwu@hust.edu.cn,6;3;8,I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),2,6,0,yes,9/25/19,Hong Kong University of Science and Technology;Hong Kong University of Science and Technology,39;39,47;47,,9/25/19,1,0,0,0,0,0,190;3526,82;157,5;32,21;322,m;m
3051,ICLR,2020,Learning to Group: A Bottom-Up Framework for 3D Part Discovery in Unseen Categories,Tiange Luo;Kaichun Mo;Zhiao Huang;Jiarui Xu;Siyu Hu;Liwei Wang;Hao Su,luotg@pku.edu.cn;kaichun@cs.stanford.edu;z2huang@eng.ucsd.edu;jxuat@connect.ust.hk;sy89128@mail.ustc.edu.cn;wanglw@cis.pku.edu.cn;haosu@eng.ucsd.edu,8;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),1,5,0,yes,9/25/19,"Peking University;Stanford University;University of California, San Diego;The Hong Kong University of Science and Technology;University of Science and Technology of China;Peking University;University of California, San Diego",22;4;11;39;481;22;11,24;4;31;47;80;24;31,6;2,9/25/19,0,0,0,0,0,0,104;2501;266;829;45;2396;17303,8;16;6;93;26;75;48,4;5;3;17;3;23;15,23;822;44;7;1;264;2681,m;m
3052,ICLR,2020,Learning Compositional Koopman Operators for Model-Based Control,Yunzhu Li;Hao He;Jiajun Wu;Dina Katabi;Antonio Torralba,liyunzhu@mit.edu;haohe@mit.edu;jiajunwu.cs@gmail.com;dina@csail.mit.edu;torralba@csail.mit.edu,6;8;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,7,0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Google;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;-1;2;2,5;5;-1;5;5,10;8,9/25/19,7,5,4,0,0,1,428;109;4039;19583;3085,12;29;89;253;66,9;4;30;67;13,41;11;386;2607;448,m;m
3053,ICLR,2020,Meta-Learning without Memorization,Mingzhang Yin;George Tucker;Mingyuan Zhou;Sergey Levine;Chelsea Finn,mzyin@utexas.edu;gjt@google.com;mingyuan.zhou@mccombs.utexas.edu;svlevine@eecs.berkeley.edu;cbfinn@cs.stanford.edu,6;8;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,5,1,yes,9/25/19,"University of Texas, Austin;Google;University of Texas, Austin;University of California Berkeley;Stanford University",22;-1;22;5;4,38;-1;38;13;4,6,9/25/19,11,6,1,0,0,0,109;2705;2033;959;7879,13;75;115;34;101,6;21;24;9;34,16;301;235;103;1060,m;f
3054,ICLR,2020,Guiding Program Synthesis by Learning to Generate Examples,Larissa Laich;Pavol Bielik;Martin Vechev,llaich@ethz.ch;pavol.bielik@inf.ethz.ch;martin.vechev@inf.ethz.ch,8;3;8,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,9,0,yes,9/25/19,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10,13;13;13,,9/25/19,0,0,0,0,0,0,39;518;4246,5;26;153,3;11;36,2;56;467,f;m
3055,ICLR,2020,Truth or backpropaganda? An empirical investigation of deep learning theory,Micah Goldblum;Jonas Geiping;Avi Schwarzschild;Michael Moeller;Tom Goldstein,goldblumcello@gmail.com;jonas.geiping@uni-siegen.de;avi1@umd.edu;michael.moeller@uni-siegen.de;tomg@cs.umd.edu,8;8;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),3,4,0,yes,9/25/19,"University of Maryland, College Park;University of Siegen;University of Maryland, College Park;University of Siegen;University of Maryland, College Park",12;323;12;323;12,91;570;91;570;91,1;8,9/25/19,3,1,1,0,0,1,25;25;4;210;228,11;12;5;22;61,3;3;1;5;10,2;2;1;11;26,m;m
3056,ICLR,2020,What Can Neural Networks Reason About?,Keyulu Xu;Jingling Li;Mozhi Zhang;Simon S. Du;Ken-ichi Kawarabayashi;Stefanie Jegelka,keyulu@mit.edu;jingling@cs.umd.edu;mozhi@cs.umd.edu;ssdu@ias.edu;k_keniti@nii.ac.jp;stefje@mit.edu,8;8;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,10,0,yes,9/25/19,"Massachusetts Institute of Technology;University of Maryland, College Park;University of Maryland, College Park;Institue for Advanced Study, Princeton;Meiji University;Massachusetts Institute of Technology",2;12;12;-1;481;2,5;91;91;-1;332;5,1;10,5/30/19,24,15,7,1,6,4,834;40;43;2086;4979;3398,7;8;5;55;363;115,7;4;2;20;33;28,223;8;6;313;550;547,f;f
3057,ICLR,2020,U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation,Junho Kim;Minjae Kim;Hyeonwoo Kang;Kwang Hee Lee,takis0112@gmail.com;minjaekim@ncsoft.com;hwkang0131@ncsoft.com;lkwanghee@gmail.com,8;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,NCSOFT;NCSOFT;NCSOFT;Boeing Korea Engineering and Technology Center,-1;-1;-1;-1,-1;-1;-1;-1,,7/25/19,19,6,10,2,0,4,740;323;94;23,125;65;9;13,14;8;2;2,52;29;6;4,m;m
3058,ICLR,2020,Learning to Learn by Zeroth-Order Oracle,Yangjun Ruan;Yuanhao Xiong;Sashank Reddi;Sanjiv Kumar;Cho-Jui Hsieh,ruanyj3107@zju.edu.cn;yhxiong@cs.ucla.edu;sashank@google.com;sanjivk@google.com;chohsieh@cs.ucla.edu,6;6;8,I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,5,0,yes,9/25/19,"Zhejiang University;University of California, Los Angeles;Google;Google;University of California, Los Angeles",56;20;-1;-1;20,107;17;-1;-1;17,4;9,9/25/19,2,1,0,0,0,0,51;143;2194;941;12827,3;7;53;142;168,2;3;20;13;41,10;30;418;113;1746,m;m
3059,ICLR,2020,Lite Transformer with Long-Short Range Attention,Zhanghao Wu*;Zhijian Liu*;Ji Lin;Yujun Lin;Song Han,zhanghao.wu@outlook.com;zhijian@mit.edu;jilin@mit.edu;yujunlin@mit.edu;songhan@mit.edu,6;8;6,I have read many papers in this area.:N/A:N/A:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2;2,5;5;5;5;5,3,9/25/19,5,3,3,0,0,2,10;575;292;477;19,6;20;71;17;9,2;7;7;5;3,2;112;16;83;5,m;m
3060,ICLR,2020,Regularizing activations in neural networks via distribution matching with the Wasserstein metric,Taejong Joo;Donggu Kang;Byunghoon Kim,tjoo@estsoft.com;emppunity@gmail.com;byungkim@hanyang.ac.kr,6;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:N/A:N/A:N/A,Accept (Poster),0,4,0,yes,9/25/19,Estsoft;;Hanyang University,-1;-1;233,-1;-1;393,3;1;8,9/25/19,2,1,2,0,0,0,6;75;26,6;13;10,2;5;2,0;2;2,m;u
3061,ICLR,2020,The Ingredients of Real World Robotic Reinforcement Learning,Henry Zhu;Justin Yu;Abhishek Gupta;Dhruv Shah;Kristian Hartikainen;Avi Singh;Vikash Kumar;Sergey Levine,henryzhu@berkeley.edu;justinvyu@berkeley.edu;abhigupta@berkeley.edu;shah@eecs.berkeley.edu;kristian.hartikainen@gmail.com;avisingh@cs.berkeley.edu;vikashplus@gmail.com;svlevine@eecs.berkeley.edu,8;8;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Spotlight),0,5,0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of Oxford;University of California Berkeley;;University of California Berkeley,5;5;5;5;50;5;-1;5,13;13;13;13;1;13;-1;13,,9/25/19,3,1,1,0,0,0,139;48;1103;8;297;689;138;24893,6;32;48;10;11;24;41;310,2;4;16;2;5;9;6;74,4;3;70;0;54;95;7;3235,m;m
3062,ICLR,2020,Relational State-Space Model for Stochastic Multi-Object Systems,Fan Yang;Ling Chen;Fan Zhou;Yusong Gao;Wei Cao,fanyang01@zju.edu.cn;lingchen@cs.zju.edu.cn;fanzhou@zju.edu.cn;jianchuan.gys@alibaba-inc.com;mingsong.cw@alibaba-inc.com,6;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,7,0,yes,9/25/19,Zhejiang University;Zhejiang University;Zhejiang University;Alibaba Group;Alibaba Group,56;56;56;-1;-1,107;107;107;-1;-1,10,9/25/19,0,0,0,0,0,0,396;162;7;91;129,140;28;17;13;39,4;4;1;4;7,28;10;0;8;8,m;m
3063,ICLR,2020,Empirical Bayes Transductive Meta-Learning with Synthetic Gradients,Shell Xu Hu;Pablo Garcia Moreno;Yang Xiao;Xi Shen;Guillaume Obozinski;Neil Lawrence;Andreas Damianou,dom343@gmail.com;morepabl@amazon.com;yang.xiao@enpc.fr;xi.shen@enpc.fr;guillaume.obozinski@epfl.ch;n.lawrence@sheffield.ac.uk;damianou@amazon.com,6;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,1,yes,9/25/19,ENPC;Amazon;ENPC;ENPC;Swiss Federal Institute of Technology Lausanne;University of Sheffield;Amazon,-1;-1;-1;-1;481;205;-1,264;-1;264;264;38;117;-1,8;1;6,9/25/19,1,0,0,0,0,0,5;3;77;967;5381;11202;17,2;7;33;210;65;264;7,1;1;5;15;29;52;2,0;0;4;33;581;1393;0,m;m
3064,ICLR,2020,Implicit Bias of Gradient Descent based Adversarial Training on Separable Data,Yan Li;Ethan X.Fang;Huan Xu;Tuo Zhao,yli939@gatech.edu;xxf13@psu.edu;huan.xu@isye.gatech.edu;tourzhao@gatech.edu,3;8;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,4,0,yes,9/25/19,Georgia Institute of Technology;Pennsylvania State University;Georgia Institute of Technology;Georgia Institute of Technology,13;41;13;13,38;78;38;38,4,6/7/19,3,2,0,0,0,0,12007;295;144;2458,902;21;35;109,43;8;6;19,835;64;12;205,u;m
3065,ICLR,2020,word2ket: Space-efficient Word Embeddings inspired by Quantum Entanglement,Aliakbar Panahi;Seyran Saeedi;Tom Arodz,panahia@vcu.edu;saeedis@vcu.edu;tarodz@vcu.edu,8;8;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,3,0,yes,9/25/19,Virginia Commonwealth University;Virginia Commonwealth University;Virginia Commonwealth University,266;266;266,1397;1397;1397,3,9/25/19,3,2,0,0,0,0,3;11;464,7;6;41,1;2;10,0;0;27,m;m
3066,ICLR,2020,Towards Fast Adaptation of Neural Architectures with Meta Learning,Dongze Lian;Yin Zheng;Yintao Xu;Yanxiong Lu;Leyu Lin;Peilin Zhao;Junzhou Huang;Shenghua Gao,liandz@shanghaitech.edu.cn;yzheng3xg@gmail.com;xuyt@shanghaitech.edu.cn;alanlu@tencent.com;goshawklin@tencent.com;masonzhao@tencent.com;jzhuang@uta.edu;gaoshh@shanghaitech.edu.cn,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,"ShanghaiTech University;Tencent AI Lab;ShanghaiTech University;Tencent AI Lab;Tencent AI Lab;Tencent AI Lab;University of Texas, Arlington;ShanghaiTech University",481;-1;481;-1;-1;-1;118;481,1397;-1;1397;-1;-1;-1;708;1397,6,9/25/19,4,3,3,0,0,1,175;3;8;79;47;3602;342;34,12;5;3;8;15;133;49;8,5;1;2;4;3;31;8;3,48;1;1;6;5;393;42;2,m;m
3067,ICLR,2020,Hamiltonian Generative Networks,Peter Toth;Danilo J. Rezende;Andrew Jaegle;Sébastien Racanière;Aleksandar Botev;Irina Higgins,petertoth@google.com;danilor@google.com;drewjaegle@google.com;sracaniere@google.com;botev@google.com;irinah@google.com,6;6;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,9,0,yes,9/25/19,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,5,9/25/19,16,8,2,0,0,0,558;8814;142;797;264;2018,36;63;22;32;9;25,8;27;6;12;6;11,20;1132;4;79;36;302,m;f
3068,ICLR,2020,Residual Energy-Based Models for Text Generation,Yuntian Deng;Anton Bakhtin;Myle Ott;Arthur Szlam;Marc'Aurelio Ranzato,dengyuntian@seas.harvard.edu;yolo@fb.com;aszlam@fb.com;ranzato@fb.com,6;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,1,yes,9/25/19,Harvard University;Facebook;Facebook;Facebook,39;-1;-1;-1,7;-1;-1;-1,3,9/25/19,6,3,2,1,0,0,1479;178;3841;8926;-1,29;13;39;87;-1,13;6;20;33;-1,232;27;772;940;0,m;m
3069,ICLR,2020,Plug and Play Language Models: A Simple Approach to Controlled Text Generation,Sumanth Dathathri;Andrea Madotto;Janice Lan;Jane Hung;Eric Frank;Piero Molino;Jason Yosinski;Rosanne Liu,dathathris@gmail.com;amadotto@connect.ust.hk;lan.janice.j@gmail.com;jane.hung@uber.com;mysterefrank@uber.com;piero@uber.com;yosinski@uber.com;rosanne@uber.com,6;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),6,7,1,yes,9/25/19,California Institute of Technology;The Hong Kong University of Science and Technology;Uber;Uber;Uber;Uber;Uber;Uber,143;39;-1;-1;-1;-1;-1;-1,2;47;-1;-1;-1;-1;-1;-1,3,9/25/19,21,10,10,0,0,4,85;386;73;193;2448;488;8488;399,17;42;7;10;102;33;52;12,4;10;3;6;28;8;22;7,13;45;12;20;222;60;655;67,m;f
3070,ICLR,2020,"Deep 3D Pan via local adaptive t-shaped"" convolutions with global and local adaptive dilations""",Juan Luis Gonzalez Bello;Munchurl Kim,juanluisgb@kaist.ac.kr;mkimee@kaist.ac.kr,6;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,481;481,110;110,1,9/25/19,0,0,0,0,0,0,10;14,8;10,2;3,0;1,m;m
3071,ICLR,2020,Conservative Uncertainty Estimation By Fitting  Prior Networks,Kamil Ciosek;Vincent Fortuin;Ryota Tomioka;Katja Hofmann;Richard Turner,kamil.ciosek@microsoft.com;fortuin@inf.ethz.ch;ryoto@microsoft.com;katja.hofmann@microsoft.com;ret26@cam.ac.uk,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),2,13,0,yes,9/25/19,Microsoft;Swiss Federal Institute of Technology;Microsoft;Microsoft;University of Cambridge,-1;10;-1;-1;71,-1;13;-1;-1;3,11;2,9/25/19,0,0,0,0,0,0,136;79;4894;178;3055,20;18;86;36;175,7;6;29;8;30,14;5;660;11;315,m;m
3072,ICLR,2020,Dynamics-Aware Unsupervised Discovery of Skills,Archit Sharma;Shixiang Gu;Sergey Levine;Vikash Kumar;Karol Hausman,architsh@google.com;shanegu@google.com;slevine@google.com;vikashplus@google.com;karolhausman@google.com,8;8;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,3,0,yes,9/25/19,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,6,7/2/19,25,18,8,2,0,3,24;3857;25142;138;872,4;39;340;41;62,2;21;75;6;15,3;478;3258;7;53,m;m
3073,ICLR,2020,Implementation Matters in Deep RL: A Case Study on PPO and TRPO,Logan Engstrom;Andrew Ilyas;Shibani Santurkar;Dimitris Tsipras;Firdaus Janoos;Larry Rudolph;Aleksander Madry,ailyas@mit.edu;engstrom@mit.edu;shibani@mit.edu;tsipras@mit.edu;firdaus.janoos@twosigma.com;rudolph@csail.mit.edu;madry@mit.edu,8;8;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,10,1,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Two Sigma;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2;-1;2;2,5;5;5;5;-1;5;5,,9/25/19,10,3,4,3,0,2,2055;1947;1562;3896;372;5948;5523,27;29;26;33;40;186;84,15;15;14;17;11;39;30,271;273;187;935;27;403;1099,m;m
3074,ICLR,2020,Mutual Information Gradient Estimation for  Representation Learning,Liangjian Wen;Yiji Zhou;Lirong He;Mingyuan Zhou;Zenglin Xu,wlj6816@gmail.com;zhouyiji@outlook.com;ronghe1217@gmail.com;mingyuan.zhou@mccombs.utexas.edu;zenglin@gmail.com,8;6;3;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,14,0,yes,9/25/19,"University of Electronic Science and Technology of China;University of Electronic Science and Technology of China;University of Electronic Science and Technology of China;University of Texas, Austin;University of Electronic Science and Technology of China",481;481;481;22;481,628;628;628;38;628,,9/25/19,2,1,1,0,0,0,159;16;172;2033;2093,12;3;20;115;138,4;2;6;24;24,9;0;9;235;144,u;m
3075,ICLR,2020,Minimizing FLOPs to Learn Efficient Sparse Representations,Biswajit Paria;Chih-Kuan Yeh;Ian E.H. Yen;Ning Xu;Pradeep Ravikumar;Barnabás Póczos,bparia@cs.cmu.edu;cjyeh@cs.cmu.edu;a061105@gmail.com;ningxu01@gmail.com;pradeepr@cs.cmu.edu;bapoczos@cs.cmu.edu,8;3;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,7,0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;;Amazon;Carnegie Mellon University;Carnegie Mellon University,1;1;-1;-1;1;1,27;27;-1;-1;27;27,,9/25/19,0,0,0,0,0,0,62;284;512;190;8670;5863,10;21;52;50;181;244,4;7;12;5;38;40,6;45;71;10;1214;715,m;m
3076,ICLR,2020,Data-Independent Neural Pruning via Coresets,Ben Mussay;Margarita Osadchy;Vladimir Braverman;Samson Zhou;Dan Feldman,bengordoncshaifa@gmail.com;rita@cs.haifa.ac.il;vova@cs.jhu.edu;samsonzhou@gmail.com;dannyf.post@gmail.co,8;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,3,0,yes,9/25/19,University of Haifa;University of Haifa;Johns Hopkins University;Carnegie Mellon University;,172;172;73;1;-1,544;544;12;27;-1,4,7/9/19,2,1,1,0,0,0,7;1006;1165;166;1834,3;42;102;39;112,2;12;18;7;20,0;59;104;10;128,m;m
3077,ICLR,2020,LEARNED STEP SIZE QUANTIZATION,Steven K. Esser;Jeffrey L. McKinstry;Deepika Bablani;Rathinakumar Appuswamy;Dharmendra S. Modha,sesser@us.ibm.com;jlmckins@us.ibm.com;deepika.bablani@ibm.com;rappusw@us.ibm.com;dmodha@us.ibm.com,6;8;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,4,1,yes,9/25/19,International Business Machines;International Business Machines;International Business Machines;International Business Machines;International Business Machines,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,,2/21/19,22,11,8,2,2,5,3647;779;176;989;9911,25;34;19;26;95,17;11;7;13;37,327;82;16;96;1065,m;m
3078,ICLR,2020,Learning To Explore Using Active Neural SLAM,Devendra Singh Chaplot;Dhiraj Gandhi;Saurabh Gupta;Abhinav Gupta;Ruslan Salakhutdinov,chaplot@cs.cmu.edu;dhirajgandhi@fb.com;saurabhg@illinois.edu;abhinavg@cs.cmu.edu;rsalakhu@cs.cmu.edu,6;3;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0,yes,9/25/19,"Carnegie Mellon University;Facebook;University of Illinois, Urbana Champaign;Carnegie Mellon University;Carnegie Mellon University",1;-1;3;1;1,27;-1;48;27;27,,9/25/19,10,6,5,0,0,2,787;396;5250;-1;69005,30;20;63;-1;254,11;8;22;-1;82,100;22;612;0;7875,m;m
3079,ICLR,2020,Towards Stabilizing Batch Statistics in Backward Propagation of Batch Normalization,Junjie Yan;Ruosi Wan;Xiangyu Zhang;Wei Zhang;Yichen Wei;Jian Sun,jjyan17@fudan.edu.cn;wanruosi@megvii.com;zhangxiangyu@megvii.com;weizh@fudan.edu.cn;weiyichen@megvii.com;sunjian@megvii.com,6;6;8,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,1,yes,9/25/19,Fudan University;Megvii Technology Inc.;Megvii Technology Inc.;Fudan University;Megvii Technology Inc.;Megvii Technology Inc.,79;-1;-1;79;-1;-1,109;-1;-1;109;-1;-1,1;2,9/25/19,2,1,0,0,0,0,7065;13;64794;760;8840;3762,166;7;327;151;80;223,43;2;44;13;36;27,996;1;11989;38;1556;305,m;m
3080,ICLR,2020,Tensor Decompositions for Temporal Knowledge Base Completion,Timothée Lacroix;Guillaume Obozinski;Nicolas Usunier,timothee.lax@gmail.com;guillaume.obozinski@epfl.ch;usunier@fb.com,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,Facebook;Swiss Federal Institute of Technology Lausanne;Facebook,-1;481;-1,-1;38;-1,10,9/25/19,0,0,0,0,0,0,197;5381;6244,5;65;109,3;29;30,41;581;1186,m;m
3081,ICLR,2020,Generalization bounds for deep convolutional neural networks,Philip M. Long;Hanie Sedghi,plong@google.com;hsedghi@google.com,6;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),0,8,0,yes,9/25/19,Google;Google,-1;-1,-1;-1,1;8,5/29/19,25,14,5,1,8,2,5277;579,156;30,36;12,401;53,m;f
3082,ICLR,2020,Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples,Eleni Triantafillou;Tyler Zhu;Vincent Dumoulin;Pascal Lamblin;Utku Evci;Kelvin Xu;Ross Goroshin;Carles Gelada;Kevin Swersky;Pierre-Antoine Manzagol;Hugo Larochelle,eleni@cs.toronto.edu;tylerzhu@google.com;vdumoulin@google.com;lamblinp@google.com;evcu@google.com;kelvinxu@berkeley.edu;goroshin@google.com;cgel@google.com;kswersky@google.com;manzagop@google.com;hugolarochelle@google.com,3;8;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0,yes,9/25/19,"Department of Computer Science, University of Toronto;Google;Google;Google;Google;University of California Berkeley;Google;Google;Google;Google;Google",18;-1;-1;-1;-1;5;-1;-1;-1;-1;-1,18;-1;-1;-1;-1;13;-1;-1;-1;-1;-1,6;8,3/7/19,71,41,30,6,18,17,591;440;8319;6550;-1;7537;1679;231;5797;9307;25332,31;5;22;13;-1;19;14;6;52;13;124,9;3;17;12;-1;11;9;5;23;8;44,76;58;1232;467;0;762;159;38;885;991;2884,f;m
3083,ICLR,2020,Latent Normalizing Flows for Many-to-Many Cross-Domain Mappings,Shweta Mahajan;Iryna Gurevych;Stefan Roth,mahajan@aiphes.tu-darmstadt.de;gurevych@ukp.informatik.tu-darmstadt.de;stefan.roth@visinf.tu-darmstadt.de,8;8;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0,yes,9/25/19,TU Darmstadt;TU Darmstadt;TU Darmstadt,64;64;64,289;289;289,5,9/25/19,1,0,0,0,0,0,6;9038;1740,4;416;30,1;51;8,0;910;279,f;m
3084,ICLR,2020,Robustness Verification for Transformers,Zhouxing Shi;Huan Zhang;Kai-Wei Chang;Minlie Huang;Cho-Jui Hsieh,zhouxingshichn@gmail.com;huan@huan-zhang.com;kw@kwchang.net;aihuang@tsinghua.edu.cn;chohsieh@cs.ucla.edu,3;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,"Tsinghua University;University of California, Los Angeles;University of Virginia Main Campus;Tsinghua University;University of California, Los Angeles",8;20;59;8;20,23;17;107;23;17,1,9/25/19,5,3,3,0,0,1,12;2086;30;4321;292,6;31;27;161;21,2;19;3;31;4,2;289;2;484;55,m;m
3085,ICLR,2020,Explain Your Move: Understanding Agent Actions Using Specific and Relevant Feature Attribution,Nikaash Puri;Sukriti Verma;Piyush Gupta;Dhruv Kayastha;Shripad Deshmukh;Balaji Krishnamurthy;Sameer Singh,nikpuri@adobe.com;dce.sukriti@gmail.com;piygupta@adobe.com;dhruvkayastha@iitkgp.ac.in;shripad@smail.iitm.ac.in;kbalaji@adobe.com;sameer@uci.edu,8;6;8,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Poster),0,11,1,yes,9/25/19,"Adobe Systems;;Adobe Systems;Indian Institute of Technology Kharagpur;Indian Institute of Technology Madras;Adobe Systems;University of California, Irvine",-1;-1;-1;266;154;-1;35,-1;-1;-1;476;641;-1;96,,9/25/19,3,3,2,0,0,1,30;36;95;1;1;333;9063,9;14;35;2;2;74;220,2;3;5;1;1;9;27,4;7;7;1;1;19;937,m;m
3086,ICLR,2020,AdvectiveNet: An Eulerian-Lagrangian Fluidic Reservoir for Point Cloud Processing     ,Xingzhe He;Helen Lu Cao;Bo Zhu,xingzhe.he95@gmail.com;helen.l.cao.22@dartmouth.edu;bo.zhu@dartmouth.edu,6;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,1,1,yes,9/25/19,;Dartmouth College;Dartmouth College,-1;154;154,-1;94;94,2,9/25/19,1,0,1,0,0,0,1;125;1855,5;10;109,1;5;19,0;7;126,m;m
3087,ICLR,2020,Batch-shaping for learning conditional channel gated networks,Babak Ehteshami Bejnordi;Tijmen Blankevoort;Max Welling,behtesha@qti.qualcomm.com;tijmen@qti.qualcomm.com;mwelling@qti.qualcomm.com,8;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),1,3,0,yes,9/25/19,"Qualcomm Inc, QualComm;Qualcomm Inc, QualComm;Qualcomm Inc, QualComm",-1;-1;-1,-1;-1;-1,2,7/15/19,3,1,1,0,0,0,3402;87;752,27;13;59,12;3;14,108;17;112,m;m
3088,ICLR,2020,A Closer Look at Deep Policy Gradients,Andrew Ilyas;Logan Engstrom;Shibani Santurkar;Dimitris Tsipras;Firdaus Janoos;Larry Rudolph;Aleksander Madry,ailyas@mit.edu;engstrom@mit.edu;shibani@mit.edu;tsipras@mit.edu;firdaus.janoos@twosigma.com;rudolph@csail.mit.edu;madry@mit.edu,8;6;8,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Talk),0,3,1,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Two Sigma;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2;-1;2;2,5;5;5;5;-1;5;5,,11/6/18,0,0,0,0,0,0,1947;2055;1562;3896;372;5948;5523,29;27;26;33;40;186;84,15;15;14;17;11;39;30,273;271;187;935;27;403;1099,m;m
3089,ICLR,2020,Provable robustness against all adversarial $l_p$-perturbations for $p\geq 1$,Francesco Croce;Matthias Hein,francesco91.croce@gmail.com;matthias.hein@uni-tuebingen.de,6;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,University of Tuebingen;University of Tuebingen,154;154,91;91,4,5/27/19,10,3,0,0,0,1,295;5156,33;167,9;41,24;560,m;m
3090,ICLR,2020,Distributionally Robust Neural Networks,Shiori Sagawa*;Pang Wei Koh*;Tatsunori B. Hashimoto;Percy Liang,ssagawa@cs.stanford.edu;koh.pangwei@gmail.com;thashim@stanford.edu;pliang@cs.stanford.edu,8;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,4,0,yes,9/25/19,Stanford University;Stanford University;Stanford University;Stanford University,4;4;4;4,4;4;4;4,3;8,9/25/19,8,6,4,1,0,3,82;2251;525;12798,12;22;43;145,7;12;12;48,6;199;63;2071,f;m
3091,ICLR,2020,Nesterov Accelerated Gradient and Scale Invariance for Adversarial Attacks,Jiadong Lin;Chuanbiao Song;Kun He;Liwei Wang;John E. Hopcroft,jdlin@hust.edu.cn;cbsong@hust.edu.cn;brooklet60@hust.edu.cn;wanglw@cis.pku.edu.cn;jeh@cs.cornell.edu,6;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Peking University;Cornell University,39;39;39;22;7,47;47;47;24;19,4,8/17/19,1,0,0,0,0,0,121;47;230;98;28945,15;4;13;23;303,4;2;8;6;60,5;9;44;21;2742,m;m
3092,ICLR,2020,Fast Neural Network Adaptation via Parameter Remapping and Architecture Search,Jiemin Fang*;Yuzhu Sun*;Kangjian Peng*;Qian Zhang;Yuan Li;Wenyu Liu;Xinggang Wang,jaminfong@hust.edu.cn;yzsun@hust.edu.cn;kangjian.peng@horizon.ai;qian01.zhang@horizon.ai;yuan.li@horizon.ai;liuwy@hust.edu.cn;xgwang@hust.edu.cn,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,8,0,yes,9/25/19,Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Horizon Robotics;Horizon Robotics;Horizon Robotics;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology,39;39;-1;-1;-1;39;39,47;47;-1;-1;-1;47;47,2,9/25/19,4,1,4,0,0,0,33;19;6;521;322;343;3517,7;7;4;147;69;38;123,4;2;2;8;10;7;33,2;0;0;37;22;53;512,m;m
3093,ICLR,2020,Understanding Generalization in Recurrent Neural Networks,Zhuozhuo Tu;Fengxiang He;Dacheng Tao,zhtu3055@uni.sydney.edu.au;fengxiang.he@sydney.edu.au;dacheng.tao@sydney.edu.au,6;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,University of Sydney;University of Sydney;University of Sydney,86;86;86,60;60;60,1;8,9/25/19,1,0,0,0,0,0,2;56;3401,2;10;164,1;6;30,0;5;251,u;m
3094,ICLR,2020,Piecewise linear activations substantially shape the loss surfaces of neural networks,Fengxiang He;Bohan Wang;Dacheng Tao,fengxiang.he@sydney.edu.au;bhwangfy@gmail.com;dacheng.tao@sydney.edu.au,6;6;3,I do not know much about this area.:N/A:N/A:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,19,0,yes,9/25/19,University of Sydney;University of Science and Technology of China;University of Sydney,86;481;86,60;80;60,1,9/25/19,3,0,2,0,0,0,56;16;80,10;6;29,6;2;5,5;0;4,m;m
3095,ICLR,2020,ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning,Weihao Yu;Zihang Jiang;Yanfei Dong;Jiashi Feng,weihaoyu6@gmail.com;jzihang@u.nus.edu;yanfei.dong43@gmail.com;elefjia@nus.edu.sg,6;8;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0,yes,9/25/19,National University of Singapore;National University of Singapore;National University of Singapore;National University of Singapore,16;16;16;16,25;25;25;25,3,9/25/19,2,2,1,0,0,1,86;22;10;9533,9;5;7;332,5;3;2;52,13;2;2;1232,m;m
3096,ICLR,2020,Poly-encoders: Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring,Samuel Humeau;Kurt Shuster;Marie-Anne Lachaux;Jason Weston,samuelhumeau@fb.com;kshuster@fb.com;malachaux@fb.com;jaseweston@gmail.com,8;8;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,1,0,yes,9/25/19,Facebook;Facebook;Facebook;,-1;-1;-1;-1,-1;-1;-1;-1,,4/22/19,29,18,16,2,6,8,339;307;59;45686,15;16;5;242,10;8;4;78,30;50;12;5869,m;m
3097,ICLR,2020,Understanding the Limitations of Variational Mutual Information Estimators,Jiaming Song;Stefano Ermon,jiaming.tsong@gmail.com;ermon@cs.stanford.edu,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Accept (Poster),0,0,0,yes,9/25/19,Stanford University;Stanford University,4;4,4;4,,9/25/19,16,8,6,2,0,3,811;4958,44;203,14;31,127;664,m;m
3098,ICLR,2020,A Baseline for Few-Shot Image Classification,Guneet Singh Dhillon;Pratik Chaudhari;Avinash Ravichandran;Stefano Soatto,guneetdhillon@utexas.edu;pratikac@seas.upenn.edu;avinash.a.ravichandran@gmail.com;soattos@amazon.com,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,5,0,yes,9/25/19,"University of Texas, Austin;University of Pennsylvania;Amazon;Amazon",22;19;-1;-1,38;11;-1;-1,6,9/6/19,25,10,8,2,0,5,227;192;1333;15761,8;17;38;458,3;7;14;62,25;10;230;1445,m;m
3099,ICLR,2020,Hierarchical Foresight: Self-Supervised Learning of Long-Horizon Tasks via Visual Subgoal Generation,Suraj Nair;Chelsea Finn,surajn@stanford.edu;chelseaf@google.com,6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.,Accept (Poster),1,2,0,yes,9/25/19,Stanford University;Google,4;-1,4;-1,,9/12/19,7,5,2,0,0,0,139;7879,11;101,6;34,12;1060,m;f
3100,ICLR,2020,Pseudo-LiDAR++: Accurate Depth for 3D Object Detection in Autonomous Driving,Yurong You;Yan Wang;Wei-Lun Chao;Divyansh Garg;Geoff Pleiss;Bharath Hariharan;Mark Campbell;Kilian Q. Weinberger,yy785@cornell.edu;yw763@cornell.edu;weilunchao760414@gmail.com;dg595@cornell.edu;gp346@cornell.edu;bharathh@cs.cornell.edu;mc288@cornell.edu;kqw4@cornell.edu,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,Cornell University;Cornell University;Ohio State University;Cornell University;Cornell University;Cornell University;Cornell University;Cornell University,7;7;77;7;7;7;7;7,19;19;373;19;19;19;19;19,2,6/14/19,28,13,14,3,0,4,356;325;1751;137;1536;924;934;24205,22;11;44;6;17;24;49;165,8;6;14;3;11;6;12;54,27;44;302;20;242;190;91;3866,m;m
3101,ICLR,2020,V4D: 4D Convolutional Neural Networks for Video-level Representation Learning,Shiwen Zhang;Sheng Guo;Weilin Huang;Matthew R. Scott;Limin Wang,shizhang@malong.com;sheng@malong.com;whuang@malong.com;mscott@malong.com;07wanglimin@gmail.com,6;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Accept (Poster),0,4,0,yes,9/25/19,Malong Technologies;Malong Technologies;Malong Technologies;Malong Technologies;Zhejiang University,-1;-1;-1;-1;56,-1;-1;-1;-1;107,,9/25/19,2,0,0,0,0,0,262;166;2329;482;246,35;29;76;54;24,7;6;21;10;5,8;11;205;48;29,u;m
3102,ICLR,2020,Certified Defenses for Adversarial Patches,Ping-yeh Chiang*;Renkun Ni*;Ahmed Abdelkader;Chen Zhu;Christoph Studor;Tom Goldstein,pchiang@cs.umd.edu;rn9zm@cs.umd.edu;akader@cs.umd.edu;chenzhu@cs.umd.edu;studer@cornell.edu;tomg@cs.umd.edu,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,9,0,yes,9/25/19,"University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park;Cornell University;University of Maryland, College Park",12;12;12;12;7;12,91;91;91;91;19;91,4;2,9/25/19,11,9,5,2,0,4,24;59;224;47;211;228,7;8;55;21;57;61,3;4;8;3;9;10,5;9;18;10;18;26,m;m
3103,ICLR,2020,Chameleon: Adaptive Code Optimization for Expedited Deep Neural Network Compilation,Byung Hoon Ahn;Prannoy Pilligundla;Amir Yazdanbakhsh;Hadi Esmaeilzadeh,bhahn@eng.ucsd.edu;ppilligu@eng.ucsd.edu;ayazdan@google.com;hadi@eng.ucsd.edu,6;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,"University of California, San Diego;University of California, San Diego;Google;University of California, San Diego",11;11;-1;11,31;31;-1;31,,9/25/19,2,1,0,0,0,0,16;47;103;5149,15;9;18;109,3;3;3;27,0;8;10;405,m;m
3104,ICLR,2020,Semantically-Guided Representation Learning for Self-Supervised Monocular Depth,Vitor Guizilini;Rui Hou;Jie Li;Rares Ambrus;Adrien Gaidon,vitor.guizilini@tri.global;rayhou@umich.edu;jie.li@tri.global;rares.ambrus@tri.global;adrien.gaidon@tri.global,6;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,8,0,yes,9/25/19,Toyota Research Institute;University of Michigan;Toyota Research Institute;Toyota Research Institute;Toyota Research Institute,-1;8;-1;-1;-1,-1;21;-1;-1;-1,2,9/25/19,2,0,1,1,0,0,260;219;149;462;1425,52;55;84;30;53,10;5;3;11;17,20;15;15;19;166,m;m
3105,ICLR,2020,Target-Embedding Autoencoders for Supervised Representation Learning,Daniel Jarrett;Mihaela van der Schaar,daniel.jarrett@eng.ox.ac.uk;mv472@damtp.cam.ac.uk,8;6;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,19,0,yes,9/25/19,University of Oxford;University of Cambridge,50;71,1;3,8,9/25/19,1,0,0,0,0,0,12;8828,6;643,2;42,0;547,m;f
3106,ICLR,2020,MetaPix: Few-Shot Video Retargeting,Jessica Lee;Deva Ramanan;Rohit Girdhar,jl5@cs.cmu.edu;deva@cs.cmu.edu;rgirdhar@cs.cmu.edu,6;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Poster),0,3,0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1,27;27;27,5;6,9/25/19,2,2,0,0,0,0,381;32040;933,42;163;20,9;59;8,33;5389;127,f;m
3107,ICLR,2020,Lazy-CFR: fast and near-optimal regret minimization for extensive games with imperfect information,Yichi Zhou;Tongzheng Ren;Jialian Li;Dong Yan;Jun Zhu,vofhqn@gmail.com;rtz19970824@gmail.com;lijialia16@mails.tsinghua.edu.cn;sproblvem@gmail.com;dcszj@mail.tsinghua.edu.cn,8;8;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,5,0,yes,9/25/19,";University of Texas, Austin;Tsinghua University;;Tsinghua University",-1;22;8;-1;8,-1;38;23;-1;23,1,10/10/18,2,1,0,1,2,0,11;44;53;23;4636,9;11;23;10;204,2;3;4;2;35,0;5;1;2;534,m;m
3108,ICLR,2020,Neural Module Networks for Reasoning over Text,Nitish Gupta;Kevin Lin;Dan Roth;Sameer Singh;Matt Gardner,gnnitish@gmail.com;kevinlin@eecs.berkeley.edu;danroth@seas.upenn.edu;sameer@uci.edu;mattg@allenai.org,6;6;8,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,1,yes,9/25/19,"University of Pennsylvania;University of California Berkeley;University of Pennsylvania;University of California, Irvine;Allen Institute for Artificial Intelligence",19;5;19;35;-1,11;13;11;96;-1,3,9/25/19,5,2,1,0,0,0,140;72;18936;5682;5695,15;8;505;117;55,5;5;68;27;19,11;6;1885;775;1063,m;m
3109,ICLR,2020,Shifted and Squeezed 8-bit Floating Point format for Low-Precision Training of Deep Neural Networks,Leopold Cambier;Anahita Bhiwandiwalla;Ting Gong;Oguz H. Elibol;Mehran Nekuii;Hanlin Tang,lcambier@stanford.edu;anahita.bhiwandiwalla@intel.com;ting.gong@intel.com;oguz.h.elibol@intel.com;mehran.nekuii@intel.com;hanlin.tang@intel.com,6;6;1;8,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,Stanford University;Intel;Intel;Intel;Intel;Intel,4;-1;-1;-1;-1;-1,4;-1;-1;-1;-1;-1,,9/25/19,1,1,0,0,0,0,8;44;95;282;61;495,4;7;35;34;10;35,1;3;5;8;5;11,0;1;4;8;7;71,m;m
3110,ICLR,2020,ProxSGD: Training Structured Neural Networks under Regularization and Constraints,Yang Yang;Yaxiong Yuan;Avraam Chatzimichailidis;Ruud JG van Sloun;Lei Lei;Symeon Chatzinotas,yang.yang@itwm.fraunhofer.de;yaxiong.yuan@uni.lu;avraam.chatzimichailidis@itwm.fraunhofer.de;r.j.g.v.sloun@tue.nl;lei.lei@uni.lu;symeon.chatzinotas@uni.lu,3;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,1,yes,9/25/19,"Fraunhofer IIS;Interdisciplinary Centre for Security, Reliability and Trust (SnT);Fraunhofer IIS;Eindhoven University of Technology;Interdisciplinary Centre for Security, Reliability and Trust (SnT);Interdisciplinary Centre for Security, Reliability and Trust (SnT)",-1;-1;-1;205;-1;-1,-1;-1;-1;185;-1;-1,9,9/25/19,0,0,0,0,0,0,2638;19;0;117;518;67,189;11;2;33;50;61,11;2;0;6;10;4,135;2;0;5;32;3,m;m
3111,ICLR,2020,BayesOpt Adversarial Attack,Binxin Ru;Adam Cobb;Arno Blaas;Yarin Gal,robin@robots.ox.ac.uk;adam.cobb@worc.ox.ac.uk;arno@robots.ox.ac.uk;yarin@cs.ox.ac.uk,6;3;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,4,0,yes,9/25/19,University of Oxford;University of Oxford;University of Oxford;University of Oxford,50;50;50;50,1;1;1;1,4;11,9/25/19,5,1,1,0,0,0,11;47;16;6605,10;19;5;91,3;4;3;20,1;2;0;987,m;m
3112,ICLR,2020,Distance-Based Learning from Errors for Confidence Calibration,Chen Xing;Sercan Arik;Zizhao Zhang;Tomas Pfister,xingchen1113@gmail.com;soarik@google.com;zizhaoz@google.com;tpfister@google.com,6;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,Nankai University;Google;Google;Google,481;-1;-1;-1,366;-1;-1;-1,,9/25/19,2,1,0,0,0,0,11;1391;978;2335,3;54;59;47,2;17;15;16,2;119;102;285,f;m
3113,ICLR,2020,Compressive Transformers for Long-Range Sequence Modelling,Jack W. Rae;Anna Potapenko;Siddhant M. Jayakumar;Chloe Hillier;Timothy P. Lillicrap,jwrae@google.com;apotapenko@google.com;sidmj@google.com;chillier@google.com;countzero@google.com,8;8;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,8,1,yes,9/25/19,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,3,9/25/19,19,10,3,0,0,2,631;156;192;24158;-1,19;12;14;74;-1,10;6;9;39;-1,74;16;23;2943;0,m;m
3114,ICLR,2020,VL-BERT: Pre-training of Generic Visual-Linguistic Representations,Weijie Su;Xizhou Zhu;Yue Cao;Bin Li;Lewei Lu;Furu Wei;Jifeng Dai,jackroos@mail.ustc.edu.cn;ezra0408@mail.ustc.edu.cn;yuecao@microsoft.com;binli@ustc.edu.cn;lewlu@microsoft.com;fuwei@microsoft.com;jifdai@microsoft.com,3;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,4,0,yes,9/25/19,University of Science and Technology of China;University of Science and Technology of China;Microsoft;University of Science and Technology of China;Microsoft;Microsoft;Microsoft,481;481;-1;481;-1;-1;-1,80;80;-1;80;-1;-1;-1,,8/22/19,72,36,31,1,0,9,801;734;89;297;66;7172;7202,14;19;14;65;1;164;46,9;9;3;8;1;43;21,110;121;9;14;9;783;1057,m;m
3115,ICLR,2020,Symplectic Recurrent Neural Networks,Zhengdao Chen;Jianyu Zhang;Martin Arjovsky;Léon Bottou,zc1216@nyu.edu;edzhang@tju.edu.cn;martinarjovsky@gmail.com;leonb@fb.com,8;8;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,4,1,yes,9/25/19,New York University;Zhejiang University;New York University;Facebook,25;56;25;-1,29;107;29;-1,,9/25/19,16,10,2,0,0,3,245;64;8036;4561,12;24;15;28,7;4;9;11,29;7;1795;348,m;m
3116,ICLR,2020,Learning to Coordinate Manipulation Skills via Skill Behavior Diversification,Youngwoon Lee;Jingyun Yang;Joseph J. Lim,lee504@usc.edu;jingyuny@usc.edu;limjj@usc.edu,6;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,5,0,yes,9/25/19,University of Southern California;University of Southern California;University of Southern California,31;31;31,62;62;62,,9/25/19,2,1,2,0,0,1,409;182;2976,11;34;51,5;7;21,69;17;270,m;m
3117,ICLR,2020,Learning Expensive Coordination: An Event-Based Deep RL Approach,Zhenyu Shi*;Runsheng Yu*;Xinrun Wang*;Rundong Wang;Youzhi Zhang;Hanjiang Lai;Bo An,shizhy6@mail2.sysu.edu.cn;runsheng.yu@ntu.edu.sg;xwang033@e.ntu.edu.sg;rundong001@e.ntu.edu.sg;yzhang137@e.ntu.edu.sg;laihanj3@mail.sysu.edu.cn;boan@ntu.edu.sg,6;8;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Accept (Poster),0,5,0,yes,9/25/19,SUN YAT-SEN UNIVERSITY;National Taiwan University;National Taiwan University;National Taiwan University;National Taiwan University;SUN YAT-SEN UNIVERSITY;National Taiwan University,481;86;86;86;86;481;86,299;120;120;120;120;299;120,,9/25/19,0,0,0,0,0,0,230;35;14;558;331;1722;73,35;8;10;30;51;46;36,6;2;2;9;9;16;4,7;1;1;26;15;340;7,m;m
3118,ICLR,2020,Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML,Aniruddh Raghu;Maithra Raghu;Samy Bengio;Oriol Vinyals,aniruddhraghu@gmail.com;maithrar@gmail.com;bengio@google.com;vinyals@google.com,3;8;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Poster),0,5,0,yes,9/25/19,Massachusetts Institute of Technology;Cornell University;Google;Google,2;7;-1;-1,5;19;-1;-1,6,9/19/19,33,14,13,5,0,6,184;1123;26795;4573,8;27;332;29,6;10;67;16,21;149;3497;412,m;m
3119,ICLR,2020,Iterative energy-based projection on a normal data manifold for anomaly localization,David Dehaene;Oriel Frigo;Sébastien Combrexelle;Pierre Eline,david@anotherbrain.ai;oriel@anotherbrain.ai;sebastien@anotherbrain.ai;pierre@anotherbrain.ai,3;6;8,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,AnotherBrain;AnotherBrain;AnotherBrain;AnotherBrain,-1;-1;-1;-1,-1;-1;-1;-1,2,9/25/19,3,2,2,0,0,2,29;124;42;2,5;12;17;8,2;4;4;1,5;12;3;2,u;u
3120,ICLR,2020,Harnessing Structures for Value-Based Planning and Reinforcement Learning,Yuzhe Yang;Guo Zhang;Zhi Xu;Dina Katabi,yuzhe@mit.edu;guozhang@mit.edu;zhixu@mit.edu;dina@csail.mit.edu,8;6;8,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,8,0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2,5;5;5;5,,9/25/19,5,3,2,0,0,0,211;1187;299;19583,28;129;70;255,7;19;7;67,7;51;18;2607,m;f
3121,ICLR,2020,A closer look at the approximation capabilities of neural networks,Kai Fong Ernest Chong,ernest_chong@sutd.edu.sg,8;6;6;6,I have read many papers in this area.:N/A:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:N/A:N/A:I did not assess the derivations or theory.;I have read many papers in this area.:N/A:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:N/A:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,1,yes,9/25/19,Singapore University of Technology and Design,481,1397,1,9/25/19,0,0,0,0,0,0,40,18,3,0,m
3122,ICLR,2020,Spectral  Embedding of Regularized Block Models,Nathan De Lara;Thomas Bonald,ndelara@enst.fr;bonald@enst.fr,6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,1,0,yes,9/25/19,Télécom ParisTech;Télécom ParisTech,481;481,187;187,10,9/25/19,1,1,0,0,0,0,17;3951,9;177,2;30,2;404,u;m
3123,ICLR,2020,MMA Training: Direct Input Space Margin Maximization through Adversarial Training,Gavin Weiguang Ding;Yash Sharma;Kry Yik Chau Lui;Ruitong Huang,gavin.w.ding@gmail.com;yash.sharma@bethgelab.org;yikchau.y.lui@borealisai.com;ruitong.huang@borealisai.com,3;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Accept (Poster),1,10,0,yes,9/25/19,"Borealis AI;Centre for Integrative Neuroscience, AG Bethge;Borealis AI;Borealis AI",-1;-1;-1;-1,-1;-1;-1;-1,4;1,12/6/18,21,14,8,1,3,3,243;1234;40;402,21;31;4;29,9;10;3;10,12;142;5;32,m;m
3124,ICLR,2020,Monotonic Multihead Attention,Xutai Ma;Juan Miguel Pino;James Cross;Liezl Puzon;Jiatao Gu,xutai_ma@jhu.edu;juancarabina@fb.com;jcross@fb.com;lie@fb.com;jgu@fb.com,6;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,9,0,yes,9/25/19,Johns Hopkins University;Facebook;Facebook;Facebook;Facebook,73;-1;-1;-1;-1,12;-1;-1;-1;-1,3,9/25/19,12,8,8,1,0,4,63;389;40;16;1440,14;46;14;7;41,5;11;3;2;15,18;39;4;4;217,m;m
3125,ICLR,2020,Learning representations for binary-classification without backpropagation,Mathias Lechner,mathias.lechner@ist.ac.at,6;6;8,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,Institute of Science and Technology Austria,481,1397,1,9/25/19,1,1,0,0,0,0,29,13,3,1,m
3126,ICLR,2020,Action Semantics Network: Considering the Effects of Actions in Multiagent Systems,Weixun Wang;Tianpei Yang;Yong Liu;Jianye Hao;Xiaotian Hao;Yujing Hu;Yingfeng Chen;Changjie Fan;Yang Gao,wxwang@tju.edu.cn;tpyang@tju.edu.cn;lucasliunju@gmail.com;jianye.hao@tju.edu.cn;xiaotianhao@tju.edu.cn;huyujing@corp.netease.com;chenyingfeng1@corp.netease.com;fanchangjie@corp.netease.com;gaoy@nju.edu.cn,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,Zhejiang University;Zhejiang University;;Zhejiang University;Zhejiang University;Corp.netease;University of Science and Technology of China;Corp.netease;Zhejiang University,56;56;-1;56;56;-1;481;-1;56,107;107;-1;107;107;-1;80;-1;107,,7/26/19,1,1,1,0,0,1,745;47;1426;491;55;161;133;99;2357,44;18;202;138;21;20;31;28;238,11;4;18;11;5;5;7;6;26,54;8;118;42;3;6;5;11;136,u;m
3127,ICLR,2020,Finite Depth and Width Corrections to the Neural Tangent Kernel,Boris Hanin;Mihai Nica,bhanin@math.tamu.edu;mnica@math.utoronto.ca,6;8;8,I have published in this field for several years.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,3,0,yes,9/25/19,Texas A&M;Toronto University,44;18,177;18,1,9/13/19,11,6,2,0,0,1,479;20,31;19,12;2,42;2,m;m
3128,ICLR,2020,What graph neural networks cannot learn: depth vs width,Andreas Loukas,andreas.loukas@epfl.ch,6;8;8,I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,Swiss Federal Institute of Technology Lausanne,481,38,10,7/6/19,18,16,6,3,0,3,628,53,15,51,m
3129,ICLR,2020,Neural Oblivious Decision Ensembles for Deep Learning on Tabular Data,Sergei Popov;Stanislav Morozov;Artem Babenko,sapopov@yandex-team.ru;stanis-morozov@yandex.ru;artem.babenko@phystech.edu,6;8;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,Yandex;Yandex;Moscow Institute of Physics and Technology,-1;-1;481,-1;-1;234,3,9/13/19,7,5,1,0,0,2,761;36;1657,108;17;29,15;3;10,26;11;282,m;m
3130,ICLR,2020,Gradient $\ell_1$ Regularization for Quantization Robustness,Milad Alizadeh;Arash Behboodi;Mart van Baalen;Christos Louizos;Tijmen Blankevoort;Max Welling,milada@qti.qualcomm.com;behboodi@qti.qualcomm.com;mart@qti.qualcomm.com;clouizos@qti.qualcomm.com;tijmen@qti.qualcomm.com;mwelling@qti.qualcomm.com,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,"Qualcomm Inc, QualComm;Qualcomm Inc, QualComm;Qualcomm Inc, QualComm;Qualcomm Inc, QualComm;Qualcomm Inc, QualComm;Qualcomm Inc, QualComm",-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,,9/25/19,2,2,1,0,0,2,88;318;38;1273;87;28145,27;66;5;23;13;329,5;9;2;12;3;60,13;17;11;239;17;5302,m;m
3131,ICLR,2020,PairNorm: Tackling Oversmoothing in GNNs,Lingxiao Zhao;Leman Akoglu,lingxiao@cmu.edu;lakoglu@andrew.cmu.edu,8;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),0,2,0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University,1;1,27;27,10,9/25/19,18,14,7,1,0,5,141;4078,24;125,8;32,7;415,m;f
3132,ICLR,2020,Cross-lingual Alignment vs Joint Training: A Comparative Study and A Simple Unified Framework,Zirui Wang*;Jiateng Xie*;Ruochen Xu;Yiming Yang;Graham Neubig;Jaime G. Carbonell,ziruiw@cs.cmu.edu;jiatengx@cs.cmu.edu;ruochenx@cs.cmu.edu;yiming@cs.cmu.edu;gneubig@cs.cmu.edu;jgc@cs.cmu.edu,8;8;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),2,5,0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1;1;1,27;27;27;27;27;27,6,9/25/19,3,2,2,0,0,0,32;92;109;21623;5304;16003,5;8;14;273;443;507,2;3;6;49;38;55,2;9;15;2592;547;1648,m;m
3133,ICLR,2020,Generalization of Two-layer Neural Networks: An Asymptotic Viewpoint,Jimmy Ba;Murat Erdogdu;Taiji Suzuki;Denny Wu;Tianzong Zhang,jba@cs.toronto.edu;erdogdu@cs.toronto.edu;taiji@mist.i.u-tokyo.ac.jp;dennywu@cs.toronto.edu;ztz16@mails.tsinghua.edu.cn,8;6;8,I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,3,0,yes,9/25/19,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;The University of Tokyo;Department of Computer Science, University of Toronto;Tsinghua University",18;18;56;18;8,18;18;36;18;23,8,9/25/19,8,6,2,0,0,0,52924;627;2318;138;10,56;47;177;16;3,22;10;26;6;2,8625;99;237;3;0,m;m
3134,ICLR,2020,LAMOL: LAnguage MOdeling for Lifelong Language Learning,Fan-Keng Sun*;Cheng-Hao Ho*;Hung-Yi Lee,fankeng@mit.edu;jojotenya@gmail.com;hungyilee@ntu.edu.tw,6;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I made a quick assessment of this paper.:N/A,Accept (Poster),0,4,0,yes,9/25/19,Massachusetts Institute of Technology;;National Taiwan University,2;-1;86,5;-1;120,3,9/7/19,0,0,0,0,0,0,26;1;1119,6;2;134,2;1;18,7;1;76,m;m
3135,ICLR,2020,Neural Stored-program Memory,Hung Le;Truyen Tran;Svetha Venkatesh,lethai@deakin.edu.au;truyen.tran@deakin.edu.au;svetha.venkatesh@deakin.edu.au,8;6;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,8,0,yes,9/25/19,Deakin University;Deakin University;Deakin University,481;481;481,332;332;332,6,5/25/19,5,3,0,0,0,0,791;1938;615,49;133;141,11;24;12,88;127;44,m;f
3136,ICLR,2020,BatchEnsemble: an Alternative Approach to Efficient Ensemble and Lifelong Learning,Yeming Wen;Dustin Tran;Jimmy Ba,ywen@cs.toronto.edu;trandustin@google.com;jba@cs.toronto.edu,6;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,9,1,yes,9/25/19,"Department of Computer Science, University of Toronto;Google;Department of Computer Science, University of Toronto",18;-1;18,18;-1;18,,9/25/19,8,3,4,0,0,1,144;1809;52924,10;50;56,4;20;22,13;197;8625,m;m
3137,ICLR,2020,Neural Arithmetic Units,Andreas Madsen;Alexander Rosenberg Johansen,amwebdk@gmail.com;alexander@herhjemme.dk,6;8;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,11,0,yes,9/25/19,;Technical University of Denmark,-1;481,-1;182,,9/25/19,3,1,0,0,0,1,1249;109,54;10,20;5,79;8,m;m
3138,ICLR,2020,"To Relieve Your Headache of Training an MRF, Take AdVIL",Chongxuan Li;Chao Du;Kun Xu;Max Welling;Jun Zhu;Bo Zhang,chongxuanli1991@gmail.com;duchao0726@gmail.com;kunxu.thu@gmail.com;m.welling@uva.nl;dcszj@mail.tsinghua.edu.cn;dcszb@mail.tsinghua.edu.cn,6;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,6,0,yes,9/25/19,Tsinghua University;Tsinghua University;Tsinghua University;University of Amsterdam;Tsinghua University;Tsinghua University,8;8;8;172;8;8,23;23;23;62;23;23,4,1/24/19,2,0,1,0,0,0,504;279;11658;27129;-1;318,19;41;322;270;-1;103,7;9;43;59;-1;6,60;8;726;5160;0;46,m;m
3139,ICLR,2020,State Alignment-based Imitation Learning,Fangchen Liu;Zhan Ling;Tongzhou Mu;Hao Su,fliu@eng.ucsd.edu;z6ling@eng.ucsd.edu;t3mu@eng.ucsd.edu;haosu@eng.ucsd.edu,6;3;8,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,12,0,yes,9/25/19,"University of California, San Diego;University of California, San Diego;University of California, San Diego;University of California, San Diego",11;11;11;11,31;31;31;31,,9/25/19,4,2,1,0,0,0,476;4;15;17303,15;9;5;48,5;1;3;15,76;0;0;2681,f;m
3140,ICLR,2020,Influence-Based Multi-Agent Exploration,Tonghan Wang*;Jianhao Wang*;Yi Wu;Chongjie Zhang,tonghanwang1996@gmail.com;1040594377@qq.com;jxwuyi@openai.com;chongjie@tsinghua.edu.cn,6;8;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Spotlight),3,4,0,yes,9/25/19,Tsinghua University;Tsinghua University;OpenAI;Tsinghua University,8;8;-1;8,23;23;-1;23,,9/25/19,3,1,1,0,0,1,15;192;773;551,6;19;169;59,2;4;14;12,4;9;32;44,m;m
3141,ICLR,2020,AE-OT: A NEW GENERATIVE MODEL BASED ON EXTENDED SEMI-DISCRETE OPTIMAL TRANSPORT,Dongsheng An;Yang Guo;Na Lei;Zhongxuan Luo;Shing-Tung Yau;Xianfeng Gu,doan@cs.stonybrook.edu;yangguo@cs.stonybrook.edu;nalei@dlut.edu.cn;zxluo@dlut.edu.cn;yau@math.harvard.edu;gu@cs.stonybrook.edu,3;3;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0,yes,9/25/19,"State University of New York, Stony Brook;State University of New York, Stony Brook;South China University of Technology;South China University of Technology;Harvard University;State University of New York, Stony Brook",41;41;481;481;39;41,304;304;501;501;7;304,5;4;9,9/25/19,0,0,0,0,0,0,58;20;717;803;78;818,12;19;125;174;7;42,4;3;14;14;2;11,6;2;15;57;7;60,m;m
3142,ICLR,2020,CoPhy: Counterfactual Learning of Physical Dynamics,Fabien Baradel;Natalia Neverova;Julien Mille;Greg Mori;Christian Wolf,fabien.baradel@insa-lyon.fr;nneverova@fb.com;julien.mille@insa-cvl.fr;mori@cs.sfu.ca;christian.wolf@insa-lyon.fr,6;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,6,0,yes,9/25/19,INSA de Lyon;Facebook;;Simon Fraser University;INSA de Lyon,481;-1;-1;64;481,1397;-1;-1;272;1397,,9/25/19,7,5,1,0,0,1,255;435;696;9711;2941,9;19;62;197;113,7;7;15;45;28,32;51;71;817;236,m;m
3143,ICLR,2020,Disentangling Factors of Variations Using Few Labels,Francesco Locatello;Michael Tschannen;Stefan Bauer;Gunnar Rätsch;Bernhard Schölkopf;Olivier Bachem,flocatello@tuebingen.mpg.de;tschannen@google.com;stefan.bauer@tuebingen.mpg.de;raetsch@inf.ethz.ch;bs@tuebingen.mpg.de;bachem@google.com,1;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,5,0,yes,9/25/19,"Max Planck Institute for Intelligent Systems, Max-Planck Institute;Google;Max Planck Institute for Intelligent Systems, Max-Planck Institute;Swiss Federal Institute of Technology;Max Planck Institute for Intelligent Systems, Max-Planck Institute;Google",-1;-1;-1;10;-1;-1,-1;-1;-1;13;-1;-1,,5/3/19,21,10,5,1,0,2,452;1229;1780;9598;77134;904,29;39;170;244;860;38,12;18;21;52;119;14,49;160;117;784;9933;101,m;m
3144,ICLR,2020,Uncertainty-guided Continual Learning with Bayesian Neural Networks,Sayna Ebrahimi;Mohamed Elhoseiny;Trevor Darrell;Marcus Rohrbach,sayna@berkeley.edu;mohamed.elhoseiny@gmail.com;trevor@eecs.berkeley.edu;maroffm@gmail.com,6;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0,yes,9/25/19,University of California Berkeley;KAUST;University of California Berkeley;Facebook,5;128;5;-1,13;1397;13;-1,11,6/6/19,16,6,5,0,0,0,174;1447;90979;11691,19;67;559;90,7;18;112;45,24;189;11527;1548,f;m
3145,ICLR,2020,Composing Task-Agnostic Policies with Deep Reinforcement Learning,Ahmed H. Qureshi;Jacob J. Johnson;Yuzhe Qin;Taylor Henderson;Byron Boots;Michael C. Yip,a1qureshi@ucsd.edu;jjj025@eng.ucsd.edu;y1qin@eng.ucsd.edu;tjwest@ucsd.edu;bboots@cs.washington.edu;yip@ucsd.edu,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,13,0,yes,9/25/19,"University of California, San Diego;University of California, San Diego;University of California, San Diego;University of California, San Diego;University of Washington;University of California, San Diego",11;11;11;11;6;11,31;31;31;31;26;31,6,9/25/19,4,2,2,0,0,0,297;208;30;15;2468;690,21;15;7;6;135;43,10;9;3;2;27;14,11;7;1;0;252;52,m;m
3146,ICLR,2020,Making Sense of Reinforcement Learning and Probabilistic Inference,Brendan O'Donoghue;Ian Osband;Catalin Ionescu,bodonoghue85@gmail.com;iosband@google.com;cdi@google.com,6;8;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,5,1,yes,9/25/19,;Google;Google,-1;-1;-1,-1;-1;-1,,9/25/19,2,2,0,0,0,0,2441;2492;1388,30;28;38,16;19;10,300;357;171,m;m
3147,ICLR,2020,The Logical Expressiveness of Graph Neural Networks,Pablo Barceló;Egor V. Kostylev;Mikael Monet;Jorge Pérez;Juan Reutter;Juan Pablo Silva,pbarcelo@gmail.com;egor.kostylev@cs.ox.ac.uk;mikael.monet@imfd.cl;jorge.perez.rojas@gmail.com;juan.reutter@gmail.com;jpsilvapena@gmail.com,8;8;8,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,4,0,yes,9/25/19,Universidad de Chile;University of Oxford;Millenium Instititute for Foundational Research on Data;Universidad de Chile;Pontificia Universidad Católica;Universidad de Chile,323;50;-1;323;481;323,848;1;-1;848;570;848,10,9/25/19,10,3,4,1,0,3,175;625;11;9;951;15,20;66;3;7;79;9,6;16;1;1;18;2,28;52;4;3;78;3,m;m
3148,ICLR,2020,Language GANs Falling Short,Massimo Caccia;Lucas Caccia;William Fedus;Hugo Larochelle;Joelle Pineau;Laurent Charlin,massimo.p.caccia@gmail.com;lucas.page-caccia@mail.mcgill.ca;liam.fedus@gmail.com;hugolarochelle@google.com;jpineau@cs.mcgill.ca;lcharlin@gmail.com,6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,10,1,yes,9/25/19,University of Montreal;McGill University;University of Montreal;Google;McGill University;University of Montreal,128;86;128;-1;86;128,85;42;85;-1;42;85,3;4;5,11/6/18,59,40,23,6,211,13,78;89;696;25332;11328;2500,6;8;25;124;267;41,3;3;10;44;46;20,17;19;91;2884;1235;337,m;m
3149,ICLR,2020,Directional Message Passing for Molecular Graphs,Johannes Klicpera;Janek Groß;Stephan Günnemann,klicpera@in.tum.de;grossja@in.tum.de;guennemann@in.tum.de,8;8;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,3,0,yes,9/25/19,Technical University Munich;Technical University Munich;Technical University Munich,53;53;53,43;43;43,10,9/25/19,12,9,3,0,0,4,135;9;2528,7;1;139,5;1;28,40;3;291,m;m
3150,ICLR,2020,A Target-Agnostic Attack on Deep Models: Exploiting Security Vulnerabilities of Transfer Learning,Shahbaz Rezaei;Xin Liu,srezaei@ucdavis.edu;xinliu@ucdavis.edu,6;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),0,5,0,yes,9/25/19,"University of California, Davis;University of California, Davis",79;79,55;55,4;6;2,4/8/19,6,3,0,0,0,0,67;9679,12;573,5;39,4;847,m;f
3151,ICLR,2020,At Stability's Edge: How to Adjust Hyperparameters to Preserve Minima Selection in Asynchronous Training of Neural Networks?,Niv Giladi;Mor Shpigel Nacson;Elad Hoffer;Daniel Soudry,giladiniv@gmail.com;mor.shpigel@gmail.com;elad.hoffer@gmail.com;daniel.soudry@gmail.com,6;8;8,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,3,0,yes,9/25/19,Technion;Technion;Technion;Technion,26;26;26;26,412;412;412;412,9;8,9/25/19,2,1,1,0,0,1,23;93;1493;5011,5;5;27;76,2;3;12;26,2;18;176;642,m;m
3152,ICLR,2020,Smoothness and Stability in GANs,Casey Chu;Kentaro Minami;Kenji Fukumizu,caseychu@stanford.edu;minami@preferred.jp;fukumizu@ism.ac.jp,8;6;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0,yes,9/25/19,"Stanford University;Preferred Networks, Inc.;The Institute of Statistical Mathematics, Japan",4;-1;-1,4;-1;-1,5;4,9/25/19,9,4,4,0,0,1,78;44;5832,9;6;195,3;3;35,10;3;762,m;m
3153,ICLR,2020,Infinite-Horizon Differentiable Model Predictive Control,Sebastian East;Marco Gallieri;Jonathan Masci;Jan Koutnik;Mark Cannon,sebastian.east@bath.edu;marco@nnaisense.com;jonathan@nnaisense.com;jan@nnaisense.com;mark.cannon@eng.ox.ac.uk,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,7,0,yes,9/25/19,University of Oxford;NNAISENSE;NNAISENSE;NNAISENSE;University of Oxford,50;-1;-1;-1;50,1;-1;-1;-1;1,,9/25/19,1,1,0,0,0,0,21;147;5373;3122;3503,9;17;47;50;208,3;6;22;14;31,0;9;474;297;246,m;m
3154,ICLR,2020,Observational Overfitting in Reinforcement Learning,Xingyou Song;Yiding Jiang;Stephen Tu;Yilun Du;Behnam Neyshabur,xsong@berkeley.edu;ydjiang@google.com;stephentu@google.com;yilundu@mit.edu;neyshabur@google.com,6;8;8,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0,yes,9/25/19,University of California Berkeley;Google;Google;Massachusetts Institute of Technology;Google,5;-1;-1;2;-1,13;-1;-1;5;-1,8,9/25/19,6,5,2,0,0,2,29;268;2128;128;2442,15;9;46;27;27,3;6;18;6;19,4;20;322;10;319,m;m
3155,ICLR,2020,Improved Sample Complexities for Deep Neural Networks and Robust Classification via an All-Layer Margin,Colin Wei;Tengyu Ma,colinwei@stanford.edu;tengyuma@cs.stanford.edu,3;8;8;6,I have published in this field for several years.:I did not assess the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,7,0,yes,9/25/19,Stanford University;Stanford University,4;4,4;4,4;1;8,9/25/19,14,4,1,0,0,0,313;3939,16;88,8;32,34;508,m;m
3156,ICLR,2020,SQIL: Imitation Learning via Reinforcement Learning with Sparse Rewards,Siddharth Reddy;Anca D. Dragan;Sergey Levine,sgr@berkeley.edu;anca@berkeley.edu;svlevine@eecs.berkeley.edu,6;6;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,13;13;13,5;4;1,5/27/19,3,3,1,0,0,1,88;3704;24893,14;127;310,4;31;74,8;286;3235,m;m
3157,ICLR,2020,Feature Interaction Interpretability: A Case for Explaining Ad-Recommendation Systems via Neural Interaction Detection,Michael Tsang;Dehua Cheng;Hanpeng Liu;Xue Feng;Eric Zhou;Yan Liu,tsangm@usc.edu;dehuacheng@fb.com;hanpengl@usc.edu;xfeng@fb.com;hanningz@fb.com;yanliu.cs@usc.edu,6;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,University of Southern California;Facebook;University of Southern California;Facebook;Facebook;University of Southern California,31;-1;31;-1;-1;31,62;-1;62;-1;-1;62,,9/25/19,1,1,1,0,0,0,2417;430;130;35;41;165,101;34;6;23;10;16,25;12;3;3;3;3,198;29;12;1;8;16,m;f
3158,ICLR,2020,Selection via Proxy: Efficient Data Selection for Deep Learning,Cody Coleman;Christopher Yeh;Stephen Mussmann;Baharan Mirzasoleiman;Peter Bailis;Percy Liang;Jure Leskovec;Matei Zaharia,cody@cs.stanford.edu;chrisyeh@stanford.edu;mussmann@stanford.edu;baharanm@stanford.edu;pbailis@cs.stanford.edu;pliang@cs.stanford.edu;jure@cs.stanford.edu;matei@cs.stanford.edu,6;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,8,0,yes,9/25/19,Stanford University;Stanford University;Stanford University;Stanford University;Stanford University;Stanford University;Stanford University;Stanford University,4;4;4;4;4;4;4;4,4;4;4;4;4;4;4;4,,6/26/19,4,2,1,0,0,0,554;124;118;945;2243;12798;48502;29007,16;9;15;28;121;145;302;136,9;4;7;12;24;48;93;39,57;13;6;145;231;2071;6080;3459,m;m
3159,ICLR,2020,Your classifier is secretly an energy based model and you should treat it like one,Will Grathwohl;Kuan-Chieh Wang;Joern-Henrik Jacobsen;David Duvenaud;Mohammad Norouzi;Kevin Swersky,wgrathwohl@cs.toronto.edu;wangkua1@cs.toronto.edu;j.jacobsen@vectorinstitute.ai;duvenaud@cs.toronto.edu;mnorouzi@google.com;kswersky@google.com,6;8;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,10,3,yes,9/25/19,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Vector Institute;Department of Computer Science, University of Toronto;Google;Google",18;18;-1;18;-1;-1,18;18;-1;18;-1;-1,5,9/25/19,29,12,14,2,0,6,391;180;207;6070;8136;5797,13;11;11;76;126;52,5;3;5;29;32;23,87;24;38;763;1021;885,m;m
3160,ICLR,2020,Padé Activation Units: End-to-end Learning of Flexible Activation Functions in Deep Networks,Alejandro Molina;Patrick Schramowski;Kristian Kersting,molina@cs.tu-darmstadt.de;schramowski@cs.tu-darmstadt.de;kersting@cs.tu-darmstadt.de,6;6;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),0,7,0,yes,9/25/19,TU Darmstadt;TU Darmstadt;TU Darmstadt,64;64;64,289;289;289,,7/15/19,4,3,0,0,0,1,225;17;5839,33;10;343,9;3;41,18;1;464,m;m
3161,ICLR,2020,Lipschitz constant estimation of Neural Networks via sparse polynomial optimization,Fabian Latorre;Paul Rolland;Volkan Cevher,fabian.latorre@epfl.ch;paul.rolland@epfl.ch;volkan.cevher@epfl.ch,8;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,5,0,yes,9/25/19,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne,481;481;481,38;38;38,1,9/25/19,10,7,3,1,0,3,0;115;14,3;44;10,0;5;1,0;9;3,m;m
3162,ICLR,2020,AMRL: Aggregated Memory For Reinforcement Learning,Jacob Beck;Kamil Ciosek;Sam Devlin;Sebastian Tschiatschek;Cheng Zhang;Katja Hofmann,jacob_beck@alumni.brown.edu;kamil.ciosek@microsoft.com;sam.devlin@microsoft.com;sebastian.tschiatschek@microsoft.com;cheng.zhang@microsoft.com;katja.hofmann@microsoft.com,6;8;6,I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0,yes,9/25/19,Brown University;Microsoft;Microsoft;Microsoft;Microsoft;Microsoft,67;-1;-1;-1;-1;-1,53;-1;-1;-1;-1;-1,3,9/25/19,2,0,0,0,0,0,1396;136;655;781;160;178,78;20;73;64;112;36,16;7;12;13;7;8,95;14;35;49;6;11,m;f
3163,ICLR,2020,Memory-Based Graph Networks,Amir Hosein Khasahmadi;Kaveh Hassani;Parsa Moradi;Leo Lee;Quaid Morris,amirhosein.khasahmadi@mail.utoronto.ca;kaveh.hassani@autodesk.com;parsa.moradi73@gmail.com;ljlee@psi.toronto.edu;quaid.morris@utoronto.ca,6;6;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,8,0,yes,9/25/19,Toronto University;Autodesk Inc;;University of Toronto;Toronto University,18;-1;-1;18;18,18;-1;-1;18;18,10,9/25/19,3,2,2,1,0,2,3;44;21;244;15130,2;12;5;69;197,1;3;2;6;50,2;7;4;12;1297,m;m
3164,ICLR,2020,One-Shot Pruning of Recurrent Neural Networks by Jacobian Spectrum Evaluation,Shunshi Zhang;Bradly C. Stadie,matthew.zhang@mail.utoronto.ca;bstadie@berkeley.edu,6;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,6,0,yes,9/25/19,Toronto University;University of California Berkeley,18;5,18;13,,9/25/19,2,0,0,0,0,0,41;778,8;17,2;7,0;50,m;m
3165,ICLR,2020,Network Randomization: A Simple Technique for Generalization in Deep Reinforcement Learning,Kimin Lee;Kibok Lee;Jinwoo Shin;Honglak Lee,kiminlee@kaist.ac.kr;kibok@umich.edu;jinwoos@kaist.ac.kr;honglak@eecs.umich.edu,6;3;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Poster),0,4,0,yes,9/25/19,Korea Advanced Institute of Science and Technology;University of Michigan;Korea Advanced Institute of Science and Technology;University of Michigan,481;8;481;8,110;21;110;21,8,9/25/19,8,6,2,0,0,1,507;749;1752;24514,27;35;186;166,8;12;19;62,108;112;225;2837,m;m
3166,ICLR,2020,On Mutual Information Maximization for Representation Learning,Michael Tschannen;Josip Djolonga;Paul K. Rubenstein;Sylvain Gelly;Mario Lucic,mi.tschannen@gmail.com;josip@djolonga.com;paruby@gmail.com;sylvaingelly@google.com;lucic@google.com,6;8;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,Google;Google;University of Cambridge;Google;Google,-1;-1;71;-1;-1,-1;-1;3;-1;-1,,7/31/19,45,18,13,4,0,4,1229;353;132;3609;1625,39;21;12;112;48,18;9;6;26;21,160;24;21;464;196,m;m
3167,ICLR,2020,Hypermodels for Exploration,Vikranth Dwaracherla;Xiuyuan Lu;Morteza Ibrahimi;Ian Osband;Zheng Wen;Benjamin Van Roy,vikranthd@google.com;lxlu@google.com;mibrahimi@google.com;iosband@google.com;zhengwen@google.com;benvanroy@google.com,6;8;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,7,0,yes,9/25/19,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,1,9/25/19,1,0,0,0,0,0,25;81;173;2492;2;7351,9;17;15;28;7;162,3;5;7;19;1;42,2;2;20;357;0;662,m;m
3168,ICLR,2020,On the Equivalence between Positional Node Embeddings and Structural Graph Representations,Balasubramaniam Srinivasan;Bruno Ribeiro,bsriniv@purdue.edu;ribeiro@cs.purdue.edu,8;8;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,8,2,yes,9/25/19,Purdue University;Purdue University,27;27,88;88,1;10,9/25/19,6,4,0,0,0,0,122;14,46;6,6;2,2;0,m;m
3169,ICLR,2020,GraphSAINT: Graph Sampling Based Inductive Learning Method,Hanqing Zeng;Hongkuan Zhou;Ajitesh Srivastava;Rajgopal Kannan;Viktor Prasanna,zengh@usc.edu;hongkuaz@usc.edu;ajiteshs@usc.edu;rajgopal.kannan.civ@mail.mil;prasanna@usc.edu,6;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),1,5,0,yes,9/25/19,University of Southern California;University of Southern California;University of Southern California;Army Reserach laboratory;University of Southern California,31;31;31;-1;31,62;62;62;-1;62,10,7/10/19,20,14,12,1,0,5,201;42;128;1694;100,29;6;41;175;19,8;4;6;19;3,15;6;12;88;17,m;m
3170,ICLR,2020,Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow ReLU networks,Ziwei Ji;Matus Telgarsky,ziweiji2@illinois.edu;mjt@illinois.edu,6;8;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:N/A:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:N/A:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,13,0,yes,9/25/19,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",3;3,48;48,1,9/25/19,21,9,7,4,0,4,169;2045,16;44,5;15,28;346,m;m
3171,ICLR,2020,Cross-Lingual Ability of Multilingual BERT: An Empirical Study,Karthikeyan K;Zihan Wang;Stephen Mayhew;Dan Roth,kkarthi@seas.upenn.edu;zihanw2@illinois.edu;mayhew@seas.upenn.edu;danroth@seas.upenn.edu,3;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,5,0,yes,9/25/19,"University of Pennsylvania;University of Illinois, Urbana Champaign;University of Pennsylvania;University of Pennsylvania",19;3;19;19,11;48;11;11,3,9/25/19,26,18,3,2,0,2,80;47;207;101,39;23;26;13,4;3;7;3,3;3;30;5,m;m
3172,ICLR,2020,Model Based Reinforcement Learning for Atari,Łukasz Kaiser;Mohammad Babaeizadeh;Piotr Miłos;Błażej Osiński;Roy H Campbell;Konrad Czechowski;Dumitru Erhan;Chelsea Finn;Piotr Kozakowski;Sergey Levine;Afroz Mohiuddin;Ryan Sepassi;George Tucker;Henryk Michalewski,lukaszkaiser@google.com;mbz@google.com;pmilos@mimuw.edu.pl;blazej.osinski@gmail.com;rhc@illinois.edu;konrad.czechowski@gmail.com;dumitru@google.com;chelseaf@google.com;kozak000@gmail.com;slevine@google.com;afrozm@google.com;rsepassi@google.com;gjt@google.com;henrykmichalewski@gmail.com,6;8;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Spotlight),0,4,0,yes,9/25/19,"Google;Google;University of Washington, Seattle;University of Washington, Seattle;University of Illinois, Urbana Champaign;University of Washington, Seattle;Google;Google;University of Washington, Seattle;Google;Google;Google;Google;",-1;-1;6;6;3;6;-1;-1;6;-1;-1;-1;-1;-1,-1;-1;26;26;48;26;-1;-1;26;-1;-1;-1;-1;-1,,3/1/19,114,44,32,3,0,8,22913;628;437;136;12297;137;43043;7879;109;24893;-1;563;2705;368,75;19;46;5;560;15;60;101;4;310;-1;6;75;53,23;8;10;3;54;4;31;34;1;74;-1;4;21;9,3899;94;37;11;961;11;6039;1060;7;3235;0;70;301;26,m;m
3173,ICLR,2020,InfoGraph: Unsupervised and Semi-supervised Graph-Level Representation Learning via Mutual Information Maximization,Fan-Yun Sun;Jordan Hoffman;Vikas Verma;Jian Tang,sunfanyun@gmail.com;jhoffmann@g.harvard.edu;vikasverma.iitm@gmail.com;jian.tang@hec.ca,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,5,0,yes,9/25/19,National Taiwan University;Harvard University;;HEC Montreal,86;39;-1;128,120;7;-1;85,3;10;8,7/31/19,17,7,8,1,0,7,31;103;-1;5178,8;23;-1;154,3;7;-1;34,9;8;0;450,m;m
3174,ICLR,2020,Black-Box Adversarial Attack with Transferable Model-based Embedding,Zhichao Huang;Tong Zhang,zhuangbx@connect.ust.hk;tongzhang@tongzhang-ml.org,6;8;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,The Hong Kong University of Science and Technology;The Hong Kong University of Science and Technology,39;39,47;47,4,9/25/19,3,0,2,0,0,0,63;164,18;78,5;5,6;14,m;m
3175,ICLR,2020,Inductive Matrix Completion Based on Graph Neural Networks,Muhan Zhang;Yixin Chen,muhan@wustl.edu;chen@cse.wustl.edu,6;8;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Spotlight),0,5,0,yes,9/25/19,"Washington University, St. Louis;Washington University, St. Louis",100;100,52;52,6;10,4/26/19,7,4,1,0,2,1,22;681,6;54,3;7,0;90,m;m
3176,ICLR,2020,Probability Calibration for Knowledge Graph Embedding Models,Pedro Tabacof;Luca Costabello,tabacof@gmail.com;luca.costabello@accenture.com,6;8;6;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,5,0,yes,9/25/19,;Accenture,-1;-1,-1;-1,10,9/25/19,5,4,1,0,0,0,169;213,8;35,5;8,13;21,m;m
3177,ICLR,2020,Intensity-Free Learning of Temporal Point Processes,Oleksandr Shchur;Marin Biloš;Stephan Günnemann,shchur@in.tum.de;bilos@in.tum.de;guennemann@in.tum.de,8;6;8,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,5,4,yes,9/25/19,Technical University Munich;Technical University Munich;Technical University Munich,53;53;53,43;43;43,,9/25/19,3,1,1,0,0,0,262;5;2528,12;3;139,4;2;28,38;0;291,m;m
3178,ICLR,2020,Certified Robustness for Top-k Predictions against Adversarial Perturbations via Randomized Smoothing,Jinyuan Jia;Xiaoyu Cao;Binghui Wang;Neil Zhenqiang Gong,jinyuan.jia@duke.edu;xiaoyu.cao@duke.edu;binghui.wang@duke.edu;neil.gong@duke.edu,6;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0,yes,9/25/19,Duke University;Duke University;Duke University;Duke University,47;47;47;47,20;20;20;20,4;1,9/25/19,10,3,6,0,0,1,25;164;397;1640,8;25;35;72,3;6;10;22,3;16;19;146,m;m
3179,ICLR,2020,Simple and Effective Regularization Methods for Training on Noisily Labeled Data with Generalization Guarantee,Wei Hu;Zhiyuan Li;Dingli Yu,huwei@cs.princeton.edu;zhiyuanli@cs.princeton.edu;dingliy@cs.princeton.edu,6;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,Princeton University;Princeton University;Princeton University,31;31;31,6;6;6,1;8,5/27/19,13,7,1,1,0,1,18;675;11,15;13;8,2;10;1,1;116;0,m;m
3180,ICLR,2020,"Understanding l4-based Dictionary Learning: Interpretation, Stability, and Robustness",Yuexiang Zhai;Hermish Mehta;Zhengyuan Zhou;Yi Ma,ysz@berkeley.edu;hermish@berkeley.edu;zyzhou@stanford.edu;yima@eecs.berkeley.edu,8;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,2,0,yes,9/25/19,University of California Berkeley;University of California Berkeley;Stanford University;University of California Berkeley,5;5;4;5,13;13;4;13,,9/25/19,3,3,3,0,0,3,27;9;963;34958,6;4;85;549,3;2;16;58,7;3;116;5265,m;m
3181,ICLR,2020,RGBD-GAN: Unsupervised 3D Representation Learning From Natural Image Datasets via RGBD Image Synthesis,Atsuhiro Noguchi;Tatsuya Harada,noguchi@mi.t.u-tokyo.ac.jp;harada@mi.t.u-tokyo.ac.jp,6;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,The University of Tokyo;The University of Tokyo,56;56,36;36,5,9/25/19,4,4,0,0,0,0,17;2594,3;213,2;26,7;327,u;m
3182,ICLR,2020,Estimating counterfactual treatment outcomes over time through adversarially balanced representations,Ioana Bica;Ahmed M Alaa;James Jordon;Mihaela van der Schaar,ioana.bica@eng.ox.ac.uk;a7med3laa@hotmail.com;james.jordon@wolfson.ox.ac.uk;mschaar@turing.ac.uk,6;6;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Spotlight),0,5,0,yes,9/25/19,University of Oxford;;University of Oxford;Alan Turing Institute,50;-1;50;-1,1;-1;1;-1,4,9/25/19,4,2,0,0,0,0,12;409;844;8828,7;72;105;642,3;12;16;42,1;20;33;547,f;f
3183,ICLR,2020,Exploring Model-based Planning with Policy Networks,Tingwu Wang;Jimmy Ba,tingwuwang@cs.toronto.edu;jba@cs.toronto.edu,8;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",18;18,18;18,,6/20/19,21,11,9,0,0,5,190;52924,8;56,5;22,17;8625,m;m
3184,ICLR,2020,LambdaNet: Probabilistic Type Inference using Graph Neural Networks,Jiayi Wei;Maruth Goyal;Greg Durrett;Isil Dillig,jiayi@cs.utexas.edu;maruth@utexas.edu;gdurrett@cs.utexas.edu;isil@cs.utexas.edu,6;8;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Poster),0,8,0,yes,9/25/19,"University of Texas, Austin;University of Texas, Austin;University of Texas, Austin;University of Texas, Austin",22;22;22;22,38;38;38;38,10,9/25/19,3,0,2,0,0,0,40;5;1253;2197,9;5;48;99,3;2;16;26,2;0;163;204,m;f
3185,ICLR,2020,Precision Gating: Improving Neural Network Efficiency with Dynamic Dual-Precision Activations,Yichi Zhang;Ritchie Zhao;Weizhe Hua;Nayun Xu;G. Edward Suh;Zhiru Zhang,yz2499@cornell.edu;rz252@cornell.edu;wh399@cornell.edu;nx38@cornell.edu;edward.suh@cornell.edu;zhiruz@cornell.edu,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A,Accept (Poster),0,6,0,yes,9/25/19,Cornell University;Cornell University;Cornell University;Cornell University;Cornell University;Cornell University,7;7;7;7;7;7,19;19;19;19;19;19,,9/25/19,0,0,0,0,0,0,35;487;98;15;6272;2158,11;21;13;4;114;105,2;10;6;1;32;22,3;54;12;0;819;203,m;m
3186,ICLR,2020,Variational Template Machine for Data-to-Text Generation,Rong Ye;Wenxian Shi;Hao Zhou;Zhongyu Wei;Lei Li,rye18@fudan.edu.cn;shiwenxian@bytedance.com;zhouhao.nlp@bytedance.com;zywei@fudan.edu.cn;lileilab@bytedance.com,8;3;8,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Accept (Poster),0,4,0,yes,9/25/19,Fudan University;Bytedance;Bytedance;Fudan University;Bytedance,79;-1;-1;79;-1,109;-1;-1;109;-1,,9/25/19,1,0,0,0,0,0,112;24;174;783;346,24;7;73;80;78,6;3;8;14;8,6;1;26;83;47,u;m
3187,ICLR,2020,On Universal Equivariant Set Networks,Nimrod Segol;Yaron Lipman,nimrod.segol@weizmann.ac.il;yaron.lipman@weizmann.ac.il,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0,yes,9/25/19,Weizmann Institute;Weizmann Institute,108;108,1397;1397,1;2,9/25/19,8,4,1,0,0,1,59;5288,4;99,3;41,12;531,m;m
3188,ICLR,2020,The Gambler's Problem and Beyond,Baoxiang Wang;Shuai Li;Jiajin Li;Siu On Chan,bxwang@cse.cuhk.edu.hk;shuaili8@sjtu.edu.cn;jjli@se.cuhk.edu.hk;siuon@cse.cuhk.edu.hk,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,9,0,yes,9/25/19,The Chinese University of Hong Kong;Shanghai Jiao Tong University;The Chinese University of Hong Kong;The Chinese University of Hong Kong,59;53;59;59,35;157;35;35,,9/25/19,0,0,0,0,0,0,54;654;36;378,14;158;13;22,3;14;2;8,3;34;2;40,m;m
3189,ICLR,2020,Learning to Represent Programs with Property Signatures,Augustus Odena;Charles Sutton,augustusodena@google.com;csutton@inf.ed.ac.uk,6;6;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,8,0,yes,9/25/19,Google;University of Edinburgh,-1;33,-1;30,,9/25/19,2,0,0,0,0,0,3293;50,25;18,13;3,477;4,m;m
3190,ICLR,2020,Learning to Balance: Bayesian Meta-Learning for Imbalanced and Out-of-distribution Tasks,Hae Beom Lee;Hayeon Lee;Donghyun Na;Saehoon Kim;Minseop Park;Eunho Yang;Sung Ju Hwang,haebeom.lee@kaist.ac.kr;hayeon926@kaist.ac.kr;donghyun.na@kaist.ac.kr;shkim@aitrics.com;mike_seop@aitrics.com;eunhoy@kaist.ac.kr;sjhwang82@kaist.ac.kr,8;8;8,I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Accept (Talk),0,4,0,yes,9/25/19,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;AITRICS;AITRICS;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,481;481;481;-1;-1;481;481,110;110;110;-1;-1;110;110,11;6,5/30/19,6,5,0,0,0,0,96;-1;165;424;123;1067;1128,23;-1;10;35;7;75;71,5;-1;4;10;4;16;16,4;0;6;68;28;169;125,m;m
3191,ICLR,2020,Smooth markets: A basic mechanism for organizing gradient-based learners,David Balduzzi;Wojciech M. Czarnecki;Tom Anthony;Ian Gemp;Edward Hughes;Joel Leibo;Georgios Piliouras;Thore Graepel,dbalduzzi@google.com;lejlot@google.com;edwardhughes@google.com;jzl@google.com;imgemp@google.com;twa@google.com;georgios.piliouras@gmail.com;thore@google.com,8;8,I have read many papers in this area.:N/A:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:N/A:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,2,0,yes,9/25/19,Google;Google;Google;Google;Google;Google;Singapore University of Technology and Design;Google,-1;-1;-1;-1;-1;-1;481;-1,-1;-1;-1;-1;-1;-1;1397;-1,5;4,9/25/19,4,3,0,0,0,1,2521;3;154;15;152;3547;1257;19274,61;1;18;4;20;75;99;161,21;1;6;2;7;29;19;45,359;1;9;1;13;357;84;1406,m;m
3192,ICLR,2020,Thinking While Moving: Deep Reinforcement Learning with Concurrent Control,Ted Xiao;Eric Jang;Dmitry Kalashnikov;Sergey Levine;Julian Ibarz;Karol Hausman;Alexander Herzog,tedxiao@google.com;ejang@google.com;dkalashnikov@google.com;slevine@google.com;julianibarz@google.com;karolhausman@google.com;alexherzog@google.com,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Accept (Poster),0,5,0,yes,9/25/19,Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,,9/25/19,0,0,0,0,0,0,83;2181;336;959;2860;872;0,15;138;67;34;22;62;3,3;26;11;9;13;15;0,2;140;4;103;349;53;0,m;m
3193,ICLR,2020,Demystifying Inter-Class Disentanglement,Aviv Gabbay;Yedid Hoshen,avivga@gmail.com;yedid@cs.huji.ac.il,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,7,0,yes,9/25/19,Hebrew University of Jerusalem;Hebrew University of Jerusalem,67;67,216;216,4,6/27/19,7,4,3,2,0,2,96;503,8;33,5;10,10;59,m;m
3194,ICLR,2020,Meta-Learning Deep Energy-Based Memory Models,Sergey Bartunov;Jack Rae;Simon Osindero;Timothy Lillicrap,bartunov@google.com;jwrae@google.com;osindero@google.com;countzero@google.com,6;6;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,8,0,yes,9/25/19,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,6,9/25/19,5,3,1,0,0,0,1148;631;14818;24158,13;19;38;74,8;10;21;39,125;74;1866;2943,m;m
3195,ICLR,2020,Self-Supervised Learning of Appliance Usage,Chen-Yu Hsu;Abbas Zeitoun;Guang-He Lee;Dina Katabi;Tommi Jaakkola,cyhsu@mit.edu;zeitoun@mit.edu;guanghe@csail.mit.edu;dina@csail.mit.edu;tommi@csail.mit.edu,6;3;8,I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,6,0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2;2,5;5;5;5;5,,9/25/19,1,1,0,0,0,0,444;1;103;19583;22299,31;1;18;254;293,9;1;6;67;69,34;0;15;2607;2326,m;m
3196,ICLR,2020,Higher-Order Function Networks for Learning Composable 3D Object Representations,Eric Mitchell;Selim Engin;Volkan Isler;Daniel D Lee,eric.anthony.mitchell95@gmail.com;engin003@umn.edu;isler@umn.edu;ddlee@seas.upenn.edu,6;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,"Stanford University;University of Minnesota, Minneapolis;University of Minnesota, Minneapolis;University of Pennsylvania",4;59;59;19,4;79;79;11,,7/24/19,4,2,2,0,0,0,41;16;3391;9481,25;3;188;183,3;2;29;31,3;1;167;1468,m;m
3197,ICLR,2020,Learning to solve the credit assignment problem,Benjamin James Lansdell;Prashanth Ravi Prakash;Konrad Paul Kording,ben.lansdell@gmail.com;prprak@seas.upenn.edu;koerding@gmail.com,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,6,0,yes,9/25/19,University of Pennsylvania;University of Pennsylvania;University of Pennsylvania,19;19;19,11;11;11,1;9,6/3/19,5,3,3,1,0,1,88;160;1151,13;13;35,5;5;12,5;13;80,m;m
3198,ICLR,2020,Prediction Poisoning: Towards Defenses Against DNN Model Stealing Attacks,Tribhuvanesh Orekondy;Bernt Schiele;Mario Fritz,orekondy@mpi-inf.mpg.de;schiele@mpi-inf.mpg.de;fritz@cispa.saarland,8;3;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,1,yes,9/25/19,"Saarland Informatics Campus, Max-Planck Institute;Saarland Informatics Campus, Max-Planck Institute;CISPA Helmholtz Center for Information Security",-1;-1;143,-1;-1;1397,4,9/25/19,4,2,1,0,0,1,115;41307;7800,13;503;198,4;99;46,23;5119;1012,m;m
3199,ICLR,2020,Difference-Seeking Generative Adversarial Network--Unseen Sample Generation,Yi Lin Sung;Sung-Hsien Hsieh;Soo-Chang Pei;Chun-Shien Lu,r06942076@ntu.edu.tw;parvaty316@hotmail.com;peisc@ntu.edu.tw;lcs@iis.sinica.edu.tw,6;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0,yes,9/25/19,National Taiwan University;;National Taiwan University;Academia Sinica,86;-1;86;-1,120;-1;120;-1,5;4,9/25/19,0,0,0,0,0,0,0;177;77;3221,1;31;22;171,0;5;5;31,0;26;4;219,m;m
3200,ICLR,2020,Reducing Transformer Depth on Demand with Structured Dropout,Angela Fan;Edouard Grave;Armand Joulin,angelafan@fb.com;egrave@fb.com;ajoulin@fb.com,8;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),1,10,0,yes,9/25/19,Facebook;Facebook;Facebook,-1;-1;-1,-1;-1;-1,3,9/25/19,30,12,17,3,0,6,1838;7711;10653,26;57;74,14;23;32,268;1136;1533,f;m
3201,ICLR,2020,Actor-Critic Provably Finds Nash Equilibria of Linear-Quadratic Mean-Field Games,Zuyue Fu;Zhuoran Yang;Yongxin Chen;Zhaoran Wang,zuyuefu2022@u.northwestern.edu;zy6@princeton.edu;yongchen@gatech.edu;zhaoranwang@gmail.com,8;6;6,I have read many papers in this area.:N/A:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:N/A:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,16,0,yes,9/25/19,Northwestern University;Princeton University;Georgia Institute of Technology;Northwestern University,44;31;13;44,22;6;38;22,1;9,9/25/19,6,3,1,0,0,2,11;182;892;1193,6;41;107;78,2;7;15;20,3;22;39;132,m;m
3202,ICLR,2020,Quantifying the Cost of Reliable Photo Authentication via High-Performance Learned Lossy Representations,Pawel Korus;Nasir Memon,pkorus@nyu.edu;memon@nyu.edu,6;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,New York University;New York University,25;25,29;29,,9/25/19,0,0,0,0,0,0,344;13325,30;390,9;59,32;1155,m;m
3203,ICLR,2020,Multi-agent Reinforcement Learning for Networked System Control,Tianshu Chu;Sandeep Chinchali;Sachin Katti,cts198859@hotmail.com;csandeep@stanford.edu;skatti@stanford.edu,6;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0,yes,9/25/19,VMware Inc;Stanford University;Stanford University,-1;4;4,-1;4;4,,9/25/19,1,1,0,0,0,0,372;199;4,54;10;8,9;7;1,36;17;0,m;m
3204,ICLR,2020,Learning Space Partitions for Nearest Neighbor Search,Yihe Dong;Piotr Indyk;Ilya Razenshteyn;Tal Wagner,yihedong@gmail.com;indyk@mit.edu;ilyaraz@microsoft.com;tal.wagner@gmail.com,6;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,4,0,yes,9/25/19,Microsoft;Massachusetts Institute of Technology;Microsoft;Massachusetts Institute of Technology,-1;2;-1;2,-1;5;-1;5,10,1/24/19,2,0,1,0,0,0,42;24320;1345;117,13;286;53;22,3;66;18;6,2;3234;126;18,m;m
3205,ICLR,2020,On Computation and Generalization of Generative Adversarial Imitation Learning,Minshuo Chen;Yizhou Wang;Tianyi Liu;Zhuoran Yang;Xingguo Li;Zhaoran Wang;Tuo Zhao,mchen393@gatech.edu;wyzjack990122@gmail.com;tianyiliu@gatech.edu;zy6@princeton.edu;xingguol@princeton.edu;zhaoran.wang@northwestern.edu;tourzhao@gatech.edu,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Accept (Poster),0,4,0,yes,9/25/19,Georgia Institute of Technology;Xi'an Jiaotong University;Georgia Institute of Technology;Princeton University;Princeton University;Northwestern University;Georgia Institute of Technology,13;481;13;31;31;44;13,38;555;38;6;6;22;38,5;4;8,9/25/19,2,1,1,0,0,1,58;108;35;182;606;-1;2458,21;26;10;41;77;-1;109,4;6;2;7;12;-1;19,6;16;3;22;65;0;205,m;m
3206,ICLR,2020,Evaluating The Search Phase of Neural Architecture Search,Kaicheng Yu;Christian Sciuto;Martin Jaggi;Claudiu Musat;Mathieu Salzmann,kaicheng.yu@epfl.ch;sciutochristian@gmail.com;martin.jaggi@epfl.ch;claudiu.musat@swisscom.com;mathieu.salzmann@epfl.ch,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0,yes,9/25/19,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swisscom;Swiss Federal Institute of Technology Lausanne,481;481;481;-1;481,38;38;38;-1;38,,2/21/19,95,42,26,7,12,8,144;96;3856;214;5661,9;2;114;39;198,5;1;27;6;42,13;8;577;30;623,m;m
3207,ICLR,2020,Critical initialisation in continuous approximations of binary neural networks,George Stamatescu;Federica Gerace;Carlo Lucibello;Ian Fuss;Langford White,george.stamatescu@gmail.com;federicagerace91@gmail.com;carlo.lucibello@gmail.com;ian.fuss@adelaide.edu.au;lang.white@adelaide.edu.au,6;3;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,4,0,yes,9/25/19,The University of Adelaide;;Bocconi University;The University of Adelaide;The University of Adelaide,128;-1;323;128;128,120;-1;1397;120;120,,2/1/19,0,0,0,0,0,0,19;51;230;179;864,6;13;15;18;117,2;4;6;5;14,1;3;14;19;59,m;m
3208,ICLR,2020,Evolutionary Population Curriculum for Scaling Multi-Agent Reinforcement Learning,Qian Long*;Zihan Zhou*;Abhinav Gupta;Fei Fang;Yi Wu†;Xiaolong Wang†,qianlong@cs.cmu.edu;footoredo@sjtu.edu.cn;abhinavg@cs.cmu.edu;feif@cs.cmu.edu;jxwuyi@gmail.com;dragonwxl123@gmail.com,6;6;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Accept (Poster),0,4,0,yes,9/25/19,Carnegie Mellon University;Shanghai Jiao Tong University;Carnegie Mellon University;Carnegie Mellon University;OpenAI;University of California Berkeley,1;53;1;1;-1;5,27;157;27;27;-1;13,,9/25/19,1,1,1,0,0,0,11;2095;-1;123;2485;1176,33;71;-1;29;69;120,2;21;-1;5;4;18,0;201;0;2;736;92,f;m
3209,ICLR,2020,Probabilistic Connection Importance Inference and Lossless Compression of Deep Neural Networks,Xin Xing;Long Sha;Pengyu Hong;Zuofeng Shang;Jun S. Liu,xin_xing@fas.harvard.edu;longsha@brandeis.edu;hongpeng@brandeis.edu;zuofeng.shang@njit.edu;jliu@stat.harvard.edu,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,10,0,yes,9/25/19,Harvard University;Brandeis University;Brandeis University;New Jersey Institute of Technology;Harvard University,39;323;323;172;39,7;244;244;564;7,,9/25/19,1,0,1,0,0,0,28;318;1805;212;25180,9;32;92;44;339,2;10;22;9;69,5;18;97;22;2510,m;m
3210,ICLR,2020,Picking Winning Tickets Before Training by Preserving Gradient Flow,Chaoqi Wang;Guodong Zhang;Roger Grosse,cqwang@cs.toronto.edu;gdzhang@cs.toronto.edu;rgrosse@cs.toronto.edu,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,12,0,yes,9/25/19,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",18;18;18,18;18;18,8,9/25/19,18,10,11,1,0,8,12;1335;5791,2;16;51,1;11;28,6;225;815,m;m
3211,ICLR,2020,CAQL: Continuous Action Q-Learning,Moonkyung Ryu;Yinlam Chow;Ross Anderson;Christian Tjandraatmadja;Craig Boutilier,mkryu@google.com;yinlamchow@google.com;rander@google.com;ctjandra@google.com;cboutilier@google.com,6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:N/A:N/A,Accept (Poster),1,2,0,yes,9/25/19,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,,9/25/19,4,1,2,0,0,0,59;763;352;123;14037,8;50;24;17;276,4;15;5;5;55,8;92;41;20;1714,m;m
3212,ICLR,2020,Stable Rank Normalization for Improved Generalization in Neural Networks and GANs,Amartya Sanyal;Philip H. Torr;Puneet K. Dokania,amartya.sanyal@cs.ox.ac.uk;philip.torr@eng.ox.ac.uk;puneet@robots.ox.ac.uk,6;8;8,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,9,1,yes,9/25/19,University of Oxford;University of Oxford;University of Oxford,50;50;50,1;1;1,5;1;8,6/11/19,3,1,1,0,0,0,59;28788;45,14;356;9,3;84;4,4;3878;6,m;m
3213,ICLR,2020,Dynamic Model Pruning with Feedback,Tao Lin;Sebastian U. Stich;Luis Barba;Daniil Dmitriev;Martin Jaggi,tao.lin@epfl.ch;sebastian.stich@epfl.ch;luis.barba@inf.ethz.ch;daniil.dmitriev@epfl.ch;martin.jaggi@epfl.ch,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),2,5,1,yes,9/25/19,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne,481;481;10;481;481,38;38;13;38;38,,9/25/19,7,2,3,0,0,1,163;984;734;8;3856,7;48;116;10;114,6;16;14;1;27,17;142;32;1;577,m;m
3214,ICLR,2020,Emergent Tool Use From Multi-Agent Autocurricula,Bowen Baker;Ingmar Kanitscheider;Todor Markov;Yi Wu;Glenn Powell;Bob McGrew;Igor Mordatch,bowen@openai.com;ingmar@openai.com;todor@openai.com;jxwuyi@openai.com;glenn@openai.com;bmcgrew@openai.com;imordatch@google.com,6;8;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:N/A:N/A:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:N/A:N/A,Accept (Spotlight),1,5,0,yes,9/25/19,OpenAI;OpenAI;OpenAI;OpenAI;OpenAI;OpenAI;Google,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,,9/17/19,49,27,5,1,0,8,1263;1076;122;773;581;1247;3078,13;19;7;169;9;7;49,7;10;2;14;4;7;27,149;106;19;32;57;180;351,m;m
3215,ICLR,2020,Conditional Learning of Fair Representations,Han Zhao;Amanda Coston;Tameem Adel;Geoffrey J. Gordon,han.zhao@cs.cmu.edu;acoston@cs.cmu.edu;tah47@cam.ac.uk;ggordon@cs.cmu.edu,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,6,0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;University of Cambridge;Carnegie Mellon University,1;1;71;1,27;27;3;27,7,9/25/19,4,3,0,0,0,0,853;22;443;10736,91;12;25;186,11;4;9;48,34;0;51;1228,m;m
3216,ICLR,2020,You CAN Teach an Old Dog New Tricks! On Training Knowledge Graph Embeddings,Daniel Ruffinelli;Samuel Broscheit;Rainer Gemulla,daniel@informatik.uni-mannheim.de;broscheit@informatik.uni-mannheim.de;rgemulla@uni-mannheim.de,6;6;8,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),6,4,0,yes,9/25/19,University of Mannheim;University of Mannheim;University of Mannheim,233;233;233,157;157;157,10,9/25/19,9,5,1,1,0,0,79;121;2531,11;16;69,5;7;22,13;19;367,m;m
3217,ICLR,2020,Disagreement-Regularized Imitation Learning,Kiante Brantley;Wen Sun;Mikael Henaff,kdbrant@cs.umd.edu;wen.sun@microsoft.com;mihenaff@microsoft.com,8;8;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),1,4,1,yes,9/25/19,"University of Maryland, College Park;Microsoft;Microsoft",12;-1;-1,91;-1;-1,5;4;1,9/25/19,4,2,2,0,0,0,58;934;2407,9;144;23,4;17;14,4;33;204,m;u
3218,ICLR,2020,Physics-as-Inverse-Graphics: Unsupervised Physical Parameter Estimation from Video,Miguel Jaques;Michael Burke;Timothy Hospedales,m.a.m.jaques@sms.ed.ac.uk;michael.burke@ed.ac.uk;t.hospedales@ed.ac.uk,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,9,0,yes,9/25/19,University of Edinburgh;University of Edinburgh;University of Edinburgh,33;33;33,30;30;30,,5/27/19,2,2,2,0,0,1,14;731;384,4;61;31,2;15;9,1;54;34,m;m
3219,ICLR,2020,Model-Augmented Actor-Critic: Backpropagating through Paths,Ignasi Clavera;Yao Fu;Pieter Abbeel,iclavera@berkeley.edu;violetfuyao@berkeley.edu;pabbeel@cs.berkeley.edu,3;8;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,13;13;13,,9/25/19,1,1,0,0,0,0,437;157;37294,15;49;438,8;8;94,57;9;4481,m;m
3220,ICLR,2020,Disentanglement by Nonlinear ICA with General Incompressible-flow Networks (GIN),Peter Sorrenson;Carsten Rother;Ullrich Köthe,peter.sorrenson@gmail.com;carsten.rother@iwr.uni-heidelberg.de;ullrich.koethe@iwr.uni-heidelberg.de,6;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,5,0,yes,9/25/19,Heidelberg University;Heidelberg University;Heidelberg University,205;205;205,44;44;44,5;1,9/25/19,5,2,1,3,0,0,4;360;3025,4;34;111,1;6;24,0;38;244,m;m
3221,ICLR,2020,Generative Ratio Matching Networks,Akash Srivastava;Kai Xu;Michael U. Gutmann;Charles Sutton,akash.srivastava@me.com;kai.xu@ed.ac.uk;michael.gutmann@ed.ac.uk;charlessutton@google.com,6;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,10,0,yes,9/25/19,International Business Machines;University of Edinburgh;University of Edinburgh;Google,-1;33;33;-1,-1;30;30;-1,5;4,5/31/18,41,0,0,0,41,0,490;600;765;50,49;69;45;18,7;11;14;3,99;35;100;4,m;m
3222,ICLR,2020,CM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement Learning,Jiachen Yang;Alireza Nakhaei;David Isele;Kikuo Fujimura;Hongyuan Zha,yjiachen@gmail.com;anakhaei@honda-ri.com;disele@honda-ri.com;kfujimura@honda-ri.com;zha@cc.gatech.edu,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Accept (Poster),0,4,0,yes,9/25/19,Georgia Institute of Technology;Honda Research Institute;Honda Research Institute;Honda Research Institute;Georgia Institute of Technology,13;-1;-1;-1;13,38;-1;-1;-1;38,,9/13/18,12,7,1,2,17,0,1578;363;262;3894;14480,149;43;24;161;408,20;10;7;28;62,46;19;13;227;1225,m;m
3223,ICLR,2020,Order Learning and Its Application to Age Estimation,Kyungsun Lim;Nyeong-Ho Shin;Young-Yoon Lee;Chang-Su Kim,kslim@mcl.korea.ac.kr;nhshin@mcl.korea.ac.kr;yy77lee@gmail.com;changsukim@korea.ac.kr,6;8;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,Korea University;Korea University;Samsung;Korea University,323;323;-1;323,179;179;-1;179,7;2;10,9/25/19,1,1,0,0,0,0,31;0;184;74,4;2;22;18,2;0;8;4,5;0;18;4,u;m
3224,ICLR,2020,Learning to Link,Maria-Florina Balcan;Travis Dick;Manuel Lang,ninamf@cs.cmu.edu;tdick@ttic.edu;manuel.lang@student.kit.edu,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,Carnegie Mellon University;Toyota Technological Institute at Chicago;Karlsruhe Institute of Technology,1;-1;154,27;-1;174,,7/1/19,4,1,2,0,0,1,5098;17;428,178;12;11,40;3;7,480;2;26,f;m
3225,ICLR,2020,DiffTaichi: Differentiable Programming for Physical Simulation,Yuanming Hu;Luke Anderson;Tzu-Mao Li;Qi Sun;Nathan Carr;Jonathan Ragan-Kelley;Fredo Durand,yuanmhu@gmail.com;lukea@mit.edu;tzumao@berkeley.edu;qisu@adobe.com;ncarr@adobe.com;jrk@berkeley.edu;fredo@mit.edu,6;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;University of California Berkeley;Adobe Systems;Adobe Systems;University of California Berkeley;Massachusetts Institute of Technology,2;2;5;-1;-1;5;2,5;5;13;-1;-1;13;5,,9/25/19,15,11,5,0,0,1,624;195;287;1684;564;995;22425,30;26;15;133;39;36;361,11;7;8;14;12;17;76,77;14;31;54;46;99;2451,m;m
3226,ICLR,2020,Adaptive Structural Fingerprints for Graph Attention Networks,Kai Zhang;Yaokang Zhu;Jun Wang;Jie Zhang,kzhang980@gmail.com;52184501026@stu.ecnu.edu.cn;wongjun@gmail.com;jzhang080@gmail.com,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,7,0,yes,9/25/19,;Australian National University;;,-1;108;-1;-1,-1;50;-1;-1,10,9/25/19,3,2,1,0,0,0,833;3;189;346,112;2;122;181,13;1;8;9,55;0;5;13,m;m
3227,ICLR,2020,Kernelized Wasserstein Natural Gradient,M Arbel;A Gretton;W Li;G Montufar,michael.n.arbel@gmail.com;arthur.gretton@gmail.com;wcli@math.ucla.edu;guidomontufar@gmail.com,8;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,6,0,yes,9/25/19,"University College London;;University of California, Los Angeles;Max Planck Institute MIS",50;-1;20;-1,15;-1;17;-1,,9/25/19,2,0,1,0,0,0,275;13138;443;1193,11;216;57;60,5;48;12;14,64;2236;16;77,m;m
3228,ICLR,2020,DeepV2D: Video to Depth with Differentiable Structure from Motion,Zachary Teed;Jia Deng,zteed@princeton.edu;jiadeng@princeton.edu,8;6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,Princeton University;Princeton University,31;31,6;6,,12/11/18,12,8,6,2,4,2,21;14944,4;81,2;28,2;2557,m;m
3229,ICLR,2020,Measuring the Reliability of Reinforcement Learning Algorithms,Stephanie C.Y. Chan;Samuel Fishman;Anoop Korattikara;John Canny;Sergio Guadarrama,scychan@google.com;sfishman@google.com;kbanoop@google.com;canny@google.com;sguada@google.com,8;8;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Spotlight),0,5,0,yes,9/25/19,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,,9/25/19,4,3,2,1,0,1,34;0;1429;33288;18292,6;1;17;358;74,3;0;7;58;23,1;0;181;2645;2261,f;m
3230,ICLR,2020,SELF: Learning to Filter Noisy Labels with Self-Ensembling,Duc Tam Nguyen;Chaithanya Kumar Mummadi;Thi Phuong Nhung Ngo;Thi Hoai Phuong Nguyen;Laura Beggel;Thomas Brox,ductam.nguyen08@gmail.com;chaithanyakumar.mummadi@de.bosch.com;thiphuongnhung.ngo@de.bosch.com;hoai.phuong.nguyen198@gmail.com;laura.beggel@de.bosch.com;brox@cs.uni-freiburg.de,6;8;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,6,1,yes,9/25/19,Universität Freiburg;Bosch;Bosch;Karlsruhe Institute of Technology;Bosch;Universität Freiburg,118;-1;-1;154;-1;118,85;-1;-1;174;-1;85,,9/25/19,9,5,2,0,0,1,35;36;8;13;38;38737,16;6;2;3;6;257,4;5;1;2;4;73,3;4;1;1;3;5989,m;m
3231,ICLR,2020,Incorporating BERT into Neural Machine Translation,Jinhua Zhu;Yingce Xia;Lijun Wu;Di He;Tao Qin;Wengang Zhou;Houqiang Li;Tieyan Liu,teslazhu@mail.ustc.edu.cn;yingce.xia@gmail.com;wulijun3@mail2.sysu.edu.cn;di_he@pku.edu.cn;taoqin@microsoft.com;zhwg@ustc.edu.cn;lihq@ustc.edu.cn;tyliu@microsoft.com,6;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Poster),0,4,0,yes,9/25/19,University of Science and Technology of China;Microsoft;SUN YAT-SEN UNIVERSITY;Peking University;Microsoft;University of Science and Technology of China;University of Science and Technology of China;Microsoft,481;-1;481;22;-1;481;481;-1,80;-1;299;24;-1;80;80;-1,3,9/25/19,12,2,8,1,0,2,36;1288;205;40;15;3191;6175;13552,11;49;36;29;15;179;386;369,3;14;7;4;2;28;39;51,8;138;21;10;2;310;603;1723,u;m
3232,ICLR,2020,Mogrifier LSTM,Gábor Melis;Tomáš Kočiský;Phil Blunsom,melisgl@google.com;tkocisky@google.com;pblunsom@google.com,6;8;8,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Talk),0,3,0,yes,9/25/19,Google;Google;Google,-1;-1;-1,-1;-1;-1,3;8,9/4/19,3,1,0,0,0,0,611;2781;11713,14;16;144,8;10;47,99;429;1357,m;m
3233,ICLR,2020,Transferring Optimality Across Data Distributions via Homotopy Methods,Matilde Gargiani;Andrea Zanelli;Quoc Tran Dinh;Moritz Diehl;Frank Hutter,gargiani@informatik.uni-freiburg.de;andrea.zanelli@imtek.uni-freiburg.de;quoctd@email.unc.edu;moritz.diehl@imtek.uni-freiburg.de;fh@cs.uni-freiburg.de,3;6;8,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,"Universität Freiburg;Universität Freiburg;University of North Carolina, Chapel Hill;Universität Freiburg;Universität Freiburg",118;118;73;118;118,85;85;54;85;85,9,9/25/19,1,0,0,0,0,0,21;173;619;6547;13008,7;32;66;315;233,2;7;14;36;51,2;5;44;481;1554,f;m
3234,ICLR,2020,Quantum Algorithms for Deep Convolutional Neural Networks,Iordanis Kerenidis;Jonas Landman;Anupam Prakash,jkeren@gmail.com;landman@irif.fr;anupamprakash1@gmail.com,6;8;8;6,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0,yes,9/25/19,Université Paris Diderot;Universite Paris Diderot;Universite Paris Diderot,481;481;481,1397;1397;1397,,9/25/19,4,1,2,0,0,0,1603;26;616,100;3;39,22;2;11,152;2;67,m;m
3235,ICLR,2020,Unrestricted Adversarial Examples via Semantic Manipulation,Anand Bhattad;Min Jin Chong;Kaizhao Liang;Bo Li;D. A. Forsyth,bhattad2@illinois.edu;mchong6@illinois.edu;kl2@illinois.edu;lbo@illinois.edu;daf@illinois.edu,6;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",3;3;3;3;3,48;48;48;48;48,4,4/12/19,11,8,4,0,0,4,39;99;26;235;20774,6;6;4;147;426,4;4;2;9;62,8;16;7;25;1616,m;m
3236,ICLR,2020,A Stochastic Derivative Free Optimization Method with Momentum,Eduard Gorbunov;Adel Bibi;Ozan Sener;El Houcine Bergou;Peter Richtarik,eduard.gorbunov@phystech.edu;adel.bibi@kaust.edu.sa;ozan.sener@intel.com;houcine.bergou@kaust.edu.sa;peter.richtarik@kaust.edu.sa,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,Moscow Institute of Physics and Technology;KAUST;Intel;KAUST;KAUST,481;128;-1;128;128,234;1397;-1;1397;1397,,5/30/19,2,0,0,0,3,0,25;462;1183;71;5790,10;25;33;14;159,3;9;13;5;37,2;65;213;6;597,m;m
3237,ICLR,2020,Learning The Difference That Makes A Difference With Counterfactually-Augmented Data,Divyansh Kaushik;Eduard Hovy;Zachary Lipton,dkaushik@cs.cmu.edu;hovy@cmu.edu;zlipton@cmu.edu,8;8;1;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Accept (Spotlight),0,5,0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1,27;27;27,3,9/25/19,28,15,5,3,0,3,137;23872;4885,6;579;97,4;76;29,12;2499;438,m;m
3238,ICLR,2020,Fast is better than free: Revisiting adversarial training,Eric Wong;Leslie Rice;J. Zico Kolter,ericwong@cs.cmu.edu;larice@cs.cmu.edu;zkolter@cs.cmu.edu,6;6;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),10,11,0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1,27;27;27,4,9/25/19,43,28,15,5,0,7,726;72;7966,22;5;117,6;3;36,104;8;1085,m;m
3239,ICLR,2020,Disentangling neural mechanisms for perceptual grouping,Junkyung Kim*;Drew Linsley*;Kalpit Thakkar;Thomas Serre,junkyung_kim@brown.edu;drew_linsley@brown.edu;kalpit_thakkar@brown.edu;thomas_serre@brown.edu,8;6;8,I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Spotlight),0,5,0,yes,9/25/19,Brown University;Brown University;Brown University;Brown University,67;67;67;67,53;53;53;53,,6/4/19,10,3,1,0,0,0,53;132;32;8070,19;21;3;117,4;7;3;29,2;6;3;1222,m;m
3240,ICLR,2020,How to 0wn the NAS in Your Spare Time,Sanghyun Hong;Michael Davinroy;Yiǧitcan Kaya;Dana Dachman-Soled;Tudor Dumitraş,shhong@cs.umd.edu;michael.davinroy@gmail.com;cankaya@umiacs.umd.edu;danadach@ece.umd.edu;tdumitra@umiacs.umd.edu,6;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Accept (Poster),0,6,0,yes,9/25/19,"University of Maryland, College Park;Swarthmore College;University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park",12;-1;12;12;12,91;-1;91;91;91,4;10,9/25/19,4,2,1,0,0,0,521;17;107;850;1790,55;3;12;68;100,13;2;5;17;20,11;3;17;47;140,m;m
3241,ICLR,2020,Option Discovery using Deep Skill Chaining,Akhil Bagaria;George Konidaris,akhil_bagaria@brown.edu;gdk@cs.brown.edu,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,7,0,yes,9/25/19,Brown University;Brown University,67;67,53;53,,9/25/19,0,0,0,0,0,0,23;3018,3;145,1;29,4;191,m;m
3242,ICLR,2020,Optimal Strategies Against Generative Attacks,Roy Mor;Erez Peterfreund;Matan Gavish;Amir Globerson,roy16mor@gmail.com;erezpeter@cs.huji.ac.il;matan.gavish@mail.huji.ac.il;amir.globerson@gmail.com,8;8;8;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,4,0,yes,9/25/19,Tel Aviv University;Hebrew University of Jerusalem;Hebrew University of Jerusalem;Tel Aviv University,35;67;67;35,188;216;216;188,5;4,9/25/19,0,0,0,0,0,0,0;3;1052;4536,2;7;45;124,0;1;13;32,0;0;98;574,m;m
3243,ICLR,2020,Massively Multilingual Sparse Word Representations,Gábor Berend,berendg@inf.u-szeged.hu,8;8;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,5,0,yes,9/25/19,University of Szeged,323,874,3,9/25/19,0,0,0,0,0,0,255,31,8,15,m
3244,ICLR,2020,RaPP: Novelty Detection with Reconstruction along Projection Pathway,Ki Hyun Kim;Sangwoo Shim;Yongsub Lim;Jongseob Jeon;Jeongwoo Choi;Byungchan Kim;Andre S. Yoon,khkim@makinarocks.ai;sangwoo@makinarocks.ai;yongsub@makinarocks.ai;jongseob.jeon@makinarocks.ai;jeongwoo@makinarocks.ai;kbc8894@makinarocks.ai;andre@makinarocks.ai,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,8,1,yes,9/25/19,MakinaRocks;MakinaRocks;MakinaRocks;MakinaRocks;MakinaRocks;MakinaRocks;MakinaRocks,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,,9/25/19,1,0,0,0,0,0,77;117;237;1;272;251;2,24;24;18;1;38;16;2,6;3;7;1;10;2;1,5;5;30;0;10;9;0,m;m
3245,ICLR,2020,Domain Adaptive Multibranch Networks,Róger Bermúdez-Chacón;Mathieu Salzmann;Pascal Fua,roger.bermudez@epfl.ch;mathieu.salzmann@epfl.ch;pascal.fua@epfl.ch,6;3;8,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne,481;481;481,38;38;38,10,9/25/19,0,0,0,0,0,0,27;5661;4410,6;198;81,2;42;21,0;623;586,m;m
3246,ICLR,2020,Continual learning with hypernetworks,Johannes von Oswald;Christian Henning;João Sacramento;Benjamin F. Grewe,voswaldj@ethz.ch;henningc@ethz.ch;sacramento@ini.ethz.ch;bgrewe@ethz.ch,6;8;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0,yes,9/25/19,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10;10,13;13;13;13,6,6/3/19,9,3,6,0,0,0,15;52;249;758,9;17;26;19,2;4;8;9,0;1;15;30,m;m
3247,ICLR,2020,Distributed Bandit Learning: Near-Optimal Regret with Efficient Communication,Yuanhao Wang;Jiachen Hu;Xiaoyu Chen;Liwei Wang,yuanhao-16@mails.tsinghua.edu.cn;nickh@pku.edu.cn;cxy30@pku.edu.cn;wanglw@cis.pku.edu.cn,6;6;8,I have published in this field for several years.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:N/A:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,Tsinghua University;Peking University;Peking University;Peking University,8;22;22;22,23;24;24;24,1,4/12/19,2,2,1,0,0,0,76;6;-1;2396,5;5;-1;75,3;2;-1;23,12;0;0;264,m;m
3248,ICLR,2020,SVQN: Sequential Variational Soft Q-Learning Networks,Shiyu Huang;Hang Su;Jun Zhu;Ting Chen,huangsy1314@163.com;suhangss@mail.tsinghua.edu.cn;dcszj@tsinghua.edu.cn;tingchen@tsinghua.edu.cn,8;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,2,0,yes,9/25/19,Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University,8;8;8;8,23;23;23;23,10;8,9/25/19,1,0,1,0,0,0,1675;1613;705;488,185;69;96;42,21;13;15;10,127;226;15;89,m;m
3249,ICLR,2020,PROGRESSIVE LEARNING AND DISENTANGLEMENT OF HIERARCHICAL REPRESENTATIONS,Zhiyuan Li;Jaideep Vitthal Murkute;Prashnna Kumar Gyawali;Linwei Wang,zl7904@rit.edu;jvm6526@rit.edu;pkg2182@rit.edu;linwei.wang@rit.edu,8;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,6,0,yes,9/25/19,Rochester Institute of Technology;Rochester Institute of Technology;Rochester Institute of Technology;Rochester Institute of Technology,128;128;128;128,843;843;843;843,5,9/25/19,1,1,0,1,0,1,9;0;61;5,11;2;12;9,1;0;2;1,1;0;6;0,m;f
3250,ICLR,2020,Learning transport cost from subset correspondence,Ruishan Liu;Akshay Balsubramani;James Zou,ruishan@stanford.edu;akshay7@gmail.com;jamesyzou@gmail.com,3;6;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,Stanford University;;Stanford University,4;-1;4,4;-1;4,,9/25/19,1,0,0,0,0,0,90;286;1618,17;25;94,5;7;21,3;32;167,f;m
3251,ICLR,2020,Meta Dropout: Learning to Perturb Latent Features for Generalization,Hae Beom Lee;Taewook Nam;Eunho Yang;Sung Ju Hwang,haebeom.lee@kaist.ac.kr;namsan@kaist.ac.kr;eunhoy@kaist.ac.kr;sjhwang82@kaist.ac.kr,8;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:N/A:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0,yes,9/25/19,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,481;481;481;481,110;110;110;110,6;8,5/30/19,4,1,1,0,0,0,96;44;1067;1128,23;3;75;71,5;2;16;16,4;2;169;125,m;m
3252,ICLR,2020,On Robustness of Neural Ordinary Differential Equations,Hanshu YAN;Jiawei DU;Vincent TAN;Jiashi FENG,hanshu.yan@u.nus.edu;dujiawei@u.nus.edu;vtan@nus.edu.sg;elefjia@nus.edu.sg,6;8;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,11,1,yes,9/25/19,National University of Singapore;National University of Singapore;National University of Singapore;National University of Singapore,16;16;16;16,25;25;25;25,4,9/25/19,11,8,1,0,0,0,12;115;2953;9533,9;33;277;332,1;5;25;52,0;6;228;1232,m;m
3253,ICLR,2020,Towards neural networks that provably know when they don't know,Alexander Meinke;Matthias Hein,alexander.meinke@uni-tuebingen.de;matthias.hein@uni-tuebingen.de,6;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,University of Tuebingen;University of Tuebingen,154;154,91;91,,9/25/19,17,7,6,0,0,1,22;524,5;53,2;8,1;43,m;m
3254,ICLR,2020,Exploration in Reinforcement Learning with Deep Covering Options,Yuu Jinnai;Jee Won Park;Marlos C. Machado;George Konidaris,yuu_jinnai@brown.edu;jee_won_park@brown.edu;marlosm@google.com;gdk@cs.brown.edu,6;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,8,0,yes,9/25/19,Brown University;Brown University;Google;Brown University,67;67;-1;67,53;53;-1;53,1;10,9/25/19,2,2,0,0,0,0,92;235;637;3018,23;22;31;145,6;6;11;29,7;11;79;191,m;m
3255,ICLR,2020,Mixed Precision DNNs: All you need is a good parametrization,Stefan Uhlich;Lukas Mauch;Fabien Cardinaux;Kazuki Yoshiyama;Javier Alonso Garcia;Stephen Tiedemann;Thomas Kemp;Akira Nakamura,stefan.uhlich@sony.com;lukas.mauch@sony.com;fabien.cardinaux@sony.com;kazuki.yoshiyama@sony.com;javier.alonso@sony.com;stephen.tiedemann@sony.com;thomas.kemp@sony.com;akira.b.nakamura@sony.com,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:N/A:N/A,Accept (Poster),1,7,1,yes,9/25/19,Sony Europe Ltd.;Sony Europe Ltd.;Sony Europe Ltd.;Sony Europe Ltd.;Sony Europe Ltd.;Sony Europe Ltd.;Sony Europe Ltd.;Sony Europe Ltd.,-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1,,5/27/19,6,5,2,0,0,2,270;177;860;54;20;16;1153;1102,30;22;41;7;11;18;67;267,7;8;13;4;3;2;15;15,37;16;61;2;1;1;88;62,m;m
3256,ICLR,2020,Depth-Width Trade-offs for ReLU Networks via Sharkovsky's Theorem,Vaggos Chatziafratis;Sai Ganesh Nagarajan;Ioannis Panageas;Xiao Wang,vaggos@cs.stanford.edu;sai_nagarajan@mymail.sutd.edu.sg;ioannis@sutd.edu.sg;xiao_wang@sutd.edu.sg,8;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,3,0,yes,9/25/19,Stanford University;Singapore University of Technology and Design;Singapore University of Technology and Design;Singapore University of Technology and Design,4;481;481;481,4;1397;1397;1397,1,9/25/19,4,3,1,1,0,0,95;35;497;4404,13;9;35;361,5;3;10;31,10;1;41;307,m;m
3257,ICLR,2020,Learning to Move with Affordance Maps,William Qi;Ravi Teja Mullapudi;Saurabh Gupta;Deva Ramanan,wq@cs.cmu.edu;raviteja.mullapudi@gmail.com;saurabhg@illinois.edu;deva@cs.cmu.edu,6;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,8,0,yes,9/25/19,"Carnegie Mellon University;Carnegie Mellon University;University of Illinois, Urbana Champaign;Carnegie Mellon University",1;1;3;1,27;27;48;27,,9/25/19,1,1,0,0,0,0,1;261;59;32040,1;13;30;163,1;5;4;59,0;38;6;5389,m;m
3258,ICLR,2020,Data-dependent Gaussian Prior Objective for Language Generation,Zuchao Li;Rui Wang;Kehai Chen;Masso Utiyama;Eiichiro Sumita;Zhuosheng Zhang;Hai Zhao,charlee@sjtu.edu.cn;wangrui@nict.go.jp;khchen@nict.go.jp;mutiyama@nict.go.jp;eiichiro.sumita@nict.go.jp;zhangzs@sjtu.edu.cn;zhaohai@cs.sjtu.edu.cn,8;8;8,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,8,0,yes,9/25/19,"Shanghai Jiao Tong University;National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology;National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology;National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology;National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology;Shanghai Jiao Tong University;Shanghai Jiao Tong University",53;-1;-1;-1;-1;53;53,157;-1;-1;-1;-1;157;157,3;7,9/25/19,5,3,2,2,0,1,339;591;263;5;4424;391;1300,29;54;39;1;378;32;262,9;14;8;1;31;13;18,34;47;15;1;341;45;76,m;m
3259,ICLR,2020,Sign Bits Are All You Need for Black-Box Attacks,Abdullah Al-Dujaili;Una-May O'Reilly,ash.aldujaili@gmail.com;unamay@csail.mit.edu,6;6;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2,5;5,4,9/25/19,2,2,2,0,0,2,36;3604,10;220,4;31,5;328,m;f
3260,ICLR,2020,Biologically inspired sleep algorithm for increased generalization and adversarial robustness in deep neural networks,Timothy Tadros;Giri Krishnan;Ramyaa Ramyaa;Maxim Bazhenov,tttadros@ucsd.edu;gkrishnan@ucsd.edu;ramyaa.ramyaa@gmail.com;mbazhenov@ucsd.edu,8;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Accept (Poster),0,3,0,yes,9/25/19,"University of California, San Diego;University of California, San Diego;;University of California, San Diego",11;11;-1;11,31;31;-1;31,4;8,9/25/19,0,0,0,0,0,0,4;98;73;4362,6;15;18;205,1;4;4;37,0;6;9;388,m;m
3261,ICLR,2020,SNODE: Spectral Discretization of Neural ODEs for System Identification,Alessio Quaglino;Marco Gallieri;Jonathan Masci;Jan Koutník,alessio@nnaisense.com;marco@nnaisense.com;jonathan@nnaisense.com;jan@nnaisense.com,8;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,7,0,yes,9/25/19,NNAISENSE;NNAISENSE;NNAISENSE;NNAISENSE,-1;-1;-1;-1,-1;-1;-1;-1,8,6/17/19,4,1,2,0,0,0,50;147;5373;3122,18;17;47;50,4;6;22;14,1;9;474;297,m;m
3262,ICLR,2020,Query2box: Reasoning over Knowledge Graphs in Vector Space Using Box Embeddings,Hongyu Ren*;Weihua Hu*;Jure Leskovec,hyren@cs.stanford.edu;weihuahu@stanford.edu;jure@cs.stanford.edu,8;6;6,I do not know much about this area.:I carefully checked the experiments.:I made a quick assessment of this paper.:N/A;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,Stanford University;Stanford University;Stanford University,4;4;4,4;4;4,1;10,9/25/19,3,1,1,0,0,0,240;1346;48502,42;63;302,10;13;93,15;296;6080,m;m
3263,ICLR,2020,Theory and Evaluation Metrics for Learning Disentangled Representations,Kien Do;Truyen Tran,dkdo@deakin.edu.au;truyen.tran@deakin.edu.au,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,Deakin University;Deakin University,481;481,332;332,5,8/26/19,3,2,2,0,0,1,67;1938,11;133,5;24,7;127,m;m
3264,ICLR,2020,Learning to Plan in High Dimensions via Neural Exploration-Exploitation Trees,Binghong Chen;Bo Dai;Qinjie Lin;Guo Ye;Han Liu;Le Song,binghong@gatech.edu;bodai@google.com;qinjielin2018@u.northwestern.edu;guoye2018@u.northwestern.edu;hanliu@northwestern.edu;lsong@cc.gatech.edu,8;6;8,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,3,0,yes,9/25/19,Georgia Institute of Technology;Google;Northwestern University;Northwestern University;Northwestern University;Georgia Institute of Technology,13;-1;44;44;44;13,38;-1;22;22;22;38,,9/25/19,1,0,0,0,0,0,10;418;22;234;13;9519,5;58;11;79;28;329,2;8;3;6;2;54,0;64;0;11;0;1114,m;m
3265,ICLR,2020,And the Bit Goes Down: Revisiting the Quantization of Neural Networks,Pierre Stock;Armand Joulin;Rémi Gribonval;Benjamin Graham;Hervé Jégou,pstock@fb.com;ajoulin@fb.com;remi.gribonval@inria.fr;benjamingraham@fb.com;rvj@fb.com,6;8;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,5,3,yes,9/25/19,Facebook;Facebook;INRIA;Facebook;Facebook,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,,7/12/19,15,6,5,0,0,1,102;10653;9827;958;14008,10;74;215;48;165,4;32;44;11;40,4;1533;1135;102;2386,m;m
3266,ICLR,2020,Continual Learning with Adaptive Weights (CLAW),Tameem Adel;Han Zhao;Richard E. Turner,tah47@cam.ac.uk;han.zhao@cs.cmu.edu;ret26@cam.ac.uk,3;8;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,1,yes,9/25/19,University of Cambridge;Carnegie Mellon University;University of Cambridge,71;1;71,3;27;3,6,9/25/19,3,0,0,0,0,0,443;43;3055,25;13;175,9;2;30,51;5;315,m;m
3267,ICLR,2020,Depth-Adaptive Transformer,Maha Elbayad;Jiatao Gu;Edouard Grave;Michael Auli,maha.elbayad@inria.fr;thomagram@gmail.com;egrave@fb.com;michael.auli@gmail.com,6;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Poster),0,7,1,yes,9/25/19,INRIA;Facebook;Facebook;Facebook,-1;-1;-1;-1,-1;-1;-1;-1,,9/25/19,6,4,2,0,0,2,59;1440;7711;6931,6;41;57;71,3;15;23;31,7;217;1136;1027,f;m
3268,ICLR,2020,Robust Local Features for Improving the Generalization of Adversarial Training,Chuanbiao Song;Kun He;Jiadong Lin;Liwei Wang;John E. Hopcroft,cbsong@hust.edu.cn;brooklet60@hust.edu.cn;jdlin@hust.edu.cn;wanglw@cis.pku.edu.cn;jeh@cs.cornell.edu,6;3;8,I do not know much about this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,7,0,yes,9/25/19,Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Peking University;Cornell University,39;39;39;22;7,47;47;47;24;19,4;8,9/23/19,4,1,0,0,0,0,3;-1;121;98;28945,1;-1;15;23;303,1;-1;4;6;60,0;0;5;21;2742,m;m
3269,ICLR,2020,Learning deep graph matching with channel-independent embedding and Hungarian attention,Tianshu Yu;Runzhong Wang;Junchi Yan;Baoxin Li,tianshuy@asu.edu;runzhong.wang@sjtu.edu.cn;yanjunchi@sjtu.edu.cn;baoxin.li@asu.edu,3;6;6,I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,3,0,yes,9/25/19,Arizona State University;Shanghai Jiao Tong University;Shanghai Jiao Tong University;Arizona State University,95;53;53;95,155;157;157;155,10,9/25/19,2,2,1,0,0,0,70;37;2284;3431,21;4;148;225,6;2;27;28,1;6;150;304,m;m
3270,ICLR,2020,Large Batch Optimization for Deep Learning: Training BERT in 76 minutes,Yang You;Jing Li;Sashank Reddi;Jonathan Hseu;Sanjiv Kumar;Srinadh Bhojanapalli;Xiaodan Song;James Demmel;Kurt Keutzer;Cho-Jui Hsieh,youyang@cs.berkeley.edu;jingli@google.com;sashank@google.com;jhseu@google.com;sanjivk@google.com;bsrinadh@google.com;xiaodansong@google.com;demmel@berkeley.edu;keutzer@berkeley.edu;chohsieh@cs.ucla.edu,3;8;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,3,0,yes,9/25/19,"University of California Berkeley;Google;Google;Google;Google;Google;Google;University of California Berkeley;University of California Berkeley;University of California, Los Angeles",5;-1;-1;-1;-1;-1;-1;5;5;20,13;-1;-1;-1;-1;-1;-1;13;13;17,9,4/1/19,46,20,16,1,0,4,836;71;2194;92;941;1642;3824;20309;16995;12827,29;36;53;5;142;28;48;429;417;168,12;3;20;3;13;16;17;66;60;41,97;6;418;10;113;199;307;2080;1592;1746,m;m
3271,ICLR,2020,Reanalysis of Variance Reduced Temporal Difference Learning,Tengyu Xu;Zhe Wang;Yi Zhou;Yingbin Liang,xu.3260@osu.edu;wang.10982@osu.edu;yi.zhou@utah.edu;liang.889@osu.edu,3;8;8;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0,yes,9/25/19,Ohio State University;Ohio State University;University of Utah;Ohio State University,77;77;233;77,373;373;366;373,9,9/25/19,6,4,2,0,0,0,75;231;28;5076,14;102;72;212,6;8;3;32,7;30;6;363,m;f
3272,ICLR,2020,Unsupervised Model Selection for Variational Disentangled Representation Learning,Sunny Duan;Loic Matthey;Andre Saraiva;Nick Watters;Chris Burgess;Alexander Lerchner;Irina Higgins,sunnyd@google.com;lmatthey@google.com;andresnds@google.com;nwatters@google.com;cpburgess@google.com;lerchner@google.com;irinah@google.com,6;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),2,14,0,yes,9/25/19,Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,5;7,5/29/19,5,2,3,0,0,0,49;317;-1;321;527;2146;2018,2;9;-1;11;18;23;25,2;6;-1;6;9;15;11,1;29;0;31;52;327;302,m;f
3273,ICLR,2020,On the Weaknesses of Reinforcement Learning for Neural Machine Translation,Leshem Choshen;Lior Fox;Zohar Aizenbud;Omri Abend,leshem.choshen@mail.huji.ac.il;lior.fox@mail.huji.ac.il;zohar.aizenbud@mail.huji.ac.il;oabend@cs.huji.ac.il,6;3;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0,yes,9/25/19,Hebrew University of Jerusalem;Hebrew University of Jerusalem;Hebrew University of Jerusalem;Hebrew University of Jerusalem,67;67;67;67,216;216;216;216,3;4;5;1,7/3/19,3,1,1,0,0,0,117;27;22;725,17;4;4;50,7;2;2;17,14;7;4;89,m;m
3274,ICLR,2020,Training binary neural networks with real-to-binary convolutions,Brais Martinez;Jing Yang;Adrian Bulat;Georgios Tzimiropoulos,brais.mart@gmail.com;psxjy3@nottingham.ac.uk;adrian@adrianbulat.com;yorgos.tzimiropoulos@nottingham.ac.uk,6;6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,8,0,yes,9/25/19,Imperial College London;The University of Nottingham;Samsung;The University of Nottingham,73;233;-1;233,10;152;-1;152,,9/25/19,4,2,3,1,0,2,1734;68;1373;4733,46;49;29;102,19;4;11;32,250;3;175;565,m;m
3275,ICLR,2020,Mixed-curvature Variational Autoencoders,Ondrej Skopek;Octavian-Eugen Ganea;Gary Bécigneul,oskopek@oskopek.com;oct@mit.edu;garyb@mit.edu,8;8;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,6,0,yes,9/25/19,Google;Massachusetts Institute of Technology;Massachusetts Institute of Technology,-1;2;2,-1;5;5,5,9/25/19,6,3,1,0,0,1,12;529;243,6;17;17,2;9;5,1;96;47,m;m
3276,ICLR,2020,Program Guided Agent,Shao-Hua Sun;Te-Lin Wu;Joseph J. Lim,shaohuas@usc.edu;telinwu@usc.edu;limjj@usc.edu,6;8;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,16,0,yes,9/25/19,University of Southern California;University of Southern California;University of Southern California,31;31;31,62;62;62,3;6;8,9/25/19,0,0,0,0,0,0,1502;97;2976,36;15;51,7;4;21,158;16;270,m;m
3277,ICLR,2020,Revisiting Self-Training for Neural Sequence Generation,Junxian He;Jiatao Gu;Jiajun Shen;Marc'Aurelio Ranzato,junxianh@cs.cmu.edu;thomagram@gmail.com;jiajunshen@fb.com;ranzato@fb.com,8;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:N/A:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,Carnegie Mellon University;Facebook;Facebook;Facebook,1;-1;-1;-1,27;-1;-1;-1,3,9/25/19,18,10,8,0,0,2,248;1440;121;51,14;41;19;17,8;15;5;4,45;217;15;6,m;m
3278,ICLR,2020,"Ridge Regression: Structure, Cross-Validation, and Sketching",Sifan Liu;Edgar Dobriban,sfliu@stanford.edu;dobribanedgar@gmail.com,6;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,2,0,yes,9/25/19,Stanford University;University of Pennsylvania,4;19,4;11,,9/25/19,5,1,2,1,0,1,77;561,16;42,5;11,5;46,f;m
3279,ICLR,2020,Deep Network Classification by Scattering and Homotopy Dictionary Learning,John Zarka;Louis Thiry;Tomas Angles;Stephane Mallat,john.zarka@ens.fr;louis.thiry@ens.fr;tomas.angles@ens.fr;stephane.mallat@ens.fr,8;6;8,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,Ecole Normale Superieure;Ecole Normale Superieure;Ecole Normale Superieure;Ecole Normale Superieure,100;100;100;100,45;45;45;45,1,9/25/19,2,1,0,0,0,0,12;47;41;1162,4;8;6;31,2;2;3;12,0;0;2;96,m;f
3280,ICLR,2020,FreeLB: Enhanced Adversarial Training for Natural Language Understanding,Chen Zhu;Yu Cheng;Zhe Gan;Siqi Sun;Tom Goldstein;Jingjing Liu,chenzhu@cs.umd.edu;yu.cheng@microsoft.com;zhe.gan@microsoft.com;siqi.sun@microsoft.com;tomg@cs.umd.edu;jingjl@microsoft.com,8;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,3,0,yes,9/25/19,"University of Maryland, College Park;Microsoft;Microsoft;Microsoft;University of Maryland, College Park;Microsoft",12;-1;-1;-1;12;-1,91;-1;-1;-1;91;-1,3;4;8,9/25/19,38,16,14,2,0,9,725;10195;2442;627;228;403,62;883;90;49;61;80,11;46;26;11;10;6,77;538;342;58;26;37,m;f
3281,ICLR,2020,Reinforcement Learning with Competitive  Ensembles of Information-Constrained Primitives,Anirudh Goyal;Shagun Sodhani;Jonathan Binas;Xue Bin Peng;Sergey Levine;Yoshua Bengio,anirudhgoyal9119@gmail.com;sshagunsodhani@gmail.com;jbinas@gmail.com;xbpeng@berkeley.edu;svlevine@eecs.berkeley.edu;yoshua.bengio@mila.quebec,8;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,7,0,yes,9/25/19,University of Montreal;University of Montreal;University of Montreal;University of California Berkeley;University of California Berkeley;University of Montreal,128;128;128;5;5;128,85;85;85;13;13;85,8,6/25/19,7,3,1,0,9,0,1137;78;693;1116;24893;208566,46;23;28;23;310;807,12;5;10;11;74;147,130;7;94;84;3235;24297,m;m
3282,ICLR,2020,Adversarially Robust Representations with Smooth Encoders,Taylan Cemgil;Sumedh Ghaisas;Krishnamurthy (Dj) Dvijotham;Pushmeet Kohli,taylancemgil@google.com;sumedhg@google.com;dvij@google.com;pushmeet@google.com,6;3;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,5;4;1,9/25/19,0,0,0,0,0,0,55;39;1143;22578,12;4;76;313,4;2;17;69,4;1;101;2782,m;m
3283,ICLR,2020,Scalable Neural Methods for Reasoning With a Symbolic Knowledge   Base,William W. Cohen;Haitian Sun;R. Alex Hofer;Matthew Siegler,wcohen@google.com;haitiansun@google.com;rofer@google.com;msiegler@google.com,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:N/A:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,,9/25/19,1,1,0,0,0,0,22767;87;185;980,424;18;42;81,69;3;8;13,2635;16;2;63,m;m
3284,ICLR,2020,Curriculum Loss: Robust Learning and Generalization  against Label Corruption,Yueming Lyu;Ivor W. Tsang,lv_yueming@outlook.com;ivor.tsang@uts.edu.au,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,University of Technology Sydney;University of Technology Sydney,108;108,193;193,1;8,5/24/19,8,7,0,0,0,0,64;10966,9;253,5;51,3;1283,m;m
3285,ICLR,2020,Low-dimensional statistical manifold embedding of directed graphs,Thorben Funke;Tian Guo;Alen Lancic;Nino Antulov-Fantulin,fun@biba.uni-bremen.de;tian.guo0980@gmail.com;alen.lancic@math.hr;nino.antulov@gess.ethz.ch,8;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,Universität Bremen;;;Swiss Federal Institute of Technology,154;-1;-1;10,360;-1;-1;13,10,5/24/19,0,0,0,0,0,0,5;311;101;68,9;58;12;19,1;11;5;6,0;18;9;2,m;m
3286,ICLR,2020,VideoFlow: A Conditional Flow-Based Model for Stochastic Video Generation,Manoj Kumar;Mohammad Babaeizadeh;Dumitru Erhan;Chelsea Finn;Sergey Levine;Laurent Dinh;Durk Kingma,manojkumarsivaraj334@gmail.com;mb2@uiuc.edu;dumitru@google.com;cbfinn@eecs.berkeley.edu;slevine@google.com;laurentdinh@google.com;d.p.kingma@uva.nl,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0,yes,9/25/19,";University of Illinois, Urbana-Champaign;Google;University of California Berkeley;Google;Google;University of Amsterdam",-1;3;-1;5;-1;-1;172,-1;48;-1;13;-1;-1;62,5,3/4/19,5,2,2,0,0,0,179;628;43043;7879;24893;4771;50,60;19;60;101;310;24;3,6;8;31;34;74;12;2,11;94;6039;1060;3235;632;6,m;m
3287,ICLR,2020,Self-Adversarial Learning with Comparative Discrimination for Text Generation,Wangchunshu Zhou;Tao Ge;Ke Xu;Furu Wei;Ming Zhou,v-waz@microsoft.com;tage@microsoft.com;kexu@nlsde.buaa.edu.cn;fuwei@microsoft.com;mingzhou@microsoft.com,8;3;8,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,6,0,yes,9/25/19,Microsoft;Microsoft;Beihang University;Microsoft;Microsoft,-1;-1;118;-1;-1,-1;-1;594;-1;-1,5;4,9/25/19,3,2,0,0,0,0,22;85;302;7172;1370,12;39;114;164;205,3;5;8;43;19,2;4;15;783;52,m;m
3288,ICLR,2020,Deep neuroethology of a virtual rodent,Josh Merel;Diego Aldarondo;Jesse Marshall;Yuval Tassa;Greg Wayne;Bence Olveczky,jsmerel@google.com;diegoaldarondo@g.harvard.edu;jesse_d_marshall@fas.harvard.edu;tassa@google.com;gregwayne@google.com;olveczky@fas.harvard.edu,6;8;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:N/A:N/A:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Spotlight),0,4,0,yes,9/25/19,Google;Harvard University;Harvard University;Google;Google;Harvard University,-1;39;39;-1;-1;39,-1;7;7;-1;-1;7,,9/25/19,4,3,2,0,0,0,1684;110;1049;7224;777;0,29;5;15;45;8;1,16;2;9;24;2;0,129;16;79;1202;100;0,m;m
3289,ICLR,2020,On the Global Convergence  of Training Deep Linear ResNets,Difan Zou;Philip M. Long;Quanquan Gu,knowzou@ucla.edu;plong@google.com;qgu@cs.ucla.edu,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,9,0,yes,9/25/19,"University of California, Los Angeles;Google;University of California, Los Angeles",20;-1;20,17;-1;17,1;9,9/25/19,0,0,0,0,0,0,426;5277;3895,31;156;174,11;36;34,24;401;411,m;m
3290,ICLR,2020,Measuring and Improving the Use of Graph Information in Graph Neural Networks,Yifan Hou;Jian Zhang;James Cheng;Kaili Ma;Richard T. B. Ma;Hongzhi Chen;Ming-Chang Yang,yfhou@cse.cuhk.edu.hk;jzhang@cse.cuhk.edu.hk;jcheng@cse.cuhk.edu.hk;klma@cse.cuhk.edu.hk;tbma@comp.nus.edu.sg;hzchen@cse.cuhk.edu.hk;mcyang@cse.cuhk.edu.hk,8;3;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0,yes,9/25/19,The Chinese University of Hong Kong;The Chinese University of Hong Kong;The Chinese University of Hong Kong;The Chinese University of Hong Kong;National University of Singapore;The Chinese University of Hong Kong;The Chinese University of Hong Kong,59;59;59;59;16;59;59,35;35;35;35;25;35;35,10,9/25/19,5,3,1,0,0,1,195;236;4;15;1571;37;297,57;62;7;5;96;14;47,8;8;1;2;20;4;10,7;15;1;1;79;3;23,m;m
3291,ICLR,2020,AssembleNet: Searching for Multi-Stream Neural Connectivity in Video Architectures,Michael S. Ryoo;AJ Piergiovanni;Mingxing Tan;Anelia Angelova,mryoo@google.com;ajpiergi@indiana.edu;tanmingxing@google.com;anelia@google.com,8;8;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Poster),0,4,1,yes,9/25/19,Google;University of Arizona;Google;Google,-1;172;-1;-1,-1;103;-1;-1,,5/30/19,9,4,0,0,0,0,2889;180;1706;1812,101;31;35;50,25;8;13;18,271;11;288;166,m;f
3292,ICLR,2020,Identity Crisis: Memorization and Generalization Under Extreme Overparameterization,Chiyuan Zhang;Samy Bengio;Moritz Hardt;Michael C. Mozer;Yoram Singer,pluskid@gmail.com;bengio@google.com;moritzhardt@gmail.com;mcmozer@google.com;y.s@cs.princeton.edu,8;6;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,Massachusetts Institute of Technology;Google;;Google;Princeton University,2;-1;-1;-1;31,5;-1;-1;-1;6,8,2/13/19,23,16,2,0,8,2,5635;26795;7854;-1;35018,74;332;89;-1;211,27;67;33;-1;59,631;3497;964;0;4948,m;m
3293,ICLR,2020,Spike-based causal inference for weight alignment,Jordan Guerguiev;Konrad Kording;Blake Richards,jordan.guerguiev@utoronto.ca;koerding@gmail.com;blake.richards@mcgill.ca,6;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,Toronto University;University of Pennsylvania;McGill University,18;19;86,18;11;42,,9/25/19,8,3,1,1,0,1,166;5210;1654,5;110;56,3;30;18,13;341;108,m;m
3294,ICLR,2020,VariBAD: A Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning,Luisa Zintgraf;Kyriacos Shiarlis;Maximilian Igl;Sebastian Schulze;Yarin Gal;Katja Hofmann;Shimon Whiteson,luisa.zintgraf@cs.ox.ac.uk;kikos1988@gmail.com;maximilian.igl@gmail.com;sebastian.schulze@eng.ox.ac.uk;yarin.gal@cs.ox.ac.uk;katja.hofmann@microsoft.com;shimon.whiteson@cs.ox.ac.uk,6;8;1;8,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,7,0,yes,9/25/19,University of Oxford;;University of Oxford;University of Oxford;University of Oxford;Microsoft;University of Oxford,50;-1;50;50;50;-1;50,1;-1;1;1;1;-1;1,11;6,9/25/19,7,4,3,0,0,2,475;238;272;228;6605;178;5445,16;11;12;78;91;36;203,8;7;6;8;20;8;38,52;17;42;5;987;11;588,f;m
3295,ICLR,2020,Expected Information Maximization: Using the I-Projection for Mixture Density Estimation,Philipp Becker;Oleg Arenz;Gerhard Neumann,philippbecker93@googlemail.com;oleg@robot-learning.de;geri@robot-learning.de,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,TU Darmstadt;TU Darmstadt;University of Lincoln,64;64;481,289;289;617,5;4;1,9/25/19,0,0,0,0,0,0,116;62;3366,30;16;163,6;3;29,6;8;250,m;m
3296,ICLR,2020,Fast Task Inference with Variational Intrinsic Successor Features,Steven Hansen;Will Dabney;Andre Barreto;David Warde-Farley;Tom Van de Wiele;Volodymyr Mnih,stevenhansen@google.com;wdabney@google.com;andrebarreto@google.com;dwf@google.com;tvdwiele@gmail.com;vmnih@google.com,6;8;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,4,0,yes,9/25/19,Google;Google;Google;Google;;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,8,6/12/19,8,4,1,0,11,2,10;1790;407;24682;1347;21340,5;25;12;27;156;38,2;13;6;19;17;27,2;340;38;3892;95;3387,m;m
3297,ICLR,2020,Kernel of CycleGAN as a principal homogeneous space,Nikita Moriakov;Jonas Adler;Jonas Teuwen,nikita.moriakov@radboudumc.nl;jonasadl@kth.se;jonas.teuwen@radboudumc.nl,3;6;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,3,1,yes,9/25/19,"Radboud University Medical Center;KTH Royal Institute of Technology, Stockholm, Sweden;Radboud University Medical Center",390;128;390,128;222;128,4,9/25/19,0,0,0,0,0,0,10;516;122,16;28;33,2;7;7,0;45;4,f;m
3298,ICLR,2020,"Generative Models for Effective ML on Private, Decentralized Datasets",Sean Augenstein;H. Brendan McMahan;Daniel Ramage;Swaroop Ramaswamy;Peter Kairouz;Mingqing Chen;Rajiv Mathews;Blaise Aguera y Arcas,saugenst@google.com;mcmahan@google.com;dramage@google.com;swaroopram@google.com;kairouz@google.com;mingqing@google.com;mathews@google.com;blaisea@google.com,6;8;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1,5,9/25/19,11,6,2,0,0,2,259;5944;21397;39;1029;329;165;8,14;68;46;9;50;31;9;4,7;32;30;3;11;11;6;1,24;862;3098;3;115;21;14;2,m;m
3299,ICLR,2020,Gradient-Based Neural DAG Learning,Sébastien Lachapelle;Philippe Brouillard;Tristan Deleu;Simon Lacoste-Julien,sebastien.lachapelle@umontreal.ca;philippebrouillard@gmail.com;tristan.deleu@gmail.com;slacoste@iro.umontreal.ca,6;6;8,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,5,0,yes,9/25/19,University of Montreal;University of Montreal;University of Montreal;University of Montreal,128;128;128;128,85;85;85;85,10,6/5/19,11,7,6,1,3,3,3838;24;94;13,76;5;13;3,27;2;5;2,610;5;6;3,m;m
3300,ICLR,2020,DeepSphere: a graph-based spherical CNN,Michaël Defferrard;Martino Milani;Frédérick Gusset;Nathanaël Perraudin,michael.defferrard@epfl.ch;martino.milani@epfl.ch;frederick.gusset@epfl.ch;nathanael.perraudin@sdsc.ethz.ch,6;6;8,I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,4,0,yes,9/25/19,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology,481;481;481;10,38;38;38;13,10,9/25/19,1,0,0,0,0,0,1979;0;1;744,17;3;3;38,4;0;1;12,319;0;0;75,m;m
3301,ICLR,2020,IMPACT: Importance Weighted Asynchronous Architectures with Clipped Target Networks,Michael Luo;Jiahao Yao;Richard Liaw;Eric Liang;Ion Stoica,michael.luo@berkeley.edu;jiahaoyao@berkeley.edu;rliaw@berkeley.edu;ekhliang@gmail.com;istoica@berkeley.edu,6;6;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5;5,13;13;13;13;13,,9/25/19,0,0,0,0,0,0,11;8;501;1042;57969,12;6;17;23;492,2;2;10;12;113,1;0;65;95;7246,m;m
3302,ICLR,2020,Improving Neural Language Generation with Spectrum Control,Lingxiao Wang;Jing Huang;Kevin Huang;Ziniu Hu;Guangtao Wang;Quanquan Gu,lingxw@cs.ucla.edu;jing.huang@jd.com;kevin.huang3@jd.com;bull@cs.ucla.edu;guangtao.wang@jd.com;qgu@cs.ucla.edu,6;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,"University of California, Los Angeles;JD AI Research;JD AI Research;University of California, Los Angeles;JD AI Research;University of California, Los Angeles",20;-1;-1;20;-1;20,17;-1;-1;17;-1;17,3,9/25/19,2,1,0,0,0,0,311;2697;14;132;677;3895,56;57;4;24;39;174,10;16;2;6;12;34,18;242;0;16;46;411,m;m
3303,ICLR,2020,Harnessing the Power of Infinitely Wide Deep Nets on Small-data Tasks,Sanjeev Arora;Simon S. Du;Zhiyuan Li;Ruslan Salakhutdinov;Ruosong Wang;Dingli Yu,arora@cs.princeton.edu;ssdu@ias.edu;zhiyuanli@cs.princeton.edu;rsalakhu@cs.cmu.edu;ruosongw@andrew.cmu.edu;dingliy@cs.princeton.edu,8;6;8,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Spotlight),0,4,0,yes,9/25/19,"Princeton University;Institue for Advanced Study, Princeton;Princeton University;Carnegie Mellon University;Carnegie Mellon University;Princeton University",31;-1;31;1;1;31,6;-1;6;27;27;6,6,9/25/19,22,14,9,5,0,4,2317;2086;675;69005;-1;958,82;55;13;254;-1;99,14;20;10;82;-1;16,254;313;116;7875;0;53,m;m
3304,ICLR,2020,NAS evaluation is frustratingly hard,Antoine Yang;Pedro M. Esperança;Fabio M. Carlucci,antoineyang3@gmail.com;pedro.esperanca@huawei.com;fabiom.carlucci@gmail.com,8;1;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Poster),0,4,0,yes,9/25/19,ENS Paris-Saclay;Huawei Technologies Ltd.;Huawei Technologies Ltd.,481;-1;-1,644;-1;-1,1,9/25/19,26,13,8,3,0,4,27;129;437,2;16;18,2;5;10,4;10;62,m;m
3305,ICLR,2020,Understanding and Robustifying Differentiable Architecture Search,Arber Zela;Thomas Elsken;Tonmoy Saikia;Yassine Marrakchi;Thomas Brox;Frank Hutter,zelaa@cs.uni-freiburg.de;thomas.elsken@de.bosch.com;saikiat@cs.uni-freiburg.de;marrakch@cs.uni-freiburg.de;brox@cs.uni-freiburg.de;fh@cs.uni-freiburg.de,8;8;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Talk),1,5,0,yes,9/25/19,Universität Freiburg;Bosch;Universität Freiburg;Universität Freiburg;Universität Freiburg;Universität Freiburg,118;-1;118;118;118;118,85;-1;85;85;85;85,3;8,9/20/19,27,16,9,4,0,5,101;697;1133;244;38737;13008,7;24;9;9;257;233,4;8;4;5;73;51,11;71;179;15;5989;1554,m;m
3306,ICLR,2020,Meta-Q-Learning,Rasool Fakoor;Pratik Chaudhari;Stefano Soatto;Alexander J. Smola,rasool.fakoor@mavs.uta.edu;pratikac@seas.upenn.edu;soatto@cs.ucla.edu;alex@smola.org,6;8;8,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,10,0,yes,9/25/19,"University of Texas, Arlington;University of Pennsylvania;University of California, Los Angeles;Carnegie-Mellon University",118;19;20;1,708;11;17;27,,9/25/19,2,1,0,0,0,0,155;6;15761;65796,16;12;457;389,4;2;62;99,8;0;1445;9104,m;m
3307,ICLR,2020,Budgeted Training: Rethinking Deep Neural Network Training Under Resource Constraints,Mengtian Li;Ersin Yumer;Deva Ramanan,mtli@cs.cmu.edu;meyumer@gmail.com;deva@cs.cmu.edu,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,1,yes,9/25/19,Carnegie Mellon University;Uber;Carnegie Mellon University,1;-1;1,27;-1;27,2,5/12/19,5,1,0,0,0,1,210;1868;32040,43;50;163,10;20;59,4;256;5389,f;m
3308,ICLR,2020,Training individually fair ML models with sensitive subspace robustness,Mikhail Yurochkin;Amanda Bower;Yuekai Sun,mikhail.yurochkin@ibm.com;amandarg@umich.edu;yuekai@umich.edu,6;6;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,4,0,yes,9/25/19,International Business Machines;University of Michigan;University of Michigan,-1;8;8,-1;21;21,4;7,6/28/19,4,4,3,2,0,0,139;64;1229,27;23;46,6;4;15,7;2;173,m;m
3309,ICLR,2020,Deformable Kernels: Adapting Effective Receptive Fields for Object Deformation,Hang Gao;Xizhou Zhu;Stephen Lin;Jifeng Dai,hangg@berkeley.edu;ezra0408@mail.ustc.edu.cn;stevelin@microsoft.com;jifdai@microsoft.com,6;6;6,I do not know much about this area.:N/A:N/A:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:N/A:N/A,Accept (Poster),0,4,0,yes,9/25/19,University of California Berkeley;University of Science and Technology of China;Microsoft;Microsoft,5;481;-1;-1,13;80;-1;-1,,9/25/19,4,2,1,0,0,0,752;734;77;7202,56;19;18;46,15;9;4;21,48;121;1;1057,m;m
3310,ICLR,2020,From Variational to Deterministic Autoencoders,Partha Ghosh;Mehdi S. M. Sajjadi;Antonio Vergari;Michael Black;Bernhard Scholkopf,partha.ghosh@tuebingen.mpg.de;msajjadi@tue.mpg.de;antonio.vergari@tuebingen.mpg.de;black@tue.mpg.de;bs@tue.mpg.de,6;8;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,11,0,yes,9/25/19,"Max Planck Institute for Intelligent Systems, Max-Planck Institute;Max Planck Institute for Intelligent Systems, Max-Planck Institute;Max Planck Institute for Intelligent Systems, Max-Planck Institute;Max Planck Institute for Intelligent Systems, Max-Planck Institute;Max Planck Institute for Intelligent Systems, Max-Planck Institute",-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,5,3/29/19,35,20,9,1,13,5,281;672;290;31998;77134,62;13;31;335;860,8;9;10;87;119,22;104;21;3586;9933,m;m
3311,ICLR,2020,GraphAF: a Flow-based Autoregressive Model for Molecular Graph Generation,Chence Shi*;Minkai Xu*;Zhaocheng Zhu;Weinan Zhang;Ming Zhang;Jian Tang,chenceshi@pku.edu.cn;mkxu@apex.sjtu.edu.cn;zhaocheng.zhu@umontreal.ca;wnzhang@sjtu.edu.cn;mzhang_cs@pku.edu.cn;jian.tang@hec.ca,6;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,17,0,yes,9/25/19,Peking University;Shanghai Jiao Tong University;University of Montreal;Shanghai Jiao Tong University;Peking University;HEC Montreal,22;53;128;53;22;128,24;157;85;157;24;85,5;10,9/25/19,9,4,5,1,0,4,34;7;54;4997;23;276,3;6;10;207;76;74,2;1;5;31;2;8,6;3;9;705;3;20,m;m
3312,ICLR,2020,A Theoretical Analysis of the Number of Shots in Few-Shot Learning,Tianshi Cao;Marc T Law;Sanja Fidler,tianshi.cao@mail.utoronto.ca;law@cs.toronto.edu;fidler@cs.toronto.edu,8;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,"Toronto University;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",18;18;18,18;18;18,6,9/25/19,3,3,1,0,0,1,7;330;10743,7;34;160,2;9;49,1;24;1386,u;f
3313,ICLR,2020,Deep Learning of Determinantal Point Processes via Proper Spectral Sub-gradient,Tianshu Yu;Yikang Li;Baoxin Li,tianshuy@asu.edu;yikang.li@asu.edu;baoxin.li@asu.edu,3;8;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,Arizona State University;Arizona State University;Arizona State University,95;95;95,155;155;155,2,9/25/19,0,0,0,0,0,0,70;665;3431,21;72;225,6;12;28,1;80;304,m;m
3314,ICLR,2020,Augmenting Genetic Algorithms with Deep Neural Networks for Exploring the Chemical Space,AkshatKumar Nigam;Pascal Friederich;Mario Krenn;Alan Aspuru-Guzik,akshat.nigam@mail.utoronto.ca;pascal.friederich@utoronto.ca;mario.krenn@utoronto.ca;alan@aspuru.com,8;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,7,0,yes,9/25/19,Toronto University;Toronto University;Toronto University;Toronto University,18;18;18;18,18;18;18;18,5,9/25/19,6,2,1,0,0,1,30;413;2052;12809,4;51;87;348,3;15;22;53,4;6;26;453,m;m
3315,ICLR,2020,Denoising and Regularization via Exploiting the Structural Bias of Convolutional Generators,Reinhard Heckel;Mahdi Soltanolkotabi,reinhard.heckel@tum.de;msoltoon@gmail.com,6;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,5,0,yes,9/25/19,Technical University Munich;University of Southern California,53;31,43;62,1,9/25/19,8,6,0,1,0,0,971;2866,58;56,16;20,117;351,m;m
3316,ICLR,2020,SNOW: Subscribing to Knowledge via Channel Pooling for Transfer & Lifelong Learning of Convolutional Neural Networks,Chungkuk Yoo;Bumsoo Kang;Minsik Cho,ckyoo@ibm.com;steve.kang@kaist.ac.kr;thyeros@gmail.com,8;8;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,6,0,yes,9/25/19,International Business Machines;Korea Advanced Institute of Science and Technology;,-1;481;-1,-1;110;-1,,9/25/19,0,0,0,0,0,0,440;1;936,35;5;58,10;1;17,27;0;114,m;f
3317,ICLR,2020,A Meta-Transfer Objective for Learning to Disentangle Causal Mechanisms,Yoshua Bengio;Tristan Deleu;Nasim Rahaman;Nan Rosemary Ke;Sebastien Lachapelle;Olexa Bilaniuk;Anirudh Goyal;Christopher Pal,yoshua.bengio@mila.quebec;tristan.deleu@gmail.com;nasim.rahaman@tuebingen.mpg.de;rosemary.nan.ke@gmail.com;sebastien.lachapelle@umontreal.ca;obilaniu@gmail.com;anirudhgoyal9119@gmail.com;chris.j.pal@gmail.com,8;8;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,8,0,yes,9/25/19,"University of Montreal;University of Montreal;Max Planck Institute for Intelligent Systems, Max-Planck Institute;Polytechnique Montreal;University of Montreal;University of Montreal;University of Montreal;Ecole Polytechnique de Montreal",128;128;-1;390;128;128;128;390,85;85;-1;1397;85;85;85;1397,10;1;6,1/30/19,47,29,5,1,17,2,208566;94;234;760;57;281;1137;8489,807;13;15;32;3;14;46;120,147;5;6;13;3;7;12;33,24297;6;26;76;2;36;130;764,m;m
3318,ICLR,2020,Classification-Based Anomaly Detection for General Data,Liron Bergman;Yedid Hoshen,liron.bergman@mail.huji.ac.il;yedid@cs.huji.ac.il,6;8;8,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Accept (Poster),0,3,1,yes,9/25/19,Hebrew University of Jerusalem;Hebrew University of Jerusalem,67;67,216;216,8,9/25/19,8,5,4,0,0,1,9;503,2;33,2;10,1;59,m;m
3319,ICLR,2020,B-Spline CNNs on Lie groups,Erik J Bekkers,e.j.bekkers@tue.nl,8;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:N/A:N/A:I did not assess the derivations or theory.,Accept (Poster),0,6,0,yes,9/25/19,Eindhoven University of Technology,205,185,,9/25/19,10,5,4,0,0,3,598,37,13,43,m
3320,ICLR,2020,Global Relational Models of Source Code,Vincent J. Hellendoorn;Charles Sutton;Rishabh Singh;Petros Maniatis;David Bieber,vjhellendoorn@gmail.com;charlessutton@google.com;rising@google.com;maniatis@google.com;dbieber@google.com,6;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),1,8,0,yes,9/25/19,"University of California, Davis;Google;Google;Google;Google",79;-1;-1;-1;-1,55;-1;-1;-1;-1,10,9/25/19,6,2,2,0,0,1,159;50;913;5924;45,7;18;53;87;4,4;3;12;36;2,21;4;96;653;2,m;m
3321,ICLR,2020,NeurQuRI: Neural Question Requirement Inspector for Answerability Prediction in Machine Reading Comprehension,Seohyun Back;Sai Chetan Chinthakindi;Akhil Kedia;Haejun Lee;Jaegul Choo,scv.back@samsung.com;sai.chetan@samsung.com;akhil.kedia@samsung.com;haejun82.lee@samsung.com;jchoo@korea.ac.kr,6;8;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,6,0,yes,9/25/19,Samsung;Samsung;Samsung;Samsung;Korea University,-1;-1;-1;-1;323,-1;-1;-1;-1;179,,9/25/19,3,2,1,0,0,0,29;3;3;125;2415,6;2;2;31;124,3;1;1;6;22,1;0;0;9;389,m;m
3322,ICLR,2020,Adjustable Real-time Style Transfer,Mohammad Babaeizadeh;Golnaz Ghiasi,mb2@uiuc.edu;golnazg@google.com,6;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,"University of Illinois, Urbana-Champaign;Google",3;-1,48;-1,,11/21/18,5,2,1,0,2,0,628;1029,19;21,8;12,94;120,m;f
3323,ICLR,2020,Ranking Policy Gradient,Kaixiang Lin;Jiayu Zhou,linkaixi@msu.edu;jiayuz@msu.edu,6;3;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY,481;481,299;299,1,6/24/19,1,1,0,0,4,0,193;2277,12;126,6;22,25;223,m;m
3324,ICLR,2020,Neural Epitome Search for Architecture-Agnostic Network Compression,Daquan Zhou;Xiaojie Jin;Qibin Hou;Kaixin Wang;Jianchao Yang;Jiashi Feng,zhoudaquan21@gmail.com;jinxiaojie@bytedance.com;andrewhoux@gmail.com;kaixin.wang@u.nus.edu;yangjianchao@bytedance.com;elefjia@nus.edu.sg,6;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0,yes,9/25/19,National University of Singapore;Bytedance;National University of Singapore;National University of Singapore;Bytedance;National University of Singapore,16;-1;16;16;-1;16,25;-1;25;25;-1;25,,7/12/19,0,0,0,0,0,0,23;1062;57;0;2264;9533,8;52;17;5;20;332,2;16;3;0;8;52,8;122;3;0;430;1232,u;m
3325,ICLR,2020,Learning to Guide Random Search,Ozan Sener;Vladlen Koltun,ozansener@gmail.com;vkoltun@gmail.com,6;8;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,Intel;Intel,-1;-1,-1;-1,11,9/25/19,1,0,1,0,0,0,1183;18005,33;191,13;63,213;2537,m;m
3326,ICLR,2020,BERTScore: Evaluating Text Generation with BERT,Tianyi Zhang*;Varsha Kishore*;Felix Wu*;Kilian Q. Weinberger;Yoav Artzi,zty27x@gmail.com;vk352@cornell.edu;fw245@cornell.edu;kqw4@cornell.edu;yoav@cs.cornell.edu,8;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,Cornell University;Cornell University;Cornell University;Cornell University;Cornell University,7;7;7;7;7,19;19;19;19;19,3;4,4/21/19,90,43,60,2,0,28,259;117;929;24393;2195,6;7;18;166;45,3;2;10;54;22,87;28;184;3878;322,m;m
3327,ICLR,2020,Reinforced Genetic Algorithm Learning for Optimizing Computation Graphs,Aditya Paliwal;Felix Gimeno;Vinod Nair;Yujia Li;Miles Lubin;Pushmeet Kohli;Oriol Vinyals,adipal@google.com;fgimeno@google.com;vinair@google.com;yujiali@google.com;mlubin@google.com;pushmeet@google.com;vinyals@google.com,6;6;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,10,5/7/19,6,1,2,0,0,0,32;17;980;516;1507;22578;4573,3;4;58;34;37;313;29,3;2;17;8;17;69;16,5;1;60;51;151;2782;412,m;m
3328,ICLR,2020,Meta-Learning Acquisition Functions for Transfer Learning in Bayesian Optimization,Michael Volpp;Lukas P. Fröhlich;Kirsten Fischer;Andreas Doerr;Stefan Falkner;Frank Hutter;Christian Daniel,mvolpp89@googlemail.com;lukas.froehlich@de.bosch.com;k.fischer-lotte@online.de;andreas.doerr3@de.bosch.com;stefan.falkner@de.bosch.com;fh@cs.uni-freiburg.de;christian.daniel@de.bosch.com,8;8;6,I have published one or two papers in this area.:I carefully checked the experiments.:N/A:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Spotlight),0,5,0,yes,9/25/19,Bosch;Bosch;RWTH Aachen University;Bosch;Bosch;Universität Freiburg;Bosch,-1;-1;95;-1;-1;118;-1,-1;-1;98;-1;-1;85;-1,11;6;8,4/4/19,0,0,0,0,0,0,5;37;876;102;801;13008;970,6;9;52;13;33;233;60,2;2;10;5;11;51;15,0;2;36;13;79;1554;88,m;m
3329,ICLR,2020,Adaptive Correlated Monte Carlo for Contextual Categorical Sequence Generation,Xinjie Fan;Yizhe Zhang;Zhendong Wang;Mingyuan Zhou,xfan@utexas.edu;yizhe.zhang@microsoft.com;zw2533@columbia.edu;mingyuan.zhou@mccombs.utexas.edu,6;8;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,"University of Texas, Austin;Microsoft;Columbia University;University of Texas, Austin",22;-1;15;22,38;-1;16;38,,9/25/19,1,1,0,0,0,0,35;1561;3;2033,4;79;4;115,2;17;1;24,2;175;0;235,m;m
3330,ICLR,2020,Fooling Detection Alone is Not Enough: Adversarial Attack against Multiple Object Tracking,Yunhan Jia;Yantao Lu;Junjie Shen;Qi Alfred Chen;Hao Chen;Zhenyu Zhong;Tao Wei,jack0082010@gmail.com;ylu25@syr.edu;junjies1@uci.edu;alfchen@uci.edu;chen@ucdavis.edu;edwardzhong@baidu.com;lenx.wei@gmail.com,6;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,3,0,yes,9/25/19,"University of Michigan;Syracuse University;University of California, Irvine;University of California, Irvine;University of California, Davis;Baidu;",8;233;35;35;79;-1;-1,21;292;96;96;55;-1;-1,4;2,5/27/19,4,2,0,0,0,0,281;9;80;736;205;162;2596,20;4;30;40;104;34;150,8;3;5;14;6;6;20,23;0;2;61;12;8;231,m;m
3331,ICLR,2020,Pre-training Tasks for Embedding-based Large-scale Retrieval,Wei-Cheng Chang;Felix X. Yu;Yin-Wen Chang;Yiming Yang;Sanjiv Kumar,wchang2@cs.cmu.edu;felixyu@google.com;yinwen@google.com;yiming@cs.cmu.edu;sanjivk@google.com,6;6;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,8,0,yes,9/25/19,Carnegie Mellon University;Google;Google;Carnegie Mellon University;Google,1;-1;-1;1;-1,27;-1;-1;27;-1,,9/25/19,5,4,1,0,0,0,23;2746;551;21623;941,11;66;12;273;142,4;25;5;49;13,1;317;37;2592;113,m;m
3332,ICLR,2020,MACER: Attack-free and Scalable Robust Training via Maximizing Certified Radius,Runtian Zhai;Chen Dan;Di He;Huan Zhang;Boqing Gong;Pradeep Ravikumar;Cho-Jui Hsieh;Liwei Wang,zhairuntian@pku.edu.cn;cdan@cs.cmu.edu;dihe@microsoft.com;huan@huan-zhang.com;boqinggo@outlook.com;pradeepr@cs.cmu.edu;chohsieh@cs.ucla.edu;wanglw@cis.pku.edu.cn,8;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0,yes,9/25/19,"Peking University;Carnegie Mellon University;Microsoft;University of California, Los Angeles;International Computer Science Institute;Carnegie Mellon University;University of California, Los Angeles;Peking University",22;1;-1;20;-1;1;20;22,24;27;-1;17;-1;27;17;24,4,9/25/19,10,5,5,0,0,3,35;31;40;249;3864;8670;292;2004,2;28;29;100;85;181;21;16,2;4;4;6;25;38;4;9,4;3;10;10;755;1214;55;289,m;m
3333,ICLR,2020,Rényi Fair Inference,Sina Baharlouei;Maher Nouiehed;Ahmad Beirami;Meisam Razaviyayn,baharlou@usc.edu;nouiehed@usc.edu;beirami@mit.edu;razaviya@usc.edu,8;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,University of Southern California;University of Southern California;Massachusetts Institute of Technology;University of Southern California,31;31;2;31,62;62;5;62,4;7,6/28/19,10,4,2,0,0,0,11;103;-1;3740,3;11;-1;83,2;5;-1;24,0;10;0;491,m;m
3334,ICLR,2020,An Exponential Learning Rate Schedule for Deep Learning,Zhiyuan Li;Sanjeev Arora,zhiyuanli@cs.princeton.edu;arora@cs.princeton.edu,6;8;8;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Spotlight),0,4,0,yes,9/25/19,Princeton University;Princeton University,31;31,6;6,1;8,9/25/19,13,8,2,2,0,1,675;2317,13;82,10;14,116;254,m;m
3335,ICLR,2020,Graph Convolutional Reinforcement Learning,Jiechuan Jiang;Chen Dun;Tiejun Huang;Zongqing Lu,jiechuan.jiang@pku.edu.cn;cd46@rice.edu;tjhuang@pku.edu.cn;zongqing.lu@pku.edu.cn,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Poster),1,12,0,yes,9/25/19,Peking University;Rice University;Peking University;Peking University,22;84;22;22,24;105;24;24,10,10/22/18,32,15,10,1,76,3,113;35;-1;832,7;9;-1;99,3;2;-1;18,13;3;0;56,u;m
3336,ICLR,2020,In Search for a SAT-friendly Binarized Neural Network Architecture,Nina Narodytska;Hongce Zhang;Aarti Gupta;Toby Walsh,n.narodytska@gmail.com;hongcez@princeton.edu;aartig@cs.princeton.edu;toby.walsh@data61.csiro.au,8;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,6,0,yes,9/25/19,";Princeton University;Princeton University;, CSIRO",-1;31;31;233,-1;6;6;-1,,9/25/19,1,1,1,1,0,1,1486;16;208;512,99;9;41;37,20;2;7;10,150;2;14;63,f;m
3337,ICLR,2020,A Constructive Prediction of the Generalization Error Across Scales,Jonathan S. Rosenfeld;Amir Rosenfeld;Yonatan Belinkov;Nir Shavit,jonsr@mit.edu;amir@eecs.yorku.ca;belinkov@mit.edu;shanir@csail.mit.edu,6;8;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,5,0,yes,9/25/19,Massachusetts Institute of Technology;York University;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;172;2;2,5;416;5;5,8,9/25/19,12,5,1,0,0,0,12;215;1578;9494,4;22;63;223,1;7;21;48,0;25;163;1341,m;m
3338,ICLR,2020,Improving Generalization in Meta Reinforcement Learning using Learned Objectives,Louis Kirsch;Sjoerd van Steenkiste;Juergen Schmidhuber,louis@idsia.ch;sjoerd@idsia.ch;juergen@idsia.ch,6;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:N/A:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Spotlight),0,7,1,yes,9/25/19,IDSIA;IDSIA;IDSIA,-1;-1;-1,-1;-1;-1,,9/25/19,9,4,0,0,0,0,63;267;6704,7;12;117,3;7;27,6;33;638,m;m
3339,ICLR,2020,Exploratory Not Explanatory: Counterfactual Analysis of Saliency Maps for Deep Reinforcement Learning,Akanksha Atrey;Kaleigh Clary;David Jensen,aatrey@cs.umass.edu;kclary@cs.umass.edu;jensen@cs.umass.edu,8;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,8,0,yes,9/25/19,"University of Massachusetts, Amherst;University of Massachusetts, Amherst;University of Massachusetts, Amherst",28;28;28,209;209;209,,9/25/19,2,0,0,0,0,0,24;17;21,11;8;13,3;2;2,1;0;1,f;m
3340,ICLR,2020,A Generalized Training Approach for Multiagent Learning,Paul Muller;Shayegan Omidshafiei;Mark Rowland;Karl Tuyls;Julien Perolat;Siqi Liu;Daniel Hennes;Luke Marris;Marc Lanctot;Edward Hughes;Zhe Wang;Guy Lever;Nicolas Heess;Thore Graepel;Remi Munos,pmuller@google.com;somidshafiei@google.com;markrowland@google.com;karltuyls@google.com;perolat@google.com;liusiqi@google.com;hennes@google.com;marris@google.com;lanctot@google.com;edwardhughes@google.com;zhewang@google.com;guylever@google.com;heess@google.com;thore@google.com;munos@google.com,8;8;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,10,0,yes,9/25/19,Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,,9/25/19,7,5,2,1,0,1,171;514;847;3780;694;59;687;258;10914;152;233;2068;11597;19274;1747,42;37;36;282;38;22;72;4;70;20;57;30;104;161;30,7;12;12;33;16;5;14;4;23;7;8;14;37;45;13,10;32;58;278;44;7;43;10;752;13;15;322;1644;1406;261,m;m
3341,ICLR,2020,Scalable Model Compression by Entropy Penalized Reparameterization,Deniz Oktay;Johannes Ballé;Saurabh Singh;Abhinav Shrivastava,doktay@princeton.edu;jballe@google.com;saurabhsingh@google.com;abhinav@cs.umd.edu,6;8;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,"Princeton University;Google;Google;University of Maryland, College Park",31;-1;-1;12,6;-1;-1;91,,6/15/19,4,1,1,0,6,0,13;1192;4209;3560,9;41;271;56,2;14;30;16,0;228;264;350,m;m
3342,ICLR,2020,Rotation-invariant clustering of neuronal responses in primary visual cortex,Ivan Ustyuzhaninov;Santiago A. Cadena;Emmanouil Froudarakis;Paul G. Fahey;Edgar Y. Walker;Erick Cobos;Jacob Reimer;Fabian H. Sinz;Andreas S. Tolias;Matthias Bethge;Alexander S. Ecker,ivan.ustyuzhaninov@bethgelab.org;santiago.cadena@bethgelab.org;froudara@bcm.edu;paul.fahey@bcm.edu;eywalker@bcm.edu;ecobos@bcm.edu;reimer@bcm.edu;fabian.sinz@bcm.edu;astolias@bcm.edu;matthias@bethgelab.org;alexander.ecker@uni-tuebingen.de,8;8;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,7,0,yes,9/25/19,"Centre for Integrative Neuroscience, AG Bethge;Centre for Integrative Neuroscience, AG Bethge;Baylor College of Medicine;Baylor College of Medicine;Baylor College of Medicine;Baylor College of Medicine;Baylor College of Medicine;Baylor College of Medicine;Baylor College of Medicine;Centre for Integrative Neuroscience, AG Bethge;University of Tuebingen",-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;154,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;91,5,9/25/19,0,0,0,0,0,0,84;66;837;96;126;46;716;1847;6160;11714;6377,13;8;36;24;18;10;34;44;176;414;124,5;4;11;5;6;4;12;17;39;47;24,8;3;78;4;8;1;33;262;517;1275;813,m;m
3343,ICLR,2020,Ensemble Distribution Distillation,Andrey Malinin;Bruno Mlodozeniec;Mark Gales,am969@yandex-team.ru;bkm28@cam.ac.uk;mjfg@eng.cam.ac.uk,6;6;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,5,0,yes,9/25/19,Yandex;University of Cambridge;University of Cambridge,-1;71;71,-1;3;3,,4/30/19,18,11,9,4,2,3,257;14;10544,23;2;369,8;1;48,31;3;1047,m;m
3344,ICLR,2020,ES-MAML: Simple Hessian-Free Meta Learning,Xingyou Song;Wenbo Gao;Yuxiang Yang;Krzysztof Choromanski;Aldo Pacchiano;Yunhao Tang,xsong@berkeley.edu;wg2279@columbia.edu;yxyang@google.com;kchoro@google.com;pacchiano@berkeley.edu;yt2541@columbia.edu,8;8;1;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,7,0,yes,9/25/19,University of California Berkeley;Columbia University;Google;Google;University of California Berkeley;Columbia University,5;15;-1;-1;5;15,13;16;-1;-1;13;16,,9/25/19,12,7,5,0,0,2,29;90;36;901;99;95,15;23;30;92;40;29,3;7;3;15;6;7,4;12;2;79;8;4,m;m
3345,ICLR,2020,SAdam: A Variant of Adam for Strongly Convex Functions,Guanghui Wang;Shiyin Lu;Quan Cheng;Wei-wei Tu;Lijun Zhang,guhuwang@gmail.com;lsy1116@qq.com;chengquangm@gmail.com;tuwwcn@gmail.com;zljzju@gmail.com,8;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,8,0,yes,9/25/19,Zhejiang University;Zhejiang University;Zhejiang University;;Zhejiang University,56;56;56;-1;56,107;107;107;-1;107,1,5/8/19,8,7,1,2,2,1,473;58;-1;24;3490,51;18;-1;4;360,8;5;-1;3;29,34;7;0;1;320,m;m
3346,ICLR,2020,Continual Learning with Bayesian Neural Networks for Non-Stationary Data,Richard Kurle;Botond Cseke;Alexej Klushyn;Patrick van der Smagt;Stephan Günnemann,richard.kurle@tum.de;botond.cseke@argmax.ai;a.klushyn@tum.de;smagt@argmax.ai;guennemann@in.tum.de,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,7,0,yes,9/25/19,"Technical University Munich;Volkswagen Group, ML Research Lab;Technical University Munich;Volkswagen Group, ML Research Lab;Technical University Munich",53;-1;53;-1;53,43;-1;43;-1;43,11,9/25/19,0,0,0,0,0,0,67;194;49;3990;2528,6;8;8;123;139,3;5;4;26;28,5;22;3;419;291,m;m
3347,ICLR,2020,Automated curriculum generation through setter-solver interactions,Sebastien Racaniere;Andrew Lampinen;Adam Santoro;David Reichert;Vlad Firoiu;Timothy Lillicrap,lampinen@stanford.edu;sracaniere@google.com;adamsantoro@google.com;reichert@google.com;vladfi@google.com;countzero@google.com,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,Stanford University;Google;Google;Google;Google;Google,4;-1;-1;-1;-1;-1,4;-1;-1;-1;-1;-1,,9/25/19,8,3,0,0,0,0,797;91;3065;958;442;24158,32;22;35;18;7;74,12;5;20;11;4;39,79;6;347;87;88;2943,m;m
3348,ICLR,2020,NAS-Bench-201: Extending the Scope of Reproducible Neural Architecture Search,Xuanyi Dong;Yi Yang,xuanyi.dxy@gmail.com;yi.yang@uts.edu.au,8;8;8,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:N/A:N/A,Accept (Spotlight),0,14,2,yes,9/25/19,University of Technology Sydney;University of Technology Sydney,108;108,193;193,10,9/25/19,36,19,19,4,0,12,267;2823,12;321,7;15,42;288,m;m
3349,ICLR,2020,Dynamic Time Lag Regression: Predicting What & When,Mandar Chandorkar;Cyril Furtlehner;Bala Poduval;Enrico Camporeale;Michele Sebag,mandar.chandorkar@cwi.nl;furtlehn@lri.fr;bala.poduval@unh.edu;e.camporeale@cwi.nl;michele.sebag@lri.fr,6;6;6;8,I do not know much about this area.:I did not assess the experiments.:N/A:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,"Centrum voor Wiskunde en Informatica;CNRS, Université Paris-Saclay;University of New Hampshire;Centrum voor Wiskunde en Informatica;CNRS, Université Paris-Saclay",-1;-1;266;-1;-1,-1;-1;1397;-1;-1,11;1,9/25/19,0,0,0,0,0,0,38;413;43;625;271,13;77;25;95;51,4;11;4;15;9,1;29;2;33;27,m;f
3350,ICLR,2020,Double Neural Counterfactual Regret Minimization,Hui Li;Kailiang Hu;Shaohua Zhang;Yuan Qi;Le Song,ken.lh@antfin.com;hkl163251@antfin.com;yaohua.zsh@antfin.com;yuan.qi@antfin.com;lsong@cc.gatech.edu,6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,2,0,yes,9/25/19,Alibaba Group and Ant Financial Services Group;Antfin;Antfin;Antfin;Georgia Institute of Technology,-1;-1;-1;-1;13,-1;-1;-1;-1;38,,12/27/18,9,5,3,0,4,2,15391;11;-1;6302;9519,1304;4;-1;253;329,55;2;-1;32;54,941;2;0;667;1114,u;m
3351,ICLR,2020,Robust anomaly detection and backdoor attack detection via differential privacy,Min Du;Ruoxi Jia;Dawn Song,min.du@berkeley.edu;ruoxijia@berkeley.edu;dawnsong@berkeley.edu,6;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,13;13;13,4,9/25/19,6,3,0,1,0,1,267;414;37432,66;36;278,9;13;95,32;24;4087,f;f
3352,ICLR,2020,Optimistic Exploration even with a Pessimistic Initialisation,Tabish Rashid;Bei Peng;Wendelin Boehmer;Shimon Whiteson,tabish.rashid@cs.ox.ac.uk;bei.peng@cs.ox.ac.uk;wendelin.boehmer@cs.ox.ac.uk;shimon.whiteson@cs.ox.ac.uk,6;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,7,0,yes,9/25/19,University of Oxford;University of Oxford;University of Oxford;University of Oxford,50;50;50;50,1;1;1;1,,9/25/19,3,2,3,0,0,2,290;489;166;5445,10;71;18;203,5;11;8;38,74;17;13;588,m;m
3353,ICLR,2020,Training Generative Adversarial Networks from Incomplete Observations using Factorised Discriminators,Daniel Stoller;Sebastian Ewert;Simon Dixon,d.stoller@qmul.ac.uk;sewert@spotify.com;s.e.dixon@qmul.ac.uk,6;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,Queen Mary University London;Queen Mary University London;Queen Mary University London,233;233;233,110;110;110,5;4;2,5/29/19,3,0,0,0,3,0,156;1155;5721,20;64;360,4;17;39,31;112;504,m;m
3354,ICLR,2020,Mathematical Reasoning in Latent Space,Dennis Lee;Christian Szegedy;Markus Rabe;Sarah Loos;Kshitij Bansal,ldennis@google.com;szegedy@google.com;mrabe@google.com;smoos@google.com;kbk@google.com,8;8;8,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,7,0,yes,9/25/19,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,1;10,9/25/19,6,2,1,0,0,1,239;67199;642;627;62,24;37;38;35;21,6;17;14;14;5,19;9260;83;38;9,u;m
3355,ICLR,2020,Deep Symbolic Superoptimization Without Human Knowledge,Hui Shi;Yang Zhang;Xinyun Chen;Yuandong Tian;Jishen Zhao,hshi@ucsd.edu;yang.zhang2@ibm.com;xinyun.chen@berkeley.edu;yuandong@fb.com;jzhao@ucsd.edu,6;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0,yes,9/25/19,"University of California, San Diego;International Business Machines;University of California Berkeley;Facebook;University of California, San Diego",11;-1;5;-1;11,31;-1;13;-1;31,,9/25/19,0,0,0,0,0,0,7;317;46;2498;1982,15;67;9;85;82,2;8;2;25;20,0;29;4;293;250,f;f
3356,ICLR,2020,Neural Execution of Graph Algorithms,Petar Veličković;Rex Ying;Matilde Padovano;Raia Hadsell;Charles Blundell,petarv@google.com;rexying@stanford.edu;mp861@cam.ac.uk;raia@google.com;cblundell@google.com,8;8;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,4,0,yes,9/25/19,Google;Stanford University;University of Cambridge;Google;Google,-1;4;71;-1;-1,-1;4;3;-1;-1,10,9/25/19,10,6,4,1,0,2,1620;1921;9;8331;6176,32;22;1;63;50,9;12;1;26;22,419;313;2;804;1088,m;m
3357,ICLR,2020,Encoding word order in complex embeddings,Benyou Wang;Donghao Zhao;Christina Lioma;Qiuchi Li;Peng Zhang;Jakob Grue Simonsen,wang@dei.unipd.it;zhaodh@tju.edu.cn;chrh@di.ku.dk;qiuchili@dei.unipd.it;pzhang@tju.edu.cn;simonsen@di.ku.dk,6;8;6;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,9,0,yes,9/25/19,Universita' degli studi di Padova;Zhejiang University;University of Copenhagen;Universita' degli studi di Padova;Zhejiang University;University of Copenhagen,-1;56;100;-1;56;100,-1;107;101;-1;107;101,3,9/25/19,3,0,2,0,0,0,427;42;1526;100;1;889,28;9;90;23;15;101,10;3;15;6;1;15,51;4;154;6;0;92,m;m
3358,ICLR,2020,Deep Double Descent: Where Bigger Models and More Data Hurt,Preetum Nakkiran;Gal Kaplun;Yamini Bansal;Tristan Yang;Boaz Barak;Ilya Sutskever,preetum@cs.harvard.edu;galkaplun@g.harvard.edu;ybansal@g.harvard.edu;tristanyang@college.harvard.edu;b@boazbarak.org;ilyasu@openai.com,8;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,4,1,yes,9/25/19,Harvard University;Harvard University;Harvard University;Harvard University;Harvard University;OpenAI,39;39;39;39;39;-1,7;7;7;7;7;-1,,9/25/19,49,26,7,3,0,6,362;65;191;60;6831;134431,27;6;6;3;210;90,8;3;3;2;44;53,26;7;36;7;572;17068,m;m
3359,ICLR,2020,The Early Phase of Neural Network Training,Jonathan Frankle;David J. Schwab;Ari S. Morcos,jfrankle@mit.edu;dschwab@gc.cuny.edu;arimorcos@gmail.com,8;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,Massachusetts Institute of Technology;The City College of New York;Facebook,2;205;-1,5;1397;-1,,9/25/19,8,6,1,1,0,2,647;37;1034,22;30;32,9;2;12,99;2;116,m;m
3360,ICLR,2020,"Don't Use Large Mini-batches, Use Local SGD",Tao Lin;Sebastian U. Stich;Kumar Kshitij Patel;Martin Jaggi,tao.lin@epfl.ch;sebastian.stich@epfl.ch;kumarkshitijpatel@gmail.com;martin.jaggi@epfl.ch,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),7,4,0,yes,9/25/19,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Toyota Technological Institute at Chicago;Swiss Federal Institute of Technology Lausanne,481;481;-1;481,38;38;-1;38,8,8/22/18,91,58,32,1,27,11,163;984;-1;3856,7;48;-1;114,6;16;-1;27,17;142;0;577,m;m
3361,ICLR,2020,A Closer Look at the Optimization Landscapes of Generative Adversarial Networks,Hugo Berard;Gauthier Gidel;Amjad Almahairi;Pascal Vincent;Simon Lacoste-Julien,berard.hugo@gmail.com;gauthier.gidel@umontreal.ca;amjadmahayri@gmail.com;vincentp@iro.umontreal.ca;slacoste@iro.umontreal.ca,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,7,0,yes,9/25/19,University of Montreal;University of Montreal;Element AI;University of Montreal;University of Montreal,128;128;-1;128;128,85;85;-1;85;85,5;4;9,6/11/19,14,2,2,1,0,0,107;326;2074;15950;3838,5;23;11;118;76,3;10;7;34;27,21;47;172;1344;610,m;m
3362,ICLR,2020,Learning to Control PDEs with Differentiable Physics,Philipp Holl;Nils Thuerey;Vladlen Koltun,philipp.holl@tum.de;nils.thuerey@tum.de;vkoltun@gmail.com,6;8;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,6,0,yes,9/25/19,Technical University Munich;Technical University Munich;Intel,53;53;-1,43;43;-1,,9/25/19,12,8,3,0,0,1,37;2623;18005,5;121;191,2;32;63,2;189;2537,m;m
3363,ICLR,2020,Compression based bound for non-compressed network: unified generalization error analysis of large compressible deep neural network,Taiji Suzuki;Hiroshi Abe;Tomoaki Nishimura,taiji@mist.i.u-tokyo.ac.jp;abe@ipride.co.jp;tomoaki.nishimura@nttdata.com,8;6;6,I have published in this field for several years.:N/A:N/A:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,8,0,yes,9/25/19,The University of Tokyo;;Nttdata,56;-1;-1,36;-1;-1,1;8,9/25/19,3,0,1,0,0,0,2318;-1;-1,177;-1;-1,26;-1;-1,237;0;0,m;m
3364,ICLR,2020,Rethinking the Hyperparameters for Fine-tuning,Hao Li;Pratik Chaudhari;Hao Yang;Michael Lam;Avinash Ravichandran;Rahul Bhotika;Stefano Soatto,hao.li.ict@gmail.com;pratikac@seas.upenn.edu;lancelot365@gmail.com;michlam@amazon.com;avinash.a.ravichandran@gmail.com;bhotikar@amazon.com;soatto@ucla.edu,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0,yes,9/25/19,"Amazon;University of Pennsylvania;Amazon;Amazon;Amazon;Amazon;University of California, Los Angeles",-1;19;-1;-1;-1;-1;20,-1;11;-1;-1;-1;-1;17,6;2,9/25/19,2,0,1,0,0,0,203;6;3;53;1333;404;15761,24;12;12;20;38;41;457,8;2;1;4;14;11;62,12;0;0;2;230;31;1445,m;m
3365,ICLR,2020,PAC Confidence Sets for Deep Neural Networks via Calibrated Prediction,Sangdon Park;Osbert Bastani;Nikolai Matni;Insup Lee,sangdonp@cis.upenn.edu;obastani@seas.upenn.edu;nmatni@seas.upenn.edu;lee@cis.upenn.edu,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0,yes,9/25/19,University of Pennsylvania;University of Pennsylvania;University of Pennsylvania;University of Pennsylvania,19;19;19;19,11;11;11;11,8,9/25/19,3,2,0,0,0,0,217;718;1036;12161,40;49;80;633,8;13;18;53,14;56;68;919,m;m
3366,ICLR,2020,Deep Semi-Supervised Anomaly Detection,Lukas Ruff;Robert A. Vandermeulen;Nico Görnitz;Alexander Binder;Emmanuel Müller;Klaus-Robert Müller;Marius Kloft,contact@lukasruff.com;vandermeulen@cs.uni-kl.de;nico.goernitz@tu-berlin.de;alexander_binder@sutd.edu.sg;mueller@bit.uni-bonn.de;klaus-robert.mueller@tu-berlin.de;kloft@cs.uni-kl.de,6;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,TU Berlin;TU Kaiserslautern;TU Berlin;Singapore University of Technology and Design;University of Bonn;TU Berlin;TU Kaiserslautern,108;154;108;481;128;108;154,149;601;149;1397;106;149;601,,6/6/19,18,10,6,1,0,3,221;236;586;3077;2304;4463;2064,8;14;30;87;110;125;98,3;5;11;17;25;28;23,51;53;66;377;266;319;260,m;m
3367,ICLR,2020,AtomNAS: Fine-Grained End-to-End Neural Architecture Search,Jieru Mei;Yingwei Li;Xiaochen Lian;Xiaojie Jin;Linjie Yang;Alan Yuille;Jianchao Yang,meijieru@gmail.com;yingwei.li@jhu.edu;xiaochen.lian@bytedance.com;jinxiaojie@bytedance.com;linjie.yang@bytedance.com;alan.l.yuille@gmail.com;yangjianchao@bytedance.com,6;6;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,4,0,yes,9/25/19,Johns Hopkins University;Johns Hopkins University;Bytedance;Bytedance;Bytedance;Johns Hopkins University;Bytedance,73;73;-1;-1;-1;73;-1,12;12;-1;-1;-1;12;-1,,9/25/19,8,3,2,0,0,1,28;39;148;1062;40;33367;2264,7;14;8;52;10;495;20,3;4;4;16;3;80;8,1;2;9;122;12;3754;430,m;m
3368,ICLR,2020,"Triple Wins: Boosting Accuracy, Robustness and Efficiency Together by Enabling Input-Adaptive Inference",Ting-Kuei Hu;Tianlong Chen;Haotao Wang;Zhangyang Wang,tkhu@tamu.edu;wiwjp619@tamu.edu;htwang@tamu.edu;atlaswang@tamu.edu,8;8;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Accept (Poster),0,4,0,yes,9/25/19,Texas A&M;Texas A&M;Texas A&M;Texas A&M,44;44;44;44,177;177;177;177,4,9/25/19,6,4,0,0,0,0,27;94;21;2947,6;36;7;167,4;5;3;28,1;14;1;383,m;m
3369,ICLR,2020,CLEVRER: Collision Events for Video Representation and Reasoning,Kexin Yi*;Chuang Gan*;Yunzhu Li;Pushmeet Kohli;Jiajun Wu;Antonio Torralba;Joshua B. Tenenbaum,kyi@g.harvard.edu;ganchuang1990@gmail.com;liyunzhu@mit.edu;pushmeet@google.com;jiajunwu@mit.edu;torralba@mit.edu;jbt@mit.edu,6;8;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Accept (Spotlight),0,4,0,yes,9/25/19,Harvard University;International Business Machines;Massachusetts Institute of Technology;Google;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,39;-1;2;-1;2;2;2,7;-1;5;-1;5;5;5,,9/25/19,17,8,1,1,0,1,160;2233;428;22578;4039;3085;31160,13;81;12;313;89;66;594,5;26;9;69;30;13;83,16;230;41;2782;386;448;2697,u;m
3370,ICLR,2020,Computation Reallocation for Object Detection,Feng Liang;Chen Lin;Ronghao Guo;Ming Sun;Wei Wu;Junjie Yan;Wanli Ouyang,liangfeng@sensetime.com;linchen@sensetime.com;guoronghao@sensetime.com;sunming1@sensetime.com;wuwei@sensetime.com;yanjunjie@sensetime.com;wanli.ouyang@sydney.edu.au,3;6;6;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0,yes,9/25/19,SenseTime Group Limited;SenseTime Group Limited;SenseTime Group Limited;SenseTime Group Limited;SenseTime Group Limited;SenseTime Group Limited;University of Sydney,-1;-1;-1;-1;-1;-1;86,-1;-1;-1;-1;-1;-1;60,2,9/25/19,3,1,1,0,0,0,505;74;10;569;339;7065;10918,26;56;6;44;110;166;150,5;4;2;7;5;43;53,57;4;0;25;74;996;1201,m;m
3371,ICLR,2020,Making Efficient Use of Demonstrations to Solve Hard Exploration Problems,Caglar Gulcehre;Tom Le Paine;Bobak Shahriari;Misha Denil;Matt Hoffman;Hubert Soyer;Richard Tanburn;Steven Kapturowski;Neil Rabinowitz;Duncan Williams;Gabriel Barth-Maron;Ziyu Wang;Nando de Freitas;Worlds Team,caglarg@google.com;tpaine@google.com;bshahr@google.com;mdenil@google.com;mwhoffman@google.com;soyer@google.com;tanburn@google.com;skapturowski@google.com;ncr@google.com;duncanwilliams@google.com;gabrielbm@google.com;ziyu@google.com;nandodefreitas@google.com;deepmind-worlds-team@google.com,6;6;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,4,0,yes,9/25/19,Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,,9/3/19,9,6,3,1,0,2,19800;525;1294;3379;8619;2212;40;106;3060;9;603;4319;19467;7,36;17;16;38;94;22;8;8;37;13;14;51;184;1,26;9;6;20;29;11;4;4;17;2;9;21;55;1,3009;66;117;285;1226;272;1;26;398;1;75;492;1854;1,m;m
3372,ICLR,2020,Robust And Interpretable Blind Image Denoising Via Bias-Free Convolutional Neural Networks,Sreyas Mohan;Zahra Kadkhodaie;Eero P. Simoncelli;Carlos Fernandez-Granda,sm7582@nyu.edu;zk388@nyu.edu;eero.simoncelli@nyu.edu;cfgranda@cims.nyu.edu,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,4,1,yes,9/25/19,New York University;New York University;New York University;New York University,25;25;25;25,29;29;29;29,8,6/13/19,2,0,0,0,2,0,37;0;28534;945,10;3;299;24,4;0;72;9,0;0;3137;86,m;m
3373,ICLR,2020,Permutation Equivariant Models for Compositional Generalization in Language,Jonathan Gordon;David Lopez-Paz;Marco Baroni;Diane Bouchacourt,jg801@cam.ac.uk;dlp@fb.com;mbaroni@fb.com;dianeb@fb.com,8;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0,yes,9/25/19,University of Cambridge;Facebook;Facebook;Facebook,71;-1;-1;-1,3;-1;-1;-1,3;8,9/25/19,4,4,1,0,0,1,293;2483;114;197,22;46;24;16,8;19;6;5,22;428;7;20,m;f
3374,ICLR,2020,Automated Relational Meta-learning,Huaxiu Yao;Xian Wu;Zhiqiang Tao;Yaliang Li;Bolin Ding;Ruirui Li;Zhenhui Li,huaxiuyao@psu.edu;xwu9@nd.edu;zqtao@ece.neu.edu;yaliangl.ub@gmail.com;bolin.ding@alibaba-inc.com;rrli@cs.ucla.edu;jessieli@ist.psu.edu,8;8;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,5,0,yes,9/25/19,"Pennsylvania State University;University of Notre Dame;Northeastern University;;Alibaba Group;University of California, Los Angeles;Pennsylvania State University",41;118;16;-1;-1;20;41,78;157;906;-1;-1;17;78,6;10,9/25/19,7,5,2,0,0,0,502;49;430;1634;3761;5;3507,27;83;31;114;99;4;117,9;4;11;18;30;1;30,72;3;51;165;386;0;323,m;f
3375,ICLR,2020,A Probabilistic Formulation of Unsupervised Text Style Transfer,Junxian He;Xinyi Wang;Graham Neubig;Taylor Berg-Kirkpatrick,junxianh@cs.cmu.edu;xinyiw1@cs.cmu.edu;gneubig@cs.cmu.edu;tberg@eng.ucsd.edu,6;6;8,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A,Accept (Spotlight),0,4,0,yes,9/25/19,"Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;University of California, San Diego",1;1;1;11,27;27;27;31,3;4;5,9/25/19,2,0,2,0,0,0,248;194;5418;1984,14;67;442;48,8;9;38;20,45;10;557;342,m;m
3376,ICLR,2020,Discrepancy Ratio: Evaluating Model Performance When Even Experts Disagree on the Truth,Igor Lovchinsky;Alon Daks;Israel Malkin;Pouya Samangouei;Ardavan Saeedi;Yang Liu;Swami Sankaranarayanan;Tomer Gafner;Ben Sternlieb;Patrick Maher;Nathan Silberman,ilovchinsky@butterflynetwork.com;adaks@butterflynetwork.com;imalkin@butterflynetwork.com;psamangouei@butterflynetwork.com;asaeedi@butterflynetwork.com;yliu@butterflynetwork.com;ssankaranarayanan@butterflynetwork.com;tgafner@butterflynetwork.com;bsternlieb@butterflynetwork.com;pmaher@butterflynetwork.com;nsilberman@butterflynetwork.com,8;6;6,I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0,yes,9/25/19,Butterfly Network Inc;Butterfly Network Inc;Butterfly Network Inc;Butterfly Network Inc;Butterfly Network Inc;Butterfly Network Inc;Butterfly Network Inc;Butterfly Network Inc;Butterfly Network Inc;Butterfly Network Inc;Butterfly Network Inc,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,,9/25/19,0,0,0,0,0,0,767;10;45;709;882;63;1160;0;0;1019;4479,25;8;13;14;22;28;31;2;1;172;22,9;2;3;7;8;4;14;0;0;19;13,17;3;3;98;79;9;109;0;0;90;783,m;m
3377,ICLR,2020,Combining Q-Learning and Search with Amortized Value Estimates,Jessica B. Hamrick;Victor Bapst;Alvaro Sanchez-Gonzalez;Tobias Pfaff;Theophane Weber;Lars Buesing;Peter W. Battaglia,jhamrick@google.com;vbapst@google.com;alvarosg@google.com;tpfaff@google.com;theophane@google.com;lbuesing@google.com;peterbattaglia@google.com,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,10,0,yes,9/25/19,Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,,9/25/19,5,1,2,0,0,0,2301;1614;493;694;272;2457;4598,51;32;29;29;20;54;88,15;15;13;10;7;21;29,167;167;34;54;35;259;424,f;m
3378,ICLR,2020,Generalized Convolutional Forest Networks for Domain Generalization and Visual Recognition,Jongbin Ryu;Gitaek Kwon;Ming-Hsuan Yang;Jongwoo Lim,jongbin.ryu@gmail.com;kwongitack@gmail.com;mhyang@ucmerced.edu;jlim@hanyang.ac.kr,6;3;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,Hanyang University;Hanyang University;University of California at Merced;Hanyang University,233;233;481;233,393;393;354;393,8,9/25/19,0,0,0,0,0,0,3;0;5518;9182,6;1;126;73,1;0;26;26,1;0;1260;2058,m;m
3379,ICLR,2020,Graph Constrained Reinforcement Learning for Natural Language Action Spaces,Prithviraj Ammanabrolu;Matthew Hausknecht,raj.ammanabrolu@gatech.edu;matthew.hausknecht@microsoft.com,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,3,0,yes,9/25/19,Georgia Institute of Technology;Microsoft,13;-1,38;-1,3;10,9/25/19,4,4,2,0,0,0,128;2961,12;38,4;17,8;322,m;m
3380,ICLR,2020,Improved memory in recurrent neural networks with sequential non-normal dynamics,Emin Orhan;Xaq Pitkow,aeminorhan@gmail.com;xaq@rice.edu,6;8;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0,yes,9/25/19,New York University;Rice University,25;84,29;105,,5/31/19,2,1,1,0,0,0,251;900,20;33,8;11,15;53,m;m
3381,ICLR,2020,A Learning-based Iterative Method for Solving Vehicle Routing Problems,Hao Lu;Xingwen Zhang;Shuang Yang,haolu@princeton.edu;xingwen.zhang@antfin.com;shuang.yang@antfin.com,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:N/A:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0,yes,9/25/19,Princeton University;Antfin;Antfin,31;-1;-1,6;-1;-1,,9/25/19,3,1,2,0,0,1,112;8;935,56;7;57,6;2;6,13;1;88,m;m
3382,ICLR,2020,Emergence of functional and structural properties of the head direction system by optimization of recurrent neural networks,Christopher J. Cueva;Peter Y. Wang;Matthew Chin;Xue-Xin Wei,ccueva@gmail.com;peterwang724@gmail.com;mattchin35@gmail.com;weixxpku@gmail.com,6;8;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Accept (Spotlight),0,4,0,yes,9/25/19,Columbia University;Columbia University;;Columbia University,15;15;-1;15,16;16;-1;16,,9/25/19,1,0,0,0,0,0,284;12;311;323,9;3;56;11,6;1;10;3,20;0;18;23,m;m
3383,ICLR,2020,GenDICE: Generalized Offline Estimation of Stationary Values,Ruiyi Zhang*;Bo Dai*;Lihong Li;Dale Schuurmans,ryzhang@cs.duke.edu;bodai@google.com;lihongli.cs@gmail.com;schuurmans@google.com,8;8;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Talk),0,6,0,yes,9/25/19,Duke University;Google;Google;Google,47;-1;-1;-1,20;-1;-1;-1,1,9/25/19,11,7,7,0,0,2,238;418;47;193,30;58;9;37,10;8;3;8,32;64;12;27,m;m
3384,ICLR,2020,Adversarially robust transfer learning,Ali Shafahi;Parsa Saadatpanah;Chen Zhu;Amin Ghiasi;Christoph Studer;David Jacobs;Tom Goldstein,ashafahi@cs.umd.edu;parsa@cs.umd.edu;chenzhu@cs.umd.edu;amin@cs.umd.edu;studer@cornell.edu;djacobs@cs.umd.edu;tomg@cs.umd.edu,8;8;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0,yes,9/25/19,"University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park;Cornell University;University of Maryland, College Park;University of Maryland, College Park",12;12;12;12;7;12;12,91;91;91;91;19;91;91,4;6;8,5/20/19,7,4,1,1,0,2,453;39;641;120;211;842;5954,33;6;75;6;57;54;98,8;3;15;4;9;11;27,47;2;54;16;18;48;731,m;m
3385,ICLR,2020,Learning from Explanations with Neural Execution Tree,Ziqi Wang*;Yujia Qin*;Wenxuan Zhou;Jun Yan;Qinyuan Ye;Leonardo Neves;Zhiyuan Liu;Xiang Ren,ziqi-wan16@mails.tsinghua.edu.cn;qinyj16@mails.tsinghua.edu.cn;zhouwenx@usc.edu;yanjun@usc.edu;qinyuany@usc.edu;lneves@snap.com;liuzy@tsinghua.edu.cn;xiangren@usc.edu,8;8;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,6,0,yes,9/25/19,Tsinghua University;Tsinghua University;University of Southern California;University of Southern California;University of Southern California;Snap Inc.;Tsinghua University;University of Southern California,8;8;31;31;31;-1;8;31,23;23;62;62;62;-1;23;62,3;8,9/25/19,4,4,2,1,0,0,11;199;892;23;9;22;689;1380,16;19;30;3;5;31;53;121,2;6;8;3;2;3;11;20,0;6;96;1;1;1;139;57,m;m
3386,ICLR,2020,Universal Approximation with Certified Networks,Maximilian Baader;Matthew Mirman;Martin Vechev,mbaader@inf.ethz.ch;matthew.mirman@inf.ethz.ch;martin.vechev@inf.ethz.ch,3;8;6,I have published in this field for several years.:N/A:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:N/A:N/A:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),1,7,0,yes,9/25/19,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10,13;13;13,4;1,9/25/19,2,0,0,0,0,0,12;493;4246,5;15;153,2;4;36,1;64;467,m;m
3387,ICLR,2020,Extreme Classification via Adversarial Softmax Approximation,Robert Bamler;Stephan Mandt,rbamler@uci.edu;stephan.mandt@gmail.com,6;3;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,"University of California, Irvine;University of California, Irvine",35;35,96;96,4;1,9/25/19,2,0,1,0,0,0,332;1300,31;67,8;17,17;119,m;m
3388,ICLR,2020,Synthesizing Programmatic Policies that Inductively Generalize,Jeevana Priya Inala;Osbert Bastani;Zenna Tavares;Armando Solar-Lezama,jinala@csail.mit.edu;obastani@seas.upenn.edu;zenna@mit.edu;asolar@csail.mit.edu,8;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,11,0,yes,9/25/19,Massachusetts Institute of Technology;University of Pennsylvania;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;19;2;2,5;11;5;5,8,9/25/19,0,0,0,0,0,0,81;718;9;3955,13;49;8;132,5;13;2;29,8;56;1;396,f;m
3389,ICLR,2020,Image-guided Neural Object Rendering,Justus Thies;Michael Zollhöfer;Christian Theobalt;Marc Stamminger;Matthias Nießner,justus.thies@tum.de;michael@zollhoefer.com;marc.stamminger@fau.de;theobalt@mpi-inf.mpg.de;niessner@tum.de,6;8;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,1,0,yes,9/25/19,"Technical University Munich;Facebook;University of Erlangen-Nuremberg;Saarland Informatics Campus, Max-Planck Institute;Technical University Munich",53;-1;205;-1;53,43;-1;182;-1;43,5,11/26/18,15,6,6,1,5,0,1675;19219;27298;4832;21875,43;96;291;210;140,18;31;62;30;39,199;1321;1888;402;1787,m;m
3390,ICLR,2020,Is a Good Representation Sufficient for Sample Efficient Reinforcement Learning?,Simon S. Du;Sham M. Kakade;Ruosong Wang;Lin F. Yang,ssdu@ias.edu;sham@cs.washington.edu;ruosongw@andrew.cmu.edu;linyang@ee.ucla.edu,6;8;8,I have read many papers in this area.:N/A:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Spotlight),0,6,0,yes,9/25/19,"Institue for Advanced Study, Princeton;University of Washington;Carnegie Mellon University;University of California, Los Angeles",-1;6;1;20,-1;26;27;17,,9/25/19,22,15,7,3,0,3,2086;13740;-1;678,55;198;-1;82,20;58;-1;14,313;1986;0;44,m;m
3391,ICLR,2020,Interpretable Complex-Valued Neural Networks for Privacy Protection,Liyao Xiang;Hao Zhang;Haotian Ma;Yifan Zhang;Jie Ren;Quanshi Zhang,xiangliyao08@sjtu.edu.cn;1603023-zh@sjtu.edu.cn;11612807@mail.sustc.edu.cn;zhangyf_sjtu@sjtu.edu.cn;ariesrj@sjtu.edu.cn;zqs1022@sjtu.edu.cn,6;6;6,I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,Shanghai Jiao Tong University;Shanghai Jiao Tong University;University of Science and Technology of China;Shanghai Jiao Tong University;Shanghai Jiao Tong University;Shanghai Jiao Tong University,53;53;481;53;53;53,157;157;80;157;157;157,4,1/28/19,2,0,0,0,0,0,81;368;65;19;4;1174,15;88;22;31;6;77,4;7;5;3;2;17,7;39;1;0;0;41,f;m
3392,ICLR,2020,The Shape of Data: Intrinsic Distance for Data Distributions,Anton Tsitsulin;Marina Munkhoeva;Davide Mottin;Panagiotis Karras;Alex Bronstein;Ivan Oseledets;Emmanuel Mueller,tsitsulin@bit.uni-bonn.de;marina.munkhoeva@skolkovotech.ru;davide@cs.au.dk;piekarras@gmail.com;bron@cs.technion.ac.il;i.oseledets@skoltech.ru;mueller@bit.uni-bonn.de,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),1,4,0,yes,9/25/19,University of Bonn;Skolkovo Institute of Science and Technology;Aarhus University;Aarhus University;Technion;Skolkovo Institute of Science and Technology;University of Bonn,128;-1;108;108;26;-1;128,106;-1;115;115;412;-1;106,5,5/27/19,2,0,1,0,0,0,117;19;413;2074;8434;4436;2304,10;5;40;83;264;199;110,2;2;10;21;48;30;25,33;1;42;225;691;404;266,m;m
3393,ICLR,2020,Dynamics-Aware Embeddings,William Whitney;Rajat Agarwal;Kyunghyun Cho;Abhinav Gupta,wfwhitney@gmail.com;ra2630@nyu.edu;kyunghyun.cho@nyu.edu;abhinavg@cs.cmu.edu,8;8;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,8,0,yes,9/25/19,New York University;New York University;New York University;Carnegie Mellon University,25;25;25;1,29;29;29;27,,8/25/19,0,0,0,0,0,0,709;100;46450;17553,10;13;272;233,5;3;52;64,50;7;6610;1943,m;m
3394,ICLR,2020,Extreme Tensoring for Low-Memory Preconditioning ,Xinyi Chen;Naman Agarwal;Elad Hazan;Cyril Zhang;Yi Zhang,xinyic@google.com;namanagarwal@google.com;ehazan@cs.princeton.edu;cyril.zhang@cs.princeton.edu;y.zhang@cs.princeton.edu,8;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,5,0,yes,9/25/19,Google;Google;Princeton University;Princeton University;Princeton University,-1;-1;31;31;31,-1;-1;6;6;6,3,2/12/19,2,1,0,0,2,0,182;620;12184;198;1514,33;32;149;18;433,9;12;44;6;17,6;91;2025;20;77,f;m
3395,ICLR,2020,Sequential Latent Knowledge Selection for Knowledge-Grounded Dialogue,Byeongchang Kim;Jaewoo Ahn;Gunhee Kim,byeongchang.kim@vision.snu.ac.kr;jaewoo.ahn@vision.snu.ac.kr;gunhee@snu.ac.kr,8;6;8,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,3,0,yes,9/25/19,Seoul National University;Seoul National University;Seoul National University,41;41;41,64;64;64,,9/25/19,2,2,1,0,0,2,90;2;56,6;2;18,4;1;2,13;2;11,m;m
3396,ICLR,2020,Geom-GCN: Geometric Graph Convolutional Networks,Hongbin Pei;Bingzhe Wei;Kevin Chen-Chuan Chang;Yu Lei;Bo Yang,gspeihongbing@163.com;bwei6@illinois.edu;kcchang@illinois.edu;csylei@comp.polyu.edu.hk;ybo@jlu.edu.cn,6;8;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),2,6,2,yes,9/25/19,"Jilin University;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;The Hong Kong Polytechnic University;Jilin University",481;3;3;172;481,952;48;48;171;952,10,9/25/19,13,12,6,2,0,4,80;9;6935;37;143,10;4;219;39;121,5;1;40;3;7,4;2;567;5;10,m;m
3397,ICLR,2020,Compositional languages emerge in a neural iterated learning model,Yi Ren;Shangmin Guo;Matthieu Labeau;Shay B. Cohen;Simon Kirby,y.ren-18@sms.ed.ac.uk;s.guo-16@sms.ed.ac.uk;matthieu.labeau@gmail.com;scohen@inf.ed.ac.uk;simon.kirby@ed.ac.uk,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,17,0,yes,9/25/19,University of Edinburgh;University of Edinburgh;Télécom ParisTech;University of Edinburgh;University of Edinburgh,33;33;481;33;33,30;30;187;30;30,3;8,9/25/19,1,1,1,0,0,1,150;31;90;1886;6623,45;6;17;117;341,4;3;4;24;42,14;3;9;252;492,m;m
3398,ICLR,2020,Knowledge Consistency between Neural Networks and Beyond,Ruofan Liang;Tianlin Li;Longfei Li;Jing Wang;Quanshi Zhang,nexuslrf@sjtu.edu.cn;litl@act.buaa.edu.cn;1776752575@sjtu.edu.cn;wangjing215@huawei.com;zqs1022@sjtu.edu.cn,8;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,Shanghai Jiao Tong University;Beihang University;Shanghai Jiao Tong University;Huawei Technologies Ltd.;Shanghai Jiao Tong University,53;118;53;-1;53,157;594;157;-1;157,,8/5/19,2,0,0,0,0,0,2;1;72;245;1174,2;5;22;113;77,1;1;4;7;17,0;0;6;13;41,m;m
3399,ICLR,2020,Differentially Private Meta-Learning,Jeffrey Li;Mikhail Khodak;Sebastian Caldas;Ameet Talwalkar,jwl3@andrew.cmu.edu;khodak@cs.cmu.edu;scaldas@cs.cmu.edu;talwalkar@cmu.edu,6;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Accept (Poster),0,4,0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1,27;27;27;27,3;6,9/12/19,7,6,0,0,0,1,36;372;135;6515,14;23;5;79,3;8;4;34,3;54;20;768,m;m
3400,ICLR,2020,Stochastic Conditional Generative Networks with Basis Decomposition,Ze Wang;Xiuyuan Cheng;Guillermo Sapiro;Qiang Qiu,ze.w@duke.edu;xiuyuan.cheng@duke.edu;guillermo.sapiro@duke.edu;qiang.qiu@duke.edu,6;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0,yes,9/25/19,Duke University;Duke University;Duke University;Duke University,47;47;47;47,20;20;20;20,5;4,9/25/19,1,1,0,0,0,0,42;405;44214;31,12;52;650;16,2;12;86;3,1;35;3902;5,m;m
3401,ICLR,2020,Meta-Learning with Warped Gradient Descent,Sebastian Flennerhag;Andrei A. Rusu;Razvan Pascanu;Francesco Visin;Hujun Yin;Raia Hadsell,flennerhag@google.com;andreirusu@google.com;razp@google.com;visin@google.com;hujun.yin@manchester.ac.uk;raia@google.com,8;8;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Talk),2,3,0,yes,9/25/19,Google;Google;Google;Google;University of Manchester;Google,-1;-1;-1;-1;266;-1,-1;-1;-1;-1;55;-1,6,8/30/19,17,7,6,0,0,2,42;3061;17189;-1;2269;8331,7;16;101;-1;232;63,3;13;46;-1;21;26,4;446;1700;0;178;804,m;f
3402,ICLR,2020,Neural Text Generation With Unlikelihood Training,Sean Welleck;Ilia Kulikov;Stephen Roller;Emily Dinan;Kyunghyun Cho;Jason Weston,wellecks@nyu.edu;kulikov@cs.nyu.edu;roller@fb.com;edinan@fb.com;kyunghyun.cho@nyu.edu;jase@fb.com,6;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,New York University;New York University;Facebook;Facebook;New York University;Facebook,25;25;-1;-1;25;-1,29;29;-1;-1;29;-1,3,8/12/19,32,21,8,3,0,3,120;88;930;593;46450;45686,10;10;30;19;272;242,6;4;15;8;52;78,13;4;137;122;6610;5869,m;m
3403,ICLR,2020,Duration-of-Stay Storage Assignment under Uncertainty,Michael Lingzhi Li;Elliott Wolf;Daniel Wintz,mlli@mit.edu;ewolf@lineagelogistics.com;dwintz@lineagelogistics.com,6;3;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,16,0,yes,9/25/19,Massachusetts Institute of Technology;Lineagelogistics;Lineagelogistics,2;-1;-1,5;-1;-1,,3/12/19,0,0,0,0,0,0,76;2;223,20;5;22,4;1;6,8;0;6,m;m
3404,ICLR,2020,Non-Autoregressive Dialog State Tracking,Hung Le;Richard Socher;Steven C.H. Hoi,l.hung1610@gmail.com;rsocher@salesforce.com;shoi@salesforce.com,6;1;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Poster),1,3,0,yes,9/25/19,Singapore Management University;SalesForce.com;SalesForce.com,92;-1;-1,1397;-1;-1,,9/25/19,6,3,1,0,0,1,279;53531;9066,35;180;295,7;49;50,18;8917;849,m;m
3405,ICLR,2020,Scalable and Order-robust Continual Learning with Additive Parameter Decomposition,Jaehong Yoon;Saehoon Kim;Eunho Yang;Sung Ju Hwang,jaehong.yoon@kaist.ac.kr;shkim@aitrics.com;eunhoy@kaist.ac.kr;sjhwang82@kaist.ac.kr,1;6;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Accept (Poster),0,16,0,yes,9/25/19,Korea Advanced Institute of Science and Technology;AITRICS;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,481;-1;481;481,110;-1;110;110,7,2/25/19,2,1,2,0,0,0,260;424;1067;1128,14;35;75;71,4;10;16;16,38;68;169;125,m;m
3406,ICLR,2020,Automatically Discovering and Learning New Visual Categories with Ranking Statistics,Kai Han;Sylvestre-Alvise Rebuffi;Sebastien Ehrhardt;Andrea Vedaldi;Andrew Zisserman,khan@robots.ox.ac.uk;srebuffi@robots.ox.ac.uk;hyenal@robots.ox.ac.uk;vedaldi@robots.ox.ac.uk;az@robots.ox.ac.uk,3;6;6,I have published in this field for several years.:I carefully checked the experiments.:N/A:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,5,0,yes,9/25/19,University of Oxford;University of Oxford;University of Oxford;University of Oxford;University of Oxford,50;50;50;50;50,1;1;1;1;1,,9/25/19,3,3,2,0,0,1,380;639;4;34629;141121,45;8;4;202;797,11;4;1;63;141,36;161;1;4685;20187,m;m
3407,ICLR,2020,On Identifiability in Transformers,Gino Brunner;Yang Liu;Damian Pascual;Oliver Richter;Massimiliano Ciaramita;Roger Wattenhofer,brunnegi@ethz.ch;liu.yang@alumni.ethz.ch;dpascual@ethz.ch;richtero@ethz.ch;massi@google.com;wattenhofer@ethz.ch,8;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,7,1,yes,9/25/19,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Google;Swiss Federal Institute of Technology,10;10;10;10;-1;10,13;13;13;13;-1;13,,8/12/19,17,10,3,0,0,0,120;1552;28;51;2931;18888,20;211;5;15;74;571,7;22;2;4;28;68,9;99;0;0;259;1863,m;m
3408,ICLR,2020,Robust Reinforcement Learning for Continuous Control with Model Misspecification,Daniel J. Mankowitz;Nir Levine;Rae Jeong;Abbas Abdolmaleki;Jost Tobias Springenberg;Yuanyuan Shi;Jackie Kay;Todd Hester;Timothy Mann;Martin Riedmiller,dmankowitz@google.com;nirlevine@google.com;raejeong@google.com;aabdolmaleki@google.com;springenberg@google.com;yyshi@google.com;kayj@google.com;toddhester@google.com;timothymann@google.com;riedmiller@google.com,8;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,9,0,yes,9/25/19,Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,,6/18/19,8,2,2,0,0,0,561;95;7;549;7449;-1;-1;1543;229;25961,29;9;4;45;54;-1;-1;51;28;190,12;5;1;10;28;-1;-1;20;6;40,35;5;0;59;798;0;0;140;24;3578,m;m
3409,ICLR,2020,Enhancing Adversarial Defense by k-Winners-Take-All,Chang Xiao;Peilin Zhong;Changxi Zheng,chang@cs.columbia.edu;pz2225@columbia.edu;cxz@cs.columbia.edu,8;8;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Spotlight),0,5,0,yes,9/25/19,Columbia University;Columbia University;Columbia University,15;15;15,16;16;16,4,5/25/19,8,4,0,0,0,0,77;478;61,25;25;13,3;8;4,7;27;5,m;m
3410,ICLR,2020,Are Pre-trained Language Models Aware of Phrases? Simple but Strong Baselines for Grammar Induction,Taeuk Kim;Jihun Choi;Daniel Edmiston;Sang-goo Lee,taeuk@europa.snu.ac.kr;jhchoi@europa.snu.ac.kr;danedmiston@uchicago.edu;sglee@europa.snu.ac.kr,6;8;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Accept (Poster),0,6,0,yes,9/25/19,Seoul National University;Seoul National University;University of Chicago;Seoul National University,41;41;48;41,64;64;9;64,3,9/25/19,4,2,2,0,0,0,28;323;178;120,12;63;53;35,4;9;7;4,5;26;16;12,m;m
3411,ICLR,2020,Neural Policy Gradient Methods: Global Optimality and Rates of Convergence,Lingxiao Wang;Qi Cai;Zhuoran Yang;Zhaoran Wang,lingxiaowang2022@u.northwestern.edu;qicai2022@u.northwestern.edu;zy6@princeton.edu;zhaoranwang@gmail.com,8;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),0,5,0,yes,9/25/19,Northwestern University;Northwestern University;Princeton University;Northwestern University,44;44;31;44,22;22;6;22,1;9,8/29/19,20,7,6,0,0,2,311;65;182;1193,56;10;41;78,10;2;7;20,18;3;22;132,f;m
3412,ICLR,2020,Enhancing Transformation-Based Defenses Against Adversarial Attacks with a Distribution Classifier,Connie Kou;Hwee Kuan Lee;Ee-Chien Chang;Teck Khim Ng,conniekoukl@gmail.com;leehk@bii.a-star.edu.sg;changec@comp.nus.edu.sg;ngtk@comp.nus.edu.sg,6;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,11,0,yes,9/25/19,National University of Singapore;A*STAR;National University of Singapore;National University of Singapore,16;-1;16;16,25;-1;25;25,4,6/1/19,5,1,2,0,0,0,10;718;2511;217,6;109;154;7,2;16;24;3,1;44;179;22,f;m
3413,ICLR,2020,Sample Efficient Policy Gradient Methods with Recursive Variance Reduction,Pan Xu;Felicia Gao;Quanquan Gu,panxu@cs.ucla.edu;fxgao1160@engineering.ucla.edu;qgu@cs.ucla.edu,6;8;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,5,0,yes,9/25/19,"University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles",20;20;20,17;17;17,9,9/18/19,10,7,2,1,0,4,295;28;3895,31;6;174,10;3;34,42;9;411,m;m
3414,ICLR,2020,ALBERT: A Lite BERT for Self-supervised Learning of Language Representations,Zhenzhong Lan;Mingda Chen;Sebastian Goodman;Kevin Gimpel;Piyush Sharma;Radu Soricut,lanzhzh@google.com;mchen@ttic.edu;seabass@google.com;kgimpel@ttic.edu;piyushsharma@google.com;rsoricut@google.com,6;8;8,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A,Accept (Spotlight),8,8,0,yes,9/25/19,Google;Toyota Technological Institute at Chicago;Google;Toyota Technological Institute at Chicago;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,3,9/25/19,339,153,159,11,0,77,1543;383;412;5796;457;2374,36;11;8;99;9;48,15;4;3;31;4;17,198;85;90;825;101;342,m;m
3415,ICLR,2020,State-only Imitation with Transition Dynamics Mismatch,Tanmay Gangwani;Jian Peng,gangwan2@illinois.edu;jianpeng@illinois.edu,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,7,0,yes,9/25/19,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",3;3,48;48,4,9/25/19,2,2,0,0,0,0,61;46,14;89,4;3,5;0,m;m
3416,ICLR,2020,Watch the Unobserved: A Simple Approach to Parallelizing Monte Carlo Tree Search,Anji Liu;Jianshu Chen;Mingze Yu;Yu Zhai;Xuewen Zhou;Ji Liu,anjiliu219@gmail.com;chenjianshu@gmail.com;yumingze@kuaishou.com;zhaiyu@kuaishou.com;zhouxuewen@kuaishou.com;ji.liu.uwisc@gmail.com,6;8;8,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Talk),0,9,0,yes,9/25/19,"University of California, Los Angeles;Tencent AI Lab;Kuaishou;Kuaishou;Kuaishou;University of Rochester",20;-1;-1;-1;-1;100,17;-1;-1;-1;-1;173,,10/28/18,3,2,2,0,0,0,24;18;3;37;17;54,7;14;2;23;13;8,3;3;1;3;3;3,0;0;0;1;0;3,u;m
3417,ICLR,2020,Gradientless Descent: High-Dimensional Zeroth-Order Optimization,Daniel Golovin;John Karro;Greg Kochanski;Chansoo Lee;Xingyou Song;Qiuyi Zhang,dgg@google.com;karro@google.com;gpk@google.com;chansoo@google.com;xingyousong@google.com;qiuyiz@google.com,6;6;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,4,0,yes,9/25/19,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,,9/25/19,3,3,0,0,0,0,3039;458;1538;89;29;36,57;15;79;27;15;19,19;6;19;5;3;4,372;49;123;3;4;0,m;m
3418,ICLR,2020,Black-box Off-policy Estimation for Infinite-Horizon Reinforcement Learning,Ali Mousavi;Lihong Li;Qiang Liu;Denny Zhou,ali.mousavi1988@gmail.com;lihongli.cs@gmail.com;dennyzhou@google.com;lqiang@cs.utexas.edu,6;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0,yes,9/25/19,"Google;Google;Google;University of Texas, Austin",-1;-1;-1;22,-1;-1;-1;38,8,9/25/19,5,2,2,0,0,1,253;300;492;9316,33;40;164;88,5;8;12;34,13;22;32;1355,m;m
3419,ICLR,2020,Hyper-SAGNN: a self-attention based graph neural network for hypergraphs,Ruochi Zhang;Yuesong Zou;Jian Ma,ruochiz@andrew.cmu.edu;logic.zys@gmail.com;jianma@cs.cmu.edu,8;8,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,1,yes,9/25/19,Carnegie Mellon University;Tsinghua University;Carnegie Mellon University,1;8;1,27;23;27,10,9/25/19,4,1,1,0,0,0,240;3;198,26;1;45,8;1;6,20;0;17,m;m
3420,ICLR,2020,Unpaired Point Cloud Completion on Real Scans using Adversarial Training,Xuelin Chen;Baoquan Chen;Niloy J. Mitra,xuelin.chen.sdu@gmail.com;baoquan.chen@gmail.com;n.mitra@cs.ucl.ac.uk,6;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Poster),0,4,0,yes,9/25/19,Shandong University;Peking University;University College London,154;22;50,658;24;15,,3/29/19,3,2,2,0,0,0,294;5179;10968,13;216;248,4;38;53,11;463;909,m;m
3421,ICLR,2020,The intriguing role of module criticality in the generalization of deep networks,Niladri Chatterji;Behnam Neyshabur;Hanie Sedghi,niladri.chatterji@berkeley.edu;neyshabur@google.com;hsedghi@google.com,8;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,5,0,yes,9/25/19,University of California Berkeley;Google;Google,5;-1;-1,13;-1;-1,8,9/25/19,2,2,0,0,0,0,267;2442;579,16;27;30,7;19;12,38;319;53,m;f
3422,ICLR,2020,SpikeGrad: An ANN-equivalent Computation Model for Implementing Backpropagation with Spikes,Johannes C. Thiele;Olivier Bichler;Antoine Dupret,johannes.thiele@cea.fr;olivier.bichler@cea.fr;antoine.dupret@cea.fr,6;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,7,0,yes,9/25/19,CEA;CEA;CEA,233;233;233,1027;1027;1027,,6/3/19,3,2,0,0,0,0,39;2057;560,8;69;138,3;22;12,4;122;24,m;m
3423,ICLR,2020,Physics-aware Difference Graph Networks for Sparsely-Observed Dynamics,Sungyong Seo*;Chuizheng Meng*;Yan Liu,sungyons@usc.edu;chuizhem@usc.edu;yanliu.cs@usc.edu,6;8;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,University of Southern California;University of Southern California;University of Southern California,31;31;31,62;62;62,10,9/25/19,3,2,1,0,0,0,480;106;151,26;11;33,7;3;5,49;13;10,m;f
3424,ICLR,2020,Neural Network Branching for Neural Network Verification ,Jingyue Lu;M. Pawan Kumar,jingyue.lu@spc.ox.ac.uk;pawan@robots.ox.ac.uk,6;8;8,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,7,0,yes,9/25/19,University of Oxford;University of Oxford,50;50,1;1,1;10,9/25/19,3,1,2,0,0,1,30;2663,7;83,3;24,1;248,f;m
3425,ICLR,2020,DivideMix: Learning with Noisy Labels as Semi-supervised Learning,Junnan Li;Richard Socher;Steven C.H. Hoi,junnan.li@salesforce.com;rsocher@salesforce.com;shoi@salesforce.com,6;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),1,4,0,yes,9/25/19,SalesForce.com;SalesForce.com;SalesForce.com,-1;-1;-1,-1;-1;-1,,9/25/19,6,4,4,1,0,3,31;53531;9066,28;180;295,3;49;50,3;8917;849,m;m
3426,ICLR,2020,Reinforcement Learning Based Graph-to-Sequence Model for Natural Question Generation,Yu Chen;Lingfei Wu;Mohammed J. Zaki,cheny39@rpi.edu;lwu@email.wm.edu;zaki@cs.rpi.edu,6;6;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Accept (Poster),0,6,1,yes,9/25/19,Rensselaer Polytechnic Institute;College of William and Mary;Rensselaer Polytechnic Institute,172;154;172,438;235;438,10,8/14/19,11,6,6,0,0,0,-1;653;17324,-1;62;353,-1;15;58,0;62;1882,f;m
3427,ICLR,2020,Bayesian Meta Sampling for Fast Uncertainty Adaptation,Zhenyi Wang;Yang Zhao;Ping Yu;Ruiyi Zhang;Changyou Chen,zhenyiwa@buffalo.edu;yzhao63@buffalo.edu;pingyu@buffalo.edu;ryzhang@cs.duke.edu;changyou@buffalo.edu,6;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Accept (Poster),0,6,0,yes,9/25/19,"State University of New York, Buffalo;State University of New York, Buffalo;State University of New York, Buffalo;Duke University;State University of New York, Buffalo",84;84;84;47;84,263;263;263;20;263,11,9/25/19,1,0,1,0,0,0,4;485;6;20;223,7;96;8;17;40,1;8;1;3;8,0;88;0;1;26,u;m
3428,ICLR,2020,BREAKING  CERTIFIED  DEFENSES:  SEMANTIC  ADVERSARIAL  EXAMPLES  WITH  SPOOFED  ROBUSTNESS  CERTIFICATES,Amin Ghiasi;Ali Shafahi;Tom Goldstein,amin@cs.umd.edu;ashafahi@cs.umd.edu;tomg@cs.umd.edu,6;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Accept (Poster),0,3,0,yes,9/25/19,"University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park",12;12;12,91;91;91,4,9/25/19,6,3,0,0,0,0,120;453;228,6;33;61,4;8;10,16;47;26,m;m
3429,ICLR,2020,Progressive Memory Banks for Incremental Domain Adaptation,Nabiha Asghar;Lili Mou;Kira A. Selby;Kevin D. Pantasdo;Pascal Poupart;Xin Jiang,nasghar@uwaterloo.ca;doublepower.mou@gmail.com;kaselby@uwaterloo.ca;kevin.pantasdo@uwaterloo.ca;ppoupart@uwaterloo.ca;jiang.xin@huawei.com,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,0,0,yes,9/25/19,University of Waterloo;;University of Waterloo;University of Waterloo;University of Waterloo;Huawei Technologies Ltd.,28;-1;28;28;28;-1,235;-1;235;235;235;-1,3,11/1/18,4,2,2,0,9,1,164;2100;30;4;4273;663,17;67;3;4;173;23,6;21;2;1;35;9,13;238;3;0;460;88,f;m
3430,ICLR,2020,Deep Learning For Symbolic Mathematics,Guillaume Lample;François Charton,guillaume.lample@gmail.com;fcharton@fb.com,8;6;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Spotlight),7,8,0,yes,9/25/19,Facebook;Facebook,-1;-1,-1;-1,,9/25/19,29,10,9,0,0,3,4837;21,24;2,18;1,1005;3,m;m
3431,ICLR,2020,Contrastive Learning of Structured World Models,Thomas Kipf;Elise van der Pol;Max Welling,t.n.kipf@uva.nl;e.e.vanderpol@uva.nl;m.welling@uva.nl,8;8;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Talk),0,10,1,yes,9/25/19,University of Amsterdam;University of Amsterdam;University of Amsterdam,172;172;172,62;62;62,10,9/25/19,14,9,6,0,0,3,5508;164;752,26;13;59,12;6;14,1484;24;112,m;m
3432,ICLR,2020,Decentralized Deep Learning with Arbitrary Communication Compression,Anastasia Koloskova*;Tao Lin*;Sebastian U Stich;Martin Jaggi,anastasia.koloskova@epfl.ch;tao.lin@epfl.ch;sebastian.stich@epfl.ch;martin.jaggi@epfl.ch,3;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0,yes,9/25/19,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne,481;481;481;481,38;38;38;38,,7/22/19,20,12,1,1,0,2,105;163;984;3856,6;7;48;114,4;6;16;27,13;17;142;577,f;m
3433,ICLR,2020,Masked Based Unsupervised Content Transfer,Ron Mokady;Sagie Benaim;Lior Wolf;Amit Bermano,sagiebenaim@gmail.com;ron.mokady@gmail.com;wolf@fb.com;amit.bermano@gmail.com,6;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,7,0,yes,9/25/19,Tel Aviv University;Tel Aviv University;Facebook;,35;35;-1;-1,188;188;-1;-1,2,6/15/19,1,0,1,0,0,0,1;172;14176;274,2;15;199;25,1;5;45;10,0;26;1640;5,m;m
3434,ICLR,2020,Fair Resource Allocation in Federated Learning,Tian Li;Maziar Sanjabi;Ahmad Beirami;Virginia Smith,tianli@cmu.edu;maziar.sanjabi@gmail.com;ahmad.beirami@gmail.com;smithv@cmu.edu,6;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0,yes,9/25/19,Carnegie Mellon University;University of Southern California;Facebook;Carnegie Mellon University,1;31;-1;1,27;62;-1;27,7,5/25/19,25,14,7,4,0,6,270;992;-1;1879,7;27;-1;101,6;16;-1;17,34;117;0;208,f;f
3435,ICLR,2020,Gap-Aware Mitigation of Gradient Staleness,Saar Barkai;Ido Hakimi;Assaf Schuster,saarbarkai@gmail.com;idohakimi@gmail.com;assaf@cs.technion.ac.il,6;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0,yes,9/25/19,Technion;Technion;Technion,26;26;26,412;412;412,1;9,9/24/19,0,0,0,0,0,0,2;4;4139,3;3;254,1;1;34,0;0;346,u;m
3436,ICLR,2020,Graph Neural Networks Exponentially Lose Expressive Power for Node Classification,Kenta Oono;Taiji Suzuki,kenta_oono@mist.i.u-tokyo.ac.jp;taiji@mist.i.u-tokyo.ac.jp,8;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),1,3,1,yes,9/25/19,The University of Tokyo;The University of Tokyo,56;56,36;36,10,5/27/19,17,11,2,2,0,3,664;2318,11;177,5;26,69;237,m;m
3437,ICLR,2020,A FRAMEWORK  FOR ROBUSTNESS CERTIFICATION  OF SMOOTHED CLASSIFIERS USING  F-DIVERGENCES,Krishnamurthy (Dj) Dvijotham;Jamie Hayes;Borja Balle;Zico Kolter;Chongli Qin;Andras Gyorgy;Kai Xiao;Sven Gowal;Pushmeet Kohli,dvij@google.com;j.hayes@cs.ucl.ac.uk;bballe@google.com;zkolter@cs.cmu.edu;chongliqin@google.com;agyorgy@google.com;kaix@mit.edu;sgowal@google.com;pushmeet@google.com,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,7,0,yes,9/25/19,Google;University College London;Google;Carnegie Mellon University;Google;Google;Massachusetts Institute of Technology;Google;Google,-1;50;-1;1;-1;-1;2;-1;-1,-1;15;-1;27;-1;-1;5;-1;-1,4;1,9/25/19,11,8,4,0,0,2,1143;706;805;66;229;37;122;568;22578,76;46;74;10;8;10;26;34;313,17;11;15;4;7;3;6;11;69,101;89;75;7;33;4;6;62;2782,m;m
3438,ICLR,2020,I Am Going MAD: Maximum Discrepancy Competition for Comparing Classifiers Adaptively,Haotao Wang;Tianlong Chen;Zhangyang Wang;Kede Ma,htwang@tamu.edu;wiwjp619@tamu.edu;atlaswang@tamu.edu;kede.ma@cityu.edu.hk,3;8;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,9,0,yes,9/25/19,Texas A&M;Texas A&M;Texas A&M;City University of Hong Kong,44;44;44;92,177;177;177;35,,9/25/19,1,1,0,0,0,0,21;94;2947;2146,7;36;167;58,3;5;28;21,1;14;383;297,m;m
3439,ICLR,2020,Analysis of Video Feature Learning in Two-Stream CNNs on the Example of Zebrafish Swim Bout Classification,Bennet Breier;Arno Onken,b.breier@sms.ed.ac.uk;aonken@inf.ed.ac.uk,6;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,3,0,yes,9/25/19,University of Edinburgh;University of Edinburgh,33;33,30;30,,9/25/19,0,0,0,0,0,0,0;197,3;34,0;7,0;9,m;m
3440,ICLR,2020,"On the steerability"" of generative adversarial networks""",Ali Jahanian*;Lucy Chai*;Phillip Isola,jahanian@mit.edu;lrchai@mit.edu;phillipi@mit.edu,8;8;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,4,0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2,5;5;5,5;4;8,7/16/19,29,14,6,0,0,1,125;38;12579,14;4;73,6;2;27,4;1;2186,m;m
3441,ICLR,2020,Imitation Learning via Off-Policy Distribution Matching,Ilya Kostrikov;Ofir Nachum;Jonathan Tompson,kostrikov@cs.nyu.edu;ofirnachum@google.com;tompson@google.com,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,5,1,yes,9/25/19,New York University;Google;Google,25;-1;-1,29;-1;-1,,9/25/19,6,4,3,0,0,2,617;1060;3247,15;42;30,9;15;16,61;157;378,m;m
3442,ICLR,2020,A Function Space View of Bounded Norm Infinite Width ReLU Nets: The Multivariate Case,Greg Ongie;Rebecca Willett;Daniel Soudry;Nathan Srebro,gongie@uchicago.edu;willett@uchicago.edu;daniel.soudry@technion.ac.il;nati@ttic.edu,8;6;6,I have published one or two papers in this area.:N/A:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,University of Chicago;University of Chicago;Technion;Toyota Technological Institute at Chicago,48;48;26;-1,9;9;412;-1,,9/25/19,14,7,2,1,0,4,339;210;5011;13596,31;12;76;176,9;7;26;52,39;42;642;1618,m;m
3443,ICLR,2020,ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators,Kevin Clark;Minh-Thang Luong;Quoc V. Le;Christopher D. Manning,kevclark@cs.stanford.edu;thangluong@google.com;qvl@google.com;manning@cs.stanford.edu,8;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),1,6,0,yes,9/25/19,Stanford University;Google;Google;Stanford University,4;-1;-1;4,4;-1;-1;4,3,9/25/19,40,24,14,1,0,13,1210;3111;48901;90732,35;33;193;480,9;20;81;115,165;359;6080;11703,m;m
3444,ICLR,2020,Quantifying Point-Prediction Uncertainty in Neural Networks via Residual Estimation with an I/O Kernel,Xin Qiu;Elliot Meyerson;Risto Miikkulainen,qiuxin.nju@gmail.com;elliot.meyerson@cognizant.com;risto@cognizant.com,6;6;8;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,11,0,yes,9/25/19,;Cognizant;Cognizant,-1;-1;-1,-1;-1;-1,11,6/3/19,4,2,2,0,0,1,300;614;12970,49;23;442,9;10;49,14;41;1319,m;m
3445,ICLR,2020,Span Recovery for Deep Neural Networks with Applications to Input Obfuscation,Rajesh Jayaram;David P. Woodruff;Qiuyi Zhang,rkjayara@cs.cmu.edu;dwoodruf@andrew.cmu.edu;qiuyiz@google.com,3;6;8;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;Google,1;1;-1,27;27;-1,4,9/25/19,0,0,0,0,0,0,59;7140;36,16;432;19,4;44;4,4;558;0,m;m
3446,ICLR,2020,Posterior sampling for multi-agent reinforcement learning: solving extensive games with imperfect information,Yichi Zhou;Jialian Li;Jun Zhu,vofhqn@gmail.com;lijialia16@mails.tsinghua.edu.cn;dcszj@mail.tsinghua.edu.cn,6;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,8,0,yes,9/25/19,;Tsinghua University;Tsinghua University,-1;8;8,-1;23;23,,9/25/19,1,0,0,0,0,0,11;1;705,9;8;96,2;1;15,0;0;15,m;m
3447,ICLR,2020,Co-Attentive Equivariant Neural Networks: Focusing Equivariance On Transformations Co-Occurring in Data,David W. Romero;Mark Hoogendoorn,d.w.romeroguzman@vu.nl;m.hoogendoorn@vu.nl,6;8;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,11,0,yes,9/25/19,VU University Toronto;VU University Toronto,18;18,18;18,,9/25/19,4,2,2,0,0,0,133;163,20;30,5;8,5;6,m;m
3448,ICLR,2020,Explanation  by Progressive  Exaggeration,Sumedha Singla;Brian Pollack;Junxiang Chen;Kayhan Batmanghelich,sumedha.singla@pitt.edu;kayhan@pitt.edu;cjx880409@gmail.com;kayhan@pitt.edu,8;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Accept (Spotlight),0,3,0,yes,9/25/19,University of Pittsburgh;University of Pittsburgh;;University of Pittsburgh,79;79;-1;79,113;113;-1;113,,9/25/19,2,2,1,0,0,2,3;10352;60;731,5;365;18;34,1;45;5;8,1;544;5;109,f;m
3449,ICLR,2020,Principled Weight Initialization for Hypernetworks,Oscar Chang;Lampros Flokas;Hod Lipson,oscar.chang@columbia.edu;lamflokas@cs.columbia.edu;hod.lipson@columbia.edu,8;8;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Talk),0,6,0,yes,9/25/19,Columbia University;Columbia University;Columbia University,15;15;15,16;16;16,11,9/25/19,4,3,1,0,0,1,69;32;1121,20;9;73,4;4;15,6;4;28,m;m
3450,ICLR,2020,On the Variance of the Adaptive Learning Rate and Beyond,Liyuan Liu;Haoming Jiang;Pengcheng He;Weizhu Chen;Xiaodong Liu;Jianfeng Gao;Jiawei Han,ll2@illinois.edu;jianghm@gatech.edu;penhe@microsoft.com;wzchen@microsoft.com;xiaodl@microsoft.com;jfgao@microsoft.com;hanj@illinois.edu,6;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),4,5,0,yes,9/25/19,"University of Illinois, Urbana Champaign;Georgia Institute of Technology;Microsoft;Microsoft;Microsoft;Microsoft;University of Illinois, Urbana Champaign",3;13;-1;-1;-1;-1;3,48;38;-1;-1;-1;-1;48,3;8,8/8/19,178,30,100,4,0,31,565;257;504;1840;1638;18900;687,22;30;11;62;171;353;56,7;6;6;20;17;61;8,70;39;115;237;234;2683;49,u;m
3451,ICLR,2020,Curvature Graph Network,Ze Ye;Kin Sum Liu;Tengfei Ma;Jie Gao;Chao Chen,yeze16159@gmail.com;kiliu@cs.stonybrook.edu;tengfei.ma1@ibm.com;jgao@cs.stonybrook.edu;chao.chen.1@stonybrook.edu,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Accept (Poster),0,3,0,yes,9/25/19,"State University of New York, Stony Brook;State University of New York, Stony Brook;International Business Machines;State University of New York, Stony Brook;State University of New York, Stony Brook",41;41;-1;41;41,304;304;-1;304;304,10,9/25/19,2,2,0,0,0,0,46;77;4;348;96,23;23;3;22;83,3;6;2;5;4,3;6;1;36;5,u;m
3452,ICLR,2020,"Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds",Jordan T. Ash;Chicheng Zhang;Akshay Krishnamurthy;John Langford;Alekh Agarwal,jordanta@cs.princeton.edu;chichengz@cs.arizona.edu;akshay.krishnamurthy@microsoft.com;jcl@microsoft.com;alekha@microsoft.com,8;6;8,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,4,0,yes,9/25/19,Princeton University;University of Arizona;Microsoft;Microsoft;Microsoft,31;172;-1;-1;-1,6;103;-1;-1;-1,,6/9/19,14,7,2,0,0,0,116;220;133;12036;5237,15;31;23;198;109,5;8;5;49;36,12;6;13;1281;678,m;m
3453,ICLR,2020,StructPool: Structured Graph Pooling via Conditional Random Fields,Hao Yuan;Shuiwang Ji,hao.yuan@tamu.edu;sji@tamu.edu,6;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,1,yes,9/25/19,Texas A&M;Texas A&M,44;44,177;177,10,9/25/19,5,3,1,0,0,0,5;8754,8;136,1;35,0;728,u;m
3454,ICLR,2020,Weakly Supervised Clustering by Exploiting Unique Class Count,Mustafa Umit Oner;Hwee Kuan Lee;Wing-Kin Sung,umitoner@comp.nus.edu.sg;leehk@bii.a-star.edu.sg;ksung@comp.nus.edu.sg,6;1;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,8,0,yes,9/25/19,National University of Singapore;A*STAR;National University of Singapore,16;-1;16,25;-1;25,1;2,6/18/19,1,1,1,0,0,0,1;718;2566,4;109;27,1;16;11,0;44;217,m;m
3455,ICLR,2020,Gradients as Features for Deep Representation Learning,Fangzhou Mu;Yingyu Liang;Yin Li,fmu@cs.wisc.edu;yliang@cs.wisc.edu;yin.li@wisc.edu,6;3;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,University of Southern California;University of Southern California;University of Southern California,31;31;31,62;62;62,,9/25/19,1,1,1,0,0,0,3;2882;98,2;81;39,1;25;6,0;425;1,u;u
3456,ICLR,2020,Composition-based Multi-Relational Graph Convolutional Networks,Shikhar Vashishth;Soumya Sanyal;Vikram Nitin;Partha Talukdar,shikhar@iisc.ac.in;sanyal.soumya8@gmail.com;vikram.nitin@columbia.edu;ppt@iisc.ac.in,6;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,Indian Institute of Science;Indian Institute of Science;Columbia University;Indian Institute of Science,95;95;15;95,301;301;16;301,10,9/25/19,5,3,2,0,0,1,169;26;23;2427,19;6;5;102,7;4;3;28,29;5;2;214,m;m
3457,ICLR,2020,Behaviour Suite for Reinforcement Learning,Ian Osband;Yotam Doron;Matteo Hessel;John Aslanides;Eren Sezener;Andre Saraiva;Katrina McKinney;Tor Lattimore;Csaba Szepesvari;Satinder Singh;Benjamin Van Roy;Richard Sutton;David Silver;Hado Van Hasselt,ian.osband@gmail.com;ydoron@google.com;mtthss@google.com;jaslanides@google.com;esezener@google.com;andresnds@google.com;mckinneyk@google.com;lattimore@google.com;szepi@google.com;baveja@google.com;benvanroy@google.com;suttonr@google.com;davidsilver@google.com;hado@google.com,8;6;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Spotlight),0,11,0,yes,9/25/19,Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,,8/9/19,23,10,7,1,0,3,2492;572;2317;187;21;58;164;842;11496;88;7351;48186;43236;5638,28;7;27;11;4;8;2;67;311;28;162;289;159;51,19;4;13;6;1;4;2;17;47;6;42;66;56;21,357;108;371;24;3;3;15;94;1810;8;662;7042;5956;930,m;u
3458,ICLR,2020,Identifying through Flows for Recovering Latent Representations,Shen Li;Bryan Hooi;Gim Hee Lee,maths.shenli@gmail.com;bhooi@comp.nus.edu.sg;dcslgh@nus.edu.sg,6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,National University of Singapore;National University of Singapore;National University of Singapore,16;16;16,25;25;25,5;1,9/25/19,0,0,0,0,0,0,86;645;2192,47;53;69,5;13;22,8;80;180,u;m
3459,ICLR,2020,Federated Adversarial Domain Adaptation,Xingchao Peng;Zijun Huang;Yizhe Zhu;Kate Saenko,xpeng@bu.edu;zijun.huang@columbia.edu;yizhe.zhu@rutgers.edu;saenko@bu.edu,6;6;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),2,5,0,yes,9/25/19,Boston University;Columbia University;Rutgers University;Boston University,67;15;34;67,61;16;168;61,4,9/25/19,5,2,3,0,0,2,678;23;278;17431,19;6;34;178,11;3;7;56,81;3;51;2403,m;f
3460,ICLR,2020,High Fidelity Speech Synthesis with Adversarial Networks,Mikołaj Bińkowski;Jeff Donahue;Sander Dieleman;Aidan Clark;Erich Elsen;Norman Casagrande;Luis C. Cobo;Karen Simonyan,mikbinkowski@gmail.com;jeffdonahue@google.com;sedielem@google.com;aidanclark@google.com;eriche@google.com;ncasagrande@google.com;luisca@google.com;simonyan@google.com,8;6;8,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Talk),1,5,0,yes,9/25/19,Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1,5;4,9/25/19,17,6,8,1,0,1,286;35933;14082;87;4689;1137;585;61410,7;56;47;12;53;20;8;95,4;25;20;6;21;10;8;40,65;4756;1113;12;493;147;80;10836,m;u
3461,ICLR,2020,Why Not to Use Zero Imputation? Correcting Sparsity Bias in Training Neural Networks,Joonyoung Yi;Juhyuk Lee;Kwang Joon Kim;Sung Ju Hwang;Eunho Yang,joonyoung.yi@kaist.ac.kr;sehkmg@kaist.ac.kr;preppie@yuhs.ac;sjhwang82@kaist.ac.kr;eunhoy@kaist.ac.kr,6;6;6,I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,12,0,yes,9/25/19,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,481;481;-1;481;481,110;110;-1;110;110,4,6/1/19,2,1,0,0,0,0,0;60;4;1128;1067,2;9;9;71;75,0;5;1;16;16,0;0;0;125;169,m;m
3462,ICLR,2020,Adversarial Training and Provable Defenses: Bridging the Gap,Mislav Balunovic;Martin Vechev,bmislav@student.ethz.ch;martin.vechev@inf.ethz.ch,8;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Talk),2,8,0,yes,9/25/19,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10,13;13,4,9/25/19,9,3,1,0,0,0,68;4246,8;153,5;36,2;467,m;m
3463,ICLR,2020,Can gradient clipping mitigate label noise?,Aditya Krishna Menon;Ankit Singh Rawat;Sashank J. Reddi;Sanjiv Kumar,adityakmenon@google.com;ankitsrawat@google.com;sashank@google.com;sanjivk@google.com,6;8;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,1;9,9/25/19,5,4,1,0,0,0,2376;1431;2194;941,77;84;53;142,23;19;20;13,291;140;418;113,m;m
3464,ICLR,2020,Comparing Rewinding and Fine-tuning in Neural Network Pruning,Alex Renda;Jonathan Frankle;Michael Carbin,renda@csail.mit.edu;jfrankle@csail.mit.edu;mcarbin@csail.mit.edu,8;8;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Talk),0,4,1,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2,5;5;5,,9/25/19,9,5,3,1,0,1,22;647;2277,8;22;61,3;9;19,4;99;212,m;m
3465,ICLR,2020,Meta-learning curiosity algorithms,Ferran Alet*;Martin F. Schneider*;Tomas Lozano-Perez;Leslie Pack Kaelbling,ferranalet@gmail.com;martinfs@mit.edu;tlp@csail.mit.edu;lpk@csail.mit.edu,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,4,0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2,5;5;5;5,6;8,9/25/19,4,3,1,1,0,0,207;2350;13381;20202,12;100;206;364,6;29;47;55,23;205;1006;1748,m;m
3466,ICLR,2020,Lookahead: A Far-sighted Alternative of Magnitude-based Pruning,Sejun Park*;Jaeho Lee*;Sangwoo Mo;Jinwoo Shin,sejun.park@kaist.ac.kr;jaeho-lee@kaist.ac.kr;swmo@kaist.ac.kr;jinwoos@kaist.ac.kr,6;6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,13,0,yes,9/25/19,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,481;481;481;481,110;110;110;110,8,9/25/19,1,1,0,0,0,0,184;68;55;1752,76;44;7;185,8;4;2;19,6;4;9;225,u;m
3467,ICLR,2020,Fantastic Generalization Measures and Where to Find Them,Yiding Jiang*;Behnam Neyshabur*;Hossein Mobahi;Dilip Krishnan;Samy Bengio,ydjiang@google.com;neyshabur@google.com;dilipkay@google.com;hmobahi@google.com;bengio@google.com,8;3;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),1,4,0,yes,9/25/19,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,8,9/25/19,19,14,3,0,0,4,268;2442;2166;5051;26795,9;27;43;73;332,6;19;17;22;67,20;319;174;654;3497,m;m
3468,ICLR,2020,Convergence of Gradient Methods on Bilinear Zero-Sum Games,Guojun Zhang;Yaoliang Yu,guojun.zhang@uwaterloo.ca;yaoliang.yu@uwaterloo.ca,8;6;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,4,0,yes,9/25/19,University of Waterloo;University of Waterloo,28;28,235;235,5;4;9,8/15/19,3,2,0,0,0,0,630;1705,108;95,15;20,26;134,m;m
3469,ICLR,2020,Multi-Agent Interactions Modeling with Correlated Policies,Minghuan Liu;Ming Zhou;Weinan Zhang;Yuzheng Zhuang;Jun Wang;Wulong Liu;Yong Yu,minghuanliu@sjtu.edu.cn;mingak@sjtu.edu.cn;wnzhang@sjtu.edu.cn;zhuangyuzheng@huawei.com;w.j@huawei.com;liuwulong@huawei.com;yyu@apex.sjtu.edu.cn,8;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0,yes,9/25/19,Shanghai Jiao Tong University;Shanghai Jiao Tong University;Shanghai Jiao Tong University;Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.;Shanghai Jiao Tong University,53;53;53;-1;-1;-1;53,157;157;157;-1;-1;-1;157,4,9/25/19,1,1,0,0,0,0,327;1370;4997;10;106;128;29332,29;205;207;8;60;33;1552,10;19;31;1;4;6;75,2;52;705;2;13;9;2773,m;m
3470,ICLR,2020,"Real or Not Real, that is the Question",Yuanbo Xiangli*;Yubin Deng*;Bo Dai*;Chen Change Loy;Dahua Lin,xy019@ie.cuhk.edu.hk;danny.s.deng.ds@gmail.com;doubledaibo@gmail.com;ccloy@ntu.edu.sg;dhlin@ie.cuhk.edu.hk,6;6;8,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,9,0,yes,9/25/19,The Chinese University of Hong Kong;;The Chinese University of Hong Kong;National Taiwan University;The Chinese University of Hong Kong,59;-1;59;86;59,35;-1;35;120;35,5;4,9/25/19,3,2,1,0,0,0,6;726;418;17158;6222,5;11;59;189;146,1;6;8;57;37,0;159;64;2893;1079,m;m
3471,ICLR,2020,Structured Object-Aware Physics Prediction for Video Modeling and Planning,Jannik Kossen;Karl Stelzner;Marcel Hussing;Claas Voelcker;Kristian Kersting,kossen@stud.uni-heidelberg.de;stelzner@cs.tu-darmstadt.de;marcel.hussing@stud.tu-darmstadt.de;c.voelcker@stud.tu-darmstadt.de;kersting@cs.tu-darmstadt.de,6;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,8,0,yes,9/25/19,Heidelberg University;TU Darmstadt;TU Darmstadt;TU Darmstadt;TU Darmstadt,205;64;64;64;64,44;289;289;289;289,,9/25/19,6,4,0,0,0,0,4;49;2;2;5839,11;8;1;2;343,1;4;1;1;41,0;4;0;0;464,m;m
3472,ICLR,2020,Gradient Descent Maximizes the Margin of Homogeneous Neural Networks,Kaifeng Lyu;Jian Li,vfleaking@gmail.com;lijian83@mail.tsinghua.edu.cn,6;8;8,I do not know much about this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published in this field for several years.:N/A:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,3,0,yes,9/25/19,Tsinghua University;Tsinghua University,8;8,23;23,1,6/13/19,35,22,3,5,0,4,73;162,6;62,3;6,9;6,u;u
3473,ICLR,2020,Implementing Inductive bias for different navigation tasks through diverse RNN attrractors,Tie XU;Omri Barak,fexutie@gmail.com;omri.barak@gmail.com,6;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0,yes,9/25/19,Technion;Technion,26;26,412;412,,9/25/19,0,0,0,0,0,0,135;2439,38;41,7;16,2;166,m;m
3474,ICLR,2020,End to End Trainable Active Contours via Differentiable Rendering,Shir Gur;Tal Shaharabany;Lior Wolf,shiretzet@gmail.com;shaharabany@mail.tau.ac.il;wolf@fb.com,8;8;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,6,2,yes,9/25/19,Tel Aviv University;Tel Aviv University;Facebook,35;35;-1,188;188;-1,2,9/25/19,1,0,1,0,0,0,22;0;458,6;2;73,3;0;11,2;0;54,m;m
3475,ICLR,2020,ReMixMatch: Semi-Supervised Learning with Distribution Matching and Augmentation Anchoring,David Berthelot;Nicholas Carlini;Ekin D. Cubuk;Alex Kurakin;Kihyuk Sohn;Han Zhang;Colin Raffel,dberth@google.com;ncarlini@google.com;cubuk@google.com;kurakin@google.com;kihyuks@google.com;zhanghan@google.com;craffel@google.com,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:N/A:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,,9/25/19,37,22,17,1,0,5,1454;999;1032;145;3647;1444;4773,30;19;29;6;49;71;62,11;8;13;5;20;8;23,239;159;133;11;575;163;497,m;m
3476,ICLR,2020,DropEdge: Towards Deep Graph Convolutional Networks on Node Classification,Yu Rong;Wenbing Huang;Tingyang Xu;Junzhou Huang,yu.rong@hotmail.com;hwenbing@126.com;tingyangxu@tencent.com;jzhuang@uta.edu,3;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),7,3,2,yes,9/25/19,"Tencent AI Lab;Tsinghua University;Tencent AI Lab;University of Texas, Arlington",-1;8;-1;118,-1;23;-1;708,10;8,7/25/19,19,14,8,1,0,4,233;180;147;342,63;10;29;49,5;4;6;8,10;21;18;42,m;m
3477,ICLR,2020,Neural Tangents: Fast and Easy Infinite Neural Networks in Python,Roman Novak;Lechao Xiao;Jiri Hron;Jaehoon Lee;Alexander A. Alemi;Jascha Sohl-Dickstein;Samuel S. Schoenholz,romann@google.com;xlc@google.com;jh2084@cam.ac.uk;jaehlee@google.com;alemi@google.com;jaschasd@google.com;schsam@google.com,8;3;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,12,0,yes,9/25/19,Google;Google;University of Cambridge;Google;Google;Google;Google,-1;-1;71;-1;-1;-1;-1,-1;-1;3;-1;-1;-1;-1,11,9/25/19,15,5,11,0,0,2,742;453;468;612;1320;5112;3133,12;19;28;55;53;102;70,9;8;7;8;14;33;21,112;63;65;99;186;720;388,m;m
3478,ICLR,2020,FasterSeg: Searching for Faster Real-time Semantic Segmentation,Wuyang Chen;Xinyu Gong;Xianming Liu;Qian Zhang;Yuan Li;Zhangyang Wang,wuyang.chen@tamu.edu;xy_gong@tamu.edu;xianming.liu@horizon.ai;qian01.zhang@horizon.ai;yuan.li@horizon.ai;atlaswang@tamu.edu,8;6;8,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0,yes,9/25/19,Texas A&M;Texas A&M;Horizon Robotics;Horizon Robotics;Horizon Robotics;Texas A&M,44;44;-1;-1;-1;44,177;177;-1;-1;-1;177,2,9/25/19,9,4,2,0,0,1,146;19;83;124;322;2947,18;9;16;76;69;167,6;3;4;6;10;28,10;2;9;7;22;383,m;m
3479,ICLR,2020,Understanding Knowledge Distillation in Non-autoregressive Machine Translation,Chunting Zhou;Jiatao Gu;Graham Neubig,chuntinz@andrew.cmu.edu;jgu@fb.com;gneubig@cs.cmu.edu,8;8;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,5,2,yes,9/25/19,Carnegie Mellon University;Facebook;Carnegie Mellon University,1;-1;1,27;-1;27,3,9/25/19,18,10,5,1,0,4,559;1440;5418,20;41;442,9;15;38,63;217;557,f;m
3480,ICLR,2020,Building Deep Equivariant Capsule Networks,Sai Raam Venkataraman;S. Balasubramanian;R. Raghunatha Sarma,vsairaam@sssihl.edu.in;sbalasubramanian@sssihl.edu.in;rraghunathasarma@sssihl.edu.in,8;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,10,0,yes,9/25/19,Sri Sathya Sai Institute of Higher Learning;Sri Sathya Sai Institute of Higher Learning;Sri Sathya Sai Institute of Higher Learning,-1;-1;-1,-1;-1;-1,,8/4/19,4,1,1,0,0,1,4;16;6,3;9;6,1;3;2,1;2;1,m;m
3481,ICLR,2020,Understanding Architectures Learnt by Cell-based Neural Architecture Search,Yao Shu;Wei Wang;Shaofeng Cai,shuyao@comp.nus.edu.sg;wangwei@comp.nus.edu.sg;shaofeng@comp.nus.edu.sg,8;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0,yes,9/25/19,National University of Singapore;National University of Singapore;National University of Singapore,16;16;16,25;25;25,3;8,9/20/19,5,4,0,0,0,1,66;95;23,17;52;11,3;5;4,3;7;3,f;m
3482,ICLR,2020,Dynamically Pruned Message Passing Networks for Large-scale Knowledge Graph Reasoning,Xiaoran Xu;Wei Feng;Yunsheng Jiang;Xiaohui Xie;Zhiqing Sun;Zhi-Hong Deng,xiaoran.xu@hulu.com;wei.feng@hulu.com;yunsheng.jiang@hulu.com;xiaohui.xie@hulu.com;zhiqings@andrew.cmu.edu;zhdeng@pku.edu.cn,6;6;8,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,3,0,yes,9/25/19,Hulu LLC.;Hulu LLC.;Hulu LLC.;Hulu LLC.;Carnegie Mellon University;Peking University,-1;-1;-1;-1;1;22,-1;-1;-1;-1;27;24,10,9/25/19,0,0,0,0,0,0,283;736;109;94;187;1521,24;173;14;21;14;132,7;16;5;4;6;20,5;42;5;11;65;215,m;m
3483,ICLR,2020,Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model,Wenhan Xiong;Jingfei Du;William Yang Wang;Veselin Stoyanov,xwhan@cs.ucsb.edu;jingfeidu@fb.com;william@cs.ucsb.edu;ves@fb.com,8;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,5,0,yes,9/25/19,UC Santa Barbara;Facebook;UC Santa Barbara;Facebook,38;-1;38;-1,57;-1;57;-1,3;6,9/25/19,9,8,3,0,0,4,313;904;2333;3990,22;8;128;45,6;4;27;24,63;267;281;671,m;m
3484,ICLR,2020,Linear Symmetric Quantization of Neural Networks for Low-precision Integer Hardware,Xiandong Zhao;Ying Wang;Xuyi Cai;Cheng Liu;Lei Zhang,zhaoxiandong@ict.ac.cn;wangying2009@ict.ac.cn;caixuyi18s@ict.ac.cn;liucheng@ict.ac.cn;zlei@ict.ac.cn,6;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0,yes,9/25/19,"Institute of Computing Technology, Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences",59;59;59;59;59,1397;1397;1397;1397;1397,2,9/25/19,1,1,0,0,0,0,31;-1;3;266;243,19;-1;2;51;82,4;-1;1;7;5,0;0;0;8;14,u;m
3485,ICLR,2020,Consistency Regularization for Generative Adversarial Networks,Han Zhang;Zizhao Zhang;Augustus Odena;Honglak Lee,zhanghan@google.com;zizhaoz@google.com;augustusodena@google.com;honglak@google.com,6;6;8,I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:N/A:N/A,Accept (Poster),0,5,1,yes,9/25/19,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,5;4,9/25/19,21,12,7,1,0,4,24;978;3293;24514,11;59;25;166,2;15;13;62,4;102;477;2837,u;m
3486,ICLR,2020,Short and Sparse Deconvolution --- A Geometric Approach,Yenson Lau;Qing Qu;Han-Wen Kuo;Pengcheng Zhou;Yuqian Zhang;John Wright,y.lau@columbia.edu;qq213@nyu.edu;hk2673@columbia.edu;pz2230@columbia.edu;yz2557@cornell.edu;jw2966@columbia.edu,6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,Columbia University;New York University;Columbia University;Columbia University;Cornell University;Columbia University,15;25;15;15;7;15,16;29;16;16;19;16,,8/28/19,7,2,1,0,0,0,68;596;223;683;436;49,6;48;19;49;52;13,5;12;8;9;13;4,4;35;13;52;17;5,u;m
3487,ICLR,2020,GENESIS: Generative Scene Inference and Sampling with Object-Centric Latent Representations,Martin Engelcke;Adam R. Kosiorek;Oiwi Parker Jones;Ingmar Posner,martin@robots.ox.ac.uk;adamk@robots.ox.ac.uk;oiwi@robots.ox.ac.uk;ingmar@robots.ox.ac.uk,8;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,7,0,yes,9/25/19,University of Oxford;University of Oxford;University of Oxford;University of Oxford,50;50;50;50,1;1;1;1,5,7/30/19,25,16,7,1,0,2,467;349;418;2530,7;19;17;102,5;9;4;28,46;46;22;161,m;m
3488,ICLR,2020,BinaryDuo: Reducing Gradient Mismatch in Binary Activation Network by Coupling Binary Activations,Hyungjun Kim;Kyungsu Kim;Jinseok Kim;Jae-Joon Kim,hyungjun.kim@postech.ac.kr;kyungsu.kim@postech.ac.kr;jinseok.kim@postech.ac.kr;jaejoon@postech.ac.kr,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),1,7,1,yes,9/25/19,POSTECH;POSTECH;POSTECH;POSTECH,118;118;118;118,146;146;146;146,,9/25/19,4,0,1,0,0,1,110;9;73;178,34;17;50;58,6;2;5;4,2;1;10;6,m;m
3489,ICLR,2020,Transformer-XH: Multi-Evidence Reasoning with eXtra Hop Attention,Chen Zhao;Chenyan Xiong;Corby Rosset;Xia Song;Paul Bennett;Saurabh Tiwary,chenz@cs.umd.edu;chenyan.xiong@microsoft.com;corbin.rosset@microsoft.com;xiaso@microsoft.com;paul.n.bennett@microsoft.com;satiwary@microsoft.com,8;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,8,0,yes,9/25/19,"University of Maryland, College Park;Microsoft;Microsoft;Microsoft;Microsoft;Microsoft",12;-1;-1;-1;-1;-1,91;-1;-1;-1;-1;-1,3;10,9/25/19,5,1,2,0,0,0,106;37;45;595;488;504,66;8;11;41;44;14,5;3;4;6;9;5,4;3;2;101;18;97,u;u
3490,ICLR,2020,Sub-policy Adaptation for Hierarchical Reinforcement Learning,Alexander Li;Carlos Florensa;Ignasi Clavera;Pieter Abbeel,alexli1@berkeley.edu;florensa@berkeley.edu;iclavera@berkeley.edu;pabbeel@berkeley.edu,8;3,I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5,13;13;13;13,,6/13/19,2,1,0,0,5,0,31;488;437;37679,19;10;15;438,3;6;8;95,0;46;57;4511,m;m
3491,ICLR,2020,"Watch, Try, Learn: Meta-Learning from Demonstrations and Rewards",Allan Zhou;Eric Jang;Daniel Kappler;Alex Herzog;Mohi Khansari;Paul Wohlhart;Yunfei Bai;Mrinal Kalakrishnan;Sergey Levine;Chelsea Finn,ayz@stanford.edu;ejang@google.com;kappler@google.com;alexherzog@google.com;khansari@google.com;wohlhart@google.com;yunfeibai@google.com;kalakris@google.com;slevine@google.com;cbfinn@cs.stanford.edu,6;3;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,Stanford University;Google;Google;Google;Google;Google;Google;Google;Google;Stanford University,4;-1;-1;-1;-1;-1;-1;-1;-1;4,4;-1;-1;-1;-1;-1;-1;-1;-1;4,,6/7/19,5,3,0,0,0,0,41;1967;497;5;59;3445;855;2913;25074;7946,12;15;41;1;12;33;46;46;310;101,3;11;11;1;5;18;12;25;75;34,2;295;36;0;1;652;50;209;3254;1068,m;f
3492,ICLR,2020,Deep Orientation Uncertainty Learning based on a Bingham Loss,Igor Gilitschenski;Roshni Sahoo;Wilko Schwarting;Alexander Amini;Sertac Karaman;Daniela Rus,igilitschenski@mit.edu;rsahoo@mit.edu;wilkos@mit.edu;amini@mit.edu;sertac@mit.edu;rus@csail.mit.edu,6;6,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2;2;2,5;5;5;5;5;5,2,9/25/19,4,2,4,0,0,2,853;3;352;4;198;5023,79;1;27;6;32;166,15;1;11;1;7;29,44;2;24;2;17;217,m;f
3493,ICLR,2020,Overlearning Reveals Sensitive Attributes,Congzheng Song;Vitaly Shmatikov,cs2296@cornell.edu;shmat@cs.cornell.edu,6;1;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Poster),0,3,0,yes,9/25/19,Cornell University;Cornell University,7;7,19;19,3;7,5/28/19,9,7,2,0,0,0,1025;11422,22;145,10;54,130;1031,m;m
3494,ICLR,2020,Understanding the Limitations of Conditional Generative Models,Ethan Fetaya;Joern-Henrik Jacobsen;Will Grathwohl;Richard Zemel,ethanf@cs.toronto.edu;j.jacobsen@vectorinstitute.ai;wgrathwohl@cs.toronto.edu;zemel@cs.toronto.edu,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,4,0,yes,9/25/19,"Department of Computer Science, University of Toronto;Vector Institute;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",18;-1;18;18,18;-1;18;18,5;4,6/4/19,3,3,1,0,0,0,708;207;391;21903,26;11;13;209,12;5;5;52,76;38;87;2523,m;m
3495,ICLR,2020,Learning Disentangled Representations for CounterFactual Regression,Negar Hassanpour;Russell Greiner,hassanpo@ualberta.ca;rgreiner@ualberta.ca,8;8;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,1,yes,9/25/19,University of Alberta;University of Alberta,100;100,136;136,,9/25/19,2,2,1,1,0,1,52;10796,14;330,4;41,6;827,f;m
3496,ICLR,2020,Dynamical Distance Learning for Semi-Supervised and Unsupervised Skill Discovery,Kristian Hartikainen;Xinyang Geng;Tuomas Haarnoja;Sergey Levine,kristian.hartikainen@gmail.com;young.geng@berkeley.edu;tuomash@google.com;svlevine@eecs.berkeley.edu,6;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,9,3,yes,9/25/19,University of Oxford;University of California Berkeley;Google;University of California Berkeley,50;5;-1;5,1;13;-1;13,,7/18/19,9,5,3,0,0,2,297;313;1501;25074,11;8;22;310,5;4;7;75,54;36;354;3254,m;m
3497,ICLR,2020,Learning-Augmented Data Stream Algorithms,Tanqiu Jiang;Yi Li;Honghao Lin;Yisong Ruan;David P. Woodruff,taj320@lehigh.edu;yili@ntu.edu.sg;honghao_lin@sjtu.edu.cn;24320152202802@stu.xmu.edu.cn;dwoodruf@andrew.cmu.edu,8;8;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,3,0,yes,9/25/19,Lehigh University;National Taiwan University;Shanghai Jiao Tong University;Xiamen University;Carnegie Mellon University,266;86;53;64;1,633;120;157;8;27,,9/25/19,1,0,1,0,0,0,1;19;4;1;7140,1;21;2;1;432,1;2;1;1;44,0;0;0;0;558,u;m
3498,ICLR,2020,"Kaleidoscope: An Efficient, Learnable Representation For All Structured Linear Maps",Tri Dao;Nimit Sohoni;Albert Gu;Matthew Eichhorn;Amit Blonder;Megan Leszczynski;Atri Rudra;Christopher Ré,trid@stanford.edu;nims@stanford.edu;albertgu@stanford.edu;mae226@cornell.edu;amitblon@buffalo.edu;mleszczy@stanford.edu;atri@buffalo.edu;chrismre@cs.stanford.edu,6;8;8,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,5,0,yes,9/25/19,"Stanford University;Stanford University;Stanford University;Cornell University;State University of New York, Buffalo;Stanford University;State University of New York, Buffalo;Stanford University",4;4;4;7;84;4;84;4,4;4;4;19;263;4;263;4,1,9/25/19,0,0,0,0,0,0,88;152;231;10;0;68;2193;2785,16;7;31;4;2;7;160;70,5;2;7;2;0;2;25;20,11;5;29;1;0;4;185;415,m;m
3499,ICLR,2020,Playing the lottery with rewards and multiple languages: lottery tickets in RL and NLP,Haonan Yu;Sergey Edunov;Yuandong Tian;Ari S. Morcos,haonanu@gmail.com;edunov@fb.com;yuandong@fb.com;arimorcos@gmail.com,6;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,2,0,yes,9/25/19,;Facebook;Facebook;Facebook,-1;-1;-1;-1,-1;-1;-1;-1,3,6/6/19,21,14,3,0,6,1,918;1247;2498;1034,37;19;85;32,12;11;25;12,99;185;293;116,m;m
3500,ICLR,2020,Learning the Arrow of Time for Problems in Reinforcement Learning,Nasim Rahaman;Steffen Wolf;Anirudh Goyal;Roman Remme;Yoshua Bengio,nasim.rahaman@tuebingen.mpg.de;steffen.wolf@iwr.uni-heidelberg.de;anirudhgoyal9119@gmail.com;roman.remme@iwr.uni-heidelberg.de;yoshua.bengio@mila.quebec,6;8;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,16,0,yes,9/25/19,"Max Planck Institute for Intelligent Systems, Max-Planck Institute;Heidelberg University;University of Montreal;Heidelberg University;University of Montreal",-1;205;128;205;128,-1;44;85;44;85,,9/25/19,0,0,0,0,0,0,234;386;1137;1;211930,15;37;46;5;807,6;9;12;1;149,26;37;130;0;24480,m;m
3501,ICLR,2020,DD-PPO: Learning Near-Perfect PointGoal Navigators from 2.5 Billion Frames,Erik Wijmans;Abhishek Kadian;Ari Morcos;Stefan Lee;Irfan Essa;Devi Parikh;Manolis Savva;Dhruv Batra,etw@gatech.edu;akadian@fb.com;arimorcos@gmail.com;leestef@oregonstate.edu;irfan@gatech.edu;parikh@gatech.edu;msavva@sfu.ca;dbatra@gatech.edu,8;8;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,Georgia Institute of Technology;Facebook;Facebook;Oregon State University;Georgia Institute of Technology;Georgia Institute of Technology;Simon Fraser University;Georgia Institute of Technology,13;-1;-1;77;13;13;64;13,38;-1;-1;373;38;38;272;38,,9/25/19,13,5,6,1,0,2,242;102;1034;155;11629;86;4152;10396,13;10;32;27;236;18;57;174,6;3;12;7;51;5;24;42,32;21;116;24;993;10;835;1469,m;m
3502,ICLR,2020,Information Geometry of Orthogonal Initializations and Training,Piotr Aleksander Sokół;Il Memming Park,piotr.sokol@stonybrook.edu;memming.park@stonybrook.edu,6;8;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,8,0,yes,9/25/19,"State University of New York, Stony Brook;State University of New York, Stony Brook",41;41,304;304,1,10/9/18,5,1,2,0,17,0,16;693,10;60,2;13,0;46,m;m
3503,ICLR,2020,Strategies for Pre-training Graph Neural Networks,Weihua Hu*;Bowen Liu*;Joseph Gomes;Marinka Zitnik;Percy Liang;Vijay Pande;Jure Leskovec,weihuahu@stanford.edu;liubowen@stanford.edu;joegomes@stanford.edu;marinka@cs.stanford.edu;pliang@cs.stanford.edu;pande@stanford.edu;jure@cs.stanford.edu,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),1,3,0,yes,9/25/19,Stanford University;Stanford University;Stanford University;Stanford University;Stanford University;Stanford University;Stanford University,4;4;4;4;4;4;4,4;4;4;4;4;4;4,10;8,5/29/19,18,9,7,0,0,2,1346;24;2454;1969;188;19473;49600,63;7;176;89;27;471;302,13;2;26;19;7;76;93,296;4;117;191;11;811;6117,m;m
3504,ICLR,2020,Cross-Domain Few-Shot Classification via Learned Feature-Wise Transformation,Hung-Yu Tseng;Hsin-Ying Lee;Jia-Bin Huang;Ming-Hsuan Yang,htseng6@ucmerced.edu;hlee246@ucmerced.edu;jbhuang@vt.edu;mhyang@ucmerced.edu,6;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,5,0,yes,9/25/19,University of California at Merced;University of California at Merced;Virginia Tech;University of California at Merced,481;481;79;481,354;354;240;354,6;8,9/25/19,8,3,3,1,0,1,489;655;5543;5518,24;26;81;127,9;8;30;26,99;117;1099;1260,u;m
3505,ICLR,2020,How much Position Information Do Convolutional Neural Networks Encode?,Md Amirul Islam*;Sen Jia*;Neil D. B. Bruce,amirul@scs.ryerson.ca;sen.jia@ryerson.ca;bruce@ryerson.ca,8;8;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,3,5,yes,9/25/19,Ryerson University;Ryerson University;Ryerson University,323;323;323,739;739;739,,9/25/19,10,7,1,0,0,1,240;197;2433,36;15;74,7;8;16,16;16;390,m;m
3506,ICLR,2020,Generalization through Memorization: Nearest Neighbor Language Models,Urvashi Khandelwal;Omer Levy;Dan Jurafsky;Luke Zettlemoyer;Mike Lewis,urvashik@stanford.edu;omerlevy@gmail.com;jurafsky@stanford.edu;lsz@fb.com;mikelewis@fb.com,3;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),1,8,0,yes,9/25/19,Stanford University;Facebook;Stanford University;Facebook;Facebook,4;-1;4;-1;-1,4;-1;4;-1;-1,3,9/25/19,13,1,4,0,0,1,740;7667;30165;15157;69,12;58;309;176;8,8;30;76;53;5,83;1231;3511;2580;4,f;m
3507,ICLR,2020,On the Relationship between Self-Attention and Convolutional Layers,Jean-Baptiste Cordonnier;Andreas Loukas;Martin Jaggi,jean-baptiste.cordonnier@epfl.ch;andreas.loukas@epfl.ch;martin.jaggi@epfl.ch,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0,yes,9/25/19,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne,481;481;481,38;38;38,1,9/25/19,15,7,6,0,0,1,125;628;3883,6;53;114,3;15;27,24;51;580,m;m
3508,ICLR,2020,On Bonus Based Exploration Methods In The Arcade Learning Environment,Adrien Ali Taiga;William Fedus;Marlos C. Machado;Aaron Courville;Marc G. Bellemare,adrien.alitaiga@gmail.com;liamfedus@google.com;marlosm@google.com;aaron.courville@gmail.com;bellemare@google.com,6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,2,0,yes,9/25/19,University of Montreal;Google;Google;University of Montreal;Google,128;-1;-1;128;-1,85;-1;-1;85;-1,,9/25/19,3,0,1,0,0,1,3;696;637;63697;4112,1;25;31;203;57,1;10;11;66;25,1;91;79;8033;644,u;m
3509,ICLR,2020,The Curious Case of Neural Text Degeneration,Ari Holtzman;Jan Buys;Li Du;Maxwell Forbes;Yejin Choi,ahai@cs.washington.edu;jbuys@cs.uct.ac.za;dul2@cs.washington.edu;mbforbes@cs.washington.edu;yejin@cs.washington.edu,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,7,0,yes,9/25/19,University of Washington;University of Cape Town;University of Washington;University of Washington;University of Washington,6;390;6;6;6,26;136;26;26;26,3,4/22/19,144,78,67,6,0,45,620;396;-1;584;8068,18;19;-1;14;138,10;8;-1;8;43,96;62;0;92;1016,m;f
3510,ICLR,2020,A Mutual Information Maximization Perspective of Language Representation Learning,Lingpeng Kong;Cyprien de Masson d'Autume;Lei Yu;Wang Ling;Zihang Dai;Dani Yogatama,lingpenk@google.com;cyprien@google.com;leiyu@google.com;lingwang@google.com;zihangd@google.com;dyogatama@google.com,8;8;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Spotlight),0,5,1,yes,9/25/19,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,3;1;2,9/25/19,12,7,1,0,0,0,1091;74;177;489;2445;3601,32;6;88;100;27;41,14;4;8;9;14;21,112;10;6;21;442;408,m;m
3511,ICLR,2020,Measuring Compositional Generalization: A Comprehensive Method on Realistic Data,Daniel Keysers;Nathanael Schärli;Nathan Scales;Hylke Buisman;Daniel Furrer;Sergii Kashubin;Nikola Momchev;Danila Sinopalnikov;Lukasz Stafiniak;Tibor Tihon;Dmitry Tsarkov;Xiao Wang;Marc van Zee;Olivier Bousquet,keysers@google.com;schaerli@google.com;nkscales@google.com;hylke@google.com;danielfurrer@google.com;sergik@google.com;nikola@google.com;sinopalnikov@google.com;lukstafi@google.com;ttihon@google.com;tsar@google.com;wangxiao@google.com;marcvanzee@google.com;obousquet@google.com,6;8;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,4,0,yes,9/25/19,Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,3;8,9/25/19,5,3,1,0,0,0,4678;2;16;265;166;2;2;2;9;2;1779;4404;142;178,142;1;5;11;12;1;1;1;3;1;37;365;34;31,35;1;2;2;4;1;1;1;2;1;15;31;7;6,318;0;0;37;12;0;0;0;1;0;168;307;3;16,m;m
3512,ICLR,2020,GLAD: Learning Sparse Graph Recovery,Harsh Shrivastava;Xinshi Chen;Binghong Chen;Guanghui Lan;Srinivas Aluru;Han Liu;Le Song,hshrivastava3@gatech.edu;xinshi.chen@gatech.edu;binghong@gatech.edu;george.lan@isye.gatech.edu;aluru@cc.gatech.edu;hanliu@northwestern.edu;lsong@cc.gatech.edu,8;8;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,9,0,yes,9/25/19,Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Northwestern University;Georgia Institute of Technology,13;13;13;13;13;44;13,38;38;38;38;38;22;38,9;10,6/1/19,4,3,2,0,0,0,27;36;10;4687;3;-1;9670,14;14;5;90;1;-1;329,3;3;2;28;1;-1;54,0;1;0;631;0;0;1120,m;m
3513,ICLR,2020,Query-efficient Meta Attack to Deep Neural Networks,Jiawei Du;Hu Zhang;Joey Tianyi Zhou;Yi Yang;Jiashi Feng,dujiawei@u.nus.edu;hu.zhang-1@student.uts.edu.au;joey.tianyi.zhou@gmail.com;yi.yang@uts.edu.au;elefjia@nus.edu.sg,8;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),1,7,0,yes,9/25/19,National University of Singapore;University of Technology Sydney;;University of Technology Sydney;National University of Singapore,16;108;-1;108;16,25;193;-1;193;25,4,6/6/19,8,2,2,0,0,0,115;83;981;72;9533,33;34;84;26;332,5;4;18;6;52,6;9;94;1;1232,m;m
3514,ICLR,2020,Bounds on Over-Parameterization for Guaranteed Existence of Descent Paths in Shallow ReLU Networks,Arsalan Sharifnassab;Saber Salehkaleybar;S. Jamaloddin Golestani,a.sharifnassab@gmail.com;saber.salehk@gmail.com;golestani@sharif.edu,6;6,I have read many papers in this area.:N/A:N/A:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,2,0,yes,9/25/19,Sharif University of Technology;Sharif University of Technology;Sharif University of Technology,323;323;323,564;564;564,,9/25/19,0,0,0,0,0,0,52;67;137,16;30;5,4;4;2,4;5;20,m;u
3515,ICLR,2020,"A critical analysis of self-supervision, or what we can learn from a single image",Asano YM.;Rupprecht C.;Vedaldi A.,yuki@robots.ox.ac.uk;chrisr@robots.ox.ac.uk;vedaldi@robots.ox.ac.uk,6;6;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Poster),0,4,0,yes,9/25/19,University of Oxford;University of Oxford;University of Oxford,50;50;50,1;1;1,,4/30/19,10,5,1,0,0,0,52;189;35073,21;35;202,5;7;63,6;10;4696,m;m
3516,ICLR,2020,Geometric Insights into the Convergence of Nonlinear TD Learning,David Brandfonbrener;Joan Bruna,david.brandfonbrener@nyu.edu;bruna@cims.nyu.edu,8;6;3;8,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,New York University;New York University,25;25,29;29,1;9,5/29/19,5,2,0,0,0,0,17;11626,6;90,3;29,0;1292,m;m
3517,ICLR,2020,Geometric Analysis of Nonconvex Optimization Landscapes for Overcomplete Learning,Qing Qu;Yuexiang Zhai;Xiao Li;Yuqian Zhang;Zhihui Zhu,qingqu1006@gmail.com;ysz@berkeley.edu;xli@ee.cuhk.edu.hk;yqz.zhang@gmail.com;zzhu29@jhu.edu,8;8;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,5,0,yes,9/25/19,New York University;University of California Berkeley;The Chinese University of Hong Kong;Columbia University;Johns Hopkins University,25;5;59;15;73,29;13;35;16;12,9,9/25/19,5,4,1,2,0,2,596;27;283;436;945,48;6;94;52;101,12;3;10;13;16,35;7;6;17;49,m;m
3518,ICLR,2020,Network Deconvolution,Chengxi Ye;Matthew Evanusa;Hua He;Anton Mitrokhin;Tom Goldstein;James A. Yorke;Cornelia Fermuller;Yiannis Aloimonos,yechengxi@gmail.com;mevanusa@umd.edu;huah@umd.edu;amitrokh@umd.edu;tomg@cs.umd.edu;yorke@umd.edu;fer@umiacs.umd.edu;yiannis@cs.umd.edu,8;8;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,3,0,yes,9/25/19,"Amazon;University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park",-1;12;12;12;12;12;12;12,-1;91;91;91;91;91;91;91,,5/28/19,2,1,1,0,0,0,510;25;1895;103;6538;3828;2999;347,24;7;132;15;117;92;187;326,10;3;20;5;28;29;32;42,50;0;114;9;758;269;186;347,m;m
3519,ICLR,2020,Pure and Spurious Critical Points: a Geometric Study of Linear Networks,Matthew Trager;Kathlén Kohn;Joan Bruna,matthew.trager@cims.nyu.edu;kathlen.korn@gmail.com;bruna@cims.nyu.edu,8;3;3,I do not know much about this area.:N/A:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0,yes,9/25/19,"New York University;KTH Royal Institute of Technology, Stockholm, Sweden;New York University",25;128;25,29;222;29,,9/25/19,3,2,1,1,0,1,112;23;11626,26;10;90,6;3;29,9;1;1292,u;m
3520,ICLR,2020,PC-DARTS: Partial Channel Connections for Memory-Efficient Architecture Search,Yuhui Xu;Lingxi Xie;Xiaopeng Zhang;Xin Chen;Guo-Jun Qi;Qi Tian;Hongkai Xiong,yuhuixu@sjtu.edu.cn;198808xc@gmail.com;zxphistory@gmail.com;1410452@tongji.edu.cn;guojunq@gmail.com;tian.qi1@huawei.com;xionghongkai@sjtu.edu.cn,6;8;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,5,0,yes,9/25/19,Shanghai Jiao Tong University;Huawei Technologies Ltd.;National University of Singapore;Tsinghua University;University of Central Florida;Huawei Technologies Ltd.;Shanghai Jiao Tong University,53;-1;16;8;77;-1;53,157;-1;25;23;609;-1;157,,7/12/19,62,26,22,3,0,14,96;2206;489;1022;512;2389;1338,11;108;118;184;27;275;222,4;24;12;9;7;24;18,14;228;47;193;51;122;107,m;m
3521,ICLR,2020,Towards a Deep Network Architecture for Structured Smoothness,Haroun Habeeb;Oluwasanmi Koyejo,haroun7@gmail.com;sanmi@illinois.edu,6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Accept (Poster),0,2,0,yes,9/25/19,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",3;3,48;48,,9/25/19,0,0,0,0,0,0,5;1232,2;98,1;17,0;126,m;m
3522,ICLR,2020,RNNs Incrementally Evolving on an Equilibrium Manifold: A Panacea for Vanishing and Exploding Gradients?,Anil Kag;Ziming Zhang;Venkatesh Saligrama,anilkag@bu.edu;zzhang@merl.com;srv@bu.edu,8;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0,yes,9/25/19,Boston University;Mitsubishi Electric Research Labs;Boston University,67;-1;67,61;-1;61,,9/25/19,1,0,1,0,0,0,21;2573;5067,8;78;271,2;20;33,1;343;518,m;m
3523,ICLR,2020,"Deep Imitative Models for Flexible Inference, Planning, and Control",Nicholas Rhinehart;Rowan McAllister;Sergey Levine,nrhineha@cs.cmu.edu;rmcallister@berkeley.edu;svlevine@eecs.berkeley.edu,8;8;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),0,7,0,yes,9/25/19,Carnegie Mellon University;University of California Berkeley;University of California Berkeley,1;5;5,27;13;13,,10/15/18,32,20,13,2,126,3,390;576;24893,22;29;310,9;11;74,46;88;3235,m;m
3524,ICLR,2020,The Implicit Bias of Depth: How Incremental Learning Drives Generalization,Daniel Gissin;Shai Shalev-Shwartz;Amit Daniely,daniel.gissin@mail.huji.ac.il;shais@cs.huji.ac.il;amit.daniely@mail.huji.ac.il,6;6;6,I have published one or two papers in this area.:N/A:N/A:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,Hebrew University of Jerusalem;Hebrew University of Jerusalem;Hebrew University of Jerusalem,67;67;67,216;216;216,8,9/25/19,2,2,0,0,0,0,19;14101;901,2;168;46,2;48;16,3;1829;112,m;m
3525,ICLR,2020,BackPACK: Packing more into Backprop,Felix Dangel;Frederik Kunstner;Philipp Hennig,felix.dangel@tuebingen.mpg.de;kunstner@cs.ubc.ca;philipp.hennig@uni-tuebingen.de,8;8;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,3,0,yes,9/25/19,"Max Planck Institute for Intelligent Systems, Max-Planck Institute;University of British Columbia;University of Tuebingen",-1;35;154,-1;34;91,,9/25/19,9,3,9,0,0,0,44;39;2203,6;6;100,3;3;24,0;4;203,m;m
3526,ICLR,2020,Economy Statistical Recurrent Units For Inferring Nonlinear Granger Causality,Saurabh Khanna;Vincent Y. F. Tan,elesaur@nus.edu.sg;vtan@nus.edu.sg,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0,yes,9/25/19,National University of Singapore;National University of Singapore,16;16,25;25,,9/25/19,1,1,0,1,0,1,48;2953,12;277,5;25,5;228,m;m
3527,ICLR,2020,Detecting Extrapolation with Local Ensembles,David Madras;James Atwood;Alexander D'Amour,david.madras@mail.utoronto.ca;atwoodj@google.com;alexdamour@google.com,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,7,0,yes,9/25/19,Toronto University;Google;Google,18;-1;-1,18;-1;-1,,9/25/19,3,1,2,0,0,0,224;724;69,13;28;4,5;8;2,35;78;10,m;m
3528,ICLR,2020,Few-shot Text Classification with Distributional Signatures,Yujia Bao;Menghua Wu;Shiyu Chang;Regina Barzilay,yujia@csail.mit.edu;rmwu@mit.edu;shiyu.chang@ibm.com;regina@csail.mit.edu,6;1;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;International Business Machines;Massachusetts Institute of Technology,2;2;-1;2,5;5;-1;5,3;6;2,8/16/19,5,1,4,0,0,2,63;489;2991;12076,12;25;111;234,5;8;28;56,6;37;397;1215,m;f
3529,ICLR,2020,Scaling Autoregressive Video Models,Dirk Weissenborn;Oscar Täckström;Jakob Uszkoreit,diwe@google.com;oscar.tackstrom@gmail.com;usz@google.com,8;6;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Spotlight),0,6,0,yes,9/25/19,Google;Sana Labs;Google,-1;-1;-1,-1;-1;-1,4,6/6/19,10,8,7,0,0,3,626;2169;12039,28;33;36,10;16;23,61;324;2746,m;m
3530,ICLR,2020,Reformer: The Efficient Transformer,Nikita Kitaev;Lukasz Kaiser;Anselm Levskaya,kitaev@cs.berkeley.edu;lukaszkaiser@google.com;levskaya@google.com,8;6;8,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Talk),4,3,0,yes,9/25/19,University of California Berkeley;Google;Google,5;-1;-1,13;-1;-1,,9/25/19,60,35,23,1,0,12,471;22913;1540,12;75;11,9;23;8,63;3899;87,f;m
3531,ICLR,2020,Robust Subspace Recovery Layer for Unsupervised Anomaly Detection,Chieh-Hsin Lai;Dongmian Zou;Gilad Lerman,laixx313@umn.edu;dzou@umn.edu;lerman@umn.edu,8;6;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0,yes,9/25/19,"University of Minnesota, Minneapolis;University of Minnesota, Minneapolis;University of Minnesota, Minneapolis",59;59;59,79;79;79,,3/30/19,1,1,1,0,2,0,1;93;1784,3;14;78,1;5;25,0;13;193,m;m
3532,ICLR,2020,SCALOR: Generative World Models with Scalable Object Representations,Jindong Jiang*;Sepehr Janghorbani*;Gerard De Melo;Sungjin Ahn,jindong.jiang@rutgers.edu;sj620@scarletmail.rutgers.edu;gdm@demelo.org;sjn.ahn@gmail.com,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Accept (Poster),0,7,0,yes,9/25/19,Rutgers University;Rutgers University;Rutgers University;Rutgers University,34;34;34;34,168;168;168;168,5,9/25/19,4,3,2,0,0,1,43;16;2471;1389,9;8;165;41,4;3;29;12,7;1;230;161,m;f
3533,ICLR,2020,Learning Robust Representations via Multi-View Information Bottleneck,Marco Federici;Anjan Dutta;Patrick Forré;Nate Kushman;Zeynep Akata,m.federici@uva.nl;duttanjan@gmail.com;patrickforre@gmail.com;nate@kushman.org;zeynepakata@gmail.com,8;6;8,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),0,5,1,yes,9/25/19,University of Amsterdam;University of Exeter;University of Amsterdam;Microsoft Research;University of Tuebingen,172;390;172;-1;154,62;146;62;-1;91,8,9/25/19,3,2,0,0,0,1,3;963;98;1524;5737,5;144;13;29;67,1;17;6;17;24,1;60;14;201;814,m;f
3534,ICLR,2020,Mixup Inference: Better Exploiting Mixup to Defend Adversarial Attacks,Tianyu Pang*;Kun Xu*;Jun Zhu,pty17@mails.tsinghua.edu.cn;kunxu.thu@gmail.com;dcszj@mail.tsinghua.edu.cn,6;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,9,0,yes,9/25/19,Tsinghua University;Tsinghua University;Tsinghua University,8;8;8,23;23;23,4;8,9/25/19,5,3,1,0,0,1,971;12368;-1,26;387;-1,10;43;-1,181;739;0,m;m
3535,ICLR,2020,Adversarial Lipschitz Regularization,Dávid Terjék,david.terjek92@gmail.com,6;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,9,0,yes,9/25/19,,,,5;4,7/12/19,2,0,1,0,0,0,2,2,1,0,m;u
3536,ICLR,2020,Are Transformers universal approximators of sequence-to-sequence functions?,Chulhee Yun;Srinadh Bhojanapalli;Ankit Singh Rawat;Sashank Reddi;Sanjiv Kumar,chulheey@mit.edu;bsrinadh@google.com;ankitsrawat@google.com;sashank@google.com;sanjivk@google.com,6;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,5,0,yes,9/25/19,Massachusetts Institute of Technology;Google;Google;Google;Google,2;-1;-1;-1;-1,5;-1;-1;-1;-1,3;1,9/25/19,5,1,1,0,0,2,202;1642;1431;2194;941,22;28;84;53;142,6;16;19;20;13,14;199;140;418;113,m;m
3537,ICLR,2020,Escaping Saddle Points Faster with Stochastic Momentum,Jun-Kun Wang;Chi-Heng Lin;Jacob Abernethy,jimwang@gatech.edu;cl3385@gatech.edu;prof@gatech.edu,3;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,9,0,yes,9/25/19,Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology,13;13;13,38;38;38,9,9/25/19,0,0,0,0,0,0,48;13;1891,11;9;96,4;2;23,2;0;236,m;m
3538,ICLR,2020,SlowMo: Improving Communication-Efficient Distributed SGD with Slow Momentum,Jianyu Wang;Vinayak Tantia;Nicolas Ballas;Michael Rabbat,jianyuw1@andrew.cmu.edu;tantia@fb.com;ballasn@fb.com;mikerabbat@fb.com,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,Carnegie Mellon University;Facebook;Facebook;Facebook,1;-1;-1;-1,27;-1;-1;-1,3;9;8,9/25/19,10,6,2,0,0,0,291;37;5111;664,8;4;54;12,5;2;21;4,38;1;597;40,m;m
3539,ICLR,2020,SUMO: Unbiased Estimation of Log Marginal Probability for Latent Variable Models,Yucen Luo;Alex Beatson;Mohammad Norouzi;Jun Zhu;David Duvenaud;Ryan P. Adams;Ricky T. Q. Chen,luoyc15@mails.tsinghua.edu.cn;abeatson@cs.princeton.edu;mnorouzi@google.com;dcszj@mail.tsinghua.edu.cn;duvenaud@cs.toronto.edu;rpa@princeton.edu;rtqichen@cs.toronto.edu,8;8;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,5,0,yes,9/25/19,"Tsinghua University;Princeton University;Google;Tsinghua University;Department of Computer Science, University of Toronto;Princeton University;Department of Computer Science, University of Toronto",8;31;-1;8;18;31;18,23;6;-1;23;18;6;18,,9/25/19,4,4,2,0,0,1,168;114;8136;705;6070;12643;308,11;14;126;96;76;178;9,4;5;32;15;29;45;6,22;7;1021;15;763;1340;73,f;m
3540,ICLR,2020,Symplectic ODE-Net: Learning Hamiltonian Dynamics with Control,Yaofeng Desmond Zhong;Biswadip Dey;Amit Chakraborty,y.zhong@princeton.edu;biswadip.dey@siemens.com;amit.chakraborty@siemens.com,8;6;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,6,0,yes,9/25/19,Princeton University;Siemens Corporate Research;Siemens Corporate Research,31;-1;-1,6;-1;-1,10;8,9/25/19,16,11,7,0,0,3,27;159;1542,5;34;116,3;7;16,4;5;75,m;m
3541,ICLR,2020,Intrinsically Motivated Discovery of Diverse Patterns in Self-Organizing Systems,Chris Reinke;Mayalen Etcheverry;Pierre-Yves Oudeyer,chris.reinke@inria.fr;mayalen.etcheverry@inria.fr;chris.reinke@inria.fr;pierre-yves.oudeyer@inria.fr,8;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.,Accept (Talk),0,5,0,yes,9/25/19,INRIA;INRIA;INRIA;INRIA,-1;-1;-1;-1,-1;-1;-1;-1,,8/19/19,3,2,2,0,0,0,16;9;5658,7;5;302,3;2;35,0;0;318,m;m
3542,ICLR,2020,On the Convergence of FedAvg on Non-IID Data,Xiang Li;Kaixuan Huang;Wenhao Yang;Shusen Wang;Zhihua Zhang,smslixiang@pku.edu.cn;hackyhuang@pku.edu.cn;yangwhsms@gmail.com;shusen.wang@stevens.edu;zhzhang@math.pku.edu.cn,8;8;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,3,0,yes,9/25/19,Peking University;Peking University;Peking University;Stevens Institute of Technology;Peking University,22;22;22;154;22,24;24;24;605;24,1;9,7/4/19,68,45,15,4,0,15,774;170;209;1023;163,143;16;27;76;45,15;7;8;18;6,88;22;20;94;22,m;m
3543,ICLR,2020,The asymptotic spectrum of the Hessian of DNN throughout training,Arthur Jacot;Franck Gabriel;Clement Hongler,arthur.jacot@epfl.ch;franck.gabriel@epfl.ch;clement.hongler@epfl.ch,3;6;8,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne,481;481;481,38;38;38,,9/25/19,4,1,1,0,0,0,460;651;550,9;47;14,4;10;4,128;136;137,m;m
3544,ICLR,2020,AutoQ: Automated Kernel-Wise Neural Network Quantization ,Qian Lou;Feng Guo;Minje Kim;Lantao Liu;Lei Jiang.,louqian@iu.edu;fengguo@iu.edu;minje@indiana.edu;lantao@iu.edu;jiang60@iu.edu,6;3;8;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:N/A:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,"Indiana University, Bloomington;Indiana University, Bloomington;University of Arizona;Indiana University, Bloomington;Indiana University, Bloomington",73;73;172;73;73,134;134;103;134;134,,2/15/19,2,1,1,0,0,0,83;5;1264;428;23,16;16;77;54;35,3;1;14;12;2,7;0;110;18;0,m;m
3545,ICLR,2020,Learning Self-Correctable Policies and Value Functions from Demonstrations with Negative Sampling,Yuping Luo;Huazhe Xu;Tengyu Ma,yupingl@cs.princeton.edu;huazhe_xu@eecs.berkeley.edu;tengyuma@stanford.edu,6;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0,yes,9/25/19,Princeton University;University of California Berkeley;Stanford University,31;5;4,6;13;4,,7/12/19,2,1,1,0,0,0,1420;815;3939,58;14;88,16;7;32,132;99;508,m;m
3546,ICLR,2020,Projection-Based Constrained Policy Optimization,Tsung-Yen Yang;Justinian Rosca;Karthik Narasimhan;Peter J. Ramadge,ty3@princeton.edu;justinian.rosca@siemens.com;karthikn@cs.princeton.edu;ramadge@princeton.edu,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,Princeton University;Siemens Corporate Research;Princeton University;Princeton University,31;-1;31;31,6;-1;6;6,1;7,9/25/19,3,3,2,0,0,0,-1;-1;1181;-1,-1;-1;34;-1,-1;-1;13;-1,0;0;125;0,f;m
3547,ICLR,2020,Variational Autoencoders for Highly Multivariate Spatial Point Processes Intensities,Baichuan Yuan;Xiaowei Wang;Jianxin Ma;Chang Zhou;Andrea L. Bertozzi;Hongxia Yang,ybcmath@gmail.com;daemon.wxw@alibaba-inc.com;majx13fromthu@gmail.com;ericzhou.zc@alibaba-inc.com;bertozzi@math.ucla.edu;yang.yhx@alibaba-inc.com,6;8;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.,Accept (Poster),0,3,0,yes,9/25/19,"University of California, Los Angeles;Alibaba Group;Tsinghua University;Alibaba Group;University of California, Los Angeles;Alibaba Group",20;-1;8;-1;20;-1,17;-1;23;-1;17;-1,5;1;8,9/25/19,1,0,1,0,0,0,58;10120;79;1558;10302;614,12;534;33;193;405;89,3;46;5;21;52;12,5;451;8;107;750;74,m;f
3548,ICLR,2020,Single Episode Policy Transfer in Reinforcement Learning,Jiachen Yang;Brenden Petersen;Hongyuan Zha;Daniel Faissol,yjiachen@gmail.com;petersen33@llnl.gov;zha@cc.gatech.edu;faissol1@llnl.gov,8;8;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,7,0,yes,9/25/19,Georgia Institute of Technology;Lawrence Livermore National Labs;Georgia Institute of Technology;Lawrence Livermore National Labs,13;-1;13;-1,38;-1;38;-1,,9/25/19,5,0,0,0,0,0,1578;72;14480;13,149;19;408;8,20;6;62;2,46;2;1225;0,m;m
3549,ICLR,2020,Transferable Perturbations of Deep Feature Distributions,Nathan Inkawhich;Kevin Liang;Lawrence Carin;Yiran Chen,nathan.inkawhich@duke.edu;kevin.liang@duke.edu;lcarin@duke.edu;yiran.chen@duke.edu,3;8;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,6,0,yes,9/25/19,Duke University;Duke University;Duke University;Duke University,47;47;47;47,20;20;20;20,4,9/25/19,1,1,1,0,0,0,25;175;19642;634,5;19;819;75,2;7;66;12,0;5;1988;48,m;m
3550,ICLR,2020,Pruned Graph Scattering Transforms,Vassilis N. Ioannidis;Siheng Chen;Georgios B. Giannakis,ioann006@umn.edu;schen@merl.com;georgios@umn.edu,6;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,8,0,yes,9/25/19,"University of Minnesota, Minneapolis;Mitsubishi Electric Research Labs;University of Minnesota, Minneapolis",59;-1;59,79;-1;79,10;8,9/25/19,2,1,0,0,0,0,143;947;44603,26;45;1236,7;13;105,9;93;4438,m;m
3551,ICLR,2020,Efficient Probabilistic Logic Reasoning with Graph Neural Networks,Yuyu Zhang;Xinshi Chen;Yuan Yang;Arun Ramamurthy;Bo Li;Yuan Qi;Le Song,yuyu@gatech.edu;xinshi.chen@gatech.edu;yuanyang@gatech.edu;arun.ramamurthy@siemens.com;lbo@illinois.edu;yuan.qi@antfin.com;lsong@cc.gatech.edu,3;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,4,yes,9/25/19,"Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Siemens Corporate Research;University of Illinois, Urbana Champaign;Antfin;Georgia Institute of Technology",13;13;13;-1;3;-1;13,38;38;38;-1;48;-1;38,10,9/25/19,5,2,1,0,0,0,405;36;105;142;235;422;9519,79;14;73;39;147;24;329,7;3;4;7;9;5;54,46;1;7;7;25;37;1114,m;m
3552,ICLR,2020,Decoupling Representation and Classifier for Long-Tailed Recognition,Bingyi Kang;Saining Xie;Marcus Rohrbach;Zhicheng Yan;Albert Gordo;Jiashi Feng;Yannis Kalantidis,kang@u.nus.edu;xiesaining@gmail.com;maroffm@gmail.com;zhicheng.yan@live.com;albert.gordo.s@gmail.com;elefjia@nus.edu.sg;ykalant@image.ntua.gr,6;8;6,I have read many papers in this area.:I carefully checked the experiments.:N/A:N/A;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,1,yes,9/25/19,National University of Singapore;Facebook;Facebook;;Facebook;National University of Singapore;National Technical University of Athens,16;-1;-1;-1;-1;16;323,25;-1;-1;-1;-1;25;776,6,9/25/19,12,10,7,1,0,4,869;5529;11691;544;2814;9533;2660,41;26;90;32;44;332;42,15;13;45;12;20;52;18,68;960;1548;52;576;1232;410,m;m
3553,ICLR,2020,Coherent Gradients: An Approach to Understanding Generalization in Gradient Descent-based Optimization,Satrajit Chatterjee,satrajit@gmail.com,8;8;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,21,0,yes,9/25/19,Google,-1,-1,,9/25/19,4,2,1,0,0,0,1231,43,16,133,m
3554,ICLR,2020,Environmental drivers of systematicity and generalization in a situated agent,Felix Hill;Andrew Lampinen;Rosalia Schneider;Stephen Clark;Matthew Botvinick;James L. McClelland;Adam Santoro,felixhill@google.com;lampinen@stanford.edo;rgschneider@google.com;clarkstephen@google.com;botvinick@google.com;jlmcc@google.com;adamsantoro@google.com,6;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,19,0,yes,9/25/19,Google;;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,8,9/25/19,4,3,1,1,0,1,3564;91;180;6483;13789;43165;3065,52;22;9;189;147;424;35,22;5;4;44;45;82;20,675;6;36;640;1426;3307;347,m;m
3555,ICLR,2020,Compositional Language Continual Learning,Yuanpeng Li;Liang Zhao;Kenneth Church;Mohamed Elhoseiny,yuanpeng16@gmail.com;lzhao4ever@gmail.com;kenneth.ward.church@gmail.com;mohamed.elhoseiny@gmail.com,3;6;8,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,4,0,yes,9/25/19,;;;KAUST,-1;-1;-1;128,-1;-1;-1;1397,3,9/25/19,2,1,0,0,0,0,56;571;12240;1447,14;79;208;67,5;10;46;18,5;45;1080;189,m;m
3556,ICLR,2020,Vid2Game: Controllable Characters Extracted from Real-World Videos,Oran Gafni;Lior Wolf;Yaniv Taigman,oran.gafni@gmail.com;wolf@fb.com;yaniv@fb.com,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,3,0,yes,9/25/19,Facebook;Facebook;Facebook,-1;-1;-1,-1;-1;-1,,4/17/19,3,2,1,0,0,1,9;14176;5918,3;199;26,2;45;16,1;1640;563,m;m
3557,ICLR,2020,Infinite-horizon Off-Policy Policy Evaluation with Multiple Behavior Policies,Xinyun Chen;Lu Wang;Yizhe Hang;Heng Ge;Hongyuan Zha,chenxinyun@cuhk.edu.cn;luwang@stu.ecnu.edu.cn;hangyhan@mail.ustc.edu.cn;hengge@mail.sdu.edu.cn;zhahy@cuhk.edu.cn,6;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,4,0,yes,9/25/19,Tsinghua University;Australian National University;University of Science and Technology of China;Shandong University;Tsinghua University,8;108;481;154;8,23;50;80;658;23,,9/25/19,0,0,0,0,0,0,1701;527;0;326;14480,51;93;1;48;408,15;10;0;11;62,145;34;0;9;1225,m;m
3558,ICLR,2020,Learning Efficient Parameter Server Synchronization Policies for Distributed SGD,Rong Zhu;Sheng Yang;Andreas Pfadler;Zhengping Qian;Jingren Zhou,red.zr@alibaba-inc.com;yangsheng@hit.edu.cn;andreaswernerrober@alibaba-inc.com;zhengping.qzp@alibaba-inc.com;jingren.zhou@alibaba-inc.com,6;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,9,0,yes,9/25/19,Alibaba Group;Harbin Institute of Technology;Alibaba Group;Alibaba Group;Alibaba Group,-1;172;-1;-1;-1,-1;424;-1;-1;-1,,9/25/19,0,0,0,0,0,0,132;150;71;810;3168,59;56;13;17;130,5;7;3;8;26,1;13;9;75;296,m;m
3559,ICLR,2020,Unsupervised Clustering using Pseudo-semi-supervised Learning,Divam Gupta;Ramachandran Ramjee;Nipun Kwatra;Muthian Sivathanu,divam@cmu.edu;ramjee@microsoft.com;nipun.kwatra@microsoft.com;muthian@microsoft.com,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0,yes,9/25/19,Carnegie Mellon University;Microsoft;Microsoft;Microsoft,1;-1;-1;-1,27;-1;-1;-1,10,9/25/19,1,0,1,0,0,0,7;6930;1054;609,9;112;19;25,2;36;11;10,0;862;103;49,m;m
3560,ICLR,2020,Tree-Structured Attention with Hierarchical Accumulation,Xuan-Phi Nguyen;Shafiq Joty;Steven Hoi;Richard Socher,nxphi47@gmail.com;sjoty@salesforce.com,8;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,0,0,yes,9/25/19,National Taiwan University;SalesForce.com,86;-1,120;-1,3,9/25/19,2,2,1,0,0,2,5;2015;9066;53531,6;131;295;180,2;24;50;49,2;203;849;8917,m;m
3561,ICLR,2020,Sparse Coding with Gated Learned ISTA,Kailun Wu;Yiwen Guo;Ziang Li;Changshui Zhang,wukl14@mails.tsinghua.edu.cn;guoyiwen.ai@bytedance.com;liza19@mails.tsinghua.edu.cn;zcs@mail.tsinghua.edu.cn,8;8;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Accept (Spotlight),0,4,0,yes,9/25/19,Tsinghua University;Bytedance;Tsinghua University;Tsinghua University,8;-1;8;8,23;-1;23;23,,9/25/19,1,1,1,0,0,0,22;1041;103;728,10;27;9;41,2;7;1;8,3;159;5;57,m;m
3562,ICLR,2020,A Signal Propagation Perspective for Pruning Neural Networks at Initialization,Namhoon Lee;Thalaiyasingam Ajanthan;Stephen Gould;Philip H. S. Torr,namhoon@robots.ox.ac.uk;thalaiyasingam.ajanthan@anu.edu.au;stephen.gould@anu.edu.au;phst@robots.ox.ac.uk,8;3;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Spotlight),0,3,0,yes,9/25/19,University of Oxford;Australian National University;Australian National University;University of Oxford,50;108;108;50,1;50;50;1,,6/14/19,14,8,6,0,0,5,20;333;5657;28788,8;25;114;356,2;7;31;84,6;77;871;3878,m;m
3563,ICLR,2020,Functional vs. parametric equivalence of ReLU networks,Mary Phuong;Christoph H. Lampert,bphuong@ist.ac.at;chl@ist.ac.at,3;8;6,I have published in this field for several years.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:N/A:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:N/A:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),1,4,0,yes,9/25/19,Institute of Science and Technology Austria;Institute of Science and Technology Austria,481;481,1397;1397,1,9/25/19,1,0,0,0,0,0,48;7646,5;153,3;35,5;1153,f;m
3564,ICLR,2020,Jacobian Adversarially Regularized Networks for Robustness,Alvin Chan;Yi Tay;Yew Soon Ong;Jie Fu,guoweial001@e.ntu.edu.sg;ytay017@e.ntu.edu.sg;asysong@ntu.edu.sg;jie.fu@polymtl.ca,6;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,1,yes,9/25/19,National Taiwan University;National Taiwan University;National Taiwan University;Polytechnique Montreal,86;86;86;390,120;120;120;1397,5;4,9/25/19,4,2,0,0,0,0,1592;1429;440;704,133;68;26;116,19;19;6;14,116;166;29;37,m;m
3565,ICLR,2020,Reinforced active learning for image segmentation,Arantxa Casanova;Pedro O. Pinheiro;Negar Rostamzadeh;Christopher J. Pal,arantxa.casanova-paga@polymtl.ca;pedro@opinheiro.com;negar@elementai.com;chris.j.pal@gmail.com,6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0,yes,9/25/19,Polytechnique Montreal;Opinheiro;Element AI;Ecole Polytechnique de Montreal,390;-1;-1;390,1397;-1;-1;1397,1;2,9/25/19,1,0,1,0,0,0,1315;2454;389;8489,9;36;32;120,4;13;10;33,361;239;52;764,f;m
3566,ICLR,2020,Maximum Likelihood Constraint Inference for Inverse Reinforcement Learning,Dexter R.R. Scobee;S. Shankar Sastry,dscobee@eecs.berkeley.edu;sastry@eecs.berkeley.edu,3;6;6;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,4,0,yes,9/25/19,University of California Berkeley;University of California Berkeley,5;5,13;13,,9/12/19,0,0,0,0,0,0,8;33758,5;460,2;78,0;3463,m;m
3567,ICLR,2020,Weakly Supervised Disentanglement with Guarantees,Rui Shu;Yining Chen;Abhishek Kumar;Stefano Ermon;Ben Poole,ruishu@stanford.edu;cynnjjs@stanford.edu;abhishk@google.com;ermon@cs.stanford.edu;pooleb@google.com,8;8;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),1,10,0,yes,9/25/19,Stanford University;Stanford University;Google;Stanford University;Google,4;4;-1;4;-1,4;4;-1;4;-1,,9/25/19,8,5,5,1,0,1,556;962;3177;4958;4294,47;87;41;203;41,12;15;20;31;19,64;74;513;664;701,m;m
3568,ICLR,2020,PCMC-Net: Feature-based Pairwise Choice Markov Chains,Alix Lhéritier,alherit@gmail.com,8;6;6;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0,yes,9/25/19,Amadeus IT Group,-1,-1,,9/25/19,0,0,0,0,0,0,26,6,3,1,m;u
3569,ICLR,2020,Capsules with Inverted Dot-Product Attention Routing,Yao-Hung Hubert Tsai;Nitish Srivastava;Hanlin Goh;Ruslan Salakhutdinov,yaohungt@cs.cmu.edu;nitish_srivastava@apple.com;hanlin@apple.com;rsalakhutdinov@apple.com,3;6;8,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),1,4,0,yes,9/25/19,Carnegie Mellon University;Apple;Apple;Apple,1;-1;-1;-1,27;-1;-1;-1,,9/25/19,1,1,1,0,0,1,553;25788;388;69005,63;32;32;254,12;15;10;82,73;2376;22;7875,m;m
3570,ICLR,2020,Contrastive Representation Distillation,Yonglong Tian;Dilip Krishnan;Phillip Isola,yonglong@mit.edu;dilipkay@google.com;phillipi@mit.edu,3;6;6,I do not know much about this area.:I carefully checked the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,Massachusetts Institute of Technology;Google;Massachusetts Institute of Technology,2;-1;2,5;-1;5,,9/25/19,25,20,10,2,0,7,1681;5051;12579,22;73;73,13;22;27,184;654;2186,m;m
3571,ICLR,2020,Variance Reduction With Sparse Gradients,Melih Elibol;Lihua Lei;Michael I. Jordan,elibol@cs.berkeley.edu;lihualei@stanford.edu;jordan@cs.berkeley.edu,6;3;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0,yes,9/25/19,University of California Berkeley;Stanford University;University of California Berkeley,5;4;5,13;4;13,3,9/25/19,1,0,1,0,0,0,111;382;118202,5;36;847,3;8;140,8;62;16063,m;m
3572,ICLR,2020,FSNet: Compression of Deep Convolutional Neural Networks by Filter Summary,Yingzhen Yang;Jiahui Yu;Nebojsa Jojic;Jun Huan;Thomas S. Huang,superyyzg@gmail.com;jyu79@illinois.edu;jojic@microsoft.com;lukehuan@shenshangtech.com;t-huang1@illinois.edu,8;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,2,0,yes,9/25/19,"Independent Researcher;University of Illinois, Urbana Champaign;Microsoft;Shenshangtech;University of Illinois, Urbana Champaign",-1;3;-1;-1;3,-1;48;-1;-1;48,2,2/8/19,4,1,1,0,4,1,1848;-1;3500;2722;-1,83;-1;139;194;-1,19;-1;29;23;-1,173;0;317;240;0,m;m
3573,ICLR,2020,RNA Secondary Structure Prediction By Learning Unrolled Algorithms,Xinshi Chen;Yu Li;Ramzan Umarov;Xin Gao;Le Song,xinshi.chen@gatech.edu;yu.li@kaust.edu.sa;ramzan.umarov@kaust.edu.sa;xin.gao@kaust.edu.sa;lsong@cc.gatech.edu,8;6;8;8,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Talk),0,12,0,yes,9/25/19,Georgia Institute of Technology;KAUST;KAUST;KAUST;Georgia Institute of Technology,13;128;128;128;13,38;1397;1397;1397;38,,9/25/19,2,1,2,0,0,0,36;997;90;25;9519,14;433;12;32;329,3;14;3;2;54,1;113;9;1;1114,f;m
3574,ICLR,2020,The Variational Bandwidth Bottleneck: Stochastic Evaluation on an Information Budget,Anirudh Goyal;Yoshua Bengio;Matthew Botvinick;Sergey Levine,anirudhgoyal9119@gmail.com;yoshua.bengio@mila.quebec;botvinick@google.com;svlevine@eecs.berkeley.edu,6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,10,1,yes,9/25/19,University of Montreal;University of Montreal;Google;University of California Berkeley,128;128;-1;5,85;85;-1;13,8,9/25/19,2,1,0,0,0,0,1137;208566;13789;68,46;807;147;31,12;147;45;4,130;24297;1426;4,m;m
3575,ICLR,2020,Mixout: Effective Regularization to Finetune Large-scale Pretrained Language Models,Cheolhyoung Lee;Kyunghyun Cho;Wanmo Kang,bloodwass@kaist.ac.kr;kyunghyun.cho@nyu.edu;wanmo.kang@kaist.edu,6;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,5,1,yes,9/25/19,Korea Advanced Institute of Science and Technology;New York University;KAIST,481;25;20,110;29;110,3;8,9/25/19,5,4,3,2,0,2,8;46450;403,2;272;49,2;52;11,2;6610;37,m;m
3576,ICLR,2020,Novelty Detection Via Blurring,Sungik Choi;Sae-Young Chung,si_choi@kaist.ac.kr;schung@kaist.ac.kr,6;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,6,0,yes,9/25/19,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,481;481,110;110,5;4,9/25/19,0,0,0,0,0,0,8;4832,4;156,1;26,1;536,m;m
3577,ICLR,2020,Locality and Compositionality in Zero-Shot Learning,Tristan Sylvain;Linda Petrini;Devon Hjelm,tristan.sylvain@gmail.com;lindapetrini@gmail.com;devon.hjelm@microsoft.com,6;8;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,Microsoft;University of Amsterdam;Microsoft,-1;172;-1,-1;62;-1,6;8,9/25/19,2,1,0,1,0,0,84;1;1720,14;1;43,3;1;13,4;0;268,m;m
3578,ICLR,2020,"Prediction, Consistency, Curvature: Representation Learning for Locally-Linear Control",Nir Levine;Yinlam Chow;Rui Shu;Ang Li;Mohammad Ghavamzadeh;Hung Bui,nirlevine@google.com;yinlamchow@google.com;ruishu@stanford.edu;anglili@google.com;mgh@fb.com;v.hungbh1@vinai.io,6;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,3,0,yes,9/25/19,Google;Google;Stanford University;Google;Facebook;VinAI Research,-1;-1;4;-1;-1;-1,-1;-1;4;-1;-1;-1,1,9/4/19,6,3,2,1,0,0,95;763;556;541;538;2928,9;50;47;26;36;104,5;15;12;8;11;25,5;92;64;70;60;183,m;m
3579,ICLR,2020,Federated Learning with Matched Averaging,Hongyi Wang;Mikhail Yurochkin;Yuekai Sun;Dimitris Papailiopoulos;Yasaman Khazaeni,hongyiwang@cs.wisc.edu;mikhail.yurochkin@ibm.com;yuekai@umich.edu;dimitris@papail.io;yasaman.khazaeni@us.ibm.com,8;8;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,5,1,yes,9/25/19,University of Southern California;International Business Machines;University of Michigan;University of Wisconsin - Madison;International Business Machines,31;-1;8;14;-1,62;-1;21;51;-1,,9/25/19,12,6,5,1,0,2,103;139;1229;2993;192,24;27;46;73;30,5;6;15;24;9,11;7;173;386;5,m;f
3580,ICLR,2020,Efficient and Information-Preserving Future Frame Prediction and Beyond,Wei Yu;Yichao Lu;Steve Easterbrook;Sanja Fidler,gnosis@cs.toronto.edu;yichao@cs.toronto.edu;sme@cs.toronto.edu;fidler@cs.toronto.edu,6;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",18;18;18;18,18;18;18;18,5;2,9/25/19,2,2,2,0,0,1,12;960;7461;10743,66;50;216;160,2;11;42;49,1;92;589;1386,m;f
3581,ICLR,2020,Tranquil Clouds: Neural Networks for Learning Temporally Coherent Features in Point Clouds,Lukas Prantl;Nuttapong Chentanez;Stefan Jeschke;Nils Thuerey,lukas.prantl@tum.de;nuttapong26@gmail.com;jeschke@stefan-jeschke.com;nils.thuerey@tum.de,6;8;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,3,0,yes,9/25/19,Technical University Munich;;Stefan-jeschke;Technical University Munich,53;-1;-1;53,43;-1;-1;43,5,7/3/19,0,0,0,0,0,0,1810;2466;907;2623,208;60;51;122,19;22;19;32,56;180;52;189,m;m
3582,ICLR,2020,Robust training with ensemble consensus,Jisoo Lee;Sae-Young Chung,jisoolee@kaist.ac.kr;schung@kaist.ac.kr,3;6;8,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,5,0,yes,9/25/19,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,481;481,110;110,,9/25/19,2,1,1,0,0,0,3;4832,9;156,1;26,0;536,f;m
3583,ICLR,2020,Asymptotics of Wide Networks from Feynman Diagrams,Ethan Dyer;Guy Gur-Ari,edyer@google.com;guyga@google.com,6;6;8,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,4,0,yes,9/25/19,Google;Google,-1;-1,-1;-1,,9/25/19,17,11,7,2,0,2,459;1158,33;21,12;14,32;105,m;m
3584,ICLR,2020,Unbiased Contrastive Divergence Algorithm for Training Energy-Based Latent Variable Models,Yixuan Qiu;Lingsong Zhang;Xiao Wang,yixuanq@andrew.cmu.edu;lingsong@purdue.edu;wangxiao@purdue.edu,6;8;8,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,3,2,yes,9/25/19,Carnegie Mellon University;Purdue University;Purdue University,1;27;27,27;88;88,,9/25/19,1,0,1,0,0,0,77;248;4404,15;32;361,3;9;31,6;18;307,m;f
3585,ICLR,2020,Frequency-based Search-control in Dyna,Yangchen Pan;Jincheng Mei;Amir-massoud Farahmand,pan6@ualberta.ca;jmei2@ualberta.ca;farahmand@vectorinstitute.ai,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A,Accept (Poster),0,3,0,yes,9/25/19,University of Alberta;University of Alberta;Vector Institute,100;100;-1,136;136;-1,1,9/25/19,1,0,0,0,0,0,54;61;233,13;15;32,4;4;8,6;6;16,m;m
3586,ICLR,2020,Energy-based models for atomic-resolution protein conformations,Yilun Du;Joshua Meier;Jerry Ma;Rob Fergus;Alexander Rives,yilundu@mit.edu;jmeier@fb.com;maj@fb.com;robfergus@fb.com;arives@cs.nyu.edu,8;8;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,5,0,yes,9/25/19,Massachusetts Institute of Technology;Facebook;Facebook;Facebook;New York University,2;-1;-1;-1;25,5;-1;-1;-1;29,,9/25/19,4,1,2,0,0,0,128;152;16;53064;42,27;13;6;128;6,6;4;3;61;2,10;10;0;6486;1,m;m
3587,ICLR,2020,Augmenting Non-Collaborative Dialog Systems with Explicit Semantic and Strategic Dialog History,Yiheng Zhou;Yulia Tsvetkov;Alan W Black;Zhou Yu,yihengz1@cs.cmu.edu;ytsvetko@cs.cmu.edu;awb@cs.cmu.edu;joyu@ucdavis.edu,6;3;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,"Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;University of California, Davis",1;1;1;79,27;27;27;55,,9/25/19,1,0,0,0,0,0,373;1737;13660;553,12;69;448;142,6;20;53;11,9;208;1157;29,m;f
3588,ICLR,2020,Discovering Motor Programs by Recomposing Demonstrations,Tanmay Shankar;Shubham Tulsiani;Lerrel Pinto;Abhinav Gupta,tanmayshankar@fb.com;shubhtuls@fb.com;lerrel.pinto@gmail.com;abhinavg@cs.cmu.edu,3;8;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Accept (Poster),0,9,0,yes,9/25/19,Facebook;Facebook;University of California Berkeley;Carnegie Mellon University,-1;-1;5;1,-1;-1;13;27,,9/25/19,1,1,0,0,0,0,4;1857;1360;-1,5;36;33;-1,1;18;12;-1,0;232;103;0,m;m
3589,ICLR,2020,Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers,Junjie LIU;Zhe XU;Runbin SHI;Ray C. C. Cheung;Hayden K.H. So,jjliu@eee.hku.hk;zhexu22-c@my.cityu.edu.hk;rbshi@eee.hku.hk;r.cheung@cityu.edu.hk;hso@eee.hku.hk,6;3;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,3,yes,9/25/19,The University of Hong Kong;City University of Hong Kong;The University of Hong Kong;City University of Hong Kong;The University of Hong Kong,92;92;92;92;92,35;35;35;35;35,,9/25/19,3,1,1,0,0,0,139;-1;34;1609;797,43;-1;11;145;93,5;-1;3;20;14,7;0;3;154;65,m;m
3590,ICLR,2020,RTFM: Generalising to New Environment Dynamics via Reading,Victor Zhong;Tim Rocktäschel;Edward Grefenstette,victor@victorzhong.com;tim.rocktaeschel@gmail.com;egrefen@gmail.com,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:N/A:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),1,4,0,yes,9/25/19,University of Washington;Facebook AI Research;Facebook,6;-1;-1,26;-1;-1,,9/25/19,7,3,1,0,0,0,1830;2316;7175,17;48;57,10;22;25,331;289;859,m;m
3591,ICLR,2020,Causal Discovery with Reinforcement Learning,Shengyu Zhu;Ignavier Ng;Zhitang Chen,zhushengyu@huawei.com;ignavierng@cs.toronto.edu;chenzhitang2@huawei.com,8;8;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,6,2,yes,9/25/19,"Huawei Technologies Ltd.;Department of Computer Science, University of Toronto;Huawei Technologies Ltd.",-1;18;-1,-1;18;-1,10,6/11/19,9,8,4,1,0,2,95;-1;212,21;-1;39,5;-1;8,7;0;16,m;m
3592,ICLR,2020,FEW-SHOT LEARNING ON GRAPHS VIA SUPER-CLASSES BASED ON GRAPH SPECTRAL MEASURES,Jatin Chauhan;Deepak Nathani;Manohar Kaul,chauhanjatin100@gmail.com;deepakn1019@gmail.com;mkaul@iith.ac.in,6;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,6,0,yes,9/25/19,Indian Institute of Technology Hyderabad;Indian Institute of Technology Hyderabad;Indian Institute of Technology Hyderabad,205;205;205,713;713;713,6;10,9/25/19,3,1,1,0,0,0,35;39;385,3;5;26,2;2;9,9;9;27,u;u
3593,ICLR,2020,Controlling generative models with continuous factors of variations,Antoine Plumerault;Hervé Le Borgne;Céline Hudelot,antoine.plumerault@cea.fr;herve.le-borgne@cea.fr;celine.hudelot@centralesupelec.fr,6;6;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,CEA;CEA;CentraleSupelec,233;233;481,1027;1027;534,3;5;2,9/25/19,2,1,1,0,0,1,2;684;975,3;85;82,1;13;16,1;34;50,m;f
3594,ICLR,2020,Defending Against Physically Realizable Attacks on Image Classification,Tong Wu;Liang Tong;Yevgeniy Vorobeychik,tongwu@wustl.edu;liangtong@wustl.edu;yvorobeychik@wustl.edu,8;8;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Spotlight),0,9,0,yes,9/25/19,"Washington University, St. Louis;Washington University, St. Louis;Washington University, St. Louis",100;100;100,52;52;52,4,9/20/19,8,4,2,0,0,1,43;407;2452,17;64;226,4;8;27,1;19;175,m;m
3595,ICLR,2020,A Neural Dirichlet Process Mixture Model for Task-Free Continual Learning,Soochan Lee;Junsoo Ha;Dongsu Zhang;Gunhee Kim,soochan.lee@vision.snu.ac.kr;junsooha@hanyang.ac.kr;96lives@snu.ac.kr;gunhee@snu.ac.kr,6;6;8,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,1,yes,9/25/19,Seoul National University;Hanyang University;Seoul National University;Seoul National University,41;233;41;41,64;393;64;64,5;11,9/25/19,9,4,3,1,0,5,39;38;9;2032,3;14;9;86,3;4;1;24,8;5;4;255,m;m
3596,ICLR,2020,HiLLoC: lossless image compression with hierarchical latent variable models,James Townsend;Thomas Bird;Julius Kunze;David Barber,james.townsend@cs.ucl.ac.uk;thomas.bird@cs.ucl.ac.uk;julius.kunze@cs.ucl.ac.uk;david.barber@ucl.ac.uk,8;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,4,0,yes,9/25/19,University College London;University College London;University College London;University College London,50;50;50;50,15;15;15;15,5,9/25/19,2,1,2,0,0,0,8076;570;63;3898,237;46;8;200,42;10;3;28,630;38;6;416,m;m
3597,ICLR,2020,Multilingual Alignment of Contextual Word Representations,Steven Cao;Nikita Kitaev;Dan Klein,stevencao@berkeley.edu;kitaev@berkeley.edu;klein@berkeley.edu,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),0,8,0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,13;13;13,6,9/25/19,13,10,1,2,0,2,56;471;19863,5;12;233,4;9;65,5;63;2211,m;m
3598,ICLR,2020,Jelly Bean World: A Testbed for Never-Ending Learning,Emmanouil Antonios Platanios;Abulhair Saparov;Tom Mitchell,e.a.platanios@cs.cmu.edu;asaparov@cs.cmu.edu;tom.mitchell@cs.cmu.edu,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A,Accept (Poster),0,5,0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1,27;27;27,,9/25/19,0,0,0,0,0,0,617;518;154,23;7;39,8;4;8,50;47;11,m;m
3599,ICLR,2020,V-MPO: On-Policy Maximum a Posteriori Policy Optimization for Discrete and Continuous Control,H. Francis Song;Abbas Abdolmaleki;Jost Tobias Springenberg;Aidan Clark;Hubert Soyer;Jack W. Rae;Seb Noury;Arun Ahuja;Siqi Liu;Dhruva Tirumala;Nicolas Heess;Dan Belov;Martin Riedmiller;Matthew M. Botvinick,songf@google.com;aabdolmaleki@google.com;springenberg@google.com;aidanclark@google.com;soyer@google.com;jwrae@google.com;snoury@google.com;arahuja@google.com;liusiqi@google.com;dhruvat@google.com;heess@google.com;danbelov@google.com;riedmiller@google.com;botvinick@google.com,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),1,7,0,yes,9/25/19,Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,,9/25/19,10,5,6,0,0,2,1224;549;7449;87;2212;631;507;759;59;557;11597;618;25961;17176,27;45;54;12;22;19;4;37;22;10;104;9;190;58,13;10;28;6;11;10;4;11;5;6;37;6;40;36,118;59;798;12;272;74;91;49;7;62;1644;71;3578;1544,m;m
3600,ICLR,2020,Deep Graph Matching Consensus,Matthias Fey;Jan E. Lenssen;Christopher Morris;Jonathan Masci;Nils M. Kriege,matthias.fey@tu-dortmund.de;janeric.lenssen@udo.edu;christopher.morris@tu-dortmund.de;jonathan@nnaisense.com;nils.kriege@tu-dortmund.de,6;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,6,0,yes,9/25/19,TU Dortmund;TU Dortmund University;TU Dortmund;NNAISENSE;TU Dortmund,233;233;233;-1;233,354;354;354;-1;354,2;10,9/25/19,7,4,3,0,0,1,568;545;511;5616;464,14;19;40;47;42,7;6;8;22;10,64;58;80;480;52,m;m
3601,ICLR,2020,CATER: A diagnostic dataset for Compositional Actions & TEmporal Reasoning,Rohit Girdhar;Deva Ramanan,rgirdhar@cs.cmu.edu;deva@cs.cmu.edu,8;8;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Talk),0,3,1,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University,1;1,27;27,2,9/25/19,5,2,2,0,0,1,933;32040,20;163,8;59,127;5389,m;m
3602,ICLR,2020,Differentiable Reasoning over a Virtual Knowledge Base,Bhuwan Dhingra;Manzil Zaheer;Vidhisha Balachandran;Graham Neubig;Ruslan Salakhutdinov;William W. Cohen,bdhingra@andrew.cmu.edu;manzilzaheer@google.com;vbalacha@andrew.cmu.edu;gneubig@cs.cmu.edu;rsalakhu@cs.cmu.edu;wcohen@google.com,8;8;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,6,0,yes,9/25/19,Carnegie Mellon University;Google;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Google,1;-1;1;1;1;-1,27;-1;27;27;27;-1,3,9/25/19,4,3,0,0,0,0,1203;1602;2;5304;69005;22767,36;63;4;443;254;424,15;17;1;38;82;69,169;263;1;547;7875;2635,m;m
3603,ICLR,2020,Finding and Visualizing Weaknesses of Deep Reinforcement Learning Agents,Christian Rupprecht;Cyril Ibrahim;Christopher J. Pal,christian.rupprecht@eng.ox.ac.uk;cyril.ibrahim@elementai.com;christopher.pal@polymtl.ca,6;3;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,4,0,yes,9/25/19,University of Oxford;Element AI;Polytechnique Montreal,50;-1;390,1;-1;1397,5,4/2/19,2,0,0,0,2,0,1259;13;8489,45;3;120,13;2;33,211;3;764,m;m
3604,ICLR,2020,Intriguing Properties of Adversarial Training at Scale,Cihang Xie;Alan Yuille,cihangxie306@gmail.com;alan.l.yuille@gmail.com,6;8;6,I have published one or two papers in this area.:I carefully checked the experiments.:I made a quick assessment of this paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Poster),0,4,1,yes,9/25/19,Johns Hopkins University;Johns Hopkins University,73;73,12;12,4,6/10/19,12,7,0,1,0,1,1411;35020,23;534,12;81,159;3872,m;m
3605,ICLR,2020,Four Things Everyone Should Know to Improve Batch Normalization,Cecilia Summers;Michael J. Dinneen,ceciliasummers07@gmail.com;mjd@cs.auckland.ac.nz,6;3;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,2,yes,9/25/19,University of Auckland;University of Auckland,266;266,177;177,,6/9/19,2,1,1,0,0,0,18;862,4;166,1;16,0;53,f;m
3606,ICLR,2020,Effect of Activation Functions on the Training of Overparametrized Neural Nets,Abhishek Panigrahi;Abhishek Shetty;Navin Goyal,abhishekpanigrahi034@gmail.com;ashetty1995@gmail.com;navingo@microsoft.com,8;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,2,0,yes,9/25/19,Microsoft;Cornell University;Microsoft,-1;7;-1,-1;19;-1,1,8/16/19,3,1,0,0,0,0,21;22;1845,9;24;81,3;3;16,2;2;240,m;m
3607,ICLR,2020,Differentiation of Blackbox Combinatorial Solvers,Marin Vlastelica Pogančić;Anselm Paulus;Vit Musil;Georg Martius;Michal Rolinek,marin.vlastelica@tue.mpg.de;anselm.paulus@tuebingen.mpg.de;vejtek@atrey.karlin.mff.cuni.cz;georg.martius@tuebingen.mpg.de;michal.rolinek@tuebingen.mpg.de,8;8;8,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,6,0,yes,9/25/19,"Max Planck Institute for Intelligent Systems, Max-Planck Institute;Max Planck Institute for Intelligent Systems, Max-Planck Institute;Charles University, Prague;Max Planck Institute for Intelligent Systems, Max-Planck Institute;Max Planck Institute for Intelligent Systems, Max-Planck Institute",-1;-1;481;-1;-1,-1;-1;495;-1;-1,,9/25/19,11,5,3,0,0,0,31;13;21;556;181,3;3;6;77;20,2;2;3;14;9,1;0;0;29;19,m;m
3608,ICLR,2020,Learning Heuristics for Quantified Boolean Formulas through Reinforcement Learning,Gil Lederman;Markus Rabe;Sanjit Seshia;Edward A. Lee,gilled@berkeley.edu;mrabe@google.com;sshesia@eecs.berkeley.edu;eal@eecs.berkeley.edu,3;8;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,University of California Berkeley;Google;University of California Berkeley;University of California Berkeley,5;-1;5;5,13;-1;13;13,,7/20/18,4,3,3,1,0,1,30;642;10056;24612,7;38;320;574,3;14;49;65,2;83;820;2046,m;m
3609,ICLR,2020,Inductive representation learning on temporal graphs,da Xu;chuanwei ruan;evren korpeoglu;sushant kumar;kannan achan,da.xu@walmartlabs.com;ruanchuanwei@gmail.com;ekorpeoglu@walmart.com;skumar4@walmartlabs.com;kachan@walmartlabs.com,8;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),1,4,0,yes,9/25/19,University of California Berkeley;;Walmart;Walmartlabs;Walmartlabs,5;-1;-1;-1;-1,13;-1;-1;-1;-1,1;10,9/25/19,3,1,1,0,0,1,14477;56;54;14;822,492;10;14;10;35,53;3;4;3;10,974;5;3;2;67,m;m
3610,ICLR,2020,Multi-Scale Representation Learning  for Spatial Feature Distributions using Grid Cells,Gengchen Mai;Krzysztof Janowicz;Bo Yan;Rui Zhu;Ling Cai;Ni Lao,gengchen_mai@geog.ucsb.edu;janowicz@ucsb.edu;boyan1@linkedin.com;ruizhu@geog.ucsb.edu;lingcai@ucsb.edu;noon99@gmail.com,8;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,7,1,yes,9/25/19,UC Santa Barbara;UC Santa Barbara;LinkedIn;UC Santa Barbara;UC Santa Barbara;mosaix.ai,38;38;-1;38;38;-1,57;57;-1;57;57;-1,3,9/25/19,5,3,2,0,0,0,247;5691;6;46;219;2965,30;292;13;39;51;50,9;40;1;4;7;17,12;407;0;1;26;386,m;m
3611,ICLR,2020,Efficient Riemannian Optimization on the Stiefel Manifold via the Cayley Transform,Jun Li;Fuxin Li;Sinisa Todorovic,liju2@oregonstate.edu;fuxin.li@oregonstate.edu;sinisa@oregonstate.edu,3;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,9,2,yes,9/25/19,Oregon State University;Oregon State University;Oregon State University,77;77;77,373;373;373,9,9/25/19,8,5,4,0,0,1,324;2412;4399,97;82;152,10;24;38,19;318;363,m;m
3612,ICLR,2020,"Neural tangent kernels, transportation mappings, and universal approximation",Ziwei Ji;Matus Telgarsky;Ruicheng Xian,ziweiji2@illinois.edu;mjt@illinois.edu;rxian2@illinois.edu,6;8,I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:N/A:N/A:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,1,yes,9/25/19,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",3;3;3,48;48;48,1,9/25/19,3,1,1,0,0,1,169;2045;6,16;44;2,5;15;2,28;346;1,m;m
3613,ICLR,2020,Dream to Control: Learning Behaviors by Latent Imagination,Danijar Hafner;Timothy Lillicrap;Jimmy Ba;Mohammad Norouzi,mail@danijar.com;countzero@google.com;jba@cs.toronto.edu;mnorouzi@google.com,6;8;6;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),1,6,1,yes,9/25/19,"Department of Computer Science, University of Toronto;Google;Department of Computer Science, University of Toronto;Google",18;-1;18;-1,18;-1;18;-1,,9/25/19,24,15,14,3,0,8,644;24158;52924;8136,24;74;56;126,9;39;22;32,68;2943;8625;1021,m;m
3614,ICLR,2020,From Inference to Generation: End-to-end Fully Self-supervised Generation of Human Face from Speech,Hyeong-Seok Choi;Changdae Park;Kyogu Lee,kekepa15@snu.ac.kr;cdpark@connect.ust.hk;kglee@snu.ac.kr,8;6;3,I have read many papers in this area.:I carefully checked the experiments.:N/A:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),1,5,0,yes,9/25/19,Seoul National University;The Hong Kong University of Science and Technology;Seoul National University,41;39;41,64;47;64,5,9/25/19,1,1,0,0,0,0,35;62;1323,11;32;120,2;4;20,8;2;111,m;m
3615,ICLR,2020,Neural Symbolic Reader: Scalable Integration of Distributed and Symbolic Representations for Reading Comprehension,Xinyun Chen;Chen Liang;Adams Wei Yu;Denny Zhou;Dawn Song;Quoc V. Le,xinyun.chen@berkeley.edu;crazydonkey@google.com;adamsyuwei@google.com;dennyzhou@google.com;dawnsong.travel@gmail.com;qvl@google.com,8;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Spotlight),0,8,0,yes,9/25/19,University of California Berkeley;Google;Google;Google;University of California Berkeley;Google,5;-1;-1;-1;5;-1,13;-1;-1;-1;13;-1,,9/25/19,6,2,1,0,0,0,46;118;817;74;37432;48901,9;47;28;11;278;193,2;6;13;5;95;81,4;3;146;10;4087;6080,f;m
3616,ICLR,2020,Population-Guided Parallel Policy Search for Reinforcement Learning,Whiyoung Jung;Giseung Park;Youngchul Sung,wy.jung@kaist.ac.kr;gs.park@kaist.ac.kr;ycsung@kaist.ac.kr,6;8;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,481;481;481,110;110;110,,9/25/19,2,0,0,0,0,0,1;1;1456,4;5;121,1;1;21,0;0;118,m;m
3617,ICLR,2020,Abstract Diagrammatic Reasoning with Multiplex Graph Networks,Duo Wang;Mateja Jamnik;Pietro Lio,wd263@cam.ac.uk;mateja.jamnik@cl.cam.ac.uk;pietro.lio@cl.cam.ac.uk,6;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,7,0,yes,9/25/19,University of Cambridge;University of Cambridge;University of Cambridge,71;71;71,3;3;3,10,9/25/19,2,1,2,0,0,1,7;441;767,8;104;96,2;11;8,1;13;81,m;m
3618,ICLR,2020,Logic and the 2-Simplicial Transformer,James Clift;Dmitry Doryn;Daniel Murfet;James Wallbridge,jamesedwardclift@gmail.com;dmitry.doryn@gmail.com;d.murfet@unimelb.edu.au;james.wallbridge@gmail.com,3;3;8,I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,8,0,yes,9/25/19,;;The University of Melbourne;,-1;-1;118;-1,-1;-1;32;-1,,9/2/19,1,1,0,0,0,0,11;60;383;17,7;15;60;8,1;5;9;3,1;3;31;0,m;m
3619,ICLR,2020,Meta Reinforcement Learning with Autonomous Inference of Subtask Dependencies,Sungryull Sohn;Hyunjae Woo;Jongwook Choi;Honglak Lee,srsohn@umich.edu;hjwoo@umich.edu;jwook@umich.edu;honglak@eecs.umich.edu,6;6;6,I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0,yes,9/25/19,University of Michigan;University of Michigan;University of Michigan;University of Michigan,8;8;8;8,21;21;21;21,10;1;6,9/25/19,1,1,0,0,0,0,5;39;18;24514,6;10;24;166,1;3;2;62,0;1;0;2837,m;m
3620,ICLR,2020,A Latent Morphology Model for Open-Vocabulary Neural Machine Translation,Duygu Ataman;Wilker Aziz;Alexandra Birch,duyguataman@gmail.com;will.aziz@gmail.com;a.birch@ed.ac.uk,6;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,0,0,yes,9/25/19,University of Zurich;University of Amsterdam;University of Edinburgh,143;172;33,90;62;30,3;2;8,9/25/19,1,1,1,0,0,1,86;795;10138,13;49;89,5;12;23,5;69;1841,f;f
3621,ICLR,2020,Mutual Mean-Teaching: Pseudo Label Refinery for Unsupervised Domain Adaptation on Person Re-identification,Yixiao Ge;Dapeng Chen;Hongsheng Li,yxge@link.cuhk.edu.hk;chendapeng@sensetime.com;hsli@ee.cuhk.edu.hk,6;6;8,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,The Chinese University of Hong Kong;SenseTime Group Limited;The Chinese University of Hong Kong,59;-1;59,35;-1;35,,9/25/19,11,5,4,1,0,3,80;20;44,7;17;23,2;3;4,8;2;6,m;m
3622,ICLR,2020,Variational Hetero-Encoder Randomized GANs for Joint Image-Text Modeling,Hao Zhang;Bo Chen;Long Tian;Zhengjue Wang;Mingyuan Zhou,zhanghao_xidian@163.com;bchen@mail.xidian.edu.cn;tianlong_xidian@163.com;zhengjuewang@163.com;mingyuan.zhou@mccombs.utexas.edu,6;3;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,"163;Tsinghua University;163;Xidian University;University of Texas, Austin",-1;8;-1;481;22,-1;23;-1;919;38,5;4;11,5/18/19,0,0,0,0,0,0,6374;9261;227;31;2033,522;980;53;8;115,33;41;8;4;24,286;291;3;0;235,m;m
3623,ICLR,2020,Toward Evaluating Robustness of Deep Reinforcement Learning with Continuous Control,Tsui-Wei Weng;Krishnamurthy (Dj) Dvijotham*;Jonathan Uesato*;Kai Xiao*;Sven Gowal*;Robert Stanforth*;Pushmeet Kohli,twweng@mit.edu;dvij@google.com;juesato@google.com;kaix@mit.edu;sgowal@google.com;stanforth@google.com;pushmeet@google.com,6;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,8,0,yes,9/25/19,Massachusetts Institute of Technology;Google;Google;Massachusetts Institute of Technology;Google;Google;Google,2;-1;-1;2;-1;-1;-1,5;-1;-1;5;-1;-1;-1,4,9/25/19,0,0,0,0,0,0,633;1143;952;122;568;484;22578,27;76;17;26;34;18;313,10;17;11;6;11;9;69,72;101;116;6;62;57;2782,f;m
3624,ICLR,2020,BlockSwap: Fisher-guided Block Substitution for Network Compression on a Budget,Jack Turner;Elliot J. Crowley;Michael O'Boyle;Amos Storkey;Gavin Gray,jack.turner@ed.ac.uk;elliot.j.crowley@ed.ac.uk;mob@inf.ed.ac.uk;a.storkey@ed.ac.uk;g.d.b.gray@ed.ac.uk,6;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,University of Edinburgh;University of Edinburgh;University of Edinburgh;University of Edinburgh;University of Edinburgh,33;33;33;33;33,30;30;30;30;30,,6/10/19,1,1,0,0,0,0,78;298;667;3905;59,17;23;76;198;33,4;9;11;32;3,6;29;38;446;2,m;m
3625,ICLR,2020,Decoding As Dynamic Programming For Recurrent Autoregressive Models,Najam Zaidi;Trevor Cohn;Gholamreza Haffari,syed.zaidi1@monash.edu;t.cohn@unimelb.edu.au;reza.haffari@gmail.com,8;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Accept (Poster),0,4,0,yes,9/25/19,Monash University;The University of Melbourne;,118;118;-1,75;32;-1,10,9/25/19,0,0,0,0,0,0,90;4967;1735,4;180;129,2;40;22,5;468;140,m;m
3626,ICLR,2020,LEARNING EXECUTION THROUGH NEURAL CODE FUSION,Zhan Shi;Kevin Swersky;Daniel Tarlow;Parthasarathy Ranganathan;Milad Hashemi,zshi17@cs.utexas.edu;kswersky@google.com;dtarlow@google.com;parthas@google.com;miladh@google.com,6;8;3,I do not know much about this area.:I assessed the sensibility of the experiments.:N/A:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Accept (Poster),0,3,0,yes,9/25/19,"University of Texas, Austin;Google;Google;Google;Google",22;-1;-1;-1;-1,38;-1;-1;-1;-1,6;10,6/17/19,3,1,1,0,0,0,1172;5797;2565;152;250,147;52;69;12;22,17;23;23;4;7,104;885;310;23;32,m;m
3627,ICLR,2020,Detecting and Diagnosing Adversarial Images with Class-Conditional Capsule Reconstructions,Yao Qin;Nicholas Frosst;Sara Sabour;Colin Raffel;Garrison Cottrell;Geoffrey Hinton,yaq007@eng.ucsd.edu;frosst@google.com;sasabour@google.com;craffel@google.com;gary@eng.ucsd.edu;geoffhinton@google.com,6;8;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,"University of California, San Diego;Google;Google;Google;University of California, San Diego;Google",11;-1;-1;-1;11;-1,31;-1;-1;-1;31;-1,4,7/5/19,10,3,4,1,0,2,788;1964;1971;4773;11225;217566,44;12;12;62;312;415,10;6;7;23;44;128,122;485;490;497;1130;21700,f;m
3628,ICLR,2020,Reconstructing continuous distributions of 3D protein structure from cryo-EM images,Ellen D. Zhong;Tristan Bepler;Joseph H. Davis;Bonnie Berger,zhonge@mit.edu;tbepler@mit.edu;jhdavis@mit.edu;bab@mit.edu,8;8;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Spotlight),0,3,1,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2,5;5;5;5,5,9/11/19,12,6,2,0,0,1,73;30;677;2374,10;7;39;65,5;4;10;16,4;2;45;150,f;f
3629,ICLR,2020,Sampling-Free Learning of Bayesian Quantized Neural Networks,Jiahao Su;Milan Cvitkovic;Furong Huang,jiahaosu@terpmail.umd.edu;mcvitkov@caltech.edu;furongh@cs.umd.edu,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,"University of Maryland, College Park;California Institute of Technology;University of Maryland, College Park",12;143;12,91;2;91,11,9/25/19,0,0,0,0,0,0,19;45;1752,13;11;130,3;4;20,3;3;188,m;f
3630,ICLR,2020,On Solving Minimax Optimization Locally: A Follow-the-Ridge Approach,Yuanhao Wang*;Guodong Zhang*;Jimmy Ba,yuanhao-16@mails.tsinghua.edu.cn;gdzhang@cs.toronto.edu;jba@cs.toronto.edu,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),1,21,0,yes,9/25/19,"Tsinghua University;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",8;18;18,23;18;18,5,9/25/19,8,4,2,2,0,3,76;1335;52924,5;16;56,3;11;22,12;225;8625,m;m
3631,ICLR,2020,Lagrangian Fluid Simulation with Continuous Convolutions,Benjamin Ummenhofer;Lukas Prantl;Nils Thuerey;Vladlen Koltun,benjamin.ummenhofer@intel.com;lukas.prantl@tum.de;nils.thuerey@tum.de;vkoltun@gmail.com,8;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0,yes,9/25/19,Intel;Technical University Munich;Technical University Munich;Intel,-1;53;53;-1,-1;43;43;-1,10,9/25/19,9,4,5,1,0,1,484;1810;2623;18005,13;208;122;191,7;19;32;63,60;56;189;2537,m;m
3632,ICLR,2020,Stochastic Weight Averaging in Parallel: Large-Batch Training That Generalizes Well,Vipul Gupta;Santiago Akle Serrano;Dennis DeCoste,vipul_gupta@berkeley.edu;sakle@apple.com;ddecoste@apple.com,6;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,University of California Berkeley;Apple;Apple,5;-1;-1,13;-1;-1,2;8,9/25/19,1,0,0,0,0,0,460;7;2608,32;3;73,7;2;23,41;0;228,m;m
3633,ICLR,2020,Adversarial AutoAugment,Xinyu Zhang;Qiang Wang;Jian Zhang;Zhao Zhong,zhangxinyu10@huawei.com;wangqiang168@huawei.com;zhangjian157@huawei.com;zorro.zhongzhao@huawei.com,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,10,0,yes,9/25/19,Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.,-1;-1;-1;-1,-1;-1;-1;-1,4;8,9/25/19,9,4,4,0,0,1,711;139;625;702,102;95;81;254,12;6;13;11,74;8;43;68,m;m
3634,ICLR,2020,Episodic Reinforcement Learning with Associative Memory,Guangxiang Zhu*;Zichuan Lin*;Guangwen Yang;Chongjie Zhang,guangxiangzhu@outlook.com;linzc16@mails.tsinghua.edu.cn;ygw@tsinghua.edu.cn;chongjie@tsinghua.edu.cn,6;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),1,4,0,yes,9/25/19,Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University,8;8;8;8,23;23;23;23,10,9/25/19,0,0,0,0,0,0,104;24;2788;551,21;9;306;59,6;3;22;12,4;2;158;44,m;m
3635,ICLR,2020,NAS-Bench-1Shot1: Benchmarking and Dissecting One-shot Neural Architecture Search,Arber Zela;Julien Siems;Frank Hutter,zelaa@cs.uni-freiburg.de;siemsj@cs.uni-freiburg.de;fh@cs.uni-freiburg.de,1;8;8,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0,yes,9/25/19,Universität Freiburg;Universität Freiburg;Universität Freiburg,118;118;118,85;85;85,,9/25/19,17,13,10,5,0,5,101;23;13008,7;4;233,4;2;51,11;5;1554,m;m
3636,ICLR,2020,You Only Train Once: Loss-Conditional Training of Deep Networks,Alexey Dosovitskiy;Josip Djolonga,adosovitskiy@gmail.com;josip@djolonga.com,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),0,5,0,yes,9/25/19,Google;Google,-1;-1,-1;-1,5,9/25/19,2,0,0,0,0,0,2042;353,13;21,5;9,338;24,m;m
3637,ICLR,2020,Q-learning with UCB Exploration is Sample Efficient for Infinite-Horizon MDP,Yuanhao Wang;Kefan Dong;Xiaoyu Chen;Liwei Wang,yuanhao-16@mails.tsinghua.edu.cn;dkf16@mails.tsinghua.edu.cn;cxy30@pku.edu.cn;wanglw@cis.pku.edu.cn,6;6;6;6,I have read many papers in this area.:N/A:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:N/A:N/A:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:N/A:N/A:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,8,0,yes,9/25/19,Tsinghua University;Tsinghua University;Peking University;Peking University,8;8;22;22,23;23;24;24,5;1,1/27/19,22,12,7,2,0,8,76;32;969;858,5;5;97;42,3;3;13;13,12;8;76;107,m;m
3638,ICLR,2020,Deep Audio Priors Emerge From Harmonic Convolutional Networks,Zhoutong Zhang;Yunyun Wang;Chuang Gan;Jiajun Wu;Joshua B. Tenenbaum;Antonio Torralba;William T. Freeman,ztzhang@mit.edu;wyy@mit.edu;ganchuang1990@gmail.com;jiajunwu@mit.edu;jbt@mit.edu;torralba@mit.edu;billf@mit.edu,6;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,6,1,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;International Business Machines;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;-1;2;2;2;2,5;5;-1;5;5;5;5,8,9/25/19,3,1,2,0,0,1,332;-1;2233;4039;31160;3085;44210,17;-1;81;89;594;66;402,7;-1;26;30;83;13;93,50;0;230;386;2697;448;4346,m;m
3639,ICLR,2020,Abductive Commonsense Reasoning,Chandra Bhagavatula;Ronan Le Bras;Chaitanya Malaviya;Keisuke Sakaguchi;Ari Holtzman;Hannah Rashkin;Doug Downey;Wen-tau Yih;Yejin Choi,chandrab@allenai.org;ronanlb@allenai.org;chaitanyam@allenai.org;keisukes@allenai.org;arih@allenai.org;hrashkin@uw.edu;dougd@allenai.org;scottyih@fb.com;yejinc@allenai.org,8;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,"Allen Institute for Artificial Intelligence;Allen Institute for Artificial Intelligence;Allen Institute for Artificial Intelligence;Allen Institute for Artificial Intelligence;Allen Institute for Artificial Intelligence;University of Washington, Seattle;Allen Institute for Artificial Intelligence;Facebook;Allen Institute for Artificial Intelligence",-1;-1;-1;-1;-1;6;-1;-1;-1,-1;-1;-1;-1;-1;26;-1;-1;-1,3,8/15/19,21,13,3,0,0,3,770;517;480;596;620;646;3882;87;8068,29;53;15;27;18;18;67;18;138,11;14;8;14;10;12;22;6;43,115;51;55;82;96;119;424;11;1016,m;f
3640,ICLR,2020,DeepHoyer: Learning Sparser Neural Network with Differentiable Scale-Invariant Sparsity Measures,Huanrui Yang;Wei Wen;Hai Li,huanrui.yang@duke.edu;wei.wen@duke.edu;hai.li@duke.edu,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,Duke University;Duke University;Duke University,47;47;47,20;20;20,,8/27/19,6,2,3,0,0,1,77;295;321,16;60;40,6;7;7,10;32;41,m;f
3641,ICLR,2020,DeFINE: Deep Factorized Input Token Embeddings for Neural Sequence Modeling,Sachin Mehta;Rik Koncel-Kedziorski;Mohammad Rastegari;Hannaneh Hajishirzi,sacmehta@uw.edu;kedzior@uw.edu;mohammadr@allenai.org;hannaneh@washington.edu,6;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,13,1,yes,9/25/19,"University of Washington, Seattle;University of Washington, Seattle;Allen Institute for Artificial Intelligence;University of Washington",6;6;-1;6,26;26;-1;26,3,9/25/19,1,0,0,0,0,0,42;200;-1;2907,6;14;-1;96,4;8;-1;25,2;23;0;599,m;f
3642,ICLR,2020,Never Give Up: Learning Directed Exploration Strategies,Adrià Puigdomènech Badia;Pablo Sprechmann;Alex Vitvitskyi;Daniel Guo;Bilal Piot;Steven Kapturowski;Olivier Tieleman;Martin Arjovsky;Alexander Pritzel;Andrew Bolt;Charles Blundell,adriap@google.com;psprechmann@google.com;avlife@google.com;danielguo@google.com;piot@google.com;skapturowski@google.com;tieleman@google.com;martinarjovsky@gmail.com;apritzel@google.com;abolt@google.com;cblundell@google.com,6;8;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0,yes,9/25/19,Google;Google;Google;Google;Google;Google;Google;New York University;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;25;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;29;-1;-1;-1,,9/25/19,7,6,3,0,0,1,4143;2225;8;116;2106;106;153;8036;5339;4;6176,11;44;3;12;70;8;23;15;25;1;50,6;18;2;4;18;4;4;9;13;1;22,772;217;0;14;276;26;2;1795;1077;0;1088,m;m
3643,ICLR,2020,Beyond Linearization: On Quadratic and Higher-Order Approximation of Wide Neural Networks,Yu Bai;Jason D. Lee,yubai.pku@gmail.com;jasondlee88@gmail.com,6;6;6,I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:N/A:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,1,yes,9/25/19,SalesForce.com;University of Southern California,-1;31,-1;62,1;8,9/25/19,15,7,4,1,0,0,-1;4788,-1;120,-1;36,0;614,m;m
3644,ICLR,2020,Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity,Jingzhao Zhang;Tianxing He;Suvrit Sra;Ali Jadbabaie,jzhzhang@mit.edu;tianxing@mit.edu;suvrit@mit.edu;jadbabai@mit.edu,8;8;8,I have read many papers in this area.:I did not assess the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,8,0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2,5;5;5;5,1,5/28/19,8,2,2,0,0,0,439;150;6583;15351,41;18;163;300,7;5;41;47,17;19;1010;1005,m;m
3645,ICLR,2020,Thieves on Sesame Street! Model Extraction of BERT-based APIs,Kalpesh Krishna;Gaurav Singh Tomar;Ankur P. Parikh;Nicolas Papernot;Mohit Iyyer,kalpesh@cs.umass.edu;gtomar@google.com;aparikh@google.com;papernot@google.com;miyyer@cs.umass.edu,8;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Accept (Poster),0,3,1,yes,9/25/19,"University of Massachusetts, Amherst;Google;Google;Google;University of Massachusetts, Amherst",28;-1;-1;-1;28,209;-1;-1;-1;209,3;4;6,9/25/19,5,3,1,1,0,2,42;98;1519;9462;5810,10;9;43;66;44,4;5;17;27;15,9;11;191;1100;1045,m;m
3646,ICLR,2020,Accelerating SGD with momentum for over-parameterized learning,Chaoyue Liu;Mikhail Belkin,liu.2656@buckeyemail.osu.edu;mbelkin@cse.ohio-state.edu,3;8;8,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,Ohio State University;Ohio State University,77;77,373;373,1;9,10/31/18,9,5,3,0,0,1,11;17243,4;120,2;40,1;2069,u;m
3647,ICLR,2020,GraphZoom: A Multi-level Spectral Approach for Accurate and Scalable Graph Embedding,Chenhui Deng;Zhiqiang Zhao;Yongyu Wang;Zhiru Zhang;Zhuo Feng,cd574@cornell.edu;qzzhao@mtu.edu;yongyuw@mtu.edu;zhiruz@cornell.edu;zfeng12@stevens.edu,6;8;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Talk),0,7,0,yes,9/25/19,Cornell University;Michigan Technological University;Michigan Technological University;Cornell University;Stevens Institute of Technology,7;323;323;7;154,19;1397;1397;19;605,10,9/25/19,3,0,0,0,0,0,54;11;-1;2158;56,17;12;-1;105;21,4;2;-1;22;4,0;0;0;203;3,m;m
3648,ICLR,2020,Low-Resource Knowledge-Grounded Dialogue Generation,Xueliang Zhao;Wei Wu;Chongyang Tao;Can Xu;Dongyan Zhao;Rui Yan,xl.zhao@pku.edu.cn;wuwei@microsoft.com;chongyangtao@pku.edu.cn;can.xu@microsoft.com;zhaody@pku.edu.cn;ruiyan@pku.edu.cn,6;8;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,4,0,yes,9/25/19,Peking University;Microsoft;Peking University;Microsoft;Peking University;Peking University,22;-1;22;-1;22;22,24;-1;24;-1;24;24,,9/25/19,3,2,1,0,0,0,8;339;195;381;2718;212,2;110;19;73;163;35,2;5;7;8;28;4,1;74;23;45;311;27,m;m
3649,ICLR,2020,Diverse Trajectory Forecasting with Determinantal Point Processes,Ye Yuan;Kris M. Kitani,yyuan2@cs.cmu.edu;kkitani@cs.cmu.edu,6;6;8,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University,1;1,27;27,5,7/11/19,10,5,4,0,0,1,370;3444,47;164,9;25,32;311,m;m
3650,ICLR,2020,Estimating Gradients for Discrete Random Variables by Sampling without Replacement,Wouter Kool;Herke van Hoof;Max Welling,w.w.m.kool@uva.nl;h.c.vanhoof@uva.nl;m.welling@uva.nl,6;8;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Spotlight),0,6,0,yes,9/25/19,University of Amsterdam;University of Amsterdam;University of Amsterdam,172;172;172,62;62;62,,9/25/19,6,3,3,1,0,1,682;1238;27129,28;45;270,11;16;59,65;203;5160,m;m
3651,ICLR,2020,Maxmin Q-learning: Controlling the Estimation Bias of Q-learning,Qingfeng Lan;Yangchen Pan;Alona Fyshe;Martha White,qlan3@ualberta.ca;pan6@ualberta.ca;alona@ualberta.ca;whitem@ualberta.ca,6;3;8,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,11,0,yes,9/25/19,University of Alberta;University of Alberta;University of Alberta;University of Alberta,100;100;100;100,136;136;136;136,1;8,9/25/19,5,3,4,1,0,2,7;54;845;310,3;13;46;33,2;4;13;6,2;6;90;20,u;f
3652,ICLR,2020,Editable Neural Networks,Anton Sinitsin;Vsevolod Plokhotnyuk;Dmitry Pyrkin;Sergei Popov;Artem Babenko,ant.sinitsin@gmail.com;vsevolod-pl@yandex.ru;alagaster@yandex.ru;sapopov@yandex-team.ru;artem.babenko@phystech.edu,6;3;8,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,8,0,yes,9/25/19,Higher School of Economics;Higher School of Economics;Higher School of Economics;Yandex;Moscow Institute of Physics and Technology,481;481;481;-1;481,251;251;251;-1;234,3,9/25/19,1,1,0,0,0,0,3;1;1;1;1657,2;1;1;5;29,1;1;1;1;10,0;0;0;0;282,m;m
3653,ICLR,2020,Rethinking Softmax Cross-Entropy Loss for Adversarial Robustness,Tianyu Pang;Kun Xu;Yinpeng Dong;Chao Du;Ning Chen;Jun Zhu,pty17@mails.tsinghua.edu.cn;kunxu.thu@gmail.com;dyp17@mails.tsinghua.edu.cn;duchao0726@gmail.com;ningchen@mail.tsinghua.edu.cn;dcszj@mail.tsinghua.edu.cn,6;6;8;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,14,0,yes,9/25/19,Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University,8;8;8;8;8;8,23;23;23;23;23;23,4;8,5/25/19,8,5,3,0,0,0,971;1429;1381;1318;4984;15601,26;154;28;107;589;1056,10;20;15;16;33;40,181;67;206;96;407;799,m;m
3654,ICLR,2020,Self-labelling via simultaneous clustering and representation learning,Asano YM.;Rupprecht C.;Vedaldi A.,yuki@robots.ox.ac.uk;chrisr@robots.ox.ac.uk;vedaldi@robots.ox.ac.uk,8;3;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Spotlight),0,5,2,yes,9/25/19,University of Oxford;University of Oxford;University of Oxford,50;50;50,1;1;1,,9/25/19,12,7,6,0,0,3,52;189;35073,21;35;202,5;7;63,6;10;4696,m;m
3655,ICLR,2020,Semi-Supervised Generative Modeling for Controllable Speech Synthesis,Raza Habib;Soroosh Mariooryad;Matt Shannon;Eric Battenberg;RJ Skerry-Ryan;Daisy Stanton;David Kao;Tom Bagby,raza.habib@cs.ucl.ac.uk;soroosh@google.com;mattshannon@google.com;ebattenberg@google.com;rjryan@google.com;daisy@google.com;davidkao@google.com;tombagby@google.com,6;8;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,9,0,yes,9/25/19,University College London;Google;Google;Google;Google;Google;Google;Google,50;-1;-1;-1;-1;-1;-1;-1,15;-1;-1;-1;-1;-1;-1;-1,5,9/25/19,3,1,0,0,0,0,517;404;568;2826;1443;938;1066;138,9;20;29;31;14;12;77;10,3;13;13;14;10;9;14;6,39;21;39;268;279;167;80;5,m;m
3656,ICLR,2020,EMPIR: Ensembles of Mixed Precision Deep Networks for Increased Robustness Against Adversarial Attacks,Sanchari Sen;Balaraman Ravindran;Anand Raghunathan,sen9@purdue.edu;ravi@cse.iitm.ac.in;raghunathan@purdue.edu,6;6;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,Purdue University;Indian Institute of Technology Madras;Purdue University,27;154;27,88;641;88,4,9/25/19,4,3,1,0,0,1,38;2586;284,10;236;30,4;28;4,4;206;32,f;m
3657,ICLR,2020,Training Recurrent Neural Networks Online by Learning Explicit State Variables,Somjit Nath;Vincent Liu;Alan Chan;Xin Li;Adam White;Martha White,somjit@ualberta.ca;vliu1@ualberta.ca;achan4@ualberta.ca;xzli@ualberta.ca;amw8@ualberta.ca;whitem@ualberta.ca,6;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,9,0,yes,9/25/19,University of Alberta;University of Alberta;University of Alberta;University of Alberta;University of Alberta;University of Alberta,100;100;100;100;100;100,136;136;136;136;136;136,9,9/25/19,0,0,0,0,0,0,26;1543;97;504;472;310,7;147;23;161;32;33,2;16;4;10;8;6,1;187;11;21;42;20,m;f
3658,ICLR,2020,MEMO: A Deep Network for Flexible Combination of Episodic Memories,Andrea Banino;Adrià Puigdomènech Badia;Raphael Köster;Martin J. Chadwick;Vinicius Zambaldi;Demis Hassabis;Caswell Barry;Matthew Botvinick;Dharshan Kumaran;Charles Blundell,abanino@google.com;adriap@google.com;rkoster@google.com;mjchadwick@google.com;vzambaldi@google.com;dhteam@google.com;caswell.barry@ucl.ac.uk;botvinick@google.com;dkumaran@google.com;cblundell@google.com,8;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,2,0,yes,9/25/19,Google;Google;Google;Google;Google;Google;University College London;Google;Google;Google,-1;-1;-1;-1;-1;-1;50;-1;-1;-1,-1;-1;-1;-1;-1;-1;15;-1;-1;-1,,9/25/19,4,2,0,0,0,0,677;4143;8;1176;1542;24137;1106;13789;17280;6176,9;11;4;38;18;55;27;147;65;50,4;6;2;16;12;31;9;45;36;22,44;772;2;80;160;2594;143;1426;2301;1088,m;m
3659,ICLR,2020,Differentiable learning of numerical rules in knowledge graphs,Po-Wei Wang;Daria Stepanova;Csaba Domokos;J. Zico Kolter,poweiw@cs.cmu.edu;daria.stepanova@de.bosch.com;csaba.domokos@de.bosch.com;zkolter@cs.cmu.edu,3;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,6,0,yes,9/25/19,Carnegie Mellon University;Bosch;Bosch;Carnegie Mellon University,1;-1;-1;1,27;-1;-1;27,10,9/25/19,0,0,0,0,0,0,247;193;515;7776,17;37;33;107,7;8;9;35,31;6;66;1073,m;m
3660,ICLR,2020,Multiplicative Interactions and Where to Find Them,Siddhant M. Jayakumar;Wojciech M. Czarnecki;Jacob Menick;Jonathan Schwarz;Jack Rae;Simon Osindero;Yee Whye Teh;Tim Harley;Razvan Pascanu,sidmj@google.com;lejlot@google.com;jmenick@google.com;schwarzjn@google.com;jwrae@google.com;osindero@google.com;ywteh@google.com;tharley@google.com;razp@google.com,8;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Poster),0,3,0,yes,9/25/19,Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1,,9/25/19,7,7,0,0,0,1,192;406;558;149;631;14818;23746;4173;17189,14;27;8;85;19;38;247;14;101,9;6;7;8;10;21;53;7;46,23;37;70;11;74;1866;3235;816;1700,m;m
3661,ICLR,2020,Recurrent neural circuits for contour detection,Drew Linsley*;Junkyung Kim*;Alekh Ashok;Thomas Serre,drew_linsley@brown.edu;junkyung_kim@brown.edu;alekh_karkada_ashok@brown.edu;thomas_serre@brown.edu,6;8;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,7,0,yes,9/25/19,Brown University;Brown University;Brown University;Brown University,67;67;67;67,53;53;53;53,2,9/25/19,2,1,0,0,0,0,66;53;3;8070,13;19;5;117,3;4;1;29,3;2;0;1222,m;m
3662,ICLR,2020,Provable Benefit of Orthogonal Initialization in Optimizing Deep Linear Networks,Wei Hu;Lechao Xiao;Jeffrey Pennington,huwei@cs.princeton.edu;xlc@google.com;jpennin@google.com,6;8;3,I have published one or two papers in this area.:N/A:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,Princeton University;Google;Google,31;-1;-1,6;-1;-1,1,9/25/19,9,5,0,2,0,1,20;453;16550,30;19;51,2;8;20,1;63;2640,m;m
3663,ICLR,2020,Sliced Cramer Synaptic Consolidation for Preserving Deeply Learned Representations,Soheil Kolouri;Nicholas A. Ketz;Andrea Soltoggio;Praveen K. Pilly,skolouri@hrl.com;naketz@hrl.com;a.soltoggio@lboro.ac.uk;pkpilly@hrl.com,8;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,2,0,yes,9/25/19,HRL Labs;HRL Labs;Loughborough University;HRL Labs,-1;-1;390;-1,-1;-1;374;-1,,9/25/19,3,2,0,0,0,0,836;117;515;480,61;12;45;50,18;5;11;11,42;8;68;18,m;m
3664,ICLR,2020,Neural Outlier Rejection for Self-Supervised Keypoint Learning,Jiexiong Tang;Hanme Kim;Vitor Guizilini;Sudeep Pillai;Rares Ambrus,jiexiong@kth.se;hanme.kim@tri.global;vitor.guizilini@tri.global;sudeep.pillai@tri.global;rares.ambrus@tri.global,6;8;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,8,0,yes,9/25/19,"KTH Royal Institute of Technology, Stockholm, Sweden;Toyota Research Institute;Toyota Research Institute;Toyota Research Institute;Toyota Research Institute",128;-1;-1;-1;-1,222;-1;-1;-1;-1,,9/25/19,1,0,0,0,0,0,964;277;260;419;462,12;8;52;30;30,6;3;10;9;11,93;15;20;25;19,m;m
3665,ICLR,2020,vq-wav2vec: Self-Supervised Learning of Discrete Speech Representations,Alexei Baevski;Steffen Schneider;Michael Auli,alexei.b@gmail.com;stes@fb.com;michael.auli@gmail.com,8;8;6;8,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,5,0,yes,9/25/19,Facebook;Facebook;Facebook,-1;-1;-1,-1;-1;-1,3,9/25/19,27,18,10,2,0,3,720;86;6931,17;6;71,9;4;31,105;8;1027,m;m
3666,ICLR,2020,A Fair Comparison of Graph Neural Networks for Graph Classification,Federico Errica;Marco Podda;Davide Bacciu;Alessio Micheli,federico.errica@phd.unipi.it;marco.podda@di.unipi.it;bacciu@di.unipi.it;micheli@di.unipi.it,8;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,12,0,yes,9/25/19,University of Pisa;University of Pisa;University of Pisa;University of Pisa,233;233;233;233,366;366;366;366,10,9/25/19,22,8,11,2,0,4,59;43;651;1843,6;7;113;187,4;4;13;22,6;5;26;79,m;m
3667,ICLR,2020,Towards Hierarchical Importance Attribution: Explaining Compositional Semantics for Neural Sequence Models,Xisen Jin;Zhongyu Wei;Junyi Du;Xiangyang Xue;Xiang Ren,xisenjin@usc.edu;zywei@fudan.edu.cn;junyidu@usc.edu;xyxue@fudan.edu.cn;xiangren@usc.edu,6;8;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,7,0,yes,9/25/19,University of Southern California;Fudan University;University of Southern California;Fudan University;University of Southern California,31;79;31;79;31,62;109;62;109;62,3,9/25/19,7,5,5,0,0,2,95;783;10;895;2436,7;80;7;114;165,3;14;2;16;24,22;83;2;51;309,m;m
3668,ICLR,2020,Understanding Why Neural Networks Generalize Well Through GSNR of Parameters,Jinlong Liu;Yunzhi Bai;Guoqing Jiang;Ting Chen;Huayan Wang,ljlwykqh@126.com;yunzhi.bai@outlook.fr;jianggq@pku.edu.cn;roushi0322@sina.cn;wanghuayan@kuaishou.com,6;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,4,0,yes,9/25/19,126;Kuaishou;Peking University;;Kuaishou,-1;-1;22;-1;-1,-1;-1;24;-1;-1,8,9/25/19,4,3,0,0,0,1,78;2;30;18;211,20;3;4;14;4,4;1;2;3;2,4;0;1;1;16,m;m
3669,ICLR,2020,Intrinsic Motivation for Encouraging Synergistic Behavior,Rohan Chitnis;Shubham Tulsiani;Saurabh Gupta;Abhinav Gupta,ronuchit@mit.edu;shubhtuls@fb.com;saurabhg@illinois.edu;abhinavg@cs.cmu.edu,6;8;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,"Massachusetts Institute of Technology;Facebook;University of Illinois, Urbana Champaign;Carnegie Mellon University",2;-1;3;1,5;-1;48;27,,9/25/19,0,0,0,0,0,0,344;1857;1;369,16;36;5;44,6;18;1;10,28;232;0;35,m;m
3670,ICLR,2020,Sharing Knowledge in Multi-Task Deep Reinforcement Learning,Carlo D'Eramo;Davide Tateo;Andrea Bonarini;Marcello Restelli;Jan Peters,carlo@robot-learning.de;davide@robot-learning.de;andrea.bonarini@polimi.it;marcello.restelli@polimi.it;peters@ias.tu-darmstadt.de,6;6;6,I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,TU Darmstadt;TU Darmstadt;Politecnico di Milano;Politecnico di Milano;TU Darmstadt,64;64;128;128;64,289;289;347;347;289,1,9/25/19,3,3,0,0,0,0,54;20;1985;1289;123,15;9;228;145;37,3;3;23;19;3,3;3;105;83;4,m;m
3671,ICLR,2020,Provable Filter Pruning for Efficient Neural Networks,Lucas Liebenwein;Cenk Baykal;Harry Lang;Dan Feldman;Daniela Rus,lucasl@mit.edu;baykal@mit.edu;hlang08@gmail.com;dannyf.post@gmail.com;rus@csail.mit.edu,3;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;;University of Haifa;Massachusetts Institute of Technology,2;2;-1;172;2,5;5;-1;544;5,,9/25/19,3,2,0,0,0,0,50;131;1132;1834;5023,10;22;90;112;166,4;6;18;20;29,5;8;113;128;217,m;f
3672,ICLR,2020,Additive Powers-of-Two Quantization: An Efficient Non-uniform Discretization for Neural Networks,Yuhang Li;Xin Dong;Wei Wang,loafyuhang@gmail.com;xindong@g.harvard.edu;wangwei@comp.nus.edu.sg,6;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,4,1,yes,9/25/19,University of Electronic Science and Technology of China;Harvard University;National University of Singapore,481;39;16,628;7;25,,9/25/19,3,2,1,0,0,1,8;81;106,10;48;137,2;4;5,3;7;10,u;m
3673,ICLR,2020,Variational Recurrent Models for Solving Partially Observable Control Tasks,Dongqi Han;Kenji Doya;Jun Tani,dongqi.han@oist.jp;doya@oist.jp;jun.tani@oist.jp,6;8;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0,yes,9/25/19,Okinawa Institute of Science and Technology Graduate University;Okinawa Institute of Science and Technology Graduate University;Okinawa Institute of Science and Technology Graduate University,-1;-1;-1,-1;-1;-1,,9/25/19,2,1,0,0,0,0,6;7939;3725,15;245;269,1;36;28,0;613;216,m;m
3674,ICLR,2020,SEED RL: Scalable and Efficient Deep-RL with Accelerated Central Inference,Lasse Espeholt;Raphaël Marinier;Piotr Stanczyk;Ke Wang;Marcin Michalski‎,lespeholt@google.com;raphaelm@google.com;stanczyk@google.com;kewa@google.com;michalski@google.com,8;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Accept (Talk),0,4,0,yes,9/25/19,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,,9/25/19,8,3,3,0,0,0,3291;60;39;31;20,9;5;75;13;7,7;2;4;3;2,484;11;2;2;2,m;m
3675,ICLR,2020,Discriminative Particle Filter Reinforcement Learning for Complex Partial observations,Xiao Ma;Peter Karkus;David Hsu;Wee Sun Lee;Nan Ye,xiao-ma@comp.nus.edu.sg;karkus@comp.nus.edu.sg;dyhsu@comp.nus.edu.sg;leews@comp.nus.edu.sg;nan.ye@uq.edu.au,8;6;8,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,5,0,yes,9/25/19,National University of Singapore;National University of Singapore;National University of Singapore;National University of Singapore;University of Queensland,16;16;16;16;205,25;25;25;25;66,5,9/25/19,1,1,0,0,0,0,118;157;1146;61;85,131;16;31;16;23,4;5;9;5;3,5;10;91;6;4,f;m
3676,ICLR,2020,White Noise Analysis of Neural Networks,Ali Borji;Sikun Lin,aliborji@gmail.com;sikun@ucsb.edu,3;8;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Spotlight),0,7,0,yes,9/25/19,University of Central Florida;UC Santa Barbara,77;38,609;57,4;2,9/25/19,3,0,2,1,0,0,145;49,21;5,6;3,13;1,m;f
3677,ICLR,2020,RaCT: Toward Amortized Ranking-Critical Training For Collaborative Filtering ,Sam Lobel*;Chunyuan Li*;Jianfeng Gao;Lawrence Carin,samuel_lobel@brown.edu;chunyuan.li@microsoft.com;jfgao@microsoft.com;lcarin@duke.edu,8;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,3,0,yes,9/25/19,Brown University;Microsoft;Microsoft;Duke University,67;-1;-1;47,53;-1;-1;20,,6/10/19,1,1,0,0,0,0,1;2089;18900;20051,2;81;353;822,1;25;61;66,0;242;2683;1999,m;m
3678,ICLR,2020,Skip Connections Matter: On the Transferability of Adversarial Examples Generated with ResNets,Dongxian Wu;Yisen Wang;Shu-Tao Xia;James Bailey;Xingjun Ma,wu-dx16@mails.tsinghua.edu.cn;eewangyisen@gmail.com;xiast@sz.tsinghua.edu.cn;baileyj@unimelb.edu.au;xingjun.ma@unimelb.edu.au,8;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Accept (Spotlight),0,4,1,yes,9/25/19,Tsinghua University;Tsinghua University;Tsinghua University;The University of Melbourne;The University of Melbourne,8;8;8;118;118,23;23;23;32;32,4,9/25/19,9,7,4,0,0,2,12;-1;73;5802;569,6;-1;51;335;32,1;-1;5;35;9,2;0;5;669;83,m;m
3679,ICLR,2020,The Local Elasticity of Neural Networks,Hangfeng He;Weijie Su,hangfeng@seas.upenn.edu;suw@wharton.upenn.edu,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0,yes,9/25/19,University of Pennsylvania;University of Pennsylvania,19;19,11;11,,9/25/19,3,3,0,0,0,0,183;401,22;31,8;10,14;70,m;m
3680,ICLR,2020,Deep probabilistic subsampling for task-adaptive compressed sensing,Iris A.M. Huijben;Bastiaan S. Veeling;Ruud J.G. van Sloun,i.a.m.huijben@tue.nl;basveeling@gmail.com;r.j.g.v.sloun@tue.nl,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,8,1,yes,9/25/19,Eindhoven University of Technology;Google;Eindhoven University of Technology,205;-1;205,185;-1;185,,9/25/19,1,1,0,0,0,0,4;99;117,5;14;33,1;5;6,0;19;5,f;m
3681,ICLR,2020,Simplified Action Decoder for Deep Multi-Agent Reinforcement Learning,Hengyuan Hu;Jakob N Foerster,hengyuan@fb.com;jakobfoerster@gmail.com,8;8;8,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Spotlight),0,9,0,yes,9/25/19,Facebook;Facebook,-1;-1,-1;-1,4,9/25/19,5,1,3,1,0,2,360;2107,9;58,5;19,34;340,m;m
3682,ICLR,2020,On the Need for Topology-Aware Generative Models for Manifold-Based Defenses,Uyeong Jang;Susmit Jha;Somesh Jha,wjang@cs.wisc.edu;susmit.jha@sri.com;jha@cs.wisc.edu,8;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,0,yes,9/25/19,University of Southern California;SRI International;University of Southern California,31;-1;31,62;-1;62,5;4,9/7/19,0,0,0,0,0,0,57;1380;18974,8;73;331,4;16;67,5;125;1824,m;m
3683,ICLR,2020,Functional Regularisation for  Continual Learning with Gaussian Processes,Michalis K. Titsias;Jonathan Schwarz;Alexander G. de G. Matthews;Razvan Pascanu;Yee Whye Teh,mtitsias@google.com;schwarzjn@google.com;alexmatthews@google.com;razp@google.com;ywteh@google.com,3;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,5,0,yes,9/25/19,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,11,1/31/19,19,4,1,0,0,0,2767;572;1016;17189;23746,61;27;24;101;247,20;8;12;46;53,395;98;129;1700;3235,m;m
3684,ICLR,2020,Graph inference learning for semi-supervised classification,Chunyan Xu;Zhen Cui;Xiaobin Hong;Tong Zhang;Jian Yang;Wei Liu,cyx@njust.edu.cn;zhen.cui@njust.edu.cn;xbhong@njust.edu.cn;tong.zhang@njust.edu.cn;csjyang@njust.edu.cn;wl2223@columbia.edu,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Accept (Poster),0,0,0,yes,9/25/19,Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Columbia University,39;39;39;39;39;15,47;47;47;47;47;16,10,9/25/19,2,2,1,0,0,1,1383;1743;23;164;744;100,76;145;18;78;129;160,14;21;3;5;6;4,186;122;2;14;183;2,f;m
3685,ICLR,2020,Input Complexity and Out-of-distribution Detection with Likelihood-based Generative Models,Joan Serrà;David Álvarez;Vicenç Gómez;Olga Slizovskaia;José F. Núñez;Jordi Luque,joansj@gmail.com;davidalvarezdlt@gmail.com;vicen.gomez@upf.edu;oslizovskaia@gmail.com;jfn237@nyu.edu;jordi.luqueserrano@telefonica.com,6;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,5,0,yes,9/25/19,Dolby Laboratories;Telefonica Research;Universitat Pompeu Fabra;Universitat Pompeu Fabra;New York University;Telefonica Research,-1;-1;481;481;25;-1,-1;-1;141;141;29;-1,5;11,9/25/19,9,5,2,1,0,3,13;8;1187;97;17;429,9;4;56;14;7;60,2;1;17;5;2;12,3;2;123;9;3;39,m;m
3686,ICLR,2020,AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty,Dan Hendrycks*;Norman Mu*;Ekin Dogus Cubuk;Barret Zoph;Justin Gilmer;Balaji Lakshminarayanan,hendrycks@berkeley.edu;normanmu@google.com;cubuk@google.com;barretzoph@google.com;gilmer@google.com;balajiln@google.com,8;8;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Accept (Poster),0,14,0,yes,9/25/19,University of California Berkeley;Google;Google;Google;Google;Google,5;-1;-1;-1;-1;-1,13;-1;-1;-1;-1;-1,,9/25/19,24,9,7,1,0,2,1540;41;1032;7122;3434;2985,27;6;29;30;45;43,14;2;13;21;19;23,292;8;133;1308;469;389,m;m
3687,ICLR,2020,Learn to Explain Efficiently via Neural Logic Inductive Learning,Yuan Yang;Le Song,yyang754@gatech.edu;lsong@cc.gatech.edu,6;8;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,14,0,yes,9/25/19,Georgia Institute of Technology;Georgia Institute of Technology,13;13,38;38,,9/25/19,0,0,0,0,0,0,617;9519,243;329,12;54,35;1114,m;m
3688,ICLR,2020,Doubly Robust Bias Reduction in Infinite Horizon Off-Policy Estimation,Ziyang Tang*;Yihao Feng*;Lihong Li;Dengyong Zhou;Qiang Liu,ztang@cs.utexas.edu;yihao@cs.utexas.edu;lihongli.cs@gmail.com;dennyzhou@google.com;lqiang@cs.utexas.edu,8;8;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Spotlight),0,10,0,yes,9/25/19,"University of Texas, Austin;University of Texas, Austin;Google;Google;University of Texas, Austin",22;22;-1;-1;22,38;38;-1;-1;38,,9/25/19,4,2,0,0,0,1,130;148;47;9077;60,22;12;9;78;70,5;7;3;34;3,29;15;12;1342;9,m;m
3689,ICLR,2020,Towards Stable and Efficient Training of Verifiably Robust Neural Networks,Huan Zhang;Hongge Chen;Chaowei Xiao;Sven Gowal;Robert Stanforth;Bo Li;Duane Boning;Cho-Jui Hsieh,huan@huan-zhang.com;chenhg@mit.edu;xiaocw@umich.edu;sgowal@google.com;stanforth@google.com;lbo@illinois.edu;boning@mtl.mit.edu;chohsieh@cs.ucla.edu,6;3;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),3,7,0,yes,9/25/19,"University of California, Los Angeles;Massachusetts Institute of Technology;University of Michigan;Google;Google;University of Illinois, Urbana Champaign;Massachusetts Institute of Technology;University of California, Los Angeles",20;2;8;-1;-1;3;2;20,17;5;21;-1;-1;48;5;17,4;1,6/14/19,30,18,9,4,0,8,2086;519;1386;-1;-1;1061;2898;12827,31;41;31;-1;-1;137;284;168,19;9;12;-1;-1;11;27;41,289;66;149;0;0;63;200;1746,m;m
3690,ICLR,2020,Adversarial Policies: Attacking Deep Reinforcement Learning,Adam Gleave;Michael Dennis;Cody Wild;Neel Kant;Sergey Levine;Stuart Russell,gleave@berkeley.edu;michael_dennis@berkeley.edu;codywild@berkeley.edu;kantneel@berkeley.edu;svlevine@eecs.berkeley.edu;russell@cs.berkeley.edu,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,8,0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5;5;5,13;13;13;13;13;13,4,5/25/19,22,12,3,0,0,1,147;22;38;70;24893;42,8;2;10;14;310;30,5;1;3;4;74;4,17;1;1;4;3235;5,m;m
3691,ICLR,2020,DDSP: Differentiable Digital Signal Processing,Jesse Engel;Lamtharn (Hanoi) Hantrakul;Chenjie Gu;Adam Roberts,jesseengel@google.com;hanoih@google.com;gcj@google.com;adarob@google.com,6;8;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Accept (Spotlight),0,9,2,yes,9/25/19,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,5;4,9/25/19,8,3,5,0,0,1,2230;36;501;10658,36;7;45;36,12;4;12;19,251;5;58;1460,m;m
3692,ICLR,2020,Enabling Deep Spiking Neural Networks with Hybrid Conversion and Spike Timing Dependent Backpropagation,Nitin Rathi;Gopalakrishnan Srinivasan;Priyadarshini Panda;Kaushik Roy,rathi2@purdue.edu;srinivg@purdue.edu;priya.panda@yale.edu;kaushik@purdue.edu,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,Purdue University;Purdue University;Yale University;Purdue University,27;27;64;27,88;88;8;88,,9/25/19,4,2,2,0,0,2,31;27;654;221,6;21;66;70,3;3;14;9,2;2;43;21,m;m
3693,ICLR,2020,Towards Verified Robustness under Text Deletion Interventions,Johannes Welbl;Po-Sen Huang;Robert Stanforth;Sven Gowal;Krishnamurthy (Dj) Dvijotham;Martin Szummer;Pushmeet Kohli,johannes.welbl.14@ucl.ac.uk;posenhuang@google.com;stanforth@google.com;sgowal@google.com;dvij@google.com;szummer@google.com;pushmeet@google.com,8;6;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:N/A:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,4,0,yes,9/25/19,University College London;Google;Google;Google;Google;Google;Google,50;-1;-1;-1;-1;-1;-1,15;-1;-1;-1;-1;-1;-1,3;1,9/25/19,2,2,1,1,0,0,1018;1772;484;568;1143;4039;22578,19;59;18;34;76;45;313,9;17;9;11;17;22;69,271;250;57;62;101;397;2782,m;m
3694,ICLR,2020,N-BEATS: Neural basis expansion analysis for interpretable time series forecasting,Boris N. Oreshkin;Dmitri Carpov;Nicolas Chapados;Yoshua Bengio,boris@elementai.com;dmitri.carpov@elementai.com;chapados@elementai.com;yoshua.bengio@mila.quebec,6;8;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,8,0,yes,9/25/19,Element AI;Element AI;Element AI;University of Montreal,-1;-1;-1;128,-1;-1;-1;85,,5/24/19,12,6,4,0,0,2,768;11;495;208566,46;2;44;807,12;1;12;147,89;1;33;24297,m;m
3695,ICLR,2020,Improving Adversarial Robustness Requires Revisiting Misclassified Examples,Yisen Wang;Difan Zou;Jinfeng Yi;James Bailey;Xingjun Ma;Quanquan Gu,eewangyisen@gmail.com;knowzou@ucla.edu;jinfengyi.ustc@gmail.com;baileyj@unimelb.edu.au;xingjun.ma@unimelb.edu.au;qgu@cs.ucla.edu,6;6;8,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,5,1,yes,9/25/19,"Tsinghua University;University of California, Los Angeles;JD AI Research;The University of Melbourne;The University of Melbourne;University of California, Los Angeles",8;20;-1;118;118;20,23;17;-1;32;32;17,4,9/25/19,15,11,1,0,0,3,-1;426;2040;5640;569;3895,-1;31;79;330;32;174,-1;11;24;35;9;34,0;24;244;666;83;411,m;m
3696,ICLR,2020,On Generalization Error Bounds of Noisy Gradient Methods for Non-Convex Learning,Jian Li;Xuanyuan Luo;Mingda Qiao,ljiian83@mail.tsinghua.edu.cn;luo-xy19@mails.tsinghua.edu.cn;mqiao@stanford.edu,6;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Accept (Poster),1,9,1,yes,9/25/19,Tsinghua University;Tsinghua University;Stanford University,8;8;4,23;23;4,11;8,2/2/19,8,4,1,2,0,2,22778;8;119,1069;1;13,72;1;7,2081;2;23,f;m
3697,ICLR,2020,SPACE: Unsupervised Object-Oriented Scene Representation via Spatial Attention and Decomposition,Zhixuan Lin;Yi-Fu Wu;Skand Vishwanath Peri;Weihao Sun;Gautam Singh;Fei Deng;Jindong Jiang;Sungjin Ahn,zxlin@zju.edu.cn;yifu.wu@gmail.com;pvskand@protonmail.com;ws383@scarletmail.rutgers.edu;singh.gautam.iitg@gmail.com;fei.deng@rutgers.edu;jindong.jiang@rutgers.edu;sjn.ahn@gmail.com,6;6;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,8,0,yes,9/25/19,Zhejiang University;Rutgers University;Rutgers University;Rutgers University;Rutgers University;Rutgers University;Rutgers University;Rutgers University,56;34;34;34;34;34;34;34,107;168;168;168;168;168;168;168,5,9/25/19,8,3,4,1,0,0,14;8;26;7;645;9;43;1389,9;2;7;4;50;7;9;41,2;1;4;1;12;2;4;12,0;0;6;0;40;0;7;161,m;f
3698,ICLR,2020,Learning from Unlabelled Videos Using Contrastive Predictive Neural 3D Mapping,Adam W. Harley;Shrinidhi K. Lakshmikanth;Fangyu Li;Xian Zhou;Hsiao-Yu Fish Tung;Katerina Fragkiadaki,aharley@cmu.edu;kowshika@cmu.edu;fangyul@cmu.edu;zhouxian@cmu.edu;htung@cs.cmu.edu;katef@cs.cmu.edu,6;6;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,8,0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1;1;1,27;27;27;27;27;27,2,6/10/19,0,0,0,0,0,0,509;31;540;44;449;1944,20;7;101;13;25;55,8;3;13;3;8;15,56;1;19;4;45;204,m;f
3699,ICLR,2020,CLN2INV: Learning Loop Invariants with Continuous Logic Networks,Gabriel Ryan;Justin Wong;Jianan Yao;Ronghui Gu;Suman Jana,gabe@cs.columbia.edu;justin.wong@columbia.edu;jy3022@columbia.edu;ronghui.gu@columbia.edu;suman@cs.columbia.edu,3;8,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,6,1,yes,9/25/19,Columbia University;Columbia University;Columbia University;Columbia University;Columbia University,15;15;15;15;15,16;16;16;16;16,,9/25/19,2,1,2,0,0,0,15;1008;173;418;3718,11;14;8;20;79,2;3;4;9;29,1;23;9;37;481,m;m
3700,ICLR,2020,"Pay Attention to Features, Transfer Learn Faster CNNs",Kafeng Wang;Xitong Gao;Yiren Zhao;Xingjian Li;Dejing Dou;Cheng-Zhong Xu,kf.wang@siat.ac.cn;xt.gao@siat.ac.cn;yiren.zhao@cl.cam.ac.uk;lixingjian@baidu.com;doudejing@baidu.com;czxu@um.edu.mo,6;6;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Accept (Poster),0,3,0,yes,9/25/19,Chinese Academy of Sciences;Chinese Academy of Sciences;University of Cambridge;Baidu;Baidu;,59;59;71;-1;-1;-1,1397;1397;3;-1;-1;-1,6,9/25/19,3,1,1,0,0,0,2;115;136;96;2318;5729,2;14;17;15;153;344,1;6;7;5;24;42,0;18;13;13;182;366,m;m
3701,ICLR,2020,Sign-OPT: A Query-Efficient Hard-label Adversarial Attack,Minhao Cheng;Simranjit Singh;Patrick H. Chen;Pin-Yu Chen;Sijia Liu;Cho-Jui Hsieh,mhcheng@ucla.edu;simranjit@cs.ucla.edu;patrickchen@ucla.edu;pin-yu.chen@ibm.com;sijia.liu@ibm.com;chohsieh@cs.ucla.edu,6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Accept (Poster),0,2,0,yes,9/25/19,"University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles;International Business Machines;International Business Machines;University of California, Los Angeles",20;20;20;-1;-1;20,17;17;17;-1;-1;17,4,9/24/19,14,7,5,1,0,3,305;567;146;194;298;12827,20;88;31;44;52;168,6;14;6;6;11;41,42;17;14;22;24;1746,m;m
3702,ICLR,2020,GraphQA: Protein Model Quality Assessment using Graph Convolutional Network,Federico Baldassarre;David Menéndez Hurtado;Arne Elofsson;Hossein Azizpour,baldassarre.fe@gmail.com;david.menendez.hurtado@scilifelab.se;arne@bioinfo.se;azizpour@kth.se,3;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,6,0,yes,9/25/19,"KTH Royal Institute of Technology, Stockholm, Sweden;Science for Life Laboratory;;KTH Royal Institute of Technology, Stockholm, Sweden",128;-1;-1;128,222;-1;-1;222,10,9/25/19,1,0,0,0,0,0,30;5;9906;4287,6;4;207;30,2;1;53;11,2;1;827;298,m;m
3703,ICLR,2020,Compositional Embeddings: Joint Perception and Comparison of Class Label Sets,Zeqian Li;Jacob Whitehill,zli14@wpi.edu;jrwhitehill@wpi.edu,6;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Worcester Polytechnic Institute;Worcester Polytechnic Institute,172;172,628;628,2,9/25/19,0,0,0,0,0,0,17;2365,8;55,3;16,0;312,m;m
3704,ICLR,2020,Generating Multi-Sentence Abstractive Summaries of Interleaved Texts,Sanjeev Kumar Karn;Francine Chen;Yan-Ying Chen;Ulli Waltinger;Hinrich Schütze,skarn@cis.lmu.de;chen@fxpal.com;yanying@fxpal.com;ulli.waltinger@siemens.com;hinrich@hotmail.com,3;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,"Institut für Informatik;FX Palo Alto Laboratory, Inc.;FX Palo Alto Laboratory, Inc.;Siemens Corporate Research;Institut für Informatik",-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,,9/25/19,0,0,0,0,0,0,30;587;732;305;19297,7;39;45;47;317,3;10;11;10;46,1;46;46;24;2286,m;m
3705,ICLR,2020,Generalizing Reinforcement Learning to Unseen Actions,Ayush Jain*;Andrew Szot*;Jincheng Zhou;Joseph J. Lim,ayushj@usc.edu;szot@usc.edu;jinchenz@usc.edu;limjj@usc.edu,6;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,University of Southern California;University of Southern California;University of Southern California;University of Southern California,31;31;31;31,62;62;62;62,6;8,9/25/19,0,0,0,0,0,0,818;0;68;2976,29;1;21;51,4;0;4;21,19;0;3;270,m;m
3706,ICLR,2020,Mixture-of-Experts Variational Autoencoder for clustering and generating from similarity-based representations,Andreas Kopf;Vincent Fortuin;Vignesh Ram Somnath;Manfred Claassen,akopf@ethz.ch;fortuin@inf.ethz.ch;vsomnath@student.ethz.ch;mclaassen@ethz.ch,3;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10;10,13;13;13;13,5,9/25/19,0,0,0,0,0,0,996;79;0;568,144;18;2;33,16;6;0;12,60;5;0;17,m;m
3707,ICLR,2020,On Symmetry and Initialization for Neural Networks,Ido Nachum;Amir Yehudayoff,ido0808@gmail.com;amir.yehudayoff@gmail.com,3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,6,0,yes,9/25/19,Swiss Federal Institute of Technology Lausanne;,481;-1,38;-1,1;8,7/1/19,0,0,0,0,0,0,37;1456,9;107,3;21,3;115,m;m
3708,ICLR,2020,Training Interpretable Convolutional Neural Networks towards Class-specific Filters,Haoyu Liang;Zhihao Ouyang;Hang Su;Yuyuan Zeng;Zihao He;Shu-tao Xia;Jun Zhu;Bo Zhang,lianghy18@mails.tsinghua.edu.cn;oyzh18@mails.tsinghua.edu.cn;suhangss@mail.tsinghua.edu.cn;zengyy19@mails.tsinghua.edu.cn;zihaoh@usc.edu;xiast@sz.tsinghua.edu.cn;dcszj@mail.tsinghua.edu.cn;dcszb@mail.tsinghua.edu.cn,6;6;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0,yes,9/25/19,Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University;University of Southern California;Tsinghua University;Tsinghua University;Tsinghua University,8;8;8;8;31;8;8;8,23;23;23;23;62;23;23;23,,9/25/19,0,0,0,0,0,0,51;0;1613;5;1;73;705;10,10;2;69;4;9;51;96;33,5;0;13;1;1;5;15;2,5;0;226;1;0;5;15;0,m;m
3709,ICLR,2020,InfoCNF: Efficient Conditional Continuous Normalizing Flow Using Adaptive Solvers,Tan M. Nguyen;Animesh Garg;Richard G. Baraniuk;Anima Anandkumar,mn15@rice.edu;garg@cs.toronto.edu;richb@rice.edu;anima@caltech.edu,3;3;6;1,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,10,0,yes,9/25/19,"Rice University;Department of Computer Science, University of Toronto;Rice University;California Institute of Technology",84;18;84;143,105;18;105;2,5,9/25/19,4,1,1,0,0,0,11;1040;30222;5451,6;79;661;187,2;20;84;38,0;66;2757;761,m;f
3710,ICLR,2020,All SMILES Variational Autoencoder for Molecular Property Prediction and Optimization,Zaccary Alperstein;Artem Cherkasov;Jason Rolfe,zalperst@gmail.com;artc@interchange.ubc.ca;rolfe22@gmail.com,3;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,2,7,0,yes,9/25/19,;University of British Columbia;D-Wave Systems,-1;35;-1,-1;34;-1,5;10,9/25/19,0,0,0,0,0,0,15;3496;242,6;182;7,2;31;3,2;161;17,m;m
3711,ICLR,2020,On Incorporating Semantic Prior Knowlegde in Deep Learning Through Embedding-Space Constraints,Damien Teney;Ehsan Abbasnejad;Anton van den Hengel,damien.teney@adelaide.edu.au;ehsan.abbasnejad@adelaide.edu.au;anton.vandenhengel@adelaide.edu.au,6;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,The University of Adelaide;The University of Adelaide;The University of Adelaide,128;128;128,120;120;120,,9/25/19,3,2,2,0,0,0,1927;379;10899,34;49;281,12;10;52,384;33;930,m;m
3712,ICLR,2020,Learning Latent Representations for Inverse Dynamics using Generalized Experiences,Aditi Mavalankar;Sicun Gao,amavalan@eng.ucsd.edu;sicung@ucsd.edu,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,"University of California, San Diego;University of California, San Diego",11;11,31;31,,9/25/19,0,0,0,0,0,0,1;1090,8;66,1;12,0;90,f;m
3713,ICLR,2020,Understanding Attention Mechanisms,Bingyuan Liu;Yogesh Balaji;Lingzhou Xue;Martin Renqiang Min,bul37@psu.edu;yogesh@cs.umd.edu;lzxue@psu.edu;renqiang@nec-labs.com,6;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,5,0,yes,9/25/19,"Pennsylvania State University;University of Maryland, College Park;Pennsylvania State University;NEC-Labs",41;12;41;-1,78;91;78;-1,,9/25/19,0,0,0,0,0,0,0;497;848;897,3;19;54;58,0;7;13;15,0;61;82;105,m;m
3714,ICLR,2020,Differentiable Architecture Compression,Shashank Singh;Ashish Khetan;Zohar Karnin,sss1@andrew.cmu.edu;khetan2@illinois.edu;zkarnin@gmail.com,3;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,"Carnegie Mellon University;University of Illinois, Urbana Champaign;Amazon",1;3;-1,27;48;-1,,5/20/19,6,5,2,0,0,1,41;314;878,34;25;57,4;8;19,2;45;146,m;m
3715,ICLR,2020,On Stochastic Sign Descent Methods,Mher Safaryan;Peter Richtárik,mher.safaryan@gmail.com;peter.richtarik@kaust.edu.sa,6;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,KAUST;KAUST,128;128,1397;1397,9,5/30/19,2,2,0,0,0,0,10;5790,9;159,2;37,0;597,m;m
3716,ICLR,2020,The Generalization-Stability Tradeoff in Neural Network Pruning,Brian R. Bartoldson;Ari S. Morcos;Adrian Barbu;Gordon Erlebacher,bbartoldson@fsu.edu;arimorcos@gmail.com;abarbu@stat.fsu.edu;gerlebacher@fsu.edu,3;1;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,7,0,yes,9/25/19,SUN YAT-SEN UNIVERSITY;Facebook;SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY,481;-1;481;481,299;-1;299;299,8,6/9/19,1,1,0,0,0,0,4;1034;1743;1380,3;32;95;134,1;12;19;19,0;116;157;64,m;m
3717,ICLR,2020,Evaluations and Methods for Explanation through Robustness Analysis,Cheng-Yu Hsieh;Chih-Kuan Yeh;Xuanqing Liu;Pradeep Ravikumar;Seungyeon Kim;Sanjiv Kumar;Cho-Jui Hsieh,r05922048@ntu.edu.tw;cjyeh@cs.cmu.edu;xqliu@cs.ucla.edu;pradeepr@cs.cmu.edu;seungyeonk@google.com;sanjivk@google.com;chohsieh@cs.ucla.edu,6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,1,6,0,yes,9/25/19,"National Taiwan University;Carnegie Mellon University;University of California, Los Angeles;Carnegie Mellon University;Google;Google;University of California, Los Angeles",86;1;20;1;-1;-1;20,120;27;17;27;-1;-1;17,4,9/25/19,2,1,0,0,0,0,216;284;256;8670;405;75;12827,48;21;20;181;43;14;168,9;7;6;38;8;4;41,9;45;37;1214;64;1;1746,m;m
3718,ICLR,2020,Unsupervised Meta-Learning for Reinforcement Learning,Abhishek Gupta;Benjamin Eysenbach;Chelsea Finn;Sergey Levine,abhigupta@berkeley.edu;beysenba@cs.cmu.edu;cbfinn@eecs.berkeley.edu;svlevine@eecs.berkeley.edu,3;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,University of California Berkeley;Carnegie Mellon University;University of California Berkeley;University of California Berkeley,5;1;5;5,13;27;13;13,6,6/12/18,35,26,10,1,0,4,1421;357;7879;24893,89;16;101;310,18;6;34;74,146;54;1060;3235,m;m
3719,ICLR,2020,A General Upper Bound for Unsupervised Domain Adaptation,Dexuan Zhang;Tatsuya Harada,dexuan.zhang@mi.t.u-tokyo.ac.jp;harada@mi.t.u-tokyo.ac.jp,1;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,10,0,yes,9/25/19,The University of Tokyo;The University of Tokyo,56;56,36;36,4;1,9/25/19,0,0,0,0,0,0,387;2594,25;213,9;26,32;327,m;m
3720,ICLR,2020,FoveaBox: Beyound Anchor-based Object Detection,Tao Kong;Fuchun Sun;Huaping Liu;Yuning Jiang;Lei Li;Jianbo Shi,taokongcn@gmail.com;fcsun@tsinghua.edu.cn;hpliu@tsinghua.edu.cn;jiangyuning@bytedance.com;lileilab@bytedance.com;jshi@seas.upenn.edu,6;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:N/A:N/A,Reject,1,4,0,yes,9/25/19,Bytedance;Tsinghua University;Tsinghua University;Bytedance;Bytedance;University of Pennsylvania,-1;8;8;-1;-1;19,-1;23;23;-1;-1;11,2;8,4/8/19,50,30,14,3,3,8,2009;7065;4947;1599;157;25194,128;586;478;53;71;164,22;41;35;16;5;46,126;323;306;192;13;2550,m;m
3721,ICLR,2020,CloudLSTM: A Recurrent Neural Model for Spatiotemporal Point-cloud Stream Forecasting,Chaoyun Zhang;Marco Fiore;Iain Murray;Paul Patras,chaoyun.zhang@ed.ac.uk;marco.fiore@ieiit.cnr.it;i.murray@ed.ac.uk;paul.patras@ed.ac.uk,8;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,7,0,yes,9/25/19,University of Edinburgh;;University of Edinburgh;University of Edinburgh,33;-1;33;33,30;-1;30;30,,7/29/19,1,0,1,0,0,0,468;3916;5241;786,14;230;223;51,6;31;29;15,37;284;586;43,m;m
3722,ICLR,2020,Step Size Optimization,Gyoung S. Na;Dongmin Hyeon;Hwanjo Yu,ngs0726@gmail.com;dmhyeon@postech.ac.kr;hwanjoyu@postech.ac.kr,3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,1,4,0,yes,9/25/19,POSTECH;POSTECH;POSTECH,118;118;118,146;146;146,,9/25/19,0,0,0,0,0,0,16;2;3477,5;2;149,2;1;29,1;0;309,u;m
3723,ICLR,2020,Neural Arithmetic Unit by reusing many small pre-trained networks,Ammar Ahmad;Oneeb Babar;Murtaza Taj,ammarahmad977@gmail.com;oneebalibabar@gmail.com;murtaza.taj@lums.edu.pk,1;1;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,0,0,yes,9/25/19,;Lahore University of Management Sciences;Lahore University of Management Sciences,-1;481;481,-1;932;932,,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1,-1;-1;-1,-1;-1;-1,0;0;0,u;m
3724,ICLR,2020,OvA-INN: Continual Learning with Invertible Neural Networks,HOCQUET Guillaume;BICHLER Olivier;QUERLIOZ Damien,guillaume.hocquet@live.fr;olivier.bichler@cea.fr;damien.querlioz@c2n.upsaclay.fr,6;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,5,0,yes,9/25/19,CEA;CEA;,233;233;-1,1027;1027;-1,,9/25/19,0,0,0,0,0,0,2;2057;0,2;69;3,1;22;0,0;122;0,u;u
3725,ICLR,2020,Amortized Nesterov's Momentum: Robust and Lightweight  Momentum for Deep Learning,Kaiwen Zhou;Yanghua Jin;Qinghua Ding;James Cheng,kwzhou@cse.cuhk.edu.hk;jinyh@preferred.jp;qhding@cse.cuhk.edu.hk;jcheng@cse.cuhk.edu.hk,3;8;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,9,0,yes,9/25/19,"The Chinese University of Hong Kong;Preferred Networks, Inc.;The Chinese University of Hong Kong;The Chinese University of Hong Kong",59;-1;59;59,35;-1;35;35,1;9;8,9/25/19,0,0,0,0,0,0,256;73;3258;4,49;8;72;7,10;3;28;1,13;4;318;1,u;m
3726,ICLR,2020,Improving Sample Efficiency in Model-Free Reinforcement Learning from Images,Denis Yarats;Amy Zhang;Ilya Kostrikov;Brandon Amos;Joelle Pineau;Rob Fergus,denisyarats@cs.nyu.edu;amyzhang@fb.com;ik1078@nyu.edu;brandon.amos.cs@gmail.com;jpineau@fb.com;robfergus@fb.com,6;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,2,0,yes,9/25/19,New York University;Facebook;New York University;Facebook;Facebook;Facebook,25;-1;25;-1;-1;-1,29;-1;29;-1;-1;-1,,9/25/19,10,4,3,1,0,0,1668;788;617;1981;11328;53064,12;47;15;40;267;128,7;13;9;19;46;61,251;94;61;221;1235;6486,m;m
3727,ICLR,2020,Curriculum Learning for Deep Generative Models with Clustering,Deli Zhao;Jiapeng Zhu;Zhenfang Guo;Bo Zhang,zhaodeli@gmail.com;jengzhu0@gmail.com;guozhenfang@pku.edu.cn;zhangbo@xiaomi.com,6;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,;;Peking University;Xiaomi,-1;-1;22;-1,-1;-1;24;-1,5;4,6/27/19,0,0,0,0,0,0,44;497;2;318,17;15;3;103,4;4;1;6,4;55;0;46,m;m
3728,ICLR,2020,Autoencoder-based Initialization for Recurrent Neural Networks with a Linear Memory,Antonio Carta;Alessandro Sperduti;Davide Bacciu,antonio.carta@di.unipi.it;sperduti@math.unipd.it;bacciu@di.unipi.it,3;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0,yes,9/25/19,University of Pisa;Universita' degli studi di Padova;University of Pisa,233;-1;233,366;-1;366,,9/25/19,0,0,0,0,0,0,40;4258;651,30;263;113,4;32;13,3;276;26,m;m
3729,ICLR,2020,Universal Approximation with Deep Narrow Networks,Patrick Kidger;Terry Lyons,kidger@maths.ox.ac.uk;tlyons@maths.ox.ac.uk,6;8;3,I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,University of Oxford;University of Oxford,50;50,1;1,1,5/21/19,5,2,1,0,2,1,37;2691,6;120,4;26,1;318,m;m
3730,ICLR,2020,Probing Emergent Semantics in Predictive Agents via Question Answering,Abhishek Das;Federico Carnevale;Hamza Merzic;Laura Rimell;Rosalia Schneider;Alden Hung;Josh Abramson;Arun Ahuja;Stephen Clark;Greg Wayne;Felix Hill,abhshkdz@gatech.edu;fedecarnev@google.com;hamzamerzic@google.com;laurarimell@google.com;rgschneider@google.com;aldenhung@google.com;jabramson@google.com;arahuja@google.com;clarkstephen@google.com;gregwayne@google.com;felixhill@google.com,3;8;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,9,0,yes,9/25/19,Georgia Institute of Technology;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,13;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,38;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,,9/25/19,1,0,0,0,0,0,3181;151;29;581;180;0;123;759;1595;777;3564,22;10;6;43;9;1;5;37;62;8;52,13;4;3;11;4;0;4;11;11;2;22,431;8;3;68;36;0;7;49;121;100;675,m;m
3731,ICLR,2020,Scheduled Intrinsic Drive: A Hierarchical Take on Intrinsically Motivated Exploration,Jingwei Zhang;Niklas Wetzel;Nicolai Dorka;Joschka Boedecker;Wolfram Burgard,zhang@cs.uni-freiburg.de;wetzel@cs.uni-freiburg.de;dorka@informatik.uni-freiburg.de;jboedeck@cs.uni-freiburg.de;burgard@informatik.uni-freiburg.de,3;8;6;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,8,0,yes,9/25/19,Universität Freiburg;Universität Freiburg;Universität Freiburg;Universität Freiburg;Universität Freiburg,118;118;118;118;118,85;85;85;85;85,,3/18/19,5,2,0,1,0,0,287;4;13;1167;49783,62;2;4;50;781,9;1;2;17;98,14;0;0;86;3944,m;m
3732,ICLR,2020,Stein Bridging: Enabling Mutual Reinforcement between Explicit and Implicit Generative Models,Qitian Wu;Rui Gao;Hongyuan Zha,echo740@sjtu.edu.cn;rui.gao@mccombs.utexas.edu;zha@cc.gatech.edu,3;1;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0,yes,9/25/19,"Shanghai Jiao Tong University;University of Texas, Austin;Georgia Institute of Technology",53;22;13,157;38;38,5;4,9/25/19,0,0,0,0,0,0,34;22;14480,10;26;408,3;3;62,6;0;1225,m;m
3733,ICLR,2020,New Loss Functions for Fast Maximum Inner Product Search,Ruiqi Guo;Quan Geng;David Simcha;Felix Chern;Phil Sun;Sanjiv Kumar,guorq@google.com;qgeng@google.com;dsimcha@google.com;fchern@google.com;sunphil@google.com;sanjivk@google.com,3;3;6,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,,8/27/19,0,0,0,0,0,0,1597;330;1217;1;-1;941,43;20;12;4;-1;142,12;8;6;1;-1;13,238;32;41;0;-1;113,m;m
3734,ICLR,2020,End-to-end named entity recognition and relation extraction using pre-trained language models,John Giorgi;Xindi Wang;Nicola Sahar;Won Young Shin;Gary Bader;Bo Wang,john.giorgi@utoronto.ca;xindi.wang@uhnresearch.ca;nicola.sahar@mail.utoronto.ca;wonyoung.shin@mail.utoronto.ca;gary.bader@utoronto.ca;bowang@vectorinstitute.ai,6;3;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:N/A:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,4,1,yes,9/25/19,Toronto University;University Health Network;Toronto University;Toronto University;Toronto University;Vector Institute,18;481;18;18;18;-1,18;1397;18;18;18;-1,3,9/25/19,1,0,0,0,0,0,103;102;0;0;34923;447,4;10;3;1;339;74,3;1;0;0;74;9,5;8;0;0;2804;19,u;m
3735,ICLR,2020,AdaGAN: Adaptive GAN for Many-to-Many Non-Parallel Voice Conversion,Maitreya Patel;Mirali Purohit;Mihir Parmar;Nirmesh J. Shah;Hemant A. Patil,maitreya_patel@daiict.ac.in;purohit_mirali@daiict.ac.in;mihirparmar@asu.edu;nirmesh88_shah@daiict.ac.in;hemant_patil@daiict.ac.in,1;1;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,1,3,0,yes,9/25/19,"Dhirubhai Ambani Institute of Information and Communication Technology, Gandhinagar;Dhirubhai Ambani Institute of Information and Communication Technology, Gandhinagar;Arizona State University;Dhirubhai Ambani Institute of Information and Communication Technology, Gandhinagar;Dhirubhai Ambani Institute of Information and Communication Technology, Gandhinagar",-1;-1;95;-1;-1,-1;-1;155;-1;-1,5;4;6,9/25/19,0,0,0,0,0,0,1;0;1;189;14,7;1;3;38;17,1;0;1;8;2,0;0;0;6;1,m;m
3736,ICLR,2020,Goal-Conditioned Video Prediction,Oleh Rybkin;Karl Pertsch;Frederik Ebert;Dinesh Jayaraman;Chelsea Finn;Sergey Levine,oleh@seas.upenn.edu;pertsch@usc.edu;febert@berkeley.edu;dineshjayaraman@berkeley.edu;cbfinn@cs.stanford.edu;svlevine@eecs.berkeley.edu,6;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:N/A:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,4,0,yes,9/25/19,University of Pennsylvania;University of Southern California;University of California Berkeley;University of California Berkeley;Stanford University;University of California Berkeley,19;31;5;5;4;5,11;62;13;13;4;13,5,9/25/19,0,0,0,0,0,0,21;45;457;1206;7879;24893,12;10;16;42;101;310,3;4;8;16;34;74,1;8;70;128;1060;3235,m;m
3737,ICLR,2020,wMAN: WEAKLY-SUPERVISED MOMENT ALIGNMENT NETWORK FOR TEXT-BASED VIDEO SEGMENT RETRIEVAL,Reuben Tan;Huijuan Xu;Kate Saenko;Bryan A. Plummer,rxtan@bu.edu;huijuan@berkeley.edu;saenko@bu.edu;bplumme2@illinois.edu,6;6;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,"Boston University;University of California Berkeley;Boston University;University of Illinois, Urbana Champaign",67;5;67;3,61;13;61;48,10,9/25/19,3,3,1,0,0,1,14;1071;17431;802,7;46;178;27,3;10;56;9,3;82;2403;144,m;m
3738,ICLR,2020,Data augmentation instead of explicit regularization,Alex Hernandez-Garcia;Peter König,alexhg15@gmail.com;pkoenig@uos.de,3;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,9,0,yes,9/25/19,University of Osnabrück;University of Osnabrück,323;323,1397;1397,8,2/15/18,32,8,13,2,6,0,87;780,15;135,5;18,1;38,m;m
3739,ICLR,2020,Implicit λ-Jeffreys Autoencoders: Taking the Best of Both Worlds,Aibek Alanov;Max Kochurov;Artem Sobolev;Daniil Yashkov;Dmitry Vetrov,alanov.aibek@gmail.com;maxim.v.kochurov@gmail.com;asobolev@bayesgroup.ru;daniil.yashkov@phystech.edu;vetrovd@yandex.ru,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Higher School of Economics;Skolkovo Institute of Science and Technology;Samsung;Moscow Institute of Physics and Technology;Higher School of Economics,481;-1;-1;481;481,251;-1;-1;234;251,5;4,9/25/19,0,0,0,0,0,0,3;11;56;22;2097,4;8;65;6;124,1;2;4;2;16,0;2;1;3;285,m;m
3740,ICLR,2020,LOSSLESS SINGLE IMAGE SUPER RESOLUTION FROM LOW-QUALITY JPG IMAGES,Yong Shi;Biao Li;Bo Wang;Zhiquan Qi;Jiabin Liu;Fan Meng,yshi@unomaha.edu;libiao17@mails.ucas.ac.cn;wangbo@uibe.edu.cn;qizhiquan@foxmail.com;liujiabin008@126.com;mengfan@cufe.edu.cn,3;6;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,1,0,yes,9/25/19,"University of Nebraska, Omaha;Chinese Academy of Sciences;Tsinghua University;University of Chinese Academy of Sciences;126;Tsinghua University",390;59;8;59;-1;8,1397;1397;23;1397;-1;23,2,9/25/19,0,0,0,0,0,0,8;123;62;6;0;0,12;66;45;4;2;4,1;7;4;1;0;0,0;12;1;0;0;0,m;m
3741,ICLR,2020,Improving Batch Normalization with Skewness Reduction for Deep Neural Networks,Pak Lun Kevin Ding;Sarah Martin;Baoxin Li,kevinding@asu.edu;samart44@asu.edu;baoxin.li@asu.edu,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,Arizona State University;Arizona State University;Arizona State University,95;95;95,155;155;155,,9/25/19,0,0,0,0,0,0,104;44;3431,14;33;225,5;3;28,1;3;304,m;m
3742,ICLR,2020,Cascade Style Transfer,Zhizhong Wang;Lei Zhao;Qihang Mo;Sihuan Lin;Zhiwen Zuo;Wei Xing;Dongming Lu,endywon@zju.edu.cn;cszhl@zju.edu.cn;moqihang@zju.edu.cn;linsh@zju.edu.cn;zzwcs@zju.edu.cn;wxing@zju.edu.cn;ldm@zju.edu.cn,1;1;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0,yes,9/25/19,Zhejiang University;Zhejiang University;Zhejiang University;Zhejiang University;Zhejiang University;Zhejiang University;Zhejiang University,56;56;56;56;56;56;56,107;107;107;107;107;107;107,,9/25/19,0,0,0,0,0,0,69;68;0;0;0;25;12,37;22;5;4;2;44;20,4;4;0;0;0;3;2,3;5;0;0;0;0;0,u;u
3743,ICLR,2020,Recurrent Hierarchical Topic-Guided Neural Language Models,Dandan Guo;Bo Chen;Ruiying Lu;Mingyuan Zhou,gdd_xidian@126.com;bchen@mail.xidian.edu.cn;ruiyinglu_xidian@163.com;mingyuan.zhou@mccombs.utexas.edu,1;1;8;8;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,8,4,yes,9/25/19,"Xidian University;Tsinghua University;Xidian University;University of Texas, Austin",481;8;481;22,919;23;919;38,3,9/25/19,0,0,0,0,0,0,1;21;0;2033,7;101;3;115,1;2;0;24,0;0;0;235,u;m
3744,ICLR,2020,The Frechet Distance of training and test distribution predicts the generalization gap,Julian Zilly;Hannes Zilly;Oliver Richter;Roger Wattenhofer;Andrea Censi;Emilio Frazzoli,jzilly@ethz.ch;hzilly@ethz.ch;richtero@ethz.ch;wattenhofer@ethz.ch;acensi@ethz.ch;emilio.frazzoli@idsc.mavt.ethz.ch,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10;10;10;10,13;13;13;13;13;13,8;1;6,9/25/19,0,0,0,0,0,0,455;2;51;18888;168;16782,13;3;15;571;24;469,3;1;4;68;5;61,59;0;0;1863;14;1325,m;m
3745,ICLR,2020,Objective Mismatch in Model-based Reinforcement Learning,Nathan Lambert;Brandon Amos;Omry Yadan;Roberto Calandra,nol@berkeley.edu;brandon.amos.cs@gmail.com;omry@fb.com;rcalandra@fb.com,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,University of California Berkeley;Facebook;Facebook;Facebook,5;-1;-1;-1,13;-1;-1;-1,,9/25/19,1,1,0,0,0,0,145;1981;68;1023,20;40;4;58,5;19;2;15,10;221;11;102,m;m
3746,ICLR,2020,Learning Explainable Models Using Attribution Priors,Gabriel Erion;Joseph D. Janizek;Pascal Sturmfels;Scott M. Lundberg;Su-In Lee,erion@cs.washington.edu;jjanizek@cs.washington.edu;psturm@cs.washington.edu;slund1@cs.washington.edu;suinlee@cs.washington.edu,8;3;1,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,University of Washington;University of Washington;University of Washington;University of Washington;University of Washington,6;6;6;6;6,26;26;26;26;26,8,6/25/19,14,3,3,0,3,1,265;201;59;434;3603,11;12;9;11;105,5;5;5;8;22,43;2;6;53;365,m;f
3747,ICLR,2020,Improved Modeling of Complex Systems Using Hybrid Physics/Machine Learning/Stochastic Models,Anand Ramakrishnan;Warren B. Jackson;Kent Evans,aramakrishnan@wpi.edu;jackson@parc.com;kent.evans@parc.com,1;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,Worcester Polytechnic Institute;Xerox;Xerox,172;-1;-1,628;-1;-1,,9/25/19,0,0,0,0,0,0,-1;-1;-1,-1;-1;-1,-1;-1;-1,0;0;0,m;m
3748,ICLR,2020,Semi-supervised 3D Face Reconstruction with Nonlinear Disentangled Representations,Zhongpai Gao;Juyong Zhang;Yudong Guo;Chao Ma;Guangtao Zhai;Xiaokang Yang,gaozhongpai@sjtu.edu.cn;juyong@ustc.edu.cn;gyd2011@mail.ustc.edu.cn;chaoma@sjtu.edu.cn;zhaiguangtao@sjtu.edu.cn;xkyang@sjtu.edu.cn,3;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0,yes,9/25/19,Shanghai Jiao Tong University;University of Science and Technology of China;University of Science and Technology of China;Shanghai Jiao Tong University;Shanghai Jiao Tong University;Shanghai Jiao Tong University,53;481;481;53;53;53,157;80;80;157;157;157,4;7,9/25/19,0,0,0,0,0,0,165;1235;507;378;224;269,39;105;44;63;34;64,8;21;11;10;6;9,4;103;35;10;11;13,m;m
3749,ICLR,2020,Efficient Exploration via State Marginal Matching,Lisa Lee;Benjain Eysenbach;Emilio Parisotto;Erix Xing;Sergey Levine;Ruslan Salakhutdinov,lslee@cs.cmu.edu;beysenba@cs.cmu.edu;eparisot@cs.cmu.edu;epxing@cs.cmu.edu;svlevine@eecs.berkeley.edu;rsalakhu@cs.cmu.edu,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;University of California Berkeley;Carnegie Mellon University,1;1;1;1;5;1,27;27;27;27;13;27,,6/12/19,18,7,4,0,10,1,713;357;888;25065;24893;69005,64;16;17;605;310;254,9;6;10;77;74;82,71;54;76;2695;3235;7875,f;m
3750,ICLR,2020,Adversarial Filters of Dataset Biases,Ronan Le Bras;Swabha Swayamdipta;Chandra Bhagavatula;Rowan Zellers;Matthew Peters;Ashish Sabharwal;Yejin Choi,ronanlb@allenai.org;swabhas@allenai.org;chandrab@allenai.org;rowanz@cs.washington.edu;matthewp@allenai.org;ashishs@allenai.org;yejinc@allenai.org,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Allen Institute for Artificial Intelligence;Allen Institute for Artificial Intelligence;Allen Institute for Artificial Intelligence;University of Washington;Allen Institute for Artificial Intelligence;Allen Institute for Artificial Intelligence;Allen Institute for Artificial Intelligence,-1;-1;-1;6;-1;-1;-1,-1;-1;-1;26;-1;-1;-1,3;4,9/25/19,6,4,2,1,0,2,517;1022;770;831;5516;3355;8068,53;24;29;12;45;146;138,14;12;11;9;19;32;43,51;137;115;168;1047;402;1016,m;f
3751,ICLR,2020,Insights on Visual Representations for Embodied Navigation Tasks,Erik Wijmans;Julian Straub;Irfan Essa;Dhruv Batra;Judy Hoffman;Ari Morcos,etw@gatech.edu;julian.straub@oculus.com;irfan@gatech.edu;dbatra@gatech.edu;judy@gatech.edu;arimorcos@gmail.com,3;3;3,I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,3,0,yes,9/25/19,Georgia Institute of Technology;Oculus;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Facebook,13;-1;13;13;13;-1,38;-1;38;38;38;-1,,9/25/19,0,0,0,0,0,0,242;587;261;1700;9487;1034,13;32;7;27;60;32,6;13;2;15;32;12,32;81;16;291;1123;116,m;m
3752,ICLR,2020,Crafting Data-free Universal Adversaries with Dilate Loss,Deepak Babu Sam;ABINAYA K;Sudharsan K A;Venkatesh Babu RADHAKRISHNAN,deepaksam@iisc.ac.in;abinayak@iisc.ac.in;sudharsanka16@gmail.com;venky@iisc.ac.in,8;3;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,"Indian Institute of Science;Indian Institute of Science;National Institute of Technology, Tiruchirappalli;Indian Institute of Science",95;95;481;95,301;301;894;301,4,9/25/19,0,0,0,0,0,0,465;20;0;34,8;14;1;6,5;2;0;2,96;1;0;5,m;u
3753,ICLR,2020,Stochastic Gradient Descent with Biased but Consistent Gradient Estimators,Jie Chen;Ronny Luss,chenjie@us.ibm.com;rluss@us.ibm.com,6;1;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,15,0,yes,9/25/19,International Business Machines;International Business Machines,-1;-1,-1;-1,10,7/31/18,14,7,2,1,10,1,30461;560,2996;33,71;11,1902;54,m;m
3754,ICLR,2020,Wasserstein Robust Reinforcement Learning,Mohammed Amin Abdullah;Hang Ren;Haitham Bou-Ammar;Vladimir Milenkovic;Rui Luo;Mingtian Zhang;Jun Wang,mohammed.abdullah@huawei.com;hang.ren1@huawei.com;haitham.ammar@huawei.com;vladimir.milenkovic@huawei.com;ruiluo@huawei.com;w.j@huawei.com,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,,7/30/19,5,3,1,0,0,0,158;311;722;19;1330;127;159,39;56;53;18;48;37;18,7;10;17;2;10;6;5,16;5;37;1;76;16;8,m;m
3755,ICLR,2020,A Training Scheme for the Uncertain Neuromorphic Computing Chips,Qingtian Zhang;Bin Gao;Huaqiang Wu,zhangqt0103@mail.tsinghua.edu.cn;gaob1@tsinghua.edu.cn;wuhq@tsinghua.edu.cn,1;6;1,I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,2,0,yes,9/25/19,Tsinghua University;Tsinghua University;Tsinghua University,8;8;8,23;23;23,,9/25/19,0,0,0,0,0,0,496;1288;1645,39;146;129,9;19;20,10;37;47,u;m
3756,ICLR,2020,A Stochastic Trust Region Method for Non-convex Minimization,Zebang Shen;Pan Zhou;Cong Fang;Jiahao Xie;Alejandro Ribeiro,shenzebang@zju.edu.cn;pzhou@u.nus.edu;fangcong@pku.edu.cn;xiejh@zju.edu.cn;aribeiro@seas.upenn.edu,3;6;8;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0,yes,9/25/19,Zhejiang University;National University of Singapore;Peking University;Zhejiang University;University of Pennsylvania,56;16;22;56;19,107;25;24;107;11,1;9,3/4/19,2,1,1,0,0,1,122;110;316;-1;380,35;64;40;-1;69,7;5;9;-1;11,10;6;63;0;22,m;m
3757,ICLR,2020,MoET: Interpretable and Verifiable Reinforcement Learning via Mixture of Expert Trees,Marko Vasic;Andrija Petrovic;Kaiyuan Wang;Mladen Nikolic;Rishabh Singh;Sarfraz Khurshid,vasic@utexas.edu;aapetrovic@mas.bg.ac.rs;kaiyuanw@google.com;nikolic@matf.bg.ac.rs;rising@google.com;khurshid@ece.utexas.edu,3;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0,yes,9/25/19,"University of Texas, Austin;University of Belgrade;Google;University of Belgrade;Google;University of Texas, Austin",22;481;-1;481;-1;22,38;1397;-1;1397;-1;38,1,6/16/19,1,0,0,0,4,0,62;214;272;243;41;6535,8;32;38;32;15;256,4;5;10;9;2;42,5;14;19;9;1;529,m;m
3758,ICLR,2020,Deceptive Opponent Modeling with Proactive Network Interdiction for Stochastic Goal Recognition Control,Junren Luo;Wei Gao;Zhiyong Liao;Weilin Yuan;Wanpeng Zhang;Shaofei Chen,luojunren17@nudt.edu.cn;gaowei14@nudt.edu.cn,1;1;1,I have read many papers in this area.:N/A:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:N/A:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,0,0,yes,9/25/19,National University of Defense Technology;National University of Defense Technology,-1;-1,-1;-1,,9/25/19,0,0,0,0,0,0,1;338;0;27;12;123,7;271;4;19;30;27,1;7;0;3;2;6,0;19;0;2;2;6,m;m
3759,ICLR,2020,Regularization Matters in Policy Optimization,Zhuang Liu;Xuanlin Li;Bingyi Kang;Trevor Darrell,zhuangl@berkeley.edu;xuanlinli17@berkeley.edu;kang@u.nus.edu;trevor@eecs.berkeley.edu,3;3;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,12,1,yes,9/25/19,University of California Berkeley;University of California Berkeley;National University of Singapore;University of California Berkeley,5;5;16;5,13;13;25;13,,9/25/19,6,6,1,0,0,1,37577;17;869;90979,341;12;41;559,90;3;15;112,2383;2;68;11527,m;m
3760,ICLR,2020,Sentence embedding with contrastive multi-views learning,Antoine Simoulin,antoine.simoulin@gmail.com,1;1;3,I have published one or two papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0,yes,9/25/19,Université Paris Diderot,481,1397,,9/25/19,0,0,0,0,0,0,2,2,1,0,m;u
3761,ICLR,2020,Robust Domain Randomization for Reinforcement Learning,Reda Bahi Slaoui;William R. Clements;Jakob N. Foerster;Sébastien Toth,reda.bahi.slaoui@gmail.com;william.clements@unchartech.com;jakobfoerster@gmail.com;sebastien.toth@unchartech.com,3;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Ecole Normale Superieure;Uncharted Technologies;Facebook;Uncharted Technologies,100;-1;-1;-1,45;-1;-1;-1,8,9/25/19,3,2,1,0,0,1,7;541;2141;7,3;35;61;3,2;9;19;2,1;27;343;1,m;u
3762,ICLR,2020,Regularizing Trajectories to Mitigate Catastrophic Forgetting,Paul Michel;Elisabeth Salesky;Graham Neubig,pmichel1@cs.cmu.edu;esalesky@gmail.com;gneubig@cs.cmu.edu,6;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,6,0,yes,9/25/19,Carnegie Mellon University;;Carnegie Mellon University,1;-1;1,27;-1;27,,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1,-1;-1;-1,-1;-1;-1,0;0;0,m;m
3763,ICLR,2020,Coloring graph neural networks for node disambiguation,George Dasoulas;Ludovic Dos Santos;Kevin Scaman;Aladin Virmaux,george.dasoulas1@gmail.com;kevin.scaman@gmail.com;ludovic.dos.santos@huawei.com;aladin.virmaux@huawei.com,6;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Ecole polytechnique;Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.,481;-1;-1;-1,93;-1;-1;-1,10,9/25/19,3,2,1,1,0,0,3;36;330;45,4;20;23;8,1;3;9;2,0;2;58;5,m;m
3764,ICLR,2020,Undersensitivity in Neural Reading Comprehension,Johannes Welbl;Pasquale Minervini;Max Bartolo;Pontus Stenetorp;Sebastian Riedel,johannes.welbl.14@ucl.ac.uk;p.minervini@gmail.com;maxbartolo@gmail.com;pontus.stenetorp@gmail.com;s.riedel@ucl.ac.uk,6;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,University College London;University College London;University College London;;University College London,50;50;50;-1;50,15;15;15;-1;15,4,9/25/19,2,2,0,0,0,1,1018;597;39;1684;409,19;41;4;48;60,9;9;2;13;10,271;170;8;321;23,m;m
3765,ICLR,2020,RotationOut as a Regularization Method for Neural Network,Kai Hu;Barnabas Poczos,kaihu@cmu.edu;bapoczos@cs.cmu.edu,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University,1;1,27;27,,9/25/19,0,0,0,0,0,0,3880;5863,492;244,31;40,110;715,m;m
3766,ICLR,2020,Meta-Learning with Network Pruning for Overfitting Reduction,Hongduan Tian;Bo Liu;Xiao-Tong Yuan;Qingshan Liu,hongduan_tian@nuist.edu.cn;kfliubo@gmail.com;xtyuan1980@gmail.com;qsliu@nuist.edu.cn,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Nanjing University of Information Science & Technology;Rutgers University;Nanjing University of Information Science & Technology;Nanjing University of Information Science & Technology,481;34;481;481,1397;168;1397;1397,6;8,9/25/19,0,0,0,0,0,0,0;1154;2206;65,2;127;98;23,0;13;23;3,0;79;215;2,m;m
3767,ICLR,2020,An implicit function learning approach for parametric modal regression,Yangchen Pan;Martha White;Amir-massoud Farahmand,pan6@ualberta.ca;whitem@ualberta.ca;farahmand@vectorinstitute.ai,1;3;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,3,0,yes,9/25/19,University of Alberta;University of Alberta;Vector Institute,100;100;-1,136;136;-1,1,9/25/19,0,0,0,0,0,0,54;310;233,13;33;32,4;6;8,6;20;16,u;m
3768,ICLR,2020,On the Evaluation of Conditional GANs,Terrance DeVries;Adriana Romero;Luis Pineda;Graham W. Taylor;Michal Drozdzal,terrance@uoguelph.ca;adrianars@fb.com;lep@fb.com;gwtaylor@uoguelph.ca;mdrozdzal@fb.com,3;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,5,0,yes,9/25/19,University of Guelph;Facebook;Facebook;University of Guelph;Facebook,266;-1;-1;266;-1,558;-1;-1;558;-1,5;4;1,7/11/19,3,2,2,0,0,0,846;3926;1438;5971;1657,10;53;172;143;39,7;14;19;31;13,142;644;109;508;159,m;m
3769,ICLR,2020,CZ-GEM:  A  FRAMEWORK  FOR DISENTANGLED REPRESENTATION LEARNING,Akash Srivastava;Yamini Bansal;Yukun Ding;Bernhard Egger;Prasanna Sattigeri;Josh Tenenbaum;David D. Cox;Dan Gutfreund,akash.srivastava@me.com;ybansal@g.harvard.edu;yding5@nd.edu;egger@mit.edu;psattig@us.ibm.com;jbt@mit.edu;david.d.cox@ibm.com;dgutfre@us.ibm.com,1;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,4,0,yes,9/25/19,International Business Machines;Harvard University;University of Notre Dame;Massachusetts Institute of Technology;International Business Machines;Massachusetts Institute of Technology;International Business Machines;International Business Machines,-1;39;118;2;-1;2;-1;-1,-1;7;157;5;-1;5;-1;-1,5,9/25/19,0,0,0,0,0,0,490;191;135;822;610;250;5978;616,49;6;26;76;59;28;109;39,7;3;5;16;12;6;31;14,99;36;5;73;73;28;418;69,m;m
3770,ICLR,2020,On Weight-Sharing and Bilevel Optimization in Architecture Search,Mikhail Khodak;Liam Li;Maria-Florina Balcan;Ameet Talwalkar,khodak@cmu.edu;me@liamcli.com;ninamf@cs.cmu.edu;talwalkar@cmu.edu,3;3,I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,2,0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1,27;27;27;27,3;8,9/25/19,0,0,0,0,0,0,372;198;4742;6515,23;7;176;79,8;3;38;34,54;35;479;768,m;m
3771,ICLR,2020,Are there any 'object detectors' in the hidden layers of CNNs trained to identify objects or scenes?,Ella M. Gale;Nicholas Martin;Ryan Blything;Anh Nguyen;Jeffrey S. Bowers,ella.gale@bristol.ac.uk;nm13850@bristol.ac.uk;ryan.blything@bristol.ac.uk;anhnguyen@auburn.edu;j.bowers@bristol.ac.uk,3;3;8,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,3,0,yes,9/25/19,University of Bristol;University of Bristol;University of Bristol;Auburn University;University of Bristol,128;128;128;390;128,87;87;87;651;87,,9/25/19,0,0,0,0,0,0,54;84722;70;6;2568,9;1571;12;10;124,2;136;4;2;28,4;4264;9;0;250,f;m
3772,ICLR,2020,Dimensional Reweighting Graph Convolution Networks,Xu Zou;Qiuye Jia;Jianwei Zhang;Chang Zhou;Zijun Yao;Hongxia Yang;Jie Tang,zoux18@mails.tsinghua.edu.cn;jqy@stanford.edu;zhangjianwei.zjw@alibaba-inc.com;ericzhou.zc@alibaba-inc.com;yaozijun@bupt.edu.cn;yang.yhx@alibaba-inc.com;jietang@tsinghua.edu.cn,3;6;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,7,0,yes,9/25/19,Tsinghua University;Stanford University;Alibaba Group;Alibaba Group;Beijing University of Post and Telecommunication;Alibaba Group;Tsinghua University,8;4;-1;-1;481;-1;8,23;4;-1;-1;1397;-1;23,1;10,7/4/19,0,0,0,0,0,0,197;0;78;1558;-1;614;1671,18;2;20;193;-1;89;161,4;0;4;21;-1;12;24,5;0;2;107;0;74;67,m;m
3773,ICLR,2020,Leveraging inductive bias of neural networks for learning without explicit human annotations,Fatih Furkan Yilmaz;Reinhard Heckel,fy11@rice.edu;rh43@rice.edu,6;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,2,0,yes,9/25/19,Rice University;Rice University,84;84,105;105,4,9/25/19,0,0,0,0,0,0,-1;971,-1;58,-1;16,0;117,m;m
3774,ICLR,2020,XLDA: Cross-Lingual Data Augmentation for Natural Language Inference and Question Answering,Jasdeep Singh;Bryan McCann;Nitish Shirish Keskar;Caiming Xiong;Richard Socher,jasdeep@cs.stanford.edu;bmccann@salesforce.com;nkeskar@salesforce.com;cxiong@salesforce.com;rsocher@salesforce.com,1;8;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,Stanford University;SalesForce.com;SalesForce.com;SalesForce.com;SalesForce.com,4;-1;-1;-1;-1,4;-1;-1;-1;-1,3;6,5/27/19,10,5,5,1,0,3,303;56;2198;6301;53531,38;27;28;156;180,9;4;13;31;49,20;5;336;1045;8917,m;m
3775,ICLR,2020,Ergodic Inference: Accelerate Convergence by Optimisation,Yichuan Zhang;José Miguel Hernández-Lobato,yichuan.zhang@eng.cam.ac.uk;jmh233@cam.ac.uk,3;8;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,2,yes,9/25/19,University of Cambridge;University of Cambridge,71;71,3;3,,5/25/18,0,0,0,0,0,0,94;3824,31;114,4;28,4;420,m;m
3776,ICLR,2020,Synthetic vs Real: Deep Learning on Controlled Noise,Lu Jiang;Di Huang;Weilong Yang,lujiang@google.com;dihuang@google.com;weilongyang@google.com,3;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,4,0,yes,9/25/19,Google;Google;Google,-1;-1;-1,-1;-1;-1,,9/25/19,2,1,0,0,0,0,108;785;781,35;48;34,5;10;11,10;43;76,m;m
3777,ICLR,2020,Layerwise Learning Rates for Object Features in Unsupervised and Supervised Neural Networks And Consequent Predictions for the Infant Visual System,Rhodri Cusack;Cliona O'Doherty;Anna Birbeck;Anna Truzzi,cusackrh@tcd.ie;odoherc1@tcd.ie;birbecka@tcd.ie;truzzia@tcd.ie,3;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:N/A:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:N/A:N/A,Reject,0,5,0,yes,9/25/19,"Trinity College, Dublin;Trinity College, Dublin;Trinity College, Dublin;Trinity College, Dublin",481;481;481;481,164;164;164;164,,9/25/19,0,0,0,0,0,0,421;0;0;132,38;1;1;21,10;0;0;7,30;0;0;3,m;f
3778,ICLR,2020,Non-Sequential Melody Generation,Mitchell Billard;Robert Bishop;Moustafa Elsisy;Laura Graves;Antonina Kolokolova;Vineel Nagisetty;Zachary Northcott;Heather Patey,mlb238@mun.ca;r.bishop@mun.ca;mmatelsisy@mun.ca;cmgraves@mun.ca;kol@mun.ca;vnagisetty@mun.ca;zmnorthcott@mun.ca;hpatey@gmail.com,1;3;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,3,0,yes,9/25/19,Memorial University of Newfoundland;Memorial University of Newfoundland;Memorial University of Newfoundland;Memorial University of Newfoundland;Memorial University of Newfoundland;Memorial University of Newfoundland;Memorial University of Newfoundland;,390;390;390;390;390;390;390;-1,557;557;557;557;557;557;557;-1,5;4,9/25/19,0,0,0,0,0,0,0;1307;0;853;250;0;0;0,1;151;1;49;48;3;1;1,0;20;0;17;10;0;0;0,0;84;0;78;20;0;0;0,m;u
3779,ICLR,2020,A Dynamic Approach to Accelerate Deep Learning Training,John Osorio;Adrià Armejach;Eric Petit;Marc Casas,john.osorio@bsc.es;adria.armejach@bsc.es;eric.petit@intel.com;marc.casas@bsc.es,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Barcelona Supercomputing Center;Barcelona Supercomputing Center;Intel;Barcelona Supercomputing Center,-1;-1;-1;-1,-1;-1;-1;-1,,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1;-1,-1;-1;-1;-1,-1;-1;-1;-1,0;0;0;0,u;u
3780,ICLR,2020,Emergence of Compositional Language with Deep Generational Transmission,Michael Cogswell;Jiasen Lu;Stefan Lee;Devi Parikh;Dhruv Batra,cogswell@gatech.edu;jiasenlu@gatech.edu;steflee@gatech.edu;parikh@gatech.edu;dbatra@gatech.edu,6;1;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:N/A:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,6,0,yes,9/25/19,Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology,13;13;13;13;13,38;38;38;38;38,8,4/19/19,11,6,0,0,0,1,2475;3808;155;15347;1700,12;24;27;185;27,9;13;7;55;15,343;745;24;2411;291,m;m
3781,ICLR,2020,AutoGrow: Automatic Layer Growing in Deep Convolutional Networks,Wei Wen;Feng Yan;Hai Li,wei.wen@duke.edu;fyan@unr.edu;hai.li@duke.edu,3;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,1,yes,9/25/19,"Duke University;University of Nevada, Reno;Duke University",47;266;47,20;1397;20,,6/7/19,2,0,1,0,0,0,2385;11665;162,98;930;26,19;52;6,265;403;17,m;f
3782,ICLR,2020,Model-Agnostic Feature Selection with Additional Mutual Information,Mukund Sudarshan;Aahlad Manas Puli;Lakshmi Subramanian;Sriram Sankararaman;Rajesh Ranganath,ms7490@nyu.edu;apm470@nyu.edu;lakshmi@cs.nyu.edu;sriram@cs.ucla.edu;rajeshr@cims.nyu.edu,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,"New York University;New York University;New York University;University of California, Los Angeles;New York University",25;25;25;20;25,29;29;29;17;29,,9/25/19,0,0,0,0,0,0,17;14;24;5554;1,11;3;10;79;3,3;1;2;30;1,1;0;0;506;0,m;m
3783,ICLR,2020,Certifying Neural Network Audio Classifiers,Wonryong Ryou;Mislav Balunovic;Gagandeep Singh;Martin Vechev,wryou@student.ethz.ch;bmislav@student.ethz.ch;gsingh@inf.ethz.ch;martin.vechev@inf.ethz.ch,6;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10;10,13;13;13;13,,9/25/19,0,0,0,0,0,0,0;68;24;4246,3;8;23;153,0;5;3;36,0;2;0;467,m;m
3784,ICLR,2020,Wasserstein-Bounded Generative Adversarial Networks,Peng Zhou;Bingbing Ni;Lingxi Xie;Xiaopeng Zhang;Hang Wang;Cong Geng;Qi Tian,zhoupengcv@sjtu.edu.cn;nibingbing@sjtu.edu.cn;198808xc@gmail.com;zxphistory@gmail.com;wang--hang@sjtu.edu.cn;gengcong@sjtu.edu.cn;tian.qi1@huawei.com,1;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,0,0,yes,9/25/19,Shanghai Jiao Tong University;Shanghai Jiao Tong University;Huawei Technologies Ltd.;National University of Singapore;Shanghai Jiao Tong University;Shanghai Jiao Tong University;Huawei Technologies Ltd.,53;53;-1;16;53;53;-1,157;157;-1;25;157;157;-1,5;4,9/25/19,0,0,0,0,0,0,296;4304;2206;489;12;591;323,51;171;108;118;23;17;90,5;36;24;12;2;6;8,11;334;228;47;1;39;24,m;m
3785,ICLR,2020,An Explicitly Relational Neural Network Architecture,Murray Shanahan;Kyriacos Nikiforou;Antonia Creswell;Christos Kaplanis;David Barrett;Marta Garnelo,mshanahan@google.com;knikiforou@google.com;tonicreswell@google.com;christos.kaplanis14@imperial.ac.uk;barrettdavid@google.com;garnelo@google.com,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,4,1,yes,9/25/19,Google;Google;Google;Imperial College London;Google;Google,-1;-1;-1;73;-1;-1,-1;-1;-1;10;-1;-1,,5/24/19,5,2,2,0,0,0,4905;32;469;66;459;952,166;12;15;6;34;23,38;3;7;4;6;11,460;1;34;4;25;116,m;f
3786,ICLR,2020,Do Deep Neural Networks for Segmentation Understand Insideness?,Kimberly M Villalobos;Vilim Stih;Amineh Ahmadinejad;Jamell Dozier;Andrew Francl;Frederico Azevedo;Tomotake Sasaki;Xavier Boix,kimvc@mit.edu;vilim@neuro.mpg.de;amineh@mit.edu;jamell@mit.edu;francl@mit.edu;fazevedo@mit.edu;tomotake.sasaki@fujitsu.com;xboix@mit.edu,6;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,1,yes,9/25/19,Massachusetts Institute of Technology;Max Planck Institute of Neurobiology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Fujitsu Laboratories Ltd.;Massachusetts Institute of Technology,2;-1;2;2;2;2;-1;2,5;-1;5;5;5;5;-1;5,2,9/25/19,1,1,0,0,0,0,1;59;0;0;19;9;11;1454,4;9;1;3;6;14;14;51,1;3;0;0;2;1;2;16,0;1;0;0;0;0;0;226,f;m
3787,ICLR,2020,Split LBI for Deep Learning: Structural Sparsity via Differential Inclusion Paths,Yanwei Fu;Chen Liu;Donghao Li;Xinwei Sun;Jinshan ZENG;Yuan Yao,yanweifu@fudan.edu.cn;corwinliu9669@gmail.com;donghao.li@connect.ust.hk;xinsun@microsoft.com;jsh.zeng@gmail.com;yuany@ust.hk,3;8;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0,yes,9/25/19,Fudan University;Fudan University;The Hong Kong University of Science and Technology;Microsoft;Australian National University;The Hong Kong University of Science and Technology,79;79;39;-1;108;39,109;109;47;-1;50;47,9;8,9/25/19,0,0,0,0,0,0,1915;268;0;69;792;73,90;88;3;17;48;31,22;8;0;5;11;2,254;13;0;3;59;8,m;m
3788,ICLR,2020,Contextual Text Style Transfer,Yu Cheng;Zhe Gan;Yizhe Zhang;Oussama Elachqar;Dianqi Li;Jingjing Liu,yu.cheng@microsoft.com;zhe.gan@microsoft.com;yizhe.zhang@microsoft.com;ouelachq@microsoft.com;dianqili@uw.edu;jingjl@microsoft.com,3;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,"Microsoft;Microsoft;Microsoft;Microsoft;University of Washington, Seattle;Microsoft",-1;-1;-1;-1;6;-1,-1;-1;-1;-1;26;-1,,9/25/19,1,0,0,0,0,0,10026;2349;27;5;799;144,853;85;13;5;10;31,46;25;3;1;6;6,509;322;2;0;28;28,m;f
3789,ICLR,2020,FLAT MANIFOLD VAES,Nutan Chen;Alexej Klushyn;Francesco Ferroni;Justin Bayer;Patrick van der Smagt,nutan.chen@gmail.com;a.klushyn@gmail.com;francescoferroni1@gmail.com;bayer.justin@googlemail.com;smagt@argmax.ai,1;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,11,0,yes,9/25/19,"Data Lab, Volkswagen Group;Technical University Munich;;Data Lab, Volkswagen Group;Volkswagen Group, ML Research Lab",-1;53;-1;-1;-1,-1;43;-1;-1;-1,,9/25/19,0,0,0,0,0,0,260;49;25;2350;3990,21;8;9;44;123,8;4;3;11;26,20;3;1;180;419,m;m
3790,ICLR,2020,An Information Theoretic Approach to Distributed Representation Learning,Abdellatif Zaidi;Inaki Estella Aguerri,abdellatif.zaidi@u-pem.fr;inaki.estella@huawei.com,3;3;8;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,Université Paris-Est;Huawei Technologies Ltd.,481;-1,1397;-1,1,9/25/19,0,0,0,0,0,0,946;156,132;37,16;7,45;6,m;m
3791,ICLR,2020,LIA: Latently Invertible Autoencoder with Adversarial Learning,Jiapeng Zhu;Deli Zhao;Bolei Zhou;Bo Zhang,jengzhu0@gmail.com;zhaodeli@gmail.com;bzhou@ie.cuhk.edu.hk;zhangbo@xiaomi.com,3;3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,5,0,yes,9/25/19,;;The Chinese University of Hong Kong;Xiaomi,-1;-1;59;-1,-1;-1;35;-1,5;4;2,9/25/19,7,2,5,0,0,2,497;44;318;-1,15;17;103;-1,4;4;6;-1,55;4;46;0,m;m
3792,ICLR,2020,Connecting the Dots Between MLE and RL for Sequence Prediction,Bowen Tan;Zhiting Hu;Zichao Yang;Ruslan Salakhutdinov;Eric Xing,bwkevintan@gmail.com;zhitinghu@gmail.com;yangtze2301@gmail.com;rsalakhu@cs.cmu.edu;epxing@cs.cmu.edu,3;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,;Carnegie Mellon University;;Carnegie Mellon University;Carnegie Mellon University,-1;1;-1;1;1,-1;27;-1;27;27,3,11/24/18,14,9,7,0,8,2,96;3281;5699;69005;25065,22;64;56;254;605,6;29;18;82;77,5;364;628;7875;2695,m;m
3793,ICLR,2020,BEYOND SUPERVISED LEARNING: RECOGNIZING UNSEEN ATTRIBUTE-OBJECT PAIRS WITH VISION-LANGUAGE FUSION AND ATTRACTOR NETWORKS,Hui Chen;Zhixiong Nan;Nanning Zheng,chenhui0622@stu.xjtu.edu.cn;nanzhixiong@stu.xjtu.edu.cn;nnzheng@mail.xjtu.edu.cn,1;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,0,0,yes,9/25/19,Xi'an Jiaotong University;Xi'an Jiaotong University;Xi'an Jiaotong University,481;481;481,555;555;555,5,9/25/19,0,0,0,0,0,0,184;22;1113,75;10;61,8;2;12,6;1;177,m;m
3794,ICLR,2020,Distance-based Composable Representations with Neural Networks,Graham Spinks;Marie-Francine Moens,graham.spinks@cs.kuleuven.be;sien.moens@cs.kuleuven.be,3;3;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,KU Leuven;KU Leuven,118;118,45;45,,9/25/19,0,0,0,0,0,0,12;404,7;34,2;6,1;27,m;m
3795,ICLR,2020,Efficient Deep Representation Learning by Adaptive Latent Space Sampling,Yuanhan Mo;Shuo Wang;Chengliang Dai;Rui Zhou;Zhongzhao Teng;Wenjia Bai;Yike Guo,y.mo16@imperial.ac.uk;shuo.wang@imperial.ac.uk;c.dai@imperial.ac.uk;rui.zhou18@imperial.ac.uk;zt215@cam.ac.uk;w.bai@imperial.ac.uk;y.guo@imperial.ac.uk,8;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,Imperial College London;Imperial College London;Imperial College London;Imperial College London;University of Cambridge;Imperial College London;Imperial College London,73;73;73;73;71;73;73,10;10;10;10;3;10;10,5;2,9/25/19,0,0,0,0,0,0,232;122;10;234;18;2299;157,11;18;12;32;8;113;48,3;5;2;5;2;22;8,17;6;1;17;0;125;9,m;m
3796,ICLR,2020,Out-of-Distribution Detection Using Layerwise Uncertainty in Deep Neural Networks,Hirono Okamoto;Masahiro Suzuki;Yutaka Matsuo,h-okamoto@weblab.t.u-tokyo.ac.jp;masa@weblab.t.u-tokyo.ac.jp;matsuo@weblab.t.u-tokyo.ac.jp,3;1;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,6,0,yes,9/25/19,The University of Tokyo;The University of Tokyo;The University of Tokyo,56;56;56,36;36;36,,9/25/19,0,0,0,0,0,0,3;795;7676,5;316;381,1;12;34,0;78;512,m;m
3797,ICLR,2020,Effect of top-down connections in Hierarchical Sparse Coding,Victor Boutin;Angelo Franciosini;Franck Ruffier;Laurent Perrinet,victor.boutin@univ-amu.fr;angelo.franciosini@univ-amu.fr;franck.ruffier@univ-amu.fr;laurent.perrinet@univ-amu.fr,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,Aix Marseille Univ;Aix Marseille Univ;Aix Marseille Univ;Aix Marseille Univ,481;481;481;481,329;329;329;329,,9/25/19,0,0,0,0,0,0,11;10;1008;358,9;11;75;52,2;2;15;10,0;0;46;24,u;m
3798,ICLR,2020,Multitask Soft Option Learning,Maximilian Igl;Andrew Gambardella;Jinke He;Nantas Nardelli;N. Siddharth;Wendelin Böhmer;Shimon Whiteson,maximilian.igl@gmail.com;gambs@robots.ox.ac.uk;jinkehe1996@gmail.com;nantas@robots.ox.ac.uk;nsid@robots.ox.ac.uk;wendelin.boehmer@cs.ox.ac.uk;shimon.whiteson@cs.ox.ac.uk,8;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,University of Oxford;University of Oxford;;University of Oxford;University of Oxford;University of Oxford;University of Oxford,50;50;-1;50;50;50;50,1;1;-1;1;1;1;1,,4/1/19,4,1,2,0,7,0,272;5;-1;800;216;166;5445,12;3;-1;16;21;18;203,6;2;-1;8;9;8;38,42;0;0;124;15;13;588,m;m
3799,ICLR,2020,Compositional Transfer in Hierarchical Reinforcement Learning,Markus Wulfmeier;Abbas Abdolmaleki;Roland Hafner;Jost Tobias Springenberg;Michael Neunert;Tim Hertweck;Thomas Lampe;Noah Siegel;Nicolas Heess;Martin Riedmiller,mwulfmeier@google.com;aabdolmaleki@google.com;rhafner@google.com;springenberg@google.com;neunertm@google.com;thertweck@google.com;thomaslampe@google.com;heess@google.com;riedmiller@google.com,3;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1,,6/26/19,0,0,0,0,0,0,615;549;733;7449;681;17;1165;10;11597;25961,27;45;24;54;33;5;85;3;104;190,12;10;10;28;14;2;16;1;37;40,51;59;33;798;39;1;51;2;1644;3578,m;m
3800,ICLR,2020,Federated User Representation Learning,Duc Bui;Kshitiz Malik;Jack Goetz;Seungwhan Moon;Honglei Liu;Anuj Kumar;Kang G. Shin,ducbui@umich.edu;kmalik2@fb.com;jrgoetz@umich.edu;shanemoon@fb.com;honglei@fb.com;anujk@fb.com;kgshin@umich.edu,8;3;1,I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,University of Michigan;Facebook;University of Michigan;Facebook;Facebook;Facebook;University of Michigan,8;-1;8;-1;-1;-1;8,21;-1;21;-1;-1;-1;21,,9/25/19,3,2,0,0,0,0,861;4;41;220;276;214;28438,57;2;7;28;62;96;1081,12;1;3;10;10;7;81,42;0;7;19;4;13;2286,m;m
3801,ICLR,2020,Fairness with Wasserstein Adversarial Networks,serrurier Mathieu;Loubes Jean-Michel;Edouard Pauwels,mathieu.serrurier@irit.fr;loubes@math.univ-toulouse.fr;edouard.pauwels@irit.fr,1;1;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,"IRIT, University of Toulouse;Université de Toulouse;IRIT, University of Toulouse",-1;-1;-1,-1;-1;-1,5;4;7,9/25/19,1,1,0,0,0,0,1;5;652,1;3;46,1;1;12,0;0;57,m;m
3802,ICLR,2020,Quantum Expectation-Maximization for Gaussian Mixture Models,Iordanis Kerenidis;Anupam Prakash;Alessandro Luongo,jkeren@gmail.com;anupamprakash1@gmail.com;aluongo@irif.fr,3;3;1,I have published in this field for several years.:N/A:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,7,0,yes,9/25/19,Université Paris Diderot;Universite Paris Diderot;Universite Paris Diderot,481;481;481,1397;1397;1397,5,8/19/19,1,0,0,0,0,0,1603;616;49,100;39;6,22;11;3,152;67;2,m;m
3803,ICLR,2020,Deep Evidential Uncertainty,Alexander Amini;Wilko Schwarting;Ava Soleimany;Daniela Rus,amini@mit.edu;wilkos@mit.edu;asolei@mit.edu;rus@csail.mit.edu,6;6;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,12,0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2,5;5;5;5,4;2,9/25/19,0,0,0,0,0,0,4;352;24;5023,6;27;9;166,1;11;1;29,2;24;0;217,m;f
3804,ICLR,2020,Variational Information Bottleneck for Unsupervised Clustering: Deep Gaussian Mixture Embedding,Yigit Ugur;George Arvanitakis;Abdellatif Zaidi,ygtugur@gmail.com;george.arvanitakis@huawei.com;abdellatif.zaidi@u-pem.fr,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,1,0,0,yes,9/25/19,Huawei Technologies Ltd.;Huawei Technologies Ltd.;Université Paris-Est,-1;-1;481,-1;-1;1397,5;1,5/28/19,1,1,0,0,0,1,64;96;946,9;15;132,4;5;16,2;5;45,u;m
3805,ICLR,2020,Multi-scale Attributed Node Embedding,Benedek Rozemberczki;Carl Allen;Rik Sarkar,benedek.rozemberczki@gmail.com;carl.allen@ed.ac.uk;rsarkar@inf.ed.ac.uk,6;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,5,0,yes,9/25/19,University of Edinburgh;University of Edinburgh;University of Edinburgh,33;33;33,30;30;30,1,9/25/19,15,7,7,0,0,2,92;119;1101,13;7;59,5;5;18,10;21;96,m;m
3806,ICLR,2020,Model Inversion Networks for Model-Based Optimization,Aviral Kumar;Sergey Levine,aviralkumar2907@gmail.com;svlevine@eecs.berkeley.edu,6;3;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,University of California Berkeley;University of California Berkeley,5;5,13;13,,9/25/19,0,0,0,0,0,0,143;68,24;31,6;4,32;4,m;m
3807,ICLR,2020,Variational Autoencoders with Normalizing Flow Decoders,Rogan Morrow;Wei-Chen Chiu,rogan.o.morrow@gmail.com;walon@cs.nctu.edu.tw,3;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,4,0,yes,9/25/19,National Chiao Tung University;National Chiao Tung University,143;143,564;564,5,9/25/19,3,1,1,0,0,0,3;256,3;29,1;7,0;33,u;m
3808,ICLR,2020,Adversarial Attacks on Copyright Detection Systems,Parsa Saadatpanah;Ali Shafahi;Tom Goldstein,parsa@cs.umd.edu;ashafahi@cs.umd.edu;tomg@cs.umd.edu,3;3;3;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,5,0,yes,9/25/19,"University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park",12;12;12,91;91;91,4;1,6/17/19,10,6,2,0,5,0,39;453;5954,6;33;98,3;8;27,2;47;731,m;m
3809,ICLR,2020,MODiR: Multi-Objective Dimensionality Reduction for Joint Data Visualisation,Tim Repke;Ralf Krestel,tim.repke@hpi.uni-potsdam.de;ralf.krestel@hpi.de,1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0,yes,9/25/19,University of Potsdam;Hasso Plattner Institute,323;266,272;1397,3;10,9/25/19,0,0,0,0,0,0,21;1018,11;96,3;15,3;62,u;m
3810,ICLR,2020,Actor-Critic Approach for Temporal Predictive Clustering,Changhee Lee;Mihaela van der Schaar,chl8856@gmail.com;mihaela@ee.ucla.edu,3;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,"University of California, Los Angeles;University of California, Los Angeles",20;20,17;17,,9/25/19,0,0,0,0,0,0,26;8828,26;642,3;42,0;547,m;f
3811,ICLR,2020,NeuroFabric: Identifying Ideal Topologies for Training A Priori Sparse Networks,Mihailo Isakov;Michel A. Kinsy,mihailo@bu.edu;mkinsy@bu.edu,3;3;3;3,I do not know much about this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Boston University;Boston University,67;67,61;61,,9/25/19,0,0,0,0,0,0,48;555,24;82,5;13,1;45,m;m
3812,ICLR,2020,Chart Auto-Encoders for Manifold Structured  Data,Stephan Schonsheck;Jie Chen;Rongjie Lai,schons@rpi.edu;chenjie@us.ibm.com;lair@rpi.edu,6;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0,yes,9/25/19,Rensselaer Polytechnic Institute;International Business Machines;Rensselaer Polytechnic Institute,172;-1;172,438;-1;438,5,9/25/19,0,0,0,0,0,0,12;73;951,5;43;61,2;4;18,0;5;55,m;m
3813,ICLR,2020,Putting Machine Translation in Context with the Noisy Channel Model,Lei Yu;Laurent Sartran;Wojciech Stokowiec;Wang Ling;Lingpeng Kong;Phil Blunsom;Chris Dyer,leiyu@google.com;lsartran@google.com;wstokowiec@google.com;lingwang@google.com;lingpenk@google.com;pblunsom@google.com;cdyer@google.com,3;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,3,9/25/19,5,1,1,0,0,0,177;5;84;446;1072;11713;21507,88;3;12;93;32;144;231,8;1;5;9;14;47;61,6;0;5;21;110;1357;3161,f;m
3814,ICLR,2020,Optimizing Data Usage via Differentiable Rewards,Xinyi Wang;Hieu Pham;Paul Michel;Antonios Anastasopoulos;Graham Neubig;Jaime Carbonell,xinyiw1@cs.cmu.edu;hyhieu@cmu.edu;pmichel1@cs.cmu.edu;aanastas@andrew.cmu.edu;gneubig@cs.cmu.edu;jgc@cs.cmu.edu,3;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1;1;1,27;27;27;27;27;27,3,9/25/19,3,3,3,0,0,1,194;60;520;601;5418;16003,67;17;17;41;442;507,9;3;7;10;38;55,10;2;53;51;557;1648,f;m
3815,ICLR,2020,Representing Model Uncertainty of Neural Networks in Sparse Information Form,Jongseok Lee;Rudolph Triebel,jongseok.lee@dlr.de;rudolph.triebel@dlr.de,3;1;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,9,0,yes,9/25/19,German Aerospace Center (DLR);German Aerospace Center (DLR),-1;-1,-1;-1,11,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1,-1;-1,-1;-1,0;0,m;m
3816,ICLR,2020,Unsupervised Learning of Efficient and Robust Speech Representations,Kazuya Kawakami;Luyu Wang;Chris Dyer;Phil Blunsom;Aaron van den Oord,kawakamik@google.com;luyuwang@google.com;cdyer@google.com;pblunsom@google.com;avdnoord@google.com,6;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,4,0,yes,9/25/19,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,,9/25/19,2,0,0,0,0,0,1855;120;21507;11713;165,21;46;231;144;6,7;5;61;47;3,380;5;3161;1357;23,m;m
3817,ICLR,2020,A Kolmogorov Complexity Approach to Generalization in Deep Learning,Hazar Yueksel;Kush R. Varshney;Brian Kingsbury,hazar.yueksel@ibm.com;krvarshn@us.ibm.com;bedk@us.ibm.com,1;8;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,9,0,yes,9/25/19,International Business Machines;International Business Machines;International Business Machines,-1;-1;-1,-1;-1;-1,4;8,9/25/19,0,0,0,0,0,0,-1;1844;13892,-1;185;131,-1;20;39,0;89;821,m;m
3818,ICLR,2020,Learning Invariants through Soft Unification,Nuri Cingillioglu;Alessandra Russo,nuri.cingillioglu13@imperial.ac.uk;a.russo@imperial.ac.uk,1;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0,yes,9/25/19,Imperial College London;Imperial College London,73;73,10;10,,9/16/19,0,0,0,0,0,0,5;324,3;59,2;9,0;22,m;f
3819,ICLR,2020,Collapsed amortized variational inference for switching nonlinear dynamical systems,Zhe Dong;Bryan A. Seybold;Kevin P. Murphy;Hung H. Bui,zhedong@google.com;baseybold@gmail.com;kpmurphy@google.com;bui.h.hung@gmail.com,8;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,,9/25/19,0,0,0,0,0,0,0;1106;16324;2928,14;16;83;104,0;9;41;25,0;141;2303;183,m;m
3820,ICLR,2020,ODE Analysis of Stochastic Gradient Methods with Optimism and Anchoring  for Minimax Problems and GANs,Ernest K. Ryu;Kun Yuan;Wotao Yin,eryu@math.ucla.edu;kunyuan@ucla.edu;wotaoyin@math.ucla.edu,1;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,"University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles",20;20;20,17;17;17,5;4;1,5/26/19,6,3,1,1,0,1,399;2358;17461,27;126;221,10;20;56,38;195;1912,m;m
3821,ICLR,2020,Event Discovery for History Representation in Reinforcement Learning,Aleksandr Ermolov;Enver Sangineto;Nicu Sebe,aleksandr.ermolov@unitn.it;enver.sangineto@unitn.it;niculae.sebe@unitn.it,3;6;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,25,0,yes,9/25/19,University of Trento;University of Trento;University of Trento,18;18;18,307;307;307,,9/25/19,0,0,0,0,0,0,16;1145;15462,37;68;546,2;18;65,0;117;1013,mm;m
3822,ICLR,2020,Learning to Rank Learning Curves,Martin Wistuba;Tejaswini Pedapati,martin.wistuba@ibm.com;tejaswinip@us.ibm.com,6;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,5,0,yes,9/25/19,International Business Machines;International Business Machines,-1;-1,-1;-1,6,9/25/19,1,1,0,0,0,0,750;90,53;13,15;5,78;8,m;f
3823,ICLR,2020,Scoring-Aggregating-Planning: Learning task-agnostic priors from interactions and sparse rewards for zero-shot generalization,Huazhe Xu;Boyuan Chen;Yang Gao;Trevor Darrell,huazhe_xu@eecs.berkeley.edu;boyuanchen@berkeley.edu;yg@eecs.berkeley.edu;trevordarrell@eecs.berkeley.edu,3;6;6,I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,5,0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5,13;13;13;13,6,9/25/19,0,0,0,0,0,0,815;125;480;90979,14;22;80;559,7;5;12;112,99;18;30;11527,m;m
3824,ICLR,2020,Topic Models with Survival Supervision: Archetypal Analysis and Neural Approaches,George H. Chen;Linhong Li;Ren Zuo;Amanda Coston;Jeremy C. Weiss,georgechen@cmu.edu;linhongl@andrew.cmu.edu;renzuo.wren@gmail.com;acoston@cs.cmu.edu;jeremyweiss@cmu.edu,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,3,0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;;Carnegie Mellon University;Carnegie Mellon University,1;1;-1;1;1,27;27;-1;27;27,,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,0;0;0;0;0,m;m
3825,ICLR,2020,Way Off-Policy Batch Deep Reinforcement Learning of Human Preferences in Dialog,Natasha Jaques;Asma Ghandeharioun;Judy Hanwen Shen;Craig Ferguson;Agata Lapedriza;Noah Jones;Shixiang Gu;Rosalind Picard,jaquesn@mit.edu;asma_gh@mit.edu;judyshen@mit.edu;fergusoc@mit.edu;agata@mit.edu;ncjones@mit.edu;shanegu@google.com;picard@media.mit.edu,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Google;Massachusetts Institute of Technology,2;2;2;2;2;2;-1;2,5;5;5;5;5;5;-1;5,1;8,6/30/19,27,14,12,0,0,6,928;391;65;56;6517;49;3857;28228,43;24;10;17;43;23;39;456,19;11;4;4;13;3;21;83,75;32;8;8;1165;6;478;2000,f;f
3826,ICLR,2020,Well-Read Students Learn Better: On the Importance of Pre-training Compact Models,Iulia Turc;Ming-Wei Chang;Kenton Lee;Kristina Toutanova,iuliaturc@google.com;mingweichang@google.com;kentonl@google.com;kristout@google.com,1;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,4,0,yes,9/25/19,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,3,9/25/19,18,8,6,0,0,3,36;11019;12392;14331,2;90;36;105,2;30;20;36,6;2904;3524;3225,f;f
3827,ICLR,2020,Model Imitation for Model-Based Reinforcement Learning,Yueh-Hua Wu;Ting-Han Fan;Peter J. Ramadge;Hao Su,kriswu8021@gmail.com;tinghanf@princeton.edu;ramadge@princeton.edu;haosu@eng.ucsd.edu,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,5,0,yes,9/25/19,"National Taiwan University;Princeton University;Princeton University;University of California, San Diego",86;31;31;11,120;6;6;31,,9/25/19,1,1,1,0,0,0,29;1;4979;17303,9;3;124;48,3;1;27;15,4;0;568;2681,m;m
3828,ICLR,2020,Black Box Recursive Translations for Molecular Optimization,Farhan Damani;Vishnu Sresht;Stephen Ra,farhand7@gmail.com;vishnu.sresht@pfizer.com;stephen.ra@pfizer.com,6;3;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,9,0,yes,9/25/19,Princeton University;Pfizer R&D;Pfizer R&D,31;-1;-1,6;-1;-1,10,9/25/19,1,1,0,0,0,0,244;327;177,8;22;6,5;9;4,11;9;15,m;m
3829,ICLR,2020,Improving the Generalization of Visual Navigation Policies using Invariance Regularization,Michel Aractingi;Christopher Dance;Julien Perez;Tomi Silander,michel.aractingi@naverlabs.com;christopher.dance@naverlabs.com;julien.perez@naverlabs.com;tomi.silander@naverlabs.com,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Naver Labs Europe;Naver Labs Europe;Naver Labs Europe;Naver Labs Europe,-1;-1;-1;-1,-1;-1;-1;-1,8,5/5/19,0,0,0,0,0,0,-1;-1;-1;-1,-1;-1;-1;-1,-1;-1;-1;-1,0;0;0;0,m;m
3830,ICLR,2020,PatchFormer: A neural architecture for self-supervised representation learning on images,Aravind Srinivas;Pieter Abbeel,aravind@cs.berkeley.edu;pabbeel@cs.berkeley.edu,1;1;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,0,0,yes,9/25/19,University of California Berkeley;University of California Berkeley,5;5,13;13,5,9/25/19,0,0,0,0,0,0,-1;-1,-1;-1,-1;-1,0;0,m;m
3831,ICLR,2020,Learning from Positive and Unlabeled Data  with Adversarial Training,Wenpeng Hu;Ran Le;Bing Liu;Feng Ji;Haiqing Chen;Dongyan Zhao;Jinwen Ma;Rui Yan,wenpeng.hu@pku.edu.cn;leran.pku@gmail.com;dcsliub@pku.edu.cn;zhongxiu.jf@alibaba-inc.com;zhaody@pku.edu.cn;jinwen.ma@pku.edu.cn;rui.yan@pku.edu.cn,3;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,Peking University;;Peking University;Alibaba Group;Peking University;Peking University;Peking University,22;-1;22;-1;22;22;22,24;-1;24;-1;24;24;24,5;4,9/25/19,0,0,0,0,0,0,143;6;298;72;6;9;2034;212,19;8;86;26;6;13;192;35,7;2;10;5;2;2;21;4,16;1;15;3;0;0;125;27,m;m
3832,ICLR,2020,PNAT: Non-autoregressive Transformer by Position Learning,Yu Bao;Hao Zhou;Jiangtao Feng;Mingxuan Wang;Shujian Huang;Jiajun Chen;Lei Li,baoy@smail.nju.edu.cn;zhouhao.nlp@bytedance.com;fengjiangtao@bytedance.com;wangmingxuan.89@bytedance.com;huangsj@nju.edu.cn;chenjj@nju.edu.cn;lilei.02@bytedance.com,3;3;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,2,9,0,yes,9/25/19,Zhejiang University;Bytedance;Bytedance;Bytedance;Zhejiang University;Zhejiang University;Bytedance,56;-1;-1;-1;56;56;-1,107;-1;-1;-1;107;107;-1,3,9/25/19,3,2,2,0,0,2,10;174;4;30;900;4;448,2;73;2;12;80;11;172,2;8;1;3;12;1;11,2;26;2;6;85;2;11,m;m
3833,ICLR,2020,Learning Boolean Circuits with Neural Networks,Eran Malach;Shai Shalev-Shwartz,eran.malach@mail.huji.ac.il;shais@cs.huji.ac.il,6;3;6,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Hebrew University of Jerusalem;Hebrew University of Jerusalem,67;67,216;216,,9/25/19,0,0,0,0,0,0,267;14101,12;168,5;48,41;1829,u;m
3834,ICLR,2020,Stagnant zone segmentation with U-net,Selam Waktola;Laurent Babout;Krzysztof Grudzien,selam.waktola@gmail.com,1;1;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:N/A:I made a quick assessment of this paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,1,0,yes,9/25/19,Lodz university of technology,-1,-1,2,9/25/19,0,0,0,0,0,0,9;737;0,8;67;1,2;15;0,0;24;0,m;m
3835,ICLR,2020,Model-based Saliency for the Detection of Adversarial Examples,Lisa Schut;Yarin Gal,lisaschut94@gmail.com;yarin.gal@cs.ox.ac.uk,6;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,University of Oxford;University of Oxford,50;50,1;1,4,9/25/19,0,0,0,0,0,0,0;6605,3;91,0;20,0;987,f;m
3836,ICLR,2020,Learning Temporal Coherence via Self-Supervision for GAN-based Video Generation,Mengyu Chu;You Xie;Jonas Mayer;Laura Leal-Taixé;Nils Thürey,mengyu.chu@tum.de;you.xie@tum.de;jonas.a.mayer@tum.de;leal.taixe@tum.de;nils.thuerey@tum.de,3;8;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,9,0,yes,9/25/19,Technical University Munich;Technical University Munich;Technical University Munich;Technical University Munich;Technical University Munich,53;53;53;53;53,43;43;43;43;43,5;4,11/23/18,2,2,0,0,0,0,159;142;15;2867;2623,9;12;13;73;122,5;6;2;21;32,7;4;1;525;189,f;m
3837,ICLR,2020,R-TRANSFORMER: RECURRENT NEURAL NETWORK ENHANCED TRANSFORMER,Zhiwei Wang;Yao Ma;Zitao Liu;Jiliang Tang,wangzh65@msu.edu;mayao4@msu.edu;liuzitao@100tal.com;tangjili@msu.edu,3;3;6,I have published in this field for several years.:I carefully checked the experiments.:N/A:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,0,0,yes,9/25/19,SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;100tal;SUN YAT-SEN UNIVERSITY,481;481;-1;481,299;299;-1;299,,7/12/19,10,6,4,0,0,2,801;539;35;8957,163;73;24;211,15;13;4;47,27;24;4;704,m;m
3838,ICLR,2020,A closer look at network resolution for efficient network design,Taojiannan Yang;Sijie Zhu;Yan Shen;Mi Zhang;Andrew Willis;Chen Chen,tyang30@uncc.edu;szhu3@uncc.edu;yanshen6@msu.edu;mizhang@msu.edu;arwillis@uncc.edu;chen.chen@uncc.edu,3;6;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,10,0,yes,9/25/19,"University of North Carolina, Charlotte;University of North Carolina, Charlotte;SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;University of North Carolina, Charlotte;University of North Carolina, Charlotte",73;73;481;481;73;73,1397;1397;299;299;1397;1397,6;2,9/25/19,2,1,1,0,0,0,3;61;696;8;64;229,6;25;187;10;11;87,1;5;16;2;4;8,0;3;23;1;6;6,m;m
3839,ICLR,2020,Learning in Confusion: Batch Active Learning with Noisy Oracle,Gaurav Gupta;Anit Kumar Sahu;Wan-Yi Lin,ggaurav@usc.edu;anit.sahu@gmail.com;wan-yi.lin@us.bosch.com,1;1;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,4,0,yes,9/25/19,University of Southern California;;Bosch,31;-1;-1,62;-1;-1,,9/25/19,0,0,0,0,0,0,204;488;11,48;39;12,4;10;2,15;56;0,m;f
3840,ICLR,2020,On Predictive Information Sub-optimality of RNNs,Zhe Dong;Deniz Oktay;Ben Poole;Alexander A. Alemi,zhedong@google.com;doktay@princeton.edu;pooleb@google.com;alemi@google.com,3;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Google;Princeton University;Google;Google,-1;31;-1;-1,-1;6;-1;-1,,9/25/19,0,0,0,0,0,0,0;13;4294;1320,14;9;41;53,0;2;19;14,0;0;701;186,m;m
3841,ICLR,2020,Match prediction from group comparison data using neural networks,Sunghyun Kim;Minje jang;Changho Suh,koishkim@gmail.com;jmj427@lunit.io;chsuh@kaist.ac.kr,3;1;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,4,0,yes,9/25/19,;Lunit Inc.;Korea Advanced Institute of Science and Technology,-1;-1;481,-1;-1;110,,9/25/19,0,0,0,0,0,0,208;39;341,32;8;18,3;2;3,27;2;56,m;m
3842,ICLR,2020,Defending Against Adversarial Examples by Regularized Deep Embedding,Yao Li;Martin Renqiang Min;Wenchao Yu;Cho-Jui Hsieh;Thomas Lee;Erik Kruus,yaoli@ucdavis.edu;renqiang@nec-labs.com;yuwenchao@ucla.edu;chohsieh@cs.ucla.edu;tcmlee@ucdavis.edu;kruus@nec-labs.com,3;1;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,2,11,0,yes,9/25/19,"University of California, Davis;NEC-Labs;University of California, Los Angeles;University of California, Los Angeles;University of California, Davis;NEC-Labs",79;-1;20;20;79;-1,55;-1;17;17;55;-1,4,9/25/19,0,0,0,0,0,0,82;897;418;292;1013;299,39;58;37;21;99;19,6;15;11;4;19;7,3;105;31;55;94;28,f;m
3843,ICLR,2020,Feature Partitioning for Efficient Multi-Task Architectures,Alejandro Newell;Lu Jiang;Chong Wang;Li-Jia Li;Jia Deng,anewell@cs.princeton.edu;lujiang@google.com;chong.wang@bytedance.com;lijiali@cs.stanford.edu;jiadeng@princeton.edu,6;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,3,0,yes,9/25/19,Princeton University;Google;Bytedance;Stanford University;Princeton University,31;-1;-1;4;31,6;-1;-1;4;6,,8/12/19,2,0,1,0,0,0,2018;2107;112;24552;95,6;49;47;71;26,3;26;4;32;5,401;250;13;4401;1,m;m
3844,ICLR,2020,Learnable Group Transform For Time-Series,Romain Cosentino;Behnaam Aazhang,rc57@rice.edu;aaz@rice.edu,8;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,9,0,yes,9/25/19,Rice University;Rice University,84;84,105;105,,9/25/19,0,0,0,0,0,0,22;16023,12;331,3;43,1;837,u;m
3845,ICLR,2020,SesameBERT: Attention for Anywhere,Ta-Chun Su;Hsiang-Chih Cheng,gene11117@gmail.com;musicmilif@gmail.com,3;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,;,-1;-1,-1;-1,8,9/25/19,1,0,0,0,0,0,1;1,2;1,1;1,0;0,m;m
3846,ICLR,2020,Towards A Unified Min-Max Framework for Adversarial Exploration and Robustness,Jingkang Wang;Tianyun Zhang;Sijia Liu;Pin-Yu Chen;Jiacen Xu;Makan Fardad;Bo Li,wangjksjtu@gmail.com;tzhan120@syr.edu;sijia.liu@ibm.com;pin-yu.chen@ibm.com;coldstudy@sjtu.edu.cn;makan@syr.edu;lxbosky@gmail.com,3;8;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0,yes,9/25/19,University of Toronto;Syracuse University;International Business Machines;International Business Machines;Shanghai Jiao Tong University;Syracuse University;University of California Berkeley,18;233;-1;-1;53;233;5,18;292;-1;-1;157;292;13,4,9/25/19,3,1,0,0,0,0,600;222;298;194;10;595;235,53;25;52;44;8;23;147,14;7;11;6;2;8;9,17;30;24;22;0;55;25,m;f
3847,ICLR,2020,Towards Scalable Imitation Learning for Multi-Agent Systems with Graph Neural Networks,Siyu Zhou;Chaitanya Rajasekhar;Mariano J. Phielipp;Heni Ben Amor,siyu.zhou.ac@gmail.com;crajase1@asu.edu;mariano.j.phielipp@intel.com;hbenamor@asu.edu,3;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I made a quick assessment of this paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Arizona State University;Arizona State University;Intel;Arizona State University,95;95;-1;95,155;155;-1;155,6;10,9/25/19,0,0,0,0,0,0,96;0;37;1363,19;1;17;113,5;0;3;20,8;0;2;66,m;m
3848,ICLR,2020,Dual-module Inference for Efficient Recurrent Neural Networks,Liu Liu;Lei Deng;Shuangchen Li;Jingwei Zhang;Yihua Yang;Zhenyu Gu;Yufei Ding;Yuan Xie,liu_liu@ucsb.edu;leideng@ucsb.edu;shuangchen.li@alibaba-inc.com;jingwei.zhang@alibaba-inc.com;yihua.yang@alibaba-inc.com;zhenyu.gu@alibaba-inc.com;yufeiding@cs.ucsb.edu;yuanxie@ece.ucsb.edu,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,UC Santa Barbara;UC Santa Barbara;Alibaba Group;Alibaba Group;Alibaba Group;Alibaba Group;UC Santa Barbara;UC Santa Barbara,38;38;-1;-1;-1;-1;38;38,57;57;-1;-1;-1;-1;57;57,,9/25/19,0,0,0,0,0,0,14;2438;1650;287;-1;100;14;125,25;153;53;62;-1;36;12;40,2;21;15;9;-1;6;2;4,1;121;209;14;0;1;0;3,f;m
3849,ICLR,2020,On Federated Learning of Deep Networks from Non-IID Data: Parameter Divergence and the Effects of Hyperparametric Methods,Heejae Kim;Taewoo Kim;Chan-Hyun Youn,kim881019@kaist.ac.kr;taewoo_kim@kaist.ac.kr;chyoun@kaist.ac.kr,3;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,1,15,0,yes,9/25/19,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,481;481;481,110;110;110,,9/25/19,0,0,0,0,0,0,152;299;84,30;81;35,7;8;4,5;4;6,m;m
3850,ICLR,2020,DeepXML: Scalable & Accurate Deep Extreme Classification for Matching User Queries to Advertiser Bid Phrases,Kunal Dahiya;Anshul Mittal;Deepak Saini;Kushal Dave;Himanshu Jain;Sumeet Agarwal;Manik Varma,kunalsdahiya@gmail.com;anshulmittal71@gmail.com;desaini@microsoft.com;kudave@microsoft.com;himanshu.j689@gmail.com;sumeet@iitd.ac.in;manik@microsoft.com,6;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Indian Institute of Technology Delhi;Indian Institute of Technology Delhi;Microsoft;Microsoft;;Indian Institute of Technology Delhi;Microsoft,118;118;-1;-1;-1;118;-1,441;441;-1;-1;-1;441;-1,3,9/25/19,0,0,0,0,0,0,46;312;52;165;3185;38;5575,3;52;27;16;67;15;56,2;8;6;7;19;3;24,3;18;3;9;646;2;838,m;m
3851,ICLR,2020,Few-Shot Regression via Learning Sparsifying Basis Functions,Yi Loo;Yiluan Guo;Ngai-Man Cheung,loo_yi@sutd.edu.sg;guoyl1990@outlook.com;ngaiman_cheung@sutd.edu.sg,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,1,3,0,yes,9/25/19,Singapore University of Technology and Design;Singapore University of Technology and Design;Singapore University of Technology and Design,481;481;481,1397;1397;1397,6,9/25/19,0,0,0,0,0,0,50;70;2751,14;6;185,3;3;29,1;3;187,u;m
3852,ICLR,2020,Address2vec: Generating vector embeddings for blockchain analytics,Ali Hussein;Samiiha Nalwooga,ali.hussein@ronininstitute.org;nsamiiha@gmail.com,1;1;1,I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0,yes,9/25/19,Ronin Institute;,-1;-1,-1;-1,10,9/25/19,0,0,0,0,0,0,486;0,58;2,9;0,34;0,m;f
3853,ICLR,2020,Energy-Aware Neural Architecture Optimization with Fast Splitting Steepest Descent,Dilin Wang;Meng Li;Lemeng Wu;Vikas Chandra;Qiang Liu,dilin@cs.utexas.edu;meng.li@fb.com;lmwu@cs.utexas.edu;vchandra@fb.com;lqiang@cs.utexas.edu,3;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,5,0,yes,9/25/19,"University of Texas, Austin;Facebook;University of Texas, Austin;Facebook;University of Texas, Austin",22;-1;22;-1;22,38;-1;38;-1;38,,9/25/19,2,0,0,0,0,0,637;4529;18;1416;60,25;760;11;24;70,11;27;3;9;3,118;334;1;134;9,m;m
3854,ICLR,2020,Equilibrium Propagation with Continual Weight Updates,Maxence Ernoult;Julie Grollier;Damien Querlioz;Yoshua Bengio;Benjamin Scellier,maxence.ernoult@u-psud.fr;julie.grollier@cnrs-thales.fr;damien.querlioz@u-psud.fr;yoshua.bengio@mila.quebec;benjamin.scellier@umontreal.ca,3;3;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,UPSud/INRIA University Paris-Saclay;;UPSud/INRIA University Paris-Saclay;University of Montreal;University of Montreal,481;-1;481;128;128,227;-1;227;85;85,1,9/25/19,2,1,2,0,0,0,128;4651;2958;208566;292,13;198;139;807;13,4;37;27;147;7,1;127;127;24297;27,m;m
3855,ICLR,2020,IsoNN: Isomorphic Neural Network for Graph Representation Learning and Classification,Lin Meng;Jiawei Zhang,lin@ifmlab.org;jiawei@ifmlab.org,6;3;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,4,0,yes,9/25/19,SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY,481;481,299;299,3;2;10,7/22/19,5,4,2,1,0,0,1680;62,175;44,22;5,90;12,u;u
3856,ICLR,2020,LDMGAN: Reducing Mode Collapse in GANs with Latent Distribution Matching,Zhiwen Zuo;Lei Zhao;Huiming Zhang;Qihang Mo;Haibo Chen;Zhizhong Wang;AiLin Li;Lihong Qiu;Wei Xing;Dongming Lu,zzwcs@zju.edu.cn;cszhl@zju.edh.cn;qinglanwuji@zju.edu.cn;moqihang@zju.edu.cn;feng123@zju.edu.cn;endywon@zju.edu.cn;11921050@zju.edu.cn;zjusheldon@zju.edu.cn;wxing@zju.edu.cn;ldm@zju.edu.cn,1;3;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,Zhejiang University;;Zhejiang University;Zhejiang University;Zhejiang University;Zhejiang University;Zhejiang University;Zhejiang University;Zhejiang University;Zhejiang University,56;-1;56;56;56;56;56;56;56;56,107;-1;107;107;107;107;107;107;107;107,5;4,9/25/19,0,0,0,0,0,0,69;68;366;0;10;-1;2;34;25;12,37;22;51;5;15;-1;2;15;44;20,4;4;10;0;1;-1;1;4;3;2,3;5;12;0;2;0;0;1;0;0,m;m
3857,ICLR,2020,Reinforcement Learning with Chromatic Networks,Xingyou Song;Krzysztof Choromanski;Jack Parker-Holder;Yunhao Tang;Wenbo Gao;Aldo Pacchiano;Tamas Sarlos;Deepali Jain;Yuxiang Yang,xingyousong@google.com;kchoro@google.com;jh3764@columbia.edu;yt2541@columbia.edu;wg2279@columbia.edu;pacchiano@berkeley.edu;stamas@google.com;jaindeepali@google.com;yxyang@google.com,3;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,5,0,yes,9/25/19,Google;Google;Columbia University;Columbia University;Columbia University;University of California Berkeley;Google;Google;Google,-1;-1;15;15;15;5;-1;-1;-1,-1;-1;16;16;16;13;-1;-1;-1,,7/10/19,0,0,0,0,0,0,29;901;31;95;90;99;2328;69;36,15;92;15;29;23;40;39;32;30,3;15;3;7;7;6;17;5;3,4;79;2;4;12;8;271;2;2,m;m
3858,ICLR,2020,Generalized Natural Language Grounded Navigation via Environment-agnostic Multitask Learning,Xin Wang;Vihan Jain;Eugene Ie;William Wang;Zornitsa Kozareva;Sujith Ravi,xwang@cs.ucsb.edu;vihanjain@google.com;eugeneie@google.com;william@cs.ucsb.edu;kozareva@google.com;sravi@google.com,6;3;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,6,0,yes,9/25/19,UC Santa Barbara;Google;Google;UC Santa Barbara;Google;Google,38;-1;-1;38;-1;-1,57;-1;-1;57;-1;-1,3,9/25/19,0,0,0,0,0,0,17;1025;542;10;1992;30,49;16;30;23;87;23,2;7;11;2;23;2,1;134;60;3;204;2,m;m
3859,ICLR,2020,Mincut Pooling in Graph Neural Networks,Filippo Maria Bianchi;Daniele Grattarola;Cesare Alippi,fibi@norceresearch.no;grattd@usi.ch;alippc@usi.ch,3;8;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,5,0,yes,9/25/19,NORCE the Norwegian Research Center;Università della Svizzera Italiana;Università della Svizzera Italiana,-1;143;143,-1;341;341,10,9/25/19,8,2,4,0,0,1,664;73;4195,63;11;304,14;5;33,32;12;211,m;m
3860,ICLR,2020,Off-Policy Actor-Critic with Shared Experience Replay,Simon Schmitt;Matteo Hessel;Karen Simonyan,suschmitt@google.com;mtthss@google.com;simonyan@google.com,6;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,1,yes,9/25/19,Google;Google;Google,-1;-1;-1,-1;-1;-1,,9/25/19,8,4,4,0,0,0,580;2317;61410,42;27;95,12;13;40,31;371;10836,m;m
3861,ICLR,2020,Striving for Simplicity in Off-Policy Deep Reinforcement Learning,Rishabh Agarwal;Dale Schuurmans;Mohammad Norouzi,rishabhagarwal@google.com;schuurmans@google.com;mnorouzi@google.com,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0,yes,9/25/19,Google;Google;Google,-1;-1;-1,-1;-1;-1,,7/10/19,26,14,12,5,0,7,68;6191;8136,6;247;126,3;41;32,17;638;1021,m;m
3862,ICLR,2020,Scheduling the Learning Rate Via Hypergradients: New Insights and a New Algorithm,Michele Donini;Luca Franceschi;Orchid Majumder;Massimiliano Pontil;Paolo Frasconi,mikko108382892@gmail.com;luca.franceschi@iit.it;orchid@amazon.com;massimiliano.pontil@gmail.com;paolo.frasconi@unifi.it,6;1,I have read many papers in this area.:I carefully checked the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,Amazon;Istituto Italiano di Tecnologia;Amazon;;University of Florence,-1;481;-1;-1;481,-1;1397;-1;-1;378,8,9/25/19,0,0,0,0,0,0,411;202;11;15209;9249,43;22;7;213;232,9;5;1;57;38,48;31;3;1618;624,m;m
3863,ICLR,2020,Improving Federated Learning Personalization via Model Agnostic Meta Learning,Yihan Jiang;Jakub Konečný;Keith Rush;Sreeram Kannan,yihanrogerjiang@gmail.com;konkey@google.com;krush@google.com;ksreeram@uw.edu,1;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,1,4,0,yes,9/25/19,"University of Washington, Seattle;Google;Google;University of Washington, Seattle",6;-1;-1;6,26;-1;-1;26,6,9/25/19,22,8,4,1,0,1,207;2290;21;801,17;37;2;60,6;16;2;16,16;236;3;71,m;m
3864,ICLR,2020,Understanding and Improving Transformer From a Multi-Particle Dynamic System Point of View,Yiping Lu;Zhuohan Li;Di He;Zhiqing Sun;Bin Dong;Tao Qin;Liwei Wang;Tie-Yan Liu,yplu@stanford.edu;zhuohan@berkeley.edu;di_he@pku.edu.cn;zhiqings@andrew.cmu.edu;bindong@math.pku.edu.cn;taoqin@microsoft.com;wanglw@cis.pku.edu.cn;tyliu@microsoft.com,3;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,1,0,yes,9/25/19,Stanford University;University of California Berkeley;Peking University;Carnegie Mellon University;Peking University;Microsoft;Peking University;Microsoft,4;5;22;1;22;-1;22;-1,4;13;24;27;24;-1;24;-1,3,6/6/19,14,3,2,0,0,1,479;100;2719;187;1836;3104;293;13552,11;13;259;14;138;239;36;369,7;7;27;6;20;31;10;51,49;10;110;65;125;162;31;1723,m;m
3865,ICLR,2020,Optimising Neural Network Architectures for Provable Adversarial Robustness,Henry Gouk;Timothy M. Hospedales,hgouk@inf.ed.ac.uk;t.hospedales@ed.ac.uk,3;1;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0,yes,9/25/19,University of Edinburgh;University of Edinburgh,33;33,30;30,4;1,9/25/19,0,0,0,0,0,0,97;384,18;31,3;9,9;34,m;m
3866,ICLR,2020,AN EFFICIENT HOMOTOPY TRAINING ALGORITHM FOR NEURAL NETWORKS,Qipin Chen;Wenrui Hao,qzc18@psu.edu;wxh64@psu.edu,3;3;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,Pennsylvania State University;Pennsylvania State University,41;41,78;78,,9/25/19,0,0,0,0,0,0,60;391,6;48,3;12,2;15,m;m
3867,ICLR,2020,Improving End-to-End Object Tracking Using Relational Reasoning,Fabian B. Fuchs;Adam R. Kosiorek;Li Sun;Oiwi Parker Jones;Ingmar Posner,fabian@robots.ox.ac.uk;adamk@robots.ox.ac.uk;kevin@robots.ox.ac.uk;oiwi.parkerjones@jesus.ox.ac.uk;ingmar@robots.ox.ac.uk,6;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Reject,0,5,1,yes,9/25/19,University of Oxford;University of Oxford;University of Oxford;University of Oxford;University of Oxford,50;50;50;50;50,1;1;1;1;1,,9/25/19,0,0,0,0,0,0,58;349;58;418;2530,10;19;57;17;102,3;9;4;4;28,3;46;3;22;161,m;m
3868,ICLR,2020,Bandlimiting Neural Networks Against Adversarial Attacks,Yuping Lin;Kasra Ahmadi K. A.;Hui Jiang,yuping@eecs.yorku.ca;kasraah@eecs.yorku.ca;hj@cse.yorku.ca,1;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,York University;York University;York University,172;172;172,416;416;416,4,5/30/19,1,1,0,0,0,1,51;1;3615,16;1;261,3;1;30,2;1;443,m;m
3869,ICLR,2020,NoiGAN: NOISE AWARE KNOWLEDGE GRAPH EMBEDDING WITH GAN,Kewei Cheng;Yikai Zhu;Ming Zhang;Yizhou Sun,viviancheng@cs.ucla.edu;zhuyikai.zyk@gmail.com;mzhang_cs@pku.edu.cn;yzsun@cs.ucla.edu,3;3;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,3,0,yes,9/25/19,"University of California, Los Angeles;Peking University;Peking University;University of California, Los Angeles",20;22;22;20,17;24;24;17,5;4;10,9/25/19,0,0,0,0,0,0,13;1;23;6951,4;3;76;187,1;1;2;38,1;0;3;766,f;f
3870,ICLR,2020,Gradient Surgery for Multi-Task Learning,Tianhe Yu;Saurabh Kumar;Abhishek Gupta;Karol Hausman;Sergey Levine;Chelsea Finn,tianheyu@cs.stanford.edu;szk@stanford.edu;abhigupta@berkeley.edu;hausmankarol@gmail.com;svlevine@eecs.berkeley.edu;cbfinn@cs.stanford.edu,3;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,6,0,yes,9/25/19,Stanford University;Stanford University;University of California Berkeley;Google;University of California Berkeley;Stanford University,4;4;5;-1;5;4,4;4;13;-1;13;4,,9/25/19,13,9,3,0,0,1,586;12;1421;872;959;7879,17;12;89;62;34;101,8;1;18;15;9;34,68;1;146;53;103;1060,m;f
3871,ICLR,2020,Filter redistribution templates for iteration-lessconvolutional model reduction,Ramon Izquierdo Cordova;Walterio Mayol Cuevas,ri16164@bristol.ac.uk;walterio.mayol-cuevas@bristol.ac.uk,3;6;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,6,0,yes,9/25/19,University of Bristol;University of Bristol,128;128,87;87,,9/25/19,0,0,0,0,0,0,0;0,1;1,0;0,0;0,m;m
3872,ICLR,2020,Value-Driven Hindsight Modelling,Arthur Guez;Fabio Viola;Theophane Weber;Lars Buesing;Steven Kapturowski;Doina Precup;David Silver;Nicolas Heess,aguez@google.com;fviola@google.com;theophane@google.com;lbuesing@google.com;skapturowski@google.com;doinap@google.com;davidsilver@google.com;heess@google.com,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1,,9/25/19,0,0,0,0,0,0,13436;1814;272;2457;106;10265;43236;11597,32;49;20;54;8;325;159;104,17;16;7;21;4;38;56;37,945;215;35;259;26;1114;5956;1644,m;m
3873,ICLR,2020,JAUNE: Justified And Unified Neural language Evaluation,Hassan Kané;Yusuf Kocyigit;Ali Abdalla;Pelkins Ajanoh;Mohamed Coulibali,hassanmohamed@alum.mit.edu;yusuf.kocyigit@boun.edu.tr;aabdalla@alum.mit.edu;pelkins@alum.mit.edu;mohamed-konoufo.coulibali.1@ulaval.ca,1;1;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published in this field for several years.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A,Reject,0,3,0,yes,9/25/19,Massachusetts Institute of Technology;Bogazici University;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Laval university,2;323;2;2;481,5;672;5;5;272,3,9/25/19,0,0,0,0,0,0,7;1;516;2;5,9;3;61;6;7,1;1;13;1;1,2;0;27;0;1,m;m
3874,ICLR,2020,Extreme Values are Accurate and Robust in Deep Networks,Jianguo Li;Mingjie Sun;Changshui Zhang,jianguo.li@intel.com;sunmj15@gmail.com;zcs@tsinghua.edu.cn,3;3;8,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,Intel;Carnegie Mellon University;Tsinghua University,-1;1;8,-1;27;23,4,9/25/19,0,0,0,0,0,0,90;417;12,38;34;9,6;9;2,7;51;1,m;m
3875,ICLR,2020,"Scaling Laws for the Principled Design, Initialization, and Preconditioning of ReLU Networks",Aaron Defazio;Leon Bottou,aaron.defazio@gmail.com;leon@bottou.org,3;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Facebook;Facebook,-1;-1,-1;-1,1,6/10/19,2,1,0,0,0,0,1331;4561,28;28,9;11,261;348,m;m
3876,ICLR,2020,Overcoming Catastrophic Forgetting via Hessian-free Curvature Estimates,Leonid Butyrev;Georgios Kontes;Christoffer Löffler;Christopher Mutschler,butyreld@iis.fraunhofer.de;georgios.kontes@iis.fraunhofer.de;christoffer.loeffler@iis.fraunhofer.de;christopher.mutschler@iis.fraunhofer.de,3;1;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,1,0,yes,9/25/19,Fraunhofer IIS;Fraunhofer IIS;Fraunhofer IIS;Fraunhofer IIS,-1;-1;-1;-1,-1;-1;-1;-1,,9/25/19,0,0,0,0,0,0,0;161;15;260,2;19;9;42,0;7;3;9,0;3;0;20,m;m
3877,ICLR,2020,Learning Functionally Decomposed Hierarchies for Continuous Navigation Tasks,Lukas Jendele;Sammy Christen;Emre Aksan;Otmar Hilliges,lukas.jendele@gmail.com;sammy.christen@inf.ethz.ch;eaksan@inf.ethz.ch;otmar.hilliges@inf.ethz.ch,6;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0,yes,9/25/19,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10;10,13;13;13;13,8,9/25/19,0,0,0,0,0,0,1;2;160;32,3;4;24;10,1;1;7;4,0;0;18;3,m;m
3878,ICLR,2020,Hindsight Trust Region Policy Optimization,Hanbo Zhang;Site Bai;Xuguang Lan;Nanning Zheng,zhanghanbo163@stu.xjtu.edu.cn;best99317@stu.xjtu.edu.cn;xglan@xjtu.edu.cn;nnzheng@xjtu.edu.cn,3;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Xi'an Jiaotong University;Xi'an Jiaotong University;Xi'an Jiaotong University;Xi'an Jiaotong University,481;481;481;481,555;555;555;555,,7/29/19,0,0,0,0,0,0,252;10;631;622,37;3;93;45,10;2;10;8,16;2;38;21,m;m
3879,ICLR,2020,The Probabilistic Fault Tolerance of Neural Networks in the Continuous Limit,El-Mahdi El-Mhamdi;Rachid Guerraoui;Andrei Kucharavy;Sergei Volodin,elmahdi.elmhamdi@epfl.ch;rachid.guerraoui@epfl.ch;andrei.kucharavy@epfl.ch;sergei.volodin@epfl.ch,1;3;8,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne,481;481;481;481,38;38;38;38,1;8,9/25/19,2,2,0,0,0,1,27;15462;273;9,5;579;8;15,2;53;5;2,3;1508;19;2,m;m
3880,ICLR,2020,Parallel Neural Text-to-Speech,Kainan Peng;Wei Ping;Zhao Song;Kexin Zhao,pengkainan@baidu.com;weiping.thu@gmail.com;zhaosong02@baidu.com;zhaokexin01@baidu.com,3;6;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,1,3,0,yes,9/25/19,Baidu;Baidu;Baidu;Baidu,-1;-1;-1;-1,-1;-1;-1;-1,,9/25/19,13,9,6,0,0,1,569;17;957;22,12;33;124;10,7;2;16;3,73;2;78;3,m;m
3881,ICLR,2020,"Cross-Dimensional Self-Attention for Multivariate, Geo-tagged Time Series Imputation",Jiawei Ma*;Zheng Shou*;Alireza Zareian;Hassan Mansour;Anthony Vetro;Shih-Fu Chang,jm4743@columbia.edu;zs2262@columbia.edu;alireza@cs.columbia.edu;mansour@merl.com;avetro@merl.com;sc250@columbia.edu,6;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Columbia University;Columbia University;Columbia University;Mitsubishi Electric Research Labs;Mitsubishi Electric Research Labs;Columbia University,15;15;15;-1;-1;15,16;16;16;-1;-1;16,3,5/23/19,1,0,0,0,0,0,44;1074;268;1195;5404;31759,10;28;14;100;309;584,5;9;3;17;36;87,1;158;26;98;378;3169,m;m
3882,ICLR,2020,Differential Privacy in Adversarial Learning with Provable Robustness,NhatHai Phan;My T. Thai;Ruoming Jin;Han Hu;Dejing Dou,phan@njit.edu;mythai@cise.ufl.edu;rjin1@kent.edu;hh255@njit.edu;dou@cs.uoregon.edu,3;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,New Jersey Institute of Technology;University of Florida;Bilkent University;New Jersey Institute of Technology;University of Oregon,172;128;323;172;205,564;174;548;564;288,4,3/23/19,4,1,2,0,0,0,358;4701;3331;7711;2318,33;246;151;253;153,9;33;34;39;24,15;357;304;441;182,m;m
3883,ICLR,2020,On Layer Normalization in the Transformer Architecture,Ruibin Xiong;Yunchang Yang;Di He;Kai Zheng;Shuxin Zheng;Huishuai Zhang;Yanyan Lan;Liwei Wang;Tie-Yan Liu,xiongruibin18@mails.ucas.ac.cn;1500010650@pku.edu.cn;dihe@microsoft.com;zhengk92@pku.edu.cn;shuxin.zheng@microsoft.com;huishuai.zhang@microsoft.com;lanyanyan@ict.ac.cn;wanglw@cis.pku.edu.cn;tyliu@microsoft.com,6;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,2,11,0,yes,9/25/19,"Chinese Academy of Sciences;Peking University;Microsoft;Peking University;Microsoft;Microsoft;Institute of Computing Technology, Chinese Academy of Sciences;Peking University;Microsoft",59;22;-1;22;-1;-1;59;22;-1,1397;24;-1;24;-1;-1;1397;24;-1,3,9/25/19,9,7,1,0,0,2,38;68;122;114;566;421;2633;33;13552,8;9;65;54;29;42;120;18;369,3;4;6;6;6;10;25;3;51,3;2;7;5;65;49;438;2;1723,m;m
3884,ICLR,2020,Learning RNNs with Commutative State Transitions,Edo Cohen-Karlik;Amir Globerson,edocoh@gmail.com;amir.globerson@gmail.com,1;1;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0,yes,9/25/19,Tel Aviv University;Tel Aviv University,35;35,188;188,,9/25/19,0,0,0,0,0,0,11;4536,4;124,3;32,1;574,m;m
3885,ICLR,2020,Deep Bayesian Structure Networks,Zhijie Deng;Yucen Luo;Jun Zhu;Bo Zhang,dzj17@mails.tsinghua.edu.cn;luoyc15@mails.tsinghua.edu.cn;dcszj@tsinghua.edu.cn;dcszb@tsinghua.edu.cn,3;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0,yes,9/25/19,Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University,8;8;8;8,23;23;23;23,11;2,9/25/19,0,0,0,0,0,0,81;168;705;10,26;11;96;33,5;4;15;2,10;22;15;0,m;m
3886,ICLR,2020,RPGAN: random paths as a latent space for GAN interpretability,Andrey Voynov;Artem Babenko,an.voynov@gmail.com;artem.babenko@phystech.edu,3;3;8,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Yandex;Moscow Institute of Physics and Technology,-1;481,-1;234,5;4,9/25/19,1,1,0,0,0,0,50;1657,13;29,4;10,6;282,m;m
3887,ICLR,2020,MissDeepCausal: causal inference from incomplete data using deep latent variable models,Julie Josse;Imke Mayer;Jean-Philippe Vert,julie.josse@polytechnique.edu;imke.mayer@polytechnique.edu;jpvert@google.com,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Ecole polytechnique;Ecole polytechnique;Google,481;481;-1,93;93;-1,5,9/25/19,1,0,1,0,0,0,3838;13;9682,96;7;215,18;2;51,319;0;940,f;m
3888,ICLR,2020,Active Learning Graph Neural Networks via Node Feature Propagation,Yuexin Wu;Yichong Xu;Aarti Singh;Artur Dubrawski;Yiming Yang,yuexinw@andrew.cmu.edu;yichongx@cs.cmu.edu;aarti@cs.cmu.edu;awd@cs.cmu.edu;yiming@cs.cmu.edu,3;1;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,2,4,0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1;1,27;27;27;27;27,1;10,9/25/19,6,3,3,0,0,2,762;592;343;638;21623,51;25;60;149;273,11;8;9;13;49,97;44;23;29;2592,m;f
3889,ICLR,2020,DUAL ADVERSARIAL MODEL FOR GENERATING 3D POINT CLOUD,Yuhang Zhang;Zhenwei Miao;Tiebin Mi;Robert Caiming Qiu,hang_universe@sjtu.edu.cn;zhenwei.mzw@alibaba-inc.com;mitiebin@sjtu.edu.cn;rcqiu@sjtu.edu.cn,1;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Shanghai Jiao Tong University;Alibaba Group;Shanghai Jiao Tong University;Shanghai Jiao Tong University,53;-1;53;53,157;-1;157;157,5;4,9/25/19,0,0,0,0,0,0,145;160;73;173,36;40;18;18,6;8;4;5,6;7;9;17,m;m
3890,ICLR,2020,LabelFool: A Trick in the Label Space,Yujia Liu;Tingting Jiang;Ming Jiang,yujia_liu@pku.edu.cn;ttjiang@pku.edu.cn;ming-jiang@pku.edu.cn,3;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,10,0,yes,9/25/19,Peking University;Peking University;Peking University,22;22;22,24;24;24,4,9/25/19,0,0,0,0,0,0,3;11;178,13;14;23,1;2;2,1;0;20,f;m
3891,ICLR,2020,Unsupervised Generative 3D Shape Learning from Natural Images,Attila Szabo;Givi Meishvili;Paolo Favaro,attila.szabo@inf.unibe.ch;givi.meishvili@inf.unibe.ch;paolo.favaro@inf.unibe.ch,3;3;8,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,2,5,0,yes,9/25/19,University of Bern;University of Bern;University of Bern,390;390;390,113;113;113,5;4,9/25/19,2,1,1,1,0,1,37;24;392,13;3;17,2;2;8,5;7;29,m;m
3892,ICLR,2020,Prestopping: How Does Early Stopping Help Generalization Against Label Noise?,Hwanjun Song;Minseok Kim;Dongmin Park;Jae-Gil Lee,songhwanjun@kaist.ac.kr;minseokkim@kaist.ac.kr;dongminpark@kaist.ac.kr;jaegil@kaist.ac.kr,3;3;6;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,2,9,0,yes,9/25/19,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,481;481;481;481,110;110;110;110,8,9/25/19,2,2,0,0,0,0,42;48;379;34,11;46;40;27,3;3;12;2,9;1;14;2,m;m
3893,ICLR,2020,Deep Reinforcement Learning with Implicit Human Feedback,Duo Xu;Mohit Agarwal;Raghupathy Sivakumar;Faramarz Fekri,dxu3016@gatech.edu;me.agmohit@gatech.edu;siva@ece.gatech.edu;faramarz.fekri@ece.gatech.edu,3;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,1,0,yes,9/25/19,Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology,13;13;13;13,38;38;38;38,,9/25/19,1,0,0,0,0,0,114;689;467;3202,16;46;59;244,3;6;11;30,4;79;20;205,m;m
3894,ICLR,2020,"Manifold Modeling in Embedded Space: A Perspective for Interpreting Deep Image Prior""""",Tatsuya Yokota;Hidekata Hontani;Qibin Zhao;Andrzej Cichocki,t.yokota@nitech.ac.jp;hontani@nitech.ac.jp;qibin.zhao@riken.jp;a.cichocki@riken.jp,6;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,10,0,yes,9/25/19,Meiji University;Meiji University;RIKEN;RIKEN,481;481;-1;-1,332;332;-1;-1,2,8/8/19,2,1,0,0,0,0,272;275;2312;23457,26;93;106;842,4;8;23;72,21;12;180;1977,m;m
3895,ICLR,2020,Self-Imitation Learning via Trajectory-Conditioned Policy for Hard-Exploration Tasks,Yijie Guo;Jongwook Choi;Marcin Moczulski;Samy Bengio;Mohammad Norouzi;Honglak Lee,guoyijie@umich.edu;jwook@umich.edu;moczulski@google.com;bengio@google.com;mnorouzi@google.com;honglak@google.com,6;1;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,5,0,yes,9/25/19,University of Michigan;University of Michigan;Google;Google;Google;Google,8;8;-1;-1;-1;-1,21;21;-1;-1;-1;-1,,7/24/19,0,0,0,0,0,0,644;18;449;26795;8136;24514,30;24;19;332;126;166,11;2;7;67;32;62,101;0;41;3497;1021;2837,f;m
3896,ICLR,2020,Improving Semantic Parsing with Neural Generator-Reranker Architecture,Huseyin A. Inan;Gaurav Singh Tomar;Huapu Pan,hinan1@stanford.edu;gtomar@google.com;huapupan@google.com,3;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,5,0,yes,9/25/19,Stanford University;Google;Google,4;-1;-1,4;-1;-1,3,9/25/19,1,0,0,0,0,0,115;98;669,20;9;58,6;5;13,8;11;26,m;m
3897,ICLR,2020,Learning to Make Generalizable and Diverse Predictions for Retrosynthesis,Benson Chen;Tianxiao Shen;Tommi S. Jaakkola;Regina Barzilay,bensonc@mit.edu;tianxiao@mit.edu;tommi@csail.mit.edu;regina@csail.mit.edu,6;6;1,I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2,5;5;5;5,5,9/25/19,4,0,1,0,0,0,16;340;22299;12076,5;13;293;234,2;3;69;56,0;97;2326;1215,m;f
3898,ICLR,2020,Benefit of Interpolation in Nearest Neighbor Algorithms,Yue Xing;Qifan Song;Guang Cheng,xing49@purdue.edu;qfsong@purdue.edu;chengg@purdue.edu,6;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Purdue University;Purdue University;Purdue University,27;27;27,88;88;88,,9/25/19,1,1,0,0,0,0,4;271;101,14;32;43,1;10;5,0;36;7,f;m
3899,ICLR,2020,Learning to Reach Goals Without Reinforcement Learning,Dibya Ghosh;Abhishek Gupta;Justin Fu;Ashwin Reddy;Coline Devin;Benjamin Eysenbach;Sergey Levine,dibya.ghosh@berkeley.edu;abhigupta@berkeley.edu;justinjfu@eecs.berkeley.edu;adreddy@berkeley.edu;coline@berkeley.edu;beysenba@cs.cmu.edu;svlevine@eecs.berkeley.edu,6;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;Carnegie Mellon University;University of California Berkeley,5;5;5;5;5;1;5,13;13;13;13;13;27;13,,9/25/19,6,1,4,0,0,0,100;1421;559;3;601;357;68,12;89;20;2;19;16;31,4;18;10;1;10;6;4,3;146;83;0;27;54;4,m;m
3900,ICLR,2020,Refining the variational posterior through iterative optimization,Marton Havasi;Jasper Snoek;Dustin Tran;Jonathan Gordon;José Miguel Hernández-Lobato,mh740@cam.ac.uk;jsnoek@google.com;trandustin@google.com;jg801@cam.ac.uk;jmh233@cam.ac.uk,6;3;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0,yes,9/25/19,University of Cambridge;Google;Google;University of Cambridge;University of Cambridge,71;-1;-1;71;71,3;-1;-1;3;3,11;1,9/25/19,0,0,0,0,0,0,39;4985;1809;293;3824,7;61;50;22;114,3;18;20;8;28,10;512;197;22;420,m;m
3901,ICLR,2020,Off-policy Bandits with Deficient Support,Noveen Sachdeva;Yi Su;Thorsten Joachims,ernoveen@gmail.com;ys756@cornell.edu;tj@cs.cornell.edu,6;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,International Institute of Information Technology Hyderabad;Cornell University;Cornell University,205;7;7,713;19;19,,9/25/19,1,0,1,0,0,0,28;470;41995,7;63;232,3;8;67,3;23;5767,m;m
3902,ICLR,2020,iSparse: Output Informed Sparsification of Neural Networks,Yash Garg;K. Selcuk Candan,ygarg@asu.edu;candan@asu.edu,1;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Arizona State University;Arizona State University,95;95,155;155,,9/25/19,0,0,0,0,0,0,23;3443,14;328,3;29,0;238,m;m
3903,ICLR,2020,Towards understanding the true loss surface of deep neural networks using random matrix theory and iterative spectral methods,Diego Granziol;Timur Garipov;Dmitry Vetrov;Stefan Zohren;Stephen Roberts;Andrew Gordon Wilson,diego@robots.ox.ac.uk;timgaripov@gmail.com;vetrovd@yandex.ru;zohren@robots.ox.ac.uk;sjrob@robots.ox.ac.uk;andrewgw@cims.nyu.edu,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,University of Oxford;Massachusetts Institute of Technology;Higher School of Economics;University of Oxford;University of Oxford;New York University,50;2;481;50;50;25,1;5;251;1;1;29,8,9/25/19,6,5,1,0,0,0,64;445;2097;105;74;2814,24;27;124;24;47;102,5;7;16;6;4;27,1;80;285;9;4;340,m;m
3904,ICLR,2020,PROTOTYPE-ASSISTED ADVERSARIAL LEARNING FOR UNSUPERVISED DOMAIN ADAPTATION,Dapeng Hu;Jian Liang*;Qibin Hou;Hanshu Yan;Jiashi Feng,dapeng.hu@u.nus.edu;liangjian92@gmail.com;andrewhoux@gmail.com;hanshu.yan@u.nus.edu;elefjia@nus.edu.sg,3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.,Reject,0,5,0,yes,9/25/19,National University of Singapore;National University of Singapore;National University of Singapore;National University of Singapore;National University of Singapore,16;16;16;16;16,25;25;25;25;25,4;2;8,9/25/19,0,0,0,0,0,0,387;107;57;12;9533,48;95;17;9;332,8;6;3;1;52,27;5;3;0;1232,m;m
3905,ICLR,2020,Likelihood Contribution based Multi-scale Architecture for Generative Flows,Hari Prasanna Das;Pieter Abbeel;Costas J. Spanos,hpdas@eecs.berkeley.edu;pabbeel@cs.berkeley.edu;spanos@eecs.berkeley.edu,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,10,0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,13;13;13,5,9/25/19,0,0,0,0,0,0,98;37294;4043,23;438;326,7;94;33,4;4481;174,m;m
3906,ICLR,2020,Understanding Top-k Sparsification in Distributed Deep Learning,Shaohuai Shi;Xiaowen Chu;Ka Chun Cheung;Simon See,csshshi@comp.hkbu.edu.hk;chxw@comp.hkbu.edu.hk;chcheung@nvidia.com;ssee@nvidia.com,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Boston University;Boston University;NVIDIA;NVIDIA,67;67;-1;-1,61;61;-1;-1,1,9/25/19,5,4,1,1,0,1,512;238;619;708,29;39;76;112,8;7;14;15,35;30;81;45,m;m
3907,ICLR,2020,AN ATTENTION-BASED DEEP NET FOR LEARNING TO RANK,Diego Klabjan;Baiyang Wang,d-klabjan@northwestern.edu;baiyang@u.northwestern.edu,1;1;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,Northwestern University;Northwestern University,44;44,22;22,,2/20/17,9,3,4,0,11,0,2780;90,219;7,27;3,200;3,m;m
3908,ICLR,2020,Modeling question asking using neural program generation,Ziyun Wang;Brenden M. Lake,ziyunw@nyu.edu;brenden@nyu.edu,6;1;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,5,0,yes,9/25/19,New York University;New York University,25;25,29;29,,7/23/19,2,1,1,0,0,0,77;3062,29;47,4;16,8;297,m;m
3909,ICLR,2020,Acutum: When Generalization Meets Adaptability,Xunpeng Huang;Zhengyang Liu;Zhe Wang;Yue Yu;Lei Li,huangxunpeng@bytedance.com;liuzhengyang.lozycs@bytedance.com;wang.10982@osu.edu;yuyue.elaine@bytedance.com;lilei.02@bytedance.com,3;1;6,I have published one or two papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Bytedance;Bytedance;Ohio State University;Bytedance;Bytedance,-1;-1;77;-1;-1,-1;-1;373;-1;-1,9;8,9/25/19,1,0,0,0,0,0,28;58;231;5;157,9;20;102;21;71,3;4;8;1;5,2;2;30;0;13,u;m
3910,ICLR,2020,Pseudo-Labeling and Confirmation Bias in Deep Semi-Supervised Learning,Eric Arazo;Diego Ortego;Paul Albert;Noel E. O'Connor;Kevin McGuinness,eric.arazo@insight-centre.org;diego.ortego@insight-centre.org;paul.albert@insight-centre.org;noel.oconnor@dcu.ie;kevin.mcguinness@dcu.ie,3;8;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:N/A:N/A,Reject,0,5,0,yes,9/25/19,Insight Centre for Data Analytics;Insight Centre for Data Analytics;Insight Centre for Data Analytics;Dublin City University;Dublin City University,-1;-1;-1;481;481,-1;-1;-1;601;601,,8/8/19,16,7,5,0,0,1,14;123;172;6332;1441,4;14;22;520;83,1;7;5;36;16,0;15;12;390;150,m;m
3911,ICLR,2020,Revisiting the Generalization of Adaptive Gradient Methods,Naman Agarwal;Rohan Anil;Elad Hazan;Tomer Koren;Cyril Zhang,namanagarwal@google.com;rohananil@google.com;ehazan@cs.princeton.edu;tkoren@google.com;cyril.zhang@princeton.edu,3;3;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Google;Google;Princeton University;Google;Princeton University,-1;-1;31;-1;31,-1;-1;6;-1;6,8,9/25/19,0,0,0,0,0,0,620;4;12184;851;198,32;4;149;57;18,12;1;44;17;6,91;1;2025;163;20,m;m
3912,ICLR,2020,ConQUR: Mitigating Delusional Bias in Deep Q-Learning,DiJia-Andy Su;Jayden Ooi;Tyler Lu;Dale Schuurmans;Craig Boutilier‎,andy.2008.su@gmail.com;jayden@alum.mit.edu;tyler.lu@gmail.com;schuurmans@google.com;cboutilier@google.com,3;3;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,;Massachusetts Institute of Technology;Google;Google;Google,-1;2;-1;-1;-1,-1;5;-1;-1;-1,,9/25/19,0,0,0,0,0,0,352;3;1288;193;14037,15;4;40;37;276,5;1;18;8;55,14;0;140;27;1714,m;m
3913,ICLR,2020,Weight-space symmetry in neural network loss landscapes revisited,Berfin Simsek;Johanni Brea;Bernd Illing;Wulfram Gerstner,berfin.simsek@epfl.ch;johanni.brea@epfl.ch;bernd.illing@epfl.ch;wulfram.gerstner@epfl.ch,3;6;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne,481;481;481;481,38;38;38;38,1,9/25/19,0,0,0,0,0,0,6;328;90;17037,4;26;7;373,2;9;3;63,0;30;7;1719,f;m
3914,ICLR,2020,Collaborative Filtering With A Synthetic Feedback Loop,Wenlin Wang;Hongteng Xu;Ruiyi Zhang;Wenqi Wang;Lawrence Carin,wlwang616@gmail.com;hongtengxu313@gmail.com;ryzhang@cs.duke.edu;wenqiwang@fb.com,6;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,0,0,yes,9/25/19,Duke University;Duke University;Duke University;Facebook,47;47;47;-1,20;20;20;-1,,9/25/19,0,0,0,0,0,0,194;1018;20;7;19642,45;68;17;17;819,7;18;3;1;66,21;86;1;0;1988,m;m
3915,ICLR,2020,Empowering Graph Representation Learning with Paired Training and Graph Co-Attention,Andreea Deac;Yu-Hsiang Huang;Petar Velickovic;Pietro Lio;Jian Tang,deacandr@mila.quebec;huang.yu-hsiang@courrier.uqam.ca;petar.velickovic@cst.cam.ac.uk;pl219@cam.ac.uk;jian.tang@hec.ca,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,University of Montreal;université du Québec à Montreal;University of Cambridge;University of Cambridge;HEC Montreal,128;128;71;71;128,85;85;3;3;85,10,9/25/19,0,0,0,0,0,0,21;160;1620;767;276,6;26;32;96;74,2;7;9;8;8,3;7;419;81;20,f;m
3916,ICLR,2020,Global Concavity and Optimization in a Class of Dynamic Discrete Choice Models,Yiding Feng;Ekaterina Khmelnitskaya;Denis Nekipelov,yidingfeng2021@u.northwestern.edu;eak5rf@virginia.edu;denis@virginia.edu,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Northwestern University;University of Virginia;University of Virginia,44;59;59,22;107;107,,9/25/19,0,0,0,0,0,0,25;35;959,13;12;76,3;2;15,0;1;124,m;m
3917,ICLR,2020,Training Provably Robust Models by Polyhedral Envelope Regularization,Chen Liu;Mathieu Salzmann;Sabine Süsstrunk,chen.liu@epfl.ch;mathieu.salzmann@epfl.ch;sabine.susstrunk@epfl.ch,3;8;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,6,0,yes,9/25/19,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne,481;481;481,38;38;38,4;1,9/25/19,1,1,0,0,0,0,268;5661;85,88;198;9,8;42;4,13;623;6,m;f
3918,ICLR,2020,Unifying Graph Convolutional Networks as Matrix Factorization,Zhaocheng Liu;Qiang Liu;Haoli Zhang;Jun Zhu,zhaocheng.liu@realai.ai;qiang.liu@realai.ai;haoli.zhang@realai.ai;dcszj@mail.tsinghua.edu.cn,1;6;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,7,0,yes,9/25/19,RealAI;RealAI;RealAI;Tsinghua University,-1;-1;-1;8,-1;-1;-1;23,10,9/25/19,0,0,0,0,0,0,540;60;2048;705,31;70;228;96,12;3;20;15,18;9;122;15,u;m
3919,ICLR,2020,Adversarial Inductive Transfer Learning with input and output space adaptation,Hossein Sharifi-Noghabi;Shuman Peng;Olga Zolotareva;Colin C. Collins;Martin Ester,hsharifi@sfu.ca;shumanp@sfu.ca;ozolotareva@techfak.uni-bielefeld.de;ccollins@prostatecentre.com;ester@sfu.ca,3;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Simon Fraser University;Simon Fraser University;Bielefeld University;Prostatecentre;Simon Fraser University,64;64;323;-1;64,272;272;166;-1;272,4;6,9/25/19,1,0,0,0,0,0,26;0;16;5163;24468,6;2;7;128;237,3;0;2;36;48,0;0;0;243;3601,m;m
3920,ICLR,2020,Leveraging Simple Model Predictions for Enhancing its Performance,Amit Dhurandhar;Karthikeyan Shanmugam;Ronny Luss,adhuran@us.ibm.com;karthikeyan.shanmugam2@ibm.com;rluss@us.ibm.com,6;6;6;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,1,5,0,yes,9/25/19,International Business Machines;International Business Machines;International Business Machines,-1;-1;-1,-1;-1;-1,,9/25/19,0,0,0,0,0,0,431;2229;560,64;66;33,10;18;11,25;262;54,m;m
3921,ICLR,2020,Learning to Transfer via Modelling Multi-level Task Dependency,Haonan Wang;Zhenbang Wu;Ziniu Hu;Yizhou Sun,haonan3@illinois.edu;zw12@illinois.edu;bull@cs.ucla.edu;yzsun@cs.ucla.edu,1;3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,4,0,yes,9/25/19,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;University of California, Los Angeles;University of California, Los Angeles",3;3;20;20,48;48;17;17,,9/25/19,0,0,0,0,0,0,54;0;132;6951,15;1;24;187,4;0;6;38,3;0;16;766,m;f
3922,ICLR,2020,Disentangling Style and Content in Anime Illustrations,Sitao Xiang;Hao Li,sitaoxia@usc.edu;hao@hao-li.com,3;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,6,0,yes,9/25/19,University of Southern California;Hao-li,31;-1,62;-1,5;4,5/26/19,3,2,0,0,9,0,93;546,8;77,3;14,8;46,u;u
3923,ICLR,2020,Targeted sampling of enlarged neighborhood via Monte Carlo tree search for TSP,Zhang-Hua Fu;Kai-Bin Qiu;Meng Qiu;Hongyuan Zha,fuzhanghua@cuhk.edu.cn;20150008030@m.scnu.edu.cn;qiumeng.sz@gmail.com;zhahy@cuhk.edu.cn,1;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.,Reject,0,0,0,yes,9/25/19,Tsinghua University;Australian National University;The Chinese University of Hong Kong;Tsinghua University,8;108;59;8,23;50;35;23,,9/25/19,2,1,1,0,0,0,51;92;288;14480,9;42;22;408,4;4;7;62,0;0;5;1225,u;m
3924,ICLR,2020,S2VG: Soft Stochastic Value Gradient method,Xiaoyu Tan;Chao Qu;Junwu Xiong;James Zhang,xiaoyu_tan@u.nus.edu;chaoqu.technion@gmail.com;junwu.xjw@antfin.com;james.z@antfin.com,1;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0,yes,9/25/19,National University of Singapore;;Antfin;Antfin,16;-1;-1;-1,25;-1;-1;-1,,9/25/19,0,0,0,0,0,0,-1;-1;-1;-1,-1;-1;-1;-1,-1;-1;-1;-1,0;0;0;0,u;u
3925,ICLR,2020,Learning to Defense by Learning to Attack,Zhehui Chen;Haoming Jiang;Yuyang Shi;Bo Dai;Tuo Zhao,zhchen@gatech.edu;jianghm@gatech.edu;yyshi@gatech.edu;bodai@google.com;tourzhao@gatech.edu,6;3;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,5,0,yes,9/25/19,Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Google;Georgia Institute of Technology,13;13;13;-1;13,38;38;38;-1;38,5;4,11/3/18,8,6,5,1,4,3,45;257;-1;418;2458,14;30;-1;58;109,4;6;-1;8;19,5;39;0;64;205,m;m
3926,ICLR,2020,Model Architecture Controls Gradient Descent Dynamics: A Combinatorial Path-Based Formula,Xin Zhou;Newsha Ardalani,chow459@gmail.com;newsha@baidu.com,3;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,5,0,yes,9/25/19,;Baidu,-1;-1,-1;-1,9,9/25/19,0,0,0,0,0,0,138;216,76;15,6;5,7;24,u;f
3927,ICLR,2020,Natural- to formal-language generation using Tensor Product Representations,Kezhen Chen;Qiuyuan Huang;Hamid Palangi;Paul Smolensky;Kenneth D. Forbus;Jianfeng Gao,kezhenchen2021@u.northwestern.edu;qihua@microsoft.com;hpalangi@microsoft.com;paul.smolensky@gmail.com;forbus@northwestern.edu;jfgao@microsoft.com,8;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:N/A:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,8,0,yes,9/25/19,Northwestern University;Microsoft;Microsoft;Microsoft;Northwestern University;Microsoft,44;-1;-1;-1;44;-1,22;-1;-1;-1;22;-1,,9/25/19,1,0,1,0,0,0,152;926;739;10024;11059;18900,16;52;34;206;429;353,8;14;11;40;50;61,4;132;54;933;718;2683,u;m
3928,ICLR,2020,Unsupervised Out-of-Distribution Detection with Batch Normalization,Jiaming Song;Yang Song;Stefano Ermon,jiaming.tsong@gmail.com;yangsong@cs.stanford.edu;ermon@cs.stanford.edu,1;6;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,Stanford University;Stanford University;Stanford University,4;4;4,4;4;4,5,9/25/19,5,3,1,1,0,1,811;122;4975,44;30;203,14;6;31,127;11;664,m;m
3929,ICLR,2020,Distribution Matching Prototypical Network for Unsupervised Domain Adaptation,Lei Zhu;Wei Wang;Mei Hui Zhang;Beng Chin Ooi;Chang Yao,e0203764@u.nus.edu;wangwei@comp.nus.edu.sg;meihui_zhang@bit.edu.cn;ooibc@comp.nus.edu.sg;yaochang@zjuici.com,3;3;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,10,0,yes,9/25/19,National University of Singapore;National University of Singapore;BIT;National University of Singapore;Zhejiang University,16;16;-1;16;56,25;25;-1;25;107,,9/25/19,0,0,0,0,0,0,72;106;206;13684;172,66;137;15;416;54,4;5;6;63;6,9;10;9;1161;11,m;m
3930,ICLR,2020,Deep Hierarchical-Hyperspherical Learning (DH^2L),Youngsung Kim;Jae-Joon Han,yskim.ee@gmail.com;jae-joon.han@samsung.com,3;6;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Samsung;Samsung,-1;-1,-1;-1,8,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1,-1;-1,-1;-1,0;0,m;m
3931,ICLR,2020,Neural Markov Logic Networks,Giuseppe Marra;Ondřej Kuželka,g.marra@unifi.it;kuzelo1@gmail.com,6;1;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,University of Florence;Czech Technical University in Prague,481;323,378;956,1,5/31/19,2,1,0,0,0,0,443;254,89;75,11;10,16;16,m;m
3932,ICLR,2020,Deep Randomized Least Squares Value Iteration,Guy Adam;Tom Zahavy;Oron Anschel;Nahum Shimkin,guyadam3@gmail.com;tomzahavy@gmail.com;oronanschel@gmail.com;shimkin@ee.technion.ac.il,1;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,6,0,yes,9/25/19,Technion;Technion;;Technion,26;26;-1;26,412;412;-1;412,,9/25/19,0,0,0,0,0,0,35;5;145;3492,6;8;7;130,2;2;4;27,4;2;23;349,m;m
3933,ICLR,2020,Safe Policy Learning for Continuous Control,Yinlam Chow;Ofir Nachum;Aleksandra Faust;Edgar Duenez-Guzman;Mohammad Ghavamzadeh,yinlamchow@google.com;ofirnachum@google.com;sandrafaust@google.com;duenez@google.com;mgh@fb.com,6;8;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,6,2,yes,9/25/19,Google;Google;Google;Google;Facebook,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,,9/25/19,0,0,0,0,0,0,763;1060;392;428;538,50;42;39;27;36,15;15;9;13;11,92;157;10;25;60,m;m
3934,ICLR,2020,Unsupervised Distillation of Syntactic Information from Contextualized Word Representations,Shauli Ravfogel;Yanai Elazar;Jacob Goldberger;Yoav Goldberg,shauli.ravfogel@gmail.com;yanaiela@gmail.com;jacob.goldberger@biu.ac.il;yogo@cs.biu.ac.il,6;8;6;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,12,0,yes,9/25/19,Bar Ilan University;Bar Ilan University;Bar Ilan University;Bar Ilan University,95;95;95;95,513;513;513;513,3;6,9/25/19,0,0,0,0,0,0,34;133;5263;1504,6;12;180;40,3;5;35;10,3;10;554;189,m;m
3935,ICLR,2020,Discriminator Based Corpus Generation for General Code Synthesis,Alexander Wild;Barry Porter,a.wild3@lancaster.ac.uk;b.f.porter@lancaster.ac.uk,1;1;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:N/A:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,2,1,0,yes,9/25/19,Lancaster University;Lancaster University,233;233,140;140,,9/25/19,0,0,0,0,0,0,-1;-1,-1;-1,-1;-1,0;0,m;m
3936,ICLR,2020,Situating Sentence Embedders with Nearest Neighbor Overlap,Lucy H. Lin;Noah A. Smith,lucylin@cs.washington.edu;nasmith@cs.washington.edu,3;1;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,2,0,yes,9/25/19,University of Washington;University of Washington,6;6,26;26,3,9/24/19,1,1,0,0,0,0,30;17890,6;291,2;64,0;2088,f;m
3937,ICLR,2020,Lattice Representation Learning,Luis A Lastras,lastrasl@us.ibm.com,3;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,International Business Machines,-1,-1,1,9/25/19,0,0,0,0,0,0,1255,24,8,179,m;u
3938,ICLR,2020,Towards Understanding the Transferability of Deep Representations,Hong Liu;Mingsheng Long;Jianmin Wang;Michael I. Jordan,h-l17@mails.tsinghua.edu.cn;mingsheng@tsinghua.edu.cn;jimwang@tsinghua.edu.cn;jordan@cs.berkeley.edu,3;3;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Tsinghua University;Tsinghua University;Tsinghua University;University of California Berkeley,8;8;8;5,23;23;23;13,8,9/25/19,2,2,1,0,0,1,167;6668;3142;118202,49;85;43;847,7;36;9;140,12;1320;516;16063,m;m
3939,ICLR,2020,Improved Training of Certifiably Robust Models,Chen Zhu;Renkun Ni;Ping-yeh Chiang;Hengduo Li;Furong Huang;Tom Goldstein,chenzhu@cs.umd.edu;rn9zm@cs.umd.edu;pingyeh.chiang@gmail.com;hdli@cs.umd.edu;furongh@cs.umd.edu;tomg@cs.umd.edu,6;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,6,0,yes,9/25/19,"University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park",12;12;12;12;12;12,91;91;91;91;91;91,4,9/25/19,0,0,0,0,0,0,47;59;24;97;1752;228,21;8;7;9;130;61,3;4;3;5;20;10,10;9;5;11;188;26,m;m
3940,ICLR,2020,Learning from Label Proportions with Consistency Regularization,Kuen-Han Tsai;Hsuan-Tien Lin,r06922066@csie.ntu.edu.tw;htlin@csie.ntu.edu.tw,3;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,National Taiwan University;National Taiwan University,86;86,120;120,,9/25/19,1,1,1,0,0,0,1;3098,2;104,1;22,0;270,u;m
3941,ICLR,2020,Learning Compact Embedding Layers via Differentiable Product Quantization,Ting Chen;Lala Li;Yizhou Sun,iamtingchen@gmail.com;lala@google.com;yzsun@cs.ucla.edu,3;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,"Google;Google;University of California, Los Angeles",-1;-1;20,-1;-1;17,,9/25/19,0,0,0,0,0,0,488;7;6951,42;10;187,10;2;38,89;0;766,m;f
3942,ICLR,2020,{COMPANYNAME}11K: An Unsupervised Representation Learning Dataset for Arrhythmia Subtype Discovery,Shawn Tan;Guillaume Androz;Ahmad Chamseddine;Pierre Fecteau;Aaron Courville;Yoshua Bengio;Joseph Paul Cohen,shawn@wtf.sg;guillaume.androz@icentia.com;doctor.ahmad89@gmail.com;pierre.fecteau@icentia.com;aaron.courville@gmail.com;yoshua.bengio@mila.quebec;joseph@josephpcohen.com,3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,2,0,yes,9/25/19,;Icentia Inc.;Polytechnique Montreal;Icentia;University of Montreal;University of Montreal;University of Montreal,-1;-1;390;-1;128;128;128,-1;-1;1397;-1;85;85;85,,9/25/19,1,0,0,0,0,0,139;154;19;1;62509;208566;505,12;13;21;6;203;807;62,4;4;2;1;65;147;11,33;3;0;0;7971;24297;50,m;m
3943,ICLR,2020,NADS: Neural Architecture Distribution Search for Uncertainty Awareness,Randy Ardywibowo;Shahin Boluki;Xinyu Gong;Zhangyang Wang;Xiaoning Qian,randyardywibowo@tamu.edu;s.boluki@tamu.edu;gong1994@tamu.edu;atlaswang@tamu.edu;xqian@tamu.edu,8;1;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Texas A&M;Texas A&M;Texas A&M;Texas A&M;Texas A&M,44;44;44;44;44,177;177;177;177;177,11,9/25/19,0,0,0,0,0,0,10;95;19;2947;1224,6;18;9;167;170,2;5;3;28;20,0;0;2;383;55,m;m
3944,ICLR,2020,Learning World Graph Decompositions To Accelerate Reinforcement Learning,Wenling Shang;Alex Trott;Stephan Zheng;Caiming Xiong;Richard Socher,w.shang@uva.nl;atrott@salesforce.com;stephan.zheng@salesforce.com;cxiong@salesforce.com;richard@socher.org,6;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0,yes,9/25/19,University of Amsterdam;SalesForce.com;SalesForce.com;SalesForce.com;SalesForce.com,172;-1;-1;-1;-1,62;-1;-1;-1;-1,5;10,9/25/19,0,0,0,0,0,0,502;16;484;6301;53531,18;6;29;156;180,6;2;8;31;49,46;3;36;1045;8917,f;m
3945,ICLR,2020,Group-Transformer: Towards A Lightweight Character-level Language Model,Sungrae Park;Geewook Kim;Junyeop Lee;Junbum Cha;Ji-Hoon Kim Hwalsuk Lee,sungrae.park@navercorp.com;geewook@sys.i.kyoto-u.ac.jp;junyeop.lee@navercorp.com;junbum.cha@navercorp.com;genesis.kim@navercorp.com;hwalsuk.lee@navercorp.com,6;6;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,10,0,yes,9/25/19,NAVER;Meiji University;NAVER;NAVER;NAVER;NAVER,-1;481;-1;-1;-1;-1,-1;332;-1;-1;-1;-1,3,9/25/19,0,0,0,0,0,0,180;56;42;4;-1,19;18;25;4;-1,6;2;1;1;-1,25;11;9;0;0,m;m
3946,ICLR,2020,Learning DNA folding patterns with Recurrent Neural Networks ,Michal Rozenwald;Aleksandra Galitsyna;Ekaterina Khrameeva;Grigory Sapunov;Mikhail S. Gelfand,michal.rozenwald@gmail.com;agalitzina@gmail.com;ekhrameeva@gmail.com;grigory.sapunov@gmail.codelfm;mikhail.gelfand@gmail.com,1;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,;Skolkovo Institute of Science and Technology;;;,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,,9/25/19,0,0,0,0,0,0,1;32;120;1;9917,2;15;14;4;333,1;3;5;1;56,0;1;8;0;775,f;m
3947,ICLR,2020,TSInsight: A local-global attribution framework for interpretability in time-series data,Shoaib Ahmed Siddiqui;Dominique Mercier;Andreas Dengel;Sheraz Ahmed,shoaib_ahmed.siddiqui@dfki.de;dominique.mercier@dfki.de;andreas.dengel@dfki.de;sheraz.ahmed@dfki.de,3;1;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,7,0,yes,9/25/19,German Research Center for AI;German Research Center for AI;German Research Center for AI;German Research Center for AI,-1;-1;-1;-1,-1;-1;-1;-1,4,9/25/19,1,0,0,0,0,0,292;77;333;590,25;51;80;99,7;6;10;13,46;0;23;41,m;m
3948,ICLR,2020,Imagining the Latent Space of a Variational Auto-Encoders,Zezhen Zeng;Jonathon Hare;Adam Prügel-Bennett,zz8n17@ecs.soton.ac.uk;jsh2@ecs.soton.ac.uk;apb@ecs.soton.ac.uk,3;3;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,5,0,yes,9/25/19,University of Southampton;University of Southampton;University of Southampton,172;172;172,122;122;122,5,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1,-1;-1;-1,-1;-1;-1,0;0;0,m;m
3949,ICLR,2020,Quantum Semi-Supervised Kernel Learning,Seyran Saeedi;Aliakbar Panahi;Tom Arodz,saeedis@vcu.edu;panahia@vcu.edu;tarodz@vcu.edu,6;6;6,I do not know much about this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:N/A:N/A:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Virginia Commonwealth University;Virginia Commonwealth University;Virginia Commonwealth University,266;266;266,1397;1397;1397,,9/25/19,0,0,0,0,0,0,11;3;82,6;7;8,2;1;3,0;0;1,f;m
3950,ICLR,2020,Interpretable Network Structure for Modeling Contextual Dependency,Xindian Ma;Peng Zhang;Xiaoliu Mao;Yehua Zhang;Nan Duan;Yuexian Hou;Ming Zhou.,xindianma@tju.edu.cn;pzhang@tju.edu.cn;xiaoliumao@tju.edu.cn;yehua_zhang@tju.edu.cn;nanduan@microsoft.com;yxhou@tju.edu.cn;mingzhou@microsoft.com,3;1;3,I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,3,0,yes,9/25/19,Zhejiang University;Zhejiang University;Zhejiang University;Zhejiang University;Microsoft;Zhejiang University;Microsoft,56;56;56;56;-1;56;-1,107;107;107;107;-1;107;-1,3;1,9/25/19,0,0,0,0,0,0,16;159;0;43;232;114;1370,7;75;2;11;48;10;205,1;6;0;2;7;3;19,1;14;0;1;22;20;52,u;m
3951,ICLR,2020,Frequency Principle: Fourier Analysis Sheds Light on Deep Neural Networks,Zhi-Qin John Xu;Yaoyu Zhang;Tao Luo;Yanyang Xiao;Zheng Ma,xuzhiqin@sjtu.edu.cn;yaoyu@ias.edu;luo196@purdue.edu;xyy82148@gmail.com;ma531@purdue.edu,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,"Shanghai Jiao Tong University;Institue for Advanced Study, Princeton;Purdue University;;Purdue University",53;-1;27;-1;27,157;-1;88;-1;88,8,1/19/19,34,16,7,3,4,2,49;184;6627;115;3167,8;18;465;16;354,3;8;38;5;27,2;10;474;8;163,m;m
3952,ICLR,2020,S-Flow GAN,Miron Yakov;Coscas Yona,yakov.miron@gmail.com;yona.coscas@gmail.com,1;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:N/A:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,0,0,yes,9/25/19,;,-1;-1,-1;-1,5;4;10,5/21/19,2,1,1,0,0,0,186;1,6;1,3;1,22;0,m;m
3953,ICLR,2020,Independence-aware Advantage Estimation,Pushi Zhang;Li Zhao;Guoqing Liu;Jiang Bian;Minglie Huang;Tao Qin;Tie-Yan Liu,zpschang@gmail.com;lizo@microsoft.com;lgq1001@mail.ustc.edu.cn;jiang.bian@microsoft.com;aihuang@mails.tsinghua.edu.cn;taoqin@microsoft.com;tie-yan.liu@microsoft.com,6;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,7,0,yes,9/25/19,Tsinghua University;Microsoft;University of Science and Technology of China;Microsoft;Tsinghua University;Microsoft;Microsoft,8;-1;481;-1;8;-1;-1,23;-1;80;-1;23;-1;-1,,9/25/19,1,1,1,0,0,0,1;356;38;2;2;15;13552,2;125;12;8;2;15;369,1;9;2;1;1;2;51,0;37;4;0;0;2;1723,u;m
3954,ICLR,2020,Role of two learning rates in convergence of model-agnostic meta-learning,Shiro Takagi;Yoshihiro Nagano;Yuki Yoshida;Masato Okada,takagi@mns.k.u-tokyo.ac.jp;nagano@mns.k.u-tokyo.ac.jp;yoshida@mns.k.u-tokyo.ac.jp;okada@edu.k.u-tokyo.ac.jp,3;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0,yes,9/25/19,The University of Tokyo;The University of Tokyo;The University of Tokyo;The University of Tokyo,56;56;56;56,36;36;36;36,1;6,9/25/19,0,0,0,0,0,0,9;21;120;172,7;9;51;96,1;2;6;7,0;7;5;8,m;m
3955,ICLR,2020,Neural Subgraph Isomorphism Counting,Xin Liu;Haojie Pan;Mutian He;Yangqiu Song;Xin Jiang,xliucr@cse.ust.hk;hpanad@cse.ust.hk;mhear@cse.ust.hk;yqsong@cse.ust.hk;jiang.xin@huawei.com,6;3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,5,0,yes,9/25/19,The Hong Kong University of Science and Technology;The Hong Kong University of Science and Technology;The Hong Kong University of Science and Technology;The Hong Kong University of Science and Technology;Huawei Technologies Ltd.,39;39;39;39;-1,47;47;47;47;-1,10,9/25/19,1,1,0,0,0,0,4;7;19;4186;338,14;3;4;150;99,1;1;2;34;8,0;2;2;334;17,m;m
3956,ICLR,2020,Semantic Hierarchy Emerges in the Deep Generative Representations for Scene Synthesis,Ceyuan Yang;Yujun Shen;Bolei Zhou,limbo0066@gmail.com;sy116@ie.cuhk.edu.hk;bzhou@ie.cuhk.edu.hk,6;3;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,The Chinese University of Hong Kong;The Chinese University of Hong Kong;The Chinese University of Hong Kong,59;59;59,35;35;35,5;4,9/25/19,10,7,1,0,0,0,384;1232;9930,10;66;93,5;18;33,28;96;1707,m;m
3957,ICLR,2020,Decoupling Adaptation from Modeling with Meta-Optimizers for Meta Learning,Sébastien M.R. Arnold;Shariq Iqbal;Fei Sha,arnolds@usc.edu;shariqiqbal2810@gmail.com;fsha@google.com,3;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0,yes,9/25/19,University of Southern California;University of Southern California;Google,31;31;-1,62;62;-1,6,9/25/19,2,1,0,1,0,0,3;93;116,4;12;21,1;4;5,0;15;20,m;f
3958,ICLR,2020,Black-box Adversarial Attacks with Bayesian Optimization,Satya Narayan Shukla;Anit Kumar Sahu;Devin Willmott;J. Zico Kolter,snshukla@cs.umass.edu;anit.sahu@gmail.com;devin.willmott@uky.edu;zkolter@cs.cmu.edu,6;3;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,"University of Massachusetts, Amherst;;University of Kentucky;Carnegie Mellon University",28;-1;233;1,209;-1;490;27,4;11,9/25/19,3,2,2,0,0,0,55;488;44;7776,22;39;10;107,4;10;3;35,4;56;5;1073,m;m
3959,ICLR,2020,Adversarial Imitation Attack,Mingyi Zhou;Jing Wu;Yipeng Liu;Xiaolin Huang;Shuaicheng Liu;Liaqat Ali;Xiang Zhang;Ce Zhu,zhoumingyi@std.uestc.edu.cn;wujing@std.uestc.edu.cn;yipengliu@uestc.edu.cn;xiaolinhuang@sjtu.edu.cn;liushuaicheng@uestc.edu.cn;engr_liaqat183@yahoo.com;uestchero@uestc.edu.cn;eczhu@uestc.edu.cn,6;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,University of Electronic Science and Technology of China;University of Electronic Science and Technology of China;University of Electronic Science and Technology of China;Shanghai Jiao Tong University;University of Electronic Science and Technology of China;;University of Electronic Science and Technology of China;University of Electronic Science and Technology of China,481;481;481;53;481;-1;481;481,628;628;628;157;628;-1;628;628,5;4,9/25/19,0,0,0,0,0,0,312;46;168;108;978;-1;539;115,40;47;54;39;61;-1;73;60,10;4;6;6;13;-1;8;6,13;1;17;7;95;0;59;11,u;m
3960,ICLR,2020,LOGAN:  Latent Optimisation for Generative Adversarial Networks,Yan Wu;Jeff Donahue;David Balduzzi;Karen Simonyan;Timothy Lillicrap,yanwu@google.com;jeffdonahue@google.com;dbalduzzi@google.com;simonyan@google.com;countzero@google.com,6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,2,2,yes,9/25/19,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,5;4,9/25/19,9,5,3,1,0,3,75;35933;2521;61410;24158,82;56;61;95;74,4;25;21;40;39,5;4756;359;10836;2943,m;m
3961,ICLR,2020,Searching for Stage-wise Neural Graphs In the Limit,Xin Zhou;Dejing Dou;Boyang Li,chow459@gmail.com;doudejing@baidu.com;libo0001@gmail.com,3;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,3,0,yes,9/25/19,;Baidu;National Taiwan University,-1;-1;86,-1;-1;120,10,9/25/19,0,0,0,0,0,0,138;2318;0,76;153;3,6;24;0,7;182;0,m;m
3962,ICLR,2020,Attention Interpretability Across NLP Tasks,Shikhar Vashishth;Shyam Upadhyay;Gaurav Singh Tomar;Manaal Faruqui,shikhar@iisc.ac.in;shyamupa@google.com;gtomar@google.com;mfaruqui@google.com,6;6;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,1,5,0,yes,9/25/19,Indian Institute of Science;Google;Google;Google,95;-1;-1;-1,301;-1;-1;-1,3,9/24/19,15,9,5,0,0,1,169;491;98;2218,19;29;9;52,7;13;5;19,29;53;11;206,m;m
3963,ICLR,2020,LSTOD: Latent Spatial-Temporal Origin-Destination prediction model and its applications in ride-sharing platforms,Fan Zhou;Haibo Zhou;Hongtu Zhu,zhoufan@mail.shufe.edu.cn;zhou@bios.unc.edu;zhuhongtu@didiglobal.com,1;6;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,3,0,yes,9/25/19,"Tsinghua University;University of North Carolina, Chapel Hill;DiDi AI Labs, Didi Chuxing",8;73;-1,23;54;-1,10,9/25/19,0,0,0,0,0,0,-1;-1;-1,-1;-1;-1,-1;-1;-1,0;0;0,m;m
3964,ICLR,2020,Unsupervised Universal Self-Attention Network for Graph Classification,Dai Quoc Nguyen;Tu Dinh Nguyen;Dinh Phung,dai.nguyen@monash.edu;tu.dinh.nguyen@monash.edu;dinh.phung@monash.edu,3;1;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,6,7,0,yes,9/25/19,Monash University;Monash University;Monash University,118;118;118,75;75;75,10,9/25/19,0,0,0,0,0,0,407;745;4744,36;61;301,12;13;31,66;100;413,m;m
3965,ICLR,2020,Efficacy of Pixel-Level OOD Detection for Semantic Segmentation,Matt Angus;Krzysztof Czarnecki;Rick Salay,m2angus@gsd.uwaterloo.ca;rsalay@gsd.uwaterloo.ca;k2czarne@gsd.uwaterloo.ca,3;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,University of Waterloo;University of Waterloo;University of Waterloo,28;28;28,235;235;235,2,9/25/19,1,1,0,0,0,0,21;14034;1028,5;328;86,2;54;18,0;1356;66,m;m
3966,ICLR,2020,CURSOR-BASED ADAPTIVE QUANTIZATION FOR DEEP NEURAL NETWORK,Bapu Li(*);Yanwen Fan(*);Zhiyu Cheng;Yingze Bao (* means equal contribution),baopuli@baidu.com;fanyanwen@baidu.com;zhiyucheng@baidu.com;baoyingze@baidu.com,3;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Baidu;Baidu;Baidu;Baidu,-1;-1;-1;-1,-1;-1;-1;-1,9,9/25/19,0,0,0,0,0,0,0;14;387;3,1;5;58;6,0;2;12;1,0;0;8;0,u;u
3967,ICLR,2020,MMD GAN with Random-Forest Kernels,Tao Huang;Zhen Han;Xu Jia;Hanyuan Hang,tao.huang2018@ruc.edu.cn;handarkholme@ruc.edu.cn;jiayushenyang@gmail.com;hanyuan0725@gmail.com,1;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,"University of Illinois, Urbana-Champaign;University of Illinois, Urbana-Champaign;Huawei Technologies Ltd.;University of Illinois, Urbana-Champaign",3;3;-1;3,48;48;-1;48,5,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1;-1,-1;-1;-1;-1,-1;-1;-1;-1,0;0;0;0,u;u
3968,ICLR,2020,Hyperparameter Tuning and Implicit Regularization in Minibatch SGD,Samuel L Smith;Erich Elsen;Soham De,slsmith@google.com;eriche@google.com;sohamde@google.com,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,1,4,0,yes,9/25/19,Google;Google;Google,-1;-1;-1,-1;-1;-1,,9/25/19,0,0,0,0,0,0,1268;4689;442,26;53;24,11;21;12,111;493;39,m;m
3969,ICLR,2020,Quantum Optical Experiments Modeled by Long Short-Term Memory,Thomas Adler;Manuel Erhard;Mario Krenn;Johannes Brandstetter;Johannes Kofler;Sepp Hochreiter,adler@ml.jku.at;manuel.erhard@univie.ac.at;mario.krenn@univie.ac.at;brandstetter@ml.jku.at;kofler@ml.jku.at;hochreit@ml.jku.at,3;1;1,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,Johannes Kepler University Linz;University of Vienna;University of Vienna;Johannes Kepler University Linz;Johannes Kepler University Linz;Johannes Kepler University Linz,481;205;205;481;481;481,620;134;134;620;620;620,5,9/25/19,2,1,1,0,0,0,512;621;2052;4757;2007;36390,162;29;87;374;62;111,10;10;22;35;18;28,71;1;26;234;63;6668,m;m
3970,ICLR,2020,Variational Hashing-based Collaborative Filtering with Self-Masking,Casper Hansen;Christian Hansen;Jakob Grue Simonsen;Stephen Alstrup;Christina Lioma,c.hansen@di.ku.dk;chrh@di.ku.dk;simonsen@di.ku.dk;s.alstrup@di.ku.dk;c.lioma@di.ku.dk,3;3;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,University of Copenhagen;University of Copenhagen;University of Copenhagen;University of Copenhagen;University of Copenhagen,100;100;100;100;100,101;101;101;101;101,,9/25/19,0,0,0,0,0,0,86;645;889;1426;1526,19;108;101;82;90,6;14;15;20;15,9;21;92;169;154,m;f
3971,ICLR,2020,Customizing Sequence Generation with Multi-Task Dynamical Systems,Alex Bird;Christopher K. I. Williams,abird@turing.ac.uk;ckiw@inf.ed.ac.uk,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Alan Turing Institute;University of Edinburgh,-1;33,-1;30,,9/25/19,0,0,0,0,0,0,1;29637,2;226,1;50,0;4728,m;m
3972,ICLR,2020,Batch Normalization is a Cause of Adversarial Vulnerability,Angus Galloway;Anna Golubeva;Thomas Tanay;Medhat Moussa;Graham W. Taylor,gallowaa@uoguelph.ca;agolubeva@perimeterinstitute.ca;thomas.tanay.13@ucl.ac.uk;mmoussa@uoguelph.ca;gwtaylor@uoguelph.ca,3;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,3,8,0,yes,9/25/19,University of Guelph;Perimeter Institute;University College London;University of Guelph;University of Guelph,266;-1;50;266;266,558;-1;15;558;558,4,5/6/19,16,7,1,3,0,0,85;82;182;147;5971,12;20;12;30;143,5;4;5;7;31,10;1;9;5;508,m;m
3973,ICLR,2020,Enhancing Language Emergence through Empathy,Marie Ossenkopf,mos@vs.uni-kassel.de,1;1;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,1,0,yes,9/25/19,University of Kassel,390,94,3,9/25/19,0,0,0,0,0,0,2,8,1,0,f;u
3974,ICLR,2020,A Data-Efficient Mutual Information Neural Estimator for Statistical Dependency Testing,Xiao Lin;Indranil Sur;Samuel A. Nastase;Uri Hasson;Ajay Divakaran;Mohamed R. Amer,xiao.lin@sri.com;indranil.sur@sri.com;mohamed.rabie.amer@gmail.com;ajay.divakaran@sri.com;snastase@princeton.edu;hasson@princeton.edu,1;6;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,SRI International;SRI International;Robust.AI;SRI International;Princeton University;Princeton University,-1;-1;-1;-1;31;31,-1;-1;-1;-1;6;6,1;6,9/25/19,0,0,0,0,0,0,6;9;241;9514;145;908,42;8;34;186;32;67,1;2;8;49;5;15,0;2;15;766;15;71,m;m
3975,ICLR,2020,SMiRL: Surprise Minimizing RL in Entropic Environments,Glen Berseth;Daniel Geng;Coline Devin;Dinesh Jayaraman;Chelsea Finn;Sergey Levine,gberseth@gmail.com;dangengdg@berkeley.edu;coline.devin@gmail.com;dinesh.jayaraman123@gmail.com;cbfinn@eecs.berkeley.edu;svlevine@eecs.berkeley.edu,6;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,5,0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley;Facebook;University of California Berkeley;University of California Berkeley,5;5;5;-1;5;5,13;13;13;-1;13;13,11,9/25/19,3,2,1,0,0,0,640;4;601;1206;7879;68,45;5;19;42;101;31,11;1;10;16;34;4,21;0;27;128;1060;4,m;m
3976,ICLR,2020,The Role of Embedding Complexity in Domain-invariant Representations,Ching-Yao Chuang;Antonio Torralba;Stefanie Jegelka,cychuang@mit.edu;torralba@mit.edu;stefje@mit.edu,3;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,3,0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2,5;5;5,1;8,9/25/19,0,0,0,0,0,0,146;49145;3398,6;281;115,3;90;28,19;6369;547,m;f
3977,ICLR,2020,Learning to Infer User Interface Attributes from Images,Philippe Schlattner;Pavol Bielik;Martin Vechev,pschlatt@ethz.ch;pavol.bielik@inf.ethz.ch;martin.vechev@inf.ethz.ch,1;3;8,I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0,yes,9/25/19,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10,13;13;13,10,9/25/19,0,0,0,0,0,0,1;518;4246,2;26;153,1;11;36,0;56;467,m;m
3978,ICLR,2020,Semi-supervised Learning by Coaching,Hieu Pham;Quoc V. Le,hyhieu@cmu.edu;qvl@google.com,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,7,0,yes,9/25/19,Carnegie Mellon University;Google,1;-1,27;-1,,9/25/19,1,0,1,0,0,0,60;48901,17;193,3;81,2;6080,m;m
3979,ICLR,2020,Learning Representations in Reinforcement Learning: an Information Bottleneck Approach,Yingjun Pei;Xinwen Hou,peiyingjun4@gmail.com;xwhou@nlpr.ia.ac.cn,3;6;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,"Beijing University of Post and Telecommunication;Institute of automation, Chinese academy of science, Chinese Academy of Sciences",481;59,1397;1397,1,9/25/19,4,0,1,0,0,0,2;740,1;35,1;11,0;63,u;u
3980,ICLR,2020,Evaluating Lossy Compression Rates of Deep Generative Models,Sicong Huang;Alireza Makhzani;Yanshuai Cao;Roger Grosse,huang@cs.toronto.edu;a.makhzani@gmail.com;yanshuai.cao@borealisai.com;rgrosse@cs.toronto.edu,3;8;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,"Department of Computer Science, University of Toronto;;Borealis AI;Department of Computer Science, University of Toronto",18;-1;-1;18,18;-1;-1;18,5;4,9/25/19,2,0,2,0,0,0,30;1458;309;5791,3;13;20;51,2;9;7;28,4;214;35;815,m;m
3981,ICLR,2020,Kronecker Attention Networks,Hongyang Gao;Zhengyang Wang;Shuiwang Ji,hongyang.gao@tamu.edu;zhengyang.wang@tamu.edu;sji@tamu.edu,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,Texas A&M;Texas A&M;Texas A&M,44;44;44,177;177;177,,9/25/19,0,0,0,0,0,0,289;5;8754,21;16;136,8;1;35,39;0;728,m;m
3982,ICLR,2020,Stochastic Neural Physics Predictor,Piotr Tatarczyk;Damian Mrowca;Li Fei-Fei;Daniel L. K. Yamins;Nils Thuerey,piotr.tatarczyk@tum.de;mrowca@stanford.edu;feifeili@cs.stanford.edu;yamins@stanford.edu;nils.thuerey@tum.de,3;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Technical University Munich;Stanford University;Stanford University;Stanford University;Technical University Munich,53;4;4;4;53,43;4;4;4;43,10,9/25/19,0,0,0,0,0,0,0;137;80554;602;2623,1;11;450;42;122,0;5;95;9;32,0;8;11761;36;189,m;m
3983,ICLR,2020,Spectral Nonlocal Block for Neural Network,Lei Zhu;Qi She;Lidan Zhang;Ping guo,lei1.zhu@intel.com;qi.she@intel.com;lidan.zhang@intel.com;ping.guo@intel.com,6;6;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,Intel;Intel;Intel;Intel,-1;-1;-1;-1,-1;-1;-1;-1,2;10,9/25/19,0,0,0,0,0,0,1980;170;0;145,250;28;3;26,24;7;0;5,138;11;0;4,m;u
3984,ICLR,2020,Metagross: Meta Gated Recursive Controller Units for Sequence Modeling,Yi Tay;Yikang Shen;Alvin Chan;Yew Soon Ong,ytay017@e.ntu.edu.sg;yikang.shn@gmail.com;guoweial001@e.ntu.edu.sg;asysong@ntu.edu.sg,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,National Taiwan University;University of Montreal;National Taiwan University;National Taiwan University,86;128;86;86,120;85;120;120,3,9/25/19,0,0,0,0,0,0,1429;6;1592;8125,68;6;133;356,19;2;19;44,166;1;116;482,m;m
3985,ICLR,2020,Diving into Optimization of Topology in Neural Networks,Kun Yuan;Quanquan Li;Yucong Zhou;Jing Shao;Junjie Yan,yuankun@sensetime.com;liquanquan@sensetime.com;zhouyucong@sensetime.com;shaojing@sensetime.com;yanjunjie@sensetime.com,6;6;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,7,0,yes,9/25/19,SenseTime Group Limited;SenseTime Group Limited;SenseTime Group Limited;SenseTime Group Limited;SenseTime Group Limited,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,8,9/25/19,1,0,1,0,0,0,2358;309;110;539;7065,126;25;5;62;166,20;5;2;9;43,195;38;15;23;996,m;m
3986,ICLR,2020,The Usual Suspects? Reassessing Blame for VAE Posterior Collapse,Bin Dai;Ziyu Wang;David Wipf,daib13@mails.tsinghua.edu.cn;wzy196@gmail.com;davidwipf@gmail.com,3;8;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0,yes,9/25/19,Tsinghua University;Tsinghua University;Microsoft,8;8;-1,23;23;-1,5;1,9/25/19,0,0,0,0,0,0,73;3;4776,26;4;101,4;1;30,1;1;610,m;m
3987,ICLR,2020,Generative Restricted Kernel Machines,Arun Pandey;Joachim Schreurs;Johan A.K. Suykens,arun.pandey@esat.kuleuven.be;joachim.schreurs@esat.kuleuven.be;johan.suykens@esat.kuleuven.be,6;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,1,yes,9/25/19,KU Leuven;KU Leuven;KU Leuven,118;118;118,45;45;45,5,6/19/19,3,2,2,0,2,0,23;15;20702,9;10;697,3;3;62,0;0;1879,m;m
3988,ICLR,2020,GNN-FiLM: Graph Neural Networks with Feature-wise Linear Modulation,Marc Brockschmidt,mabrocks@microsoft.com,6;8;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,14,0,yes,9/25/19,Microsoft,-1,-1,10,6/28/19,6,3,4,0,0,2,2529,61,22,332,m;u
3989,ICLR,2020,Which Tasks Should Be Learned Together in Multi-task Learning?,Trevor Standley;Amir R. Zamir;Dawn Chen;Leonidas Guibas;Jitendra Malik;Silvio Savarese,tstand@cs.stanford.edu;zamir@cs.stanford.edu;sdawnchen@gmail.com;guibas@cs.stanford.edu;malik@eecs.berkeley.edu;ssilvio@stanford.edu,6;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Stanford University;Stanford University;;Stanford University;University of California Berkeley;Stanford University,4;4;-1;4;5;4,4;4;-1;4;13;4,2,5/18/19,21,13,2,0,7,0,375;5260;106;46189;70398;16959,9;50;13;700;429;284,5;21;5;98;115;66,45;1129;3;5581;7775;2529,m;m
3990,ICLR,2020,Distillation $\approx$ Early Stopping? Harvesting Dark Knowledge Utilizing Anisotropic Information Retrieval For Overparameterized NN,Bin Dong;Jikai Hou;Yiping Lu;Zhihua Zhang,dongbin@math.pku.edu.cn;houjikai@pku.edu.cn;yplu@stanford.edu;zhzhang@math.pku.edu.cn,3;8;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,10,0,yes,9/25/19,Peking University;Peking University;Stanford University;Peking University,22;22;4;22,24;24;4;24,1;8,9/25/19,6,2,1,0,0,0,1836;16;479;163,138;4;11;45,20;2;7;6,125;1;49;22,m;m
3991,ICLR,2020,How Does Learning Rate Decay Help Modern Neural Networks?,Kaichao You;Mingsheng Long;Jianmin Wang;Michael I. Jordan,youkaichao@gmail.com;mingsheng@tsinghua.edu.cn;jimwang@tsinghua.edu.cn;jordan@cs.berkeley.edu,3;1;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,0,0,yes,9/25/19,Tsinghua University;Tsinghua University;Tsinghua University;University of California Berkeley,8;8;8;5,23;23;23;13,8,9/25/19,5,1,1,0,0,1,69;6668;6412;118202,5;85;267;847,4;36;40;140,16;1320;958;16063,m;m
3992,ICLR,2020,Incorporating Horizontal Connections in Convolution by Spatial Shuffling,Ikki Kishida;Hideki Nakayama,kishida@nlab.ci.i.u-tokyo.ac.jp;nakayama@nlab.ci.i.u-tokyo.ac.jp,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,The University of Tokyo;The University of Tokyo,56;56,36;36,,9/25/19,0,0,0,0,0,0,0;640,2;88,0;15,0;69,m;m
3993,ICLR,2020,Learning Cluster Structured Sparsity by Reweighting,Yulun Jiang;Lei Yu;Haijian Zhang;Zhou Liu,yljblues@whu.edu.cn;ly.wd@whu.edu.cn;haijian.zhang@whu.edu.cn;liuzhou@whu.edu.cn,1;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:N/A:N/A,Reject,0,3,0,yes,9/25/19,Wuhan University;Wuhan University;Wuhan University;Wuhan University,266;266;266;266,354;354;354;354,,9/25/19,0,0,0,0,0,0,0;177;31;172,6;88;18;46,0;8;4;8,0;6;1;5,u;u
3994,ICLR,2020,Continuous Meta-Learning without Tasks,James Harrison;Apoorva Sharma;Chelsea Finn;Marco Pavone,jharrison@stanford.edu;apoorva@stanford.edu;cbfinn@cs.stanford.edu;pavone@stanford.edu,3;6;8,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Stanford University;Stanford University;Stanford University;Stanford University,4;4;4;4,4;4;4;4,11;6;2,9/25/19,8,3,1,0,0,0,342;211;7879;3882,60;41;101;245,5;7;34;33,20;8;1060;234,m;m
3995,ICLR,2020,Generative Adversarial Networks For Data Scarcity Industrial Positron Images With Attention,Mingwei Zhu;Min Zhao;Min Yao;Ruipeng Guo,zhumingwei@nuaa.edu.cn;xymzhao@126.com;ym_nuaa@163.com;rpguo@nuaa.edu.cn,1;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,0,0,yes,9/25/19,Tsinghua University;126;163;Tsinghua University,8;-1;-1;8,23;-1;-1;23,4;6,9/25/19,0,0,0,0,0,0,30;19;147;7,5;22;32;5,2;3;4;1,0;1;13;1,u;u
3996,ICLR,2020,Support-guided Adversarial Imitation Learning,Ruohan Wang;Carlo Ciliberto;Pierluigi Amadori;Yiannis Demiris,r.wang16@ic.ac.uk;c.ciliberto@imperial.ac.uk;p.amadori@imperial.ac.uk;y.demiris@imperial.ac.uk,6;6;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Imperial College London;Imperial College London;Imperial College London;Imperial College London,73;73;73;73,10;10;10;10,4,9/25/19,0,0,0,0,0,0,89;668;72;4046,18;56;13;211,5;16;5;32,7;44;5;379,m;m
3997,ICLR,2020,IS THE LABEL TRUSTFUL: TRAINING BETTER DEEP LEARNING MODEL VIA UNCERTAINTY MINING NET,Yang Sun;Abhishek Kolagunda;Steven Eliuk;Xiaolong Wang,yang.sun1@ibm.com;abhishek.kolagunda@ibm.com;steven.eliuk@ibm.com;visionxiaolong@gmail.com,3;1;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,International Business Machines;International Business Machines;International Business Machines;International Business Machines,-1;-1;-1;-1,-1;-1;-1;-1,5,9/25/19,0,0,0,0,0,0,135;0;24;63,71;2;13;15,6;0;4;2,3;0;0;2,u;m
3998,ICLR,2020,Learning with Social Influence through  Interior Policy Differentiation,Hao Sun;Bo Dai;Jiankai Sun;Zhenghao Peng;Guodong Xu;Dahua Lin;Bolei Zhou,sh018@ie.cuhk.edu.hk;doubledaibo@gmail.com;sunjiankai@sensetime.com;pengzh@ie.cuhk.edu.hk;xg018@ie.cuhk.edu.hk;dhlin@ie.cuhk.edu.hk;bzhou@ie.cuhk.edu.hk,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,1,4,0,yes,9/25/19,The Chinese University of Hong Kong;The Chinese University of Hong Kong;SenseTime Group Limited;The Chinese University of Hong Kong;The Chinese University of Hong Kong;The Chinese University of Hong Kong;The Chinese University of Hong Kong,59;59;-1;59;59;59;59,35;35;-1;35;35;35;35,,9/25/19,0,0,0,0,0,0,190;144;155;5;61;6172;54,22;10;21;6;21;146;4,9;4;7;1;5;37;3,9;11;11;1;2;1077;3,m;m
3999,ICLR,2020,Selfish Emergent Communication,Michael Noukhovitch;Travis LaCroix;Aaron Courville,michael.noukhovitch@umontreal.ca;tlacroix@uci.edu;aaron.courville@gmail.com,3;1;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,23,0,yes,9/25/19,"University of Montreal;University of California, Irvine;University of Montreal",128;35;128,85;96;85,,9/25/19,0,0,0,0,0,0,35;16;62509,4;12;203,2;3;65,7;0;7971,m;m
4000,ICLR,2020,Coordinated Exploration via Intrinsic Rewards for Multi-Agent Reinforcement Learning,Shariq Iqbal;Fei Sha,shariqiqbal2810@gmail.com;fsha@google.com,6;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,3,0,yes,9/25/19,University of Southern California;Google,31;-1,62;-1,,5/28/19,4,4,1,0,0,0,93;9097,12;118,4;41,15;1385,m;m
4001,ICLR,2020,Mutual Exclusivity as a Challenge for Deep Neural Networks,Kanishk Gandhi;Brenden Lake,kanishk.gandhi@nyu.edu;brenden@nyu.edu,6;8;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0,yes,9/25/19,New York University;New York University,25;25,29;29,,6/24/19,12,5,2,0,0,1,17;3062,4;47,3;16,1;297,m;m
4002,ICLR,2020,The Differentiable Cross-Entropy Method,Brandon Amos;Denis Yarats,brandon.amos.cs@gmail.com;denisyarats@cs.nyu.edu,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,1,6,0,yes,9/25/19,Facebook;New York University,-1;25,-1;29,9,9/25/19,5,2,3,1,0,2,1981;1668,40;12,19;7,221;251,m;m
4003,ICLR,2020,DS-VIC: Unsupervised Discovery of Decision States for Transfer in RL,Nirbhay Modhe;Prithvijit Chattopadhyay;Mohit Sharma;Abhishek Das;Devi Parikh;Dhruv Batra;Ramakrishna Vedantam,nirbhaym@gatech.edu;prithvijit3@gatech.edu;sharma.mohit.916@gmail.com;abhshkdz@gatech.edu;parikh@gatech.edu;dbatra@gatech.edu;ramav@fb.com,3;3;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,6,0,yes,9/25/19,Georgia Institute of Technology;Georgia Institute of Technology;;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Facebook,13;13;-1;13;13;13;-1,38;38;-1;38;38;38;-1,1,9/25/19,0,0,0,0,0,0,20;173;34;93;15347;1823;4523,6;14;33;34;185;18;20,2;6;3;4;55;5;14,5;21;5;10;2411;436;841,m;m
4004,ICLR,2020,Exploration Based Language Learning for Text-Based Games,Andrea Madotto;Mahdi Namazifar;Joost Huizinga;Piero Molino;Adrien Ecoffet;Huaixiu Zheng;Alexandros Papangelis;Dian Yu;Chandra Khatri;Gokhan Tur,amadotto@connect.ust.hk;mahdin@uber.com;jhuizinga@uber.com;piero@uber.com;adrienle@uber.com;huaixiu.zheng@uber.com;apapangelis@uber.com;dianyu@ucdavis.edu;chandrak@uber.com;gokhan@uber.com,6;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,5,0,yes,9/25/19,"The Hong Kong University of Science and Technology;Uber;Uber;Uber;Uber;Uber;Uber;University of California, Davis;Uber;Uber",39;-1;-1;-1;-1;-1;-1;79;-1;-1,47;-1;-1;-1;-1;-1;-1;55;-1;-1,3,9/25/19,0,0,0,0,0,0,386;95;242;488;99;728;265;51;281;1218,42;17;13;33;7;41;48;11;29;58,10;4;6;8;2;13;9;3;8;13,45;6;18;60;10;38;12;3;22;98,m;m
4005,ICLR,2020,Representation Learning with Multisets,Vasco Portilheiro,vascop@stanford.edu,3;3;6,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Stanford University,4,4,,9/25/19,0,0,0,0,0,0,0,2,0,0,m;u
4006,ICLR,2020,Deep Gradient Boosting -- Layer-wise Input Normalization of Neural Networks,Erhan Bilal,ebilal@us.ibm.com,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,1,3,0,yes,9/25/19,International Business Machines,-1,-1,8,7/29/19,0,0,0,0,0,0,8,3,1,1,m;u
4007,ICLR,2020,Multi-Dimensional Explanation of Reviews,Diego Antognini;Claudiu Musat;Boi Faltings,diego.antognini@epfl.ch;claudiu.musat@swisscom.com;boi.faltings@epfl.ch,3;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Swiss Federal Institute of Technology Lausanne;Swisscom;Swiss Federal Institute of Technology Lausanne,481;-1;481,38;-1;38,3,9/25/19,1,1,0,0,0,0,8;214;6997,9;39;430,1;6;44,0;30;604,m;m
4008,ICLR,2020,ISBNet: Instance-aware Selective Branching Networks,Shaofeng Cai;Yao Shu;Wei Wang;Gang Chen;Beng Chin Ooi,shaofeng@comp.nus.edu.sg;shuyao@comp.nus.edu.sg;wangwei@comp.nus.edu.sg;cg@zju.edu.cn;ooibc@comp.nus.edu.sg,3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,2,0,yes,9/25/19,National University of Singapore;National University of Singapore;National University of Singapore;Zhejiang University;National University of Singapore,16;16;16;56;16,25;25;25;107;25,,5/13/19,5,2,2,0,3,1,23;228;2429;67;13684,11;95;740;71;416,4;6;24;2;63,3;9;97;4;1161,m;m
4009,ICLR,2020,Inferring Dynamical Systems with Long-Range Dependencies through Line Attractor Regularization,Dominik Schmidt;Georgia Koppe;Max Beutelspacher;Daniel Durstewitz,dominik.schmidt@zi-mannheim.de;georgia.koppe@zi-mannheim.de;max.beutelspacher@mailbox.org;daniel.durstewitz@zi-mannheim.de,3;6;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,ZI Mannheim;ZI Mannheim;;ZI Mannheim,-1;-1;-1;-1,-1;-1;-1;-1,,9/25/19,2,1,0,0,0,0,920;373;1;2044,50;26;1;85,17;10;1;21,55;22;0;125,m;m
4010,ICLR,2020,SoftAdam: Unifying SGD and Adam for better stochastic gradient descent,Abraham J. Fetterman;Christina H. Kim;Joshua Albrecht,abe@sourceress.co;christina@sourceress.co;josh@sourceress.co,3;1;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,1,3,0,yes,9/25/19,;;,-1;-1;-1,-1;-1;-1,3,9/25/19,1,0,0,0,0,0,130;6111;50,27;797;15,7;39;3,4;364;6,m;m
4011,ICLR,2020,BERT-AL: BERT for Arbitrarily Long Document Understanding,Ruixuan Zhang;Zhuoyu Wei;Yu Shi;Yining Chen,903276268@pku.edu.cn;zhuoyu.wei@microsoft.com;yushi@microsoft.com;yining.chen@microsoft.com,3;3;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,Peking University;Microsoft;Microsoft;Microsoft,22;-1;-1;-1,24;-1;-1;-1,3,9/25/19,0,0,0,0,0,0,0;50;812;962,3;7;68;87,0;3;9;15,0;1;31;74,u;u
4012,ICLR,2020,NEURAL EXECUTION ENGINES,Yujun Yan;Kevin Swersky;Danai Koutra;Parthasarathy Ranganathan;Milad Hashemi,yujunyan@umich.edu;kswersky@google.com;dkoutra@umich.edu;parthas@google.com;miladh@google.com,3;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,University of Michigan;Google;University of Michigan;Google;Google,8;-1;8;-1;-1,21;-1;21;-1;-1,10;8,9/25/19,1,0,0,0,0,0,33;5797;2608;152;250,11;52;101;12;22,3;23;24;4;7,1;885;208;23;32,f;m
4013,ICLR,2020,Learning Good Policies By Learning Good Perceptual Models,Yilun Du;Phillip Isola,yilundu@mit.edu;phillipi@mit.edu,1;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2,5;5,,9/25/19,0,0,0,0,0,0,128;12579,27;73,6;27,10;2186,m;m
4014,ICLR,2020,Quaternion Equivariant Capsule Networks for 3D Point Clouds,Yongheng Zhao;Tolga Birdal;Jan Eric Lenssen;Emanuele Menegatti;Leonidas Guibas;Federico Tombari,zhao@dei.unipd.it;tbirdal@stanford.edu;janeric.lenssen@udo.edu;emg@dei.unipd.it;guibas@cs.stanford.edu;tombari@google.com,3;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,Universita' degli studi di Padova;Stanford University;TU Dortmund University;Universita' degli studi di Padova;Stanford University;Google,-1;4;233;-1;4;-1,-1;4;354;-1;4;-1,2,9/25/19,3,1,0,0,0,0,718;368;504;2500;46189;6702,117;36;19;273;700;178,13;10;5;23;98;37,60;40;55;152;5581;953,m;m
4015,ICLR,2020,Informed Temporal Modeling via Logical Specification of Factorial LSTMs,Hongyuan Mei;Guanghui Qin;Minjie Xu;Jason Eisner,hongyuanmei@gmail.com;gqin@jhu.edu;chokkyvista06@gmail.com;jason@cs.jhu.edu,3;3;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,1,0,yes,9/25/19,Johns Hopkins University;Johns Hopkins University;Bloomberg LP;Johns Hopkins University,73;73;-1;73,12;12;-1;12,10;8,9/25/19,0,0,0,0,0,0,528;15;503;6166,22;6;61;204,5;3;13;41,54;0;30;557,m;m
4016,ICLR,2020,Needles in Haystacks: On Classifying Tiny Objects in Large Images,Nick Pawlowski;Suvrat Bhooshan;Nicolas Ballas;Francesco Ciompi;Ben Glocker;Michal Drozdzal,pawlowski.nick@gmail.com;sbh@fb.com;ballasn@fb.com;f.ciompi@gmail.com;b.glocker@imperial.ac.uk;mdrozdzal@fb.com,3;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,3,0,yes,9/25/19,Imperial College London;Facebook;Facebook;Radboud University Medical Center;Imperial College London;Facebook,73;-1;-1;390;73;-1,10;-1;-1;128;10;-1,2;8,8/16/19,3,2,0,0,0,0,491;33;5111;4689;3002;1657,26;6;54;88;53;39,12;3;21;18;15;13,40;1;597;183;342;159,m;m
4017,ICLR,2020,Probabilistic View of Multi-agent Reinforcement Learning: A Unified Approach,Shubham Gupta;Ambedkar Dukkipati,shubhamg@iisc.ac.in;ambedkar@iisc.ac.in,3;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,1,3,0,yes,9/25/19,Indian Institute of Science;Indian Institute of Science,95;95,301;301,10,9/25/19,0,0,0,0,0,0,927;483,129;118,15;12,57;39,m;m
4018,ICLR,2020,Hope For The Best But Prepare For The Worst: Cautious Adaptation In RL Agents,Jesse Zhang;Brian Cheung;Chelsea Finn;Dinesh Jayaraman;Sergey Levine,jessezhang@berkeley.edu;bcheung@berkeley.edu;cbfinn@cs.stanford.edu;dineshjayaraman@berkeley.edu;svlevine@eecs.berkeley.edu,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,3,0,yes,9/25/19,University of California Berkeley;University of California Berkeley;Stanford University;University of California Berkeley;University of California Berkeley,5;5;4;5;5,13;13;4;13;13,,9/25/19,0,0,0,0,0,0,32;381;7879;1206;24893,8;8;101;42;310,2;1;34;16;74,1;32;1060;128;3235,m;m
4019,ICLR,2020,Abstractive Dialog Summarization with Semantic Scaffolds,Lin Yuan;Zhou Yu,yuanlinzju@gmail.com;joyu@ucdavis.edu,1;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,0,0,yes,9/25/19,"Zhejiang University;University of California, Davis",56;79,107;55,,9/25/19,1,1,0,0,0,0,121;553,25;142,5;11,7;29,m;f
4020,ICLR,2020,Relative Pixel Prediction For Autoregressive Image Generation,Wang Ling;Chris Dyer;Lei Yu;Lingpeng Kong;Dani Yogatama;Susannah Young,lingwang@google.com;cdyer@google.com;leiyu@google.com;lingpenk@google.com;dyogatama@google.com;susannahy@google.com,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,0,0,yes,9/25/19,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,,9/25/19,0,0,0,0,0,0,2648;21507;177;1072;3601;0,757;231;88;32;41;2,22;61;8;14;21;0,265;3161;6;110;408;0,m;f
4021,ICLR,2020,Integrative Tensor-based Anomaly Detection System For Satellites,Youjin Shin;Sangyup Lee;Shahroz Tariq;Myeong Shin Lee;OkchulJung;Daewon Chung;Simon Woo,youjin.shin.1@stonybrook.edu;shahroz@g.skku.edu;sangyup.lee@g.skku.edu;mslee@kari.re.kr;ocjung@kari.re.kr;dwchung@kari.re.kr;swoo@g.skku.edu,1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,"State University of New York, Stony Brook;Peking University;Peking University;;;;Peking University",41;22;22;-1;-1;-1;22,304;24;24;-1;-1;-1;24,,9/25/19,0,0,0,0,0,0,138;50;40;4;0;119;288,14;13;13;2;1;28;81,5;3;4;1;0;5;10,11;2;5;1;0;10;13,f;m
4022,ICLR,2020,Gram-Gauss-Newton Method: Learning Overparameterized Neural Networks for Regression Problems,Tianle Cai*;Ruiqi Gao*;Jikai Hou*;Siyu Chen;Dong Wang;Di He;Zhihua Zhang;Liwei Wang,caitianle1998@pku.edu.cn;grq@pku.edu.cn;1600010681@pku.edu.cn;siyuchen@pku.edu.cn;wangdongcis@pku.edu.cn;di_he@pku.edu.cn;zhzhang@math.pku.edu.cn;wanglw@cis.pku.edu.cn,3;3;3;6;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,Peking University;Peking University;Peking University;Peking University;Peking University;Peking University;Peking University;Peking University,22;22;22;22;22;22;22;22,24;24;24;24;24;24;24;24,9,5/28/19,9,3,4,0,0,1,58;224;16;573;349;2719;5364;293,10;30;4;93;100;259;407;36,4;9;2;14;9;27;30;10,5;17;1;29;21;110;534;31,m;m
4023,ICLR,2020,A Greedy Approach to Max-Sliced Wasserstein GANs,András Horváth,horvath.andras@itk.ppke.hu,1;1;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,Pazmany Peter catholic University,481,1397,5;4,9/25/19,-1,-1,-1,-1,-1,-1,-1,-1,-1,0,m
4024,ICLR,2020,Improving Sequential Latent Variable Models with Autoregressive Flows,Joseph Marino;Lei Chen;Jiawei He;Stephan Mandt,jmarino@caltech.edu;lei_chen_4@sfu.ca;jiawei_he_2@sfu.ca;stephan.mandt@gmail.com,6;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,"California Institute of Technology;Simon Fraser University;Simon Fraser University;University of California, Irvine",143;64;64;35,2;272;272;96,,9/25/19,1,1,0,0,0,0,199;164;207;39,165;108;42;13,6;7;8;4,4;13;22;6,m;m
4025,ICLR,2020,Symmetry and Systematicity,Jeff Mitchell;Jeff Bowers,jeff.mitchell@bristol.ac.uk;j.bowers@bristol.ac.uk,1;1;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,9,0,yes,9/25/19,University of Bristol;University of Bristol,128;128,87;87,,9/25/19,0,0,0,0,0,0,27;2568,11;124,3;28,0;250,m;m
4026,ICLR,2020,Towards Interpretable Evaluations: A Case Study of Named Entity Recognition,Jinlan Fu;Pengfei Liu;Xuanjing Huang,fujl16@fudan.edu.cn;pfliu14@fudan.edu.cn;xjhuang@fudan.edu.cn,3;3;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,7,0,yes,9/25/19,Fudan University;Fudan University;Fudan University,79;79;79,109;109;109,3,9/25/19,0,0,0,0,0,0,61;7;54,11;12;23,3;2;3,11;1;5,f;f
4027,ICLR,2020,Bootstrapping the Expressivity with Model-based Planning,Kefan Dong;Yuping Luo;Tengyu Ma,dkf16@mails.tsinghua.edu.cn;yupingl@cs.princeton.edu;tengyuma@cs.stanford.edu,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Tsinghua University;Princeton University;Stanford University,8;31;4,23;6;4,,9/25/19,2,1,1,0,0,0,32;1420;3939,5;58;88,3;16;32,8;132;508,m;m
4028,ICLR,2020,Score and Lyrics-Free Singing Voice Generation,Jen-Yu Liu;Yu-Hua Chen;Yin-Cheng Yeh;Yi-Hsuan Yang,ciauaishere@gmail.com;r08946011@ntu.edu.tw;deanyeh.ee01@g2.nctu.edu.tw;affige@gmail.com,3;3;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,7,0,yes,9/25/19,;National Taiwan University;National Chiao Tung University;Academia Sinica,-1;86;143;-1,-1;120;564;-1,5;4,9/25/19,5,2,3,1,0,0,496;1023;10;18,45;97;6;9,14;16;2;3,22;119;0;2,u;m
4029,ICLR,2020,RL-LIM: Reinforcement Learning-based Locally Interpretable Modeling,Jinsung Yoon;Sercan O. Arik;Tomas Pfister,jsyoon0823@gmail.com;soarik@google.com;tpfister@google.com,6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,3,0,yes,9/25/19,"University of California, Los Angeles;Google;Google",20;-1;-1,17;-1;-1,,9/25/19,1,0,1,0,0,0,558;1391;2335,55;54;47,13;17;16,58;119;285,m;m
4030,ICLR,2020,Characterizing Missing Information in Deep Networks Using Backpropagated Gradients,Gukyeong Kwon;Mohit Prabhushankar;Dogancan Temel;Ghassan AlRegib,gukyeong.kwon@gatech.edu;mohit.p@gatech.edu;cantemel@gatech.edu;alregib@gatech.edu,3;1;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology,13;13;13;13,38;38;38;38,,9/25/19,0,0,0,0,0,0,27;55;162;2201,6;11;35;259,3;3;9;24,2;5;10;145,m;m
4031,ICLR,2020,Improving Multi-Manifold GANs with a Learned Noise Prior,Matthew Amodio;Smita Krishnaswamy,matthew.amodio@yale.edu;smita.krishnaswamy@yale.edu,3;8;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,3,0,yes,9/25/19,Yale University;Yale University,64;64,8;8,5;4,9/25/19,0,0,0,0,0,0,121;38,20;7,5;1,8;4,m;f
4032,ICLR,2020,Unsupervised Domain Adaptation through Self-Supervision,Yu Sun;Eric Tzeng;Trevor Darrell;Alexei A. Efros,yusun@berkeley.edu;etzeng@eecs.berkeley.edu;trevor@eecs.berkeley.edu;efros@eecs.berkeley.edu,6;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,2,5,0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5,13;13;13;13,2,9/25/19,35,15,13,2,0,1,106;7301;90979;37253,58;32;559;194,4;14;112;77,6;832;11527;4619,m;m
4033,ICLR,2020,Adversarial Paritial Multi-label Learning,Yan Yan;Yuhong Guo,yanyan.nwpu@gmail.com;yuhongguo.cs@gmail.com,8;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,Tsinghua University;Temple University,8;205,23;311,5;4,9/15/19,0,0,0,0,0,0,14216;2382,1082;119,48;28,375;223,m:f
4034,ICLR,2020,Iterative Target Augmentation for Effective Conditional Generation,Kevin Yang;Wengong Jin;Kyle Swanson;Regina Barzilay;Tommi Jaakkola,yangk@berkeley.edu;wengong@csail.mit.edu;swansonk.14@gmail.com;regina@csail.mit.edu;tommi@csail.mit.edu,3;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,University of California Berkeley;Massachusetts Institute of Technology;;Massachusetts Institute of Technology;Massachusetts Institute of Technology,5;2;-1;2;2,13;5;-1;5;5,5,9/25/19,0,0,0,0,0,0,311;524;1840;12076;22299,11;24;63;234;293,5;7;19;56;69,27;63;127;1215;2326,m;m
4035,ICLR,2020,Hyperbolic Discounting and Learning Over Multiple Horizons,William Fedus;Carles Gelada;Yoshua Bengio;Marc G. Bellemare;Hugo Larochelle,liam.fedus@gmail.com;carlesgelada@hotmail.com;yoshua.bengio@mila.quebec;bellemare@google.com;hugolarochelle@google.com,6;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0,yes,9/25/19,University of Montreal;;University of Montreal;Google;Google,128;-1;128;-1;-1,85;-1;85;-1;-1,,2/19/19,19,12,2,0,18,0,696;231;208566;4073;25332,25;6;807;57;124,10;5;147;24;44,91;38;24297;637;2884,m;m
4036,ICLR,2020,Can I Trust the Explainer? Verifying Post-Hoc Explanatory Methods,Oana-Maria Camburu*;Eleonora Giunchiglia*;Jakob Foerster;Thomas Lukasiewicz;Phil Blunsom,ocamburu@gmail.com;eleonora.giunchiglia@cs.ox.ac.uk;jakobfoerster@gmail.com;thomas.lukasiewicz@gmail.com;philblunsom@gmail.com,3;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,University of Oxford;University of Oxford;Facebook;University of Oxford;,50;50;-1;50;-1,1;1;-1;1;-1,,9/25/19,4,2,1,0,0,0,383;18;2107;4759;11713,6;4;58;292;144,4;2;19;33;47,92;1;340;443;1357,f;m
4037,ICLR,2020,A Fine-Grained Spectral Perspective on Neural Networks,Greg Yang;Hadi Salman,gregyang@microsoft.com;hadicsalman@gmail.com,6;3;6,I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0,yes,9/25/19,Microsoft;Microsoft,-1;-1,-1;-1,8,7/24/19,14,8,2,0,0,2,513;210,24;18,10;7,55;21,m;m
4038,ICLR,2020,Low Rank Training of Deep Neural Networks for Emerging Memory Technology,Albert Gural;Phillip Nadeau;Mehul Tikekar;Boris Murmann,agural@stanford.edu;phillip.nadeau@analog.com;mehul.tikekar@analog.com;murmann@stanford.edu,3;3;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,5,0,yes,9/25/19,Stanford University;;;Stanford University,4;-1;-1;4,4;-1;-1;4,,9/25/19,0,0,0,0,0,0,44;250;218;5290,7;21;16;228,4;8;6;36,6;8;32;323,m;m
4039,ICLR,2020,Cyclic Graph Dynamic Multilayer Perceptron for Periodic Signals,Mikio Furokawa;Erik Gest;Takayuki Hirano;Kamal Youcef-Toumi,mikiof@mit.edu;erikgest@mit.edu;takayuki_hirano@jsw.co.jp;youcef@mit.edu,3;6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,0,0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;;Massachusetts Institute of Technology,2;2;-1;2,5;5;-1;5,10,9/25/19,0,0,0,0,0,0,0;0;0;2829,5;2;4;264,0;0;0;23,0;0;0;162,m;m
4040,ICLR,2020,Atomic Compression Networks,Jonas Falkner;Josif Grabocka;Lars Schmidt-Thieme,falkner@ismll.uni-hildesheim.de;josif@ismll.uni-hildesheim.de;schmidt-thieme@ismll.uni-hildesheim.de,6;1;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,University of Hildesheim;University of Hildesheim;University of Hildesheim,390;390;390,1397;1397;1397,,9/25/19,0,0,0,0,0,0,0;393;9110,4;40;279,0;10;36,0;58;1377,m;m
4041,ICLR,2020,Simplicial Complex Networks,Mohammad Firouzi;Sadra Boreiri;Hamed Firouzi,mfirouzi@alphabist.com;sadra.boreiri@epfl.ch;hfirouzi@alphabist.com,1;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,"Department of Computer Science, University of Toronto;Swiss Federal Institute of Technology Lausanne;Alphabist",18;481;-1,18;38;-1,1,9/25/19,0,0,0,0,0,0,37;23;17,9;6;7,2;3;2,0;1;0,m;m
4042,ICLR,2020,A Non-asymptotic comparison of SVRG and SGD: tradeoffs between compute and speed,Qingru Zhang;Yuhuai Wu;Fartash Faghri;Tianzong Zhang;Jimmy Ba,qrzhang98@gmail.com;ywu@cs.toronto.edu;faghri@cs.toronto.edu;ztz16@mails.tsinghua.edu.cn;jba@cs.toronto.edu,6;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,5,0,yes,9/25/19,"Shanghai Jiao Tong University;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Tsinghua University;Department of Computer Science, University of Toronto",53;18;18;8;18,157;18;18;23;18,,9/25/19,0,0,0,0,0,0,42;1214;841;10;52924,11;31;12;3;56,3;13;5;2;22,5;186;138;0;8625,m;m
4043,ICLR,2020,PAC-Bayesian Neural Network Bounds,Yossi Adi;Alex Schwing;Tamir Hazan,yossiadidrum@gmail.com;aschwing@illinois.edu;tamir.hazan@technion.ac.il,3;6;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,"Facebook;University of Illinois, Urbana Champaign;Technion",-1;3;26,-1;48;412,11;1;8,9/25/19,0,0,0,0,0,0,605;3818;1987,27;118;72,9;32;21,73;353;187,m;m
4044,ICLR,2020,Promoting Coordination through Policy Regularization in Multi-Agent Deep Reinforcement Learning,Paul Barde;Julien Roy;Félix G. Harvey;Derek Nowrouzezahrai;Christopher Pal,paul.b.barde@gmail.com;jul.roy1311@gmail.com;c212.felixh@gmail.com;derek@cim.mcgill.ca;christopher.pal@polymtl.ca,6;8;3,I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,10,0,yes,9/25/19,University of Montreal;University of Montreal;Polytechnique Montreal;McGill University;Polytechnique Montreal,128;128;390;86;390,85;85;1397;42;1397,,8/6/19,0,0,0,0,0,0,0;314;0;2071;8489,4;25;2;110;120,0;7;0;26;33,0;14;0;112;764,m;m
4045,ICLR,2020,Stabilizing Transformers for Reinforcement Learning,Emilio Parisotto;Francis Song;Jack Rae;Razvan Pascanu;Caglar Gulcehre;Siddhant Jayakumar;Max Jaderberg;Raphaël Lopez Kaufman;Aidan Clark;Seb Noury;Matt Botvinick;Nicolas Heess;Raia Hadsell,eparisot@cs.cmu.edu;songf@google.com;jwrae@google.com;razp@google.com;caglarg@google.com;sidmj@google.com;jaderberg@google.com;rlopezkaufman@google.com;aidanclark@google.com;snoury@google.com;botvinick@google.com;heess@google.com;raia@google.com,1;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,7,0,yes,9/25/19,Carnegie Mellon University;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,27;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,3,9/25/19,11,3,5,0,0,1,888;1224;631;17189;19800;192;7575;24;87;507;17176;11597;8331,17;27;19;101;36;14;31;2;12;4;58;104;63,10;13;10;46;26;9;21;2;6;4;36;37;26,76;118;74;1700;3009;23;938;2;12;91;1544;1644;804,m;f
4046,ICLR,2020,Deep k-NN for Noisy Labels,Dara Bahri;Heinrich Jiang;Maya Gupta,dbahri@google.com;heinrichj@google.com;mayagupta@google.com,1;1;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,2,0,yes,9/25/19,Google;Google;Google,-1;-1;-1,-1;-1;-1,,9/25/19,0,0,0,0,0,0,31;281;3142,22;28;146,3;9;27,1;30;244,m;f
4047,ICLR,2020,Annealed Denoising score matching: learning Energy based model in high-dimensional spaces,Zengyi Li;Yubei Chen;Friedrich T. Sommer,zengyi_li@berkeley.edu;yubeic@eecs.berkeley.edu;fsommer@berkeley.edu,3;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,13;13;13,5;11,9/25/19,5,1,2,0,0,0,34;122;2028,7;23;137,4;8;26,1;17;160,m;m
4048,ICLR,2020,Sparse Networks from Scratch: Faster Training without Losing Performance,Tim Dettmers;Luke Zettlemoyer,dettmers@cs.washington.edu;lsz@cs.washington.edu,6;3;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,University of Washington;University of Washington,6;6,26;26,,7/10/19,27,14,10,2,0,4,436;15157,5;176,4;53,153;2580,m;m
4049,ICLR,2020,TPO: TREE SEARCH POLICY OPTIMIZATION FOR CONTINUOUS ACTION SPACES,Amir Yazdanbakhsh;Ebrahim Songhori;Robert Ormandi;Anna Goldie;Azalia Mirhoseini,ayazdan@google.com;esonghori@google.com;ormandi@google.com;agoldie@google.com;azalia@google.com,1;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,,9/25/19,0,0,0,0,0,0,103;433;319;517;1083,18;27;27;18;56,3;9;8;6;14,10;34;31;45;79,m;f
4050,ICLR,2020,Sample-Based Point Cloud Decoder Networks,Erich Merrill;Alan Fern,merriler@oregonstate.edu;alan.fern@oregonstate.edu,3;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,0,0,yes,9/25/19,Oregon State University;Oregon State University,77;77,373;373,,9/25/19,0,0,0,0,0,0,19;3934,4;226,2;35,0;322,m;m
4051,ICLR,2020,Bayesian Residual Policy Optimization: Scalable Bayesian Reinforcement Learning with Clairvoyant Experts,Gilwoo Lee;Brian Hou;Sanjiban Choudhury;Siddhartha S. Srinivasa,gilwoo@cs.uw.edu;bhou@cs.uw.edu;sanjibac@cs.uw.edu;siddh@cs.uw.edu,3;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,"University of Washington, Seattle;University of Washington, Seattle;University of Washington, Seattle;University of Washington, Seattle",6;6;6;6,26;26;26;26,11,9/25/19,0,0,0,0,0,0,97;201;449;10494,29;15;53;329,6;5;12;52,2;15;11;803,-1;-1
4052,ICLR,2020,Efficient Inference and Exploration for Reinforcement Learning,Yi Zhu;Jing Dong;Henry Lam,yizhu2020@u.northwestern.edu;jing.dong@gsb.columbia.edu;khl2114@columbia.edu,3;1;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Northwestern University;Columbia University;Columbia University,44;15;15,22;16;16,,9/25/19,0,0,0,0,0,0,123;128;524,97;33;27,6;5;13,7;7;44,m;m
4053,ICLR,2020,Poisoning Attacks with Generative Adversarial Nets,Luis Muñoz-González;Bjarne Pfitzner;Matteo Russo;Javier Carnerero-Cano;Emil C. Lupu,l.munoz@imperial.ac.uk;bjarne.pfitzner@hpi.de;matteor@princeton.edu;j.carnerero-cano18@imperial.ac.uk;e.c.lupu@imperial.ac.uk,6;6;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,3,0,yes,9/25/19,Imperial College London;Hasso Plattner Institute;Princeton University;Imperial College London;Imperial College London,73;266;31;73;73,10;1397;6;10;10,5;4,6/18/19,4,1,0,0,3,0,396;2;90;10;5605,28;2;30;6;238,9;1;5;2;36,35;0;0;0;361,m;m
4054,ICLR,2020,Keyframing the Future: Discovering Temporal Hierarchy with Keyframe-Inpainter Prediction,Karl Pertsch;Oleh Rybkin;Jingyun Yang;Konstantinos G. Derpanis;Kostas Daniilidis;Joseph J. Lim;Andrew Jaegle,pertsch@usc.edu;oleh@seas.upenn.edu;jingyuny@usc.edu;kosta@ryerson.ca;kostas@seas.upenn.edu;limjj@usc.edu;ajaegle@upenn.edu,3;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0,yes,9/25/19,University of Southern California;University of Pennsylvania;University of Southern California;Ryerson University;University of Pennsylvania;University of Southern California;University of Pennsylvania,31;19;31;323;19;31;19,62;11;62;739;11;62;11,,9/25/19,0,0,0,0,0,0,45;21;182;2470;10204;2976;142,10;12;34;68;342;51;22,4;3;7;22;54;21;6,8;1;17;323;939;270;4,m;m
4055,ICLR,2020,An Empirical Study on Post-processing Methods for Word Embeddings,Shuai Tang;Mahta Mousavi;Virginia R. de Sa,shuaitang93@ucsd.edu;mahta@ucsd.edu;desa@ucsd.edu,1;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,1,0,yes,9/25/19,"University of California, San Diego;University of California, San Diego;University of California, San Diego",11;11;11,31;31;31,3,5/27/19,2,0,0,0,0,0,49;67;1070,15;16;81,4;4;17,1;2;55,m;f
4056,ICLR,2020,Winning the Lottery with Continuous Sparsification,Pedro Savarese;Hugo Silva;Michael Maire,savarese@ttic.edu;hugoandradesilva664@gmail.com;mmaire@uchicago.edu,3;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,6,0,yes,9/25/19,Toyota Technological Institute at Chicago;;University of Chicago,-1;-1;48,-1;-1;9,6,9/25/19,2,1,0,0,0,0,30;1673;16144,6;159;62,2;22;25,1;104;2845,m;m
4057,ICLR,2020,Rethinking Curriculum Learning With Incremental Labels And Adaptive Compensation,Madan Ravi Ganesh;Jason J. Corso,madantrg@umich.edu;jjcorso@umich.edu,3;1;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,4,0,yes,9/25/19,University of Michigan;University of Michigan,8;8,21;21,,9/25/19,0,0,0,0,0,0,29;5912,13;231,2;32,3;589,m;m
4058,ICLR,2020,Zero-Shot Policy Transfer with Disentangled Attention,Josh Roy;George Konidaris,josh_roy@brown.edu;gdk@cs.brown.edu,1;1;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,3,0,yes,9/25/19,Brown University;Brown University,67;67,53;53,5,9/25/19,0,0,0,0,0,0,7;3018,4;145,1;29,1;191,m;m
4059,ICLR,2020,P-BN: Towards Effective Batch Normalization in the Path Space,Xufang Luo;Qi Meng;Wei Chen;Tie-Yan Liu,luoxufang@buaa.edu.cn;meq@microsoft.com;wche@microsoft.com;tyliu@microsoft.com,3;3;3,I have published one or two papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Beihang University;Microsoft;Microsoft;Microsoft,118;-1;-1;-1,594;-1;-1;-1,,9/25/19,0,0,0,0,0,0,3;301;216;13552,5;58;172;369,1;11;9;51,0;7;9;1723,f;m
4060,ICLR,2020,Convolutional Tensor-Train LSTM for Long-Term Video Prediction,Jiahao Su;Wonmin Byeon;Furong Huang;Jan Kautz;Animashree Anandkumar,jiahaosu@terpmail.umd.edu;wonmin.byeon@gmail.com;furongh@cs.umd.edu;jkautz@nvidia.com;animakumar@gmail.com,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,"University of Maryland, College Park;NVIDIA;University of Maryland, College Park;NVIDIA;University of California-Irvine",12;-1;12;-1;35,91;-1;91;-1;96,,9/25/19,0,0,0,0,0,0,19;631;1752;14134;253,13;26;130;302;33,3;8;20;58;7,3;50;188;1901;30,m;f
4061,ICLR,2020,Task-Based Top-Down Modulation Network for Multi-Task-Learning Applications,Hila Levi;Shimon Ullman,hila.levi@weizmann.ac.il;shimon.ullman@weizmann.ac.il,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,6,0,yes,9/25/19,Weizmann Institute;Weizmann Institute,108;108,1397;1397,,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1,-1;-1,-1;-1,0;0,f;m
4062,ICLR,2020,Variational Constrained Reinforcement Learning with Application to Planning at Roundabout,Yuan Tian;Minghao Han;Lixian Zhang;Wulong Liu;Jun Wang;Wei Pan,yuantian013@163.com;mhhan@hit.edu.cn;lixianzhang@hit.edu.cn;liuwulong@huawei.com;jun.wang@cs.ucl.ac.uk;wei.pan@tudelft.nl,1;1;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,Swiss Federal Institute of Technology;Harbin Institute of Technology;Harbin Institute of Technology;Huawei Technologies Ltd.;University College London;Delft University of Technology,10;172;172;-1;50;89,13;424;424;-1;15;67,,9/25/19,0,0,0,0,0,0,461;19;9209;128;189;1770,71;10;251;33;122;121,8;2;51;6;8;21,10;2;314;9;5;157,m;m
4063,ICLR,2020,Learning a Spatio-Temporal Embedding for Video Instance Segmentation,Anthony Hu;Alex Kendall;Roberto Cipolla,ah2029@cam.ac.uk;alex@wayve.ai;rc10001@cam.ac.uk,3;6;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,4,0,yes,9/25/19,University of Cambridge;Wayve Technologies;University of Cambridge,71;-1;71,3;-1;3,2,9/25/19,3,2,2,0,0,1,47;6677;25762,10;51;494,3;17;74,5;1073;2930,m;m
4064,ICLR,2020,Deep Ensembles: A Loss Landscape Perspective,Stanislav Fort;Clara Huiyi Hu;Balaji Lakshminarayanan,stanislav.fort@gmail.com;clarahu@google.com;balajiln@google.com,8;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,5,0,yes,9/25/19,Google;Google;Google,-1;-1;-1,-1;-1;-1,11,9/25/19,14,9,5,1,0,2,107;193;2985,11;16;43,7;7;23,7;9;389,m;m
4065,ICLR,2020,Adversarial Privacy Preservation under Attribute Inference Attack,Han Zhao;Jianfeng Chi;Yuan Tian;Geoffrey J. Gordon,han.zhao@cs.cmu.edu;jc6ub@virginia.edu;yuant@virginia.edu;geoff.gordon@microsoft.com,3;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,4,1,yes,9/25/19,Carnegie Mellon University;University of Virginia;University of Virginia;Microsoft,1;59;59;-1,27;107;107;-1,4;1,9/25/19,2,0,0,0,0,0,43;20;461;10736,13;11;71;186,2;3;8;48,5;0;10;1228,m;m
4066,ICLR,2020,Deep Innovation Protection,Sebastian Risi;Kenneth O. Stanley,sebr@itu.dk;kstanley@uber.com,6;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,IT University;Uber,172;-1,416;-1,,9/25/19,3,3,0,1,0,1,1561;12177,121;260,24;53,93;1529,m;m
4067,ICLR,2020,Meta-Learning Runge-Kutta,Nadine Behrmann;Patrick Schramowski;Kristian Kersting,nadine.behrmann@freenet.de;schramowski@cs.tu-darmstadt.de;kersting@cs.tu-darmstadt.de,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,6,0,yes,9/25/19,TU Darmstadt;TU Darmstadt;TU Darmstadt,64;64;64,289;289;289,6,9/25/19,0,0,0,0,0,0,0;17;5839,1;10;343,0;3;41,0;1;464,f;m
4068,ICLR,2020,"Credible Sample Elicitation by Deep Learning, for Deep Learning",Yang Liu;Zuyue Fu;Zhuoran Yang;Zhaoran Wang,yangliu@ucsc.edu;zuyuefu2022@u.northwestern.edu;zy6@princeton.edu;zhaoranwang@gmail.com,6;1,I do not know much about this area.:N/A:N/A:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0,yes,9/25/19,University of Southern California;Northwestern University;Princeton University;Northwestern University,31;44;31;44,62;22;6;22,5;4,9/25/19,0,0,0,0,0,0,983;11;182;1193,143;6;41;78,14;2;7;20,83;3;22;132,m;m
4069,ICLR,2020,Continuous Graph Flow,Zhiwei Deng;Megha Nawhal;Lili Meng;Greg Mori,zhiweid@princeton.edu;mnawhal@sfu.ca;lilimeng1103@gmail.com;mori@cs.sfu.ca,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,1,3,0,yes,9/25/19,Princeton University;Simon Fraser University;University of British Columbia;Simon Fraser University,31;64;35;64,6;272;34;272,10;5;8,8/7/19,3,1,0,0,0,0,551;43;485;9711,17;15;53;197,9;3;12;45,74;6;36;817,m;m
4070,ICLR,2020,Extreme Value k-means Clustering,Sixiao Zheng;Yanxi Hou;Yanwei Fu;Jianfeng Feng,sxzheng18@fudan.edu.cn;yxhou@fudan.edu.cn;yanweifu@fudan.edu.cn;jffeng@fudan.edu.cn,3;3;1;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,Fudan University;Fudan University;Fudan University;Fudan University,79;79;79;79,109;109;109;109,,9/25/19,0,0,0,0,0,0,3;31;1915;11,4;14;90;7,1;3;22;1,0;0;254;3,m;m
4071,ICLR,2020,Hallucinative Topological Memory for Zero-Shot Visual Planning,Kara Liu;Thanard Kurutach;Pieter Abbeel;Aviv Tamar,karamarieliu@berkeley.edu;thanard.kurutach@berkeley.edu;pabbeel@cs.berkeley.edu;aviv.tamar.mail@gmail.com,1;8;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley;Technion,5;5;5;26,13;13;13;412,10;5;6;8,9/25/19,1,1,1,0,0,0,16;197;37294;7,2;8;438;6,1;4;94;2,0;28;4481;1,f;m
4072,ICLR,2020,On the Decision Boundaries of Deep Neural Networks: A Tropical Geometry Perspective,Motasem Alfarra;Adel Bibi;Hasan Hammoud;Mohamed Gaafar;Bernard Ghanem,motasem.alfarra@kaust.edu.sa;adel.bibi@kaust.edu.sa;hasan.hammoud@kaust.edu.sa;muhamed.gaafar@gmail.com;bernard.ghanem@kaust.edu.sa,1;8;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,9,0,yes,9/25/19,KAUST;KAUST;KAUST;Itemis AG;KAUST,128;128;128;-1;128,1397;1397;1397;-1;1397,4,9/25/19,2,1,0,0,0,0,5;462;2;12;6428,9;25;1;5;198,2;9;1;2;37,0;65;0;0;1007,m;m
4073,ICLR,2020,Encoder-decoder Network as Loss Function for Summarization,Glen Jeh,glenjeh@gmail.com,1;6;1,I have published one or two papers in this area.:I carefully checked the experiments.:N/A:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I did not assess the experiments.:N/A:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,,,,,9/25/19,1,1,0,0,0,0,1,1,1,0,m
4074,ICLR,2020,Unsupervised Learning of Automotive 3D Crash Simulations using LSTMs,Amin Abbasloo;Jochen Garcke;Rodrigo Iza-Teran,amin.abbasloo@scai.fraunhofer.de;garcke@ins.uni-bonn.de;rodrigo.iza-teran@scai.fraunhofer.de,3;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,1,0,yes,9/25/19,Fraunhofer SCAI;University of Bonn;Fraunhofer SCAI,-1;128;-1,-1;106;-1,,9/25/19,0,0,0,0,0,0,27;924;66,7;69;11,2;18;3,0;41;3,m;m
4075,ICLR,2020,Dynamic Instance Hardness,Tianyi Zhou;Shengjie Wang;Jeff A. Bilmes,tianyizh@uw.edu;wangsj@cs.washington.edu;bilmes@uw.edu,3;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,"University of Washington, Seattle;University of Washington;University of Washington, Seattle",6;6;6,26;26;26,1,9/25/19,0,0,0,0,0,0,1481;2;13972,81;5;350,14;1;55,142;0;1283,m;m
4076,ICLR,2020,Beyond Classical Diffusion: Ballistic Graph Neural Network,Yimeng Min,minyimen@mila.quebec,3;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,University of Montreal,128,85,10,9/25/19,0,0,0,0,0,0,110,10,2,1,m;m
4077,ICLR,2020,INSTANCE CROSS ENTROPY FOR DEEP METRIC LEARNING,Xinshao Wang;Elyor Kodirov;Yang Hua;Neil M. Robertson,xwang39@qub.ac.uk;elyor@anyvision.co;y.hua@qub.ac.uk;n.robertson@qub.ac.uk,3;1;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,1,yes,9/25/19,Queen's University Belfast;Anyvision;Queen's University Belfast;Queen's University Belfast,266;-1;266;266,204;-1;204;204,,9/25/19,1,0,0,0,0,0,45;1062;626;155,6;33;582;49,2;10;11;8,8;148;22;6,m;m
4078,ICLR,2020,Retrieving Signals in the Frequency Domain with Deep Complex Extractors,Chiheb Trabelsi;Olexa Bilaniuk;Ousmane Dia;Ying Zhang;Mirco Ravanelli;Jonathan Binas;Negar Rostamzadeh;Christopher  J Pal,chiheb.trabelsi@polymtl.ca;olexa.bilaniuk@umontreal.ca;ousmane@elementai.com;ying@elementai.com;mirco.ravanelli@gmail.com;jbinas@gmail.com;negar@elementai.com;christopher.pal@elementai.com,6;3;6,I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0,yes,9/25/19,Polytechnique Montreal;University of Montreal;Element AI;Element AI;University of Montreal;University of Montreal;Element AI;Element AI,390;128;-1;-1;128;128;-1;-1,1397;85;-1;-1;85;85;-1;-1,,9/25/19,0,0,0,0,0,0,286;281;5;18;763;693;389;8489,11;14;8;21;37;28;32;120,4;7;1;3;18;10;10;33,41;36;1;2;72;94;52;764,m;m
4079,ICLR,2020,A Copula approach for hyperparameter transfer learning,David Salinas;Huibin Shen;Valerio Perrone,david.salinas.pro@gmail.com;huibishe@amazon.com;vperrone@amazon.com,3;1;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,4,0,yes,9/25/19,Naver Labs Europe;Amazon;Amazon,-1;-1;-1,-1;-1;-1,11;6,9/25/19,1,1,0,0,0,0,416;462;132,25;12;12,12;6;5,41;27;17,m;m
4080,ICLR,2020,Out-of-distribution Detection in Few-shot Classification,Kuan-Chieh Wang;Paul Vicol;Eleni Triantafillou;Chia-Cheng Liu;Richard Zemel,wangkua1@cs.toronto.edu;pvicol@cs.toronto.edu;eleni@cs.toronto.edu;cc.liu2018@gmail.com;zemel@cs.toronto.edu,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,5,0,yes,9/25/19,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;;Department of Computer Science, University of Toronto",18;18;18;-1;18,18;18;18;-1;18,6,9/25/19,0,0,0,0,0,0,180;146;591;246;21903,11;19;31;44;209,3;6;9;8;52,24;17;76;25;2523,m;m
4081,ICLR,2020,GQ-Net: Training Quantization-Friendly Deep Networks,Rundong Li;Rui Fan,lird@shanghaitech.edu.cn;fanrui@shanghaitech.edu.cn,3;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,8,0,yes,9/25/19,ShanghaiTech University;ShanghaiTech University,481;481,1397;1397,,9/25/19,0,0,0,0,0,0,712;294,100;58,15;9,25;14,m;m
4082,ICLR,2020,The divergences minimized by non-saturating GAN training,Matt Shannon,matt.shannon.personal@gmail.com,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Google,-1,-1,5;4,9/25/19,1,1,0,0,0,0,568,29,13,39,m
4083,ICLR,2020,"Translation Between Waves,  wave2wave",Tsuyoshi Okita;Hirotaka Hachiya;Sozo Inoue;Naonori Ueda,tsuyoshi.okita@gmail.com;hirotaka.hachiya@riken.jp;sozo.inoue@riken.jp;naonori.ueda@riken.jp,1;1;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,Meiji University;RIKEN;RIKEN;RIKEN,481;-1;-1;-1,332;-1;-1;-1,3,9/25/19,0,0,0,0,0,0,48;644;643;248,73;53;173;20,4;16;13;4,3;54;40;55,m;m
4084,ICLR,2020,MixUp as Directional Adversarial Training,Guillaume Perrault-Archambault;Yongyi Mao;Hongyu Guo;Richong Zhang,gperr050@uottawa.ca;yymao@eecs.uottawa.ca;hongyu.guo@nrc-cnrc.gc.ca;zhangrc@act.buaa.edu.cn,1;3;3,I have published in this field for several years.:I carefully checked the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0,yes,9/25/19,University of Ottawa;University of Ottawa;National Research Council Canada;Beihang University,266;266;-1;118,141;141;-1;594,4;8,6/17/19,0,0,0,0,0,0,23;1305;1099;567,4;94;68;93,1;17;13;14,1;99;117;55,m;m
4085,ICLR,2020,Deep Multiple Instance Learning with Gaussian Weighting,Basura Fernando;Hakan Bilen,basura.fernando@anu.edu.au;hbilen@ed.ac.uk,8;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Australian National University;University of Edinburgh,108;33,50;30,,9/25/19,0,0,0,0,0,0,3768;1809,78;49,28;16,515;358,m;m
4086,ICLR,2020,DyNet: Dynamic Convolution for Accelerating Convolution Neural Networks,Kane Zhang;Jian Zhang;Qiang Wang;Zhao Zhong,zhangyikang5@huawei.com;zhangjian157@huawei.com;wangqiang168@huawei.com;zorro.zhongzhao@huawei.com,6;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0,yes,9/25/19,Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.,-1;-1;-1;-1,-1;-1;-1;-1,2,9/25/19,0,0,0,0,0,0,236;4;139;702,62;9;95;254,8;1;6;11,15;1;8;68,m;m
4087,ICLR,2020,Growing Action Spaces,Gregory Farquhar;Laura Gustafson;Zeming Lin;Shimon Whiteson;Nicolas Usunier;Gabriel Synnaeve,gregory.farquhar@cs.ox.ac.uk;lgustafson@fb.com;zlin@fb.com;shimon.whiteson@cs.ox.ac.uk;usunier@fb.com;gab@fb.com,3;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,University of Oxford;Facebook;Facebook;University of Oxford;Facebook;Facebook,50;-1;-1;50;-1;-1,1;-1;-1;1;-1;-1,1,6/28/19,1,1,1,1,0,1,938;24;6087;5445;6244;1512,19;9;19;203;109;62,9;3;9;38;30;19,174;4;752;588;1186;154,m;m
4088,ICLR,2020,Model Ensemble-Based Intrinsic Reward for Sparse Reward Reinforcement Learning,Giseung Park;Whiyoung Jung;Sungho Choi;Youngchul Sung,gs.park@kaist.ac.kr;wy.jung@kaist.ac.kr;sungho.choi@kaist.ac.kr;ycsung@kaist.ac.kr,3;3;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,481;481;481;481,110;110;110;110,,9/25/19,0,0,0,0,0,0,1;1;51;1456,5;4;15;121,1;1;2;21,0;0;3;118,m;m
4089,ICLR,2020,Differentiable Hebbian Consolidation for Continual Learning,Vithursan Thangarasa;Thomas Miconi;Graham W. Taylor,vthangar@uoguelph.ca;tmiconi@uber.com;gwtaylor@uoguelph.ca,3;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,7,0,yes,9/25/19,University of Guelph;Uber;University of Guelph,266;-1;266,558;-1;558,,9/1/19,0,0,0,0,0,0,5;458;5971,6;42;143,1;13;31,0;55;508,m;m
4090,ICLR,2020,Meta-Graph: Few shot Link Prediction via Meta Learning,Avishek Joey Bose;Ankit Jain;Piero Molino;William L. Hamilton,joey.bose@mail.mcgill.ca;ankit.jain@uber.com;piero.molino@uber.com;wlh@cs.mcgill.ca,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,McGill University;Uber;Uber;McGill University,86;-1;-1;86,42;-1;-1;42,6;10,9/25/19,2,1,2,0,0,1,91;52;488;4476,16;40;33;46,5;4;8;19,8;5;60;926,m;m
4091,ICLR,2020,EXPLOITING SEMANTIC COHERENCE TO IMPROVE PREDICTION IN SATELLITE SCENE IMAGE ANALYSIS: APPLICATION TO DISEASE DENSITY ESTIMATION,Rahman Sanya;Gilbert Maiga;Ernest Mwebaze,hbasanya@gmail.com;gilmaiga@gmail.com;emwebaze@gmail.com,1;3;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,1,0,yes,9/25/19,Makerere University;;,481;-1;-1,605;-1;-1,,9/25/19,0,0,0,0,0,0,3;50;201,5;32;36,1;3;7,0;1;8,m;m
4092,ICLR,2020,VAENAS: Sampling Matters in Neural Architecture Search,Shizheng Qin;Yichen Zhu;Pengfei Hou;Xiangyu Zhang;Wenqiang Zhang;Jian Sun,szqin17@fudan.edu.cn;k.zhu@mail.utoronto.ca;houpengfei@megvii.com;zhangxiangyu@megvii.com;wqzhang@fudan.edu.cn;sunjian@megvii.com,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0,yes,9/25/19,Fudan University;Toronto University;Megvii Technology Inc.;Megvii Technology Inc.;Fudan University;Megvii Technology Inc.,79;18;-1;-1;79;-1,109;18;-1;-1;109;-1,5;2,9/25/19,0,0,0,0,0,0,2;26;7;64794;850;3762,4;11;8;327;149;223,1;3;1;44;13;27,0;5;0;11989;44;305,m;m
4093,ICLR,2020,Aggregating explanation methods for neural networks stabilizes explanations,Laura Rieger;Lars Kai Hansen,lauri@dtu.dk,8;3;8,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0,yes,9/25/19,Technical University of Denmark,481,182,,3/1/19,2,2,0,0,4,0,23;10950,12;527,3;52,0;805,m;f
4094,ICLR,2020,Towards Stable and comprehensive Domain Alignment: Max-Margin Domain-Adversarial Training,Jianfei Yang;Han Zou;Yuxun Zhou;Lihua Xie,yang0478@e.ntu.edu.sg;hanzou@berkeley.edu;yxzhou@berkeley.edu;elhxie@ntu.edu.sg,6;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,National Taiwan University;University of California Berkeley;University of California Berkeley;National Taiwan University,86;5;5;86,120;13;13;120,4,9/25/19,0,0,0,0,0,0,791;53;205;3,89;11;35;19,17;3;7;1,42;0;13;0,m;m
4095,ICLR,2020,In-Domain Representation Learning For Remote Sensing,Maxim Neumann;Andre Susano Pinto;Xiaohua Zhai;Neil Houlsby,maximneumann@google.com;andresp@google.com;xzhai@google.com;neilhoulsby@google.com,3;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,,9/25/19,3,2,2,0,0,2,910;25;842;895,72;6;35;34,15;3;14;15,68;3;106;117,m;m
4096,ICLR,2020,Ternary MobileNets via Per-Layer Hybrid Filter Banks,Dibakar Gope;Jesse G Beu;Urmish Thakker;Matthew Mattina,dibakar.gope@arm.com;jesse.beu@arm.com;urmish.thakker@arm.com;matthew.mattina@arm.com,3;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,arm;arm;arm;arm,-1;-1;-1;-1,-1;-1;-1;-1,2,9/25/19,2,1,1,0,0,0,74;164;33;1604,17;19;12;35,6;7;4;9,2;9;0;165,m;m
4097,ICLR,2020,Avoiding Negative Side-Effects and Promoting Safe Exploration with Imaginative Planning,Dhruv Ramani;Benjamin Eysenbach,dhruvramani98@gmail.com;beysenba@cs.cmu.edu,1;1;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,0,0,yes,9/25/19,National Institute of Technology Warangal;Carnegie Mellon University,-1;1,-1;27,10,9/25/19,0,0,0,0,0,0,2;357,6;16,1;6,0;54,m;m
4098,ICLR,2020,Exploring the Correlation between Likelihood of Flow-based Generative Models and Image Semantics,Xin WANG;SiuMing Yiu,xwang@cs.hku.hk;smyiu@cs.hku.hk,3;1;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,8,0,yes,9/25/19,The University of Hong Kong;The University of Hong Kong,92;92,35;35,5,9/25/19,0,0,0,0,0,0,17;746,49;26,2;7,1;63,u;m
4099,ICLR,2020,Provably Communication-efficient Data-parallel SGD via Nonuniform Quantization,Ali Ramezani-Kebrya;Fartash Faghri;Ilya Markov;Vitalii Aksenov;Dan Alistarh;Daniel M. Roy,alir@vectorinstitute.ai;faghri@cs.toronto.edu;droy@utstat.toronto.edu;dan.alistarh@ist.ac.at;markovilya197@gmail.com;vitalii.aksenov@ist.ac.at,3;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,5,0,yes,9/25/19,"Vector Institute;Department of Computer Science, University of Toronto;University of Toronto;Institute of Science and Technology Austria;;Institute of Science and Technology Austria",-1;18;18;481;-1;481,-1;18;18;1397;-1;1397,,9/25/19,0,0,0,0,0,0,85;841;592;1;1785;3139,12;12;50;17;128;97,4;5;11;1;19;24,3;138;54;0;227;333,m;m
4100,ICLR,2020,Scalable Neural Learning for Verifiable Consistency with Temporal Specifications,Sumanth Dathathri;Johannes Welbl;Krishnamurthy (Dj) Dvijotham;Ramana Kumar;Aditya Kanade;Jonathan Uesato;Sven Gowal;Po-Sen Huang;Pushmeet Kohli,sdathath@caltech.edu;johannes.welbl.14@ucl.ac.uk;dvij@google.com;ramanakumar@google.com;akanade@google.com;juesato@google.com;sgowal@google.com;posenhuang@google.com;pushmeet@google.com,3;6;8;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,9,0,yes,9/25/19,California Institute of Technology;University College London;Google;Google;Google;Google;Google;Google;Google,143;50;-1;-1;-1;-1;-1;-1;-1,2;15;-1;-1;-1;-1;-1;-1;-1,3;4,9/25/19,0,0,0,0,0,0,85;1018;1143;668;712;952;568;1772;22578,17;19;76;46;50;17;34;59;313,4;9;17;12;13;11;11;17;69,13;271;101;45;46;116;62;250;2782,m;m
4101,ICLR,2020,EvoNet: A Neural Network for Predicting the Evolution of Dynamic Graphs,Changmin Wu;Giannis Nikolentzos;Michalis Vazirgiannis,changmin.wu@polytechnique.edu;giannisnik@hotmail.com;mvazirg@lix.polytechnique.fr,3;3;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,0,0,yes,9/25/19,"Ecole polytechnique;;Ecole Polytechnique, France",481;-1;481,93;-1;93,5;10,9/25/19,1,0,1,0,0,0,15;344;9027,9;30;321,3;11;40,0;18;661,m;m
4102,ICLR,2020,SGD Learns One-Layer Networks in WGANs,Qi Lei;Jason D. Lee;Alexandros G. Dimakis;Constantinos Daskalakis,leiqi@ices.utexas.edu;jasondlee88@gmail.com;dimakis@austin.utexas.edu;costis@csail.mit.edu,3;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,"University of Texas, Austin;University of Southern California;University of Texas, Austin;Massachusetts Institute of Technology",22;31;22;2,38;62;38;5,5;4,9/25/19,5,1,1,0,0,0,1478;4788;12219;4978,149;120;204;169,20;36;50;40,90;614;1439;509,f;m
4103,ICLR,2020,Improving Robustness Without Sacrificing Accuracy with Patch Gaussian Augmentation,Raphael Gontijo Lopes;Dong Yin;Ben Poole;Justin Gilmer;Ekin D. Cubuk,iraphael@google.com;dongyin@berkeley.edu;pooleb@google.com;gilmer@google.com;cubuk@google.com,3;3;8,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,5,0,yes,9/25/19,Google;University of California Berkeley;Google;Google;Google,-1;5;-1;-1;-1,-1;13;-1;-1;-1,4;8,6/6/19,20,8,4,1,12,1,145;748;4294;3434;1392,8;132;41;45;50,5;14;19;19;19,22;65;701;469;135,m;m
4104,ICLR,2020,Tensorized Embedding Layers for Efficient Model Compression,Oleksii Hrinchuk;Valentin Khrulkov;Leyla Mirvakhabova;Ivan Oseledets,oleksii.hrinchuk@skoltech.ru;khrulkov.v@gmail.com;leyla.mirvakhabova@skoltech.ru;i.oseledets@skoltech.ru,8;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Skolkovo Institute of Science and Technology;Skolkovo Institute of Science and Technology;Skolkovo Institute of Science and Technology;Skolkovo Institute of Science and Technology,-1;-1;-1;-1,-1;-1;-1;-1,3,1/30/19,13,4,10,0,4,4,63;164;20;4436,10;12;4;199,5;7;2;30,4;17;4;404,m;m
4105,ICLR,2020,Training Deep Networks with Stochastic Gradient Normalized by Layerwise Adaptive Second Moments,Boris Ginsburg;Patrice Castonguay;Oleksii Hrinchuk;Oleksii Kuchaiev;Vitaly Lavrukhin;Ryan Leary;Jason Li;Huyen Nguyen;Yang Zhang;Jonathan M. Cohen,boris.ginsburg@gmail.com;pcastonguay@nvidia.com;grinchuk.alexey@gmail.com;kuchaev@gmail.com;vlavrukhin@yahoo.com;rleary@nvidia.com;jasoli@nvidia.com;huyenntkvn@gmail.com;yangzhang@nvidia.com;jocohen@nvidia.com,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,3,0,yes,9/25/19,NVIDIA;NVIDIA;Moscow Institute of Physics and Technology;NVIDIA;;NVIDIA;NVIDIA;;NVIDIA;NVIDIA,-1;-1;481;-1;-1;-1;-1;-1;-1;-1,-1;-1;234;-1;-1;-1;-1;-1;-1;-1,3,9/25/19,0,0,0,0,0,0,1044;861;63;1530;118;395;12;104;360;934,30;33;10;26;13;41;18;45;119;52,14;16;5;15;6;12;2;5;10;14,133;47;4;215;14;26;1;8;22;67,m;m
4106,ICLR,2020,Learning Time-Aware Assistance Functions for Numerical Fluid Solvers,Kiwon Um;Yun (Raymond) Fei;Philipp Holl;Nils Thuerey,kiwon.um@tum.de;yf2320@columbia.edu;philipp.holl@tum.de;nils.thuerey@tum.de,6;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,3,0,yes,9/25/19,Technical University Munich;Columbia University;Technical University Munich;Technical University Munich,53;15;53;53,43;16;43;43,,9/25/19,0,0,0,0,0,0,76;96;37;2623,15;25;5;122,4;4;2;32,1;3;2;189,m;m
4107,ICLR,2020,Why do These Match? Explaining the Behavior of Image Similarity Models,Bryan A. Plummer;Mariya I. Vasileva;Vitali Petsiuk;Kate Saenko;David Forsyth,bplumme2@illinois.edu;mvasile2@illinois.edu;vpetsiuk@bu.edu;saenko@bu.edu;daf@illinois.edu,6;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,5,0,yes,9/25/19,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;Boston University;Boston University;University of Illinois, Urbana Champaign",3;3;67;67;3,48;48;61;61;48,,5/26/19,1,1,0,0,0,0,802;71;96;17431;143,27;12;5;178;59,9;2;2;56;6,144;17;28;2403;8,m;m
4108,ICLR,2020,Towards an Adversarially Robust Normalization Approach,Muhammad Awais;Fahad Shamshad;Sung-Ho Bae,awais@khu.ac.kr;fahad.shamshad@itu.edu.pk;shbae@khu.ac.kr,3;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,3,0,0,yes,9/25/19,"Kyung Hee University;ITU of Punjab Lahore, Pakistan;Kyung Hee University",481;-1;481,319;-1;319,4,9/25/19,0,0,0,0,0,0,315;87;151,55;19;16,9;5;2,32;5;10,m;m
4109,ICLR,2020,When Does Self-supervision Improve Few-shot Learning?,Jong-Chyi Su;Subhransu Maji;Bharath Hariharan,jcsu@cs.umass.edu;smaji@cs.umass.edu;bharathh@cs.cornell.edu,3;3;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:N/A:N/A,Reject,0,4,0,yes,9/25/19,"University of Massachusetts, Amherst;University of Massachusetts, Amherst;Cornell University",28;28;7,209;209;19,6;8,9/25/19,6,2,1,0,0,0,119;8841;924,17;116;24,6;36;6,5;1364;190,m;m
4110,ICLR,2020,D3PG: Deep Differentiable Deterministic Policy Gradients,Tao Du;Yunfei Li;Jie Xu;Andrew Spielberg;Kui Wu;Daniela Rus;Wojciech Matusik,taodu@csail.mit.edu;l-yf16@mails.tsinghua.edu.cn;jiex@csail.mit.edu;aespielberg@csail.mit.edu;walker.kui.wu@gmail.com;rus@csail.mit.edu;wojciech@csail.mit.edu,3;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,Massachusetts Institute of Technology;Tsinghua University;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;8;2;2;2;2;2,5;23;5;5;5;5;5,9,9/25/19,0,0,0,0,0,0,649;2;698;231;624;4915;10927,71;3;140;21;43;165;255,10;1;12;10;10;29;55,32;0;32;16;38;220;801,m;m
4111,ICLR,2020,Sparse Transformer: Concentrated Attention Through Explicit Selection,Guangxiang Zhao;Junyang Lin;Zhiyuan Zhang;Xuancheng Ren;Xu Sun,1701214310@pku.edu.cn;junyang.ljy@alibaba-inc.com;zzy1210@pku.edu.cn;renxc@pku.edu.cn;xusun@pku.edu.cn,1;3;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,1,7,0,yes,9/25/19,Peking University;Alibaba Group;Peking University;Peking University;Peking University,22;-1;22;22;22,24;-1;24;24;24,3,9/25/19,3,2,0,0,0,0,14;326;2368;51;460;2368,8;26;128;47;40;128,3;10;19;4;11;19,1;35;53;1;53;53,m;m
4112,ICLR,2020,SAFE-DNN: A Deep Neural Network with Spike Assisted Feature Extraction for Noise Robust Inference,Xueyuan She;Priyabrata Saha;Daehyun Kim;Yun Long;Saibal Mukhopadhyay,xshe6@gatech.edu;priyabratasaha@gatech.edu;daehyun.kim@gatech.edu;yunlong@gatech.edu;saibal.mukhopadhyay@ece.gatech.edu,3;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,4,0,yes,9/25/19,Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology,13;13;13;13;13,38;38;38;38;38,,9/25/19,0,0,0,0,0,0,32;26;285;571;314,8;15;72;102;90,3;3;8;12;8,4;1;10;31;12,m;m
4113,ICLR,2020,TWIN GRAPH CONVOLUTIONAL NETWORKS: GCN WITH DUAL GRAPH SUPPORT FOR SEMI-SUPERVISED LEARNING,Feng Shi;Yizhou Zhao;Ziheng Xu;Tianyang Liu;Song-Chun Zhu,shi.feng@cs.ucla.edu;yizhouzhao@ucla.edu;lawrencexu@ucla.edu,3;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,"University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles",20;20;20,17;17;17,10,9/25/19,0,0,0,0,0,0,106;154;1890;49;83,30;21;11;28;41,4;7;5;4;5,8;2;36;0;7,m;m
4114,ICLR,2020,"Calibration, Entropy Rates, and Memory in Language Models",Mark Braverman;Xinyi Chen;Sham Kakade;Karthik Narasimhan;Cyril Zhang;Yi Zhang,mbraverm@cs.princeton.edu;xinyic@google.com;sham@cs.washington.edu;karthikn@cs.princeton.edu;cyril.zhang@cs.princeton.edu;y.zhang@cs.princeton.edu,6;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,Princeton University;Google;University of Washington;Princeton University;Princeton University;Princeton University,31;-1;6;31;31;31,6;-1;26;6;6;6,3;5,6/11/19,3,0,0,0,0,0,130;14046;13740;1181;198;4586,22;861;198;34;18;613,4;46;58;13;6;33,11;815;1986;125;20;94,m;m
4115,ICLR,2020,Deep End-to-end Unsupervised Anomaly Detection ,Li Tangqing;Wang Zheng;Liu Siying;Daniel Lin Wen-Yan,li_tangqing@u.nus.edu;sliu50@illinois.edu;zhwang@i2r.a-star.edu.sg;daniellin@smu.edu.sg,3;6;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,"National University of Singapore;University of Illinois, Urbana Champaign;A*STAR;Singapore Management University",16;3;-1;92,25;48;-1;1397,,9/25/19,0,0,0,0,0,0,0;69;3;0,1;74;9;1,0;5;1;0,0;3;0;0,u;m
4116,ICLR,2020,Role-Wise Data Augmentation for Knowledge Distillation,Jie Fu;Xue Geng;Bohan Zhuang;Xingdi Yuan;Adam Trischler;Jie Lin;Vijay Chandrasekhar;Chris Pal,jie.fu@polymtl.ca;geng_xue@i2r.a-star.edu.sg;bohan.zhuang@adelaide.edu.au;eryua@microsoft.com;adam.trischler@microsoft.com;lin-j@i2r.a-star.edu.sg;vijay@i2r.a-star.edu.sg;christopher.pal@polymtl.ca,3;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,4,0,yes,9/25/19,Polytechnique Montreal;A*STAR;The University of Adelaide;Microsoft;Microsoft;A*STAR;A*STAR;Polytechnique Montreal,390;-1;128;-1;-1;-1;-1;390,1397;-1;120;-1;-1;-1;-1;1397,,9/25/19,0,0,0,0,0,0,126;121;622;820;1589;430;3289;39,45;33;31;27;47;26;110;11,6;3;12;10;17;8;29;2,6;7;75;134;291;37;229;2,m;m
4117,ICLR,2020,HighRes-net: Multi-Frame Super-Resolution by Recursive Fusion,Michel Deudon;Alfredo Kalaitzis;Md Rifat Arefin;Israel Goytom;Zhichao Lin;Kris Sankaran;Vincent Michalski;Samira E Kahou;Julien Cornebise;Yoshua Bengio,michel.deudon@elementai.com;freddie@element.ai;rifat.arefin515@gmail.com;isrugeek@gmail.com;zhichao.lin@elementai.com;sankaran.kris@gmail.com;vincent.michalski@gmx.de;samira.ebrahimi-kahou@polymtl.ca;julien@elementai.com;yoshua.bengio@mila.quebec,8;3;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,17,1,yes,9/25/19,Element AI;Element AI;University of Montreal;Boston University;Element AI;University of Montreal;Goethe University;Polytechnique Montreal;Element AI;University of Montreal,-1;-1;128;67;-1;128;233;390;-1;128,-1;-1;85;61;-1;85;305;1397;-1;85,5,9/25/19,3,1,0,0,0,0,53;159;3;3;6;345;2674;3933;1256;208566,8;17;1;4;11;26;56;70;26;807,3;6;1;1;1;7;12;14;8;147,5;11;0;0;0;24;237;384;227;24297,m;m
4118,ICLR,2020,Sparse and Structured Visual Attention,Pedro Henrique Martins;Vlad Niculae;Zita Marinho;André F.T. Martins,pedrohenriqueamartins@gmail.com;vlad@vene.ro;zita.marinho@priberam.pt;andre.martins@unbabel.com,3;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,"Instituto Superior Técnico;Instituto de Telecomunicações, Portugal;;Unbabel",481;-1;-1;-1,1397;-1;-1;-1,,9/25/19,0,0,0,0,0,0,11;1232;116;2405,5;47;17;100,2;14;5;25,1;119;7;305,m;m
4119,ICLR,2020,Policy Tree Network,Zac Wellmer;Sepanta Zeighami;James Kwok,zac@1984.ai;szeighami@connect.ust.hk;jamesk@cse.ust.hk,3;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0,yes,9/25/19,Hong Kong University of Science and Technology;The Hong Kong University of Science and Technology;The Hong Kong University of Science and Technology,39;39;39,47;47;47,1,9/25/19,0,0,0,0,0,0,67;14;9752,3;5;197,1;2;51,6;4;1062,m;m
4120,ICLR,2020,Bias-Resilient Neural Network,Ehsan Adeli;Qingyu Zhao;Adolf Pfefferbaum;Edith V. Sullivan;Fei-Fei Li;Juan Carlos Niebles;Kilian M. Pohl,eadeli@stanford.edu;qingyuz@stanford.edu;edie@stanford.edu;dolfp@stanford.edu;feifeili@cs.stanford.edu;jniebles@cs.stanford.edu;kilian.pohl@stanford.edu,8;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0,yes,9/25/19,Stanford University;Stanford University;Stanford University;Stanford University;Stanford University;Stanford University;Stanford University,4;4;4;4;4;4;4,4;4;4;4;4;4;4,7;4;2,9/25/19,4,3,0,0,0,0,919;240;20625;18494;80554;5497;1456,81;32;620;440;450;82;80,15;7;77;75;95;31;19,62;9;1214;1109;11761;708;96,m;m
4121,ICLR,2020,ASYNCHRONOUS MULTI-AGENT GENERATIVE ADVERSARIAL IMITATION LEARNING,Xin Zhang;Weixiao Huang;Renjie Liao;Yanhua Li,xzhang17@wpi.edu;whuang2@wpi.edu;rjliao@cs.toronto.edu;yli15@wpi.edu,1;6;6,I have published in this field for several years.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,20,0,yes,9/25/19,"Worcester Polytechnic Institute;Worcester Polytechnic Institute;Department of Computer Science, University of Toronto;Worcester Polytechnic Institute",172;172;18;172,628;628;18;628,5;4,9/25/19,0,0,0,0,0,0,259;31;1629;2204,187;14;63;131,8;3;22;18,8;0;210;181,f;m
4122,ICLR,2020,Efficient Training of Robust and Verifiable Neural Networks,Akhilan Boopathy;Lily Weng;Sijia Liu;Pin-Yu Chen;Luca Daniel,akhilan@mit.edu;twweng@mit.edu;sijia.liu@ibm.com;pin-yu.chen@ibm.com;dluca@mit.edu,1;1;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,6,0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;International Business Machines;International Business Machines;Massachusetts Institute of Technology,2;2;-1;-1;2,5;5;-1;-1;5,4;8,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,0;0;0;0;0,m;m
4123,ICLR,2020,Adversarial Video Generation on Complex Datasets,Aidan Clark;Jeff Donahue;Karen Simonyan,aidanclark@google.com;jeffdonahue@google.com;simonyan@google.com,3;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,1,4,0,yes,9/25/19,Google;Google;Google,-1;-1;-1,-1;-1;-1,5;4,7/15/19,31,16,11,1,0,5,87;35933;61426,12;56;97,6;25;40,12;4756;10911,m;f
4124,ICLR,2020,Data Augmentation in Training CNNs: Injecting Noise to Images,Murtaza Eren Akbiyik,erenakbiyik@gmail.com,3;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,2,0,0,yes,9/25/19,International Business Machines,-1,-1,,9/25/19,0,0,0,0,0,0,0,3,0,0,m
4125,ICLR,2020,Variational Diffusion Autoencoders with Random Walk Sampling,Henry Li;Ofir Lindenbaum;Xiuyuan Cheng;Alexander Cloninger,henryli@eng.ucsd.edu;ofir.lindenbaum@yale.edu;xiuyuan.cheng@duke.edu;acloninger@ucsd.edu,8;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,"University of California, San Diego;Yale University;Duke University;University of California, San Diego",11;64;47;11,31;8;20;31,5;1,9/25/19,3,0,2,0,0,0,359;122;405;282,24;30;52;34,6;7;12;7,29;5;35;20,m;m
4126,ICLR,2020,"INFERENCE, PREDICTION, AND ENTROPY RATE OF CONTINUOUS-TIME, DISCRETE-EVENT PROCESSES",Sarah Marzen;James P. Crutchfield,smarzen@cmc.edu;chaos@cse.ucdavis.edu,1;3;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:N/A:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,"Central Methodist College;University of California, Davis",-1;79,-1;55,11,9/25/19,0,0,0,0,0,0,271;9665,44;315,11;48,15;626,f;m
4127,ICLR,2020,Scale-Equivariant Neural Networks with Decomposed Convolutional Filters,Wei Zhu;Qiang Qiu;Robert Calderbank;Guillermo Sapiro;Xiuyuan Cheng,zhu@math.duke.edu;qiang.qiu@duke.edu;robert.calderbank@duke.edu;guillermo.sapiro@duke.edu;xiuyuan.cheng@duke.edu,6;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,1,5,0,yes,9/25/19,Duke University;Duke University;Duke University;Duke University;Duke University,47;47;47;47;47,20;20;20;20;20,,9/24/19,2,1,0,0,0,0,566;31;20477;44214;405,202;16;409;650;52,13;3;54;86;12,14;5;2070;3902;35,m;m
4128,ICLR,2020,Dynamic Scale Inference by Entropy Minimization,Dequan Wang;Evan Shelhamer;Bruno Olshausen;Trevor Darrell,dqwang@eecs.berkeley.edu;shelhamer@cs.berkeley.edu;baolshausen@berkeley.edu;trevor@eecs.berkeley.edu,3;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5,13;13;13;13,2,8/8/19,3,3,0,0,0,0,1001;27530;15236;90979,23;26;133;559,11;14;38;112,199;4172;1160;11527,m;m
4129,ICLR,2020,The Effect of Residual Architecture on the Per-Layer Gradient of Deep Networks,Etai Littwin;Lior Wolf,etai.littwin@gmail.com;wolf@fb.com,1;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,1,yes,9/25/19,Tel Aviv University;Facebook,35;-1,188;-1,,9/25/19,0,0,0,0,0,0,39;458,11;73,4;11,5;54,m;m
4130,ICLR,2020,City Metro Network Expansion with Reinforcement Learning,Yu Wei;Minjia Mao;Xi Zhao;Jianhua Zou,weiyu123112@163.com;maominjia@foxmail.com;zhaoxi1@mail.xjtu.edu.cn;jhzou@sei.xjtu.edu.cn,3;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,Xi'an Jiaotong University;Foxmail;Xi'an Jiaotong University;Xi'an Jiaotong University,481;-1;481;481,555;-1;555;555,,9/25/19,0,0,0,0,0,0,587;0;9;622,154;2;18;42,11;0;2;9,32;0;0;55,m;m
4131,ICLR,2020,Dropout: Explicit Forms and Capacity Control,Raman Arora;Peter L. Bartlett;Poorya Mianjy;Nathan Srebro,arora@cs.jhu.edu;bartlett@cs.berkeley.edu;mianjy@jhu.edu;nati@ttic.edu,1;1;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,5,0,yes,9/25/19,Johns Hopkins University;University of California Berkeley;Johns Hopkins University;Toyota Technological Institute at Chicago,73;5;73;-1,12;13;12;-1,8,9/25/19,3,3,0,0,0,2,2736;72;3;13596,85;22;2;176,22;4;1;52,448;5;2;1618,m;m
4132,ICLR,2020,Verification of Generative-Model-Based Visual Transformations,Matthew Mirman;Timon Gehr;Martin Vechev,matthew.mirman@inf.ethz.ch;timon.gehr@inf.ethz.ch;martin.vechev@inf.ethz.ch,3;3;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,6,0,yes,9/25/19,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10,13;13;13,5,9/25/19,0,0,0,0,0,0,493;740;4246,15;22;153,4;10;36,64;103;467,m;m
4133,ICLR,2020,Beyond GANs: Transforming without a Target Distribution,Matthew Amodio;David van Dijk;Ruth Montgomery;Guy Wolf;Smita Krishnaswamy,matthew.amodio@yale.edu;david.vandijk@yale.edu;ruth.montgomery@yale.edu;guy.wolf@umontreal.ca;smita.krishnaswamy@yale.edu,6;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,3,0,yes,9/25/19,Yale University;Yale University;Yale University;University of Montreal;Yale University,64;64;64;128;64,8;8;8;85;8,5;4;7,9/25/19,0,0,0,0,0,0,121;772;5531;39;1689,20;51;123;16;24,5;11;43;1;8,8;45;337;4;67,m;f
4134,ICLR,2020,On Variational Learning of Controllable Representations for Text without Supervision,Peng Xu;Yanshuai Cao;Jackie Chi Kit Cheung,pxu4@ualberta.ca;yanshuaicao@gmail.com;jcheung@cs.mcgill.ca,8;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,1,yes,9/25/19,University of Alberta;;McGill University,100;-1;86,136;-1;42,5,5/28/19,2,2,0,0,3,0,1060;309;704,110;20;61,18;7;14,56;35;74,m;m
4135,ICLR,2020,Demystifying Graph Neural Network Via Graph Filter Assessment,Yewen Wang;Ziniu Hu;Yusong Ye;Yizhou Sun,wyw10804@gmail.com;bull@cs.ucla.edu;yusongye@g.ucla.edu;yzsun@cs.ucla.edu,8;1;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,3,6,0,yes,9/25/19,"University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles",20;20;20;20,17;17;17;17,10,9/25/19,1,1,1,0,0,1,8;132;5;6951,7;24;8;188,2;6;1;38,0;16;1;766,u;f
4136,ICLR,2020,Individualised Dose-Response Estimation using Generative Adversarial Nets,Ioana Bica;James Jordon;Mihaela van der Schaar,ioana.bica@eng.ox.ac.uk;james.jordon@wolfson.ox.ac.uk;mschaar@turing.ac.uk,1;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,5,0,yes,9/25/19,University of Oxford;University of Oxford;Alan Turing Institute,50;50;-1,1;1;-1,5;4,9/25/19,0,0,0,0,0,0,12;844;8828,7;105;643,3;16;42,1;33;547,f;f
4137,ICLR,2020,"``Best-of-Many-Samples"" Distribution Matching""",Apratim Bhattacharyya;Mario Fritz;Bernt Schiele,abhattac@mpi-inf.mpg.de;fritz@cispa.saarland;schiele@mpi-inf.mpg.de,3;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,0,0,yes,9/25/19,"Saarland Informatics Campus, Max-Planck Institute;CISPA Helmholtz Center for Information Security;Saarland Informatics Campus, Max-Planck Institute",-1;143;-1,-1;1397;-1,5;4,9/25/19,0,0,0,0,0,0,152;7800;41307,18;198;503,7;46;99,18;1012;5119,m;m
4138,ICLR,2020,Swoosh! Rattle! Thump! - Actions that Sound,Dhiraj Gandhi;Abhinav Gupta;Lerrel Pinto,g.prakashchand@gmail.com;abhinavg@cs.cmu.edu;lerrel.pinto@gmail.com,6;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;University of California Berkeley,1;1;5,27;27;13,,9/25/19,0,0,0,0,0,0,396;160;1360,20;55;33,8;6;12,22;6;103,m;m
4139,ICLR,2020,Near-Zero-Cost Differentially Private Deep Learning with Teacher Ensembles,Lichao Sun;Yingbo Zhou;Jia Li;Richard Socher;Philip S. Yu;Caiming Xiong,james.lichao.sun@gmail.com;yingbo.zhou@salesforce.com;jia.li@salesforce.com;rsocher@salesforce.com;psyu@uic.edu;cxiong@salesforce.com,1;1;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,3,0,yes,9/25/19,"University of Illinois, Chicago;SalesForce.com;SalesForce.com;SalesForce.com;University of Illinois, Chicago;SalesForce.com",56;-1;-1;-1;56;-1,254;-1;-1;-1;254;-1,,9/25/19,0,0,0,0,0,0,258;1593;187;53531;61969;6301,24;48;90;180;1580;156,6;16;7;49;114;31,14;216;11;8917;5816;1045,m;m
4140,ICLR,2020,On the Tunability of Optimizers in Deep Learning,Prabhu Teja S*;Florian Mai*;Thijs Vogels;Martin Jaggi;Francois Fleuret,prabhu.teja@idiap.ch;florian.mai@idiap.ch;thijs.vogels@epfl.ch;martin.jaggi@epfl.ch;francois.fleuret@idiap.ch,3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,10,0,yes,9/25/19,Idiap Research Institute;Idiap Research Institute;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Idiap Research Institute,-1;-1;481;481;-1,-1;-1;38;38;-1,,9/25/19,3,1,0,0,0,0,5;28;163;3856;5015,2;11;7;114;136,2;3;4;27;29,0;0;23;577;522,m;m
4141,ICLR,2020,GUIDEGAN:  ATTENTION  BASED  SPATIAL  GUIDANCE FOR  IMAGE-TO-IMAGE TRANSLATION,Yu Lin;Yigong Wang;Yifan Li;Zhuoyi Wang;Yang Gao;Latifur Khan,yxl163430@utdallas.edu;yxw158830@utdallas.edu;yli@utdallas.edu;zhuoyi.wang1@utdallas.edu;yxg122530@utdallas.edu;lkhan@utdallas.edu,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,5,0,yes,9/25/19,"University of Texas, Dallas;University of Texas, Dallas;University of Texas, Dallas;University of Texas, Dallas;University of Texas, Dallas;University of Texas, Dallas",86;86;86;86;86;86,319;319;319;319;319;319,5;4,9/25/19,0,0,0,0,0,0,106;49;10;-1;539;5518,45;31;17;-1;172;403,3;4;1;-1;13;40,2;4;0;0;17;407,f;m
4142,ICLR,2020,ROBUST GENERATIVE ADVERSARIAL NETWORK,Shufei Zhang;Zhuang Qian;Kaizhu Huang;Rui Zhang;Jimin Xiao,shufei.zhang@xjtlu.edu.cn;qz2009425@gmail.com;kaizhu.huang@xjtlu.edu.cn;rui.zhang02@xjtlu.edu.cn;jimin.xiao@xjtlu.edu.cn,1;3;3,I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:N/A:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,Tsinghua University;Xi'an Jiaotong-Liverpool University;Tsinghua University;Tsinghua University;Tsinghua University,8;-1;8;8;8,23;-1;23;23;23,5;4;8,9/25/19,0,0,0,0,0,0,0;122;2763;-1;467,4;25;194;-1;68,0;6;28;-1;12,0;4;192;-1;36,u;u
4143,ICLR,2020,Decoupling Hierarchical Recurrent Neural Networks With Locally Computable Losses,Asier Mujika;Felix Weissenberger;Angelika Steger,asierm@inf.ethz.ch;felix.weissenberger@inf.ethz.ch;steger@inf.ethz.ch,1;1;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,1,0,yes,9/25/19,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10,13;13;13,,9/25/19,0,0,0,0,0,0,89;47;3255,10;13;204,4;5;28,10;2;327,m;f
4144,ICLR,2020,Branched Multi-Task Networks: Deciding What Layers To Share,Simon Vandenhende;Stamatios Georgoulis;Bert De Brabandere;Luc Van Gool,simon.vandenhende@kuleuven.be;georgous@ee.ethz.ch;bert.debrabandere@esat.kuleuven.be;vangool@vision.ee.ethz.ch,3;6;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,5,0,yes,9/25/19,KU Leuven;Swiss Federal Institute of Technology;KU Leuven;Swiss Federal Institute of Technology,118;10;118;10,45;13;45;13,,4/5/19,12,8,6,0,0,0,30;451;820;82593,9;24;15;1267,3;8;11;123,0;56;112;10432,m;m
4145,ICLR,2020,Variational pSOM: Deep Probabilistic Clustering with Self-Organizing Maps,Laura Manduchi;Matthias Hüser;Gunnar Rätsch;Vincent Fortuin,lauraman@student.ethz.ch;matthias.hueser@inf.ethz.ch;gunnar.ratsch@ratschlab.org;fortuin@inf.ethz.ch,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,6,0,yes,9/25/19,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;MSKCC New York;Swiss Federal Institute of Technology,10;10;-1;10,13;13;-1;13,,9/25/19,1,0,0,0,0,0,1;176;9598;79,2;15;244;18,1;3;52;6,0;8;784;5,f;m
4146,ICLR,2020,Tensor Graph Convolutional Networks for Prediction on Dynamic Graphs,Osman Asif Malik;Shashanka Ubaru;Lior Horesh;Misha E. Kilmer;Haim Avron,osman.malik.87@gmail.com;shashanka.ubaru@ibm.com;lhoresh@us.ibm.com;misha.kilmer@tufts.edu;haimav@tauex.tau.ac.il,3;6;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,5,0,yes,9/25/19,"University of Colorado, Boulder;International Business Machines;International Business Machines;Tufts University;Tel Aviv University",44;-1;-1;172;35,123;-1;-1;139;188,3;10,9/25/19,0,0,0,0,0,0,28;163;1059;3176;1351,11;23;81;93;60,3;8;17;26;18,5;15;82;334;184,m;m
4147,ICLR,2020,Mint: Matrix-Interleaving for Multi-Task Learning,Tianhe Yu;Saurabh Kumar;Eric Mitchell;Abhishek Gupta;Karol Hausman;Sergey Levine;Chelsea Finn,tianheyu@cs.stanford.edu;szk@stanford.edu;eric.anthony.mitchell95@gmail.com;abhigupta@berkeley.edu;hausmankarol@gmail.com;svlevine@eecs.berkeley.edu;cbfinn@cs.stanford.edu,3;3;3;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Stanford University;Stanford University;Stanford University;University of California Berkeley;Google;University of California Berkeley;Stanford University,4;4;4;5;-1;5;4,4;4;4;13;-1;13;4,8,9/25/19,0,0,0,0,0,0,586;321;41;238;872;24893;7879,17;170;25;116;62;310;101,8;9;3;7;15;74;34,68;19;3;23;53;3235;1060,m;f
4148,ICLR,2020,Effective and Robust Detection of Adversarial Examples via Benford-Fourier Coefficients,Chengcheng Ma;Baoyuan Wu;Shibiao Xu;Yanbo Fan;Yong Zhang;Xiaopeng Zhang;Zhifeng Li,machengcheng2016@gmail.com;wubaoyuan1987@gmail.com;shibiao.xu@ia.ac.cn;fanyanbo0124@gmail.com;zhangyong201303@gmail.com;xiaopeng.zhang@ia.ac.cn;michaelzfli@tencent.com,6;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0,yes,9/25/19,"Institute of automation, Chinese academy of science, Chinese Academy of Sciences;Tencent AI Lab;Institute of automation, Chinese academy of science, Chinese Academy of Sciences;Tencent AI Lab;;Institute of automation, Chinese academy of science, Chinese Academy of Sciences;Tencent AI Lab",59;-1;59;-1;-1;59;-1,1397;-1;1397;-1;-1;1397;-1,4,9/25/19,0,0,0,0,0,0,60;834;287;82;6068;489;470,15;50;32;12;541;118;37,3;18;6;5;32;12;4,3;126;26;11;467;47;87,m;m
4149,ICLR,2020,Multi-objective Neural Architecture Search via Predictive Network Performance Optimization,Han Shi;Renjie Pi;Hang Xu;Zhenguo Li;James T. Kwok;Tong Zhang,hshiac@cse.ust.hk;pipilu@connect.hku.hk;xbjxh@live.com;li.zhenguo@huawei.com;jamesk@cse.ust.hk;tongzhang@tongzhang-ml.org,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,8,0,yes,9/25/19,The Hong Kong University of Science and Technology;The University of Hong Kong;Huawei Technologies Ltd.;Huawei Technologies Ltd.;The Hong Kong University of Science and Technology;The Hong Kong University of Science and Technology,39;92;-1;-1;39;39,47;35;-1;-1;47;47,11;10,9/25/19,6,2,3,0,0,0,51;23;246;86;9752;164,15;28;45;23;197;78,4;2;7;5;51;5,5;2;8;6;1062;14,m;m
4150,ICLR,2020,CONFEDERATED MACHINE LEARNING ON HORIZONTALLY AND VERTICALLY SEPARATED MEDICAL DATA FOR LARGE-SCALE HEALTH SYSTEM INTELLIGENCE,Dianbo Liu;Tim Miller;Kenneth Mandl,dianbo.liu@childrens.harvard.edu;timothy.miller@childrens.harvard.edu;kenneth.mandl@childrens.harvard.edu,3;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,Harvard University;Harvard University;Harvard University,39;39;39,7;7;7,,9/25/19,0,0,0,0,0,0,123;6;8853,32;10;370,6;1;51,9;0;409,m;m
4151,ICLR,2020,On Evaluating Explainability Algorithms,Gokula Krishnan Santhanam;Ali Alami-Idrissi;Nuno Mota;Anika Schumann;Ioana Giurgiu,gst@zurich.ibm.com;aai@zurich.ibm.com;nuno.motagoncalves@epfl.ch;ikh@zurich.ibm.com;igi@zurich.ibm.com,1;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,International Business Machines;International Business Machines;Swiss Federal Institute of Technology Lausanne;International Business Machines;International Business Machines,-1;-1;481;-1;-1,-1;-1;38;-1;-1,,9/25/19,1,0,0,0,0,0,77;0;26;339;560,4;1;16;37;30,2;0;3;11;8,3;0;3;16;58,m;f
4152,ICLR,2020,FRICATIVE PHONEME DETECTION WITH ZERO DELAY,Metehan Yurt;Alberto N. Escalante B.;Veniamin I. Morgenshtern,metehan.yurt@fau.de;alberto.escalante@sivantos.com;veniamin.morgenshtern@fau.de,3;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,3,0,yes,9/25/19,University of Erlangen-Nuremberg;Sivantos;University of Erlangen-Nuremberg,205;-1;205,182;-1;182,,9/25/19,0,0,0,0,0,0,1;18;345,2;13;25,1;3;9,0;1;29,m;m
4153,ICLR,2020,Deep Mining: Detecting Anomalous Patterns in Neural Network Activations with Subset Scanning,Skyler Speakman;Celia Cintas;Victor Akinwande;Srihari Sridharan;Edward McFowland III,skyler@ke.ibm.com;celia.cintas@ibm.com;victor.akinwande1@ibm.com;sriharis.sridharan@ke.ibm.com;mcfowland@umn.edu,3;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,"International Business Machines;International Business Machines;International Business Machines;International Business Machines;University of Minnesota, Minneapolis",-1;-1;-1;-1;59,-1;-1;-1;-1;79,4,9/25/19,0,0,0,0,0,0,206;1;16;80;141,21;8;14;21;17,6;1;2;5;5,16;0;5;3;9,m;m
4154,ICLR,2020,"GRAPHS, ENTITIES, AND STEP MIXTURE",Kyuyong Shin;Wonyoung Shin;Jung-Woo Ha;Sunyoung Kwon,p37329@gmail.com;wyshin@kaist.ac.kr;jungwoo.ha@navercorp.com;sunny.kwon@navercorp.com,3;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0,yes,9/25/19,NAVER;Korea Advanced Institute of Science and Technology;NAVER;NAVER,-1;481;-1;-1,-1;110;-1;-1,10;8,9/25/19,0,0,0,0,0,0,25;6;221;102,11;6;20;20,4;2;8;5,4;0;27;8,m;f
4155,ICLR,2020,Learning Human Postural Control with Hierarchical Acquisition Functions,Nils Rottmann;Tjasa Kunavar;Jan Babic;Jan Peters;Elmar Rueckert,rottmann@rob.uni-luebeck.de;tjasa.kunavar@ijs.si;jan.babic@ijs.si;mail@jan-peters.net;rueckert@ai-lab.science,1;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A,Reject,0,3,0,yes,9/25/19,University of Luebeck;Jozef Stefan institute;Jozef Stefan institute;TU Darmstadt;Universität zu Lübeck,266;-1;-1;64;266,1397;-1;-1;289;1397,11,9/25/19,0,0,0,0,0,0,157;0;534;123;0,12;1;59;37;2,5;0;13;3;0,10;0;18;4;0,m;m
4156,ICLR,2020,Unknown-Aware Deep Neural Network,Lei Cao;Yizhou Yan;Samuel Madden;Elke Rundensteiner,lcao@csail.mit.edu;yyan2@wpi.edu;madden@csail.mit.edu;rundenst@cs.wpi.edu,8;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Massachusetts Institute of Technology;Worcester Polytechnic Institute;Massachusetts Institute of Technology;Worcester Polytechnic Institute,2;172;2;172,5;628;5;628,8,9/25/19,0,0,0,0,0,0,7;148;34124;7902,30;37;367;596,2;6;80;44,0;9;3710;515,m;f
4157,ICLR,2020,Low Bias Gradient Estimates for Very Deep Boolean Stochastic Networks,Adeel Pervez;Taco Cohen;Efstratios Gavves,a.a.pervez@uva.nl;tacos@qti.qualcomm.com;efstratios.gavves@gmail.com,3;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,11,0,yes,9/25/19,"University of Amsterdam;Qualcomm Inc, QualComm;University of Amsterdam",172;-1;172,62;-1;62,,9/25/19,0,0,0,0,0,0,0;19;3381,2;6;73,0;2;26,0;2;473,m;m
4158,ICLR,2020,SoftLoc: Robust Temporal Localization under Label Misalignment,Julien Schroeter;Kirill Sidorov;Dave Marshall,schroeterj1@cardiff.ac.uk;sidorovk@cardiff.ac.uk;marshallad@cardiff.ac.uk,3;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,Cardiff University;Cardiff University;Cardiff University,172;172;172,196;196;196,,9/25/19,0,0,0,0,0,0,9;140;472,6;37;55,1;6;7,1;7;41,m;m
4159,ICLR,2020,Learning Video Representations using Contrastive Bidirectional Transformer,Chen Sun;Fabien Baradel;Kevin Murphy;Cordelia Schmid,chensun@google.com;fabien.baradel@insa-lyon.fr;kpmurphy@google.com;cordelias@google.com,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,3,0,yes,9/25/19,Google;INSA de Lyon;Google;Google,-1;481;-1;-1,-1;1397;-1;-1,2,9/25/19,27,8,5,1,0,4,574;255;7077;73934,52;9;82;430,8;7;32;114,78;32;541;10183,m;f
4160,ICLR,2020,ShardNet: One Filter Set to Rule Them All,Saumya Jetley;Tommaso Cavallari;Philip Torr;Stuart Golodetz,sjetley@robots.ox.ac.uk;tommaso.cavallari@five.ai;phil@five.ai;stuart@five.ai,3;3;1,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,7,0,yes,9/25/19,University of Oxford;FiveAI;FiveAI;FiveAI,50;-1;-1;-1,1;-1;-1;-1,2,9/25/19,0,0,0,0,0,0,307;178;1951;1747,15;18;30;29,9;7;9;9,29;14;459;464,f;m
4161,ICLR,2020,Explaining Time Series by Counterfactuals,Sana Tonekaboni;Shalmali Joshi;David Duvenaud;Anna Goldenberg,stonekaboni@cs.toronto.edu;shalmali@vectorinstitute.ai;duvenaud@cs.toronto.edu;anna.goldenberg@utoronto.ca,6;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,"Department of Computer Science, University of Toronto;Vector Institute;Department of Computer Science, University of Toronto;Toronto University",18;-1;18;18,18;-1;18;18,,9/25/19,0,0,0,0,0,0,48;55;6070;393,5;18;76;21,3;4;29;8,2;3;763;25,f;f
4162,ICLR,2020,On the Linguistic Capacity of Real-time Counter Automata,William Merrill,vikingarnir.will@gmail.com,6;1;6,I do not know much about this area.:N/A:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Allen Institute for Artificial Intelligence,-1,-1,3;1,9/25/19,1,0,1,0,0,0,23,8,2,5,m
4163,ICLR,2020,The fairness-accuracy landscape of neural classifiers,Susan Wei;Marc Niethammer,susan.wei@unimelb.edu.au;mn@cs.unc.edu,1;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,5,0,yes,9/25/19,"The University of Melbourne;University of North Carolina, Chapel Hill",118;73,32;54,4;7,9/25/19,0,0,0,0,0,0,311;3581,19;187,6;29,17;253,f;m
4164,ICLR,2020,Corpus Based Amharic Sentiment Lexicon Generation,Girma Neshir;Andeas Rauber;and Solomon Atnafu,girma1978@gmail.com;rauber@ifs.tuwien.ac.at;solomon.atnafu@aau.edu.et,1;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,0,0,yes,9/25/19,Addis Ababa University;TU Wien Vienna University of Technology;Addis Ababa University,481;100;481,1397;360;1397,,9/25/19,0,0,0,0,0,0,34;5277;174,2;420;46,2;38;7,5;378;9,m;m
4165,ICLR,2020,MaskConvNet: Training Efficient ConvNets from Scratch via Budget-constrained Filter Pruning,Raden Mu'az Mun'im;Jie Lin;Vijay Chandrasekhar;Koichi Shinoda,raden.m.muaz@gmail.com;lin-j@i2r.a-star.edu.sg;vijay@i2r.a-star.edu.sg;shinoda@ks.cs.titech.ac.jp,3;3;3,I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0,yes,9/25/19,Universiti Teknologi Malaysia;A*STAR;A*STAR;Tokyo Institute of Technology,481;-1;-1;172,674;-1;-1;299,,9/25/19,0,0,0,0,0,0,5;74;3046;6,2;52;110;16,1;4;28;1,0;5;227;0,m;m
4166,ICLR,2020,Curvature-based Robustness Certificates against Adversarial Examples,Sahil Singla;Soheil Feizi,ssingla@cs.umd.edu;sfeizi@cs.umd.edu,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0,yes,9/25/19,"University of Maryland, College Park;University of Maryland, College Park",12;12,91;91,4;1;9,9/25/19,0,0,0,0,0,0,427;138,41;40,11;6,35;13,m;m
4167,ICLR,2020,Pipelined Training with Stale Weights of Deep Convolutional Neural Networks,Lifu Zhang;Tarek S. Abdelrahman,lifu.zhang@mail.utoronto.ca;tsa@ece.utoronto.ca,3;6;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,6,0,yes,9/25/19,Toronto University;Toronto University,18;18,18;18,,9/25/19,0,0,0,0,0,0,4;1098,7;93,1;15,0;87,m;m
4168,ICLR,2020,Information Plane Analysis of Deep Neural Networks via Matrix--Based Renyi's Entropy and Tensor Kernels,Kristoffer Wickstrøm;Sigurd Løkse;Michael Kampffmeyer;Shujian Yu;Jose Principe;Robert Jenssen,kristoffer.k.wickstrom@uit.no;sigurd.lokse@uit.no;michael.c.kampffmeyer@uit.no;yusjlcy9011@cnel.ufl.edu;principe@cnel.ufl.edu;robert.jenssen@uit.no,3;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,UiT The Arctic University of Norway;UiT The Arctic University of Norway;UiT The Arctic University of Norway;University of Florida;University of Florida;UiT The Arctic University of Norway,-1;-1;-1;128;128;-1,419;419;419;174;174;419,8,9/25/19,5,4,2,0,0,1,30;102;437;353;17292;2106,6;14;39;56;1022;141,4;6;10;10;58;20,2;4;38;12;1277;171,m;m
4169,ICLR,2020,Disentangled Representation Learning with Sequential Residual Variational Autoencoder,Nanxiang Li;Shabnam Ghaffarzadegan;Liu Ren,nanxiang.li@us.bosch.com;shabnam.ghaffarzadegan@us.bosch.com;liu.ren@us.bosch.com,3;8;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Bosch;Bosch;Bosch,-1;-1;-1,-1;-1;-1,5,9/25/19,0,0,0,0,0,0,216;149;1128,25;23;156,7;8;15,7;6;60,m;m
4170,ICLR,2020,MULTIPOLAR: Multi-Source Policy Aggregation for Transfer Reinforcement Learning between Diverse Environmental Dynamics,Mohammadamin Barekatain;Ryo Yonetani;Masashi Hamaya,m.barekatain@tum.de;ryo.yonetani@sinicx.com;masashi.hamaya@sinicx.com,1;8;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0,yes,9/25/19,Technical University Munich;OMRON SINIC X;OMRON SINIC X,53;-1;-1,43;-1;-1,,9/25/19,0,0,0,0,0,0,284;449;98,10;47;12,4;12;5,7;32;4,m;m
4171,ICLR,2020,Simultaneous Classification and Out-of-Distribution Detection Using Deep Neural Networks,Aristotelis-Angelos Papadopoulos;Nazim Shaikh;Jiamian Wang;Mohammad Reza Rajati,aristotp@usc.edu;nshaikh@usc.edu;jiamianw@usc.edu;rajati@usc.edu,6;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,11,0,yes,9/25/19,University of Southern California;University of Southern California;University of Southern California;University of Southern California,31;31;31;31,62;62;62;62,,9/25/19,0,0,0,0,0,0,12;4;17;202,9;4;6;33,3;1;3;9,0;0;0;3,m;m
4172,ICLR,2020,"Learning vector representation of local content and matrix representation of local motion, with implications for V1",Ruiqi Gao;Jianwen Xie;Siyuan Huang;Yufan Ren;Song-Chun Zhu;Ying Nian Wu,ruiqigao@ucla.edu;jianwen@ucla.edu;huangsiyuan@ucla.edu;3160104704@zju.edu.cn;sczhu@stat.ucla.edu;ywu@stat.ucla.edu,3;1;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,5,0,yes,9/25/19,"University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles;Zhejiang University;University of California, Los Angeles;University of California, Los Angeles",20;20;20;56;20;20,17;17;17;107;17;17,,1/24/19,0,0,0,0,0,0,224;678;252;1;83;5865,30;58;37;3;41;276,9;15;10;1;5;38,17;50;22;0;7;596,f;m
4173,ICLR,2020,Data Valuation using Reinforcement Learning,Jinsung Yoon;Sercan O. Arik;Tomas Pfister,jsyoon0823@gmail.com;soarik@google.com;tpfister@google.com,6;6;3,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,"University of California, Los Angeles;Google;Google",20;-1;-1,17;-1;-1,,9/25/19,1,1,0,0,0,0,558;1391;2335,55;54;47,13;17;16,58;119;285,m;m
4174,ICLR,2020,Reject Illegal Inputs: Scaling Generative Classifiers with Supervised Deep Infomax,Xin WANG;SiuMing Yiu,xwang@cs.hku.hk;smyiu@cs.hku.hk,3;8;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,3,1,yes,9/25/19,The University of Hong Kong;The University of Hong Kong,92;92,35;35,5;4,9/25/19,1,0,1,1,0,1,17;746,49;26,2;7,1;63,u;m
4175,ICLR,2020,Surrogate-Based Constrained Langevin Sampling With Applications to Optimal Material Configuration Design,Thanh V Nguyen;Youssef Mroueh;Samuel C. Hoffman;Payel Das;Pierre Dognin;Giuseppe Romano;Chinmay Hegde,thanhng@iastate.edu;mroueh@us.ibm.com;shoffman@ibm.com;daspa@us.ibm.com;pdognin@us.ibm.com;romanog@mit.edu;chinmay@iastate.edu,3;6;6,I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Iowa State University;International Business Machines;International Business Machines;International Business Machines;International Business Machines;Massachusetts Institute of Technology;Iowa State University,172;-1;-1;-1;-1;2;172,399;-1;-1;-1;-1;5;399,1,9/25/19,0,0,0,0,0,0,484;916;153;545;119;282;168,40;53;13;67;27;44;28,7;11;6;11;6;9;4,17;151;14;19;6;16;13,m;m
4176,ICLR,2020,VILD: Variational Imitation Learning with Diverse-quality Demonstrations,Voot Tangkaratt;Bo Han;Mohammad Emtiyaz Khan;Masashi Sugiyama,voot.tangkaratt@riken.jp;bo.han@riken.jp;emtiyaz.khan@riken.jp;sugi@k.u-tokyo.ac.jp,6;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,4,0,yes,9/25/19,RIKEN;RIKEN;RIKEN;The University of Tokyo,-1;-1;-1;56,-1;-1;-1;36,10,9/15/19,0,0,0,0,0,0,210;3912;669;11910,25;353;67;711,8;27;16;53,19;299;65;1314,m;m
4177,ICLR,2020,OPTIMAL BINARY QUANTIZATION FOR DEEP NEURAL NETWORKS,Hadi Pouransari;Oncel Tuzel,mpouransari@apple.com;onceltuzel@gmail.com,3;3;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,1,yes,9/25/19,Apple;MERL,-1;-1,-1;-1,1,9/25/19,0,0,0,0,0,0,279;7991,41;75,9;33,5;1042,m;m
4178,ICLR,2020,Learning Through Limited Self-Supervision: Improving Time-Series Classification Without Additional Data via Auxiliary Tasks,Ian Fox;Harry Rubin-Falcone;Jenna Wiens,ifox@umich.edu;hrf@umich.edu;wiensj@umich.edu,1;3;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,4,0,yes,9/25/19,University of Michigan;University of Michigan;University of Michigan,8;8;8,21;21;21,,9/25/19,0,0,0,0,0,0,31;108;350,17;35;37,4;6;9,4;3;22,m;f
4179,ICLR,2020,Robust Natural Language Representation Learning for Natural Language Inference by Projecting Superficial Words out,Wanyun Cui;Guangyu Zheng;Wei Wang,cui.wanyun@sufe.edu.cn;simonzgy@outlook.com;weiwang1@fudan.edu.cn,1;3;1,I have published in this field for several years.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0,yes,9/25/19,Shanghai University of Finance and Economics;Fudan University;Fudan University,266;79;79,1397;109;109,3,9/25/19,0,0,0,0,0,0,396;16;-1,14;9;-1,6;2;-1,61;1;0,m;f
4180,ICLR,2020,DSReg: Using Distant Supervision as a Regularizer,Yuxian Meng;Muyu Li;Xiaoya Li;Wei Wu;Fei Wu;Jiwei Li,yuxian_meng@shannonai.com;muyu_li@shannonai.com;xiaoya_li@shannonai.com;wei_wu@shannonai.com;wufei@zju.edu.cn;jiwei_li@shannonai.com,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,2,0,yes,9/25/19,Shannon.AI;Shannon.AI;Shannon.AI;Shannon.AI;Zhejiang University;Shannon.AI,-1;-1;-1;-1;56;-1,-1;-1;-1;-1;107;-1,3,5/28/19,1,1,0,0,0,0,54;29;89;222;-1;-1,14;7;23;37;-1;-1,4;2;6;8;-1;-1,4;2;4;16;0;0,m;m
4181,ICLR,2020,MxPool: Multiplex Pooling for Hierarchical Graph Representation Learning,Yanyan Liang;Yanfeng Zhang;Fangjing Wang;Qian Xu,13354227340@163.com;zhangyf@mail.neu.edu.cn;inggraph@qq.com;xuqian1286@163.com,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,163;Northeastern University;;163,-1;16;-1;-1,-1;906;-1;-1,10,9/25/19,0,0,0,0,0,0,41;773;-1;13,22;63;-1;15,4;12;-1;2,5;51;-1;0,f;f
4182,ICLR,2020,Non-linear System Identification from Partial Observations via Iterative Smoothing and Learning,Kunal Menda;Jean de Becdelièvre;Jayesh K Gupta;Ilan Kroo;Mykel J. Kochenderfer;Zachary Manchester,kmenda@stanford.edu;jeandb@stanford.edu;jkg@cs.stanford.edu;kroo@stanford.edu;mykel@stanford.edu;zacmanchester@stanford.edu,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,Stanford University;Stanford University;Stanford University;Stanford University;Stanford University;Stanford University,4;4;4;4;4;4,4;4;4;4;4;4,1,9/25/19,0,0,0,0,0,0,36;0;394;389;3514;232,9;2;32;16;237;36,4;0;8;8;25;10,1;0;29;26;336;11,m;m
4183,ICLR,2020,Policy Message Passing: A New Algorithm for Probabilistic Graph Inference,Zhiwei Deng;Greg Mori,zhiweid@princeton.edu;mori@cs.sfu.ca,1;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0,yes,9/25/19,Princeton University;Simon Fraser University,31;64,6;272,10,9/25/19,0,0,0,0,0,0,551;9711,17;197,9;45,74;817,m;m
4184,ICLR,2020,Learning Mahalanobis Metric Spaces via Geometric Approximation Algorithms,Diego Ihara;Neshat Mohammadi;Anastasios Sidiropoulos,dihara@gmail.com;nmoham24@uic.edu;sidiropo@uic.edu,3;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,"University of Illinois, Chicago;University of Illinois, Chicago;University of Illinois, Chicago",56;56;56,254;254;254,4,5/24/19,1,0,0,0,0,0,20;7;867,3;6;97,1;2;14,2;0;58,m;m
4185,ICLR,2020,On Understanding Knowledge Graph Representation,Carl Allen*;Ivana Balazevic*;Timothy M Hospedales,carl.allen@ed.ac.uk;ivana.balazevic@ed.ac.uk;t.hospedales@ed.ac.uk,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,6,1,yes,9/25/19,University of Edinburgh;University of Edinburgh;University of Edinburgh,33;33;33,30;30;30,3;10,9/25/19,4,1,2,0,0,0,119;99;384,7;8;31,5;5;9,21;22;34,m;m
4186,ICLR,2020,Learning to Prove Theorems by Learning to Generate Theorems,Mingzhe Wang;Jia Deng,mingzhew@cs.princeton.edu;jiadeng@princeton.edu,6;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,6,0,yes,9/25/19,Princeton University;Princeton University,31;31,6;6,1,9/25/19,3,1,2,0,0,1,22;4083,8;594,3;31,4;142,m;m
4187,ICLR,2020,Augmenting Self-attention with Persistent Memory,Sainbayar Sukhbaatar;Edouard Grave;Guillaume Lample;Herve Jegou;Armand Joulin,sainbar@fb.com;egrave@fb.com;guismay@fb.com;rvj@fb.com;ajoulin@fb.com,3;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,2,0,yes,9/25/19,Facebook;Facebook;Facebook;Facebook;Facebook,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,3,7/2/19,20,12,4,0,0,0,2783;7711;4837;14008;10653,21;57;24;165;74,13;23;18;40;32,312;1136;1005;2386;1533,m;m
4188,ICLR,2020,Realism Index: Interpolation in Generative Models With Arbitrary Prior,Łukasz Struski;Jacek Tabor;Igor Podolak;Aleksandra Nowak;Krzysztof Maziarz,lukasz.struski@uj.edu.pl;jacek.tabor@uj.edu.pl;igor.podolak@uj.edu.pl;aknoow@gmail.com;krzysztof.s.maziarz@gmail.com,3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,6,0,yes,9/25/19,Jagiellonian University;Jagiellonian University;Jagiellonian University;;Jagiellonian University,481;481;481;-1;481,610;610;610;-1;610,5,9/25/19,0,0,0,0,0,0,5;8;149;188;445,8;4;37;23;8,1;1;6;4;2,0;1;9;5;33,m;m
4189,ICLR,2020,Fully Convolutional Graph Neural Networks using Bipartite Graph Convolutions,Marcel Nassar;Xin Wang;Evren Tumer,nassar.marcel@gmail.com;caseus.viridis@gmail.com;nervetumer@gmail.com,3;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,3,0,yes,9/25/19,"Intel;Cerebras Systems, Inc;",-1;-1;-1,-1;-1;-1,10,9/25/19,0,0,0,0,0,0,817;17;48,35;49;12,11;2;5,65;1;3,m;m
4190,ICLR,2020,Selective sampling for accelerating  training of deep neural networks,Berry Weinstein;Shai Fine;Yacov Hel-Or,berry.weinstein@post.idc.ac.il;shai.fine@idc.ac.il;toky@idc.ac.il,1;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,2,0,yes,9/25/19,interdisciplinary center herzliya;interdisciplinary center herzliya;interdisciplinary center herzliya,-1;-1;-1,-1;-1;-1,8,9/25/19,0,0,0,0,0,0,1;2196;1387,3;74;66,1;14;20,0;251;153,m;m
4191,ICLR,2020,Hierarchical Graph-to-Graph Translation for Molecules,Wengong Jin;Regina Barzilay;Tommi Jaakkola,wengong@csail.mit.edu;regina@csail.mit.edu;tommi@csail.mit.edu,3;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2,5;5;5,10,6/11/19,9,6,3,0,0,1,524;12076;22299,24;234;293,7;56;69,63;1215;2326,m;m
4192,ICLR,2020,MelNet: A Generative Model for Audio in the Frequency Domain,Sean Vasquez;Mike Lewis,seanjv@mit.edu;mikelewis@fb.com,6;3;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,3,0,yes,9/25/19,Massachusetts Institute of Technology;Facebook,2;-1,5;-1,5,6/4/19,30,19,10,0,0,2,28;4069,1;104,1;21,2;684,m;m
4193,ICLR,2020,End-to-end learning of energy-based representations for irregularly-sampled signals and images,Ronan Fablet;Lucas Drumetz;François Rousseau,ronan.fablet@imt-atlantique.fr;lucas.drumetz@imt-atlantique.fr;francois.rousseau@imt-atlantique.fr,3;1;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,IMT Atlantique;IMT Atlantique;IMT Atlantique,481;481;481,393;393;393,11,9/25/19,0,0,0,0,0,0,1570;289;2,205;41;5,22;8;1,77;25;0,m;m
4194,ICLR,2020,Reparameterized Variational Divergence Minimization for Stable Imitation,Dilip Arumugam;Debadeepta Dey;Alekh Agarwal;Asli Celikyilmaz;Elnaz Nouri;Eric Horvitz;Bill Dolan,dilip@cs.stanford.edu;dedey@microsoft.com;alekha@microsoft.com;aslicel@microsoft.com;elnouri@microsoft.com;horvitz@microsoft.com;billdol@microsoft.com,1;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,9,0,yes,9/25/19,Stanford University;Microsoft;Microsoft;Microsoft;Microsoft;Microsoft;Microsoft,4;-1;-1;-1;-1;-1;-1,4;-1;-1;-1;-1;-1;-1,5;4,9/25/19,0,0,0,0,0,0,161;1254;5237;2169;31;26670;7135,18;48;109;122;4;546;100,7;16;36;25;2;82;33,6;113;678;182;2;1766;1061,m;m
4195,ICLR,2020,TrojanNet: Exposing the Danger of Trojan Horse Attack on Neural Networks,Chuan Guo;Ruihan Wu;Kilian Q. Weinberger,cg563@cornell.edu;rw565@cornell.edu;kqw4@cornell.edu,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,1,0,yes,9/25/19,Cornell University;Cornell University;Cornell University,7;7;7,19;19;19,4;1,9/25/19,0,0,0,0,0,0,757;240;24205,158;21;165,15;8;54,33;10;3866,m;m
4196,ICLR,2020,SRDGAN: learning the noise prior for Super Resolution with Dual Generative Adversarial Networks,Jingwei GUAN;Cheng PAN;Songnan LI;Dahai YU,jwguan37@gmail.com;pancheng@tcl.com;lisn@tcl.com;dahai.yu@tcl.com,3;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0,yes,9/25/19,;Tcl;Tcl;Tcl,-1;-1;-1;-1,-1;-1;-1;-1,5;4,3/28/19,2,2,0,0,0,0,238;1209;685;53,13;187;49;27,6;17;14;4,16;56;56;3,f;m
4197,ICLR,2020,Detecting malicious PDF using CNN,Raphael Fettaya;Yishay Mansour,raphaelfettaya@gmail.com;mansour.yishay@gmail.com,1;1;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,1,0,yes,9/25/19,Tel Aviv University;Tel Aviv University,35;35,188;188,,9/25/19,0,0,0,0,0,0,0;16365,1;460,0;63,0;1806,m;m
4198,ICLR,2020,Novelty Search in representational space for sample efficient exploration,Ruo Yu Tao;Vincent François-Lavet;Joelle Pineau,ruo.tao@mail.mcgill.ca;vincent.francois-lavet@mail.mcgill.ca;jpineau@cs.mcgill.ca,1;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,McGill University;McGill University;McGill University,86;86;86,42;42;42,,9/25/19,0,0,0,0,0,0,7;314;11328,2;19;267,1;9;46,0;26;1235,m;f
4199,ICLR,2020,"Feature-Robustness, Flatness and Generalization Error for Deep Neural Networks",Henning Petzka;Linara Adilova;Michael Kamp;Cristian Sminchisescu,henning.petzka@gmail.com;adylova.linara.r@gmail.com;info@michaelkamp.org;cristian.sminchisescu@math.lth.se,1;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,8,1,yes,9/25/19,Lund University;;Monash University;Lund University,390;-1;118;390,98;-1;75;98,8,9/25/19,0,0,0,0,0,0,108;44;148;8265,20;9;19;173,4;2;5;43,13;3;6;858,m;m
4200,ICLR,2020,Robust Cross-lingual Embeddings from Parallel Sentences ,Ali Sabet;Prakhar Gupta;Jean-Baptiste Cordonnier;Robert West;Martin Jaggi,asabet@uwaterloo.ca;prakhar.gupta@epfl.ch;jean-baptiste.cordonnier@epfl.ch;robert.west@epfl.ch;martin.jaggi@epfl.ch,3;3;8,I have published in this field for several years.:I carefully checked the experiments.:N/A:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,4,0,yes,9/25/19,University of Waterloo;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne,28;481;481;481;481,235;38;38;38;38,3;6,9/25/19,0,0,0,0,0,0,196;4;125;258;3856,38;6;6;56;114,8;1;3;8;27,3;0;24;16;577,m;m
4201,ICLR,2020,PowerSGD: Powered Stochastic Gradient Descent Methods for Accelerated Non-Convex Optimization,Jun Liu;Beitong Zhou;Weigao Sun;Ruijuan Chen;Claire J. Tomlin;Ye Yuan,j.liu@uwaterloo.ca;zhoubt@hust.edu.cn;sunweigao@outlook.com;ruijuanchen@hust.edu.cn;tomlin@eecs.berkeley.edu;yye@hust.edu.cn,3;3;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,5,0,yes,9/25/19,University of Waterloo;Hong Kong University of Science and Technology;;Hong Kong University of Science and Technology;University of California Berkeley;Hong Kong University of Science and Technology,28;39;-1;39;5;39,235;47;-1;47;13;47,9;8,9/25/19,0,0,0,0,0,0,354;37;14;4;13996;4,129;9;9;6;533;14,9;3;3;1;55;1,31;1;0;0;726;0,m;m
4202,ICLR,2020,Unaligned Image-to-Sequence Transformation with Loop Consistency,Siyang Wang;Justin Lazarow;Kwonjoon Lee;Zhuowen Tu,siw030@ucsd.edu;jlazarow@ucsd.edu;kwl042@ucsd.edu;ztu@ucsd.edu,1;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:N/A:N/A;I have published one or two papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,0,0,yes,9/25/19,"University of California, San Diego;University of California, San Diego;University of California, San Diego;University of California, San Diego",11;11;11;11,31;31;31;31,,9/25/19,0,0,0,0,0,0,59;81;158;585,21;10;5;22,4;3;3;8,5;12;46;54,f;m
4203,ICLR,2020,Progressive Compressed Records: Taking a Byte Out of Deep Learning Data,Michael Kuchnik;George Amvrosiadis;Virginia Smith,mkuchnik@andrew.cmu.edu;gamvrosi@cmu.edu;smithv@cmu.edu,3;6;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,5,0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1,27;27;27,,9/25/19,0,0,0,0,0,0,12;291;80,9;29;5,2;7;2,1;21;10,m;f
4204,ICLR,2020,Training a Constrained Natural Media Painting Agent using Reinforcement Learning ,Biao Jia;Jonathan Brandt;Radomir Mech;Ning Xu;Byungmoon Kim;Dinesh Manocha,biao@cs.umd.edu;jbrandt@adobe.com;rmech@adobe.com;nxu@adobe.com;bmkim@adobe.com;dm@cs.umd.edu,3;1;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,0,0,yes,9/25/19,"University of Maryland, College Park;Adobe Systems;Adobe Systems;Adobe Systems;Adobe Systems;University of Maryland, College Park",12;-1;-1;-1;-1;12,91;-1;-1;-1;-1;91,,9/25/19,0,0,0,0,0,0,48;3583;3620;190;956;24335,12;59;74;50;34;884,4;24;28;5;14;74,1;515;370;10;54;1429,m;m
4205,ICLR,2020,Disentangling Improves VAEs' Robustness to Adversarial Attacks,Matthew Willetts;Alexander Camuto;Stephen Roberts;Chris Holmes,mwilletts@turing.ac.uk;acamuto@turing.ac.uk;sroberts@turing.ac.uk;cholmes@turing.ac.uk,3;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,1,yes,9/25/19,Alan Turing Institute;Alan Turing Institute;Alan Turing Institute;Alan Turing Institute,-1;-1;-1;-1,-1;-1;-1;-1,5;4,6/1/19,2,1,0,0,0,0,13;2;367;363,14;6;40;63,3;1;8;11,0;0;30;28,m;m
4206,ICLR,2020,Exploring Cellular Protein Localization Through Semantic Image Synthesis,Daniel Li;Qiang Ma;Andrew Liu;Justin Cheung;Dana Pe’er;Itsik Pe’er,daniel.li@columbia.edu;ma.qiang@columbia.edu;andrew@ml.berkeley.edu;justin.cheung@stonybrookmedicine.edu;peerster@gmail.com;itsik@cs.columbia.edu,3;3;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,6,0,yes,9/25/19,Columbia University;Columbia University;University of California Berkeley;Renaissance School of Medicine at Stony Brook University;;Columbia University,15;15;5;41;-1;15,16;16;13;304;-1;16,5;2,9/25/19,0,0,0,0,0,0,372;977;437;8;13444;779,23;189;17;11;98;32,7;16;4;1;43;11,25;27;20;0;775;55,m;m
4207,ICLR,2020,Encoder-Agnostic Adaptation for Conditional Language Generation,Zachary M. Ziegler;Luke Melas-Kyriazi;Sebastian Gehrmann;Alexander M. Rush,zziegler@g.harvard.edu;lmelaskyriazi@college.harvard.edu;gehrmann@seas.harvard.edu;srush@seas.harvard.edu,8;8;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Harvard University;Harvard University;Harvard University;Harvard University,39;39;39;39,7;7;7;7,3,8/19/19,8,5,4,1,0,3,125;17;630;7049,11;5;24;86,4;2;9;32,10;4;73;941,m;m
4208,ICLR,2020,Better Knowledge Retention through Metric Learning,Ke Li*;Shichong Peng*;Kailas Vodrahalli*;Jitendra Malik,ke.li@eecs.berkeley.edu;shichong.peng@mail.utoronto.ca;kailasv@berkeley.edu;malik@eecs.berkeley.edu,3;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,6,0,yes,9/25/19,University of California Berkeley;Toronto University;University of California Berkeley;University of California Berkeley,5;18;5;5,13;18;13;13,,9/25/19,0,0,0,0,0,0,453;9;54;11517,96;4;11;22,11;2;3;10,29;0;4;1816,m;m
4209,ICLR,2020,Robust Learning with Jacobian Regularization,Judy Hoffman;Daniel A. Roberts;Sho Yaida,judy@gatech.edu;dan@diffeo.com;shoyaida@fb.com,6;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Georgia Institute of Technology;Diffeo;Facebook,13;-1;-1,38;-1;-1,4;8,8/7/19,11,7,6,0,0,2,9487;1560;918,60;26;41,32;14;11,1123;121;61,f;m
4210,ICLR,2020,CAN ALTQ LEARN FASTER: EXPERIMENTS AND THEORY,Bowen Weng;Huaqing Xiong;Yingbin Liang;Wei Zhang,weng.172@buckeyemail.osu.edu;xiong.309@buckeyemail.osu.edu;liang.889@osu.edu;zhangw3@sustech.edu.cn,3;3;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Ohio State University;Ohio State University;Ohio State University;Tsinghua University,77;77;77;8,373;373;373;23,9,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1;-1,-1;-1;-1;-1,-1;-1;-1;-1,0;0;0;0,m;m
4211,ICLR,2020,A NEW POINTWISE CONVOLUTION IN DEEP NEURAL NETWORKS THROUGH EXTREMELY FAST AND NON PARAMETRIC TRANSFORMS,Joonhyun Jeong;Sung-Ho Bae,doublejtoh@khu.ac.kr;shbae@khu.ac.kr,3;8;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Kyung Hee University;Kyung Hee University,481;481,319;319,,6/25/19,0,0,0,0,0,0,1;379,3;80,1;11,0;32,m;m
4212,ICLR,2020,"Making the Shoe Fit: Architectures, Initializations, and Tuning for Learning with Privacy",Nicolas Papernot;Steve Chien;Shuang Song;Abhradeep Thakurta;Ulfar Erlingsson,papernot@google.com;schien@google.com;athakurta@google.com;shuangsong@google.com;ulfar@google.com,6;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,7,0,yes,9/25/19,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,,9/25/19,4,3,1,1,0,0,9462;18;446;1974;538,66;10;40;50;10,27;3;8;21;4,1100;1;36;264;124,m;m
4213,ICLR,2020,Toward Understanding Generalization of Over-parameterized Deep ReLU network trained with SGD in Student-teacher Setting,Yuandong Tian,yuandong.tian@gmail.com,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0,yes,9/25/19,Facebook,-1,-1,1;8,9/25/19,0,0,0,0,0,0,2498,85,25,293,m
4214,ICLR,2020,Ordinary differential equations on graph networks,Juntang Zhuang;Nicha Dvornek;Xiaoxiao Li;James S. Duncan,j.zhuang@yale.edu;nicha.dvornek@yale.edu;xiaoxiao.li@yale.edu;james.duncan@yale.edu,1;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,13,0,yes,9/25/19,Yale University;Yale University;Yale University;Yale University,64;64;64;64,8;8;8;8,10,9/25/19,2,1,1,0,0,0,90;214;133;10006,22;31;36;432,5;8;6;49,15;22;1;552,m;m
4215,ICLR,2020,Poincaré Wasserstein Autoencoder,Ivan Ovinnikov,ivan.ovinnikov@inf.ethz.ch,3;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0,yes,9/25/19,Swiss Federal Institute of Technology,10,13,10,1/5/19,24,5,6,0,13,2,11,2,1,2,m
4216,ICLR,2020,Learning Neural Surrogate Model for Warm-Starting Bayesian Optimization,Haotian Zhang;Jian Sun;Zongben Xu,zht570795275@stu.xjtu.edu.cn;jiansun@xjtu.edu.cn;zbxu@xjtu.edu.cn,3;3;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,3,0,yes,9/25/19,Xi'an Jiaotong University;Xi'an Jiaotong University;Xi'an Jiaotong University,481;481;481,555;555;555,11,9/25/19,0,0,0,0,0,0,49;3762;9187,3;223;408,1;27;48,3;305;673,m;m
4217,ICLR,2020,Music Source Separation in the Waveform Domain,Alexandre Defossez;Nicolas Usunier;Leon Bottou;Francis Bach,defossez@fb.com;usunier@fb.com;leonb@fb.com;francis.bach@inria.fr,8;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0,yes,9/25/19,Facebook;Facebook;Facebook;INRIA,-1;-1;-1;-1,-1;-1;-1;-1,,9/25/19,10,5,2,0,0,1,98;6244;4561;29808,119;109;28;215,5;30;11;79,15;1186;348;3877,m;m
4218,ICLR,2020,Self-Attentional Credit Assignment for Transfer in Reinforcement Learning,Johan Ferret;Raphaël Marinier;Matthieu Geist;Olivier Pietquin,jferret@google.com;raphaelm@google.com;mfgeist@google.com;pietquin@google.com,8;3;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,1,5,0,yes,9/25/19,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,6,7/18/19,0,0,0,0,0,0,1;60;1354;3468,2;5;133;193,1;2;21;30,0;11;103;298,m;m
4219,ICLR,2020,Gradient Descent can Learn Less Over-parameterized Two-layer Neural Networks on Classification Problems,Atsushi Nitanda;Geoffrey Chinot;Taiji Suzuki,nitanda@mist.i.u-tokyo.ac.jp;geoffreychinot@gmail.com;taiji@mist.i.u-tokyo.ac.jp,8;3;3,I do not know much about this area.:N/A:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:N/A:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,The University of Tokyo;;The University of Tokyo,56;-1;56,36;-1;36,9;8,5/23/19,0,0,0,0,0,0,212;20;2318,17;5;177,5;2;26,23;2;237,f;f
4220,ICLR,2020,Learning to Discretize: Solving 1D Scalar Conservation Laws via Deep Reinforcement Learning,Yufei Wang*;Ziju Shen*;Zichao Long;Bin Dong,wang.yufei@pku.edu.cn;zjshen@pku.edu.cn;zlong@pku.edu.cn;dongbin@math.pku.edu.cn,3;3;6,I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Peking University;Peking University;Peking University;Peking University,22;22;22;22,24;24;24;24,3;1;2;6,5/27/19,2,1,1,0,3,0,766;2;226;1836,76;2;6;138,14;1;2;20,46;0;20;125,f;m
4221,ICLR,2020,Discovering the compositional structure of vector representations with Role Learning Networks,Paul Soulos;Tom McCoy;Tal Linzen;Paul Smolensky,psoulos1@jhu.edu;tom.mccoy@jhu.edu;tal.linzen@jhu.edu;paul.smolensky@gmail.com,6;3;6,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Johns Hopkins University;Johns Hopkins University;Johns Hopkins University;Microsoft,73;73;73;-1,12;12;12;-1,,9/25/19,6,2,2,0,0,0,12;27;1288;10024,8;30;73;206,2;3;15;40,0;2;147;933,m;m
4222,ICLR,2020,A Graph Neural Network Assisted Monte Carlo Tree Search Approach to Traveling Salesman Problem,Zhihao Xing;Shikui Tu,xingzhihao@sjtu.edu.cn;tushikui@sjtu.edu.cn,6;6;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Shanghai Jiao Tong University;Shanghai Jiao Tong University,53;53,157;157,10,9/25/19,1,1,0,0,0,0,29;747,11;68,3;14,0;52,u;m
4223,ICLR,2020,A bi-diffusion based layer-wise sampling method for deep learning in large graphs,Yu He;Shiyang Wen;Wenjin Wu;Yan Zhang;Siran Yang;Yuan Wei;Di Zhang;Guojie  Song;Wei Lin;Liang Wang;Bo Zheng,herve.hy@alibaba-inc.com;shiyang.wsy@alibaba-inc.com;kevin.wwj@alibaba-inc.com;zy143424@alibaba-inc.com;siran.ysr@alibaba-inc.com;yuanxi.wy@alibaba-inc.com;di.zhangd@alibaba-inc.com;gjsong@pku.edu.cn;yangkun.lw@alibaba-inc.com;liangbo.wl@alibaba-inc.com;bozheng@alibaba-inc.com,6;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Alibaba Group;Alibaba Group;Alibaba Group;Alibaba Group;Alibaba Group;Alibaba Group;Alibaba Group;Peking University;Alibaba Group;Alibaba Group;Alibaba Group,-1;-1;-1;-1;-1;-1;-1;22;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;24;-1;-1;-1,10,9/25/19,0,0,0,0,0,0,788;154;3;114;15;19;434;2150;899;359;50,50;8;5;25;6;38;83;123;218;80;34,5;4;1;5;2;2;10;18;15;8;3,42;2;1;16;2;1;15;126;87;34;2,u;u
4224,ICLR,2020,Noise Regularization for Conditional Density Estimation,Jonas Rothfuss;Fabio Ferreira;Simon Boehm;Simon Walther;Maxim Ulrich;Tamim Asfour;Andreas Krause,jonas.rothfuss@gmail.com;fabioferreira@mailbox.org;simonboehm@gmx.de;simon.walther@kit.edu;maxim.ulrich@kit.edu;asfour@kit.edu;krausea@ethz.ch,6;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,University of California Berkeley;Stanford University;Swiss Federal Institute of Technology;Karlsruhe Institute of Technology;Karlsruhe Institute of Technology;Karlsruhe Institute of Technology;Swiss Federal Institute of Technology,5;4;10;154;154;154;10,13;4;13;174;174;174;13,1,7/21/19,4,3,1,0,0,0,122;65;160;19;151;218;16718,7;20;10;9;19;37;241,4;6;4;3;6;8;67,20;2;7;0;14;9;1781,m;m
4225,ICLR,2020,Learning with Protection: Rejection of Suspicious Samples under Adversarial Environment,Masahiro Kato;Yoshihiro Fukuhara;Hirokatsu Kataoka;Shigeo Morishima,mkato.csecon@gmail.com;gatheluck@gmail.com;hirokatsu.kataoka@aist.go.jp;shigeo@waseda.jp,3;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,RIKEN;Waseda University;AIST;Waseda University,-1;266;-1;266,-1;790;-1;790,4,9/25/19,0,0,0,0,0,0,153;4;762;0,77;6;76;6,7;1;10;0,11;0;125;0,m;m
4226,ICLR,2020,Transition Based Dependency Parser for Amharic Language Using Deep Learning,Mizanu Zelalem;Million Meshesha (PhD),mizatmymail@gmail.com;meshe84@gmail.com,1;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,Shandong University;,154;-1,658;-1,,9/25/19,0,0,0,0,0,0,0;234,1;43,0;7,0;19,m;u
4227,ICLR,2020,V1Net: A computational model of cortical horizontal connections,Vijay Veerabadran;Virginia R. de Sa,vveeraba@ucsd.edu;desa@ucsd.edu,1;1;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,3,0,yes,9/25/19,"University of California, San Diego;University of California, San Diego",11;11,31;31,2,9/25/19,0,0,0,0,0,0,54;1070,17;81,5;17,2;55,m;f
4228,ICLR,2020,HOW IMPORTANT ARE NETWORK WEIGHTS? TO WHAT EXTENT DO THEY NEED AN UPDATE?,Fawaz Sammani;Mahmoud Elsayed;Abdelsalam Hamdi,fawaz.sammani@aol.com;elsayedmahmoud@aol.com;abdelsalam.h.a.a@gmail.com,1;3;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,3,0,yes,9/25/19,Multimedia University;Multimedia University;,481;481;-1,1285;1285;-1,,9/25/19,0,0,0,0,0,0,3;1;8,10;5;8,1;1;2,0;0;0,m;m
4229,ICLR,2020,INTERNAL-CONSISTENCY CONSTRAINTS FOR EMERGENT COMMUNICATION,Charles Lovering;Ellie Pavlick,charles_lovering@brown.edu;ellie_pavlick@brown.edu,6;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Brown University;Brown University,67;67,53;53,,9/25/19,0,0,0,0,0,0,15;1301,8;62,3;16,0;233,m;f
4230,ICLR,2020,Not All Features Are Equal: Feature Leveling Deep Neural Networks for Better Interpretation,Yingjing Lu;Runde Yang,yingjinl@andrew.cmu.edu;ry82@cornell.edu,1;3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0,yes,9/25/19,Carnegie Mellon University;Cornell University,1;7,27;19,,5/24/19,1,0,0,0,0,0,2;0,6;2,1;0,0;0,m;m
4231,ICLR,2020,Neural Networks for Principal Component Analysis: A New Loss Function Provably Yields Ordered Exact Eigenvectors ,Reza Oftadeh;Jiayi Shen;Zhangyang Wang;Dylan Shell,oftadeh.reza@gmail.com;asjyjya-617@tamu.edu;atlaswang@tamu.edu;dshell@tamu.edu,6;6;3,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Texas A&M;Texas A&M;Texas A&M;Texas A&M,44;44;44;44,177;177;177;177,1,9/25/19,0,0,0,0,0,0,169;67;2947;1161,26;11;167;122,9;5;28;18,4;3;383;40,m;m
4232,ICLR,2020,GENN: Predicting Correlated Drug-drug Interactions with Graph Energy Neural Networks,Tengfei Ma;Junyuan Shang;Cao Xiao;Jimeng Sun,tengfei.ma1@ibm.com;sjy1203@pku.edu.cn;cao.xiao@iqvia.com;sun@cc.gatech.edu,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,International Business Machines;Peking University;IQVIA;Georgia Institute of Technology,-1;22;-1;13,-1;24;-1;38,10,9/25/19,2,2,2,0,0,1,155;112;945;10636,17;14;82;240,3;4;13;56,7;5;116;787,m;m
4233,ICLR,2020,Efficient Systolic Array Based on Decomposable MAC for Quantized Deep Neural Networks,Ning-Chi Huang;Huan-Jan Chou;Kai-Chiang Wu,nchuang@cs.nctu.edu.tw;kulugu2.cs07g@nctu.edu.tw;kcw@cs.nctu.edu.tw,1;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,National Chiao Tung University;National Chiao Tung University;National Chiao Tung University,143;143;143,564;564;564,,9/25/19,0,0,0,0,0,0,9;0;401,5;1;46,1;0;10,0;0;20,u;m
4234,ICLR,2020,DeepEnFM: Deep neural networks with Encoder enhanced Factorization Machine,Qiang Sun;Zhinan Cheng;Yanwei Fu;Wenxuan Wang;Yu-Gang Jiang;Xiangyang Xue,sunqiang85@gmail.com;zhinancheng.bryan@gmail.com;yanweifu@fudan.edu.cn;wxwang.iris@gmail.com;ygj@fudan.edu.cn;xyxue@fudan.edu.cn,1;3;1,I do not know much about this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Fudan University;Fudan University;Fudan University;Fudan University;Fudan University;Fudan University,79;79;79;79;79;79,109;109;109;109;109;109,,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,0;0;0;0;0;0,m;m
4235,ICLR,2020,Statistically Consistent Saliency Estimation,Emre Barut;Shunyan Luo,barut@gwu.edu;shine_lsy@gwu.edu,8;8;3;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0,yes,9/25/19,George Washington University;George Washington University,205;205,198;198,1;2,9/25/19,0,0,0,0,0,0,148;0,8;2,4;0,34;0,m;m
4236,ICLR,2020,Towards Physics-informed Deep Learning for Turbulent Flow Prediction,Rui Wang;Karthik Kashinath;Mustafa Mustafa;Adrian Albert;Rose Yu,wang.rui4@husky.neu.edu;kkashinath@lbl.gov;mmustafa@lbl.gov;aalbert@lbl.gov;roseyu@northeastern.edu,6;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,3,7,0,yes,9/25/19,Northeastern University;Lawrence Berkeley National Lab;Lawrence Berkeley National Lab;Lawrence Berkeley National Lab;Northeastern University,16;-1;-1;-1;16,906;-1;-1;-1;906,,9/25/19,8,4,1,0,0,0,891;478;323;62;886,88;89;61;11;38,16;12;8;4;12,40;31;1;3;102,m;f
4237,ICLR,2020,Domain-Agnostic Few-Shot Classification by Learning Disparate Modulators,Yongseok Choi;Junyoung Park;Subin Yi;Dong-Yeon Cho,yschoi@sktbrain.com;jypark@sktbrain.com;yisubin@sktbrain.com;dycho24@sktbrain.com,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0,yes,9/25/19,SK T-Brain;SK T-Brain;SK T-Brain;SK T-Brain,-1;-1;-1;-1,-1;-1;-1;-1,6,9/25/19,0,0,0,0,0,0,419;0;15;520,65;5;7;34,11;0;1;12,27;0;0;27,m;m
4238,ICLR,2020,Online Meta-Critic Learning for Off-Policy Actor-Critic Methods,Wei Zhou;Yiying Li;Yongxin Yang;Huaimin Wang;Timothy M. Hospedales,zhouwei14@nudt.edu.cn;liyiying10@nudt.edu.cn;yongxin.yang@ed.ac.uk;hmwang@nudt.edu.cn;t.hospedales@ed.ac.uk,3;3;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,National University of Defense Technology;National University of Defense Technology;University of Edinburgh;National University of Defense Technology;University of Edinburgh,-1;-1;33;-1;33,-1;-1;30;-1;30,6,9/25/19,1,1,0,0,0,1,202;2;2148;24;384,60;8;66;15;31,8;1;21;3;9,13;1;335;1;34,u;m
4239,ICLR,2020,Using Explainabilty to Detect Adversarial Attacks,Ohad Amosy;Gal Chechik,amosy3@gmail.com;gal.chechik@gmail.com,3;3;1;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,2,0,0,yes,9/25/19,Bar Ilan University;,95;-1,513;-1,4,9/25/19,0,0,0,0,0,0,0;-1,1;-1,0;-1,0;0,m;m
4240,ICLR,2020,Random Bias Initialization Improving Binary Neural Network Training,Xinlin Li;Vahid Partovi Nia,xinlin.li1@huawei.com;vahid.partovinia@huawei.com,1;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,0,0,yes,9/25/19,Huawei Technologies Ltd.;Huawei Technologies Ltd.,-1;-1,-1;-1,,9/25/19,0,0,0,0,0,0,186;31,34;17,9;2,13;0,m;m
4241,ICLR,2020,Bayesian Inference for Large Scale Image Classification,Jonathan Heek;Nal Kalchbrenner,jheek@google.com;nalk@google.com,6;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,3,0,yes,9/25/19,Google;Google,-1;-1,-1;-1,11;7,8/9/19,7,5,3,0,0,5,10;16442,2;27,2;18,5;1425,m;m
4242,ICLR,2020,Self-Supervised GAN Compression,Chong Yu;Jeff Pool,chongy@nvidia.com;jpool@nvidia.com,3;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:N/A:N/A,Reject,0,4,0,yes,9/25/19,NVIDIA;NVIDIA,-1;-1,-1;-1,3;4;5,9/25/19,0,0,0,0,0,0,421;2695,36;38,9;11,16;369,m;m
4243,ICLR,2020,"Generative Hierarchical Models for Parts, Objects, and Scenes",Fei Deng;Zhuo Zhi;Sungjin Ahn,fei.deng@rutgers.edu;zhizz001@stu.xjtu.edu.cn;sjn.ahn@gmail.com,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,Rutgers University;Xi'an Jiaotong University;Rutgers University,34;481;34,168;555;168,5,9/25/19,0,0,0,0,0,0,23;3;1389,23;4;41,2;1;12,2;0;161,f;m
4244,ICLR,2020,SDGM: Sparse Bayesian Classifier Based on a Discriminative Gaussian Mixture Model,Hideaki Hayashi;Seiichi Uchida,hayashi@ait.kyushu-u.ac.jp;uchida@ait.kyushu-u.ac.jp,3;3;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Meiji University;Meiji University,481;481,332;332,11;8,9/25/19,1,0,0,0,0,0,2;3187,11;307,1;22,0;365,m;m
4245,ICLR,2020,Analytical Moment Regularizer for Training Robust Networks,Modar Alfadly;Adel Bibi;Muhammed Kocabas;Bernard Ghanem,modar.alfadly@kaust.edu.sa;adel.bibi@kaust.edu.sa;muhammed.kocabas@tue.mpg.de;bernard.ghanem@kaust.edu.sa,3;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,3,0,yes,9/25/19,"KAUST;KAUST;Max Planck Institute for Intelligent Systems, Max-Planck Institute;KAUST",128;128;-1;128,1397;1397;-1;1397,4,9/25/19,0,0,0,0,0,0,40;462;113;6428,11;25;5;198,4;9;3;37,5;65;10;1007,m;m
4246,ICLR,2020,ROBUST DISCRIMINATIVE REPRESENTATION LEARNING VIA GRADIENT RESCALING: AN EMPHASIS REGULARISATION PERSPECTIVE,Xinshao Wang;Yang Hua;Elyor Kodirov;Neil M. Robertson,xwang39@qub.ac.uk;y.hua@qub.ac.uk;elyor@anyvision.co;n.robertson@qub.ac.uk,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,7,1,yes,9/25/19,Queen's University Belfast;Queen's University Belfast;Anyvision;Queen's University Belfast,266;266;-1;266,204;204;-1;204,,9/25/19,0,0,0,0,0,0,17;14;1062;155,10;34;33;49,2;1;10;8,1;1;148;6,m;m
4247,ICLR,2020,Multi-step Greedy Policies in Model-Free Deep Reinforcement Learning,Yonathan Efroni;Manan Tomar;Mohammad Ghavamzadeh,jonathan.efroni@gmail.com;manan.tomar@gmail.com;mgh@fb.com,3;3;6,I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,8,0,yes,9/25/19,Technion;Indian Institute of Technology Madras;Facebook,26;154;-1,412;641;-1,,9/25/19,1,1,0,0,0,0,86;7;538,17;6;36,5;2;11,8;0;60,m;m
4248,ICLR,2020,Hidden incentives for self-induced distributional shift,David Scott Krueger;Tegan Maharaj;Shane Legg;Jan Leike,davidscottkrueger@gmail.com;tegan.jrm@gmail.com;legg@google.com;leike@google.com,6;1;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,University of Montreal;Polytechnique Montreal;Google;Google,128;390;-1;-1,85;1397;-1;-1,6,9/25/19,0,0,0,0,0,0,1556;733;11331;832,165;18;58;47,21;7;21;14,93;79;1821;66,m;m
4249,ICLR,2020,Learning Temporal Abstraction with Information-theoretic Constraints for Hierarchical Reinforcement Learning,Wenshan Wang;Yaoyu Hu;Sebastian Scherer,wenshanw@andrew.cmu.edu;yaoyuh@andrew.cmu.edu;basti@andrew.cmu.edu,3;1;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,12,0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1,27;27;27,9,9/25/19,0,0,0,0,0,0,265;17;3531,45;20;150,8;2;27,17;1;277,f;m
4250,ICLR,2020,Generalized Clustering by Learning to Optimize Expected Normalized Cuts,Azade Nazi;Will Hang;Anna Goldie;Sujith Ravi;Azalia Mirhoseini,azade@google.com;agoldie@google.com;sravi@google.com;azalia@google.com;willhang@stanford.edu,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Google;Google;Google;Google;Stanford University,-1;-1;-1;-1;4,-1;-1;-1;-1;4,8,9/25/19,0,0,0,0,0,0,263;7;517;30;1083,28;3;18;23;56,8;2;6;2;14,18;1;45;2;79,f;f
4251,ICLR,2020,Adversarially Robust Neural Networks via Optimal Control: Bridging Robustness with Lyapunov Stability,Zhiyang Chen;Hang Su,zy-chen17@mails.tsinghua.edu.cn;suhangss@mail.tsinghua.edu.cn,1;6;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,2,0,0,yes,9/25/19,Tsinghua University;Tsinghua University,8;8,23;23,4,9/25/19,0,0,0,0,0,0,9;1613,6;69,2;13,0;226,m;m
4252,ICLR,2020,Temporal Difference Weighted Ensemble For Reinforcement Learning,Takuma Seno;Michita Imai,seno@ailab.ics.keio.ac.jp;michita@ailab.ics.keio.ac.jp,1;3;8,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,6,0,yes,9/25/19,Meiji University;Meiji University,481;481,332;332,,9/25/19,0,0,0,0,0,0,0;362,5;56,0;7,0;19,m;m
4253,ICLR,2020,Learning Generative Image Object Manipulations from Language Instructions,Martin Längkvist;Andreas Persson;Amy Loutfi,martin.langkvist@oru.se;andreas.persson@oru.se;amy.loutfi@oru.se,3;3;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,0,0,yes,9/25/19,Centre for Applied Autonomous Sensor Systems;Centre for Applied Autonomous Sensor Systems;Centre for Applied Autonomous Sensor Systems,-1;-1;-1,-1;-1;-1,3;4;5,9/25/19,0,0,0,0,0,0,972;271;3350,27;52;186,7;9;26,43;13;120,m;f
4254,ICLR,2020,A Simple Technique to Enable Saliency Methods to Pass the Sanity Checks,Arushi Gupta;Sanjeev Arora,arushig@princeton.edu;arora@cs.princeton.edu,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Princeton University;Princeton University,31;31,6;6,,9/25/19,0,0,0,0,0,0,2;2317,5;82,1;14,0;254,f;m
4255,ICLR,2020,Impact of the latent space on the ability of GANs to fit the distribution,Thomas Pinetz;Daniel Soukup;Thomas Pock,thomas.pinetz@ait.ac.at;daniel.soukup@ait.ac.at;pock@icg.tugraz.at,1;1;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0,yes,9/25/19,;;Graz University of Technology,-1;-1;108,-1;-1;542,5;4;1,9/25/19,0,0,0,0,0,0,3;200;0,5;60;3,1;6;0,1;7;0,m;m
4256,ICLR,2020,At Your Fingertips: Automatic Piano Fingering Detection,Amit Moryossef;Yanai Elazar;Yoav Goldberg,amitmoryossef@gmail.com;yanaiela@gmail.com;yoav.goldberg@gmail.com,1;3;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:N/A:N/A;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,2,0,yes,9/25/19,Bar Ilan University;Bar Ilan University;Bar-Ilan University,95;95;95,513;513;513,5;4,9/25/19,1,0,0,0,0,0,49;133;1504,6;12;40,3;5;10,8;10;189,m;m
4257,ICLR,2020,Latent Question Reformulation and Information Accumulation for Multi-Hop Machine Reading,Quentin Grail;Julien Perez;Eric Gaussier,quentin.grail@naverlabs.com;julien.perez@naverlabs.com;eric.gaussier@imag.fr,8;3;3,I have published one or two papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,4,0,yes,9/25/19,Naver Labs Europe;Naver Labs Europe;Imag Montpellier Université,-1;-1;-1,-1;-1;-1,3,9/25/19,1,0,0,0,0,0,33;391;4035,6;46;165,2;13;27,3;25;467,m;m
4258,ICLR,2020,Deep Variational Semi-Supervised Novelty Detection,Tal Daniel;Thanard Kurutach;Aviv Tamar,taldanielm@campus.technion.ac.il;thanard.kurutach@berkeley.edu;avivt@technion.ac.il,6;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Technion;University of California Berkeley;Technion,26;5;26,412;13;412,5,9/25/19,3,1,1,0,0,1,75;197;7,8;8;6,4;4;2,7;28;1,m;m
4259,ICLR,2020,Occlusion  resistant  learning  of  intuitive physics from videos,Ronan Riochet;Josef Sivic;Ivan Laptev;Emmanuel Dupoux,ronan.riochet@inria.fr;josef.sivic@ens.fr;ivan.laptev@inria.fr;emmanuel.dupoux@gmail.com,3;6;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,4,0,yes,9/25/19,INRIA;Ecole Normale Superieure;INRIA;,-1;100;-1;-1,-1;45;-1;-1,2,9/25/19,0,0,0,0,0,0,19;24667;22325;8512,3;154;149;253,2;59;53;48,1;2959;3100;730,m;m
4260,ICLR,2020,"Compressive Recovery Defense: A Defense Framework for $\ell_0, \ell_2$ and $\ell_\infty$ norm attacks.",Jasjeet Dhaliwal;Kyle Hambrook,jasjeet.dhaliwal@sjsu.edu;kyle.hambrook@sjsu.edu,3;3;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY,481;481,299;299,4,9/25/19,0,0,0,0,0,0,8;79,8;18,1;6,0;11,m;m
4261,ICLR,2020,Causally Correct Partial Models for Reinforcement Learning,Danilo J. Rezende;Ivo Danihelka;George Papamakarios;Nan Rosemary Ke;Ray Jiang;Theophane Weber;Karol Gregor;Hamza Merzic;Fabio Viola;Jane Wang;Jovana Mitrovic;Frederic Besse;Ioannis Antonoglou;Lars Buesing;Julian Schrittwieser;Thomas Hubert;David Silver,danilor@google.com;danihelka@google.com;gpapamak@google.com;rosemary.nan.ke@gmail.com;rayjiang@google.com;theophane@google.com;karolg@google.com;hamzamerzic@google.com;fviola@google.com;wangjane@google.com;mitrovic@google.com;fbesse@google.com;ioannisa@google.com;lbuesing@google.com;swj@google.com;tkhubert@google.com;davidsilver@google.com,8;1;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Reject,0,7,0,yes,9/25/19,Google;Google;Google;Polytechnique Montreal;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;390;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;1397;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,,9/25/19,1,1,0,0,0,0,8814;4952;654;760;62;-1;4833;29;1814;31;81;575;22915;2457;11350;272;44447,63;19;22;32;11;-1;44;6;49;18;8;22;16;54;10;20;162,27;16;8;13;5;-1;20;3;16;2;4;10;12;21;9;7;57,1132;555;122;76;3;0;535;3;215;2;10;58;2736;259;557;35;6080,m;m
4262,ICLR,2020,Learning scalable and transferable multi-robot/machine sequential assignment planning via graph embedding,Hyunwook Kang;Aydar Mynbay;James R. Morrison;Jinkyoo Park,hwkang@tamu.edu;aydar.mynbay@bluehole.net;james.morrison@kaist.edu;jinkyoo.park@kaist.ac.kr,3;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,3,0,yes,9/25/19,Texas A&M;;KAIST;Korea Advanced Institute of Science and Technology,44;-1;20;481,177;-1;110;110,10,9/25/19,0,0,0,0,0,0,19;0;732;74,9;2;108;24,2;0;14;5,0;0;31;2,m;m
4263,ICLR,2020,Improving Evolutionary Strategies with Generative Neural Networks,Louis Faury;Clément Calauzènes;Olivier Fercoq,l.faury@criteo.com;c.calauzenes@criteo.com;olivier.fercoq@telecom-paris.fr,8;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0,yes,9/25/19,Criteo;Criteo;Télécom Paris,-1;-1;481,-1;-1;187,5,1/31/19,2,2,2,0,0,1,4;149;1062,6;17;57,1;5;17,1;17;133,m;m
4264,ICLR,2020,Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning,Mitchell A Gordon;Kevin Duh;Nicholas Andrews,mgordo37@jhu.edu;kevinduh@cs.jhu.edu;noa@jhu.edu,6;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,3,0,yes,9/25/19,Johns Hopkins University;Johns Hopkins University;Johns Hopkins University,73;73;73,12;12;12,3;6;2,9/25/19,10,2,4,0,0,0,11;3010;264,3;191;20,1;28;9,0;347;11,m;m
4265,ICLR,2020,REFINING MONTE CARLO TREE SEARCH AGENTS BY MONTE CARLO TREE SEARCH,Katsuki Ohto,katsuki.ohto@gmail.com,1;1;1,I have published in this field for several years.:N/A:N/A:N/A;I have published in this field for several years.:I did not assess the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have read many papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,3,0,yes,9/25/19,,,,,9/25/19,0,0,0,0,0,0,2,2,1,1,m
4266,ICLR,2020,Smooth Kernels Improve Adversarial Robustness and Perceptually-Aligned Gradients,Haohan Wang;Xindi Wu;Songwei Ge;Zachary C. Lipton;Eric P. Xing,haohanw@cs.cmu.edu;xindiw@andrew.cmu.edu;songweig@andrew.cmu.edu;zlipton@cmu.edu;epxing@cs.cmu.edu,1;1;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1;1,27;27;27;27;27,4,9/25/19,0,0,0,0,0,0,57;9;22;4885;24615,18;6;12;97;605,3;1;3;29;76,5;1;3;438;2669,m;m
4267,ICLR,2020,Modeling treatment events in disease progression,Guanyang Wang;Yumeng Zhang;Yong Deng;Xuxin Huang;Lukasz Kidzinski,guanyang@stanford.edu;zym3008@gmail.com;yongdeng@stanford.edu;xxhuang@stanford.edu;lukasz.kidzinski@stanford.edu,1;1;1,I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,Stanford University;;Stanford University;Stanford University;Stanford University,4;-1;4;4;4,4;-1;4;4;4,,5/26/19,0,0,0,0,0,0,1;18;8451;30;336,5;23;600;7;36,1;2;52;1;12,0;3;207;2;34,m;m
4268,ICLR,2020,Collaborative Generated Hashing for Market Analysis and Fast Cold-start Recommendation,Yan Zhang;Ivor W. Tsang;Lixin Duan;Guowu Yang,yixianqianzy@gmail.com;ivor.tsang@uts.edu.au;lxduan@gmail.com;guowu@uestc.edu.cn,1;1;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,University of Technology Sydney;University of Technology Sydney;University of Electronic Science and Technology of China;University of Electronic Science and Technology of China,108;108;481;481,193;193;628;628,5,9/25/19,0,0,0,0,0,0,1612;10966;5;1079,133;253;7;145,19;51;2;17,90;1283;0;62,u;m
4269,ICLR,2020,Are Few-shot Learning Benchmarks Too Simple ?,Gabriel Huang;Hugo Larochelle;Simon Lacoste-Julien,gbxhuang@gmail.com;hugolarochelle@google.com;slacoste@iro.umontreal.ca,6;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0,yes,9/25/19,University of Montreal;Google;University of Montreal,128;-1;128,85;-1;85,6,2/22/19,0,0,0,0,0,0,89;25332;3838,6;124;76,4;44;27,18;2884;610,m;m
4270,ICLR,2020,Towards trustworthy predictions from deep neural networks with fast adversarial calibration,Christian Tomani;Florian Buettner,christian.tomani@gmail.com;fbuettner.phys@gmail.com,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,6,0,yes,9/25/19,Technical University Munich;Siemens Corporate Research,53;-1,43;-1,4;11,9/25/19,0,0,0,0,0,0,0;1325,1;32,0;13,0;65,m;m
4271,ICLR,2020,Collaborative Training of Balanced Random Forests for Open Set Domain Adaptation,Jongbin Ryu;Jiun Bae;Jongwoo Lim,jongbin.ryu@gmail.com;maybe@hanyang.ac.kr;jlim@hanyang.ac.kr,3;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Hanyang University;Hanyang University;Hanyang University,233;233;233,393;393;393,,9/25/19,0,0,0,0,0,0,3;0;9182,6;1;73,1;0;26,1;0;2058,m;m
4272,ICLR,2020,Accelerated Variance Reduced Stochastic Extragradient Method for Sparse Machine Learning Problems,Fanhua Shang;Lin Kong;Yuanyuan Liu;Hua Huang;Hongying Liu,fhshang@xidian.edu.cn;xdkonglin0511@163.com;yyliu@xidian.edu.cn;huanghua1115@outlook.com;hyliu@xidian.edu.cn,8;1;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Tsinghua University;Xidian University;Tsinghua University;;Tsinghua University,8;481;8;-1;8,23;919;23;-1;23,9;2,9/25/19,0,0,0,0,0,0,988;2;148;3140;1760,79;12;32;235;216,19;1;6;26;21,91;0;6;178;62,m;f
4273,ICLR,2020,Convergence Analysis of a Momentum Algorithm with Adaptive Step Size for Nonconvex Optimization,Anas Barakat;Pascal Bianchi,anas.barakat@telecom-paristech.fr;pascal.bianchi@telecom-paristech.fr,3;3;3,I have read many papers in this area.:N/A:N/A:I did not assess the derivations or theory.;I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,Télécom ParisTech;Télécom ParisTech,481;481,187;187,1;9,9/25/19,3,0,1,0,0,0,14;38,4;18,3;3,2;2,m;m
4274,ICLR,2020,RTC-VAE: HARNESSING THE PECULIARITY OF TOTAL CORRELATION  IN LEARNING DISENTANGLED REPRESENTATIONS,Ze Cheng;Juncheng B Li;Chenxu Wang;Jixuan Gu;Hao Xu;Xinjian Li;Florian Metze,ze.cheng@cn.bosch.com;junchenl@cs.cmu.edu;chenxujwang@gmail.com;jixuan.gu@sjtu.edu.cn;hao.xu-1@colorado.edu;xinjianl@cs.cmu.edu;fmetze@cs.cmu.edu,3;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,"Bosch;Carnegie Mellon University;Bosch;Shanghai Jiao Tong University;University of Colorado, Boulder;Carnegie Mellon University;Carnegie Mellon University",-1;1;-1;53;44;1;1,-1;27;-1;157;123;27;27,5;1,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,0;0;0;0;0;0;0,m;m
4275,ICLR,2020,Generalized Bayesian Posterior Expectation Distillation for Deep Neural Networks,Meet P. Vadera;Benjamin M. Marlin,mvadera@cs.umass.edu;marlin@cs.umass.edu,3;6;6,I have published one or two papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,7,0,yes,9/25/19,"University of Massachusetts, Amherst;University of Massachusetts, Amherst",28;28,209;209,11,9/25/19,2,1,1,0,0,0,7;2783,7;81,1;25,0;283,m;m
4276,ICLR,2020,Towards Simplicity in Deep Reinforcement Learning: Streamlined Off-Policy Learning,Che Wang;Yanqiu Wu;Quan Vuong;Keith Ross,cw1681@nyu.edu;yanqiu.wu@nyu.edu;quan.hovuong@gmail.com;keithwross@nyu.edu,6;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,11,0,yes,9/25/19,"New York University;New York University;University of California, San Diego;New York University",25;25;11;25,29;29;31;29,,9/25/19,0,0,0,0,0,0,14;102;1782;12156,13;19;124;295,2;5;23;56,0;5;81;1289,m;m
4277,ICLR,2020,A shallow feature extraction network with a large receptive field for stereo matching tasks,Jianguo Liu;Yunjian Feng;Guo Ji;Fuwu Yan,ljg424@163.com;1029515027@whut.edu.cn;18754806756@163.com;yanfw@whut.edu.cn,3;1;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,0,0,yes,9/25/19,163;South China University of Technology;163;South China University of Technology,-1;481;-1;481,-1;501;-1;501,2,9/25/19,0,0,0,0,0,0,217;0;32;2,65;1;75;7,6;0;4;1,17;0;1;0,u;u
4278,ICLR,2020,Evaluating Semantic Representations of Source Code,Yaza Wainakh;Moiz Rauf;Michael Pradel,yaza.wainakh@gmail.com;moiz.rauf@iste.uni-stuttgart.de;michael@binaervarianz.de,6;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0,yes,9/25/19,;University of Stuttgart;University of Stuttgart,-1;95;95,-1;292;292,3,9/25/19,1,0,1,0,0,0,22;1;1425,6;3;89,3;1;22,4;0;110,m;m
4279,ICLR,2020,UW-NET: AN INCEPTION-ATTENTION NETWORK FOR UNDERWATER IMAGE CLASSIFICATION,Miao Yang;Ke Hu;Chongyi Li;Zhiqiang Wei,lemonmiao@gmial.com;kexisibest@outlook.com;lichongyi@tju.edu.cn;weizhiqiang@ouc.edu.cn,3;1;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,0,0,yes,9/25/19,"Gmial;;Zhejiang University;University of Illinois, Urbana-Champaign",-1;-1;56;3,-1;-1;107;48,,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1;-1,-1;-1;-1;-1,-1;-1;-1;-1,0;0;0;0,u;u
4280,ICLR,2020,Deep Generative Classifier for Out-of-distribution Sample Detection,Dongha Lee;Sehun Yu;Hwanjo Yu,dongha0914@postech.ac.kr;hunu12@postech.ac.kr;hwanjoyu@postech.ac.kr,3;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,1,3,0,yes,9/25/19,POSTECH;POSTECH;POSTECH,118;118;118,146;146;146,5,9/25/19,1,0,1,0,0,0,355;2;3297,64;2;147,10;1;28,14;0;304,m;m
4281,ICLR,2020,Adaptive Loss Scaling for Mixed Precision Training,Ruizhe Zhao;Brian Vogel;Tanvir Ahmed,ruizhe.zhao15@imperial.ac.uk;vogel@preferred.jp;tanvira@preferred.jp,3;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,3,0,yes,9/25/19,"Imperial College London;Preferred Networks, Inc.;Preferred Networks, Inc.",73;-1;-1,10;-1;-1,,9/25/19,0,0,0,0,0,0,166;38;98,22;16;23,7;3;6,9;1;10,m;m
4282,ICLR,2020,On importance-weighted autoencoders,Axel Finke;Alexandre H. Thiery,axelfinke42@gmail.com;a.h.thiery@nus.edu.sg,8;3;6,I have published one or two papers in this area.:N/A:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,5,0,yes,9/25/19,National University of Singapore;National University of Singapore,16;16,25;25,5;1,9/25/19,1,0,0,0,0,0,31;382,11;21,4;9,2;36,m;m
4283,ICLR,2020,Mixed Precision Training With 8-bit Floating Point,Naveen Mellempudi;Sudarshan Srinivasan;Dipankar Das;Bharat Kaul,naveen.k.mellempudi@intel.com;sudarshan.srinivasan@intel.com;dipankar.das@intel.com;bharat.kaul@intel.com,6;1;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,7,0,yes,9/25/19,Intel;Intel;Intel;Intel,-1;-1;-1;-1,-1;-1;-1;-1,8,5/29/19,16,7,6,0,0,3,198;193;1226;477,11;36;107;34,6;8;19;11,24;12;67;49,m;m
4284,ICLR,2020,A Functional Characterization of Randomly Initialized Gradient Descent in Deep ReLU Networks,Justin Sahs;Aneel Damaraju;Ryan Pyle;Onur Tavaslioglu;Josue Ortega Caro;Hao Yang Lu;Ankit Patel,justin.sahs@bcm.edu;amd18@rice.edu;ryan.pyle@bcm.edu;onur.tavaslioglu@bcm.edu;josue.ortegacaro@bcm.edu;hl61@rice.edu;ankitp@bcm.edu,3;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Baylor College of Medicine;Rice University;Baylor College of Medicine;Baylor College of Medicine;Baylor College of Medicine;Rice University;Baylor College of Medicine,-1;84;-1;-1;-1;84;-1,-1;105;-1;-1;-1;105;-1,8,9/25/19,2,2,1,0,0,1,374;0;87;7;2;469;378,7;2;8;5;2;52;30,2;0;3;2;1;11;6,37;0;4;1;1;14;39,m;m
4285,ICLR,2020,ASGen: Answer-containing Sentence Generation to Pre-Train Question Generator for Scale-up Data in Question Answering,Akhil Kedia;Sai Chetan Chinthakindi;Seohyun Back;Haejun Lee;Jaegul Choo,akhil.kedia@samsung.com;sai.chetan@samsung.com;scv.back@samsung.com;haejun82.lee@samsung.com;jchoo@korea.ac.kr,6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,9,0,yes,9/25/19,Samsung;Samsung;Samsung;Samsung;Korea University,-1;-1;-1;-1;323,-1;-1;-1;-1;179,3,9/25/19,0,0,0,0,0,0,3;3;29;3;2415,2;2;6;4;124,1;1;3;1;22,0;0;1;0;389,m;m
4286,ICLR,2020,Invariance vs Robustness of Neural Networks,Sandesh Kamath;Amit Deshpande;K V Subrahmanyam,amitdesh@microsoft.com;ksandeshk@cmi.ac.in;kv@cmi.ac.in,3;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,0,0,yes,9/25/19,Microsoft;Chennai Mathematical Institute;Chennai Mathematical Institute,-1;-1;-1,-1;-1;-1,4;8,9/25/19,3,1,0,1,0,0,19;142;208,16;21;50,3;4;8,2;12;24,m;m
4287,ICLR,2020,Modelling the influence of data structure on learning in neural networks,S. Goldt;M. Mézard;F. Krzakala;L. Zdeborová,goldt.sebastian@gmail.com;marc.mezard@gmail.com;florent.krzakala@gmail.com;lenka.zdeborova@gmail.com,1;1;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,"Institut de Physique théorique, CNRS, CEA;Ecole Normale Superieure;Ecole Normale Superieure;CEA",-1;100;100;233,-1;45;45;1027,5,9/25/19,14,7,3,0,0,1,100;8554;4951;4738,9;282;198;167,5;42;35;34,7;813;353;340,m;f
4288,ICLR,2020,A Perturbation Analysis of Input Transformations for Adversarial Attacks,Adam Dziedzic;Sanjay Krishnan,ady@uchicago.edu;skr@uchicago.edu,3;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,7,0,yes,9/25/19,University of Chicago;University of Chicago,48;48,9;9,4,9/25/19,0,0,0,0,0,0,144;1528,24;88,7;22,2;120,m;m
4289,ICLR,2020,Understanding Isomorphism Bias in Graph Data Sets ,Ivanov Sergey;Sviridov Sergey;Evgeny Burnaev,ivanovserg990@gmail.com;sergei.sviridov@gmail.com;e.burnaev@skoltech.ru,6;1;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,5,0,yes,9/25/19,Skolkovo Institute of Science and Technology;;Skolkovo Institute of Science and Technology,-1;-1;-1,-1;-1;-1,10;8,9/25/19,4,2,1,0,0,1,25;3236;22,55;81;17,3;22;3,0;181;1,m;m
4290,ICLR,2020,Data-Efficient Image Recognition with Contrastive Predictive Coding,Olivier J Henaff;Aravind Srinivas;Jeffrey De Fauw;Ali Razavi;Carl Doersch;S. M. Ali Eslami;Aaron van den Oord,henaff@google.com;aravind@cs.berkeley.edu;defauw@google.com;alirazavi@google.com;doersch@google.com;aeslami@google.com;avdnoord@google.com,3;3;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Google;University of California Berkeley;Google;Google;Google;Google;Google,-1;5;-1;-1;-1;-1;-1,-1;13;-1;-1;-1;-1;-1,2;8,5/22/19,132,49,36,6,37,10,172;307;848;535;3028;4138;8748,11;13;9;23;31;38;40,3;5;7;7;15;19;24,10;36;68;68;290;529;1110,m;m
4291,ICLR,2020,OBJECT-ORIENTED REPRESENTATION OF 3D SCENES,Chang Chen;Sungjin Ahn,chang.chen@rutgers.edu;sjn.ahn@gmail.com,3;6;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,9,0,yes,9/25/19,Rutgers University;Rutgers University,34;34,168;168,5;8,9/25/19,0,0,0,0,0,0,36;1389,48;41,3;12,1;161,m;m
4292,ICLR,2020,Barcodes as summary of objective functions' topology,Serguei Barannikov;Alexander Korotin;Dmitry Oganesyan;Daniil Emtsev;Evgeny Burnaev,serguei.barannikov@imj-prg.fr;a.korotin@skoltech.ru;d.oganesyan@skoltech.ru;demtsev@student.ethz.ch;e.burnaev@skoltech.ru,1;1;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,"CNRS, Institut Mathematiques de Jussieu, Paris Diderot University;Skolkovo Institute of Science and Technology;Skolkovo Institute of Science and Technology;Swiss Federal Institute of Technology;Skolkovo Institute of Science and Technology",481;-1;-1;10;-1,1397;-1;-1;13;-1,,9/25/19,1,0,1,0,0,0,126;28;30;1;747,23;15;7;1;87,6;3;1;1;16,11;1;0;0;38,m;m
4293,ICLR,2020,Asynchronous Stochastic Subgradient Methods for General Nonsmooth Nonconvex Optimization,Vyacheslav Kungurtsev;Malcolm Egan;Bapi Chatterjee;Dan Alistarh,vyacheslav.kungurtsev@fel.cvut.cz;malcom.egan@insa-lyon.fr;bapi.chatterjee@ist.ac.at;dan.alistarh@ist.ac.at,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,Czech Technical University in Prague;INSA de Lyon;Institute of Science and Technology Austria;Institute of Science and Technology Austria,323;481;481;481,956;1397;1397;1397,,5/28/19,1,1,0,0,0,0,345;362;160;1785,56;60;21;127,11;8;6;19,16;16;22;227,m;m
4294,ICLR,2020,Top-down training for neural networks,Shucong Zhang;Cong-Thanh Do;Rama Doddipatla;Erfan Loweimi;Peter Bell;Steve Renals,s1603602@sms.ed.ac.uk;cong-thanh.do@crl.toshiba.co.uk;rama.doddipatla@crl.toshiba.co.uk;e.loweimi@ed.ac.uk;peter.bell@ed.ac.uk;s.renals@ed.ac.uk,3;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:N/A:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,1,5,0,yes,9/25/19,University of Edinburgh;Toshiba Research Europe Ltd.;Toshiba Research Europe Ltd.;University of Edinburgh;University of Edinburgh;University of Edinburgh,33;-1;-1;33;33;33,30;-1;-1;30;30;30,,9/25/19,0,0,0,0,0,0,5;115;83;120;7284;71,12;27;15;25;599;15,1;6;5;6;39;6,0;1;4;2;241;7,u;m
4295,ICLR,2020,A new perspective in understanding of Adam-Type algorithms and beyond,Zeyi Tao;Qi Xia;Qun Li,ztao@email.wm.edu;qxia01@email.wm.edu;liqun@cs.wm.edu,3;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,College of William and Mary;College of William and Mary;College of William and Mary,154;154;154,235;235;235,1;9;8,9/25/19,1,0,0,0,0,0,27;21;1062,5;28;64,2;2;11,2;0;31,m;m
4296,ICLR,2020,Subjective Reinforcement Learning for Open Complex Environments,Zhile Yang*;Haichuan Gao*;Xin Su;Shangqi Guo;Feng Chen,yzl18@mails.tsinghua.edu.cn;ghc18@mails.tsinghua.edu.cn;suxin16@mails.tsinghua.edu.cn;gsq15@mails.tsinghua.edu.cn;chenfeng@mail.tsinghua.edu.cn,3;3;1,I do not know much about this area.:N/A:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University,8;8;8;8;8,23;23;23;23;23,,9/25/19,0,0,0,0,0,0,586;0;56;13;320,84;1;42;8;16,13;0;4;2;4,19;0;2;1;32,u;m
4297,ICLR,2020,Discriminative Variational Autoencoder for Continual Learning with Generative Replay,Woo-Young Kang;Cheol-Ho Han;Byoung-Tak Zhang,rkddndud50@gmail.com;chhan@bi.snu.ac.kr;btzhang@bi.snu.ac.kr,1;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,7,0,yes,9/25/19,Kakao Brain;Seoul National University;Seoul National University,-1;41;41,-1;64;64,5,9/25/19,1,0,1,0,0,0,46;7;50,21;8;44,3;2;3,1;0;2,u;m
4298,ICLR,2020,BOSH: An Efficient Meta Algorithm for Decision-based Attacks,Zhenxin Xiao;Puyudi Yang;Yuchen Jiang;Kai-Wei Chang;Cho-Jui Hsieh,alanshawzju@gmail.com;pydyang@ucdavis.edu;jyc@zju.edu.cn;kw@kwchang.net;chohsieh@cs.ucla.edu,3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,"Zhejiang University;University of California, Davis;Zhejiang University;University of Virginia Main Campus;University of California, Los Angeles",56;79;56;59;20,107;55;107;107;17,4;11,9/10/19,0,0,0,0,0,0,18;31;2;30;12827,12;6;14;27;168,3;3;1;3;41,1;7;0;2;1746,m;m
4299,ICLR,2020,Behavior Regularized Offline Reinforcement Learning,Yifan Wu;George Tucker;Ofir Nachum,yw4@andrew.cmu.edu;gjt@google.com;ofirnachum@google.com,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Carnegie Mellon University;Google;Google,1;-1;-1,27;-1;-1,,9/25/19,15,10,11,3,0,3,527;2705;1060,113;75;42,13;21;15,44;301;157,m;m
4300,ICLR,2020,HyperEmbed:  Tradeoffs Between Resources and Performance in NLP Tasks with Hyperdimensional Computing enabled embedding of n-gram statistics ,Pedro Alonso;Kumar Shridhar;Denis Kleyko;Evgeny Osipov;Marcus Liwicki,pedro.alonso@ltu.se;kumar@neuralspace.ai;denis.kleyko@ltu.se;evgeny.osipov@ltu.se;marcus.liwicki@ltu.se,3;1;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,3,0,yes,9/25/19,Luleå University of Technology;TU Kaiserslautern;Luleå University of Technology;Luleå University of Technology;Luleå University of Technology,481;154;481;481;481,1397;601;1397;1397;1397,3,9/25/19,2,0,1,0,0,0,499;72;348;46;4700,15;12;52;17;260,5;5;11;4;31,6;11;12;2;377,m;m
4301,ICLR,2020,Semi-Supervised Learning with Normalizing Flows,Pavel Izmailov;Polina Kirichenko;Marc Finzi;Andrew Wilson,izmailovpavel@gmail.com;pk1822@nyu.edu;maf820@nyu.edu;andrew@cornell.edu,6;1;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,1,yes,9/25/19,New York University;New York University;New York University;Cornell University,25;25;25;7,29;29;29;19,,9/25/19,10,5,1,0,0,0,511;42;91;2814,16;13;5;102,10;4;4;27,90;7;16;340,m;m
4302,ICLR,2020,Attacking Lifelong Learning Models with Gradient Reversion,Yunhui Guo;Mingrui Liu;Yandong Li;Liqiang Wang;Tianbao Yang;Tajana Rosing,yug185@eng.ucsd.edu;mingrui-liu@uiowa.edu;lyndon.leeseu@outlook.com;lwang@cs.ucf.edu;tianbao-yang@uiowa.edu;tajana@ucsd.edu,3;3;3,I have published one or two papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,"University of California, San Diego;University of Iowa;University of Central Florida;University of Central Florida;University of Iowa;University of California, San Diego",11;154;77;77;154;11,31;227;609;609;227;31,4,9/25/19,0,0,0,0,0,0,104;238;26;1838;3236;794,19;29;11;142;187;73,5;9;3;24;29;13,15;10;4;160;351;40,m;f
4303,ICLR,2020,Deep amortized clustering,Juho Lee;Yoonho Lee;Yee Whye Teh,juho@aitrics.com;einet89@gmail.com;y.w.teh@stats.ox.ac.uk,3;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,AITRICS;;University of Oxford,-1;-1;50,-1;-1;1,,9/25/19,2,0,2,0,0,0,74;271;23746,19;34;247,2;8;53,17;31;3235,m;m
4304,ICLR,2020,Laconic Image Classification: Human vs. Machine Performance,Javier Carrasco;Aidan Hogan;Jorge Pérez,jaco_1031@hotmail.com;aidhog@gmail.com;jorge.perez.rojas@gmail.com,1;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,4,0,yes,9/25/19,Universidad de Chile;Universidad de Chile;Universidad de Chile,323;323;323,848;848;848,,9/25/19,0,0,0,0,0,0,499;3069;2675,57;133;92,11;31;25,26;285;363,m;m
4305,ICLR,2020,Understanding and Stabilizing GANs' Training Dynamics with Control Theory,Kun Xu;Chongxuan Li;Huanshu Wei;Jun Zhu;Bo Zhang,kunxu.thu@gmail.com;chongxuanli1991@gmail.com;weihuanshu94@hotmail.com;dcszj@mail.tsinghua.edu.cn;dcszb@mail.tsinghua.edu.cn,6;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0,yes,9/25/19,Tsinghua University;Tsinghua University;;Tsinghua University;Tsinghua University,8;8;-1;8;8,23;23;-1;23;23,5;4,9/25/19,0,0,0,0,0,0,11658;504;2;-1;318,322;19;3;-1;103,43;7;1;-1;6,726;60;0;0;46,m;m
4306,ICLR,2020,Benefits of Overparameterization in Single-Layer Latent Variable Generative Models,Rares-Darius Buhai;Andrej Risteski;Yoni Halpern;David Sontag,rbuhai@mit.edu;aristesk@andrew.cmu.edu;yhalpern@google.com;dsontag@csail.mit.edu,3;3;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0,yes,9/25/19,Massachusetts Institute of Technology;Carnegie Mellon University;Google;Massachusetts Institute of Technology,2;1;-1;2,5;27;-1;5,8,6/28/19,6,2,0,0,0,0,42;634;636;5718,4;35;22;100,2;12;9;32,0;68;93;784,m;m
4307,ICLR,2020,Detecting Noisy Training Data with Loss Curves,Geoff Pleiss;Tianyi Zhang;Ethan R. Elenberg;Kilian Q. Weinberger,geoff@cs.cornell.edu;tz58@cornell.edu;eelenberg@asapp.com;kqw4@cornell.edu,3;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Cornell University;Cornell University;ASAPP Inc;Cornell University,7;7;-1;7,19;19;-1;19,8,9/25/19,0,0,0,0,0,0,1577;213;271;24205,17;62;17;165,11;7;7;54,247;14;32;3866,m;m
4308,ICLR,2020,On Concept-Based Explanations in Deep Neural Networks,Chih-Kuan Yeh;Been Kim;Sercan Arik;Chun-Liang Li;Pradeep Ravikumar;Tomas Pfister,cjyeh@cs.cmu.edu;beenkim.mit@gmail.com;soarik@google.com;chunliang.tw@gmail.com;pradeep.ravikumar@gmail.com;tpfister@google.com,6;6;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,9,0,yes,9/25/19,Carnegie Mellon University;Google;Google;Google;Carnegie Mellon University;Google,1;-1;-1;-1;1;-1,27;-1;-1;-1;27;-1,,9/25/19,4,2,0,0,0,0,284;3171;1391;1278;8670;2335,21;52;54;90;181;47,7;22;17;18;38;16,45;330;119;114;1214;285,m;m
4309,ICLR,2020,Trajectory representation learning for Multi-Task NMRDPs planning,Firas JARBOUI;Vianney PERCHET;Roman EGGER,firasjarboui@gmail.com;vianney.perchet@gmail.com;roman.egger@fh-salzburg.ac.at,6;3;3,I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,ENS Paris-Saclay;ENS Paris-Saclay;,481;481;-1,644;644;-1,,9/25/19,0,0,0,0,0,0,1;625;172,4;75;51,1;15;8,0;67;17,m;m
4310,ICLR,2020,Distribution-Guided Local Explanation for Black-Box Classifiers,Weijie Fu;Meng Wang;Mengnan Du;Ninghao Liu;Shijie Hao;Xia Hu,fwj.edu@gmail.com;eric.mengwang@gmail.com;dumengnan@tamu.edu;nhliu43@tamu.edu;hfut.hsj@gmail.com;hu@cse.tamu.edu,6;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,6,0,yes,9/25/19,South China University of Technology;;Texas A&M;Texas A&M;;Texas A&M,481;-1;44;44;-1;44,501;-1;177;177;-1;177,,9/25/19,0,0,0,0,0,0,199;204;214;297;415;6269,14;120;25;28;56;241,5;7;7;9;10;37,13;17;6;13;16;624,m;m
4311,ICLR,2020,Keyword Spotter Model for Crop Pest and Disease Monitoring from Community Radio Data,Benjamin Akera;Joyce Nakatumba-Nabende;Ali Hussein;Daniel Ssendiwala;Jonathan Mukiibi,akeraben@gmail.com;jnakatumba@cis.mak.ac.ug;ali.hussein@ronininstitute.org;ssendiwaladaniel@gmail.com;jonmuk7@gmail.com,1;1;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0,yes,9/25/19,Makerere University;Makerere University;Ronin Institute;;,481;481;-1;-1;-1,605;605;-1;-1;-1,,9/25/19,0,0,0,0,0,0,0;12;22;0;0,2;9;6;1;1,0;2;1;0;0,0;0;2;0;0,m;m
4312,ICLR,2020,Shallow VAEs with RealNVP Prior Can Perform as Well as Deep Hierarchical VAEs,Haowen Xu;Wenxiao Chen;Jinlin Lai;Zhihan Li;Youjian Zhao;Dan Pei,xhw15@mails.tsinghua.edu.cn;chen-wx17@mails.tsinghua.edu.cn;laijl16@mails.tsinghua.edu.cn;lizhihan17@mails.tsinghua.edu.cn;zhaoyoujian@tsinghua.edu.cn;peidan@tsinghua.edu.cn,6;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University,8;8;8;8;8;8,23;23;23;23;23;23,5,9/25/19,0,0,0,0,0,0,163;182;2;201;37;175,18;13;2;14;9;21,4;6;1;4;2;6,10;11;0;8;5;15,m;m
4313,ICLR,2020,Adaptive network sparsification with dependent variational beta-Bernoulli dropout,Juho Lee;Saehoon Kim;Jaehong Yoon;Hae Beom Lee;Eunho Yang;Sung Ju Hwang,juho@aitrics.com;shkim@aitrics.com;jaehong.yoon@kaist.ac.kr;haebeom.lee@kaist.ac.kr;eunhoy@kaist.ac.kr;sjhwang82@kaist.ac.kr,6;3;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A,Reject,0,6,0,yes,9/25/19,AITRICS;AITRICS;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,-1;-1;481;481;481;481,-1;-1;110;110;110;110,,5/28/18,5,2,3,0,2,0,1550;424;260;49;1067;1128,108;35;14;88;75;71,20;10;4;4;16;16,105;68;38;4;169;125,m;m
4314,ICLR,2020,Scalable Generative Models for Graphs with Graph Attention Mechanism,Wataru Kawai;Yusuke Mukuta;Tatsuya Harada,w-kawai@mi.t.u-tokyo.ac.jp;mukuta@mi.t.u-tokyo.ac.jp;harada@mi.t.u-tokyo.ac.jp,3;1;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,9,0,yes,9/25/19,The University of Tokyo;The University of Tokyo;The University of Tokyo,56;56;56,36;36;36,5;10,6/5/19,1,1,0,0,0,0,16;68;2594,14;24;213,2;5;26,2;6;327,m;m
4315,ICLR,2020,On the Dynamics and Convergence of Weight Normalization for Training Neural Networks,Yonatan Dukler;Quanquan Gu;Guido Montufar,ydukler@math.ucla.edu;qgu@cs.ucla.edu;montufar@math.ucla.edu,3;6;3,I have published one or two papers in this area.:N/A:N/A:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,"University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles",20;20;20,17;17;17,1,9/25/19,0,0,0,0,0,0,9;3895;1193,6;174;60,1;34;14,0;411;77,m;m
4316,ICLR,2020,Semi-Supervised Few-Shot Learning with Prototypical Random Walks,Ahmed Ayyad;Nassir Navab;Mohamed Elhoseiny;Shadi Albarqouni,a.3ayad@gmail.com;nassir.navab@tum.de;mohamed.elhoseiny@gmail.com;shadi.albarqouni@tum.de,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,4,0,yes,9/25/19,Technical University Munich;Technical University Munich;KAUST;Technical University Munich,53;53;128;53,43;43;1397;43,6;10,3/6/19,1,1,0,0,0,1,160;4587;1447;19,39;195;67;10,7;23;18;1,5;414;189;4,m;m
4317,ICLR,2020,Agent as Scientist: Learning to Verify Hypotheses,Kenneth Marino;Rob Fergus;Arthur Szlam;Abhinav Gupta,kdmarino@cs.cmu.edu;fergus@cs.nyu.edu;aszlam@fb.com;abhinavg@cs.cmu.edu,3;3;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,5,0,yes,9/25/19,Carnegie Mellon University;New York University;Facebook;Carnegie Mellon University,1;25;-1;1,27;29;-1;27,,9/25/19,0,0,0,0,0,0,310;53064;8926;18151,6;128;87;233,4;61;33;65,41;6486;940;1966,m;m
4318,ICLR,2020,Generating Robust Audio Adversarial Examples using Iterative Proportional Clipping,Hongting Zhang;Qiben Yan;Pan Zhou,htzhang@hust.edu.cn;qyan@msu.edu;panzhou@hust.edu.cn,3;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,6,0,yes,9/25/19,Hong Kong University of Science and Technology;SUN YAT-SEN UNIVERSITY;Hong Kong University of Science and Technology,39;481;39,47;299;47,4,9/25/19,0,0,0,0,0,0,116;746;110,22;63;64,8;14;5,8;50;6,u;m
4319,ICLR,2020,Lean Images for Geo-Localization,Moti Kadosh;Yael Moses;Ariel Shamir,arik@idc.ac.il;yael@idc.ac.il,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,0,0,yes,9/25/19,interdisciplinary center herzliya;interdisciplinary center herzliya,-1;-1,-1;-1,2,9/25/19,0,0,0,0,0,0,0;2079;7871,2;59;154,0;19;41,0;150;860,m;m
4320,ICLR,2020,Are Powerful Graph Neural Nets Necessary? A Dissection on Graph Classification,Ting Chen;Song Bian;Yizhou Sun,iamtingchen@gmail.com;biansonghz@gmail.com;yzsun@cs.ucla.edu,3;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,5,0,yes,9/25/19,"Google;Zhejiang University;University of California, Los Angeles",-1;56;20,-1;107;17,10,5/11/19,10,4,2,1,7,1,488;166;6951,42;38;187,10;7;38,89;11;766,m;f
4321,ICLR,2020,DEEP GRAPH SPECTRAL EVOLUTION NETWORKS FOR GRAPH TOPOLOGICAL TRANSFORMATION,Liang Zhao;Qingzhe Li;Negar Etemadyrad;Xiaojie Guo,lzhao9@gmu.edu,6;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,George Mason University,100,282,10,9/25/19,0,0,0,0,0,0,69;4;7;2633,49;6;4;181,3;2;2;26,2;1;0;360,m;m
4322,ICLR,2020,Robust Federated Learning Through Representation Matching and Adaptive Hyper-parameters,Hesham Mostafa,hesham.mostafa@intel.com,3;3;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Intel,-1,-1,,9/25/19,0,0,0,0,0,0,654,27,11,64,m
4323,ICLR,2020,Enforcing Physical Constraints in Neural Neural Networks through Differentiable PDE Layer,"Chiyu Max"" Jiang;Karthik Kashinath;Prabhat;Philip Marcus""",chiyu.jiang@berkeley.edu;kkashinath@lbl.gov;prabhat@lbl.gov;pmarcus@me.berkeley.edu,3;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,0,0,yes,9/25/19,University of California Berkeley;Lawrence Berkeley National Lab;Lawrence Berkeley National Lab;University of California Berkeley,5;-1;-1;5,13;-1;-1;13,5;4,9/25/19,0,0,0,0,0,0,71;539;3367;2553,13;89;151;39,4;13;28;11,12;31;199;235,m;m
4324,ICLR,2020,Evo-NAS: Evolutionary-Neural Hybrid Agent for Architecture Search,Krzysztof Maziarz;Mingxing Tan;Andrey Khorlin;Kuang-Yu Samuel Chang;Andrea Gesmundo,krzysztof.s.maziarz@gmail.com;tanmingxing@google.com;akhorlin@google.com;kysc@google.com;agesmundo@google.com,3;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,11,0,yes,9/25/19,Jagiellonian University;Google;Google;Google;Google,481;-1;-1;-1;-1,610;-1;-1;-1;-1,,9/25/19,0,0,0,0,0,0,445;95;643;581;277,8;13;17;38;26,2;4;5;8;8,33;13;91;83;31,m;m
4325,ICLR,2020,HUBERT Untangles BERT to Improve Transfer across NLP Tasks,Mehrad Moradshahi;Hamid Palangi;Monica S. Lam;Paul Smolensky;Jianfeng Gao,mehrad@stanford.edu;hpalangi@microsoft.com;lam@cs.stanford.edu;paul.smolensky@gmail.com;jfgao@microsoft.com,1;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,1,4,0,yes,9/25/19,Stanford University;Microsoft;Stanford University;Microsoft;Microsoft,4;-1;4;-1;-1,4;-1;4;-1;-1,3,9/25/19,2,1,0,0,0,0,15;739;16394;10024;18900,8;34;235;206;353,2;11;62;40;61,1;54;1656;933;2683,m;m
4326,ICLR,2020,Learning De-biased Representations with Biased Representations,Hyojin Bahng;Sanghyuk Chun;Sangdoo Yun;Jaegul Choo;Seong Joon Oh,hjj552@korea.ac.kr;sanghyuk.c@navercorp.com;sangdoo.yun@navercorp.com;jchoo@korea.ac.kr;coallaoh@linecorp.com,6;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,Korea University;NAVER;NAVER;Korea University;LINE,323;-1;-1;323;-1,179;-1;-1;179;-1,,9/25/19,4,3,2,0,0,0,37;156;1053;2476;675,7;18;38;124;30,4;5;11;22;12,5;43;209;394;92,f;m
4327,ICLR,2020,ICNN: INPUT-CONDITIONED FEATURE REPRESENTATION LEARNING FOR TRANSFORMATION-INVARIANT NEURAL NETWORK,Suraj Tripathi;Chirag Singh;Abhay Kumar,surajtripathi93@gmail.com;c.singh@samsung.com;abykumar12011@gmail.com,3;1;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,1,0,yes,9/25/19,;Samsung;University of Southern California,-1;-1;31,-1;-1;62,1,9/25/19,0,0,0,0,0,0,45;48;83,23;17;59,2;3;6,4;4;2,m;m
4328,ICLR,2020,Learning Latent State Spaces for Planning through Reward Prediction,Aaron Havens;Yi Ouyang;Prabhat Nagarajan;Yasuhiro Fujita,ahavens2@illinois.edu;ouyangyi@preferred-america.com;prabhat@preferred.jp;fujita@preferred.jp,3;3;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,"University of Illinois, Urbana Champaign;Preferred Networks, Inc.;Preferred Networks, Inc.;Preferred Networks, Inc.",3;-1;-1;-1,48;-1;-1;-1,,9/25/19,1,1,0,0,0,0,1698;30;66;39,36;24;19;14,19;4;4;3,96;1;2;3,m;m
4329,ICLR,2020,Certified Robustness to Adversarial Label-Flipping Attacks via Randomized Smoothing,Elan Rosenfeld;Ezra Winston;Pradeep Ravikumar;J. Zico Kolter,ekr@andrew.cmu.edu;ewinston@andrew.cmu.edu;pradeepr@cs.cmu.edu;zkolter@cs.cmu.edu,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,8,0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1,27;27;27;27,4,9/25/19,0,0,0,0,0,0,224;53;8670;66,5;15;181;10,2;4;38;4,62;13;1214;7,m;m
4330,ICLR,2020,Optimistic Adaptive Acceleration for Optimization,Jun-Kun Wang;Xiaoyun Li;Ping Li,jimwang@gatech.edu;xl374@scarletmail.rutgers.edu;liping11@baidu.com,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Georgia Institute of Technology;Rutgers University;Baidu,13;34;-1,38;168;-1,,9/27/18,1,1,0,0,2,0,48;53;5691,11;21;674,4;3;36,2;5;358,m;m
4331,ICLR,2020,Hierarchical Bayes Autoencoders,Shuangfei Zhai;Carlos Guestrin;Joshua M. Susskind,szhai@apple.com;guestrin@apple.com;jsusskind@apple.com,1;1;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Apple;Apple;Apple,-1;-1;-1,-1;-1;-1,5;4,9/25/19,0,0,0,0,0,0,581;104;1158,24;3;15,10;2;8,55;8;86,m;m
4332,ICLR,2020,Project and Forget: Solving Large Scale Metric Constrained Problems,Anna C. Gilbert;Rishi Sonthalia,annacg@umich.edu;rsonthal@umich.edu,6;3;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,8,1,yes,9/25/19,University of Michigan;University of Michigan,8;8,21;21,1;10,9/25/19,1,1,0,0,0,0,12705;11,151;5,35;3,1524;0,f;m
4333,ICLR,2020,Deep Auto-Deferring Policy for Combinatorial Optimization,Sungsoo Ahn;Younggyo Seo;Jinwoo Shin,sungsoo.ahn@kaist.ac.kr;younggyo.seo@kaist.ac.kr;jinwoos@kaist.ac.kr,6;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,481;481;481,110;110;110,10,9/25/19,0,0,0,0,0,0,6;5;1752,20;8;186,1;1;19,0;1;225,m;m
4334,ICLR,2020,Under what circumstances do local codes emerge in feed-forward neural networks,Ella M. Gale;Nicolas Martin,ella.gale@bristol.ac.uk;nm13850@bristol.ac.uk,1;3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Reject,0,0,0,yes,9/25/19,University of Bristol;University of Bristol,128;128,87;87,,9/25/19,0,0,0,0,0,0,54;0,9;2,2;0,4;0,f;m
4335,ICLR,2020,Reinforcement Learning with Structured Hierarchical Grammar Representations of Actions,Petros Christodoulou;Robert Lange;Ali Shafti;A. Aldo Faisal,petros.christodoulou18@imperial.ac.uk;rtl17@ic.ac.uk;a.shafti@imperial.ac.uk;a.faisal@imperial.ac.uk,3;8;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Imperial College London;Imperial College London;Imperial College London;Imperial College London,73;73;73;73,10;10;10;10,,9/25/19,0,0,0,0,0,0,44;873;152;1483,12;38;41;68,3;12;6;15,1;94;3;78,m;m
4336,ICLR,2020,Global Adversarial Robustness Guarantees for Neural Networks,Luca Laurenti;Andrea Patane;Matthew Wicker;Luca Bortolussi;Luca Cardelli;Marta Kwiatkowska,luca.laurenti@cs.ox.ac.uk;andrea.patane@chch.ox.ac.uk;matthew.wicker@wolfson.ox.ac.uk;luca.bortolussi@gmail.com;luca.a.cardelli@gmail.com;marta.kwiatkowska@cs.ox.ac.uk,3;1;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,University of Oxford;University of Oxford;University of Oxford;;;University of Oxford,50;50;50;-1;-1;50,1;1;1;-1;-1;1,11;4;1,9/25/19,0,0,0,0,0,0,6787;51;143;1650;14639;29,456;11;10;192;324;15,41;4;4;21;60;2,294;3;8;88;1517;3,m;f
4337,ICLR,2020,Learning Calibratable Policies using Programmatic Style-Consistency,Eric Zhan;Albert Tseng;Yisong Yue;Adith Swaminathan;Matthew Hausknecht,ezhan@caltech.edu;atseng@caltech.edu;yyue@caltech.edu;adswamin@microsoft.com;mahauskn@microsoft.com,6;3;3,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,California Institute of Technology;California Institute of Technology;California Institute of Technology;Microsoft;Microsoft,143;143;143;-1;-1,2;2;2;-1;-1,,9/25/19,0,0,0,0,0,0,41;73;3259;977;2961,6;9;122;35;38,3;3;30;15;17,4;2;395;127;322,m;m
4338,ICLR,2020,Alternating Recurrent Dialog Model with Large-Scale Pre-Trained Language Models,Qingyang Wu;Yichi Zhang;Yu Li;Zhou Yu,wilwu@ucdavis.edu;zhangyic17@mails.tsinghua.edu.cn;yooli@ucdavis.edu;joyu@ucdavis.edu,1;3;8,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,"University of California, Davis;Tsinghua University;University of California, Davis;University of California, Davis",79;8;79;79,55;23;55;55,3,9/25/19,4,2,3,0,0,1,33;13;8;553,11;6;3;142,4;2;2;11,2;3;1;29,m;f
4339,ICLR,2020,Learning to Reason: Distilling Hierarchy via Self-Supervision and Reinforcement Learning,Jung-Su Ha;Young-Jin Park;Hyeok-Joo Chae;Soon-Seo Park;Han-Lim Choi,jung-su.ha@ipvs.uni-stuttgart.de;yjpark@lics.kaist.ac.kr;hjchae@lics.kaist.ac.kr;sspark@lics.kaist.ac.kr;hanlimc@kaist.ac.kr,6;1;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,10,0,yes,9/25/19,University of Stuttgart;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,95;481;481;481;481,292;110;110;110;110,,9/25/19,0,0,0,0,0,0,83;624;30;21;1614,33;124;12;19;192,5;12;3;2;17,3;36;1;1;144,m;m
4340,ICLR,2020,The Geometry of Sign Gradient Descent,Lukas Balles;Fabian Pedregosa;Nicolas Le Roux,lukas.balles@tuebingen.mpg.de;f@bianp.net;nicolas@le-roux.name,3;1;3,I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,"Max Planck Institute for Intelligent Systems, Max-Planck Institute;Google;Google",-1;-1;-1,-1;-1;-1,,9/25/19,0,0,0,0,0,0,230;24989;5013,10;53;172,7;16;24,33;1655;536,m;m
4341,ICLR,2020,Effective Use of Variational Embedding Capacity in Expressive End-to-End Speech Synthesis,Eric Battenberg;Soroosh Mariooryad;Daisy Stanton;RJ Skerry-Ryan;Matt Shannon;David Kao;Tom Bagby,ebattenberg@google.com;soroosh@google.com;daisy@google.com;rjryan@google.com;mattshannon@google.com;davidkao@google.com;tombagby@google.com,6;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0,yes,9/25/19,Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,1,6/8/19,10,2,1,0,0,0,2826;404;938;1443;568;282;138,31;20;12;14;29;22;10,14;13;9;10;13;7;6,268;21;167;279;39;18;5,m;m
4342,ICLR,2020,The problem with DDPG: understanding failures in deterministic environments with sparse rewards,Guillaume Matheron;Olivier Sigaud;Nicolas Perrin,matheron@isir.upmc.fr;olivier.sigaud@upmc.fr;perrin@isir.upmc.fr,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,4,0,yes,9/25/19,"Computer Science Lab  - Pierre and Marie Curie University, Paris, France;Computer Science Lab  - Pierre and Marie Curie University, Paris, France;Computer Science Lab  - Pierre and Marie Curie University, Paris, France",481;481;481,1397;1397;1397,,9/25/19,3,1,0,0,0,0,9;81;1040,4;15;93,2;4;19,0;5;59,m;m
4343,ICLR,2020,Enhanced Convolutional Neural Tangent Kernels,Dingli Yu;Ruosong Wang;Zhiyuan Li;Wei Hu;Ruslan Salakhutdinov;Sanjeev Arora;Simon S. Du,dingliy@cs.princeton.edu;ruosongw@andrew.cmu.edu;zhiyuanli@cs.princeton.edu;huwei@cs.princeton.edu;rsalakhu@cs.cmu.edu;arora@cs.princeton.edu;ssdu@ias.edu,3;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,6,0,yes,9/25/19,"Princeton University;Carnegie Mellon University;Princeton University;Princeton University;Carnegie Mellon University;Princeton University;Institue for Advanced Study, Princeton",31;1;31;31;1;31;-1,6;27;6;6;27;6;-1,,9/25/19,15,9,3,1,0,2,958;13;675;934;69005;2317;2086,99;3;13;40;254;82;55,16;2;10;8;82;14;20,53;0;116;55;7875;254;313,m;m
4344,ICLR,2020,COMBINED FLEXIBLE ACTIVATION FUNCTIONS FOR DEEP NEURAL NETWORKS,Renlong Jie;Junbin Gao;Andrey Vasnev;Minh-Ngoc Tran,renlong.jie@sydney.edu.au;junbin.gao@sydney.edu.au;andrey.vasnev@sydney.edu.au;minh-ngoc.tran@sydney.edu.au,1;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,University of Sydney;University of Sydney;University of Sydney;University of Sydney,86;86;86;86,60;60;60;60,,9/25/19,0,0,0,0,0,0,22;105;153;12,3;35;32;3,1;3;6;1,1;0;19;2,m;m
4345,ICLR,2020,Meta-RCNN: Meta Learning for Few-Shot Object Detection,Xiongwei Wu;Doyen Sahoo;Steven C. H. Hoi,xwwu.2015@smu.edu.sg;dsahoo@salesforce.com;shoi@salesforce.com,6;3;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,3,0,yes,9/25/19,Singapore Management University;SalesForce.com;SalesForce.com,92;-1;-1,1397;-1;-1,6;2,9/25/19,0,0,0,0,0,0,166;368;9066,47;27;295,8;10;50,6;50;849,m;m
4346,ICLR,2020,AutoSlim: Towards One-Shot Architecture Search for Channel Numbers,Jiahui Yu;Thomas Huang,jyu79@illinois.edu;t-huang1@illinois.edu,6;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,3,0,yes,9/25/19,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",3;3,48;48,,3/27/19,53,14,16,1,0,7,1771;71040,61;1537,13;121,260;6438,m;m
4347,ICLR,2020,Learning from Imperfect Annotations: An End-to-End Approach,Emmanouil Antonios Platanios;Maruan Al-Shedivat;Eric Xing;Tom Mitchell,e.a.platanios@cs.cmu.edu;alshedivat@cs.cmu.edu;epxing@cs.cmu.edu;tom.mitchell@cs.cmu.edu,6;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,4,1,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1,27;27;27;27,,9/25/19,0,0,0,0,0,0,617;750;25065;76,23;35;605;20,8;13;77;5,50;79;2695;4,m;m
4348,ICLR,2020,Feature Map Transform Coding for Energy-Efficient CNN Inference,Brian Chmiel;Chaim Baskin;Ron Banner;Evgenii Zheltonozhskii;Yevgeny Yermolin;Alex Karbachevsky;Alex M. Bronstein;Avi Mendelson,brian.chmiel@intel.com;chaimbaskin@cs.technion.ac.il;ron.banner@intel.com;evgeniizh@campus.technion.ac.il;yevgeny_ye@campus.technion.ac.il;alex.k@cs.technion.ac.il;bron@cs.technion.ac.il;avi.mendelson@cs.technion.ac.il,3;3;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:N/A:N/A,Reject,0,3,0,yes,9/25/19,Intel;Technion;Intel;Technion;Technion;Technion;Technion;Technion,-1;26;-1;26;26;26;26;26,-1;412;-1;412;412;412;412;412,2,5/26/19,2,2,1,0,0,0,9;92;532;78;2;2;136;1693,8;14;31;10;2;2;16;138,2;5;11;4;1;1;6;23,0;11;82;8;0;0;6;122,m;m
4349,ICLR,2020,Zero-Shot Out-of-Distribution Detection with Feature Correlations,Chandramouli S Sastry;Sageev Oore,chandramouli.sastry@gmail.com;osageev@gmail.com,3;8;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,11,0,yes,9/25/19,Dalhousie University;Dalhousie University,323;323,269;269,,9/25/19,2,0,0,0,0,0,5;334,8;28,2;9,0;32,m;m
4350,ICLR,2020,Neural Video Encoding,Abel Brown;Robert DiPietro,abelb@nvidia.com;rdipietro@nvidia.com,1;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:N/A:I read the paper thoroughly.:N/A;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,NVIDIA;NVIDIA,-1;-1,-1;-1,3;2,9/25/19,0,0,0,0,0,0,375;274,22;26,9;8,41;32,m;m
4351,ICLR,2020,Continuous Adaptation in Multi-agent Competitive Environments,Kuei-Tso Lee;Sheng-Jyh Wang,fuj30089@gmail.com;shengjyh@faculty.nctu.edu.tw,1;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,;National Chiao Tung University,-1;143,-1;564,,9/25/19,0,0,0,0,0,0,0;551,2;68,0;13,0;46,u'm
4352,ICLR,2020,Simple and Effective Stochastic Neural Networks,Tianyuan Yu;Yongxin Yang;Da Li;Timothy Hospedales;Tao Xiang,tianyuan.yu@surrey.ac.uk;yongxin.yang@surrey.ac.uk;dali.darren@hotmail.com;t.hospedales@ed.ac.uk;t.xiang@surrey.ac.uk,6;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,University of Surrey;University of Surrey;;University of Edinburgh;University of Surrey,205;205;-1;33;205,260;260;-1;30;260,4;11;8,9/25/19,0,0,0,0,0,0,32;2148;29;384;2099,16;66;26;31;109,3;21;3;9;21,3;335;0;34;181,m;m
4353,ICLR,2020,On The Difficulty of Warm-Starting Neural Network Training,Jordan T. Ash;Ryan P. Adams,jordanta@cs.princeton.edu;rpa@princeton.edu,3;1;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,3,0,yes,9/25/19,Princeton University;Princeton University,31;31,6;6,8,9/25/19,5,1,1,0,0,0,116;12643,15;178,5;45,12;1340,m;m
4354,ICLR,2020,Thwarting finite difference adversarial attacks with output randomization,Haidar Khan;Dan Park;Azer Khan;Bülent Yener,haidark@gmail.com;parkd5@gmail.com;azerkkhan@gmail.com;byener@gmail.com,3;6;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,7,0,yes,9/25/19,Amazon;;;,-1;-1;-1;-1,-1;-1;-1;-1,4;1,5/23/19,0,0,0,0,0,0,53;173;0;131,15;26;2;21,3;3;0;6,9;10;0;6,m;m
4355,ICLR,2020,Benchmarking Robustness in Object Detection: Autonomous Driving when Winter is Coming,Claudio Michaelis;Benjamin Mitzkus;Robert Geirhos;Evgenia Rusak;Oliver Bringmann;Alexander S. Ecker;Matthias Bethge;Wieland Brendel,claudio.michaelis@uni-tuebingen.de;benjamin.mitzkus@uni-tuebingen.de;robert@geirhos.de;evgenia.rusak@bethgelab.org;oliver.bringmann@uni-tuebingen.de;alexander.ecker@uni-tuebingen.de;matthias@bethgelab.org;wieland.brendel@bethgelab.org,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Reject,0,6,0,yes,9/25/19,"University of Tuebingen;University of Tuebingen;University of Tuebingen;Centre for Integrative Neuroscience, AG Bethge;University of Tuebingen;University of Tuebingen;Centre for Integrative Neuroscience, AG Bethge;Centre for Integrative Neuroscience, AG Bethge",154;154;154;-1;154;154;-1;-1,91;91;91;-1;91;91;-1;-1,2,7/17/19,15,8,3,0,0,1,415;12;585;51;1433;6377;11714;1799,11;1;10;8;235;124;414;40,6;1;5;4;20;24;47;17,45;1;60;4;64;813;1275;218,m;m
4356,ICLR,2020,Negative Sampling in Variational Autoencoders,Adrián Csiszárik;Beatrix Benkő;Dániel Varga,csadrian@renyi.hu;bbeatrix1010@gmail.com;daniel@renyi.hu,3;6;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,5,0,yes,9/25/19,Alfréd Rényi Institute of Mathematics;Eotvos Lorand University;Alfréd Rényi Institute of Mathematics,-1;-1;-1,-1;-1;-1,5;4,9/25/19,0,0,0,0,0,0,14;0;621,4;1;36,2;0;8,0;0;87,m;m
4357,ICLR,2020,An Empirical and Comparative Analysis of Data Valuation with Scalable Algorithms,Ruoxi Jia;Xuehui Sun;Jiacen Xu;Ce Zhang;Bo Li;Dawn Song,ruoxijia@berkeley.edu;zidaneandmessi@sjtu.edu.cn;coldstudy@sjtu.edu.cn;ce.zhang@inf.ethz.ch;lxbosky@gmail.com;dawnsong@gmail.com,1;1;3,I have read many papers in this area.:I did not assess the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,9,0,yes,9/25/19,University of California Berkeley;Shanghai Jiao Tong University;Shanghai Jiao Tong University;Swiss Federal Institute of Technology;University of California Berkeley;University of California Berkeley,5;53;53;10;5;5,13;157;157;13;13;13,,9/25/19,0,0,0,0,0,0,414;475;10;877;427;37432,36;16;8;65;67;278,13;9;2;14;9;95,24;24;0;81;32;4087,f;f
4358,ICLR,2020,Attraction-Repulsion Actor-Critic for Continuous Control Reinforcement Learning,Thang Doan;Bogdan Mazoure;Audrey Durand;Joelle Pineau;R Devon Hjelm,thang.doan@mail.mcgill.ca;bogdan.mazoure@mail.mcgill.ca;audrey.durand@ift.ulaval.ca;jpineau@cs.mcgill.ca;devon.hjelm@microsoft.com,3;3;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,McGill University;McGill University;Laval university;McGill University;Microsoft,86;86;481;86;-1,42;42;272;42;-1,,9/17/19,0,0,0,0,0,0,169;53;245;11328;1720,9;17;33;267;43,3;5;9;46;13,7;5;14;1235;268,f;m
4359,ICLR,2020,NormLime: A New Feature Importance Metric for Explaining Deep Neural Networks,Isaac Ahern;Adam Noack;Luis Guzman-Nateras;Dejing Dou;Boyang Li;Jun Huan,isaac@biofidelic.com;anoack2@uoregon.edu;lguzmann@uoregon.edu;dou@cs.uoregon.edu;boyangli@baidu.com;huanjun@baidu.com,3;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,University of Oregon;University of Oregon;University of Oregon;University of Oregon;Baidu;Baidu,205;205;205;205;-1;-1,288;288;288;288;-1;-1,,9/10/19,5,0,4,0,0,0,8;8;4;2318;1219;2722,3;4;1;153;115;194,2;2;1;24;20;23,0;0;0;182;43;240,m;m
4360,ICLR,2020,Unsupervised Learning of Node Embeddings by Detecting Communities,Chi Thang Duong;Dung Hoang;Truong Giang Le Ba;Thanh Le Cong;Hongzhi Yin;Matthias Weidlich;Quoc Viet Hung Nguyen;Karl Aberer,thang.duong@epfl.ch;dungmin97@gmail.com;giangpna98@gmail.com;thanhcls1316@gmail.com;h.yin1@uq.edu.au;matthias.weidlich@hu-berlin.de;quocviethung1@gmail.com;karl.aberer@epfl.ch,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,4,0,yes,9/25/19,Swiss Federal Institute of Technology Lausanne;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;University of Queensland;Humboldt Universität Berlin;;Swiss Federal Institute of Technology Lausanne,481;39;39;39;205;266;-1;481,38;47;47;47;66;73;-1;38,10,9/25/19,0,0,0,0,0,0,262;210;1;0;2758;7;342;12843,22;35;2;1;133;6;44;631,10;6;1;0;27;2;10;53,14;12;0;0;160;1;18;995,m;m
4361,ICLR,2020,Global graph curvature,Liudmila Prokhorenkova;Egor Samosvat;Pim van der Hoorn,ostroumova-la@yandex-team.ru;sameg@yandex-team.ru;pimvdhoorn@gmail.com,6;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0,yes,9/25/19,Yandex;Yandex;Eindhoven University of Technology,-1;-1;205,-1;-1;185,10,9/25/19,0,0,0,0,0,0,31;77;108,9;18;20,3;4;6,2;6;4,f;m
4362,ICLR,2020,Invertible generative models for  inverse problems: mitigating representation error and dataset bias,Muhammad Asim;Ali Ahmed;Paul Hand,msee16001@itu.edu.pk;ali.ahmed@itu.edu.pk;p.hand@northeastern.edu,6;1;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I did not assess the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,4,0,yes,9/25/19,"ITU of Punjab Lahore, Pakistan;ITU of Punjab Lahore, Pakistan;Northeastern University",-1;-1;16,-1;-1;906,5;4,5/28/19,9,3,5,2,0,3,586;214;1633,61;76;67,13;9;19,30;15;151,m;m
4363,ICLR,2020,Decaying momentum helps neural network training,John Chen;Anastasios Kyrillidis,jc114@rice.edu;anastasios@rice.edu,6;3;3,I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,10,0,yes,9/25/19,Rice University;Rice University,84;84,105;105,,9/25/19,3,2,0,0,0,1,34;1140,16;73,3;17,2;86,m;m
4364,ICLR,2020,GRAPH NEIGHBORHOOD ATTENTIVE POOLING,Zekarias Tilahun Kefato;Sarunas Girdzijauskas,zekarias@kth.se;sarunasg@kth.se,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,"KTH Royal Institute of Technology, Stockholm, Sweden;KTH Royal Institute of Technology, Stockholm, Sweden",128;128,222;222,10,9/25/19,1,0,0,0,0,0,31;34,13;8,3;3,3;5,m;m
4365,ICLR,2020,Quantum algorithm for finding the negative curvature direction,Kaining Zhang;Min-Hsiu Hsieh;Liu Liu;Dacheng Tao,kzha3670@uni.sydney.edu.au;min-hsiu.hsieh@uts.edu.au;liu.liu1@sydney.edu.au;dacheng.tao@sydney.edu.au,6;6;3,I have read many papers in this area.:N/A:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,University of Sydney;University of Technology Sydney;University of Sydney;University of Sydney,86;108;86;86,60;193;60;60,1;9,9/17/19,1,0,0,0,0,0,19;1464;245;80,14;124;76;29,2;21;7;5,0;82;30;4,u;m
4366,ICLR,2020,"Walking on the Edge: Fast, Low-Distortion Adversarial Examples",Hanwei Zhang;Teddy Furon;Yannis Avrithis;Laurent Amsaleg,hanwei.zhang@irisa.fr;teddy.furon@inria.fr;yannis@avrithis.net;laurent.amsaleg@irisa.fr,3;3;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,"IRISA, Université Bretagne Sud;INRIA;INRIA;IRISA, Université Bretagne Sud",481;-1;-1;481,1397;-1;-1;1397,4,9/25/19,2,0,1,0,0,0,22;2108;3566;2324,19;149;198;165,2;27;32;22,1;189;243;206,f;m
4367,ICLR,2020,"Deep Reasoning Networks:  Thinking Fast and Slow, for Pattern De-mixing",Di Chen;Yiwei Bai;Wenting Zhao;Sebastian Ament;John M. Gregoire;Carla P. Gomes,di@cs.cornell.edu;bywbilly@cs.cornell.edu;wzhao@cs.cornell.edu;ament@cs.cornell.edu;gregoire@caltech.edu;gomes@cs.cornell.edu,3;6;3,I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Cornell University;Cornell University;Cornell University;Cornell University;California Institute of Technology;Cornell University,7;7;7;7;143;7,19;19;19;19;2;19,,9/25/19,0,0,0,0,0,0,122;18;392;20;1161;4979,30;9;57;10;104;225,3;2;10;3;20;35,9;2;19;3;17;379,m;f
4368,ICLR,2020,Measuring Calibration in Deep Learning,Jeremy Nixon;Mike Dusenberry;Ghassen Jerfel;Linchuan Zhang;Dustin Tran,jeremynixon@google.com;dusenberrymw@google.com;ghassen@google.com;linchzhang@google.com;trandustin@google.com,6;3;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0,yes,9/25/19,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,,4/2/19,19,11,2,0,5,2,62;15;68;76;1809,10;1;9;14;50,5;1;6;5;20,4;2;4;5;197,m;m
4369,ICLR,2020,Towards Understanding the Regularization of Adversarial Robustness on Neural Networks,Yuxin Wen;Shuai Li;Kui Jia,wen.yuxin@mail.scut.edu.cn;lishuai918@gmail.com;kuijia@scut.edu.cn,6;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,7,0,yes,9/25/19,South China University of Technology;;South China University of Technology,481;-1;481,501;-1;501,4;8,9/25/19,0,0,0,0,0,0,4;13;1071,6;42;40,2;2;9,0;0;110,f;m
4370,ICLR,2020,BRIDGING ADVERSARIAL SAMPLES AND ADVERSARIAL NETWORKS,Faqiang Liu;Mingkun Xu;Guoqi Li;Jing Pei;Luping Shi,lfq18@mails.tsinghua.edu.cn;xmk18@mails.tsinghua.edu.cn;liguoqi@mail.tsinghua.edu.cn;peij@mail.tsinghua.edu.cn;lpshi@mail.tsinghua.edu.cn,6;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University,8;8;8;8;8,23;23;23;23;23,5;4,9/25/19,0,0,0,0,0,0,14;8;1181;22;307,10;8;161;6;17,2;1;17;2;5,0;1;69;1;7,m;m
4371,ICLR,2020,Kernel and Rich Regimes in Overparametrized Models,Blake Woodworth;Suriya Gunasekar;Pedro Savarese;Edward Moroshko;Itay Golan;Jason Lee;Daniel Soudry;Nathan Srebro,blake@ttic.edu;suriya@ttic.edu;savarese@ttic.edu;edward.moroshko@gmail.com;sitaygo@campus.technion.ac.il;jasondlee88@gmail.com;daniel.soudry@gmail.com;nati@ttic.edu,6;8;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Toyota Technological Institute at Chicago;Toyota Technological Institute at Chicago;Toyota Technological Institute at Chicago;Technion;Technion;University of Southern California;Technion;Toyota Technological Institute at Chicago,-1;-1;-1;26;26;31;26;-1,-1;-1;-1;412;412;62;412;-1,8,9/25/19,27,13,3,0,0,1,494;980;30;62;112;4788;5011;13596,16;25;6;11;4;120;76;176,9;15;2;4;4;36;26;52,40;117;1;1;8;614;642;1618,m;m
4372,ICLR,2020,The Intriguing Effects of Focal Loss on the Calibration of Deep Neural Networks,Jishnu Mukhoti;Viveka Kulharia;Amartya Sanyal;Stuart Golodetz;Philip Torr;Puneet Dokania,jishnumukhoti7@gmail.com;viveka@robots.ox.ac.uk;amartya.sanyal@cs.ox.ac.uk;stuart@five.ai;philip.torr@eng.ox.ac.uk;puneet@robots.ox.ac.uk,6;3;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,13,0,yes,9/25/19,;University of Oxford;University of Oxford;FiveAI;University of Oxford;University of Oxford,-1;50;50;-1;50;50,-1;1;1;-1;1;1,3;2,9/25/19,2,1,0,0,0,0,49;176;59;1747;1951;45,8;10;14;29;30;9,3;4;3;9;9;4,9;17;4;464;459;6,m;m
4373,ICLR,2020,Domain-Invariant Representations: A Look on Compression and Weights,Victor Bouvier;Céline Hudelot;Clément Chastagnol;Philippe Very;Myriam Tami,vbouvier@sidetrade.com;celine.hudelot@centralesupelec.fr;cchastagnol@sidetrade.com;pveryranchet@gmail.com;myriam.tami@centralesupelec.fr,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,5,0,yes,9/25/19,CentraleSupelec;CentraleSupelec;Sidetrade;;CentraleSupelec,481;481;-1;-1;481,534;534;-1;-1;534,1,9/25/19,0,0,0,0,0,0,22;2;1;2253;8,11;8;4;89;14,3;1;1;19;2,2;0;0;212;0,m;f
4374,ICLR,2020,Last-iterate convergence rates for min-max optimization,Jacob Abernethy;Kevin A. Lai;Andre Wibisono,prof@gatech.edu;nykal212@gmail.com;andrwbsn@gmail.com,6;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology,13;13;13,38;38;38,5;4;1;9,6/5/19,17,11,4,3,0,6,1891;248;819,96;20;31,23;7;10,236;30;121,m;m
4375,ICLR,2020,Implicit Generative Modeling for Efficient Exploration,Neale Ratzlaff;Qinxun Bai;Li Fuxin;Wei Xu,ratzlafn@oregonstate.edu;qinxun.bai@horizon.ai;lif@oregonstate.edu;wei.xu@horizon.ai,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:N/A:N/A,Reject,0,4,0,yes,9/25/19,Oregon State University;Horizon Robotics;Oregon State University;Horizon Robotics,77;-1;77;-1,373;-1;373;-1,5;11,9/25/19,0,0,0,0,0,0,6;188;2412;113,5;16;82;96,1;4;24;7,1;29;318;7,m;m
4376,ICLR,2020,Is There Mode Collapse? A Case Study on Face Generation and Its Black-box Calibration,Zhenyu Wu;Ye Yuan;Zhaowen Wang;Jianming Zhang;Zhangyang Wang;Hailin Jin,wuzhenyu_sjtu@tamu.edu;ye.yuan@tamu.edu;zhawang@adobe.com;jianmzha@adobe.com;atlaswang@tamu.edu;hljin@adobe.com,3;6;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,Texas A&M;Texas A&M;Adobe Systems;Adobe Systems;Texas A&M;Adobe Systems,44;44;-1;-1;44;-1,177;177;-1;-1;177;-1,5;4,9/25/19,0,0,0,0,0,0,559;104;32;1369;1;93,23;65;17;168;3;8,5;5;4;20;1;2,9;1;0;33;0;4,m;m
4377,ICLR,2020,Storage Efficient and Dynamic Flexible Runtime Channel Pruning via Deep Reinforcement Learning,Jianda Chen;Shangyu Chen;Sinno Jialin Pan,jianda001@e.ntu.edu.sg;schen025@e.ntu.edu.sg;sinnopan@ntu.edu.sg,3;3;6,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,National Taiwan University;National Taiwan University;National Taiwan University,86;86;86,120;120;120,,9/25/19,0,0,0,0,0,0,50;139;12967,13;13;110,5;3;34,4;16;1205,m;m
4378,ICLR,2020,Generalized Zero-shot ICD Coding,Congzheng Song;Shanghang Zhang;Najmeh Sadoughi;Pengtao Xie;Eric Xing,cs2296@cornell.edu;shanghang.zhang@petuum.com;najmeh.sadoughi@petuum.com;pengtao.xie@petuum.com;eric.xing@petuum.com,3;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Cornell University;Petuum Inc.;Petuum Inc.;Petuum Inc.;Petuum Inc.,7;-1;-1;-1;-1,19;-1;-1;-1;-1,5;4;6,9/25/19,0,0,0,0,0,0,1025;345;248;1361;25065,21;31;25;72;605,10;9;9;17;77,130;28;21;157;2695,m;m
4379,ICLR,2020,"Unsupervised Disentanglement of Pose, Appearance and Background from Images and Videos",Aysegul Dundar;Kevin J Shih;Animesh Garg;Robert Pottorf;Andrew Tao;Bryan Catanzaro,aysegul.dundar89@gmail.com;kjshih2@illinois.edu;garg@cs.stanford.edu;rpottorff@gmail.com;atao@nvidia.com;bcatanzaro@nvidia.com,3;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,"NVIDIA;University of Illinois, Urbana Champaign;Stanford University;Brigham Young University;NVIDIA;NVIDIA",-1;3;4;-1;-1;-1,-1;48;4;-1;-1;-1,,9/25/19,4,2,1,0,0,0,586;923;1040;186;1755;9386,21;25;79;29;23;73,10;10;20;8;9;28,37;147;66;20;319;1078,f;m
4380,ICLR,2020,A Bayes-Optimal View on Adversarial Examples,Eitan Richardson;Yair Weiss,eitan.richardson@gmail.com;yweiss@cs.huji.ac.il,1;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,12,1,yes,9/25/19,Hebrew University of Jerusalem;Hebrew University of Jerusalem,67;67,216;216,4,9/25/19,1,1,0,0,0,0,49;27118,5;104,4;55,6;3326,m;m
4381,ICLR,2020,"Carpe Diem, Seize the Samples Uncertain at the Moment"" for Adaptive Batch Selection""",Hwanjun Song;Minseok Kim;Sundong Kim;Jae-Gil Lee,songhwanjun@kaist.ac.kr;minseokkim@kaist.ac.kr;sundong@ibs.re.kr;jaegil@kaist.ac.kr,3;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Institute for Basic Science;Korea Advanced Institute of Science and Technology,481;481;-1;481,110;110;-1;110,,9/25/19,0,0,0,0,0,0,42;48;13;34,11;46;11;27,3;3;2;2,9;1;1;2,m;m
4382,ICLR,2020,EXACT ANALYSIS OF CURVATURE CORRECTED LEARNING DYNAMICS IN DEEP LINEAR NETWORKS,Dongsung Huh,dongsunghuh@gmail.com,6;6;1,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,,,,,9/25/19,0,0,0,0,0,0,0,4,0,0,m
4383,ICLR,2020,TreeCaps: Tree-Structured Capsule Networks for Program Source Code Processing,Vinoj Jayasundara;Nghi Duy Quoc Bui;Lingxiao Jiang;David Lo,vinojjayasundara@gmail.com;dqnbui.2016@phdis.smu.edu.sg;lxjiang@smu.edu.sg;davidlo@smu.edu.sg,1;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,University of Moratuwa;Singapore Management University;Singapore Management University;Singapore Management University,-1;92;92;92,-1;1397;1397;1397,3;10,9/25/19,0,0,0,0,0,0,38;18;2800;980,10;7;92;77,3;2;25;10,5;0;282;110,m;m
4384,ICLR,2020,Pre-trained Contextual Embedding of Source Code,Aditya Kanade;Petros Maniatis;Gogul Balakrishnan;Kensen Shi,akanade@google.com;maniatis@google.com;bgogul@google.com;kshi@google.com,6;3;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,14,1,yes,9/25/19,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,,9/25/19,4,1,2,0,0,0,712;5924;1413;51,50;87;41;9,13;36;18;3,46;653;175;2,m;m
4385,ICLR,2020,AdaX: Adaptive Gradient Descent with Exponential Long Term Memory,Wenjie Li;Zhaoyang Zhang;Xinjiang Wang;Ping Luo,li3549@purdue.edu;zhaoyangzhang@link.cuhk.edu.hk;swanxinjiang@gmail.com;pluo.lhi@gmail.com,3;3;1;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,Purdue University;The Chinese University of Hong Kong;SenseTime Group Limited;The University of Hong Kong,27;59;-1;92,88;35;-1;35,9;3;1;2,9/25/19,1,0,0,0,0,0,117;88;50;58,32;20;18;15,4;7;4;4,19;0;1;2,m;m
4386,ICLR,2020,MIST: Multiple Instance Spatial Transformer Networks,Baptiste Angles;Simon Kornblith;Shahram Izadi;Andrea Tagliasacchi;Kwang Moo Yi,baptiste.angles@gmail.com;skornblith@google.com;shahrami@google.com;taglia@google.com;kyi@uvic.ca,3;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,;Google;Google;Google;University of Victoria,-1;-1;-1;-1;172,-1;-1;-1;-1;449,,11/26/18,10,0,0,0,10,0,29;1041;13827;2196;1428,9;50;198;57;45,2;12;56;19;15,3;133;1456;155;174,m;m
4387,ICLR,2020,Differentiable Bayesian Neural Network Inference for Data Streams,Namuk Park;Taekyu Lee;Songkuk Kim,namuk.park@yonsei.ac.kr;taekyu.lee@yonsei.ac.kr;songkuk@yonsei.ac.kr,3;3;8,I have read many papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Yonsei University;Yonsei University;Yonsei University,481;481;481,196;196;196,11;2,7/12/19,0,0,0,0,0,0,0;46;620,1;10;32,0;4;6,0;0;56,m;m
4388,ICLR,2020,CAT: Compression-Aware Training for bandwidth reduction,Chaim Baskin;Brian Chmiel;Evgenii Zheltonozhskii;Ron Banner;Alex M. Bronstein;Avi Mendelson,chaimbaskin@cs.technion.ac.il;brian.chmiel@intel.com;evgeniizh@campus.technion.ac.il;ron.banner@intel.com;bron@cs.technion.ac.il;avi.mendelson@cs.technion.ac.il,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Technion;Intel;Technion;Intel;Technion;Technion,26;-1;26;-1;26;26,412;-1;412;-1;412;412,,9/25/19,2,1,1,0,0,0,92;9;78;532;136;1693,14;8;10;31;16;138,5;2;4;11;6;23,11;0;8;82;6;122,m;m
4389,ICLR,2020,Using Logical Specifications of Objectives in Multi-Objective Reinforcement Learning,Kolby Nottingham;Anand Balakrishnan;Jyotirmoy Deshmukh;Connor Christopherson;David Wingate,kolbytn@byu.edu;anandbal@usc.edu;jdeshmukh@usc.edu;connormc@byu.edu;wingated@cs.byu.edu,3;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Brigham Young University;University of Southern California;University of Southern California;Brigham Young University;Brigham Young University,-1;31;31;-1;-1,-1;62;62;-1;-1,,9/25/19,1,0,0,0,0,0,1;315;1417;1;1301,3;9;92;1;136,1;6;20;1;19,0;22;137;0;105,m;m
4390,ICLR,2020,High-Frequency guided Curriculum Learning for Class-specific Object Boundary Detection,VSR Veeravasarapu;Deepak Mittal;Abhishek Goel;Maneesh Singh,vsr.veera@gmail.com;deepak.mittal@verisk.com;abhishek.goel@verisk.com;maneesh.singh@verisk.com,1;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,Verisk Analytics;Verisk Analytics;Verisk Analytics;Verisk Analytics,-1;-1;-1;-1,-1;-1;-1;-1,,9/25/19,0,0,0,0,0,0,87;142;0;118,9;19;6;3,5;3;0;1,8;6;0;31,m;m
4391,ICLR,2020,Learning General and Reusable Features via Racecar-Training,You Xie;Nils Thuerey,you.xie@tum.de;nils.thuerey@tum.de,1;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,3,0,yes,9/25/19,Technical University Munich;Technical University Munich,53;53,43;43,,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1,-1;-1,-1;-1,0;0,m;m
4392,ICLR,2020,Weakly-supervised Knowledge Graph Alignment with Adversarial Learning,Meng Qu;Jian Tang;Yoshua Bengio,meng.qu@umontreal.ca;jian.tang@hec.ca;yoshua.bengio@mila.quebec,6;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,4,0,yes,9/25/19,University of Montreal;HEC Montreal;University of Montreal,128;128;128,85;85;85,4;1;10,7/6/19,2,0,1,0,0,0,2672;5178;208566,53;154;807,13;34;147,701;450;24297,m;m
4393,ICLR,2020,Differentially Private Mixed-Type Data Generation For Unsupervised Learning,Uthaipon Tantipongpipat;Chris Waites;Digvijay Boob;Amaresh Siva;Rachel Cummings,uthaipon@gmail.com;cwaites10@gmail.com;digvijaybb40@gmail.com;ankit.siva@gatech.edu;racheladcummings@gmail.com,1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,1,0,yes,9/25/19,Georgia Institute of Technology;;;Georgia Institute of Technology;Georgia Tech Research Corporation,13;-1;-1;13;-1,38;-1;-1;38;-1,5,9/25/19,2,1,1,0,0,0,78;4;59;2;703,13;4;10;1;43,5;2;4;1;12,13;1;3;0;49,m;f
4394,ICLR,2020,Defense against Adversarial Examples by Encoder-Assisted Search in the Latent Coding Space,Wenjing Huang;Shikui Tu;Lei Xu,huangwenjing@sjtu.edu.cn;tushikui@sjtu.edu.cn,3;3;3;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,1,7,0,yes,9/25/19,Shanghai Jiao Tong University;Shanghai Jiao Tong University,53;53,157;157,5;4,9/25/19,0,0,0,0,0,0,7;747;7,4;68;32,2;14;2,1;52;2,m;m
4395,ICLR,2020,Matrix Multilayer Perceptron,Jalil Taghia;Maria Bånkestad;Fredrik Lindsten;Thomas Schön,jalil.taghia@ericsson.com;maria.bankestad@ri.se;fredrik.lindsten@liu.se;thomas.schon@it.uu.se,6;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0,yes,9/25/19,Ericsson;;Linköping University;Uppsala University,-1;-1;481;154,-1;-1;407;102,,2/4/19,0,0,0,0,0,0,404;12;1457;4215,40;6;98;234,12;2;22;31,15;0;156;345,m;m
4396,ICLR,2020,A Simple Dynamic Learning Rate Tuning Algorithm For Automated Training of DNNs,Koyel Mukherjee;Alind Khare;Yogish Sabharwal;Ashish Verma,koyelmjee@gmail.com;kharealind@gmail.com;ysabharwal@in.ibm.com;ashish.verma1@ibm.com,1;1;1,I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,7,0,yes,9/25/19,;Georgia Institute of Technology;International Business Machines;International Business Machines,-1;13;-1;-1,-1;38;-1;-1,4,9/25/19,0,0,0,0,0,0,334;6;1212;-1,37;6;97;-1,10;2;19;-1,23;0;83;0,f;m
4397,ICLR,2020,Structural Language Models for Any-Code Generation,Uri Alon;Roy Sadaka;Omer Levy;Eran Yahav,urialon@cs.technion.ac.il;roysadaka@gmail.com;omerlevy@gmail.com;yahave@cs.technion.ac.il,6;6;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,Technion;;Facebook;Technion,26;-1;-1;26,412;-1;-1;412,3,9/25/19,5,4,1,0,0,0,352;7;7667;4049,9;4;58;144,5;2;30;37,54;0;1231;377,m;m
4398,ICLR,2020,Hierarchical Disentangle Network for Object Representation Learning,Shishi Qiao;Ruiping Wang;Shiguang Shan;Xilin Chen,qiaoshishi14@mails.ucas.ac.cn;wangruiping@ict.ac.cn;sgshan@ict.ac.cn;xlchen@ict.ac.cn,8;1;1;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,"Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences",59;59;59;59,1397;1397;1397;1397,5;4,9/25/19,0,0,0,0,0,0,6;3471;441;15416,5;110;57;512,2;26;7;58,0;527;72;1793,m;m
4399,ICLR,2020,Improved Training Techniques for Online Neural Machine Translation,Maha Elbayad;Laurent Besacier;Jakob Verbeek,maha.elbayad@inria.fr;laurent.besacier@univ-grenoble-alpes.fr;jakob.verbeek@inria.fr,3;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,3,0,yes,9/25/19,INRIA;University of Grenoble-Alpes;INRIA,-1;481;-1,-1;329;-1,3,9/25/19,0,0,0,0,0,0,60;143;8789,6;38;103,3;6;38,7;10;1106,f;m
4400,ICLR,2020,Removing the Representation Error of GAN Image Priors Using the Deep Decoder,Max Daniels;Reinhard Heckel;Paul Hand,daniels.g@husky.neu.edu;rh43@rice.edu;p.hand@northeastern.edu,1;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Northeastern University;Rice University;Northeastern University,16;84;16,906;105;906,5,9/25/19,0,0,0,0,0,0,0;971;34,1;58;8,0;16;2,0;117;1,m;m
4401,ICLR,2020,Learning a Behavioral Repertoire from Demonstrations,Niels Justesen;Miguel González Duque;Daniel Cabarcas Jaramillo;Jean-Baptiste Mouret;Sebastian Risi,noju@itu.edu;migonzalez@unal.edu.co;dcarbarc@unal.edu.co;jean-baptiste.mouret@inria.fr;sebr@itu.dk,1;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,0,0,yes,9/25/19,"ITU of Punjab Lahore, Pakistan;Universidad Nacional de Colombia;Universidad Nacional de Colombia;INRIA;IT University",-1;481;481;-1;172,-1;1397;1397;-1;416,,7/5/19,0,0,0,0,0,0,258;0;0;3206;1561,15;2;3;101;121,7;0;0;28;24,13;0;0;259;93,m;m
4402,ICLR,2020,One-Shot Neural Architecture Search via Compressive Sensing,Minsu Cho;Mohammadreza Soltani;Chinmay Hegde,chomd90@iastate.edu;mohammadreza.soltani@duke.edu;chinmay@iastate.edu,1;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Iowa State University;Duke University;Iowa State University,172;47;172,399;20;399,6,6/7/19,3,0,0,1,0,0,2439;116;2410,84;34;99,25;6;19,465;7;248,m;m
4403,ICLR,2020,Deep Audio Prior,Yapeng Tian;Chenliang Xu;Dingzeyu Li,yapengtian@rochester.edu;chenliang.xu@rochester.edu;dinli@adobe.com,3;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,University of Rochester;University of Rochester;Adobe Systems,100;100;-1,173;173;-1,,9/25/19,2,0,1,0,0,1,1510;1296;416,49;61;19,15;18;8,216;202;57,m;m
4404,ICLR,2020,XD: Cross-lingual Knowledge Distillation for Polyglot Sentence Embeddings,Maksym Del;Mark Fishel,max.del.edu@gmail.com;fishel@ut.ee,1;6;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,1,4,0,yes,9/25/19,;University of Tartu,-1;266,-1;319,3,9/25/19,0,0,0,0,0,0,7;2392,4;75,1;16,0;177,m;m
4405,ICLR,2020,Graph Neural Networks for Soft Semi-Supervised Learning on Hypergraphs,Naganand Yadati;Tingran Gao;Shahab Asoodeh;Partha Talukdar;Anand Louis,y.naganand@gmail.com;trg17@uchicago.edu;shahab@seas.harvard.edu;ppt@iisc.ac.in;anandl@iisc.ac.in,3;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Indian Institute of Science;University of Chicago;Harvard University;Indian Institute of Science;Indian Institute of Science,95;48;39;95;95,301;9;7;301;301,10,9/25/19,0,0,0,0,0,0,54;157;13;2427;331,11;31;15;102;39,4;8;2;28;12,4;12;0;214;18,m;m
4406,ICLR,2020,Self-Supervised State-Control through Intrinsic Mutual Information Rewards,Rui Zhao;Volker Tresp;Wei Xu,zhaorui.in.germany@gmail.com;volker.tresp@siemens.com;wei.xu@horizon.ai,6;3;3,I have published in this field for several years.:N/A:N/A:N/A;I have read many papers in this area.:I carefully checked the experiments.:N/A:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Siemens Corporate Research;Siemens Corporate Research;Horizon Robotics,-1;-1;-1,-1;-1;-1,,9/25/19,0,0,0,0,0,0,97;8302;42,179;287;39,4;45;2,5;807;0,m;m
4407,ICLR,2020,A Generative Model for Molecular Distance Geometry,Gregor N. C. Simm;José Miguel Hernández-Lobato,gncsimm@gmail.com;jmh233@cam.ac.uk,6;3;6,I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,1,8,0,yes,9/25/19,University of Cambridge;University of Cambridge,71;71,3;3,5;10,9/25/19,2,1,1,0,0,0,201;3824,11;114,7;28,0;420,m;m
4408,ICLR,2020,Multigrid Neural Memory,Tri Huynh;Michael Maire;Matthew R. Walter,trihuynh@uchicago.edu;mmaire@uchicago.edu;mwalter@ttic.edu,3;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,1,yes,9/25/19,University of Chicago;University of Chicago;Toyota Technological Institute at Chicago,48;48;-1,9;9;-1,,6/13/19,2,2,0,0,0,0,252;16144;3162,11;62;77,3;25;28,19;2845;186,m;m
4409,ICLR,2020,Neural Design of Contests and All-Pay Auctions using Multi-Agent Simulation,Thomas Anthony;Ian Gemp;Janos Kramar;Tom Eccles;Andrea Tacchetti;Yoram Bachrach,twa@google.com;imgemp@google.com;janosk@google.com;eccles@google.com;atacchet@google.com;yorambac@gmail.com,6;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,,9/25/19,0,0,0,0,0,0,137;259;405;119;44;2786,84;25;14;28;6;137,6;4;7;6;3;30,4;27;36;11;11;198,m;m
4410,ICLR,2020,Geometry-aware Generation of Adversarial and Cooperative Point Clouds,Yuxin Wen;Jiehong Lin;Ke Chen;Kui Jia,wen.yuxin@mail.scut.edu.cn;lin.jiehong@mail.scut.edu.cn;chenk@scut.edu.cn;kuijia@scut.edu.cn,3;3;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,South China University of Technology;South China University of Technology;South China University of Technology;South China University of Technology,481;481;481;481,501;501;501;501,4;7,9/25/19,2,1,0,0,0,0,4;7;1069;1071,6;4;143;40,2;1;17;9,0;0;116;110,m;m
4411,ICLR,2020,Point Process Flows,Nazanin Mehrasa;Ruizhi Deng;Mohamed Osama Ahmed;Bo Chang;Jiawei He;Thibaut Durand;Marcus Brubaker;Greg Mori,nmehrasa@sfu.ca;ruizhid@sfu.ca;mohamed.o.ahmed@borealisai.com;bchang@stat.ubc.ca;jha203@sfu.ca;thibaut.p.durand@borealisai.com;marcus.brubaker@borealisai.com;mori@cs.sfu.ca,6;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,7,0,yes,9/25/19,Simon Fraser University;Simon Fraser University;Borealis AI;University of British Columbia;Simon Fraser University;Borealis AI;Borealis AI;Simon Fraser University,64;64;-1;35;64;-1;-1;64,272;272;-1;34;272;-1;-1;272,,9/25/19,2,2,0,0,0,0,48;92;262;429;207;354;3978;9711,9;16;19;56;42;21;55;197,3;5;6;8;8;6;17;45,5;10;34;42;22;36;466;817,f;m
4412,ICLR,2020,Learning Effective Exploration Strategies For Contextual Bandits,Amr Sharaf;Hal Daumé III,amr@cs.umd.edu;hal@umiacs.umd.edu,1;1;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,5,0,yes,9/25/19,"University of Maryland, College Park;University of Maryland, College Park",12;12,91;91,6,9/25/19,0,0,0,0,0,0,73;48,20;11,4;3,3;7,m;m
4413,ICLR,2020,From English to Foreign Languages: Transferring Pre-trained Language Models,Ke Tran,ketranmanh@gmail.com,3;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,2,1,yes,9/25/19,Amazon,-1,-1,3;6,9/25/19,3,2,0,0,0,0,225,7,6,24,m;m
4414,ICLR,2020,Leveraging Adversarial Examples to Obtain Robust Second-Order Representations,Mohit Prabhushankar;Gukyeong Kwon;Dogancan Temel;Ghassan AlRegib,mohit.p@gatech.edu;gukyeong.kwon@gatech.edu;cantemel@gatech.edu;alregib@gatech.edu,3;1;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,0,0,yes,9/25/19,Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology,13;13;13;13,38;38;38;38,4,9/25/19,0,0,0,0,0,0,55;27;162;2201,11;6;35;259,3;3;9;24,5;2;10;145,m;m
4415,ICLR,2020,Multi-Agent Hierarchical Reinforcement Learning for Humanoid Navigation,Glen Berseth;Brandon haworth;Seonghyeon Moon;Mubbasir Kapadia;Petros Faloutsos,gberseth@gmail.com;m.brandon.haworth@gmail.com;sm2062@cs.rutgers.edu;mubbasir.kapadia@gmail.com;pfaloutsos@gmail.com,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,3,0,yes,9/25/19,University of California Berkeley;;Rutgers University;;,5;-1;34;-1;-1,13;-1;168;-1;-1,,9/25/19,0,0,0,0,0,0,640;26;24;76;6294,45;9;15;23;136,11;3;2;3;28,21;1;0;0;445,m;m
4416,ICLR,2020,SemanticAdv: Generating Adversarial Examples via Attribute-Conditional Image Editing,Haonan Qiu;Chaowei Xiao;Lei Yang;Xinchen Yan;HongLak Lee;Bo Li,haonanqiu@link.cuhk.edu.cn;xiaocw@umich.edu;yl016@ie.cuhk.edu.hk;xcyan@umich.edu;honglak@eecs.umich.edu;lxbosky@gmail.com,3;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,1,8,0,yes,9/25/19,Tsinghua University;University of Michigan;The Chinese University of Hong Kong;University of Michigan;University of Michigan;University of California Berkeley,8;8;59;8;8;5,23;21;35;21;21;13,4;2,6/19/19,12,7,2,1,2,0,38;1386;197;3104;24514;1061,10;31;46;17;166;137,3;12;9;10;62;11,2;149;7;455;2837;63,m;f
4417,ICLR,2020,Semantic Pruning for Single Class Interpretability,Kamila Abdiyeva;Martin Lukac;Kanat Alimanov,kabdiyeva@nu.edu.kz;martin.lukac@nu.edu.kz;kanat.alimanov@nu.edu.kz,3;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Australian National University;Australian National University;Australian National University,108;108;108,50;50;50,2,9/25/19,0,0,0,0,0,0,4;280;0,9;33;2,2;9;0,0;23;0,f;m
4418,ICLR,2020,Dual Graph Representation Learning,Huiling Zhu;Xin Luo;Hankz Hankui Zhuo,zhuhling6@mail.sysu.edu.cn;luo35@mail2.sysu.edu.cn;zhuohank@mail.sysu.edu.cn,3;3;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0,yes,9/25/19,SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY,481;481;481,299;299;299,10,9/25/19,0,0,0,0,0,0,62;14;48,22;54;14,4;2;3,5;0;3,m;m
4419,ICLR,2020,Semi-Implicit Back Propagation,Ren Liu;Xiaoqun Zhang,liur0810@sjtu.edu.cn;xqzhang@sjtu.edu.cn,1;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0,yes,9/25/19,Shanghai Jiao Tong University;Shanghai Jiao Tong University,53;53,157;157,9,9/25/19,0,0,0,0,0,0,92;2389,40;105,5;20,0;179,u;f
4420,ICLR,2020,The Effect of Neural Net Architecture on Gradient Confusion & Training Performance,Karthik A. Sankararaman;Soham De;Zheng Xu;W. Ronny Huang;Tom Goldstein,karthikabinavs@gmail.com;sohamde@google.com;xuzh@cs.umd.edu;wrhuang@cs.umd.edu;tomg@cs.umd.edu,8;1;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,"Facebook;Google;University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park",-1;-1;12;12;12,-1;-1;91;91;91,,9/25/19,0,0,0,0,0,0,155;442;637;333;6817,32;24;72;22;159,8;12;10;7;29,16;39;35;26;764,m;m
4421,ICLR,2020,Frequency Analysis for Graph Convolution Network,Hoang NT;Takanori Maehara,hoang.nguyen.rh@riken.jp;takanori.maehara@riken.jp,6;1;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0,yes,9/25/19,RIKEN;RIKEN,-1;-1,-1;-1,4;1;10,9/25/19,1,0,0,0,0,0,0;671,1;97,0;15,0;85,m;m
4422,ICLR,2020,A SPIKING SEQUENTIAL MODEL: RECURRENT LEAKY INTEGRATE-AND-FIRE,Daiheng Gao;Hongwei Wang;Hehui Zhang;Meng Wang;Zhenzhi Wu,samuel.gao023@gmail.com;hongwei.wang@lynxi.com;zhh@bupt.edu.cn;wangmeng_wm@bupt.edu.cn;zhenzhi.wu@lynxi.com,3;1;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,;Lynxi;Beijing University of Post and Telecommunication;Beijing University of Post and Telecommunication;Lynxi,-1;-1;481;481;-1,-1;-1;1397;1397;-1,,9/25/19,0,0,0,0,0,0,0;47;39;59;149,3;11;11;13;21,0;4;4;3;5,0;1;1;2;14,m;m
4423,ICLR,2020,Expected Tight Bounds for Robust Deep Neural Network Training,Salman Alsubaihi;Adel Bibi;Modar Alfadly;Abdullah Hamdi;Bernard Ghanem,salman.subaihi@kaust.edu.sa;adel.bibi@kaust.edu.sa;modar.alfadly@kaust.edu.sa;abdullah.hamdi@kaust.edu.sa;bernard.ghanem@kaust.edu.sa,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,1,6,0,yes,9/25/19,KAUST;KAUST;KAUST;KAUST;KAUST,128;128;128;128;128,1397;1397;1397;1397;1397,4;1,5/28/19,0,0,0,0,0,0,83;462;40;6;6428,5;25;11;14;198,1;9;4;1;37,32;65;5;0;1007,m;m
4424,ICLR,2020,Neural Operator Search,Wei Li;Shaogang Gong;Xiatian Zhu,w.li@qmul.ac.uk;s.gong@qmul.ac.uk;eddy.zhuxt@gmail.com,6;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,6,0,yes,9/25/19,Queen Mary University London;Queen Mary University London;Samsung,233;233;-1,110;110;-1,,9/25/19,0,0,0,0,0,0,67;21123;2989,91;449;79,3;72;25,2;2326;409,m;m
4425,ICLR,2020,Behavior-Guided Reinforcement Learning,Aldo Pacchiano;Jack Parker-Holder;Yunhao Tang;Anna Choromanska;Krzysztof Choromanski;Michael I. Jordan,pacchiano@berkeley.edu;jh3764@columbia.edu;yt2541@columbia.edu;achoroma@gmail.com;kchoro@google.com;jordan@cs.berkeley.edu,1;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,University of California Berkeley;Columbia University;Columbia University;New York University;Google;University of California Berkeley,5;15;15;25;-1;5,13;16;16;29;-1;13,,9/25/19,2,2,0,0,0,0,99;31;95;2529;901;118202,40;15;29;89;92;847,6;3;7;19;15;140,8;2;4;234;79;16063,m;m
4426,ICLR,2020,"Semi-supervised semantic segmentation needs strong, high-dimensional perturbations",Geoff French;Timo Aila;Samuli Laine;Michal Mackiewicz;Graham Finlayson,g.french@uea.ac.uk;taila@nvidia.com;slaine@nvidia.com;m.mackiewicz@uea.ac.uk;g.finlayson@uea.ac.uk,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,;NVIDIA;NVIDIA;;,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,2,9/25/19,4,2,1,0,0,1,18;6246;5515;104;365,11;68;72;45;52,3;32;26;4;10,2;1010;924;12;22,m;m
4427,ICLR,2020,Interpretations are useful: penalizing explanations to align neural networks with prior knowledge,Laura Rieger;Chandan Singh;W. James Murdoch;Bin Yu,lauri@dtu.dk;c_singh@berkeley.edu;jmurdoch@berkeley.edu;binyu@berkeley.edu,6;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,2,5,0,yes,9/25/19,Technical University of Denmark;University of California Berkeley;University of California Berkeley;University of California Berkeley,481;5;5;5,182;13;13;13,,9/25/19,6,3,2,1,0,0,23;311;355;1049,12;29;10;53,3;6;6;12,0;23;29;87,f;f
4428,ICLR,2020,Axial Attention in Multidimensional Transformers,Jonathan Ho;Nal Kalchbrenner;Dirk Weissenborn;Tim Salimans,jonathanho@google.com;nalk@google.com;diwe@google.com;salimans@google.com,1;6;1;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,8,9/25/19,5,3,2,0,0,0,2743;16442;626;6754,31;27;28;35,12;18;10;14,408;1425;61;1054,m;m
4429,ICLR,2020,Adversarial Robustness as a Prior for Learned Representations,Logan Engstrom;Andrew Ilyas;Shibani Santurkar;Dimitris Tsipras;Brandon Tran;Aleksander Madry,engstrom@mit.edu;ailyas@mit.edu;shibani@mit.edu;tsipras@mit.edu;btran115@mit.edu;madry@mit.edu,3;3;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,12,0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2;2;2,5;5;5;5;5;5,4,9/25/19,24,11,6,2,0,3,2055;1947;1562;3896;36;5523,27;29;26;33;5;84,15;15;14;17;2;30,271;273;187;935;4;1099,m;m
4430,ICLR,2020,Equivariant neural networks and equivarification,Erkao Bao;Linqi Song,baoerkao@gmail.com;linqi.song@cityu.edu.hk,6;3;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,;City University of Hong Kong,-1;92,-1;35,,6/16/19,2,1,0,0,2,0,146;312,10;42,4;8,3;25,m;m
4431,ICLR,2020,Do recent advancements in model-based deep reinforcement learning really improve data efficiency?,Kacper Piotr Kielak,k.kielak@bham.ac.uk,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,1,4,0,yes,9/25/19,Birmingham University,128,112,,9/25/19,2,1,1,1,0,1,2,3,1,1,m
4432,ICLR,2020,Efficient meta reinforcement learning via meta goal generation,Haotian Fu;Hongyao Tang;Jianye Hao,haotianfu@tju.edu.cn;bluecontra@tju.edu.cn;jianye.hao@tju.edu.cn,1;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,3,0,yes,9/25/19,Zhejiang University;Zhejiang University;Zhejiang University,56;56;56,107;107;107,6,9/25/19,0,0,0,0,0,0,8;35;491,4;14;138,1;4;11,0;2;42,u;u
4433,ICLR,2020,The Discriminative Jackknife: Quantifying Uncertainty in Deep Learning via Higher-Order Influence Functions,Ahmed M. Alaa;Mihaela van der Schaar,a7med3laa@hotmail.com;mihaelaucla@gmail.com,3;6;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,7,0,yes,9/25/19,;,-1;-1,-1;-1,11,9/25/19,0,0,0,0,0,0,409;8828,72;642,12;42,20;547,m;f
4434,ICLR,2020,Generative Adversarial Nets for Multiple Text Corpora,Diego Klabjan;Baiyang Wang,d-klabjan@northwestern.edu;baiyang@u.northwestern.edu,3;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,Northwestern University;Northwestern University,44;44,22;22,3;4;5,12/25/17,65,7,8,0,3,3,2780;90,219;7,27;3,200;3,m;u
4435,ICLR,2020,Learning Similarity Metrics for Numerical Simulations,Georg Kohl;Kiwon Um;Nils Thuerey,georg.kohl@tum.de;kiwon.um@tum.de;nils.thuerey@tum.de,6;3;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Technical University Munich;Technical University Munich;Technical University Munich,53;53;53,43;43;43,8,9/25/19,0,0,0,0,0,0,22;76;2623,13;15;122,2;4;32,1;1;189,m;m
4436,ICLR,2020,Universal Adversarial Attack Using Very Few Test Examples,Amit Deshpande;Sandesh Kamath;K V Subrahmanyam,amitdesh@microsoft.com;ksandeshk@cmi.ac.in;kv@cmi.ac.in,3;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,Microsoft;Chennai Mathematical Institute;Chennai Mathematical Institute,-1;-1;-1,-1;-1;-1,4;1,9/25/19,0,0,0,0,0,0,142;19;208,21;16;50,4;3;8,12;2;24,m;m
4437,ICLR,2020,Improved Generalization Bound of Permutation Invariant Deep Neural Networks,Akiyoshi Sannai;Masaaki Imaizumi,akiyoshi.sannai@riken.jp;imaizumi@ism.ac.jp,1;6;3,I have published in this field for several years.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:N/A:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0,yes,9/25/19,"RIKEN;The Institute of Statistical Mathematics, Japan",-1;-1,-1;-1,10;1;8,9/25/19,2,1,0,0,0,1,92;87,22;21,4;5,8;11,m;m
4438,ICLR,2020,Peer Loss Functions: Learning from Noisy Labels without Knowing Noise Rates,Yang Liu;Hongyi Guo,yangliu@ucsc.edu;guohongyi@sjtu.edu.cn,3;3;8,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,11,0,yes,9/25/19,University of Southern California;Shanghai Jiao Tong University,31;53,62;157,,9/25/19,3,1,1,0,0,0,1552;15,211;6,22;2,99;0,m;m
4439,ICLR,2020,Mode Connectivity and Sparse Neural Networks,Jonathan Frankle;Gintare Karolina Dziugaite;Daniel M. Roy;Michael Carbin,jfrankle@csail.mit.edu;karolina.dziugaite@gmail.com;droy@utstat.toronto.edu;mcarbin@csail.mit.edu,3;3;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,6,0,yes,9/25/19,Massachusetts Institute of Technology;Element AI;University of Toronto;Massachusetts Institute of Technology,2;-1;18;2,5;-1;18;5,,9/25/19,0,0,0,0,0,0,647;883;3139;2277,22;19;97;61,9;9;24;19,99;106;333;212,m;m
4440,ICLR,2020,Generating valid Euclidean distance matrices,Moritz Hoffmann;Frank Noe,moritz.hoffmann@fu-berlin.de;frank.noe@fu-berlin.de,8;3;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0,yes,9/25/19,Freie Universität Berlin;Freie Universität Berlin,-1;-1,-1;-1,5,9/25/19,3,1,0,0,0,0,384;6650,25;222,8;43,15;224,m;m
4441,ICLR,2020,GResNet: Graph Residual Network for Reviving Deep GNNs from Suspended Animation,Jiawei Zhang;Lin Meng,jiawei@ifmlab.org;lin@ifmlab.org,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0,yes,9/25/19,SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY,481;481,299;299,1;10,9/12/19,10,5,3,0,0,2,62;332,44;203,5;8,12;10,m;m
4442,ICLR,2020,Deep Spike Decoder (DSD),Emrah Adamey;Tarin Ziyaee;Nishanth Alapati;Jun Ye,emrah@ctrl-labs.com;tarin@ctrl-labs.com;nishanth@ctrl-labs.com;jun@ctrl-labs.com,1;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,1,0,yes,9/25/19,Ctrl-labs;Ctrl-labs;Ctrl-labs;Ctrl-labs,-1;-1;-1;-1,-1;-1;-1;-1,5,9/25/19,0,0,0,0,0,0,35;1;0;236,8;3;1;41,4;1;0;7,3;0;0;9,m;m
4443,ICLR,2020,Efficient High-Dimensional Data Representation Learning via Semi-Stochastic Block Coordinate Descent Methods,Bingkun Wei;Yangyang Li;Fanhua Shang;Yuanyuan Liu;Hongying Liu;Shengmei Shen,bkwei028@gmail.com;1615401247li@gmail.com;fhshang@xidian.edu.cn;yyliu@xidian.edu.cn;hyliu@xidian.edu.cn;jane.shen@pensees.ai,6;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University;,8;8;8;8;8;-1,23;23;23;23;23;-1,2,9/25/19,0,0,0,0,0,0,1;218;988;148;1760;615,5;33;79;32;216;31,1;9;19;6;21;11,0;11;91;6;62;82,m;f
4444,ICLR,2020,Towards Controllable and Interpretable Face Completion via  Structure-Aware and Frequency-Oriented Attentive GANs,Zeyuan Chen;Shaoliang Nie;Tianfu Wu;Christopher G. Healey,zchen23@ncsu.edu;snie@ncsu.edu;tianfu_wu@ncsu.edu;healey@ncsu.edu,6;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,North Carolina State University;North Carolina State University;North Carolina State University;North Carolina State University,86;86;86;86,310;310;310;310,5;4,9/25/19,0,0,0,0,0,0,7;16;1149;2360,10;7;90;112,2;2;17;23,0;1;54;115,m;m
4445,ICLR,2020,Why Does Hierarchy (Sometimes) Work So Well in Reinforcement Learning?,Ofir Nachum;Haoran Tang;Xingyu Lu;Shixiang Gu;Honglak Lee;Sergey Levine,ofirnachum@google.com;hrtang.alex@berkeley.edu;xingyulu0701@berkeley.edu;shanegu@google.com;honglak@google.com;svlevine@eecs.berkeley.edu,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,4,0,yes,9/25/19,Google;University of California Berkeley;University of California Berkeley;Google;Google;University of California Berkeley,-1;5;5;-1;-1;5,-1;13;13;-1;-1;13,,9/23/19,3,2,0,0,0,1,1060;638;37;3857;24514;24893,42;17;21;39;166;310,15;3;4;21;62;74,157;92;4;478;2837;3235,m;m
4446,ICLR,2020,Improving Exploration of Deep Reinforcement Learning using Planning for Policy Search,Jakob J. Hollenstein;Erwan Renaudo;Justus Piater,jakob.hollenstein@uibk.ac.at;erwan.renaudo@uibk.ac.at;justus.piater@uibk.ac.at,3;1;1,I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,3,0,yes,9/25/19,University of Innsbruck;University of Innsbruck;University of Innsbruck,481;481;481,415;415;415,,9/25/19,1,0,0,0,0,0,2;53;84,3;11;17,1;4;4,0;0;6,m;m
4447,ICLR,2020,"Improved Training Speed, Accuracy, and Data Utilization via Loss Function Optimization",Santiago Gonzalez;Risto Miikkulainen,slgonzalez@utexas.edu;risto@cs.utexas.edu,3;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,0,0,yes,9/25/19,"University of Texas, Austin;University of Texas, Austin",22;22,38;38,,5/27/19,11,4,4,0,3,1,28;12970,8;444,3;49,4;1319,m;m
4448,ICLR,2020,Knowledge Hypergraphs: Prediction Beyond Binary Relations,Bahare Fatemi;Perouz Taslakian;David Vazquez;David Poole,bfatemi@cs.ubc.ca;perouz@elementai.com;dvazquez@elementai.com;poole@cs.ubc.ca,3;6;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,5,0,yes,9/25/19,University of British Columbia;Element AI;Element AI;University of British Columbia,35;-1;-1;35,34;-1;-1;34,10,9/25/19,3,1,0,0,0,1,46;258;3;6852,10;48;4;210,4;9;1;37,7;16;1;855,f;m
4449,ICLR,2020,FR-GAN: Fair and Robust Training,Yuji Roh;Kangwook Lee;Gyeong Jo Hwang;Steven Euijong Whang;Changho Suh,rohyj113@gmail.com;kangwook.lee@wisc.edu;hkj4276@kaist.ac.kr;swhang@kaist.ac.kr;chsuh@kaist.ac.kr,3;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,3,0,yes,9/25/19,Korea Advanced Institute of Science and Technology;University of Southern California;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,481;31;481;481;481,110;62;110;110;110,5;4;7,9/25/19,0,0,0,0,0,0,54;16;0;1860;341,8;12;1;45;18,2;3;0;19;3,1;1;0;165;56,f;m
4450,ICLR,2020,Unsupervised Intuitive Physics from Past Experiences,Sebastien Ehrhardt;Aron Monszpart;Niloy Mitra;Andrea Vedaldi,hyenal@robots.ox.ac.uk;aron@nianticlabs.com;n.mitra@cs.ucl.ac.uk;vedaldi@robots.ox.ac.uk,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,7,0,yes,9/25/19,University of Oxford;Niantic Inc.;University College London;University of Oxford,50;-1;50;50,1;-1;15;1,6,5/26/19,1,0,0,0,0,0,4;292;10968;34629,4;21;248;201,1;8;53;63,1;17;909;4685,m;m
4451,ICLR,2020,Continual Learning with Delayed Feedback,THEIVENDIRAM PRANAVAN;TERENCE SIM,pranavan@u.nus.edu;tsim@comp.nus.edu.sg,1;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,National University of Singapore;National University of Singapore,16;16,25;25,,9/25/19,0,0,0,0,0,0,0;4959,1;129,0;26,0;777,m;m
4452,ICLR,2020,Adapting to Label Shift with Bias-Corrected Calibration,Avanti Shrikumar;Amr M. Alexandari;Anshul Kundaje,avanti.shrikumar@gmail.com;amr.alexandari@gmail.com;anshul@kundaje.net,6;1;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,3,0,yes,9/25/19,Stanford University;Stanford University;,4;4;-1,4;4;-1,,9/25/19,3,2,1,0,0,1,1516;553;16921,29;10;172,10;4;44,126;6;1056,f;m
4453,ICLR,2020,On the Unintended Social Bias of Training Language Generation Models with News Articles,Omar U. Florez,omar.florez@aggiemail.usu.edu,1;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,0,0,yes,9/25/19,SUN YAT-SEN UNIVERSITY,481,299,3;7,9/25/19,0,0,0,0,0,0,39,21,3,1,m
4454,ICLR,2020,Attacking Graph Convolutional Networks via Rewiring,Yao Ma;Suhang Wang;Tyler Derr;Lingfei Wu;Jiliang Tang,mayao4@msu.edu;szw494@psu.edu;derrtyle@msu.edu;wuli@us.ibm.com;tangjili@msu.edu,6;3;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,SUN YAT-SEN UNIVERSITY;Pennsylvania State University;SUN YAT-SEN UNIVERSITY;International Business Machines;SUN YAT-SEN UNIVERSITY,481;41;481;-1;481,299;78;299;-1;299,4;10,6/10/19,10,6,3,0,0,1,730;2695;22;653;8957,105;96;10;62;211,14;24;3;15;47,30;186;3;62;704,m;m
4455,ICLR,2020,Modeling Fake News in Social Networks with Deep Multi-Agent Reinforcement Learning,Christoph Aymanns;Matthias Weber;Co-Pierre Georg;Jakob Foerster,christoph.aymanns@gmail.com;matthias.weber@unisg.ch;cogeorg@gmail.com;jakobfoerster@gmail.com,3;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I did not assess the experiments.:I made a quick assessment of this paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,9,0,yes,9/25/19,University of St Gallen;University of St. Gallen;;Facebook,481;481;-1;-1,435;435;-1;-1,4,9/25/19,0,0,0,0,0,0,177;381;469;13,20;48;33;4,7;11;9;2,28;25;36;3,m;m
4456,ICLR,2020,Group-Connected Multilayer Perceptron Networks,Mohammad Kachuee;Sajad Darabi;Shayan Fazeli;Majid Sarrafzadeh,mkachuee@ucla.edu;sajad.darabi@cs.ucla.edu;shayan@cs.ucla.edu;majid@cs.ucla.edu,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,"University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles",20;20;20;20,17;17;17;17,7;10,9/25/19,0,0,0,0,0,0,93;58;53;9209,17;13;6;637,4;4;2;46,9;8;6;624,m;m
4457,ICLR,2020,Neural ODEs for Image Segmentation with Level Sets,Rafael Valle;Fitsum Reda;Mohammad Shoeybi;Patrick Legresley;Andrew Tao;Bryan Catanzaro,rafaelvalle@nvidia.com;freda@nvidia.com;mshoeybi@nvidia.com;plegresley@nvidia.com;atao@nvidia.com;bcatanzaro@nvidia.com,3;1;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Reject,1,0,0,yes,9/25/19,NVIDIA;NVIDIA;NVIDIA;NVIDIA;NVIDIA;NVIDIA,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,2,9/25/19,0,0,0,0,0,0,29;669;648;1947;1755;9386,112;32;34;19;23;73,3;11;10;12;9;28,2;110;56;170;319;1078,m;m
4458,ICLR,2020,Knowledge Transfer via Student-Teacher Collaboration,Tianxiao Gao;Ruiqin Xiong;Zhenhua Liu;Siwei ma;Feng Wu;Tiejun Huang;Wen Gao,gtx@pku.edu.cn;rqxiong@pku.edu.cn;liu-zh@pku.edu.cn;swma@pku.edu.cn;fengwu@ustc.edu.cn;tjhuang@pku.edu.cn;wgao@pku.edu.cn,6;8;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,1,4,0,yes,9/25/19,Peking University;Peking University;Peking University;Peking University;University of Science and Technology of China;Peking University;Peking University,22;22;22;22;481;22;22,24;24;24;24;80;24;24,,9/25/19,0,0,0,0,0,0,0;1942;664;151;821;558;24560,6;147;30;29;87;74;1406,0;26;6;5;14;13;69,0;169;81;13;82;20;2124,m;m
4459,ICLR,2020,VIMPNN: A physics informed neural network for estimating potential energies of out-of-equilibrium systems,Jay Morgan;Adeline Paiement;Christian Klinke,j.p.morgan@swansea.ac.uk;adeline.paiement@univ-tln.fr;christian.klinke@uni-rostock.de,6;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,3,0,yes,9/25/19,Swansea University;CNRS university Toulon;Universität Rostock,481;481;390,266;1397;-1,,9/25/19,0,0,0,0,0,0,2;410;1839,2;38;139,1;11;22,0;31;53,m;m
4460,ICLR,2020,AutoLR: A Method for Automatic Tuning of Learning Rate,Nipun Kwatra;V Thejas;Nikhil Iyer;Ramachandran Ramjee;Muthian Sivathanu,nkwatra@microsoft.com;thejasvenkatesh97@gmail.com;t-niiyer@microsoft.com;ramjee@microsoft.com;muthian@microsoft.com,6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,12,0,yes,9/25/19,Microsoft;;Microsoft;Microsoft;Microsoft,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,8,9/25/19,1,0,0,0,0,0,1054;3;10;6930;609,19;3;7;112;25,11;1;2;36;10,103;0;0;862;49,m;m
4461,ICLR,2020,Continuous Convolutional Neural Network forNonuniform Time Series,Hui Shi;Yang Zhang;Hao Wu;Shiyu Chang;Kaizhi Qian;Mark Hasegawa-Johnson;Jishen Zhao,hshi@ucsd.edu;yang.zhang2@ibm.com;haowu11@illinois.edu;kqian3@illinois.edu;jhasegaw@illinois.edu;jzhao@ucsd.edu,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,3,0,yes,9/25/19,"University of California, San Diego;International Business Machines;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;University of California, San Diego",11;-1;3;3;3;11,31;-1;48;48;48;31,8,9/25/19,0,0,0,0,0,0,7;317;14;2991;96;4623;1982,15;67;45;111;11;314;82,2;8;3;28;4;30;20,0;29;1;397;9;358;250,f;f
4462,ICLR,2020,Test-Time Training for Out-of-Distribution Generalization,Yu Sun;Xiaolong Wang;Zhuang Liu;John Miller;Alexei A. Efros;Moritz Hardt,yusun@berkeley.edu;dragonwxl123@gmail.com;zhuangl@berkeley.edu;miller_john@berkeley.edu;efros@eecs.berkeley.edu;hardt@berkeley.edu,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,3,0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5;5;5,13;13;13;13;13;13,,9/25/19,18,9,6,0,0,3,106;1176;37577;769;37253;7854,58;120;341;27;194;89,4;18;90;11;77;33,6;92;2383;68;4619;964,m;m
4463,ICLR,2020,MODELLING   BIOLOGICAL   ASSAYS   WITH ADAPTIVE DEEP KERNEL LEARNING,Prudencio Tossou;Basile Dura;Daniel Cohen;Mario Marchand;François Laviolette;Alexandre Lacoste,tossouprudencio@gmail.com;basile@invivoai.ca;daniel@invivoai.ca;mario.marchand@ift.ulaval.ca;francois.laviolette@ift.ulaval.ca;allac@elementai.com,6;3;8;3;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,3,8,0,yes,9/25/19,InVivo AI;;;Laval university;Laval university;Element AI,-1;-1;-1;481;481;-1,-1;-1;-1;272;272;-1,6,9/25/19,0,0,0,0,0,0,77;5;861;2937;4820;995,10;3;72;98;160;39,3;2;13;20;25;15,8;0;67;541;779;135,m;m
4464,ICLR,2020,Hybrid Weight Representation: A Quantization Method Represented with Ternary and Sparse-Large Weights,Jinbae Park;Sung-Ho Bae,qkrwlsqo94@gmail.com;shbae@khu.ac.kr,3;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Kyung Hee University;Kyung Hee University,481;481,319;319,,9/25/19,0,0,0,0,0,0,7;151,9;16,2;2,0;10,m;m
4465,ICLR,2020,Amharic Negation Handling,Girma Neshir,girma1978@gmail.com,1;1;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:N/A:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,Addis Ababa University,481,1397,,9/25/19,0,0,0,0,0,0,0,1,0,0,m
4466,ICLR,2020,Stiffness: A New Perspective on Generalization in Neural Networks,Stanislav Fort;Paweł Krzysztof Nowak;Stanisław Jastrzebski;Srini Narayanan,stanislav.fort@gmail.com;powalnow@google.com;staszek.jastrzebski@gmail.com;srinin@google.com,3;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,11,0,yes,9/25/19,Google;Google;New York University;Google,-1;-1;25;-1,-1;-1;29;-1,8,1/28/19,18,9,3,2,0,0,107;52;907;3155,11;20;33;55,7;5;13;19,7;3;108;405,m;m
4467,ICLR,2020,POP-Norm: A Theoretically Justified and More Accelerated Normalization Approach,Hanyang Peng;Shiqi Yu,philoso_phy0922@163.com;shiqi.yu@gmai.com,3;3;1,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0,yes,9/25/19,Shenzhen University;Gmai,-1;-1,-1;-1,9,9/25/19,0,0,0,0,0,0,8;1602,5;77,2;19,1;160,u;m
4468,ICLR,2020,A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models,Elman Mansimov;Alex Wang;Kyunghyun Cho,elman.mansimov@gmail.com;wangalexc@gmail.com;kyunghyun.cho@nyu.edu,3;3;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,New York University;New York University;New York University,25;25;25,29;29;29,3,5/29/19,12,8,0,0,3,0,1903;1199;46450,11;28;272,7;10;52,245;232;6610,m;m
4469,ICLR,2020,Pushing the bounds of dropout,Gábor Melis;Charles Blundell;Tomáš Kočiský;Karl Moritz Hermann;Chris Dyer;Phil Blunsom,melisgl@google.com;cblundell@google.com;tkocisky@google.com;kmh@google.com;cdyer@google.com;pblunsom@google.com,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0,yes,9/25/19,Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,3;1,5/23/18,9,3,4,2,19,0,611;6176;2781;5184;21507;11713,14;50;16;41;231;144,8;22;10;21;61;47,99;1088;429;710;3161;1357,m;m
4470,ICLR,2020,CRNet: Image Super-Resolution Using A Convolutional Sparse Coding  Inspired Network,Menglei Zhang;Zhou Liu;Jingwei He;Lei Yu,zmlhome@whu.edu.cn;liuzhou@whu.edu.cn;jingwei_he@whu.edu.cn;ly.wd@whu.edu.cn,3;1;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0,yes,9/25/19,Wuhan University;Wuhan University;Wuhan University;Wuhan University,266;266;266;266,354;354;354;354,2,8/3/19,0,0,0,0,0,0,448;172;-1;177,23;46;-1;88,9;8;-1;8,25;5;0;6,m;m
4471,ICLR,2020,Hierarchical Graph Matching Networks for Deep Graph Similarity Learning,Xiang Ling;Lingfei Wu;Saizhuo Wang;Tengfei Ma;Fangli Xu;Chunming Wu;Shouling Ji,lingxiang@zju.edu.cn;lwu@email.wm.edu;szwang@zju.edu.cn;tengfei.ma1@ibm.com;lili@yixue.us;wuchunming@zju.edu.cn;sji@zju.edu.cn,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Zhejiang University;College of William and Mary;Zhejiang University;International Business Machines;;Zhejiang University;Zhejiang University,56;154;56;-1;-1;56;56,107;235;107;-1;-1;107;107,10,9/25/19,2,1,0,0,0,1,34;93;2;4;323;484;48,3;33;1;3;14;116;33,2;5;1;2;6;11;3,4;8;1;1;31;38;7,m;m
4472,ICLR,2020,RATE-DISTORTION OPTIMIZATION GUIDED AUTOENCODER FOR GENERATIVE APPROACH,Keizo Kato;Jing Zhou;Akira Nakagawa,kato.keizo@jp.fujitsu.com;zhoujing@cn.fujitsu.com;anaka@jp.fujitsu.com,1;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,Fujitsu Laboratories Ltd.;Fujitsu Laboratories Ltd.;Fujitsu Laboratories Ltd.,-1;-1;-1,-1;-1;-1,5,9/25/19,1,0,0,0,0,0,62;68;270,46;77;148,4;5;7,0;0;17,m;m
4473,ICLR,2020,Learning to Recognize the Unseen Visual Predicates,Defa Zhu;Si Liu;Wentao Jiang;Guanbin Li;Tianyi Wu;Guodong Guo,zhudefa@iie.ac.cn;liusi@buaa.edu.cn;jiangwentao@buaa.edu.cn;liguanbin@mail.sysu.edu.cn;wutianyi01@baidu.com;guoguodong01@baidu.com,6;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,5,0,yes,9/25/19,"Institute of information engineering, CAS;Beihang University;Beihang University;SUN YAT-SEN UNIVERSITY;Baidu;Baidu",-1;118;118;481;-1;-1,-1;594;594;299;-1;-1,6,9/25/19,0,0,0,0,0,0,178;0;2;2097;0;6066,13;8;7;86;3;195,6;0;1;18;0;37,8;0;0;385;0;592,m;m
4474,ICLR,2020,Self-supervised Training of Proposal-based Segmentation via Background Prediction,Isinsu Katircioglu;Helge Rhodin;Victor Constantin;Jörg Spörri;Mathieu Salzmann;Pascal Fua,isinsu.katircioglu@epfl.ch;rhodin@cs.ubc.ca;victor.constantin@epfl.ch;joerg.spoerri@balgrist.ch;mathieu.salzmann@epfl.ch;pascal.fua@epfl.ch,6;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Swiss Federal Institute of Technology Lausanne;University of British Columbia;Swiss Federal Institute of Technology Lausanne;University of Zurich;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne,481;35;481;143;481;481,38;34;38;90;38;38,2,7/18/19,0,0,0,0,0,0,264;1254;121;543;5661;30326,6;44;35;49;198;476,5;14;4;13;42;77,26;149;16;33;623;3592,f;m
4475,ICLR,2020,Constrained Markov Decision Processes via Backward Value Functions,Harsh Satija;Philip Amortila;Joelle Pineau,harsh.satija@mail.mcgill.ca;philip.amortila@mail.mcgill.ca;jpineau@cs.mcgill.ca,3;8;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,McGill University;McGill University;McGill University,86;86;86,42;42;42,,9/25/19,0,0,0,0,0,0,59;0;11328,4;6;267,3;0;46,4;0;1235,m;f
4476,ICLR,2020,Deep Interaction Processes for Time-Evolving Graphs,xiaofu chang;jianfeng wen;xuqin liu;yanming fang;le song;yuan qi,xiaofu.cxf@antfin.com;sylvain.wjf@antfin.com;xuqin.lxq@antfin.com;yanming.fym@mybank.cn;le.song@antfin.com;yuan.qi@antfin.com,3;3;3,I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,5,0,yes,9/25/19,Antfin;Antfin;Peking University;;Antfin;Antfin,-1;-1;22;-1;-1;-1,-1;-1;24;-1;-1;-1,10,9/25/19,0,0,0,0,0,0,18;4;59;1;9519;1,9;5;5;9;329;8,3;1;2;1;54;1,0;0;7;0;1114;0,u;u
4477,ICLR,2020,On summarized validation curves and generalization,Mohammad Hashir;Yoshua Bengio;Joseph Paul Cohen,mohammad.hashir.khan@umontreal.ca;yoshua.bengio@mila.quebec;joseph@josephpcohen.com,3;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:N/A:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,University of Montreal;University of Montreal;University of Montreal,128;128;128,85;85;85,1,9/25/19,0,0,0,0,0,0,20;208566;505,8;807;62,3;147;11,3;24297;50,m;m
4478,ICLR,2020,The Visual Task Adaptation Benchmark,Xiaohua Zhai;Joan Puigcerver;Alexander Kolesnikov;Pierre Ruyssen;Carlos Riquelme;Mario Lucic;Josip Djolonga;Andre Susano Pinto;Maxim Neumann;Alexey Dosovitskiy;Lucas Beyer;Olivier Bachem;Michael Tschannen;Marcin Michalski;Olivier Bousquet;Sylvain Gelly;Neil Houlsby,xzhai@google.com;jpuigcerver@google.com;alexander.kolesnikoff@gmail.com;pierrot@google.com;rikel@googel.com;lucic@google.com;josipd@google.com;andresp@google.com;maximneumann@google.com;adosovitskiy@gmail.com;lbeyer@google.com;bachem@google.com;tschannen@google.com;michalski@google.com;obousquet@google.com;sylvaingelly@google.com;neilhoulsby@google.com,8;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Reject,0,7,0,yes,9/25/19,Google;Google;Google;Google;Googel;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,5,9/25/19,15,10,4,0,0,2,842;299;406;19;91;1625;353;25;910;2042;1524;904;1229;20;178;3609;895,35;22;49;3;41;48;21;6;72;13;27;38;39;7;31;112;34,14;10;11;2;5;21;9;3;15;5;11;14;18;2;6;26;15,106;36;19;2;6;196;24;3;68;338;197;101;160;2;16;464;117,m;m
4479,ICLR,2020,SPREAD  DIVERGENCE,Mingtian Zhang;David Barber;Thomas Bird;Peter Hayes;Raza Habib,mingtian.zhang.17@ucl.ac.uk;david.barber@ucl.ac.uk;thomas.bird.17@ucl.ac.uk;peter.hayes.15@ucl.ac.uk;r.habib@cs.ucl.ac.uk,3;1;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,3,0,yes,9/25/19,University College London;University College London;University College London;University College London;University College London,50;50;50;50;50,15;15;15;15;15,5,11/21/18,1,0,0,0,4,0,115;3832;540;-1;517,37;200;46;-1;9,6;27;10;-1;3,15;411;38;0;39,m;m
4480,ICLR,2020,Adversarially learned anomaly detection for time series data,Alexander Geiger;Alfredo Cuesta-Infante;Kalyan Veeramachaneni,geigera@mit.edu;alfredo.cuesta@urjc.es;kalyanv@mit.edu,1;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0,yes,9/25/19,Massachusetts Institute of Technology;Universidad Rey Juan Carlos;Massachusetts Institute of Technology,2;-1;2,5;-1;5,5;4,9/25/19,0,0,0,0,0,0,9;152;2413,4;29;129,2;6;23,0;18;270,m;m
4481,ICLR,2020,CEB Improves Model Robustness,Ian Fischer;Alex A. Alemi,iansf@google.com;alemi@google.com,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Google;Google,-1;-1,-1;-1,4,9/25/19,1,1,1,0,0,1,2706;1320,16;53,12;14,364;186,m;m
4482,ICLR,2020,Mildly Overparametrized Neural Nets can Memorize Training Data Efficiently,Rong Ge;Runzhe Wang;Haoyu Zhao,rongge@cs.duke.edu;wrz16@mails.tsinghua.edu.cn;zhaohy16@mails.tsinghua.edu.cn,1;3;8,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I did not assess the experiments.:N/A:I carefully checked the derivations and theory.,Reject,0,3,0,yes,9/25/19,Duke University;Tsinghua University;Tsinghua University,47;8;8,20;23;23,,9/25/19,6,4,0,1,0,0,477;20;122,24;7;38,6;3;6,74;0;3,m;m
4483,ICLR,2020,Data-Driven Approach to Encoding and Decoding 3-D Crystal Structures,Jordan Hoffmann;Louis Maestrati;Yoshihide Sawada;Jian Tang;Jean Michel Sellier;Yoshua Bengio,jhoffmann@g.harvard.edu;maestratilouis@gmail.com;sawada.yoshihide@jp.panasonic.com;jian.tang@hec.ca;jeanmichel.sellier@mila.quebec;yoshua.bengio@mila.quebec,8;1;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:N/A:N/A:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,6,0,yes,9/25/19,Harvard University;centrale lille;Panasonic Corporation;HEC Montreal;University of Montreal;University of Montreal,39;-1;-1;128;128;128,7;-1;-1;85;85;85,5,9/3/19,9,4,2,0,0,0,103;8;366;5178;343;208566,23;1;84;154;92;807,7;1;11;34;10;147,8;0;14;450;11;24297,m;m
4484,ICLR,2020,Mean Field Models for Neural Networks in Teacher-student Setting,Lexing Ying;Yuandong Tian,lexing@stanford.edu;yuandong@fb.com,3;3;1,I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,5,0,yes,9/25/19,Stanford University;Facebook,4;-1,4;-1,,9/25/19,0,0,0,0,0,0,9;2498,18;85,2;25,0;293,m;m
4485,ICLR,2020,A novel Bayesian estimation-based word embedding model for sentiment analysis,Jingyao Tang;Yun Xue;Ziwen Wang;Haoliang Zhao,manderous@foxmail.com;995438712@qq.com;773473833@qq.com;1044012786@qq.com,6;1;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Australian National University;;;,108;-1;-1;-1,50;-1;-1;-1,3;11,9/25/19,0,0,0,0,0,0,4;0;1;79,5;10;5;18,1;0;1;3,0;0;0;1,u;u
4486,ICLR,2020,Improving Gradient Estimation in Evolutionary Strategies With Past Descent Directions,Florian Meier;Asier Mujika;Marcelo Gauy;Angelika Steger,meierflo@inf.ethz.ch;asierm@inf.ethz.ch;marcelo.matheus@inf.ethz.ch;steger@inf.ethz.ch,3;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10;10,13;13;13;13,1,9/25/19,2,1,0,0,0,0,111;89;15;3255,33;10;11;204,7;4;3;28,5;10;1;327,m;f
4487,ICLR,2020,Few-Shot Few-Shot Learning and the role of Spatial Attention,Yann Lifchitz;Yannis Avrithis;Sylvaine Picard,yann.lifchitz@safrangroup.com;yannis@avrithis.net;sylvaine.picard@safrangroup.com,3;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0,yes,9/25/19,SAFRAN;INRIA;SAFRAN,-1;-1;-1,-1;-1;-1,6,9/25/19,0,0,0,0,0,0,25;3566;51,3;198;15,2;32;3,6;243;8,m;f
4488,ICLR,2020,Generative Cleaning Networks with Quantized Nonlinear Transform  for  Deep Neural Network Defense,Jianhe Yuan;Zhihai He,yuanjia@missouri.edu;hezhi@missouri.edu,1;8;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,10,0,yes,9/25/19,"University of Missouri, Columbia;University of Missouri, Columbia",390;390,424;424,5;4,9/25/19,0,0,0,0,0,0,53;4277,9;215,3;31,0;333,u;m
4489,ICLR,2020,Clustered Reinforcement Learning,Xiao Ma;Shen-Yi Zhao;Zhao-Heng Yin;Wu-Jun Li,max@lamda.nju.edu.cn;zhaosy@lamda.nju.edu.cn;zhaohengyin@gmail.com;liwujun@nju.edu.cn,3;6;3,I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,Zhejiang University;Zhejiang University;;Zhejiang University,56;56;-1;56,107;107;-1;107,,6/6/19,0,0,0,0,0,0,13;93;-1;2932,9;15;-1;92,3;4;-1;25,1;13;0;562,u;u
4490,ICLR,2020,Auto Completion of User Interface Layout Design Using Transformer-Based Tree Decoders,Yang Li;Julien Amelot;Xin Zhou;Samy Bengio;Si Si,liyang@google.com;jamelot@google.com;zhouxin@google.com;bengio@google.com;sisidaisy@google.com,3;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,10,9/25/19,0,0,0,0,0,0,-1;120;3335;586;299,-1;21;94;36;10,-1;6;10;12;2,0;11;225;25;46,m;f
4491,ICLR,2020,Visual Imitation with Reinforcement Learning using Recurrent Siamese Networks,Glen Berseth;Christopher Pal,gberseth@gmail.com;christopher.pal@polymtl.ca,8;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,University of California Berkeley;Polytechnique Montreal,5;390,13;1397,,9/25/19,0,0,0,0,0,0,640;39,45;11,11;2,21;2,m;m
4492,ICLR,2020,Deep Graph Translation,Xiaojie Guo;Lingfei Wu;Liang Zhao,xguo7@gmu.edu;wuli@us.ibm.com;lzhao9@gmu.edu,3;3;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,George Mason University;International Business Machines;George Mason University,100;-1;100,282;-1;282,5;4;10,5/25/18,8,4,3,0,9,0,2633;653;571,181;62;79,26;15;10,360;62;45,m;m
4493,ICLR,2020,GAN-based Gaussian Mixture Model Responsibility Learning,Wanming Huang;Shuai Jiang;Xuan Liang;Ian Oppermann;Richard Yi Da Xu,wanming.huang@student.uts.edu.au;shuai.jiang-1@student.uts.edu.au;xuan.liang@student.uts.edu.au;ianopper@outlook.com;yida.xu@uts.edu.au,1;3;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,University of Technology Sydney;University of Technology Sydney;University of Technology Sydney;NSW Government;University of Technology Sydney,108;108;108;-1;108,193;193;193;-1;193,5;4,9/25/19,0,0,0,0,0,0,13;7;380;2039;625,7;9;35;129;89,2;1;8;22;11,3;3;26;158;44,u;m
4494,ICLR,2020,CROSS-DOMAIN CASCADED DEEP TRANSLATION,Oren Katzir;Dani Lischinski;Daniel Cohen-Or,orenkatzir@mail.tau.ac.il;cohenor@gmail.com;danix3d@gmail.com,6;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Tel Aviv University;;Hebrew University of Jerusalem,35;-1;67,188;-1;216,4,6/4/19,2,0,2,0,0,0,30;13242;19779,7;148;345,3;51;74,3;1539;1576,m;m
4495,ICLR,2020,Mean-field Behaviour of Neural Tangent Kernel for Deep Neural Networks,Soufiane Hayou;Arnaud Doucet;Judith Rousseau,soufiane.hayou@stats.ox.ac.uk;doucet@stats.ox.ac.uk;judith.rousseau@stats.ox.ac.uk,6;6;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,University of Oxford;University of Oxford;University of Oxford,50;50;50,1;1;1,,5/31/19,6,2,0,0,0,1,73;21104;2538,7;246;147,3;52;25,9;2619;259,m;f
4496,ICLR,2020,NORML: Nodal Optimization for Recurrent Meta-Learning,David van Niekerk,davidpetrus94@gmail.com,1;1;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,0,0,yes,9/25/19,University of the Witwatersrand,481,193,6,9/25/19,0,0,0,0,0,0,41,6,2,2,m
4497,ICLR,2020,Programmable Neural Network Trojan for Pre-trained Feature Extractor,Yu Ji;Zinxin Liu;Xing Hu;Peiqi Wang;Youhui Zhang,jiy15@mails.tsinghua.edu.cn;liuzixin18@mails.tsinghua.edu.cn;xinghu@ucsb.edu;wpq14@mails.tsinghua.edu.cn;zyh02@tsinghua.edu.cn,3;1;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,3,0,yes,9/25/19,Tsinghua University;Tsinghua University;UC Santa Barbara;Tsinghua University;Tsinghua University,8;8;38;8;8,23;23;57;23;23,4,1/23/19,3,1,2,0,4,0,156;214;535;77;473,45;66;114;18;79,6;9;11;5;12,8;8;51;6;29,m;m
4498,ICLR,2020,Contextual Inverse Reinforcement Learning,Philip Korsunsky;Stav Belogolovsky;Tom Zahavy;Chen Tessler;Shie Mannor,philip.korsunsky@gmail.com;stav.belo@gmail.com;tomzahavy@gmail.com;chen.tessler@gmail.com;shie@ee.technion.ac.il,6;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,13,0,yes,9/25/19,Technion;Technion;Technion;Technion;Technion,26;26;26;26;26,412;412;412;412;412,,9/25/19,0,0,0,0,0,0,0;0;496;255;11682,3;3;31;10;418,0;0;10;5;50,0;0;29;17;1230,m;m
4499,ICLR,2020,Convolutional Bipartite Attractor Networks,Michael L. Iuzzolino;Yoram Singer;Michael C. Mozer,michael.iuzzolino@colorado.edu;yoram.singer@gmail.com;mcmozer@google.com,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0,yes,9/25/19,"University of Colorado, Boulder;;Google",44;-1;-1,123;-1;-1,5,6/8/19,0,0,0,0,0,0,4;35018;6955,8;211;238,1;59;44,0;4948;518,m;m
4500,ICLR,2020,DeepSimplex: Reinforcement Learning of Pivot Rules Improves the Efficiency of Simplex Algorithm in Solving Linear Programming Problems,Varun Suriyanarayana;Onur Tavaslioglu;Ankit B. Patel;Andrew J. Schaefer,vs478@cornell.edu;onur.tavaslioglu@bcm.edu;ankit.patel@bcm.edu;andrew.schaefer@rice.edu,1;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Cornell University;Baylor College of Medicine;Baylor College of Medicine;Rice University,7;-1;-1;84,19;-1;-1;105,,9/25/19,0,0,0,0,0,0,0;7;378;2191,4;5;30;125,0;2;6;25,0;1;39;169,m;m
4501,ICLR,2020,Temporal-difference learning for nonlinear value function approximation in the lazy training regime,Andrea Agazzi;Jianfeng Lu,agazzi@math.duke.edu;jianfeng@math.duke.edu,6;6;3;3,I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Duke University;Duke University,47;47,20;20,1,9/25/19,5,3,0,0,0,0,56;2131,10;149,4;20,5;193,m;m
4502,ICLR,2020,Off-policy Multi-step Q-learning,Gabriel Kalweit;Maria Huegle;Joschka Boedecker,kalweitg@cs.uni-freiburg.de;hueglem@informatik.uni-freiburg.de;jboedeck@informatik.uni-freiburg.de,1;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,14,0,yes,9/25/19,Universität Freiburg;Universität Freiburg;Universität Freiburg,118;118;118,85;85;85,1,9/25/19,1,1,1,0,0,0,52;23;1167,7;8;50,2;3;17,2;1;86,m;m
4503,ICLR,2020,Towards Interpretable Molecular Graph Representation Learning,Emmanuel Noutahi;Dominique Beani;Julien Horwood;Prudencio Tossou,emmanuel@invivoai.com;dominique@invivoai.com;julien@invivoai.com;prudencio@invivoai.com,6;1;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,5,0,yes,9/25/19,InVivo AI;InVivo AI;InVivo AI;InVivo AI,-1;-1;-1;-1,-1;-1;-1;-1,10,9/25/19,0,0,0,0,0,0,76;7;7;77,13;2;3;10,5;1;1;3,5;0;0;8,m;m
4504,ICLR,2020,Effective Mechanism to Mitigate Injuries During NFL Plays ,Arraamuthan Arulanantham;Ahamed Arshad Ahamed Anzar;Gowshalini Rajalingam;Krusanth Ingran;Prasanna S. Haddela,anzanfas@gmail.com;arulanantham.arraamuthan@my.sliit.lk;it16113800@my.sliit.lk;krusanth7@gmail.com;prasanna@sliit.lk,1;1;1,I do not know much about this area.:N/A:I made a quick assessment of this paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,Srilanka Institute of information Technology;Srilanka Institute of information Technology;Srilanka Institute of information Technology;;Srilanka Institute of information Technology,481;481;481;-1;481,1397;1397;1397;-1;1397,1,9/25/19,0,0,0,0,0,0,0;0;0;0;31,1;1;1;1;24,0;0;0;0;3,0;0;0;0;1,u;u
4505,ICLR,2020,Ecological Reinforcement Learning,John D. Co-Reyes;Suvansh Sanjeev;Glen Berseth;Abhishek Gupta;Sergey Levine,jcoreyes@eecs.berkeley.edu;suvansh@berkeley.edu;gberseth@gmail.com;abhigupta@berkeley.edu;svlevine@eecs.berkeley.edu,1;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,5,0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5;5,13;13;13;13;13,,9/25/19,0,0,0,0,0,0,135;13;640;165;24893,8;2;45;41;310,4;1;11;7;74,17;1;21;7;3235,m;m
4506,ICLR,2020,CP-GAN: Towards a Better Global Landscape of GANs,Ruoyu Sun;Tiantian Fang;Alex Schwing,ruoyus@illinois.edu;tf6@illinois.edu;aschwing@illinois.edu,8;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",3;3;3,48;48;48,5;1;9,9/25/19,0,0,0,0,0,0,56;5;3818,10;4;118,2;1;32,3;0;353,m;m
4507,ICLR,2020,Learning Semantically Meaningful Representations Through Embodiment,Viviane Clay;Peter König;Kai-Uwe Kühnberger;Gordon Pipa,vkakerbeck@uos.de;pkoenig@uos.de;kkuehnbe@uos.de;gpipa@uos.de,3;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,6,1,yes,9/25/19,University of Osnabrück;University of Osnabrück;University of Osnabrück;University of Osnabrück,323;323;323;323,1397;1397;1397;1397,,9/25/19,0,0,0,0,0,0,1;780;23;2349,3;135;7;128,1;18;4;19,0;38;0;141,f;m
4508,ICLR,2020,Graph Warp Module: an Auxiliary Module for Boosting the Power of Graph Neural Networks in Molecular Graph Analysis,Katsuhiko Ishiguro;Shin-ichi Maeda;Masanori Koyama,k.ishiguro.jp@ieee.org;ichi@preferred.jp;masomatics@preferred.jp,3;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:N/A:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:N/A:N/A,Reject,0,4,0,yes,9/25/19,"Preferred Networks, Inc.;Preferred Networks, Inc.;Preferred Networks, Inc.",-1;-1;-1,-1;-1;-1,8,2/4/19,13,2,0,0,3,1,628;2028;2498,73;99;35,14;21;12,42;267;507,m;m
4509,ICLR,2020,End-To-End Input Selection for Deep Neural Networks,Stefan Oehmcke;Fabian Gieseke,stefan.oehmcke@gmail.com;fabian.gieseke@di.ku.dk,3;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,University of Copenhagen;University of Copenhagen,100;100,101;101,,9/25/19,0,0,0,0,0,0,63;445,18;65,5;13,0;26,m;m
4510,ICLR,2020,Learning Curves for Deep Neural Networks: A field theory perspective,Omry Cohen;Or Malka;Zohar Ringel,omrycohen.38.talpiot@gmail.com;or.malka@mail.huji.ac.il;zohar.ringel@mail.huji.ac.il,1;3;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,;Hebrew University of Jerusalem;Hebrew University of Jerusalem,-1;67;67,-1;216;216,11,6/12/19,7,3,0,1,3,1,7;5;580,3;2;32,1;1;7,1;1;17,m;m
4511,ICLR,2020,Regional based query in graph active learning,Abel Roy;Louzoun Yoram,royabel10@gmail.com;louzouy@math.biu.ac.il,1;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,2,0,yes,9/25/19,Bar Ilan University;Bar Ilan University,95;95,513;513,10,6/20/19,2,0,2,0,0,0,212;2298,11;173,4;25,24;96,m;m
4512,ICLR,2020,Analysis and Interpretation of Deep CNN Representations as Perceptual Quality Features,Taimoor Tariq;Munchurl Kim,taimoor.tariq@kaist.ac.kr;mkimee@kaist.ac.kr,3;6;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0,yes,9/25/19,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,481;481,110;110,2,12/2/18,0,0,0,0,0,0,15;2057,11;172,2;22,2;167,m;m
4513,ICLR,2020,Dynamical System Embedding for Efficient Intrinsically Motivated Artificial Agents,Ruihan Zhao;Stas Tiomkin;Pieter Abbeel,philipzhao@berkeley.edu;stas@berkeley.edu;pabbeel@cs.berkeley.edu,1;3;3,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,13;13;13,,9/25/19,0,0,0,0,0,0,1;78;37294,2;14;438,1;4;94,0;3;4481,m;m
4514,ICLR,2020,Siamese Attention Networks,Hongyang Gao;Yaochen Xie;Shuiwang Ji,hongyang.gao@tamu.edu;ethanycx@tamu.edu;sji@tamu.edu,6;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,2,0,yes,9/25/19,Texas A&M;Texas A&M;Texas A&M,44;44;44,177;177;177,,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1,-1;-1;-1,-1;-1;-1,0;0;0,m;m
4515,ICLR,2020,Network Pruning for Low-Rank Binary Index,Dongsoo Lee;Se Jung Kwon;Byeongwook Kim;Parichay Kapoor;Gu-Yeon Wei,dslee3@gmail.com;mogndrewk@gmail.com;quddnr145@gmail.com;kparichay@gmail.com;gywei@g.harvard.edu,3;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A,Reject,0,0,0,yes,9/25/19,;Samsung;Samsung;;Harvard University,-1;-1;-1;-1;39,-1;-1;-1;-1;7,,5/14/19,3,2,2,0,0,0,60;23;23;15;5285,14;22;10;6;180,2;3;3;2;37,6;0;1;0;456,m;m
4516,ICLR,2020,Attention Privileged Reinforcement Learning for Domain Transfer,Sasha Salter;Dushyant Rao;Markus Wulfmeier;Raia Hadsell;Ingmar Posner,sasha@robots.ox.ac.uk;dushyantr@google.com;mwulfmeier@google.com;raia@google.com;ingmar@robots.ox.ac.uk,3;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,11,0,yes,9/25/19,University of Oxford;Google;Google;Google;University of Oxford,50;-1;-1;-1;50,1;-1;-1;-1;1,,9/25/19,2,1,0,0,0,0,26;680;615;8331;2530,13;22;27;63;102,2;11;12;26;28,2;76;51;804;161,m;m
4517,ICLR,2020,Wildly Unsupervised Domain Adaptation and Its Powerful and Efficient Solution,Feng Liu;Jie Lu;Bo Han;Gang Niu;Guangquan Zhang;Masashi Sugiyama,feng.liu-2@student.uts.edu.au;jie.lu@uts.edu.au;bo.han@riken.jp;gang.niu@riken.jp;guangquan.zhang@uts.edu.au;sugi@k.u-tokyo.ac.jp,1;8;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,9,0,yes,9/25/19,University of Technology Sydney;University of Technology Sydney;RIKEN;RIKEN;University of Technology Sydney;The University of Tokyo,108;108;-1;-1;108;56,193;193;-1;-1;193;36,,9/25/19,0,0,0,0,0,0,276;242;41;1198;37;138,86;57;37;80;16;91,10;8;3;17;3;6,5;11;2;148;1;3,m;m
4518,ICLR,2020,"Deep RL for Blood Glucose Control: Lessons, Challenges, and Opportunities",Ian Fox;Joyce Lee;Rodica Busui;Jenna Wiens,ifox@umich.edu;joyclee@med.umich.edu;rpbusui@umich.edu;wiensj@umich.edu,3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,University of Michigan;University of Michigan;University of Michigan;University of Michigan,8;8;8;8,21;21;21;21,,9/25/19,0,0,0,0,0,0,31;38;10;350,17;14;2;37,4;2;1;9,4;4;1;22,m;f
4519,ICLR,2020,Learning Likelihoods with Conditional Normalizing Flows ,Christina Winkler;Daniel Worrall;Emiel Hoogeboom;Max Welling,christina.winkler.94@gmail.com;d.e.worrall@uva.nl;e.hoogeboom@uva.nl;m.welling@uva.nl,3;6;6,I have read many papers in this area.:N/A:N/A:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,2,5,0,yes,9/25/19,University of Amsterdam;University of Amsterdam;University of Amsterdam;University of Amsterdam,172;172;172;172,62;62;62;62,5;2,9/25/19,8,3,4,0,0,1,16;18;88;27129,14;3;10;270,2;3;4;59,1;0;12;5160,f;m
4520,ICLR,2020,Detecting Out-of-Distribution Inputs to Deep Generative Models Using Typicality,Eric Nalisnick;Akihiro Matsukawa;Yee Whye Teh;Balaji Lakshminarayanan,e.nalisnick@eng.cam.ac.uk;matsukaw@deshaw.com;ywteh@google.com;balajiln@google.com,3;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,6,0,yes,9/25/19,University of Cambridge;Deshaw;Google;Google,71;-1;-1;-1,3;-1;-1;-1,5,6/7/19,19,11,10,5,0,10,680;4055;25001;3100,33;247;250;43,13;34;54;23,99;210;3272;402,m;m
4521,ICLR,2020,A Mechanism of Implicit Regularization in Deep Learning,Masayoshi Kubo;Genki Sugiura;Kenta Shinzato;Momose Oyama,kubo@i.kyoto-u.ac.jp;sugiura.genki.42n@st.kyoto-u.ac.jp;shinzato.kenta.82r@st.kyoto-u.ac.jp;oyama.momose.75c@st.kyoto-u.ac.jp,3;3;1,I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:N/A:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,10,0,yes,9/25/19,Meiji University;Meiji University;Meiji University;Meiji University,481;481;481;481,332;332;332;332,1;8,9/25/19,0,0,0,0,0,0,3;0;0;0,45;1;3;1,1;0;0;0,0;0;0;0,m;f
4522,ICLR,2020,Scaleable input gradient regularization for adversarial robustness,Chris Finlay;Adam M Oberman,christopher.finlay@mail.mcgill.ca;adam.oberman@mcgill.ca,3;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,12,0,yes,9/25/19,McGill University;McGill University,86;86,42;42,4,5/27/19,4,2,2,0,3,0,158;1689,48;88,8;22,10;157,m;m
4523,ICLR,2020,Weakly-Supervised Trajectory Segmentation for Learning Reusable Skills,Parsa Mahmoudieh;Trevor Darrell;Deepak Pathak,parsa.m@berkeley.edu;trevor@eecs.berkeley.edu;pathak@berkeley.edu,3;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,3,0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,13;13;13,6;2,9/25/19,0,0,0,0,0,0,191;90979;4225,8;559;41,4;112;14,14;11527;553,m;m
4524,ICLR,2020,Compositional Visual Generation with Energy Based Models,Yilun Du;Shuang Li;Igor Mordatch,yilundu@mit.edu;lishuang@mit.edu;mordatch@google.com,3;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,10,0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Google,2;2;-1,5;5;-1,8,9/25/19,0,0,0,0,0,0,128;387;3078,27;76;49,6;9;27,10;39;351,m;m
4525,ICLR,2020,Fourier networks for uncertainty estimates and out-of-distribution detection,Hartmut Maennel;Alexandru Țifrea,hartmutm@google.com;tifreaa@student.ethz.ch,3;6;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,2,3,0,yes,9/25/19,Google;Swiss Federal Institute of Technology,-1;10,-1;13,8,9/25/19,0,0,0,0,0,0,23;55,8;5,2;2,3;7,m;m
4526,ICLR,2020,Finding Winning Tickets with Limited (or No) Supervision,Mathilde Caron;Ari Morcos;Piotr Bojanowski;Julien Mairal;Armand Joulin,mathilde@fb.com;arimorcos@gmail.com;bojanowski@fb.com;julien.mairal@inria.fr;ajoulin@fb.com,1;3;6;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,5,0,yes,9/25/19,Facebook;Facebook;Facebook;INRIA;Facebook,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,,9/25/19,1,0,0,0,0,0,386;1034;7738;15567;10653,19;32;41;120;74,4;12;20;38;32,56;116;1123;1443;1533,f;m
4527,ICLR,2020,DASGrad: Double Adaptive Stochastic Gradient,Kin Gutierrez;Cristian Challu;Jin Li;Artur Dubrawski,kdgutier@cs.cmu.edu;cchallu@cs.cmu.edu;jinl2@cs.cmu.edu;awd@cs.cmu.edu,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,2,0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1,27;27;27;27,6;9,9/25/19,0,0,0,0,0,0,0;0;533;638,2;2;108;149,0;0;12;13,0;0;10;29,m;m
4528,ICLR,2020,Physics-Aware Flow Data Completion Using Neural Inpainting,Sebastien Foucher;Jingwei Tang;Vinicius da Costa de Azevedo;Byungsoo Kim;Markus Gross;Barbara Solenthaler,sfoucher@ethz.ch;jingwei.tang@inf.ethz.ch;vinicius.azevedo@inf.ethz.ch;kimby@inf.ethz.ch;grossm@inf.ethz.ch;solenthaler@inf.ethz.ch,1;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10;10;10;10,13;13;13;13;13;13,,9/25/19,0,0,0,0,0,0,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,0;0;0;0;0;0,m;f
4529,ICLR,2020,Towards Modular Algorithm Induction,Daniel A. Abolafia;Rishabh Singh;Manzil Zaheer;Charles Sutton,danabo@google.com;rising@google.com;manzilzaheer@google.com;charlessutton@google.com,1;1;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,1,0,yes,9/25/19,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,,9/25/19,1,0,0,0,0,0,273;913;1602;50,8;53;63;18,4;12;17;3,30;96;263;4,m;m
4530,ICLR,2020,PROVABLY BENEFITS OF DEEP HIERARCHICAL RL,Zeyu Jia;Simon S. Du;Ruosong Wang;Mengdi Wang;Lin F. Yang,jiazy@pku.edu.cn;ssdu@ias.edu;ruosongw@andrew.cmu.edu;mengdiw@princeton.edu;linyang@ee.ucla.edu,1;3;3,I have published one or two papers in this area.:N/A:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:N/A:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,"Peking University;Institue for Advanced Study, Princeton;Carnegie Mellon University;Princeton University;University of California, Los Angeles",22;-1;1;31;20,24;-1;27;6;17,,9/25/19,0,0,0,0,0,0,29;81;13;43;678,14;14;3;40;82,3;5;2;4;14,1;4;0;2;44,m;m
4531,ICLR,2020,Effects of Linguistic Labels on Learned Visual Representations in Convolutional Neural Networks: Labels matter!,Seoyoung Ahn;Gregory Zelinsky;Gary Lupyan,seoyoung.ahn@stonybrook.edu;gregory.zelinsky@stonybrook.edu;lupyan@wisc.edu,6;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,6,0,yes,9/25/19,"State University of New York, Stony Brook;State University of New York, Stony Brook;University of Southern California",41;41;31,304;304;62,,9/25/19,0,0,0,0,0,0,4;4075;3189,6;170;169,2;33;31,0;277;235,f;m
4532,ICLR,2020,What Can Learned Intrinsic Rewards Capture?,Zeyu Zheng;Junhyuk Oh;Matteo Hessel;Zhongwen Xu;Manuel Kroiss;Hado van Hasselt;David Silver;Satinder Singh,zeyu@umich.edu;junhyuk@google.com;mtthss@google.com;zhongwen@google.com;makro@google.com;hado@google.com;davidsilver@google.com;baveja@google.com,6;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0,yes,9/25/19,University of Michigan;Google;Google;Google;Google;Google;Google;Google,8;-1;-1;-1;-1;-1;-1;-1,21;-1;-1;-1;-1;-1;-1;-1,1;6,9/25/19,1,0,0,0,0,0,398;1403;2317;1367;22;5638;43236;88,48;24;27;46;3;51;159;28,10;13;13;17;1;21;56;6,51;138;371;154;3;930;5956;8,m;m
4533,ICLR,2020,QXplore: Q-Learning Exploration by Maximizing Temporal Difference Error,Riley Simmons-Edler;Ben Eisner;Daniel Yang;Anthony Bisulco;Eric Mitchell;Sebastian Seung;Daniel Lee,rileys@cs.princeton.edu;ben.a.eisner@gmail.com;daniel.yang17@gmail.com;arb426@cornell.edu;eric.anthony.mitchell95@gmail.com;sseung@princeton.edu;daniel.d.lee@samsung.com,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,5,0,yes,9/25/19,Princeton University;Samsung;;Cornell University;Stanford University;Princeton University;Samsung,31;-1;-1;7;4;31;-1,6;-1;-1;19;4;6;-1,4,9/25/19,3,0,0,0,3,0,42;134;18;-1;41;23293;-1,7;6;37;-1;25;121;-1,2;2;2;-1;3;46;-1,1;6;0;0;3;3334;0,m;m
4534,ICLR,2020,Neural Clustering Processes,Ari Pakman;Yueqi Wang;Catalin Mitelut;JinHyung Lee;Liam Paninski,aripakman@gmail.com;yueqi.wang.pku@gmail.com;mitelutco@gmail.com;jl4303@columbia.edu;liam@stat.columbia.edu,3;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Columbia University;Columbia University;;Columbia University;Columbia University,15;15;-1;15;15,16;16;-1;16;16,5;11,12/28/18,1,1,1,1,0,1,1064;-1;594;59;10942,37;-1;12;16;251,20;-1;5;3;50,109;0;30;3;986,m;m
4535,ICLR,2020,Pre-training as Batch Meta Reinforcement Learning with tiMe ,Quan Vuong;Shuang Liu;Minghua Liu;Kamil Ciosek;Hao Su;Henrik Iskov Christensen,quan.hovuong@gmail.com;s3liu@eng.ucsd.edu;minghua@ucsd.edu;kamil.ciosek@microsoft.com;haosu@eng.ucsd.edu;hichristensen@ucsd.edu,3;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,10,0,yes,9/25/19,"University of California, San Diego;University of California, San Diego;University of California, San Diego;Microsoft;University of California, San Diego;University of California, San Diego",11;11;11;-1;11;11,31;31;31;-1;31;31,8,9/25/19,0,0,0,0,0,0,1782;69;8;136;17303;8486,124;70;8;20;48;461,23;4;2;7;15;49,81;1;0;14;2681;480,-1;-1
4536,ICLR,2020,Neural Architecture Search in Embedding Space,chun-ting liu,jimliu741523@gmail.com,1;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,3,0,yes,9/25/19,,,,,9/9/19,0,0,0,0,0,0,23,9,3,0,m;m
4537,ICLR,2020,Multi-Task Learning via Scale Aware Feature Pyramid Networks and Effective Joint Head,Feng Ni,nifeng@pku.edu.cn,3;3,I have published one or two papers in this area.:I did not assess the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,1,0,yes,9/25/19,Peking University,22,24,2,9/25/19,0,0,0,0,0,0,28,19,4,1,m
4538,ICLR,2020,BANANAS: Bayesian Optimization with Neural Networks for Neural Architecture Search,Colin White;Willie Neiswanger;Yash Savani,crwhite@cs.cmu.edu;willie@cs.cmu.edu;yash@realityengines.ai,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:N/A:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,2,9,3,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;,1;1;-1,27;27;-1,11,9/25/19,1,1,1,0,0,0,610;589;11,250;35;7,13;10;1,28;59;3,m;m
4539,ICLR,2020,Isolating Latent Structure with Cross-population Variational Autoencoders,Joe Davison;Kristen A. Severson;Soumya Ghosh,jddavison@g.harvard.edu;kristen.severson@ibm.com;ghoshso@us.ibm.com,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0,yes,9/25/19,Harvard University;International Business Machines;International Business Machines,39;-1;-1,7;-1;-1,5,9/25/19,0,0,0,0,0,0,25;2;505,4;4;25,2;1;8,3;0;61,m;m
4540,ICLR,2020,Stabilizing Off-Policy Reinforcement Learning with Conservative Policy Gradients,Chen Tessler;Nadav Merlis;Shie Mannor,chen.tessler@gmail.com;merlis.nadav@gmail.com;shiemannor@gmail.com,3;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Technion;Technion;Technion,26;26;26,412;412;412,,9/25/19,0,0,0,0,0,0,255;51;11682,10;7;418,5;3;50,17;8;1230,m;m
4541,ICLR,2020,When Covariate-shifted Data Augmentation Increases Test Error And How to Fix It,Sang Michael Xie*;Aditi Raghunathan*;Fanny Yang;John C. Duchi;Percy Liang,xie@cs.stanford.edu;aditir@stanford.edu;fannyang@stanford.edu;jduchi@stanford.edu;pliang@cs.stanford.edu,3;6;3,I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Stanford University;Stanford University;Stanford University;Stanford University;Stanford University,4;4;4;4;4,4;4;4;4;4,4;1,9/25/19,1,1,1,0,0,1,743;658;234;13365;12798,8;19;17;162;145,7;8;9;42;48,46;83;23;1908;2071,m;m
4542,ICLR,2020,Learning robust visual representations using data augmentation invariance,Alex Hernandez-Garcia;Peter König;Tim C. Kietzmann,alexhg15@gmail.com;pkoenig@uos.de;t.kietzmann@donders.ru.nl,3;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,3,0,yes,9/25/19,University of Osnabrück;University of Osnabrück;Radboud University Nijmegen,323;323;390,1397;1397;128,,6/11/19,2,1,1,0,0,0,87;780;850,15;135;47,5;18;14,1;38;51,m;m
4543,ICLR,2020,Learning to Generate Grounded Visual Captions without Localization Supervision,Chih-Yao Ma;Yannis Kalantidis;Ghassan AlRegib;Peter Vajda;Marcus Rohrbach;Zsolt Kira,cyma@gatech.edu;ykalant@image.ntua.gr;vajdap@fb.com;alregib@gatech.edu;maroffm@gmail.com;zkira@gatech.edu,3;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0,yes,9/25/19,Georgia Institute of Technology;National Technical University of Athens;Facebook;Georgia Institute of Technology;Facebook;Georgia Institute of Technology,13;323;-1;13;-1;13,38;776;-1;38;-1;38,3,6/1/19,2,2,2,1,0,2,273;2660;2201;0;11691;1181,14;42;259;4;90;69,7;18;24;0;45;16,27;410;145;0;1548;168,m;m
4544,ICLR,2020,On Empirical Comparisons of Optimizers for Deep Learning,Dami Choi;Christopher J. Shallue;Zachary Nado;Jaehoon Lee;Chris J. Maddison;George E. Dahl,choidami@cs.toronto.edu;shallue@google.com;znado@google.com;jaehlee@google.com;cmaddis@google.com;gdahl@google.com,1;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,4,19,0,yes,9/25/19,"Department of Computer Science, University of Toronto;Google;Google;Google;Google;Google",18;-1;-1;-1;-1;-1,18;-1;-1;-1;-1;-1,,9/25/19,16,3,1,0,0,0,169;301;134;612;8526;18394,6;12;10;55;31;45,3;6;5;8;15;27,32;33;19;99;529;1360,f;m
4545,ICLR,2020,LEARNING DIFFICULT PERCEPTUAL TASKS WITH HODGKIN-HUXLEY NETWORKS,Alan Lockett;Ankit Patel;Paul Pfaffinger,alan.lockett@gmail.com;ankitp@bcm.edu;paulp@bcm.edu,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0,yes,9/25/19,;Baylor College of Medicine;Baylor College of Medicine,-1;-1;-1,-1;-1;-1,2,9/25/19,0,0,0,0,0,0,65;354;1783,17;17;67,4;3;24,2;33;186,m;m
4546,ICLR,2020,Adversarial Robustness Against the Union of Multiple Perturbation Models,Pratyush Maini;Eric Wong;Zico Kolter,pratyush.maini@gmail.com;ericwong@cs.cmu.edu;zkolter@cs.cmu.edu,3;6;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,2,9,0,yes,9/25/19,Indian Institute of Technology Delhi;Carnegie Mellon University;Carnegie Mellon University,118;1;1,441;27;27,4;8,9/9/19,9,8,0,0,0,0,9;535;7776,2;50;107,1;11;35,0;63;1073,m;m
4547,ICLR,2020,LightPAFF: A Two-Stage Distillation Framework for Pre-training and Fine-tuning,Kaitao Song;Hao Sun;Xu Tan;Tao Qin;Jianfeng Lu;Hongzhi Liu;Tie-Yan Liu,kt.song@njust.edu.cn;sigmeta@pku.edu.cn;xuta@microsoft.com;taoqin@microsoft.com;lujf@njust.edu.cn;liuhz@pku.edu.cn;tyliu@microsoft.com,3;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:N/A:N/A,Reject,0,4,0,yes,9/25/19,Hong Kong University of Science and Technology;Peking University;Microsoft;Microsoft;Hong Kong University of Science and Technology;Peking University;Microsoft,39;22;-1;-1;39;22;-1,47;24;-1;-1;47;24;-1,3,9/25/19,0,0,0,0,0,0,175;167;9;15;824;0;13552,8;98;28;15;106;5;369,4;6;2;2;16;0;51,38;11;1;2;10;0;1723,m;m
4548,ICLR,2020,LocalGAN: Modeling Local Distributions for Adversarial Response Generation,Zhen Xu;Baoxun Wang;Huan Zhang;Kexin Qiu;Deyuan Zhang;Chengjie Sun,xuzhenhit@gmail.com;baoxun.wang@gmail.com;zhanghuan123@pku.edu.cn;kq2131@columbia.edu;dyzhang@sau.edu.cn;cjsun@insun.hit.edu.cn,3;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0,yes,9/25/19,Harbin Institute of Technology;;Peking University;Columbia University;Tsinghua University;Harbin Institute of Technology,172;-1;22;15;8;172,424;-1;24;16;23;424,5;4,9/25/19,0,0,0,0,0,0,61;500;249;625;3;1281,86;35;100;19;9;92,5;11;6;13;1;17,2;31;10;13;0;104,m;m
4549,ICLR,2020,Partial Simulation for Imitation Learning,Nir Baram;Shie Mannor,nirb@campus.technion.ac.il;shie@ee.technion.ac.il,3;6;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,3,0,yes,9/25/19,Technion;Technion,26;26,412;412,,9/25/19,0,0,0,0,0,0,348;11682,24;418,9;50,28;1230,m;m
4550,ICLR,2020,Progressive Upsampling Audio Synthesis via Effective Adversarial Training,Youngwoo Cho;Minwook Chang;Gerard Jounghyun Kim;Jaegul Choo,cyw314@gmail.com;fromme0528@gmail.com;gjkim@korea.ac.kr;jchoo@korea.ac.kr,1;6;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Korea University;;Korea University;Korea University,323;-1;323;323,179;-1;179;179,5,9/25/19,0,0,0,0,0,0,12;0;10;2415,14;3;4;124,2;0;2;22,0;0;0;389,m;m
4551,ICLR,2020,A Quality-Diversity Controllable GAN for Text Generation,Xingyu Lou;Kaihe Xu;Zhongliang Li;Tian Xia;Shaojun Wang;Jing Xiao,louxingyu83064256@163.com;xukaihenupt@gmail.com;zlli0520@gmail.com;summerrainet2008@gmail.com;swang.usa@gmail.com;jing.xiaoj@gmail.com,1;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0,yes,9/25/19,Pingan (Shenzhen) Technology;;Google;;Wright State University;,-1;-1;-1;-1;481;-1,-1;-1;-1;-1;1397;-1,3;4;5,9/25/19,0,0,0,0,0,0,1;185;110;417;46;117,3;11;37;71;15;90,1;5;7;9;3;3,0;6;2;6;0;9,m;f
4552,ICLR,2020,Unsupervised-Learning of time-varying features,Henrik Høeg;Matthias Brix;Oswin Krause,lvt956@alumni.ku.dk;brixmatthias@gmail.com;oswin.krause@di.ku.dk,1;1;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,4,0,yes,9/25/19,University of Copenhagen;University of Copenhagen;University of Copenhagen,100;100;100,101;101;101,5,9/25/19,0,0,0,0,0,0,1;0;98,2;1;21,1;0;6,0;0;7,m;m
4553,ICLR,2020,Improving Visual Relation Detection using Depth Maps,Sahand Sharifzadeh;Sina Moayed Baharlou;Max Berrendorf;Rajat Koner;Volker Tresp,sharifzadeh@dbs.ifi.lmu.de;sina.baharlou@gmail.com;berrendorf@dbs.ifi.lmu.de;koner@dbs.ifi.lmu.de;volker.tresp@siemens.com,6;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,8,1,yes,9/25/19,Institut für Informatik;;Institut für Informatik;Institut für Informatik;Siemens Corporate Research,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,,5/2/19,3,2,0,0,0,0,32;13;9;-1;8302,9;3;13;-1;287,2;2;2;-1;45,1;0;0;0;807,m;m
4554,ICLR,2020,DeepAGREL: Biologically plausible deep learning via direct reinforcement,Isabella Pozzi;Sander M. Bohte;Pieter R. Roelfsema,pozzi@cwi.nl;s.m.bohte@cwi.nl;p.roelfsema@nin.knaw.nl,6;6;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,4,0,yes,9/25/19,Centrum voor Wiskunde en Informatica;Centrum voor Wiskunde en Informatica;,-1;-1;-1,-1;-1;-1,,9/25/19,0,0,0,0,0,0,10;2;10386,6;4;303,2;1;50,0;0;607,f;m
4555,ICLR,2020,Learn Interpretable Word Embeddings Efficiently with von Mises-Fisher Distribution,Minghong Yao;Liansheng Zhuang;Houqiang Li;Jian Yang;Shafei Wang,mhyao1@mail.ustc.edu.cn;lszhuang@ustc.edu.cn;lihq@ustc.edu.cn;nanwuyaoshi@163.com;rockingsandstorm@163.com,8;1;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,University of Science and Technology of China;University of Science and Technology of China;University of Science and Technology of China;163;163,481;481;481;-1;-1,80;80;80;-1;-1,3;1,9/25/19,0,0,0,0,0,0,12;571;6175;744;153,6;36;386;128;34,2;11;39;6;8,2;64;603;183;7,m;f
4556,ICLR,2020,Mirror Descent View For Neural Network Quantization,Thalaiyasingam Ajanthan;Kartik Gupta;Philip H. S. Torr;Richard Hartley;Puneet K. Dokania,thalaiyasingam.ajanthan@anu.edu.au;kartik.gupta@anu.edu.au;phst@robots.ox.ac.uk;richard.hartley@anu.edu.au;puneet@robots.ox.ac.uk,3;6;3;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Australian National University;Australian National University;University of Oxford;Australian National University;University of Oxford,108;108;50;108;50,50;50;1;50;1,,9/25/19,4,2,2,0,0,0,333;41;28788;395;45,25;19;356;71;9,7;4;84;11;4,77;1;3878;33;6,m;m
4557,ICLR,2020,Topological Autoencoders,Michael Moor;Max Horn;Bastian Rieck;Karsten Borgwardt,michael.moor@bsse.ethz.ch;max.horn@bsse.ethz.ch;bastian.rieck@bsse.ethz.ch;karsten.borgwardt@bsse.ethz.ch,6;8;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10;10,13;13;13;13,,6/3/19,2,1,0,0,0,0,47;60;231;12602,11;10;39;170,4;5;9;41,3;3;10;1829,m;m
4558,ICLR,2020,Utilizing Edge Features in Graph Neural Networks via Variational Information Maximization,Pengfei Chen;Weiwen Liu;Chang-Yu Hsieh;Guangyong Chen;Pheng Ann Heng,chenpf.cuhk@gmail.com;wwliu@cse.cuhk.edu.hk;kimhsieh@tencent.com;gycchen@tencent.com;pheng@cse.cuhk.edu.hk,3;6;8;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,The Chinese University of Hong Kong;The Chinese University of Hong Kong;Tencent AI Lab;Tencent AI Lab;The Chinese University of Hong Kong,59;59;-1;-1;59,35;35;-1;-1;35,10,6/13/19,6,2,2,0,0,0,1848;340;44;407;72,153;48;22;40;27,21;7;4;7;5,121;40;5;50;10,m;m
4559,ICLR,2020,Redundancy-Free Computation Graphs for Graph Neural Networks,Zhihao Jia;Sina Lin;Rex Ying;Jiaxuan You;Jure Leskovec;Alex Aiken.,zhihao@cs.stanford.edu;silin@microsoft.com;rexying@stanford.edu;jiaxuan@stanford.edu;jure@cs.stanford.edu;aiken@cs.stanford.edu,3;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,5,0,yes,9/25/19,Stanford University;Microsoft;Stanford University;Stanford University;Stanford University;Stanford University,4;-1;4;4;4;4,4;-1;4;4;4;4,10,6/9/19,0,0,0,0,0,0,771;40;1921;954;48502;2054,70;8;22;15;302;105,15;3;12;9;93;16,47;5;313;175;6080;155,m;m
4560,ICLR,2020,Long History Short-Term Memory for Long-Term Video Prediction,Wonmin Byeon;Jan Kautz,wonmin.byeon@gmail.com;jkautz@nvidia.com,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,17,0,yes,9/25/19,NVIDIA;NVIDIA,-1;-1,-1;-1,4,9/25/19,0,0,0,0,0,0,631;14134,26;302,8;58,50;1901,f;m
4561,ICLR,2020,How the Softmax Activation Hinders the Detection of Adversarial and Out-of-Distribution Examples in Neural Networks,Jonathan Aigrain;Marcin Detyniecki,jonathan.aigrain@axa.com;marcin.detyniecki@axa.com,1;1;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,3,0,yes,9/25/19,AXA;AXA,-1;-1,-1;-1,4,9/25/19,0,0,0,0,0,0,130;58,11;14,4;5,17;4,m;m
4562,ICLR,2020,Solving Packing Problems by Conditional Query Learning,Dongda Li;Changwei Ren;Zhaoquan Gu;Yuexuan Wang;Francis Lau,lidongda@gzhu.edu.cn;rcw@zju.edu.cn;zqgu@gzhu.edu.cn;amywang@zju.edu.cn;fcmlau@cs.hku.hk,1;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Tsinghua University;Zhejiang University;Tsinghua University;Zhejiang University;The University of Hong Kong,8;56;8;56;92,23;107;23;107;35,,9/25/19,0,0,0,0,0,0,1;19;326;1075;616,3;15;78;117;112,1;2;9;16;9,0;0;47;67;58,m;m
4563,ICLR,2020,Context Based Machine Translation With Recurrent Neural Network For English-Amharic Translation ,Yeabsira Asefa Ashengo;Rosa Tsegaye Aga;Surafel Lemma Abebe,yeabsira.asefa@aait.edu.et;rosatsegaye@gmail.com;surafel.lemma@aait.edu.et,1;1;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,Addis Ababa Institute of Technology;;Addis Ababa Institute of Technology,-1;-1;-1,-1;-1;-1,3,9/25/19,0,0,0,0,0,0,0;8;345,1;9;23,0;2;10,0;0;31,-1;-1
4564,ICLR,2020,"Long-term planning, short-term adjustments",Hamed Khorasgani;Chi Zhang;Chetan Gupta;Susumu Serita,hamed.khorasgani@hal.hitachi.com;chi.zhang@hal.hitachi.com;chetan.gupta@hal.hitachi.com;susumu.serita@hal.hitachi.com,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,4,0,yes,9/25/19,Hitachi America Ltd.;Hitachi America Ltd.;Hitachi America Ltd.;Computational Life Science Cluster,-1;-1;-1;-1,-1;-1;-1;-1,,9/25/19,0,0,0,0,0,0,4;5;772;19,4;11;79;9,2;1;15;3,0;0;59;0,m;m
4565,ICLR,2020,Boosting Network: Learn by Growing Filters and Layers via SplitLBI,Zuyuan Zhong;Chen Liu;Yanwei Fu;Yuan Yao,zyzhong19@fudan.edu.cn;corwinliu9669@gmail.com;yanweifu@fudan.edu.cn;yuany@ust.hk,3;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,4,0,yes,9/25/19,Fudan University;Fudan University;Fudan University;The Hong Kong University of Science and Technology,79;79;79;39,109;109;109;47,2,9/25/19,0,0,0,0,0,0,0;93;88;91,1;43;22;34,0;5;5;5,0;3;2;2,m;f
4566,ICLR,2020,MULTI-STAGE INFLUENCE FUNCTION,Hongge Chen;Si Si;Yang Li;Ciprian Chelba;Sanjiv Kumar;Duane Boning;Cho-Jui Hsieh,chenhg@mit.edu;sisidaisy@google.com;liyang@google.com;ciprianchelba@google.com;sanjivk@google.com;boning@mtl.mit.edu;chohsieh@cs.ucla.edu,6;3;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,"Massachusetts Institute of Technology;Google;Google;Google;Google;Massachusetts Institute of Technology;University of California, Los Angeles",2;-1;-1;-1;-1;2;20,5;-1;-1;-1;-1;5;17,3;2,9/25/19,1,0,0,0,0,0,519;299;360;3000;941;2898;12827,41;10;135;99;142;284;168,9;2;5;27;13;27;41,66;46;29;285;113;200;1746,m;m
4567,ICLR,2020,$\ell_1$ Adversarial Robustness Certificates: a Randomized Smoothing Approach,Jiaye Teng;Guang-He Lee;Yang Yuan,2016110299@live.sufe.edu.cn;guanghe@csail.mit.edu;yuanyang@tsinghua.edu.cn,6;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,2,4,0,yes,9/25/19,Shanghai University of Finance and Economics;Massachusetts Institute of Technology;Tsinghua University,266;2;8,1397;5;23,,9/25/19,4,2,1,0,0,0,4;103;248,2;18;28,1;6;5,0;15;53,m;m
4568,ICLR,2020,Scaling Up Neural Architecture Search with Big Single-Stage Models,Jiahui Yu;Pengchong Jin;Hanxiao Liu;Gabriel Bender;Pieter-Jan Kindermans;Mingxing Tan;Thomas Huang;Xiaodan Song;Quoc Le,jyu79@illinois.edu;pengchong@google.com;hanxiaol@google.com;gbender@google.com;pikinder@google.com;tanmingxing@google.com;t-huang1@illinois.edu;xiaodansong@google.com;qvl@google.com,3;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,3,0,yes,9/25/19,"University of Illinois, Urbana Champaign;Google;Google;Google;Google;Google;University of Illinois, Urbana Champaign;Google;Google",3;-1;-1;-1;-1;-1;3;-1;-1,48;-1;-1;-1;-1;-1;48;-1;-1,,9/25/19,4,0,2,0,0,0,90;131;2015;461;65;95;69381;3824;48901,23;11;35;25;7;13;1499;48;193,5;6;12;7;4;4;120;17;81,7;2;522;67;3;13;6372;307;6080,m;m
4569,ICLR,2020,CONTRIBUTION OF INTERNAL REFLECTION IN LANGUAGE EMERGENCE WITH AN UNDER-RESTRICTED SITUATION,Kense Todo;Masayuki Yamamura,k_todo@ali.c.titech.ac.jp;my@c.titech.ac.jp,3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,3,0,yes,9/25/19,Tokyo Institute of Technology;Tokyo Institute of Technology,172;172,299;299,,9/25/19,0,0,0,0,0,0,0;918,1;106,0;13,0;109,m;m
4570,ICLR,2020,MIM: Mutual Information Machine,Micha Livne;Kevin Swersky;David J. Fleet,mlivne@cs.toronto.edu;kswersky@google.com;leet@cs.toronto.edu,1;1;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,2,5,0,yes,9/25/19,"Department of Computer Science, University of Toronto;Google;Department of Computer Science, University of Toronto",18;-1;18,18;-1;18,5,9/25/19,1,0,0,1,0,0,82;5797;17701,10;52;218,3;23;58,9;885;1700,m;m
4571,ICLR,2020,Variable Complexity in the Univariate and Multivariate Structural Causal Model,Tomer Galanti;Ofir Nabati;Lior Wolf,tomerga2@post.tau.ac.il;ofirnabati@mail.tau.ac.il;wolf@fb.com,6;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,5,0,yes,9/25/19,Tel Aviv University;Tel Aviv University;Facebook,35;35;-1,188;188;-1,,9/25/19,0,0,0,0,0,0,72;7;458,19;3;73,5;1;11,1;2;54,m;m
4572,ICLR,2020,Copy That! Editing Sequences by Copying Spans,Sheena Panthaplackel;Miltiadis Allamanis;Marc Brockschmidt,spantha@cs.utexas.edu;miallama@microsoft.com;mabrocks@microsoft.com,6;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,6,0,yes,9/25/19,"University of Texas, Austin;Microsoft;Microsoft",22;-1;-1,38;-1;-1,3,9/25/19,0,0,0,0,0,0,1;1959;2529,3;42;61,1;18;22,0;203;332,f;m
4573,ICLR,2020,OmniNet: A unified architecture for multi-modal multi-task learning,Subhojeet Pramanik;Priyanka Agrawal;Aman Hussain,subhojeetpramanik@gmail.com;pagrawal.ml@gmail.com;email@amanhussain.com,6;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,6,0,yes,9/25/19,International Business Machines;;Amanhussain,-1;-1;-1,-1;-1;-1,3;8,7/17/19,3,1,0,0,0,1,13;34;35,5;21;11,2;3;3,1;6;2,m;m
4574,ICLR,2020,QGAN: Quantize Generative Adversarial Networks to Extreme low-bits,Peiqi Wang;Yu Ji;Xinfeng Xie;Yongqiang Lyu;Dongsheng Wang;Yuan Xie,wpq14@tsinghua.org.cn;jiy15@mails.tsinghua.edu.cn;xinfeng@ucsb.edu;luyq@tsinghua.edu.cn;wds@mail.tsinghua.edu.cn;yuanxie@ucsb.edu,3;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,SenseTime Group Limited;Tsinghua University;UC Santa Barbara;Tsinghua University;Tsinghua University;UC Santa Barbara,-1;8;38;8;8;38,-1;23;57;23;23;57,5;4,9/25/19,0,0,0,0,0,0,77;6;62;485;121;125,18;10;16;76;18;40,5;1;5;13;6;4,6;0;3;25;10;3,f;m
4575,ICLR,2020,Imagine That! Leveraging Emergent Affordances for Tool Synthesis in Reaching Tasks,Yizhe Wu;Sudhanshu Kasewa;Oliver Groth;Sasha Salter;Li Sun;Oiwi Parker Jones;Ingmar Posner,ywu@robots.ox.ac.uk;su@robots.ox.ac.uk;ogroth@robots.ox.ac.uk;sasha@robots.ox.ac.uk;kevin@robots.ox.ac.uk;oiwi.parkerjones@jesus.ox.ac.uk;ingmar@robots.ox.ac.uk,3;1;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0,yes,9/25/19,University of Oxford;University of Oxford;University of Oxford;University of Oxford;University of Oxford;University of Oxford;University of Oxford,50;50;50;50;50;50;50,1;1;1;1;1;1;1,5,9/25/19,1,1,1,0,0,0,123;94;1677;26;67;418;2530,32;5;13;13;47;17;102,5;2;4;2;4;4;28,3;8;275;2;1;22;161,m;m
4576,ICLR,2020,Enhancing Attention with Explicit Phrasal Alignments,Xuan-Phi Nguyen;Shafiq Joty;Thanh-Tung Nguyen,nxphi47@gmail.com;sjoty@salesforce.com;ng0155ng@e.ntu.edu.sg,6;3;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,0,0,yes,9/25/19,National Taiwan University;SalesForce.com;National Taiwan University,86;-1;86,120;-1;120,3,9/25/19,0,0,0,0,0,0,5;2015;10,6;131;6,2;24;1,2;203;0,m;m
4577,ICLR,2020,A GOODNESS OF FIT MEASURE FOR GENERATIVE NETWORKS,Lorenzo Luzi;Randall Balestriero;Richard Baraniuk,lorenzo.luzi.28@gmail.com;randallbalestriero@gmail.com;richb@rice.edu,3;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,4,0,yes,9/25/19,Rice University;Rice University;Rice University,84;84;84,105;105;105,5;4,9/25/19,0,0,0,0,0,0,41;115;30222,16;38;661,3;5;84,2;2;2757,m;m
4578,ICLR,2020,Learning Neural Causal Models from Unknown Interventions,Nan Rosemary Ke;Olexa Bilaniuk;Anirudh Goyal;Stephan Bauer;Hugol Larochelle;Chris Pal;Yoshua Bengio,rosemary.nan.ke@gmail.com;obilaniu@gmail.com;anirudhgoyal9119@gmail.com;stefan.a.bauer@gmail.com;hugolarochelle@google.com;chris.j.pal@gmail.com;yoshua.bengio@mila.quebec,6;8;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,13,0,yes,9/25/19,Polytechnique Montreal;University of Montreal;University of Montreal;;Google;Ecole Polytechnique de Montreal;University of Montreal,390;128;128;-1;-1;390;128,1397;85;85;-1;-1;1397;85,6;10,9/25/19,9,3,1,0,0,0,760;281;1137;1301;25332;777;208566,32;14;46;45;124;58;807,13;7;12;15;44;10;147,76;36;130;126;2884;71;24297,f;m
4579,ICLR,2020,SLM Lab: A Comprehensive Benchmark and Modular Software Framework for Reproducible Deep Reinforcement Learning,Wah Loon Keng;Laura Graesser;Milan Cvitkovic,kengzwl@gmail.com;lhgraesser@gmail.com;mcvitkov@caltech.edu,3;8;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Reject,0,3,0,yes,9/25/19,;;California Institute of Technology,-1;-1;143,-1;-1;2,,9/25/19,2,1,0,0,0,0,2;22;45,1;8;11,1;2;4,0;3;3,m;m
4580,ICLR,2020,ADA+: A GENERIC FRAMEWORK WITH MORE ADAPTIVE EXPLICIT ADJUSTMENT FOR LEARNING RATE,Yue Zhao;Xiangsheng Huang;Ludan Kou,oasis.random.time@gmail.com;xiangsheng.huang@ia.ac.cn;2015019051@mail.buct.edu.cn,3;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0,yes,9/25/19,"Institute of automation, Chinese academy of science, Chinese Academy of Sciences;Institute of automation, Chinese academy of science, Chinese Academy of Sciences;Tsinghua University",59;59;8,1397;1397;23,8,9/25/19,0,0,0,0,0,0,166;643;0,48;56;1,4;9;0,4;50;0,m;m
4581,ICLR,2020,A⋆MCTS: SEARCH WITH THEORETICAL GUARANTEE USING POLICY AND VALUE FUNCTIONS,Xian Wu;Yuandong Tian;Lexing Ying,xwu20@stanford.edu;yuandong@fb.com;lexing@stanford.edu,1;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Stanford University;Facebook;Stanford University,4;-1;4,4;-1;4,,9/25/19,0,0,0,0,0,0,49;2498;9,83;85;18,4;25;2,3;293;0,m;m
4582,ICLR,2020,Faster Neural Network Training with Data Echoing,Dami Choi;Alexandre Passos;Christopher J. Shallue;George E. Dahl,choidami@cs.toronto.edu;apassos@google.com;shallue@google.com;gdahl@google.com,6;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,"Department of Computer Science, University of Toronto;Google;Google;Google",18;-1;-1;-1,18;-1;-1;-1,1,7/12/19,3,1,0,0,0,0,169;87;301;18394,6;74;12;45,3;5;6;27,32;6;33;1360,f;m
4583,ICLR,2020,Sparse Weight Activation Training,Md Aamir Raihan;Tor M. Aamodt,araihan@ece.ubc.ca;aamodt@ece.ubc.ca,3;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,9,0,yes,9/25/19,University of British Columbia;University of British Columbia,35;35,34;34,10,9/25/19,1,0,0,0,0,0,12;4162,3;87,2;23,3;652,m;m
4584,ICLR,2020,BOOSTING ENCODER-DECODER CNN FOR INVERSE PROBLEMS,Eunju Cha;Jaeduck Jang;Junho Lee;Eunha Lee;Jong Chul Ye,eunju.cha@kaist.ac.kr;jduck.jang@samsung.com;jh0325.lee@samsung.com;eunhayo.lee@samsung.com;jong.ye@kaist.ac.kr,6;1;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Korea Advanced Institute of Science and Technology;Samsung;Samsung;Samsung;Korea Advanced Institute of Science and Technology,481;-1;-1;-1;481,110;-1;-1;-1;110,,9/25/19,0,0,0,0,0,0,202;46;85;227;6369,24;5;23;45;331,6;2;4;8;41,6;4;3;4;447,f;m
4585,ICLR,2020,Unsupervised Spatiotemporal Data Inpainting,Yuan Yin;Arthur Pajot;Emmanuel de Bézenac;Patrick Gallinari,yuan.yin@lip6.fr;arthur.pajot@lip6.fr;emmanuel.de-bezenac@lip6.fr;patrick.gallinari@lip6.fr,3;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,LIP6;LIP6;LIP6;LIP6,-1;-1;-1;-1,-1;-1;-1;-1,5;4,9/25/19,0,0,0,0,0,0,141;86;90;75,78;9;9;24,6;3;3;5,6;10;10;6,m;m
4586,ICLR,2020,Deep geometric matrix completion:  Are we doing it right?,Amit Boyarski;Sanketh Vedula;Alex Bronstein,amitboy@cs.technion.ac.il;sanketh@cs.technion.ac.il;bron@cs.technion.ac.il,3;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,1,6,0,yes,9/25/19,Technion;Technion;Technion,26;26;26,412;412;412,10,9/25/19,0,0,0,0,0,0,52;73;100,5;11;29,3;6;7,9;2;8,m;m
4587,ICLR,2020,Predictive Coding for Boosting Deep Reinforcement Learning with Sparse Rewards,Xingyu Lu;Pieter Abbeel;Stas Tiomkin,xingyulu0701@berkeley.edu;pabbeel@cs.berkeley.edu;stas@berkeley.edu,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,3,0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,13;13;13,,9/25/19,1,1,0,0,0,0,37;37294;78,21;438;14,4;94;4,4;4481;3,m;m
4588,ICLR,2020,Improved Structural Discovery and Representation Learning of Multi-Agent Data,Jennifer Hobbs;Matthew Holbrook;Nathan Frank;Long Sha;Patrick Lucey,jennifer.hobbs@statsperform.com;matthewholbrook@statsperform.com;nathan.frank@statsperform.com;long.sha@statsperform.com;patrick.lucey@statsperform.com,1;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0,yes,9/25/19,Stats Perform;Stats Perform;Stats Perform;Stats Perform;Stats Perform,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,,9/25/19,0,0,0,0,0,0,66;24;83;318;4232,25;19;15;32;104,3;3;2;10;27,4;0;3;18;572,f;m
4589,ICLR,2020,Laplacian Denoising Autoencoder,Jianbo Jiao;Linchao Bao;Yunchao Wei;Shengfeng He;Honghui Shi;Rynson Lau;Thomas Huang,jiaojianbo.i@gmail.com;linchaobao@gmail.com;wychao1987@gmail.com;shengfenghe7@gmail.com;shihonghui3@gmail.com;rynson.lau@cityu.edu.hk;t-huang1@illinois.edu,6;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,"University of Oxford;Tencent AI Lab;University of Technology Sydney;South China University of Technology;University of Oregon;City University of Hong Kong;University of Illinois, Urbana Champaign",50;-1;108;481;205;92;3,1;-1;193;501;288;35;48,,9/25/19,0,0,0,0,0,0,320;793;3490;1135;1101;107;1088,31;27;104;61;46;14;33,9;13;29;14;13;2;4,54;141;401;144;125;25;99,m;m
4590,ICLR,2020,A Boolean Task Algebra for Reinforcement Learning,Geraud Nangue Tasse;Steven James;Benjamin Rosman,nanguetasse2000s@gmail.com;steven.james@wits.ac.za;brosman@csir.co.za,8;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,10,0,yes,9/25/19,University of the Witwatersrand;University of the Witwatersrand;CSIR,481;481;233,193;193;-1,1,9/25/19,1,1,0,0,0,0,1;206;454,1;62;78,1;6;12,0;9;21,m;m
4591,ICLR,2020,"On Iterative Neural Network Pruning, Reinitialization, and the Similarity of Masks",Michela Paganini;Jessica Forde,michela@fb.com;jzf2101@columbia.edu,1;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,4,0,yes,9/25/19,Facebook;Columbia University,-1;15,-1;16,,9/25/19,0,0,0,0,0,0,3735;78,231;12,33;3,175;5,f;f
4592,ICLR,2020,Semantics Preserving Adversarial Attacks,Ousmane Amadou Dia;Elnaz Barshan;Reza Babanezhad,ousmane@elementai.com;elnaz.barshan@elementai.com;babanezhad@gmail.com,1;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Reject,0,15,0,yes,9/25/19,Element AI;Element AI;Samsung,-1;-1;-1,-1;-1;-1,5;4,9/25/19,1,1,0,0,0,1,5;202;218,8;13;15,1;5;5,1;34;30,m;m
4593,ICLR,2020,Unified recurrent network for many feature types,Alexander Stec;Diego Klabjan;Jean Utke,stec@u.northwestern.edu;d-klabjan@northwestern.edu;jutke@allstate.com,1;1;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,0,0,yes,9/25/19,Northwestern University;Northwestern University;Allstate,44;44;-1,22;22;-1,,9/24/18,2,0,0,0,2,0,12;2780;1067,7;219;67,1;27;14,1;200;76,m;m
4594,ICLR,2020,How noise affects the Hessian spectrum in overparameterized neural networks,Mingwei Wei;David Schwab,m.wei@u.northwestern.edu;dschwab@gc.cuny.edu,6;3;6,I have published in this field for several years.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,Northwestern University;The City College of New York,44;205,22;1397,8,9/25/19,3,1,1,0,0,0,135;37,14;30,7;2,5;2,m;m
4595,ICLR,2020,Symmetric-APL Activations: Training Insights and Robustness to Adversarial Attacks,Mohammadamin Tavakoli;Forest Agostinelli;Pierre Baldi,mohamadt@uci.edu;fagostin@uci.edu;pfbaldi@ics.uci.edu,6;1;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,"University of California, Irvine;University of California, Irvine;University of California, Irvine",35;35;35,96;96;96,4,9/25/19,0,0,0,0,0,0,31;306;146,6;13;31,1;6;8,1;33;7,m;m
4596,ICLR,2020,Temporal Probabilistic Asymmetric Multi-task Learning,Nguyen Anh Tuan;Hyewon Jeong;Eunho Yang;Sungju Hwang,nanhtuan@kaist.ac.kr;jhw162@kaist.ac.kr;eunhoy@kaist.ac.kr;sjhwang82@kaist.ac.kr,6;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,481;481;481;481,110;110;110;110,10,9/25/19,0,0,0,0,0,0,260;125;1067;1240,57;32;75;77,8;6;16;16,19;9;169;146,m;m
4597,ICLR,2020,Learning Deep-Latent Hierarchies by Stacking Wasserstein Autoencoders,Benoit Gaujac;Ilya Feige;David Barber,benoit.gaujac.16@ucl.ac.uk;ilya@faculty.ai;david.barber@ucl.ac.uk,1;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Reject,0,3,0,yes,9/25/19,University College London;;University College London,50;-1;50,15;-1;15,5,9/25/19,0,0,0,0,0,0,5;205;4636,3;20;401,1;5;34,1;8;379,m;m
4598,ICLR,2020,Feature Selection using Stochastic Gates,Yutaro Yamada;Ofir Lindenbaum;Sahand Negahban;Yuval Kluger,yutaro.yamada@yale.edu;ofirlin@gmail.com;sahand.negahban@yale.edu;yuval.kluger@yale.edu,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,3,0,yes,9/25/19,Yale University;Yale University;Yale University;Yale University,64;64;64;64,8;8;8;8,,9/25/19,4,1,3,0,0,0,263;122;3146;7367,22;30;45;170,5;7;19;45,12;5;470;513,m;m
4599,ICLR,2020,Stablizing Adversarial Invariance Induction by Discriminator Matching,Yusuke Iwasawa;Kei Akuzawa;Yutaka Matsuo,iwasawa@weblab.t.u-tokyo.ac.jp;akuzawa-kei@weblab.t.u-tokyo.ac.jp;matsuo@weblab.t.u-tokyo.ac.jp,3;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,The University of Tokyo;The University of Tokyo;The University of Tokyo,56;56;56,36;36;36,4;1;7;8,9/25/19,0,0,0,0,0,0,121;53;7676,39;10;381,6;2;34,10;5;512,m;m
4600,ICLR,2020,Revisiting Gradient Episodic Memory for Continual Learning,Zhiyi Chen;Tong Lin*,chenzhiy16@mails.tsinghua.edu.cn;lintong@pku.edu.cn,1;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0,yes,9/25/19,Tsinghua University;Peking University,8;22,23;24,,9/25/19,0,0,0,0,0,0,84;1,39;10,6;1,2;0,m;f
4601,ICLR,2020,"Unifying Question Answering, Text Classification, and Regression via Span Extraction",Nitish Shirish Keskar;Bryan McCann;Caiming Xiong;Richard Socher,nkeskar@salesforce.com;bmccann@salesforce.com;cxiong@salesforce.com;rsocher@salesforce.com,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,3,0,yes,9/25/19,SalesForce.com;SalesForce.com;SalesForce.com;SalesForce.com,-1;-1;-1;-1,-1;-1;-1;-1,,4/19/19,10,6,3,0,0,1,2198;958;6301;53531,28;73;156;180,13;10;31;49,336;135;1045;8917,m;m
4602,ICLR,2020,Reinforcement Learning with Probabilistically Complete Exploration,Philippe Morere;Tom Blau;Gilad Francis;Fabio Ramos,philippe.morere@sydney.edu.au;tom.blau@sydney.edu.au;gilad.francis@sydney.edu.au;fabio.ramos@sydney.edu.au,3;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,University of Sydney;University of Sydney;University of Sydney;University of Sydney,86;86;86;86,60;60;60;60,,9/25/19,1,0,0,0,0,0,86;3;29;238,27;3;12;28,5;1;3;7,7;0;1;12,m;m
4603,ICLR,2020,Global Momentum Compression for Sparse Communication in Distributed SGD,Shen-Yi Zhao;Yin-Peng Xie;Hao Gao;Wu-Jun Li,zhaosy@lamda.nju.edu.cn;xieyp@lamda.nju.edu.cn;gaoh@lamda.nju.edu.cn;liwujun@nju.edu.cn,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,0,0,yes,9/25/19,Zhejiang University;Zhejiang University;Zhejiang University;Zhejiang University,56;56;56;56,107;107;107;107,1;9;8,5/30/19,2,0,1,0,2,0,93;3;656;2932,15;3;83;92,4;1;14;25,13;0;33;562,m;m
4604,ICLR,2020,Quantifying uncertainty with GAN-based priors,Dhruv V. Patel;Assad A. Oberai,dhruvvpa@usc.edu;aoberai@usc.edu,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,9,0,yes,9/25/19,University of Southern California;University of Southern California,31;31,62;62,5;4;11;2,9/25/19,0,0,0,0,0,0,11;2325,29;206,2;24,1;107,m;m
4605,ICLR,2020,Analyzing Privacy Loss in Updates of Natural Language Models,Shruti Tople;Marc Brockschmidt;Boris Köpf;Olga Ohrimenko;Santiago Zanella-Béguelin,t-shtopl@microsoft.com;mabrocks@microsoft.com;boris.koepf@microsoft.com;oohrim@microsoft.com;santiago@microsoft.com,6;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,7,0,yes,9/25/19,Microsoft;Microsoft;Microsoft;Microsoft;Microsoft,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,3,9/25/19,0,0,0,0,0,0,390;2529;1692;1487;2258,33;61;56;61;47,9;22;20;16;21,44;332;150;134;163,f;m
4606,ICLR,2020,Layer Flexible Adaptive Computation Time for Recurrent Neural Networks,Lida Zhang;Diego Klabjan,lidazhang2018@u.northwestern.edu;d-klabjan@northwestern.edu,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0,yes,9/25/19,Northwestern University;Northwestern University,44;44,22;22,3,12/6/18,1,1,0,0,2,0,38;3062,14;219,3;29,2;195,f;m
4607,ICLR,2020,Gumbel-Matrix Routing for Flexible Multi-task Learning,Krzysztof Maziarz;Efi Kokiopoulou;Andrea Gesmundo;Luciano Sbaiz;Gabor Bartok;Jesse Berent,krzysztof.s.maziarz@gmail.com;kokiopou@google.com;agesmundo@google.com;sbaiz@google.com;bartok@google.com;jberent@google.com,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Jagiellonian University;Google;Google;Google;Google;Google,481;-1;-1;-1;-1;-1,610;-1;-1;-1;-1;-1,,9/25/19,1,0,0,0,0,0,445;26;277;866;44;413,8;9;26;55;4;20,2;3;8;14;2;8,33;3;31;61;1;30,m;m
4608,ICLR,2020,"OPTIMAL TRANSPORT, CYCLEGAN, AND PENALIZED LS FOR UNSUPERVISED LEARNING IN INVERSE PROBLEMS",Byeongsu Sim;Gyutaek Oh;Sungjun Lim;and Jong Chul Ye,byeongsu.s@kaist.ac.kr;okt0711@kaist.ac.kr;sungjunlim@gmail.com;jong.ye@kaist.ac.kr,6;6;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,9,0,yes,9/25/19,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;;Korea Advanced Institute of Science and Technology,481;481;-1;481,110;110;-1;110,8;4;1;2;5,9/25/19,7,4,4,1,0,0,7;5;28;6369,3;4;10;331,1;1;3;41,0;0;1;447,m;m
4609,ICLR,2020,Stabilizing DARTS with Amended Gradient Estimation on Architectural Parameters,Kaifeng Bi;Changping Hu;Lingxi Xie;Xin Chen;Longhui Wei;Qi Tian,bikaifeng@huawei.com;huchangping@huawei.com;198808xc@gmail.com;1410452@tongji.edu.cn;weilonghui1@huawei.com;tian.qi1@huawei.com,3;3;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,12,0,yes,9/25/19,Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.;Tsinghua University;Huawei Technologies Ltd.;Huawei Technologies Ltd.,-1;-1;-1;8;-1;-1,-1;-1;-1;23;-1;-1,,9/25/19,4,2,1,0,0,0,12;9;2206;60;531;19924,5;4;108;20;12;731,2;2;24;4;4;69,0;0;228;1;117;2099,m;m
4610,ICLR,2020,Why ADAM Beats SGD for Attention Models	,Jingzhao Zhang;Sai Praneeth Karimireddy;Andreas Veit;Seungyeon Kim;Sashank J Reddi;Sanjiv Kumar;Suvrit Sra,jzhzhang@mit.edu;sai.karimrieddy@epfl.ch;aveit@google.com;seungyeonk@google.com;sashank@google.com;sanjivk@google.com;suvrit@mit.edu,3;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,4,0,yes,9/25/19,Massachusetts Institute of Technology;Swiss Federal Institute of Technology Lausanne;Google;Google;Google;Google;Massachusetts Institute of Technology,2;481;-1;-1;-1;-1;2,5;38;-1;-1;-1;-1;5,,9/25/19,17,6,3,1,0,2,439;179;1731;405;2194;1014;392,41;12;28;43;53;229;35,7;6;17;8;20;14;9,17;25;212;64;418;61;27,-1;-1
4611,ICLR,2020,Multi-Sample Dropout for Accelerated Training and Better Generalization,Hiroshi Inoue,inouehrs@jp.ibm.com,1;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,3,0,yes,9/25/19,International Business Machines,-1,-1,8,5/23/19,7,2,1,1,0,0,888,216,14,62,m
4612,ICLR,2020,Prototype Recalls for Continual Learning,Mengmi Zhang;Tao Wang;Joo Hwee Lim;Jiashi Feng,mengmi@u.nus.edu;twangnh@gmail.com;joohwee@i2r.a-star.edu.sg;elefjia@nus.edu.sg,3;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:N/A:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,0,0,yes,9/25/19,National University of Singapore;National University of Singapore;A*STAR;National University of Singapore,16;16;-1;16,25;25;-1;25,6,5/23/19,4,1,1,0,5,0,80;3587;360;9533,20;465;55;332,4;28;9;52,8;177;25;1232,f;m
4613,ICLR,2020,Learning Surrogate Losses,Josif Grabocka;Randolf Scholz;Lars Schmidt-Thieme,josif@ismll.uni-hildesheim.de;rscholz@ismll.uni-hildesheim.de;schmidt-thieme@ismll.uni-hildesheim.de,8;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,10,0,yes,9/25/19,University of Hildesheim;University of Hildesheim;University of Hildesheim,390;390;390,1397;1397;1397,,5/24/19,7,5,0,0,0,0,393;8;9110,40;2;279,10;2;36,58;0;1377,m;m
4614,ICLR,2020,Recurrent Independent Mechanisms,Anirudh Goyal;Alex Lamb;Shagun Sodhani;Jordan Hoffmann;Sergey Levine;Yoshua Bengio;Bernhard Scholkopf,anirudhgoyal9119@gmail.com;alex6200@gmail.com;sshagunsodhani@gmail.com;jhoffmann@g.harvard.edu;svlevine@eecs.berkeley.edu;yoshua.bengio@mila.quebec;bs@tuebingen.mpg.de,6;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,13,0,yes,9/25/19,"University of Montreal;;University of Montreal;Harvard University;University of California Berkeley;University of Montreal;Max Planck Institute for Intelligent Systems, Max-Planck Institute",128;-1;128;39;5;128;-1,85;-1;85;7;13;85;-1,8,9/24/19,21,8,2,0,0,1,1137;2136;78;103;24893;208566;2164,46;22;23;23;310;807;56,12;8;5;7;74;147;13,130;203;7;8;3235;24297;257,m;m
4615,ICLR,2020,Anomaly Detection Based on Unsupervised Disentangled Representation Learning in Combination with Manifold Learning,Xiaoyan Li;Iluju Kiringa;Tet Yeap;Xiaodan Zhu;Yifeng Li,xli343@uottawa.ca;iluju.kiringa@uottawa.ca;tyeap@uottawa.ca;xiaodan.zhu@queensu.ca;yifeng.li@nrc-cnrc.gc.ca,3;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0,yes,9/25/19,University of Ottawa;University of Ottawa;University of Ottawa;Queens University;National Research Council Canada,266;266;266;266;-1,141;141;141;258;-1,5,9/25/19,1,0,0,0,0,0,2341;22;465;28;41,133;11;71;18;39,25;3;11;3;3,221;1;25;6;1,m;m
4616,ICLR,2020,Stochastic Mirror Descent on Overparameterized Nonlinear Models,Navid Azizan;Sahin Lale;Babak Hassibi,azizan@caltech.edu;alale@caltech.edu;hassibi@caltech.edu,3;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,California Institute of Technology;California Institute of Technology;California Institute of Technology,143;143;143,2;2;2,8,6/10/19,14,4,1,0,0,1,195;24;18322,23;7;468,8;3;60,10;2;1897,m;m
4617,ICLR,2020,On the implicit minimization of alternative loss functions when training deep networks,Alexandre Lemire Paquin;Brahim Chaib-draa;Philippe Giguère,alexandre.lemire-paquin.1@ulaval.ca;brahim.chaib-draa@ift.ulaval.ca;philippe.giguere@ift.ulaval.ca,1;3;3,I have read many papers in this area.:I did not assess the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,0,0,yes,9/25/19,Laval university;Laval university;Laval university,481;481;481,272;272;272,8,9/25/19,0,0,0,0,0,0,0;2832;1641,1;207;111,0;27;23,0;262;94,m;m
4618,ICLR,2020,UWGAN: UNDERWATER GAN FOR REAL-WORLD UNDERWATER COLOR RESTORATION AND DEHAZING,Nan Wang;Yabin Zhou;Fenglei Han;Lichao Wan;Haitao Zhu;Yaojing Zheng,nanwangmail@hrbeu.edu.cn;zyb0977@163.com;fenglei_han@hrbeu.edu.cn;wanlch1203@hrbeu.edu.cn;zhuhaitao_heu@163.com;yaojingzheng_heu@163.com,3;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,5,0,yes,9/25/19,Tsinghua University;163;Tsinghua University;Tsinghua University;163;163,8;-1;8;8;-1;-1,23;-1;23;23;-1;-1,5;4,9/25/19,0,0,0,0,0,0,8;10;131;178;106;0,21;8;46;29;68;1,2;2;6;5;5;0,1;1;4;15;6;0,f;f
4619,ICLR,2020,LEARNING  TO LEARN  WITH  BETTER  CONVERGENCE,Patrick H. Chen;Sashank Reddi;Sanjiv Kumar;Cho-Jui Hsieh,patrickchen@g.ucla.edu;sashank@google.com;sanjivk@google.com;chohsieh@cs.ucla.edu,3;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,"University of California, Los Angeles;Google;Google;University of California, Los Angeles",20;-1;-1;20,17;-1;-1;17,,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1;-1,-1;-1;-1;-1,-1;-1;-1;-1,0;0;0;0,m;m
4620,ICLR,2020,Minimally distorted Adversarial Examples with a Fast Adaptive Boundary Attack,Francesco Croce;Matthias Hein,francesco91.croce@gmail.com;matthias.hein@uni-tuebingen.de,6;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,University of Tuebingen;University of Tuebingen,154;154,91;91,4,7/3/19,14,5,4,0,0,2,229;524,22;53,9;8,19;43,m;m
4621,ICLR,2020,Octave Graph Convolutional Network,Heng Chang;Yu Rong;Somayeh Sojoudi;Junzhou Huang;Wenwu Zhu,changh17@mails.tsinghua.edu.cn;yu.rong@hotmail.com;sojoudi@berkeley.edu;jzhuang@uta.edu;wwzhu@tsinghua.edu.cn,6;3;3,I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,"Tsinghua University;Tencent AI Lab;University of California Berkeley;University of Texas, Arlington;Tsinghua University",8;-1;5;118;8,23;-1;13;708;23,1;10,9/25/19,0,0,0,0,0,0,124;105;63;342;409,40;37;19;49;36,6;5;5;8;9,8;10;6;42;37,m;m
4622,ICLR,2020,"Lift-the-flap: what, where and when for context reasoning",Mengmi Zhang;Claire Tseng;Karla Montejo;Joseph Kwon;Gabriel Kreiman,mengmi.zhang@childrens.harvard.edu;ctseng@college.harvard.edu;kmont057@fiu.edu;joseph.kwon@yale.edu;gabriel.kreiman@tch.harvard.edu,3;3;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,8,0,yes,9/25/19,"Harvard University;Harvard University;Indiana University, Bloomington;Yale University;Harvard University",39;39;73;64;39,7;7;134;8;7,1,9/25/19,1,0,0,0,0,0,80;181;5;6;2269,20;27;6;6;184,4;7;1;1;34,8;9;0;1;913,f;m
4623,ICLR,2020,The Dynamics of Signal Propagation in Gated Recurrent Neural Networks,Dar Gilboa;Bo Chang;Minmin Chen;Greg Yang;Samuel S. Schoenholz;Ed H. Chi;Jeffrey Pennington,dg2893@columbia.edu;bchang@stat.ubc.ca;minminc@google.com;gregyang@microsoft.com;schsam@google.com;edchi@google.com;jpennin@google.com,3;8;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I did not assess the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0,yes,9/25/19,Columbia University;University of British Columbia;Google;Microsoft;Google;Google;Google,15;35;-1;-1;-1;-1;-1,16;34;-1;-1;-1;-1;-1,8,9/25/19,0,0,0,0,0,0,65;6;1896;513;3133;10436;16550,8;9;44;24;70;210;51,4;1;18;10;21;48;20,7;1;293;55;388;914;2640,m;m
4624,ICLR,2020,Regulatory Focus: Promotion and Prevention Inclinations in Policy Search,Lanxin Lei;Zhizhong Li;Xiaoyang Li;Cong Qiu;Dahua Lin,leilansen@gmail.com;lizz@sensetime.com;lixiaoyang@nbu.edu.cn;qiucong@sensetime.com;dhlin@ie.cuhk.edu.hk,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,Tsinghua University;SenseTime Group Limited;Boston University;SenseTime Group Limited;The Chinese University of Hong Kong,8;-1;67;-1;59,23;-1;61;-1;35,,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,0;0;0;0;0,f;m
4625,ICLR,2020,WaveFlow: A Compact Flow-based Model for Raw Audio,Wei Ping;Kainan Peng;Kexin Zhao;Zhao Song,weiping.thu@gmail.com,8;6;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,3,3,1,yes,9/25/19,Baidu,-1,-1,5,9/25/19,3,0,3,1,0,2,1000;569;22;957,209;12;10;124,12;7;3;16,111;73;3;78,m;m
4626,ICLR,2020,Identifying Weights and Architectures of Unknown ReLU Networks,David Rolnick;Konrad P. Kording,drolnick@seas.upenn.edu;koerding@gmail.com,3;1;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,1,5,0,yes,9/25/19,University of Pennsylvania;University of Pennsylvania,19;19,11;11,,9/25/19,5,2,0,0,0,0,634;5210,37;110,10;30,41;341,m;m
4627,ICLR,2020,On PAC-Bayes Bounds for Deep Neural Networks using the Loss Curvature,Konstantinos Pitas,konstantinos.pitas@epfl.ch,1;3;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,13,0,yes,9/25/19,Swiss Federal Institute of Technology Lausanne,481,38,1;8,9/25/19,0,0,0,0,0,0,20,10,2,0,m;m
4628,ICLR,2020,RoBERTa: A Robustly Optimized BERT Pretraining Approach,Yinhan Liu;Myle Ott;Naman Goyal;Jingfei Du;Mandar Joshi;Danqi Chen;Omer Levy;Mike Lewis;Luke Zettlemoyer;Veselin Stoyanov,yinhanliu@fb.com;myleott@fb.com;namangoyal@instagram.com;jingfeidu@fb.com;mandar90@cs.washington.edu;danqic@cs.princeton.edu;omerlevy@gmail.com;mikelewis@fb.com;lsz@fb.com;ves@fb.com,6;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,4,0,yes,9/25/19,Facebook;Facebook;Instagram;Facebook;University of Washington;Princeton University;Facebook;Facebook;Facebook;Facebook,-1;-1;-1;-1;6;31;-1;-1;-1;-1,-1;-1;-1;-1;26;6;-1;-1;-1;-1,3,7/26/19,841,378,415,39,0,260,974;3841;1089;904;1463;5425;7667;4069;15157;3990,9;39;14;8;9;21;58;104;176;45,3;20;7;4;7;15;30;21;53;24,284;772;321;267;383;1089;1231;684;2580;671,f;m
4629,ICLR,2020,A Coordinate-Free Construction of Scalable Natural Gradient,Kevin Luk;Roger Grosse,kevin.kh.luk@gmail.com;rgrosse@cs.toronto.edu,3;3;3,I have published in this field for several years.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:N/A:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,"Borealis AI;Department of Computer Science, University of Toronto",-1;18,-1;18,,8/30/18,7,4,3,0,34,1,206;5791,36;51,10;28,11;815,m;m
4630,ICLR,2020,Evaluating and Calibrating Uncertainty Prediction in Regression Tasks,Dan Levi;Liran Gispan;Niv Giladi;Ethan Fetaya,danmlevi@gmail.com;liran.gispan@gm.com;giladiniv@gmail.com;ethanf@cs.toronto.edu,1;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,"General Motors;General Motors;Technion;Department of Computer Science, University of Toronto",-1;-1;26;18,-1;-1;412;18,2,5/28/19,12,5,5,0,0,2,1315;21;23;708,31;3;5;26,13;3;2;12,115;1;2;76,m;m
4631,ICLR,2020,Why Does the VQA Model Answer No?: Improving Reasoning through Visual and Linguistic Inference,Seungjun Jung;Junyoung Byun;Kyujin Shim;Changick Kim,seungjun45@kaist.ac.kr;bjyoung@kaist.ac.kr;kjshim1028@kaist.ac.kr;changick@kaist.ac.kr,6;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,3,0,yes,9/25/19,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,481;481;481;481,110;110;110;110,,9/25/19,0,0,0,0,0,0,6;1;0;2065,5;4;2;145,1;1;0;22,1;0;0;220,m;m
4632,ICLR,2020,Modeling Winner-Take-All Competition in Sparse Binary Projections,Wenye Li,wyli@cuhk.edu.cn,3;8;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,Tsinghua University,8,23,,7/27/19,1,1,0,0,0,0,214,40,8,13,m;m
4633,ICLR,2020,When Robustness Doesn’t Promote Robustness: Synthetic vs. Natural Distribution Shifts on ImageNet,Rohan Taori;Achal Dave;Vaishaal Shankar;Nicholas Carlini;Benjamin Recht;Ludwig Schmidt,rohantaori@berkeley.edu;achald@cs.cmu.edu;vaishaal@berkeley.edu;nicholas@carlini.com;brecht@berkeley.edu;ludwigschmidt2@gmail.com,3;6;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:N/A:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A,Reject,0,6,0,yes,9/25/19,University of California Berkeley;Carnegie Mellon University;University of California Berkeley;Carlini;University of California Berkeley;University of California Berkeley,5;1;5;-1;5;5,13;27;13;-1;13;13,4,9/25/19,2,0,0,0,0,1,42;88;463;999;97;61,4;11;24;19;14;18,2;4;8;8;4;3,7;6;44;159;4;8,m;m
4634,ICLR,2020,Perturbations are not Enough: Generating Adversarial Examples with Spatial Distortions,He Zhao;Trung Le;Paul Montague;Olivier De Vel;Tamas Abraham;Dinh Phung,ethanhezhao@gmail.com;trunglm@monash.edu;paul.montague@dst.defence.gov.au;olivier.devel@dst.defence.gov.au;tamas.abraham@dst.defence.gov.au;dinh.phung@monash.edu,3;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,1,3,0,yes,9/25/19,Monash University;Monash University;;;;Monash University,118;118;-1;-1;-1;118,75;75;-1;-1;-1;75,4,9/25/19,1,1,0,0,0,0,2182;20;312;1419;378;4744,240;7;33;87;17;301,24;2;8;16;7;31,89;0;18;105;25;413,m;m
4635,ICLR,2020,Adaptive Data Augmentation with Deep Parallel Generative Models,Boli Fang;Miao Jiang;Abhirag Nagpure;Jerry Shen,bfang@iu.edu;miajiang@iu.edu;anagpure@iu.edu;hashen@iu.edu,1;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,"Indiana University, Bloomington;Indiana University, Bloomington;Indiana University, Bloomington;Indiana University, Bloomington",73;73;73;73,134;134;134;134,5;2,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1;-1,-1;-1;-1;-1,-1;-1;-1;-1,0;0;0;0,m;m
4636,ICLR,2020,Visual Explanation for Deep Metric Learning,Sijie Zhu;Taojiannan Yang;Chen Chen,szhu3@uncc.edu;tyang30@uncc.edu;chen.chen@uncc.edu,6;3;8,I have published one or two papers in this area.:I carefully checked the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,"University of North Carolina, Charlotte;University of North Carolina, Charlotte;University of North Carolina, Charlotte",73;73;73,1397;1397;1397,,9/25/19,2,0,0,1,0,0,61;3;696,25;6;187,5;1;16,3;0;23,m;m
4637,ICLR,2020,Continual Learning via Neural Pruning,Siavash Golkar;Micheal Kagan;Kyunghyun Cho,siavash.golkar@gmail.com;makagan@slac.stanford.edu;kyunghyun.cho@nyu.edu,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,Flatiron Institute;Stanford University;New York University,-1;4;25,-1;4;29,,3/11/19,18,8,7,1,6,2,201;919;46450,19;103;272,8;16;52,5;65;6610,m;m
4638,ICLR,2020,Continual Learning using the SHDL Framework with Skewed Replay Distributions,Amarjot Singh;Jay McClelland,as2436@stanford.edu;jlmcc@stanford.edu,1;1;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,Stanford University;Stanford University,4;4,4;4,,9/25/19,0,0,0,0,0,0,362;104,57;9,12;3,12;2,m;m
4639,ICLR,2020,Goten: GPU-Outsourcing Trusted Execution of Neural Network Training and Prediction,Lucien K.L. Ng;Sherman S.M. Chow;Anna P.Y. Woo;Donald P. H. Wong;Yongjun Zhao,nkl018@ie.cuhk.edu.hk;smchow@ie.cuhk.edu.hk;woopuiyung@gmail.com;foreverjun.zhao@gmail.com,1;6;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0,yes,9/25/19,The Chinese University of Hong Kong;The Chinese University of Hong Kong;The Chinese University of Hong Kong;National Taiwan University,59;59;59;86,35;35;35;120,,9/25/19,0,0,0,0,0,0,10;4765;32;171;6,6;174;3;11;6,2;35;2;5;2,0;414;1;10;0,m;m
4640,ICLR,2020,One-way prototypical networks,Anna Kruspe,anna.kruspe@dlr.de,8;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,0,0,yes,9/25/19,German Aerospace Center (DLR),-1,-1,6,6/3/19,3,1,0,0,0,0,95,24,5,13,f
4641,ICLR,2020,FLUID FLOW MASS TRANSPORT FOR GENERATIVE NETWORKS,Jingrong Lin;Keegan Lensink;Eldad Haber,jlin@eoas.ubc.ca;klensink@eoas.ubc.ca;ehaber@eoas.ubc.ca,1;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,University of British Columbia;University of British Columbia;University of British Columbia,35;35;35,34;34;34,5;4,9/25/19,2,2,2,0,0,0,265;28;905,15;8;71,9;3;14,14;0;44,f;m
4642,ICLR,2020,Improving Model Compatibility of Generative Adversarial Networks by Boundary Calibration,Si-An Chen;Chun-Liang Li;Hsuan-Tien Lin,r05922089@csie.ntu.edu.tw;chunlial@cs.cmu.edu;htlin@csie.ntu.edu.tw,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,2,0,yes,9/25/19,National Taiwan University;Carnegie Mellon University;National Taiwan University,86;1;86,120;27;120,5;4,9/25/19,0,0,0,0,0,0,22;1278;3098,5;90;104,2;18;22,1;114;270,m;m
4643,ICLR,2020,Generative Imputation and Stochastic Prediction,Mohammad Kachuee;Kimmo Kärkkäinen;Orpaz Goldstein;Sajad Darabi;Majid Sarrafzadeh,mkachuee@ucla.edu;kimmo@cs.ucla.edu;orpgol@cs.ucla.edu;sajad.darabi@cs.ucla.edu;majid@cs.ucla.edu,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,9,0,yes,9/25/19,"University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles",20;20;20;20;20,17;17;17;17;17,,5/22/19,9,0,0,0,9,0,93;25;24;58;9209,17;10;11;13;637,4;3;3;4;46,9;4;1;8;624,m;m
4644,ICLR,2020,Denoising Improves Latent Space Geometry in Text Autoencoders,Tianxiao Shen;Jonas Mueller;Regina Barzilay;Tommi Jaakkola,tianxiao@mit.edu;jonasmue@amazon.com;regina@csail.mit.edu;tommi@csail.mit.edu,6;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,10,0,yes,9/25/19,Massachusetts Institute of Technology;Amazon;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;-1;2;2,5;-1;5;5,3;4;1,9/25/19,0,0,0,0,0,0,340;57;12076;22299,13;16;234;293,3;3;56;69,97;6;1215;2326,f;m
4645,ICLR,2020,Information-Theoretic Local Minima Characterization and Regularization,Zhiwei Jia;Hao Su,zjia@ucsd.edu;haosu@eng.ucsd.edu,1;8;3,I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,18,0,yes,9/25/19,"University of California, San Diego;University of California, San Diego",11;11,31;31,1;8,9/25/19,1,0,0,0,0,0,3;17303,5;48,1;15,0;2681,m;m
4646,ICLR,2020,Wider Networks Learn Better Features,Dar Gilboa;Guy Gur-Ari,dg2893@columbia.edu;guyga@google.com,3;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,0,0,yes,9/25/19,Columbia University;Google,15;-1,16;-1,,9/25/19,1,1,0,0,0,0,65;1158,8;21,4;14,7;105,m;m
4647,ICLR,2020,GENERALIZATION GUARANTEES FOR NEURAL NETS VIA HARNESSING THE LOW-RANKNESS OF JACOBIAN,Samet Oymak;Zalan Fabian;Mingchen Li;Mahdi Soltanolkotabi,sametoymak@gmail.com;zfabian@usc.edu;mli176@ucr.edu;msoltoon@gmail.com,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,"University of California, Riverside;University of Southern California;University of California, Riverside;University of Southern California",59;31;59;31,249;62;249;62,8,9/25/19,0,0,0,0,0,0,8;20;86;2866,11;8;16;56,2;2;5;20,0;1;4;351,m;m
4648,ICLR,2020,Implicit competitive regularization in GANs,Florian Schaefer;Hongkai Zheng;Anima Anandkumar,florian.schaefer@caltech.edu;devzhk@sjtu.edu.cn;anima@caltech.edu,6;6;8;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0,yes,9/25/19,California Institute of Technology;Shanghai Jiao Tong University;California Institute of Technology,143;53;143,2;157;2,5;4,9/25/19,2,0,0,0,0,0,339;1;5451,51;1;187,9;1;38,29;0;761,m;f
4649,ICLR,2020,TWO-STEP UNCERTAINTY NETWORK FOR TASKDRIVEN SENSOR PLACEMENT,Yangyang Sun;Yang Zhang;Hassan Foroosh;Shuo Pang,yangyang@knights.ucf.edu;yangzhang@knights.ucf.edu;foroosh@cs.ucf.edu;pang@creol.ucf.edu,1;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,University of Central Florida;University of Central Florida;University of Central Florida;University of Central Florida,77;77;77;77,609;609;609;609,5,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1;-1,-1;-1;-1;-1,-1;-1;-1;-1,0;0;0;0,m;m
4650,ICLR,2020,Efficient Content-Based Sparse Attention with Routing Transformers,Aurko Roy*;Mohammad Taghi Saffar*;David Grangier;Ashish Vaswani,aurkor@google.com;msaffar@google.com;grangier@google.com;avaswani@google.com,3;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,2,0,yes,9/25/19,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,3,9/25/19,10,7,2,0,0,2,1078;22;5166;11736,21;13;64;52,10;3;25;22,119;3;760;2710,m;m
4651,ICLR,2020,Implicit Rugosity Regularization via Data Augmentation,Daniel LeJeune;Randall Balestriero;Hamid Javadi;Richard G. Baraniuk,dlejeune@rice.edu;randallbalestriero@gmail.com;hh35@rice.edu;richb@rice.edu,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Rice University;Rice University;Rice University;Rice University,84;84;84;84,105;105;105;105,8,9/25/19,1,1,0,0,0,0,19;115;94;30222,12;38;34;661,3;5;6;84,2;2;10;2757,m;m
4652,ICLR,2020,Balancing Cost and Benefit with Tied-Multi Transformers,Raj Dabre;Raphael Rubino;Atsushi Fujita,raj.dabre@nict.go.jp;raphael.rubino@nict.go.jp;fujita@paraphrasing.org,1;6;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,3,0,yes,9/25/19,"National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology;National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology;",-1;-1;-1,-1;-1;-1,3,9/25/19,1,1,0,0,0,0,285;435;209,43;50;48,8;12;7,21;41;6,m;m
4653,ICLR,2020,Learning to Learn Kernels with Variational Random Features,Haoliang Sun;Yingjun Du;Jun Xu;Yilong Yin;Xiantong Zhen;Ling Shao,haolsun.cn@gmail.com;duyingjun@buaa.edu.cn;nankaimathxujun@gmail.com;ylyin@sdu.edu.cn;zhenxt@gmail.com;ling.shao@ieee.org,6;6;8,I have published one or two papers in this area.:I carefully checked the experiments.:N/A:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0,yes,9/25/19,Shandong University;Beihang University;Nankai University;Shandong University;Inception Institute of Artificial Intelligence;Inception Institute of Artificial Intelligence,154;118;481;154;-1;-1,658;594;366;658;-1;-1,6,9/25/19,1,0,1,0,0,0,54;3;428;3136;1438;489,15;4;43;262;90;97,3;1;5;28;20;10,3;0;128;174;78;45,m;m
4654,ICLR,2020,Learning Numeral Embedding,Chengyue Jiang;Zhonglin Nian;Kaihao Guo;Shanbo Chu;Yinggong Zhao;Libin Shen;Kewei Tu,jiangchy@shanghaitech.edu.cn;nianzhl@shanghaitech.edu.cn;guokh@shanghaitech.edu.cn;chushb@leyantech.com;ygzhao@leyantech.com;libin@leyantech.com;tukw@shanghaitech.edu.cn,3;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,ShanghaiTech University;ShanghaiTech University;ShanghaiTech University;Leyantech;Leyantech;Leyantech;ShanghaiTech University,481;481;481;-1;-1;-1;481,1397;1397;1397;-1;-1;-1;1397,3,9/25/19,0,0,0,0,0,0,12;0;0;7;293;1514;470,10;1;2;3;20;34;58,2;0;0;2;4;16;11,0;0;0;0;40;174;27,f;m
4655,ICLR,2020,Label Cleaning with Likelihood Ratio Test,Songzhu Zheng;Pengxiang Wu;Aman Goswami;Mayank Goswami;Dimitris Metaxas;Chao Chen,zheng.songzhu@stonybrook.edu;pxiangwu@gmail.com;ag77in@gmail.com;mayank.isi@gmail.com;dnm@cs.rutgers.edu;chao.chen.1@stonybrook.edu,8;3;3,I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,"State University of New York, Stony Brook;;;;Rutgers University;State University of New York, Stony Brook",41;-1;-1;-1;34;41,304;-1;-1;-1;168;304,11;1,9/25/19,0,0,0,0,0,0,0;0;8;25;21839;96,3;3;4;18;659;83,0;0;2;4;74;4,0;0;0;0;1713;5,m;m
4656,ICLR,2020,Fast Task Adaptation for Few-Shot Learning,Yingying Zhang;Qiaoyong Zhong;Di Xie;Shiliang Pu,zhangyingying7@hikvision.com;zhongqiaoyong@hikvision.com;xiedi@hikvision.com;pushiliang@hikvision.com,8;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,6,7,0,yes,9/25/19,Hikvision Research Institute;Hikvision Research Institute;Hikvision Research Institute;Hikvision Research Institute,-1;-1;-1;-1,-1;-1;-1;-1,6;8,9/25/19,0,0,0,0,0,0,93;201;2;827,49;12;3;78,5;6;1;13,4;39;0;142,m;m
4657,ICLR,2020,Using Objective Bayesian Methods to Determine the Optimal Degree of Curvature within the Loss Landscape,Devon Jarvis;Richard Klein;Benjamin Rosman,devonjarvi@gmail.com;kleinric@gmail.com;benjros@gmail.com,1;6;1,I have published in this field for several years.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0,yes,9/25/19,University of the Witwatersrand;;University of the Witwatersrand,481;-1;481,193;-1;193,11,9/25/19,0,0,0,0,0,0,3;503;454,20;176;78,1;10;12,0;25;21,m;m
4658,ICLR,2020,VIDEO AFFECTIVE IMPACT PREDICTION WITH MULTIMODAL FUSION AND LONG-SHORT TEMPORAL CONTEXT,Yin Zhao;Longjun Cai;Chaoping Tu;Jie Zhang;Wu Wei,yinzhao.zy@alibaba-inc.com;longjun.clj@alibaba-inc.com;chaoping.tcp@alibaba-inc.com;auzj_alex@mail.scut.edu.cn;weiwu@scut.edu.cn,1;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:N/A:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0,yes,9/25/19,Alibaba Group;Alibaba Group;Alibaba Group;South China University of Technology;South China University of Technology,-1;-1;-1;481;481,-1;-1;-1;501;501,,9/25/19,0,0,0,0,0,0,80;6;14;41;33,30;8;4;36;81,3;2;1;3;3,3;0;1;4;1,f;m
4659,ICLR,2020,UNITER: Learning UNiversal Image-TExt Representations,Yen-Chun Chen;Linjie Li;Licheng Yu;Ahmed El Kholy;Faisal Ahmed;Zhe Gan;Yu Cheng;Jingjing Liu,yen-chun.chen@microsoft.com;lindsey.li@microsoft.com;licheng.yu@microsoft.com;ahmed.elkholy@microsoft.com;fiahmed@microsoft.com;zhe.gan@microsoft.com;yu.cheng@microsoft.com;jingjl@microsoft.com,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,12,0,yes,9/25/19,Microsoft;Microsoft;Microsoft;Microsoft;Microsoft;Microsoft;Microsoft;Microsoft,-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1,3,9/25/19,59,34,25,3,0,12,294;112;961;694;149;76;169;279,11;8;30;35;17;6;31;41,6;3;13;10;4;3;6;6,48;19;223;87;17;15;29;28,m;f
4660,ICLR,2020,"Fast Linear Interpolation for Piecewise-Linear Functions, GAMs, and Deep Lattice Networks",Nathan Zhang;Kevin Canini;Sean Silva;and Maya R. Gupta,nzhang32@gmail.com;canini@google.com;silvasean@google.com;mayagupta@google.com,3;3;1,I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,3,0,yes,9/25/19,Stanford University;Google;Google;Google,4;-1;-1;-1,4;-1;-1;-1,,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1;-1,-1;-1;-1;-1,-1;-1;-1;-1,0;0;0;0,m;f
4661,ICLR,2020,Knowledge Graph Embedding: A Probabilistic Perspective and Generalization Bounds,Ondrej Kuzelka;Yuyi Wang,kuzelo1@gmail.com;yuyiwang920@gmail.com,6;1;3,I do not know much about this area.:N/A:N/A:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,11,0,yes,9/25/19,Czech Technical University in Prague;Swiss Federal Institute of Technology,323;10,956;13,10;1;8,9/25/19,0,0,0,0,0,0,284;310,79;61,10;12,15;12,m;m
4662,ICLR,2020,BUZz: BUffer Zones for defending  adversarial examples in image classification,Phuong Ha Nguyen*;Kaleel Mahmood*;Lam M. Nguyen;Thanh Nguyen;Marten van Dijk,phuongha.ntu@gmail.com;kaleel.mahmood@uconn.edu;lamnguyen.mltd@gmail.com;thanhng@iastate.edu;marten.van_dijk@uconn.edu,1;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,5,0,yes,9/25/19,University of Connecticut;University of Connecticut;International Business Machines;Iowa State University;University of Connecticut,154;154;-1;172;154,393;393;-1;399;393,4,9/25/19,1,1,1,0,0,0,502;95;451;77;8435,58;13;27;15;185,13;5;9;5;43,30;10;69;5;996,m;m
4663,ICLR,2020,Domain Aggregation Networks for Multi-Source Domain Adaptation,Junfeng Wen;Russell Greiner;Dale Schuurmans,junfengwen@gmail.com;rgreiner@ualberta.ca;daes@ualberta.ca,6;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,6,0,yes,9/25/19,University of Alberta;University of Alberta;University of Alberta,100;100;100,136;136;136,8,9/11/19,1,1,0,0,0,0,160;10796;193,16;330;37,7;41;8,17;827;27,m;m
4664,ICLR,2020,Privacy-preserving Representation Learning by Disentanglement,Tassilo Klein;Moin Nabi,tassilo.klein@sap.com;m.nabi@sap.com,1;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,1,0,0,yes,9/25/19,SAP;SAP,323;323,258;258,,9/25/19,0,0,0,0,0,0,612;3,43;4,10;1,66;2,m;m
4665,ICLR,2020,Angular Visual Hardness,Beidi Chen;Weiyang Liu;Animesh Garg;Zhiding Yu;Anshumali Shrivastava;Jan Kautz;Anima Anandkumar,beidi.chen@rice.edu;wyliu@gatech.edu;garg@cs.stanford.edu;zhidingy@nvidia.com;anshumali@rice.edu;jkautz@nvidia.com;anima@caltech.edu,1;8;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0,yes,9/25/19,Rice University;Georgia Institute of Technology;Stanford University;NVIDIA;Rice University;NVIDIA;California Institute of Technology,84;13;4;-1;84;-1;143,105;38;4;-1;105;-1;2,,5/17/19,3,2,0,0,0,0,96;2060;1040;2476;1135;14819;5451,16;62;79;61;101;304;187,5;15;20;15;16;60;38,4;315;66;338;111;1959;761,f;f
4666,ICLR,2020,Autoencoders and Generative Adversarial Networks for Imbalanced Sequence Classification,Stephanie Ger;Diego Klabjan,stephanieger@u.northwestern.edu;d-klabjan@northwestern.edu,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Northwestern University;Northwestern University,44;44,22;22,5;4,1/8/19,0,0,0,0,0,0,3;2780,5;219,1;27,0;200,f;m
4667,ICLR,2020,Multiagent Reinforcement Learning in Games with an Iterated Dominance Solution,Yoram Bachrach;Tor Lattimore;Marta Garnelo;Julien Perolat;David Balduzzi;Thomas Anthony;Satinder Singh;Thore Graepel,yorambac@gmail.com;lattimore@google.com;garnelo@google.com;perolat@google.com;dbalduzzi@google.com;twa@google.com;baveja@google.com;thore@google.com,1;6;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,8,0,yes,9/25/19,Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1,1,9/25/19,0,0,0,0,0,0,2786;842;952;694;2521;137;88;19274,137;67;23;38;61;84;28;161,30;17;11;16;21;6;6;45,198;94;116;44;359;4;8;1406,m;m
4668,ICLR,2020,Blockwise Adaptivity:  Faster Training and Better Generalization in Deep Learning,Shuai Zheng;James T. Kwok,zs910504@gmail.com;jamesk@cse.ust.hk,3;6;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Amazon;The Hong Kong University of Science and Technology,-1;39,-1;47,9;8,5/23/19,0,0,0,0,0,0,2971;9752,31;197,15;51,334;1062,m;m
4669,ICLR,2020,Self-Educated Language Agent with Hindsight Experience Replay for Instruction Following,Geoffrey Cideron;Mathieu Seurin;Florian Strub;Olivier Pietquin,geoffrey.cideron@inria.fr;mathieu.seurin@inria.fr;fstrub@google.com;pietquin@google.com,6;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,8,0,yes,9/25/19,INRIA;INRIA;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,8,9/25/19,4,1,1,0,0,0,4;41;1184;3468,2;6;23;193,1;3;13;30,0;3;156;298,m;m
4670,ICLR,2020,Policy path programming,Daniel McNamee,daniel.c.mcnamee@gmail.com,3;3;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,1,0,yes,9/25/19,University College London,50,15,8,9/25/19,0,0,0,0,0,0,332,24,8,18,m;m
4671,ICLR,2020,Extreme Triplet Learning: Effectively Optimizing Easy Positives and Hard Negatives,Hong Xuan;Robert Pless,xuanhong@gwu.edu;pless@gwu.edu,3;8;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,2,0,yes,9/25/19,George Washington University;George Washington University,205;205,198;198,,9/25/19,0,0,0,0,0,0,45;5225,14;191,2;32,4;457,m;m
4672,ICLR,2020,MONET: Debiasing Graph Embeddings via the Metadata-Orthogonal Training Unit,John Palowitch;Bryan Perozzi,johnpalowitch@gmail.com;bperozzi@acm.org,6;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,;,-1;-1,-1;-1,7;10,9/25/19,2,1,0,0,0,0,147;4580,14;51,5;19,7;1118,m;m
4673,ICLR,2020,DYNAMIC SELF-TRAINING FRAMEWORK  FOR GRAPH CONVOLUTIONAL NETWORKS,Ziang Zhou;Shenzhong Zhang;Zengfeng Huang,15300180085@fudan.edu.cn;17210980007@fudan.edu.cn;huangzf@fudan.edu.cn,3;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Fudan University;Fudan University;Fudan University,79;79;79,109;109;109,10,9/25/19,1,0,0,0,0,0,2;2;375,7;2;37,1;1;11,0;0;33,m;m
4674,ICLR,2020,TOWARDS FEATURE SPACE ADVERSARIAL ATTACK,Qiuling Xu;Guanhong Tao;Siyuan Cheng;Lin Tan;Xiangyu Zhang,xu1230@purdue.edu;taog@purdue.edu;516030910472@sjtu.edu.cn;lintan@purdue.edu;xyzhang@cs.purdue.edu,6;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,3,0,yes,9/25/19,Purdue University;Purdue University;Shanghai Jiao Tong University;Purdue University;Purdue University,27;27;53;27;27,88;88;157;88;88,4,9/25/19,1,1,1,0,0,0,15;40;28;134;64794,5;7;19;28;327,2;2;2;6;44,1;1;0;8;11989,m;m
4675,ICLR,2020,DIVA: Domain Invariant Variational Autoencoder,Maximilian Ilse;Jakub M. Tomczak;Christos Louizos;Max Welling,ilse.maximilian@gmail.com;jakubmkt@gmail.com;chr.louizos@gmail.com;welling.max@gmail.com,6;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0,yes,9/25/19,"University of Amsterdam;VU University Toronto;Qualcomm Inc, QualComm;University of California - Irvine",172;18;-1;35,62;18;-1;96,5;8,3/27/19,14,5,6,0,0,3,202;1268;1273;28145,9;83;23;329,4;15;12;60,45;153;239;5302,m;m
4676,ICLR,2020,Optimal Attacks on Reinforcement Learning Policies,Alessio Russo;Alexandre Proutiere,alessior@kth.se;alepro@kth.se,3;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,"KTH Royal Institute of Technology, Stockholm, Sweden;KTH Royal Institute of Technology, Stockholm, Sweden",128;128,222;222,4,7/31/19,2,0,2,0,0,0,6;4316,17;169,2;39,0;405,m;m
4677,ICLR,2020,Recurrent Event Network : Global Structure Inference Over Temporal Knowledge Graph,Woojeong Jin;He Jiang;Meng Qu;Tong Chen;Changlin Zhang;Pedro Szekely;Xiang Ren,woojeong.jin@usc.edu;jian567@usc.edu;meng.qu@umontreal.ca;tongc2@andrew.cmu.edu;changlin.zhang@usc.edu;pszekely@isi.edu;xiangren@usc.edu,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,9,0,yes,9/25/19,University of Southern California;University of Southern California;University of Montreal;Carnegie Mellon University;University of Southern California;USC/ISI;University of Southern California,31;31;128;1;31;-1;31,62;62;85;27;62;-1;62,10,9/25/19,4,3,2,1,0,2,93;324;504;797;9;130;1380,11;60;40;45;5;29;121,5;9;5;14;2;6;20,12;24;100;77;2;15;57,m;m
4678,ICLR,2020,Continual Learning with Gated Incremental Memories for Sequential Data Processing,Andrea Cossu;Antonio Carta;Davide Bacciu,cossu48@gmail.com;antonio.carta@di.unipi.it;bacciu@di.unipi.it,3;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,3,0,yes,9/25/19,Scuola Normale Superiore;University of Pisa;University of Pisa,481;233;233,152;366;366,,9/25/19,1,1,1,0,0,1,361;40;651,153;30;113,10;4;13,17;3;26,m;m
4679,ICLR,2020,Context-Aware Object Detection With Convolutional Neural Networks,Yizhou Yan;Lei Cao;Samuel Madden;Elke Rundensteiner,yyan2@wpi.edu;lcao@csail.mit.edu;madden@csail.mit.edu;rundenst@cs.wpi.edu,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Worcester Polytechnic Institute;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Worcester Polytechnic Institute,172;2;2;172,628;5;5;628,2,9/25/19,0,0,0,0,0,0,148;7;34124;7902,37;30;367;596,6;2;80;44,9;0;3710;515,f;f
4680,ICLR,2020,Resizable Neural Networks,Yichen Zhu;Xiangyu Zhang;Tong Yang;Jian Sun,k.zhu@mail.utoronto.ca;zhangxiangyu@megvii.com;yangtong@megvii.com;sunjian@megvii.com,3;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Toronto University;Megvii Technology Inc.;Megvii Technology Inc.;Megvii Technology Inc.,18;-1;-1;-1,18;-1;-1;-1,,9/25/19,0,0,0,0,0,0,26;64794;175;3762,11;327;59;223,3;44;8;27,5;11989;5;305,f;m
4681,ICLR,2020,SPROUT: Self-Progressing Robust Training,Minhao Cheng;Pin-Yu Chen;Sijia Liu;Shiyu Chang;Cho-Jui Hsieh;Payel Das,mhcheng@ucla.edu;pin-yu.chen@ibm.com;sijia.liu@ibm.com;shiyu.chang@ibm.com;chohsieh@cs.ucla.edu;daspa@us.ibm.com,3;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,"University of California, Los Angeles;International Business Machines;International Business Machines;International Business Machines;University of California, Los Angeles;International Business Machines",20;-1;-1;-1;20;-1,17;-1;-1;-1;17;-1,4,9/25/19,0,0,0,0,0,0,305;194;298;2991;12827;545,20;44;52;111;168;67,6;6;11;28;41;11,42;22;24;397;1746;19,m;f
4682,ICLR,2020,FALCON: Fast and Lightweight Convolution for Compressing and Accelerating CNN,Chun Quan;Jun-Gi Jang;Hyun Dong Lee;U Kang,chunquan_cs@outlook.com;elnino4@snu.ac.kr;hyundonglee1015@gmail.com;ukang@snu.ac.kr,6;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Seoul National University;Seoul National University;;Seoul National University,41;41;-1;41,64;64;-1;64,,9/25/19,1,0,0,0,0,0,54;13;14;5,9;10;11;7,2;2;2;1,2;0;0;0,m;m
4683,ICLR,2020,Conditional Invertible Neural Networks for Guided Image Generation,Lynton Ardizzone;Carsten Lüth;Jakob Kruse;Carsten Rother;Ullrich Köthe,lynton.ardizzone@iwr.uni-heidelberg.de;clueth@live.de;jakob.kruse@iwr.uni-heidelberg.de;carsten.rother@iwr.uni-heidelberg.de;ullrich.koethe@iwr.uni-heidelberg.de,6;6;3;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:N/A:N/A,Reject,0,4,0,yes,9/25/19,Heidelberg University;;Heidelberg University;Heidelberg University;Heidelberg University,205;-1;205;205;205,44;-1;44;44;44,5,7/4/19,18,11,7,0,0,6,112;13;123;21481;2539,14;4;12;207;98,4;1;4;67;23,18;5;22;3025;214,m;m
4684,ICLR,2020,Domain-Independent Dominance of Adaptive Methods,Pedro Savarese;David McAllester;Sudarshan Babu;Michael Maire,savarese@ttic.edu;mcallester@ttic.edu;sudarshan@ttic.edu;mmaire@uchicago.edu,3;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0,yes,9/25/19,Toyota Technological Institute at Chicago;Toyota Technological Institute at Chicago;Toyota Technological Institute at Chicago;University of Chicago,-1;-1;-1;48,-1;-1;-1;9,3;8,9/25/19,2,0,0,0,0,0,30;21269;61;16144,6;173;13;62,2;54;2;25,1;3236;4;2845,m;m
4685,ICLR,2020,RISE and DISE: Two Frameworks for Learning from Time Series with Missing Data,Alberto Garcia-Duran;Robert West,alberto.duran@epfl.ch;robert.west@epfl.ch,3;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,3,0,yes,9/25/19,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne,481;481,38;38,,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1,-1;-1,-1;-1,0;0,m;m
4686,ICLR,2020,Potential Flow Generator with $L_2$ Optimal Transport Regularity for Generative Models,Liu Yang;George Em Karniadakis,liu_yang@brown.edu;george_karniadakis@brown.edu,3;3;8,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,7,0,yes,9/25/19,Brown University;Brown University,67;67,53;53,5,8/29/19,5,3,2,0,0,2,1901;24138,202;808,14;72,85;1624,m;m
4687,ICLR,2020,Power up! Robust Graph Convolutional Network based on Graph Powering,Ming Jin;Heng Chang;Wenwu Zhu;Somayeh Sojoudi,jinming@berkeley.edu;changh@berkeley.edu;wwzhu@tsinghua.edu.cn;sojoudi@berkeley.edu,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,7,0,yes,9/25/19,University of California Berkeley;University of California Berkeley;Tsinghua University;University of California Berkeley,5;5;8;5,13;13;23;13,4;10,5/24/19,10,5,5,0,0,4,662;13;9430;63,48;5;321;19,14;2;45;5,31;4;782;6,m;f
4688,ICLR,2020,DIME: AN INFORMATION-THEORETIC DIFFICULTY MEASURE FOR AI DATASETS,Peiliang Zhang;Huan Wang;Nikhil Naik;Caiming Xiong;Richard Socher,pez35@pitt.edu;huan.wang@salesforce.com;nnaik@salesforce.com;cxiong@salesforce.com;rsocher@salesforce.com,3;3;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,University of Pittsburgh;SalesForce.com;SalesForce.com;SalesForce.com;SalesForce.com,79;-1;-1;-1;-1,113;-1;-1;-1;-1,1,9/25/19,0,0,0,0,0,0,55;226;308;6301;53531,8;60;21;156;180,3;5;8;31;49,3;11;12;1045;8917,m;m
4689,ICLR,2020,Improved Detection of Adversarial Attacks via Penetration Distortion Maximization,Shai Rozenberg;Gal Elidan;Ran El-Yaniv,shairoz@cs.technion.ac.il;elidan@google.com;elyaniv@google.com,3;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0,yes,9/25/19,Technion;Google;Google,26;-1;-1,412;-1;-1,4,9/25/19,0,0,0,0,0,0,119;2320;7134,11;73;100,6;21;32,17;219;869,m;m
4690,ICLR,2020,X-Forest: Approximate Random Projection Trees for Similarity Measurement,Yikai Zhao;Peiqing Chen;Zidong Zhao;Tong Yang;Jie Jiang;Bin Cui;Gong Zhang;Steve Uhlig,zyk@pku.edu.cn;chenpeiqing@pku.edu.cn;benkerd@pku.edu.cn;yangtongemail@gmail.com;jie.jiang@pku.edu.cn;bin.cui@pku.edu.cn;nicholas.zhang@huawei.com;steve.uhlig@qmul.ac.uk,1;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,0,0,yes,9/25/19,Peking University;Peking University;Peking University;;Peking University;Peking University;Huawei Technologies Ltd.;Queen Mary University London,22;22;22;-1;22;22;-1;233,24;24;24;-1;24;24;-1;110,,9/25/19,0,0,0,0,0,0,56;37;32;175;0;154;51;1,8;8;12;59;6;41;50;6,2;3;2;8;0;3;4;1,8;0;0;5;0;7;0;0,m;m
4691,ICLR,2020,Out-of-Distribution Image Detection Using the Normalized Compression Distance,Sehun Yu;Donga Lee;Hwanjo Yu,hunu12@postech.ac.kr;dongha0914@postech.ac.kr;hwanjoyu@postech.ac.kr,3;3;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,POSTECH;POSTECH;POSTECH,118;118;118,146;146;146,,9/25/19,2,0,0,0,0,0,2;0;3297,2;2;147,1;0;28,0;0;304,m;m
4692,ICLR,2020,A Deep Recurrent Neural Network via Unfolding Reweighted l1-l1 Minimization,Huynh Van Luong;Duy Hung Le;Nikos Deligiannis,hvanluon@etrovub.be;dle@etrovub.be;ndeligia@etrovub.be,3;6;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Vrije Universiteit Brussel;Vrije Universiteit Brussel;Vrije Universiteit Brussel,481;481;481,235;235;235,8,9/25/19,0,0,0,0,0,0,233;1;1082,36;10;144,9;1;16,13;0;41,m;m
4693,ICLR,2020,Meta-Learning for Variational Inference,Ruqi Zhang;Yingzhen Li;Chris De Sa;Sam Devlin;Cheng Zhang,rz297@cornell.edu;yingzhen.li@microsoft.com;cdesa@cs.cornell.edu;sam.devlin@microsoft.com;cheng.zhang@microsoft.com,3;3;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0,yes,9/25/19,Cornell University;Microsoft;Cornell University;Microsoft;Microsoft,7;-1;7;-1;-1,19;-1;19;-1;-1,11;5;6,9/25/19,0,0,0,0,0,0,42;1007;0;655;160,13;43;1;73;112,3;14;0;12;7,2;142;0;35;6,f;f
4694,ICLR,2020,All Simulations Are Not Equal: Simulation Reweighing for Imperfect Information Games,Qucheng Gong;Yuandong Tian,qucheng@fb.com;yuandong@fb.com,1;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,3,0,yes,9/25/19,Facebook;Facebook,-1;-1,-1;-1,,9/25/19,0,0,0,0,0,0,126;2498,8;85,4;25,5;293,m;m
4695,ICLR,2020,GRASPEL: GRAPH SPECTRAL LEARNING AT SCALE,Yongyu Wang;Zhiqiang Zhao;Zhuo Feng,yongyuw@mtu.edu;qzzhao@mtu.edu;zfeng12@stevens.edu,1;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Michigan Technological University;Michigan Technological University;Stevens Institute of Technology,323;323;154,1397;1397;605,10,9/25/19,2,0,0,0,0,0,43;11;56,15;12;21,3;2;4,6;0;3,m;m
4696,ICLR,2020,Latent Variables on Spheres for Sampling and Inference,Deli Zhao;Jiapeng Zhu;Bo Zhang,zhaodeli@gmail.com;jengzhu0@gmail.com;zhangbo@xiaomi.com,6;1;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,1,5,0,yes,9/25/19,;;Xiaomi,-1;-1;-1,-1;-1;-1,5;1,9/25/19,0,0,0,0,0,0,44;497;151,17;15;97,4;4;6,4;55;17,m;m
4697,ICLR,2020,Detecting Change in Seasonal Pattern via Autoencoder and Temporal Regularization,Raphael Fettaya;Dor Bank;Rachel Lemberg;Linoy Barel,raphaelfettaya@gmail.com;doban@microsoft.com;rlemberg@microsoft.com;t-libare@microsoft.com,1;3;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,Tel Aviv University;Microsoft;Microsoft;Microsoft,35;-1;-1;-1,188;-1;-1;-1,,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1;-1,-1;-1;-1;-1,-1;-1;-1;-1,0;0;0;0,m;f
4698,ICLR,2020,Ladder Polynomial Neural Networks,Li-Ping Liu;Ruiyuan Gu;Xiaozhe Hu,liping.liu@tufts.edu;ruiyuan.gu@tufts.edu;xiaozhe.hu@tufts.edu,3;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Tufts University;Tufts University;Tufts University,172;172;172,139;139;139,,9/25/19,0,0,0,0,0,0,15;0;578,14;1;87,2;0;13,2;0;22,m;m
4699,ICLR,2020,Universal Safeguarded Learned Convex Optimization with Guaranteed Convergence,Howard Heaton;Xiaohan Chen;Zhangyang Wang;Wotao Yin,heaton@math.ucla.edu;chernxh@tamu.edu;atlaswang@tamu.edu;wotao.yin@alibaba-inc.com,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0,yes,9/25/19,"University of California, Los Angeles;Texas A&M;Texas A&M;Alibaba Group",20;44;44;-1,17;177;177;-1,9,9/25/19,0,0,0,0,0,0,3;12;2947;17461,6;6;167;221,1;2;28;56,0;0;383;1912,m;m
4700,ICLR,2020,Knockoff-Inspired Feature Selection via Generative Models,Marco F. Duarte;Siwei Feng,mduarte@ecs.umass.edu;siwei@umass.edu,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,"University of Massachusetts, Amherst;University of Massachusetts, Amherst",28;28,209;209,5,9/25/19,0,0,0,0,0,0,8627;104,138;22,36;6,805;4,m;m
4701,ICLR,2020,Diagnosing the Environment Bias in Vision-and-Language Navigation,Yubo Zhang;Hao Tan;Mohit Bansal,zhangyb@cs.unc.edu;airsplay@cs.unc.edu;mbansal@cs.unc.edu,6;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,8,2,yes,9/25/19,"University of North Carolina, Chapel Hill;University of North Carolina, Chapel Hill;University of North Carolina, Chapel Hill",73;73;73,54;54;54,10,9/25/19,0,0,0,0,0,0,37;2015;35,17;227;12,3;25;2,2;123;5,m;m
4702,ICLR,2020,Entropy Minimization In Emergent Languages,Eugene Kharitonov;Rahma Chaabouni;Diane Bouchacourt;Marco Baroni,eugene.kharitonov@gmail.com;rchaabouni@fb.com;dianeb@fb.com;marco.baroni@unitn.it,1;6;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Reject,0,8,0,yes,9/25/19,Facebook;Facebook;Facebook;University of Trento,-1;-1;-1;18,-1;-1;-1;307,4,5/31/19,3,3,0,0,0,0,170;66;197;10601,27;17;16;212,9;6;5;44,9;2;20;1337,m;m
4703,ICLR,2020,Improving SAT Solver Heuristics with Graph Networks and Reinforcement Learning,Vitaly Kurin;Saad Godil;Shimon Whiteson;Bryan Catanzaro,vitaliykurin@gmail.com;sgodil@nvidia.com;shimon.whiteson@gmail.com;bcatanzaro@nvidia.com,8;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,5,0,yes,9/25/19,University of Oxford;NVIDIA;University of Oxford;NVIDIA,50;-1;50;-1,1;-1;1;-1,1;10,9/25/19,3,3,2,1,0,1,182;2;5445;9386,11;3;203;73,6;1;38;28,13;0;588;1078,m;m
4704,ICLR,2020,Toward Understanding The Effect of Loss Function on The Performance of Knowledge Graph Embedding,Mojtaba Nayyeri;Chengjin Xu;Yadollah Yaghoobzadeh;Hamed Shariat Yazdi;Jens Lehmann,nayyeri@cs.uni-bonn.de;xuc@cs.uni-bonn.de;yayaghoo@microsoft.com;shariat@cs.uni-bonn.de;jens.lehmann@cs.uni-bonn.de,6;3;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0,yes,9/25/19,University of Bonn;University of Bonn;Microsoft;University of Bonn;University of Bonn,128;128;-1;128;128,106;106;-1;106;106,1;10,9/2/19,2,0,0,0,0,0,19;42;167;9;13857,14;13;19;8;452,3;3;6;1;44,2;2;15;1;1899,m;m
4705,ICLR,2020,Multi-Precision Policy Enforced Training (MuPPET) : A precision-switching strategy for quantised fixed-point training of CNNs,Aditya Rajagopal;Diederik A. Vink;Stylianos I. Venieris;Christos-Savvas Bouganis,aditya.rajagopal14@imperial.ac.uk;diederik.vink14@imperial.ac.uk;stelios.ven10@gmail.com;christos-savvas.bouganis@imperial.ac.uk,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,3,0,yes,9/25/19,Imperial College London;Imperial College London;Samsung;Imperial College London,73;73;-1;73,10;10;-1;10,,9/25/19,0,0,0,0,0,0,69;0;345;1285,16;2;29;148,5;0;9;19,4;0;28;103,m;m
4706,ICLR,2020,R2D2: Reuse & Reduce via Dynamic Weight Diffusion for Training Efficient NLP Models,Yi Tay;Aston Zhang;Shuai Zhang;Alvin Chan;Luu Anh Tuan;Siu Cheung Hui,ytay017@e.ntu.edu.sg;astonz@amazon.com;cheungshuai@outlook.com;guoweial001@e.ntu.edu.sg;tuanluu@csail.mit.edu;asschui@ntu.edu.sg,3;6;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0,yes,9/25/19,National Taiwan University;Amazon;University of New South Wales;National Taiwan University;Massachusetts Institute of Technology;National Taiwan University,86;-1;481;86;2;86,120;-1;1397;120;5;120,3,9/25/19,0,0,0,0,0,0,1429;649;66;1592;918;3109,68;42;28;133;53;236,19;13;5;19;17;30,166;48;3;116;120;238,m;m
4707,ICLR,2020,Variational Autoencoders for Opponent Modeling in Multi-Agent Systems,Georgios Papoudakis;Stefano V. Albrecht,g.papoudakis@ed.ac.uk;s.albrecht@ed.ac.uk,3;1;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,6,0,yes,9/25/19,University of Edinburgh;University of Edinburgh,33;33,30;30,5,9/25/19,0,0,0,0,0,0,17;350,7;43,2;10,0;27,m;m
4708,ICLR,2020,SELF-KNOWLEDGE DISTILLATION ADVERSARIAL ATTACK,Ma Xiaoxiong[1];Wang Renzhi[1];Tian Cong;Dong Zeqian;Duan Zhenhua,maxrumi@163.com;shanicky4ever@gmail.com;tico_tools@163.com;zqdong@stu.xidian.edu.cn;zhenhua_duan@126.com,3;1;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,6,0,yes,9/25/19,Tsinghua University;Tsinghua University;163;Tsinghua University;126,8;8;-1;8;-1,23;23;-1;23;-1,4;1,9/25/19,0,0,0,0,0,0,0;2;17;0;74,5;25;39;1;63,0;1;3;0;4,0;0;0;0;2,m;m
4709,ICLR,2020,Continual Deep Learning by Functional Regularisation of Memorable Past,Pingbo Pan;Alexander Immer;Siddharth Swaroop;Runa Eschenhagen;Richard E Turner;Mohammad Emtiyaz Khan,pingbo.pan@student.uts.edu.au;alexander.immer@epfl.ch;ss2163@cam.ac.uk;reschenhagen@uni-osnabrueck.de;ret26@cam.ac.uk;emtiyaz.khan@riken.jp,1;1;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,University of Technology Sydney;Swiss Federal Institute of Technology Lausanne;University of Cambridge;Universität Osnabrück;University of Cambridge;RIKEN,108;481;71;481;71;-1,193;38;3;1397;3;-1,,9/25/19,1,0,0,0,0,0,309;10;112;29;5161;669,12;5;13;3;128;67,5;2;7;1;37;16,61;2;10;2;423;65,m;m
4710,ICLR,2020,Model Comparison of Beer data classification using an electronic nose,Mohammed Abdi;Aminat Adebiyi;Andrea Fasoli;Alberto Mannari;Ronald Labby;Luisa Bozano,mohammed.munir.abdi@ibm.com;aminat.adebiyi@ibm.com;andrea.fasoli@ibm.com;alberto.mannari@ibm.com;rlabby@us.ibm.com;lbozano@us.ibm.com,1;1;1,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:N/A:N/A:N/A,Reject,0,0,0,yes,9/25/19,International Business Machines;International Business Machines;International Business Machines;International Business Machines;International Business Machines;International Business Machines,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,0;0;0;0;0;0,m;f
4711,ICLR,2020,Deep Multi-View Learning via Task-Optimal CCA,Heather D. Couture;Roland Kwitt;J.S. Marron;Melissa Troester;Charles M. Perou;Marc Niethammer,heather@pixelscientia.com;roland.kwitt@gmail.com;marron@unc.edu;troester@unc.edu;chuck_perou@med.unc.edu;mn@cs.unc.edu,3;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,"Pixel Scientia Labs;University of Salzburg;University of North Carolina, Chapel Hill;University of North Carolina, Chapel Hill;University of North Carolina, Chapel Hill;University of North Carolina, Chapel Hill",-1;266;73;73;73;73,-1;1397;54;54;54;54,,7/17/19,0,0,0,0,0,0,105;1694;17659;6522;86089;3581,7;90;294;241;579;187,4;21;58;35;102;29,4;149;1685;326;4625;253,f;m
4712,ICLR,2020,Unsupervised Representation Learning by Predicting Random Distances,Hu Wang;Guansong Pang;Chunhua Shen;Congbo Ma,hu.wang@adelaide.edu.au;pangguansong@gmail.com;chunhua.shen@adelaide.edu.au;201520121828@mail.scut.edu.cn,6;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,The University of Adelaide;The University of Adelaide;The University of Adelaide;South China University of Technology,128;128;128;481,120;120;120;501,,9/25/19,1,1,1,0,0,0,190;400;2358;6,49;32;87;5,8;11;16;2,16;31;191;0,m;m
4713,ICLR,2020,Qgraph-bounded Q-learning: Stabilizing Model-Free Off-Policy Deep Reinforcement Learning,Sabrina Hoppe;Marc Toussaint,sabrina.hoppe@de.bosch.com;marc.toussaint@informatik.uni-stuttgart.de,6;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:N/A,Reject,0,4,0,yes,9/25/19,Bosch;University of Stuttgart,-1;95,-1;292,1;10,9/25/19,0,0,0,0,0,0,0;16,3;17,0;2,0;0,f;m
4714,ICLR,2020,Neural Maximum Common Subgraph Detection with Guided Subgraph Extraction,Yunsheng Bai;Derek Xu;Ken Gu;Xueqing Wu;Agustin Marinovic;Christopher Ro;Yizhou Sun;Wei Wang,yba@ucla.edu;derekqxu@ucla.edu;ken.qgu@gmail.com;shirley0@mail.ustc.edu.cn;amarinovic@ucla.edu;christopher.j.ro@gmail.com;yzsun@cs.ucla.edu;weiwang@cs.ucla.edu,3;3;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,9,0,yes,9/25/19,"University of California, Los Angeles;University of California, Los Angeles;;University of Science and Technology of China;University of California, Los Angeles;;University of California, Los Angeles;University of California, Los Angeles",20;20;-1;481;20;-1;20;20,17;17;-1;80;17;-1;17;17,10,9/25/19,2,2,1,0,0,0,74;9;1196;11;18;14;6951;106,17;5;15;8;5;14;188;137,5;2;7;2;3;2;38;5,12;1;57;1;1;4;766;10,m;f
4715,ICLR,2020,Blockwise Self-Attention for Long Document Understanding,Jiezhong Qiu;Hao Ma;Omer Levy;Scott Wen-tau Yih;Sinong Wang;Jie Tang,qiujz16@mails.tsinghua.edu.cn;gabe.hao.ma@gmail.com;omerlevy@gmail.com;scottyih@gmail.com;sinongwang@fb.com;jietang@tsinghua.edu.cn,3;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Tsinghua University;Facebook;Facebook;Facebook AI Research;Facebook;Tsinghua University,8;-1;-1;-1;-1;8,23;-1;-1;-1;-1;23,,9/25/19,6,4,2,0,0,1,574;979;7667;87;269;1552,14;49;58;18;30;68,8;11;30;6;10;9,80;99;1231;11;20;182,m;m
4716,ICLR,2020,GroSS Decomposition: Group-Size Series Decomposition for Whole Search-Space Training,Henry Howard-Jenkins;Yiwen Li;Victor Adrian Prisacariu,henryhj@robots.ox.ac.uk;kate@robots.ox.ac.uk;victor@robots.ox.ac.uk,3;6;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,University of Oxford;University of Oxford;University of Oxford,50;50;50,1;1;1,1,9/25/19,0,0,0,0,0,0,4;141;1591,6;36;55,1;6;21,0;3;173,m;m
4717,ICLR,2020,Emergence of Collective Policies Inside Simulations with Biased Representations,Jooyeon Kim;Alice Oh,jooyeon.kim@kaist.ac.kr;alice.oh@kaist.edu,3;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,1,0,yes,9/25/19,Korea Advanced Institute of Science and Technology;KAIST,481;20,110;110,,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1,-1;-1,-1;-1,0;0,m;f
4718,ICLR,2020,On unsupervised-supervised risk and one-class neural networks,Christophe Cerisara,cerisara@loria.fr,3;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,University of Lorraine,481,624,3;5,9/25/19,0,0,0,0,0,0,479,96,11,25,m
4719,ICLR,2020,Shifted Randomized Singular Value Decomposition,Ali Basirat,ali.basirat@lingfil.uu.se,1;1;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,Uppsala University,154,102,,9/25/19,0,0,0,0,0,0,60,26,4,3,m;m
4720,ICLR,2020,Encoding Musical Style with Transformer Autoencoders,Kristy Choi;Curtis Hawthorne;Ian Simon;Monica Dinculescu;Jesse Engel,kechoi@cs.stanford.edu;fjord@google.com;iansimon@google.com;noms@google.com;jesseengel@google.com,3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Stanford University;Google;Google;Google;Google,4;-1;-1;-1;-1,4;-1;-1;-1;-1,3,9/25/19,2,1,2,0,0,0,423;392;2527;113;2230,8;20;40;19;36,5;9;16;4;12,36;71;222;17;251,f;m
4721,ICLR,2020,Adaptive Adversarial Imitation Learning,Yiren Lu;Jonathan Tompson;Sergey Levine,luyiren92@gmail.com;tompson@google.com;slevine@google.com,6;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0,yes,9/25/19,;Google;Google,-1;-1;-1,-1;-1;-1,4,9/25/19,0,0,0,0,0,0,69;3247;959,9;30;34,5;16;9,7;378;103,f;m
4722,ICLR,2020,Adversarial Interpolation Training: A Simple Approach for Improving Model Robustness,Haichao Zhang;Wei Xu,hczhang1@gmail.com;wei.xu@horizon.ai,6;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,1,8,0,yes,9/25/19,Horizon Robotics;Horizon Robotics,-1;-1,-1;-1,4,9/25/19,4,2,3,0,0,1,255;113,18;96,4;7,22;7,m;f
4723,ICLR,2020,Do Image Classifiers Generalize Across Time?,Vaishaal Shankar;Achal Dave;Rebecca Roelofs;Deva Ramanan;Ben Recht;Ludwig Schmidt,vaishaal@berkeley.edu;achald@cs.cmu.edu;roelofs@cs.berkely.edu;deva@cs.cmu.edu;brecht@berkeley.edu;ludwigschmidt2@gmail.com,3;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,University of California Berkeley;Carnegie Mellon University;;Carnegie Mellon University;University of California Berkeley;University of California Berkeley,5;1;-1;1;5;5,13;27;-1;27;13;13,,6/5/19,1,1,1,0,0,1,463;88;622;32040;3015;61,24;11;12;163;23;18,8;4;5;59;13;3,44;6;68;5389;461;8,m;m
4724,ICLR,2020,Manifold Learning and Alignment with Generative Adversarial Networks,Jiseob Kim;Seungjae Jung;Hyundo Lee;Byoung-Tak Zhang,jkim@bi.snu.ac.kr;sjjung@bi.snu.ac.kr;hdlee@bi.snu.ac.kr;btzhang@bi.snu.ac.kr,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,Seoul National University;Seoul National University;Seoul National University;Seoul National University,41;41;41;41,64;64;64;64,5;4,9/25/19,0,0,0,0,0,0,54;1054;60;50,33;70;27;44,4;16;3;3,3;37;0;2,m;m
4725,ICLR,2020,Count-guided Weakly Supervised Localization Based on Density Map,Ming Ma;Stephan Chalup;Fayeem Aziz;Yang Liu;Defu Cheng;Zhijian Zhou,mmingabc@outlook.com;stephan.chalup@newcastle.edu.au;mdfayeembin.aziz@uon.edu.au;liu15@mails.jlu.edu.cn;chengdefu@jlu.edu.cn;zhouzhijian@jlu.edu.cn,1;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,0,0,yes,9/25/19,"Jilin University;University of Newcastle, Australia;University of Newcastle, Australia;Jilin University;Jilin University;Jilin University",481;390;390;481;481;481,952;311;311;952;952;952,5,9/25/19,0,0,0,0,0,0,5;821;4;33;108;124,38;143;5;17;9;14,2;14;1;2;4;3,0;43;0;0;2;11,m;m
4726,ICLR,2020,Neural Phrase-to-Phrase Machine Translation,Jiangtao;Feng;Lingpeng Kong;Po-sen Huang;Chong;Wang;Da;Huang Jiayuan;Mao;Kan;Qiao;Dengyong;Zhou,lingpenk@google.com,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0,yes,9/25/19,Google,-1,-1,3,11/6/18,7,0,0,0,0,0,497;31;1072;1772;19017;-1;102;236;-1;-1;-1;9077;-1,19;11;32;59;1045;-1;28;23;-1;-1;-1;78;-1,7;3;14;17;57;-1;6;9;-1;-1;-1;34;-1,52;1;110;250;1649;0;5;5;0;0;0;1342;0,m;m
4727,ICLR,2020,Walking the Tightrope: An Investigation of the Convolutional Autoencoder Bottleneck,Ilja Manakov;Markus Rohm;Volker Tresp,ilja.manakov@med.uni-muenchen.de;markus.rohm@med.uni-muenchen.de;volker.tresp@siemens.com,6;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,8,0,yes,9/25/19,Ludwig-Maximilians-Universität München;Ludwig-Maximilians-Universität München;Siemens Corporate Research,-1;-1;-1,-1;-1;-1,6;8,9/25/19,0,0,0,0,0,0,16;15;8302,4;3;287,2;1;45,1;0;807,m;m
4728,ICLR,2020,LEX-GAN: Layered Explainable Rumor Detector Based on Generative Adversarial Networks,Mingxi Cheng;Yizhi Li;Shahin Nazarian;Paul Bogdan,mingxic@usc.edu;yizhi.li@bupt.edu.cn;shahin.nazarian@usc.edu;pbogdan@usc.edu,3;1;8;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0,yes,9/25/19,University of Southern California;Beijing University of Post and Telecommunication;University of Southern California;University of Southern California,31;481;31;31,62;1397;62;62,5;4,9/25/19,0,0,0,0,0,0,45;17;845;1807,8;11;92;126,3;2;14;26,2;0;54;87,f;m
4729,ICLR,2020,A New Multi-input Model with the Attention Mechanism for Text Classification,Junhao Qiu;Ronghua Shi;Fangfang Li (the corresponding author);Jinjing Shi;Wangmin Liao,qiujunhao@csu.edu.cn;shirh@csu.edu.cn;lifangfang@csu.edu.cn;shijinjing@csu.edu.cn;0909123117@csu.edu.cn,1;1;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,0,0,yes,9/25/19,SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY,481;481;481;481;481,299;299;299;299;299,2,9/25/19,0,0,0,0,0,0,57;4;13;149;11,4;9;18;39;5,2;2;3;6;1,3;0;0;4;0,m;m
4730,ICLR,2020,Stochastic Prototype Embeddings,Tyler R. Scott;Karl Ridgeway;Michael C. Mozer,tysc7237@colorado.edu;karl.ridgeway@colorado.edu;mcmozer@google.com,3;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,"University of Colorado, Boulder;University of Colorado, Boulder;Google",44;44;-1,123;123;-1,6,9/25/19,0,0,0,0,0,0,30;164;6955,4;13;238,1;5;44,2;15;518,m;m
4731,ICLR,2020,GraphNVP: an Invertible Flow-based Model for Generating Molecular Graphs,Kaushalya Madhawa;Katsuhiko Ishiguro;Kosuke Nakago;Motoki Abe,kaushalya@net.c.titech.ac.jp;k.ishiguro.jp@ieee.org;nakago@preferred.jp;motoki@preferred.jp,3;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0,yes,9/25/19,"Tokyo Institute of Technology;Preferred Networks, Inc.;Preferred Networks, Inc.;Preferred Networks, Inc.",172;-1;-1;-1,299;-1;-1;-1,5;10,5/28/19,26,10,6,2,8,3,42;641;29;415,10;74;4;19,3;14;2;9,4;45;3;39,m;m
4732,ICLR,2020,Contrastive Multiview Coding,Yonglong Tian;Dilip Krishnan;Phillip Isola,yonglong@mit.edu;dilipkay@google.com;phillipi@mit.edu,3;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,3,0,yes,9/25/19,Massachusetts Institute of Technology;Google;Massachusetts Institute of Technology,2;-1;2,5;-1;5,,6/13/19,97,53,49,6,4,23,1681;5051;12579,22;73;73,13;22;27,184;654;2186,m;m
4733,ICLR,2020,RGTI:Response generation via templates integration for End to End dialog,Yuxin Zhang;Songyan Liu,zhangyuxin960625@gmail.com;anchor3l31@gmail.com,1;1;1;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,Beijing University of Post and Telecommunication;,481;-1,1397;-1,,9/25/19,0,0,0,0,0,0,5;168,12;26,1;5,0;1,m;m
4734,ICLR,2020,Improving the Gating Mechanism of Recurrent Neural Networks,Albert Gua;Caglar Gulcehre;Tom le Paine;Razvan Pascanu;Matt Hoffman,gua@google.com;caglarg@google.com;mwhoffman@google.com;razp@google.com,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,3,9/25/19,0,0,0,0,0,0,231;20002;525;17189;2042,31;47;17;101;66,7;26;9;46;17,29;3023;66;1700;206,m;m
4735,ICLR,2020,Analyzing the Role of Model Uncertainty for Electronic Health Records,Michael W. Dusenberry;Dustin Tran;Edward Choi;Jonas Kemp;Jeremy Nixon;Ghassen Jerfel;Katherine Heller;Andrew M. Dai,dusenberrymw@google.com;trandustin@google.com;mp2893@gmail.com;jonasbkemp@google.com;jeremynixon@google.com;ghassen@google.com;kheller@google.com;adai@google.com,3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,2,3,0,yes,9/25/19,Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1,11,6/10/19,11,3,3,1,9,0,40;1809;1016;16;62;68;115;3806,7;50;36;10;10;9;15;50,4;20;11;2;5;6;5;19,2;197;83;1;4;4;3;472,m;m
4736,ICLR,2020,Leveraging Entanglement Entropy for Deep Understanding of  Attention Matrix in Text Matching,Peng Zhang;XiaoLiu Mao;XinDian Ma;BenYou Wang;Jing Zhang;Jun Wang;DaWei Song,pzhang@tju.edu.cn;xiaoliumao@tju.edu.cn;xindianma@tju.edu.cn;wang@dei.unipd.it;18738996120@163.com;jun.wang@cs.ucl.ac.uk;dwsong@bit.edu.cn,1;1;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Zhejiang University;Zhejiang University;Zhejiang University;Universita' degli studi di Padova;Zhejiang University;University College London;BIT,56;56;56;-1;56;50;-1,107;107;107;-1;107;15;-1,1,9/25/19,0,0,0,0,0,0,39;0;16;427;38;61;4,23;2;7;28;70;77;15,3;0;1;10;3;5;2,1;0;1;51;1;2;0,m;m
4737,ICLR,2020,Superbloom: Bloom filter meets Transformer,John Anderson;Qingqing Huang;Walid Krichene;Steffen Rendle;Li Zhang,janders@google.com;qqhuang@google.com;walidk@google.com;srendle@google.com;liqzhang@google.com,6;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,5,0,yes,9/25/19,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,3,9/25/19,0,0,0,0,0,0,419;298;443;7156;886,60;64;51;51;138,6;9;12;23;12,30;5;27;1351;60,m;f
4738,ICLR,2020,LEARNING TO IMPUTE: A GENERAL FRAMEWORK FOR SEMI-SUPERVISED LEARNING,Wei-Hong Li;Chuan-Sheng Foo;Hakan Bilen,w.h.li@ed.ac.uk;foo_chuan_sheng@i2r.a-star.edu.sg;hbilen@ed.ac.uk,3;3;3,I have published one or two papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,University of Edinburgh;A*STAR;University of Edinburgh,33;-1;33,30;-1;30,8,9/25/19,2,2,0,0,0,0,17;459;1809,7;26;49,3;8;16,2;51;358,m;m
4739,ICLR,2020,Reflection-based Word Attribute Transfer,Yoichi Ishibashi;Katsuhito Sudoh;Koichiro Yoshino;Satoshi Nakamura,ishibashi.yoichi.ir3@is.naist.jp;sudoh@is.naist.jp;koichiro@is.naist.jp;s-nakamura@is.naist.jp,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,"Nara Institute of Science and Technology, Japan;Nara Institute of Science and Technology, Japan;Nara Institute of Science and Technology, Japan;Nara Institute of Science and Technology, Japan",481;481;481;481,1397;1397;1397;1397,3;7,9/25/19,0,0,0,0,0,0,2;979;398;57,6;91;95;67,1;15;11;4,0;156;16;1,m;m
4740,ICLR,2020,ExpandNets: Linear Over-parameterization to Train Compact Convolutional Networks,Shuxuan Guo;Jose M. Alvarez;Mathieu Salzmann,shuxuan.guo@epfl.ch;josea@nvidia.com;mathieu.salzmann@epfl.ch,6;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Swiss Federal Institute of Technology Lausanne;NVIDIA;Swiss Federal Institute of Technology Lausanne,481;-1;481,38;-1;38,2;8,11/26/18,0,0,0,0,0,0,6;976;5661,5;51;198,2;15;42,0;99;623,m;m
4741,ICLR,2020,PopSGD: Decentralized Stochastic Gradient Descent in the Population Model,Giorgi Nadiradze;Amirmojtaba Sabour;Aditya Sharma;Ilia Markov;Vitaly Aksenov;Dan Alistarh.,giorgi.nadiradze@ist.ac.at;amsabour79@gmail.com;adityasharma.2000.as@gmail.com;ilia.markov@ist.ac.at;aksenov.vitaly@gmail.com;dan.alistarh@ist.ac.at,3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0,yes,9/25/19,Institute of Science and Technology Austria;;Indian Institute of Technology Bombay;Institute of Science and Technology Austria;ITMO University;Institute of Science and Technology Austria,481;-1;118;481;481;481,1397;-1;480;1397;480;1397,1,9/25/19,2,1,0,0,0,0,254;3;60;276;21;1785,18;3;8;37;20;127,5;1;2;11;2;19,25;0;2;11;2;227,m;m
4742,ICLR,2020,Wide Neural Networks are Interpolating Kernel Methods: Impact of Initialization on Generalization,Manuel Nonnenmacher;David Reeb;Ingo Steinwart,manuel.nonnenmacher@de.bosch.com;david.reeb@de.bosch.com;ingo.steinwart@mathematik.uni-stuttgart.de,1;1;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Bosch;Bosch;University of Stuttgart,-1;-1;95,-1;-1;292,1;8,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1,-1;-1;-1,-1;-1;-1,0;0;0,m;m
4743,ICLR,2020,Improved Mutual Information Estimation,Youssef Mroueh*;Igor Melnyk*;Pierre Dognin*;Jerret Ross*;Tom Sercu*,mroueh@us.ibm.com;igor.melnyk@ibm.com;pdognin@us.ibm.com;rossja@us.ibm.com;tom.sercu@gmail.com,1;3;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,7,0,yes,9/25/19,International Business Machines;International Business Machines;International Business Machines;International Business Machines;International Business Machines,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,1,9/25/19,0,0,0,0,0,0,916;258;119;392;785,53;33;27;12;25,11;11;6;4;11,151;17;6;86;72,m;m
4744,ICLR,2020,Confidence Scores Make Instance-dependent Label-noise Learning Possible,Antonin Berthon;Bo Han;Gang Niu;Tongliang Liu;Masashi Sugiyama,berthon.antonin@gmail.com;bo.han@riken.jp;gang.niu@riken.jp;tongliang.liu@sydney.edu.au;sugi@k.u-tokyo.ac.jp,8;1;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,10,0,yes,9/25/19,RIKEN;RIKEN;RIKEN;University of Sydney;The University of Tokyo,-1;-1;-1;86;56,-1;-1;-1;60;36,,9/25/19,2,2,0,0,0,0,10;41;1198;154;11910,4;37;80;13;711,2;3;17;3;53,0;2;148;14;1314,m;m
4745,ICLR,2020,Is Deep Reinforcement Learning Really Superhuman on Atari? Leveling the playing field,Marin Toromanoff;Emilie Wirbel;Fabien Moutarde,marin.toromanoff@mines-paristech.fr;emilie.wirbel@valeo.com;fabien.moutarde@mines-paristech.fr,6;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,8,0,yes,9/25/19,Mines ParisTech;Valeo.ai;Mines ParisTech,481;-1;481,1397;-1;1397,,8/13/19,4,3,2,0,0,0,76;103;928,7;14;85,4;4;17,3;8;37,m;m
4746,ICLR,2020,A Bilingual Generative Transformer for Semantic Sentence Embedding,John Wieting;Graham Neubig;Taylor Berg-Kirkpatrick,jwieting@cs.cmu.edu;gneubig@cs.cmu.edu;tberg@eng.ucsd.edu,3;6;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,5,0,yes,9/25/19,"Carnegie Mellon University;Carnegie Mellon University;University of California, San Diego",1;1;11,27;27;31,3,9/25/19,0,0,0,0,0,0,1214;5304;1984,25;443;48,12;38;20,165;547;342,m;m
4747,ICLR,2020,TechKG: A Large-Scale Chinese Technology-Oriented Knowledge Graph,Feiliang Ren,renfeiliang@cse.neu.edu.cn,1;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Northeastern University,16,906,10,12/17/18,1,1,0,0,7,0,59,28,5,4,m;m
4748,ICLR,2020,Wasserstein Adversarial Regularization (WAR) on label noise,Bharath Damodaran;Kilian Fatras;Sylvain Lobry;Rémi Flamary;Devis Tuia;Nicolas Courty,bharath-bhushan.damodaran@irisa.fr;kilian.fatras@irisa.fr;sylvain.lobry@wur.nl;remi.flamary@unice.fr;devis.tuia@wur.nl;ncourty@irisa.fr,8;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,"IRISA, Université Bretagne Sud;IRISA, Université Bretagne Sud;Wageningen University and Research;Université Côte d'Azur;Wageningen University and Research;IRISA, Université Bretagne Sud",481;481;481;-1;481;481,1397;1397;59;-1;59;1397,4,9/25/19,0,0,0,0,0,0,377;17;86;1507;6643;1826,22;7;24;90;243;123,8;3;5;19;44;24,29;3;7;119;398;144,m;m
4749,ICLR,2020,Enhancing the Transformer with explicit relational encoding for math problem solving,Imanol Schlag;Paul Smolensky;Roland Fernandez;Nebojsa Jojic;Jürgen Schmidhuber;Jianfeng Gao,imanol@idsia.ch;paul.smolensky@gmail.com;rfernand@microsoft.com;jojic@microsoft.com;juergen@idsia.ch;jfgao@microsoft.com,3;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Reject,1,4,0,yes,9/25/19,IDSIA;Microsoft;Microsoft;Microsoft;IDSIA;Microsoft,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,,9/25/19,6,1,1,0,0,0,48;10024;445;255;103;18900,5;206;20;31;17;353,4;40;9;8;6;61,3;933;31;11;7;2683,m;m
4750,ICLR,2020,Constant Curvature Graph Convolutional Networks,Gregor Bachmann;Gary Bécigneul;Octavian-Eugen Ganea,gregorb@student.ethz.ch;garyb@mit.edu;oct@mit.edu,1;8;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Swiss Federal Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,10;2;2,13;5;5,10;8,9/25/19,4,2,1,0,0,0,90;243;529,118;17;17,5;5;9,1;47;96,m;m
4751,ICLR,2020,Plan2Vec: Unsupervised Representation Learning by Latent Plans,Ge Yang;Amy Zhang;Ari Morcos;Joelle Pineau;Pieter Abbeel;Roberto Calandra,yangge1987@gmail.com;amyzhang2011@gmail.com;arimorcos@gmail.com;jpineau@cs.mcgill.ca;pabbeel@cs.berkeley.edu;rcalandra@fb.com,3;1;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,;McGill University;Facebook;McGill University;University of California Berkeley;Facebook,-1;86;-1;86;5;-1,-1;42;-1;42;13;-1,10,9/25/19,0,0,0,0,0,0,94;788;1034;11328;37294;1023,31;47;32;267;438;58,4;13;12;46;94;15,2;94;116;1235;4481;102,m;m
4752,ICLR,2020,Generating Dialogue Responses From A Semantic Latent Space,Wei-Jen Ko;Avik Ray;Yilin Shen;Hongxia Jin,wjko@outlook.com;avik.r@samsung.com;yilin.shen@samsung.com;hongxia.jin@samsung.com,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,"University of Texas, Austin;Samsung;Samsung;Samsung",22;-1;-1;-1,38;-1;-1;-1,,9/25/19,0,0,0,0,0,0,97;103;1004;1561,9;25;101;180,3;6;18;23,15;11;73;119,m;f
4753,ICLR,2020,Learning Key Steps to Attack Deep Reinforcement Learning Agents,Chien-Min Yu;Hsuan-Tien Lin,r07922080@csie.ntu.edu.tw;htlin@csie.ntu.edu.tw,3;3;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,National Taiwan University;National Taiwan University,86;86,120;120,4,9/25/19,0,0,0,0,0,0,0;3098,1;104,0;22,0;270,m;m
4754,ICLR,2020,"If MaxEnt RL is the Answer, What is the Question?",Benjamin Eysenbach;Sergey Levine,beysenba@cs.cmu.edu;svlevine@eecs.berkeley.edu,8;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I did not assess the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,14,0,yes,9/25/19,Carnegie Mellon University;University of California Berkeley,1;5,27;13,,9/25/19,5,4,1,0,0,0,357;24893,16;310,6;74,54;3235,m;m
4755,ICLR,2020,Learning to Control Latent Representations for Few-Shot Learning of Named Entities,Omar U. Florez;Erik Mueller,omar.florez@aggiemail.usu.edu;erikmueller@capitalone.com,3;1;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,0,0,yes,9/25/19,SUN YAT-SEN UNIVERSITY;Capital One Bank,481;-1,299;-1,3;6,9/25/19,0,0,0,0,0,0,39;127,21;126,3;5,1;4,m;m
4756,ICLR,2020,Variational Hyper RNN for Sequence Modeling,Ruizhi Deng;Yanshuai Cao;Bo Chang;Leonid Sigal;Greg Mori;Marcus Brubaker,ruizhid@sfu.ca;yanshuaicao@gmail.com;bchang@stat.ubc.ca;lsigal@cs.ubc.ca;mori@cs.sfu.ca;marcus.brubaker@borealisai.com,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,9,0,yes,9/25/19,Simon Fraser University;;University of British Columbia;University of British Columbia;Simon Fraser University;Borealis AI,64;-1;35;35;64;-1,272;-1;34;34;272;-1,,9/25/19,0,0,0,0,0,0,92;309;429;6711;9711;3978,16;20;56;166;197;55,5;7;8;39;45;17,10;35;42;674;817;466,m;m
4757,ICLR,2020,Weighted Empirical Risk Minimization: Transfer Learning based on Importance Sampling,Robin Vogel;Mastane Achab;Charles Tillier;Stéphan Clémençon,robin.vogel@telecom-paris.fr;mastane.achab@telecom-paris.fr;charles.tillier@telecom-paris.fr;stephan.clemencon@telecom-paris.fr,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,4,0,yes,9/25/19,Télécom Paris;Télécom Paris;Télécom Paris;Télécom Paris,481;481;481;481,187;187;187;187,8;1;6,9/25/19,0,0,0,0,0,0,10;6;43;1359,8;7;12;167,1;2;2;19,1;2;2;109,f;m
4758,ICLR,2020,Provenance detection through learning transformation-resilient watermarking,Jamie Hayes;Krishnamurthy Dvijotham;Yutian Chen;Sander Dieleman;Pushmeet Kohli;Norman Casagrande,j.hayes@cs.ucl.ac.uk;dvij@google.com;yutianc@google.com;sedielem@google.com;pushmeet@google.com;ncasagrande@google.com,1;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,University College London;Google;Google;Google;Google;Google,50;-1;-1;-1;-1;-1,15;-1;-1;-1;-1;-1,5;4,9/25/19,0,0,0,0,0,0,706;1143;982;14082;22578;1137,46;76;87;47;313;20,11;17;16;20;69;10,89;101;109;1113;2782;147,m;m
4759,ICLR,2020,Underwhelming Generalization Improvements From Controlling Feature Attribution,Joseph D Viviano;Becks Simpson;Francis Dutil;Yoshua Bengio;Joseph Paul Cohen,joseph@viviano.ca;becks.simpson@imagia.com;francis.dutil@imagia.com;yoshua.bengio@mila.quebec;joseph@josephpcohen.com,3;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A,Reject,0,3,0,yes,9/25/19,University of Montreal;Imagia;Imagia;University of Montreal;University of Montreal,128;-1;-1;128;128,85;-1;-1;85;85,8,9/25/19,3,1,0,0,0,0,305;11;358;208566;505,36;5;17;807;62,9;2;6;147;11,21;0;32;24297;50,m;m
4760,ICLR,2020,Combining graph and sequence information to learn protein representations,Hassan Kané;Mohamed Coulibali;Pelkins Ajanoh;Ali Abdalla,hassanmohamed@alum.mit.edu;mohamed-konoufo.coulibali.1@ulaval.ca;pelkins@alum.mit.edu;aabdalla@alum.mit.edu,1;1;3,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,0,0,yes,9/25/19,Massachusetts Institute of Technology;Laval university;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;481;2;2,5;272;5;5,,9/25/19,0,0,0,0,0,0,7;5;2;516,9;7;6;61,1;1;1;13,2;1;0;27,m;m
4761,ICLR,2020,Sensible adversarial learning,Jungeum Kim;Xiao Wang,kim2712@purdue.edu;wangxiao@purdue.edu,3;6;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,2,9,0,yes,9/25/19,Purdue University;Purdue University,27;27,88;88,4,9/25/19,3,0,0,0,0,0,6;20,3;42,2;3,0;1,m;m
4762,ICLR,2020,YaoGAN: Learning Worst-case Competitive Algorithms from Self-generated Inputs,Goran Zuzic;Di Wang;Aranyak Mehta;D. Sivakumar,zuza777@gmail.com;wadi@google.com;aranyak@google.com;siva@google.com,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.,Reject,0,8,0,yes,9/25/19,Carnegie Mellon University;Google;Google;Google,1;-1;-1;-1,27;-1;-1;-1,5;4,9/25/19,0,0,0,0,0,0,107;38;2520;1316,16;47;63;22,6;3;20;5,7;0;288;141,m;m
4763,ICLR,2020,Learning to Remember from a Multi-Task Teacher,Yuwen Xiong;Mengye Ren;Raquel Urtasun,yuwen@cs.toronto.edu;mren@cs.toronto.edu;urtasun@uber.com,1;3;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:N/A:N/A,Reject,0,6,0,yes,9/25/19,"Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Uber",18;18;-1,18;18;-1,6,9/25/19,0,0,0,0,0,0,1355;1528;24842,17;19;245,9;12;73,251;216;3482,m;f
4764,ICLR,2020,Pretraining boosts out-of-domain robustness for pose estimation,Alexander Mathis;Mert Yüksekgönül;Byron Rogers;Matthias Bethge;Mackenzie W. Mathis,amathis@fas.harvard.edu;mertyuksekgonul@gmail.com;byron@performancegenetics.com;matthias.bethge@uni-tuebingen.de;mathis@rowland.harvard.edu,1;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,3,0,yes,9/25/19,Harvard University;Bogazici University;Performancegenetics;University of Tuebingen;Harvard University,39;323;-1;154;39,7;672;-1;91;7,6;2;8,9/24/19,7,1,2,0,0,0,864;5;7;11714;106,30;2;4;414;7,13;1;2;47;4,88;0;0;1275;2,m;f
4765,ICLR,2020,Self-Induced Curriculum Learning in Neural Machine Translation,Dana Ruiter;Cristina España-Bonet;Josef van Genabith,druiter@lsv.uni-saarland.de;cristinae@dfki.de;josef.van_genabith@dfki.de,6;3;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,4,0,yes,9/25/19,Saarland University;German Research Center for AI;German Research Center for AI,92;-1;-1,1397;-1;-1,3,9/25/19,0,0,0,0,0,0,7;348;208,4;64;25,2;11;9,0;30;19,f;m
4766,ICLR,2020,Adapt-to-Learn: Policy Transfer in Reinforcement Learning,Girish Joshi;Girish Chowdhary,girishj2@illinois.edu;girishc@illinois.edu,1;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",3;3,48;48,,9/25/19,0,0,0,0,0,0,5626;2595,411;208,36;23,237;165,m;m
4767,ICLR,2020,The Surprising Behavior Of Graph Neural Networks,Vivek Kothari;Catherine Tong;Nicholas Lane,vivek.kothari@cs.ox.ac.uk;eu.tong@cs.ox.ac.uk;nicholas.lane@cs.ox.ac.uk,6;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,University of Oxford;University of Oxford;University of Oxford,50;50;50,1;1;1,10,9/25/19,0,0,0,0,0,0,66;121;10358,11;20;170,2;4;45,0;11;704,m;m
4768,ICLR,2020,On the Reflection of Sensitivity in the Generalization Error,Mahsa Forouzesh;Farnood Salehi;Patrick Thiran,mahsa.forouzesh@epfl.ch;farnood.salehi@epfl.ch;patrick.thiran@epfl.ch,3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne,481;481;481,38;38;38,8,9/25/19,0,0,0,0,0,0,0;55;7540,1;12;202,0;4;45,0;9;720,f;m
4769,ICLR,2020,Bio-Inspired Hashing for Unsupervised Similarity Search,Chaitanya K. Ryali;John J. Hopfield;Dmitry Krotov,rckrishn@eng.ucsd.edu;hopfield@princeton.edu;krotov@ibm.com,6;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,0,1,yes,9/25/19,"University of California, San Diego;Princeton University;International Business Machines",11;31;-1,31;6;-1,,9/25/19,1,1,1,0,0,1,1;14524;258,1;70;15,1;23;6,1;1236;24,m;m
4770,ICLR,2020,Neural Program Synthesis By Self-Learning,Yifan Xu;Lu Dai;Udaikaran Singh;Kening Zhang;Zhuowen Tu,yix081@ucsd.edu;dldaisy@mail.ustc.edu.cn;u1singh@ucsd.edu;kez040@ucsd.edu;ztu@ucsd.edu,3;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:N/A:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,2,yes,9/25/19,"University of California, San Diego;University of Science and Technology of China;University of California, San Diego;University of California, San Diego;University of California, San Diego",11;481;11;11;11,31;80;31;31;31,,9/25/19,1,0,0,0,0,0,1188;52;1;98;585,139;34;1;10;22,19;4;1;3;8,34;2;0;7;54,m;m
4771,ICLR,2020,The Variational InfoMax AutoEncoder,Vinenzo Crescimanna;Bruce Graham,vincenzo.crescimanna1@stir.ac.uk;bruce.graham@stir.ac.uk,3;6;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,University of Stirling;University of Stirling,481;481,350;350,5,5/25/19,0,0,0,0,0,0,4;156,5;41,2;6,0;2,m;m
4772,ICLR,2020,Neural Network Out-of-Distribution Detection for Regression Tasks,Geoff Pleiss;Amauri Souza;Joseph Kim;Boyi Li;Kilian Q. Weinberger,geoff@cs.cornell.edu;ahd64@cornell.edu;jk2569@cornell.edu;bl728@cornell.edu;kqw4@cornell.edu,3;3;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,5,0,yes,9/25/19,Cornell University;Cornell University;Cornell University;Cornell University;Cornell University,7;7;7;7;7,19;19;19;19;19,5,9/25/19,0,0,0,0,0,0,1536;162;241;32;24205,17;8;79;5;165,11;3;9;2;54,242;56;6;7;3866,m;m
4773,ICLR,2020,Stein Self-Repulsive Dynamics: Benefits from Past Samples,Mao Ye;Tongzheng Ren;Qiang Liu,lushleaf21@gmail.com;rtz19970824@gmail.com;lqiang@cs.utexas.edu,6;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0,yes,9/25/19,"University of Electronic Science and Technology of China;University of Texas, Austin;University of Texas, Austin",481;22;22,628;38;38,,9/25/19,1,1,0,0,0,0,2166;44;403,50;11;94,16;3;12,241;5;21,m;m
4774,ICLR,2020,Mesh-Free Unsupervised Learning-Based PDE Solver of Forward and Inverse problems,Leah Bar;Nir Sochen,barleah.libra@gmail.com;sochen@tauex.tau.ac.il,6;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,5,0,yes,9/25/19,Tel Aviv University;Tel Aviv University,35;35,188;188,,9/25/19,0,0,0,0,0,0,705;5270,38;224,15;37,58;401,f;m
4775,ICLR,2020,Gaussian Conditional Random Fields for Classification,Andrija Petrovic;Mladen Nikolic;Milos Jovanovic;Boris Delibasic,aapetrovic@mas.bg.ac.rs;nikolic@matf.bg.ac.rs;milos.jovanovic@fon.bg.ac.rs;boris.delibasic@fon.bg.ac.rs,1;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,University of Belgrade;University of Belgrade;University of Belgrade;University of Belgrade,481;481;481;481,1397;1397;1397;1397,11;10,1/31/19,4,0,0,0,4,0,214;243;367;413,32;32;90;64,5;9;10;11,14;9;17;13,m;m
4776,ICLR,2020,Visual Hide and Seek,Boyuan Chen;Shuran Song;Hod Lipson;Carl Vondrick,bchen@cs.columbia.edu;shurans@cs.columbia.edu;hod.lipson@columbia.edu;vondrick@cs.columbia.edu,3;3;8,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,6,0,yes,9/25/19,Columbia University;Columbia University;Columbia University;Columbia University,15;15;15;15,16;16;16;16,,9/25/19,2,1,0,0,0,0,125;6797;1121;4135,22;45;73;55,5;23;15;25,18;1456;28;474,m;m
4777,ICLR,2020,Supervised learning with incomplete data via sparse representations,Cesar F. Caiafa;Ziyao Wang;Jordi Solé-Casals;Qibin Zhao,ccaiafa@gmail.com;zy_wang@seu.edu.cn;jordi.sole@uvic.cat;qibin.zhao@riken.jp,6;6;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,CONICET;Southeast University;University of Victoria;RIKEN,-1;-1;172;-1,-1;-1;449;-1,,9/25/19,0,0,0,0,0,0,1287;0;849;637,58;4;148;10,13;0;16;4,112;0;29;44,m;m
4778,ICLR,2020,Transfer Active Learning For Graph Neural Networks,Shengding Hu;Meng Qu;Zhiyuan Liu;Jian Tang,hsd16@mails.tsinghua.edu.cn;meng.qu@umontreal.ca;liuzy@tsinghua.edu.cn;jian.tang@hec.ca,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Tsinghua University;University of Montreal;Tsinghua University;HEC Montreal,8;128;8;128,23;85;23;85,10;1;6,9/25/19,0,0,0,0,0,0,0;504;69;276,2;40;35;74,0;5;3;8,0;100;16;20,m;m
4779,ICLR,2020,Equivariant Entity-Relationship Networks,Devon Graham;Siamak Ravanbakhsh,drgraham@cs.ubc.ca;siamak@cs.mcgill.ca,3;3;3;8;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,11,0,yes,9/25/19,University of British Columbia;McGill University,35;86,34;42,1;10,3/21/19,0,0,0,0,0,0,828;1060,54;39,17;12,63;107,m;m
4780,ICLR,2020,Assessing Generalization in TD methods for Deep Reinforcement Learning,Emmanuel Bengio;Doina Precup;Joelle Pineau,bengioe@gmail.com;dprecup@cs.mcgill.ca;jpineau@cs.mcgill.ca,3;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,5,0,yes,9/25/19,McGill University;McGill University;McGill University,86;86;86,42;42;42,8,9/25/19,0,0,0,0,0,0,836;10265;11328,16;325;267,8;38;46,73;1114;1235,m;f
4781,ICLR,2020,Chordal-GCN: Exploiting sparsity in training large-scale graph convolutional networks,Xin Jiang*;Kewei Cheng*;Song Jiang*;Yizhou Sun,jiangxjames@ucla.edu;viviancheng@cs.ucla.edu;songjiang@cs.ucla.edu;yzsun@cs.ucla.edu,6;1;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,"University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles",20;20;20;20,17;17;17;17,10,9/25/19,0,0,0,0,0,0,338;13;185;6951,99;4;33;187,8;1;7;38,17;1;13;766,m;f
4782,ICLR,2020,Certifying Distributional Robustness using Lipschitz Regularisation,Zac Cranko;Zhan Shi;Xinhua Zhang;Simon Kornblith;Richard Nock,zac.cranko@anu.edu.au;zshi22@uic.edu;zhangx@uic.edu;skornblith@google.com;richard.nock@data61.csiro.au,6;3;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,"Australian National University;University of Illinois, Chicago;University of Illinois, Chicago;Google;, CSIRO",108;56;56;-1;233,50;254;254;-1;-1,4,9/25/19,0,0,0,0,0,0,30;42;32;1041;3450,11;25;29;50;237,3;3;3;12;29,1;11;0;133;324,m;m
4783,ICLR,2020,"Continuous Control with Contexts, Provably",Simon Du;Mengdi Wang;Ruosong Wang;Lin F. Yang,ssdu@ias.edu;mengdiw@princeton.edu;ruosongw@andrew.cmu.edu;linyang@ee.ucla.edu,3;1;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,"Institue for Advanced Study, Princeton;Princeton University;Carnegie Mellon University;University of California, Los Angeles",-1;31;1;20,-1;6;27;17,1,9/25/19,1,1,0,0,0,0,2135;43;13;678,56;40;3;82,20;4;2;14,322;2;0;44,m;m
4784,ICLR,2020,Using Hindsight to Anchor Past Knowledge in Continual Learning,Arslan Chaudhry;Albert Gordo;David Lopez-Paz;Puneet K. Dokania;Philip Torr,arslan.chaudhry@eng.ox.ac.uk;agordo@fb.com;david@lopezpaz.org;puneet@robots.ox.ac.uk;philip.torr@eng.ox.ac.uk,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,2,5,1,yes,9/25/19,University of Oxford;Facebook;Facebook;University of Oxford;University of Oxford,50;-1;-1;50;50,1;-1;-1;1;1,6,9/25/19,4,2,1,1,0,0,311;2814;2483;464;28788,7;44;46;24;356,5;20;19;9;84,73;576;428;68;3878,m;m
4785,ICLR,2020,Solving single-objective tasks by preference multi-objective reinforcement learning,Jinsheng Ren;Shangqi Guo;Feng Chen,rjs17@mails.tsinghua.edu.cn;gsq15@mails.tsinghua.edu.cn;chenfeng@mail.tsinghua.edu.cn,3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0,yes,9/25/19,Tsinghua University;Tsinghua University;Tsinghua University,8;8;8,23;23;23,,9/25/19,0,0,0,0,0,0,32;13;328,9;8;28,3;2;5,5;1;9,m;m
4786,ICLR,2020,Transfer Alignment Network for Double Blind Unsupervised Domain Adaptation,Huiwen Xu;U Kang,xuhuiwen33@gmail.com;ukang@snu.ac.kr,1;1;1,I have published one or two papers in this area.:I carefully checked the experiments.:N/A:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0,yes,9/25/19,Seoul National University;Seoul National University,41;41,64;64,6,9/25/19,0,0,0,0,0,0,32;20,8;9,3;3,0;2,m;m
4787,ICLR,2020,Policy Optimization by Local Improvement through Search,Jialin Song;Joe Wenjie Jiang;Amir Yazdanbakhsh;Ebrahim Songhori;Anna Goldie;Navdeep Jaitly;Azalia Mirhoseini,jssong@caltech.edu;wenjiej@google.com;ayazdan@google.com;esonghori@google.com;agoldie@google.com;ndjaitly@google.com;azalia@google.com,3;3;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,California Institute of Technology;Google;Google;Google;Google;Google;Google,143;-1;-1;-1;-1;-1;-1,2;-1;-1;-1;-1;-1;-1,,9/25/19,0,0,0,0,0,0,222;117;103;433;517;18147;1083,62;14;18;27;18;94;56,7;4;3;9;6;39;14,23;4;10;34;45;1605;79,m;f
4788,ICLR,2020,Regularly varying representation for sentence embedding,Hamid Jalalzai;Pierre Colombo;Chloé Clavel;Eric Gaussier;Giovanna Varni;Emmanuel Vignon;Anne Sabourin,hamid.jalalzai@telecom-paris.fr;pierre.colombo@telecom-paris.fr;chloe.clavel@telecom-paris.fr;giovanna.varni@telecom-paris.fr;emmanuel.vignon@fr.ibm.com;anne.sabourin@telecom-paris.fr,3;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Télécom Paris;Télécom Paris;Télécom Paris;Télécom Paris;International Business Machines;Télécom Paris,481;481;481;481;-1;481,187;187;187;187;-1;187,3,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,0;0;0;0;0;0;0,m;f
4789,ICLR,2020,Statistical Verification of General Perturbations by Gaussian Smoothing,Marc Fischer;Maximilian Baader;Martin Vechev,marcfisc@student.ethz.ch;mbaader@inf.ethz.ch;martin.vechev@inf.ethz.ch,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10,13;13;13,8,9/25/19,2,1,1,2,0,1,144;12;4246,25;5;153,7;2;36,6;1;467,m;m
4790,ICLR,2020,Learning to Anneal and Prune Proximity Graphs for Similarity Search,Minjia Zhang;Wenhan Wang;Yuxiong He,minjiaz@microsoft.com;wenhanw@microsoft.com;yuxhe@microsoft.com,6;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Microsoft;Microsoft;Microsoft,-1;-1;-1,-1;-1;-1,10,9/25/19,0,0,0,0,0,0,214;15;1657,12;6;102,3;2;24,27;4;142,m;f
4791,ICLR,2020,Unsupervised Learning of Graph Hierarchical Abstractions with Differentiable Coarsening and Optimal Transport,Tengfei Ma;Jie Chen,tengfei.ma1@ibm.com;chenjie@us.ibm.com,3;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,International Business Machines;International Business Machines,-1;-1,-1;-1,10,9/25/19,0,0,0,0,0,0,155;143,17;103,3;5,7;4,m;f
4792,ICLR,2020,Bayesian Variational Autoencoders for Unsupervised Out-of-Distribution Detection,Erik Daxberger;José Miguel Hernández-Lobato,ead54@cam.ac.uk;jmh233@cam.ac.uk,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,University of Cambridge;University of Cambridge,71;71,3;3,5;11,9/25/19,4,0,2,1,0,0,21;3824,5;114,3;28,1;420,m;m
4793,ICLR,2020,Robust Instruction-Following in a Situated Agent via Transfer-Learning from Text,Felix Hill;Sona Mokra;Nathaniel Wong;Tim Harley,felixhill@google.com;sonka@google.com;nathanielwong@google.com;tharley@google.com,3;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,3;6,9/25/19,0,0,0,0,0,0,3564;0;0;4173,52;2;4;14,22;0;0;7,675;0;0;816,m;m
4794,ICLR,2020,Fully Polynomial-Time Randomized Approximation Schemes for Global Optimization of High-Dimensional Folded Concave Penalized Generalized Linear Models,Charles Hernandez;Hungyi Lee;Hongchen Liu,cdhernandez@ufl.edu;hungyilee@ufl.edu;hliu@ise.ufl.edu,3;3;8,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,University of Florida;University of Florida;University of Florida,128;128;128,174;174;174,9,9/25/19,0,0,0,0,0,0,92;1119;344,14;134;57,7;18;11,6;76;11,m;m
4795,ICLR,2020,MANIFOLD FORESTS: CLOSING THE GAP ON NEURAL NETWORKS,Ronan Perry;Tyler M. Tomita;Jesse Patsolic;Benjamin Falk;Joshua Vogelstein,rperry27@jhu.edu;ttomita2@jhmi.edu;jpatsolic@jhu.edu;falk.ben@jhu.edu;jovo@jhu.edu,3;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,Johns Hopkins University;;Johns Hopkins University;Johns Hopkins University;Johns Hopkins University,73;-1;73;73;73,12;-1;12;12;12,,9/25/19,1,1,0,0,0,0,1;49;21;47;4455,2;8;6;22;188,1;3;3;4;29,0;2;0;1;338,m;m
4796,ICLR,2020,Granger Causal Structure Reconstruction from Heterogeneous Multivariate Time Series,Yunfei Chu;Xiaowei Wang;Chunyan Feng;Jianxin Ma;Jingren Zhou;Hongxia Yang,yfchu@bupt.edu.cn;daemon.wxw@alibaba-inc.com;cyfeng@bupt.edu.cn;jason.mjx@alibaba-inc.com;jingren.zhou@alibaba-inc.com;yang.yhx@alibaba-inc.com,3;8;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Beijing University of Post and Telecommunication;Alibaba Group;Beijing University of Post and Telecommunication;Alibaba Group;Alibaba Group;Alibaba Group,481;-1;481;-1;-1;-1,1397;-1;1397;-1;-1;-1,,9/25/19,0,0,0,0,0,0,176;46;10;74;3168;614,20;30;11;29;130;89,9;2;3;5;26;12,8;7;0;7;296;74,m;f
4797,ICLR,2020,Towards Disentangling Non-Robust and Robust Components in Performance Metric,Yujun Shi;Benben Liao;Guangyong Chen;Yun Liu;Ming-ming Cheng;Jiashi Feng,shiyujun1016@gmail.com;bliao@tencent.com;gycchen@tencent.com;nk12csly@mail.nankai.edu.cn;cmm@nankai.edu.cn;elefjia@nus.edu.sg,1;1;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Nankai University;Tencent AI Lab;Tencent AI Lab;Nankai University;Nankai University;National University of Singapore,481;-1;-1;481;481;16,366;-1;-1;366;366;25,4;8,6/6/19,1,0,0,0,0,0,126;83;407;2424;10716;9533,13;16;40;234;155;332,5;5;7;25;42;52,8;15;50;147;1571;1232,m;m
4798,ICLR,2020,Distributed Training Across the World,Ligeng Zhu;Yao Lu;Yujun Lin;Song Han,ligeng@mit.edu;luyao11175@gmail.com;yujunlin@mit.edu;songhan@mit.edu,3;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0,yes,9/25/19,Massachusetts Institute of Technology;;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;-1;2;2,5;-1;5;5,,9/25/19,0,0,0,0,0,0,480;409;509;0,15;91;21;11,5;9;9;0,133;22;83;0,m;m
4799,ICLR,2020,Risk Averse Value Expansion for Sample Efficient and Robust Policy Learning,Bo Zhou;Fan Wang;Hongsheng Zeng;Hao Tian,zhoubo01@baidu.com;wangfan04@baidu.com;zenghongsheng@baidu.com;tianhao@baidu.com,3;3;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,Baidu;Baidu;Baidu;Baidu,-1;-1;-1;-1,-1;-1;-1;-1,1,9/25/19,0,0,0,0,0,0,5;115;6;23,23;57;6;14,2;7;2;4,0;5;0;0,m;m
4800,ICLR,2020,MANAS: Multi-Agent Neural Architecture Search,Fabio Maria Carlucci;Pedro M Esperança;Marco Singh;Victor Gabillon;Antoine Yang;Hang Xu;Zewei Chen;Jun Wang,fabiom.carlucci@gmail.com;pedro.esperanca@huawei.com;marco.singh@huawei.com;victor.gabillon@huawei.com;antoineyang3@gmail.com;xu.hang@huawei.com;chen.zewei@huawei.com;w.j@huawei.com,6;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,4,0,yes,9/25/19,Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.;ENS Paris-Saclay;Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.,-1;-1;-1;-1;481;-1;-1;-1,-1;-1;-1;-1;644;-1;-1;-1,10,9/3/19,3,2,0,0,0,0,437;129;3;472;27;77;5;1340,18;16;3;26;2;22;7;123,10;5;1;9;2;4;2;13,62;10;0;42;4;6;0;67,m;m
4801,ICLR,2020,Neural Architecture Search by Learning Action Space for Monte Carlo Tree Search,Linnan Wang;Saining Xie;Teng Li;Rodrigo Fonseca;Yuandong Tian,linnan_wang@brown.edu;s9xie@fb.com;yuandong@fb.com;tengli@fb.com,6;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,Brown University;Facebook;Facebook;Facebook,67;-1;-1;-1,53;-1;-1;-1,11,9/25/19,1,1,0,0,0,0,232;5529;95;5845;2498,21;26;39;130;85,8;13;5;32;25,21;960;4;854;293,m;m
4802,ICLR,2020,A multi-task U-net for segmentation with lazy labels,Rihuan Ke;Aurélie Bugeau;Nicolas Papadakis;Peter Schuetz;Carola-Bibiane Schönlieb,rk621@cam.ac.uk;aurelie.bugeau@labri.fr;nicolas.papadakis@math.u-bordeaux.fr;peter.schuetz@unilever.com;cbs31@cam.ac.uk,6;6;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0,yes,9/25/19,University of Cambridge;LaBRI;CNRS/IMB;Unilever;University of Cambridge,71;481;-1;-1;71,3;646;-1;-1;3,2,6/20/19,3,1,0,0,0,0,128;782;1398;774;1247,6;50;103;90;103,3;12;20;16;19,5;53;103;46;74,m;f
4803,ICLR,2020,Deep automodulators,Ari Heljakka;Yuxin Hou;Juho Kannala;Arno Solin,ari.heljakka@aalto.fi;yuxin.hou@aalto.fi;arno.solin@aalto.fi;juho.kannala@aalto.fi,3;3;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,4,0,yes,9/25/19,Aalto University;Aalto University;Aalto University;Aalto University,143;143;143;143,182;182;182;182,5,9/25/19,0,0,0,0,0,0,127;11;3645;3700,23;18;127;14,5;2;24;4,7;1;499;482,m;m
4804,ICLR,2020,Towards Certified Defense for Unrestricted Adversarial Attacks,Shengjia Zhao;Yang Song;Stefano Ermon,sjzhao@stanford.edu;yangsong@cs.stanford.edu;ermon@cs.stanford.edu,1;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,1,0,yes,9/25/19,Stanford University;Stanford University;Stanford University,4;4;4,4;4;4,4;1,9/25/19,0,0,0,0,0,0,592;181;4975,29;85;203,13;7;31,98;17;664,m;m
4805,ICLR,2020,Best feature performance in codeswitched hate speech texts,Edward Ombui;Lawrence Muchemi;Peter Wagacha,eombui@anu.ac.ke;lmuchemi@uonbi.ac.ke;waiganjo@uonbi.ac.ke,3;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,0,0,yes,9/25/19,Africa Nazarene University;University of Nairobi;University of Nairobi,481;481;481,1397;871;871,,9/25/19,0,0,0,0,0,0,1;24;308,5;23;65,1;3;9,0;2;14,m;m
4806,ICLR,2020,Evolutionary Reinforcement Learning for Sample-Efficient Multiagent Coordination,Shauharda Khadka;Somdeb Majumdar;Santiago Miret;Stephen McAleer;Kagan Tumer,shauharda.khadka@intel.com;somdeb.majumdar@intel.com;santiago.miret@intel.com;smcaleer@uci.edu;kagan.tumer@oregonstate.edu,1;6;8,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,5,0,yes,9/25/19,"Intel;Intel;Intel;University of California, Irvine;Oregon State University",-1;-1;-1;35;77,-1;-1;-1;96;373,,6/18/19,5,5,0,0,0,0,97;44;19;-1;4255,16;12;6;-1;247,5;5;2;-1;33,15;4;3;0;275,m;m
4807,ICLR,2020,Gaussian Process Meta-Representations Of Neural Networks,Theofanis Karaletsos;Thang Bui,theofanis.karaletsos@gmail.com;thang.buivn@gmail.com,6;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,12,1,yes,9/25/19,;Uber,-1;-1,-1;-1,11,9/25/19,0,0,0,0,0,0,454;697,28;30,8;12,62;94,m;m
4808,ICLR,2020,Training Deep Neural Networks with Partially Adaptive Momentum,Jinghui Chen;Dongruo Zhou;Yiqi Tang;Ziyan Yang;Yuan Cao;Quanquan Gu,jc4zg@virginia.edu;drzhou@cs.ucla.edu;yt6ze@virginia.edu;zy3cx@virginia.edu;yuanc@princeton.edu;qgu@cs.ucla.edu,1;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,"University of Virginia;University of California, Los Angeles;University of Virginia;University of Virginia;Princeton University;University of California, Los Angeles",59;20;59;59;31;20,107;17;107;107;6;17,1;9;8,9/25/19,0,0,0,0,0,0,362;349;48;4;147;3895,44;18;5;8;48;174,11;7;2;1;7;34,39;35;8;1;15;411,m;m
4809,ICLR,2020,Scalable Deep Neural Networks via Low-Rank Matrix Factorization,Atsushi Yaguchi;Taiji Suzuki;Shuhei Nitta;Yukinobu Sakata;Akiyuki Tanizawa,atsushi.yaguchi@toshiba.co.jp;taiji@mist.i.u-tokyo.ac.jp;shuhei.nitta@toshiba.co.jp;yuki.sakata@toshiba.co.jp;akiyuki.tanizawa@toshiba.co.jp,3;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,1,4,0,yes,9/25/19,Toshiba Memory;The University of Tokyo;Toshiba Memory;Toshiba Memory;Toshiba Memory,-1;56;-1;-1;-1,-1;36;-1;-1;-1,,9/25/19,2,0,1,0,0,0,40;2318;21;37;79,9;177;20;24;18,4;26;3;4;5,2;237;1;0;5,m;m
4810,ICLR,2020,Antifragile and Robust Heteroscedastic Bayesian Optimisation,Ryan Rhys-Griffiths;Miguel Garcia-Ortegon;Alexander A. Aldrick;Alpha A. Lee,rrg27@cam.ac.uk;mg770@cam.ac.uk;av495@cam.ac.uk;aal44@cam.ac.uk,3;1;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,1,0,yes,9/25/19,University of Cambridge;University of Cambridge;University of Cambridge;University of Cambridge,71;71;71;71,3;3;3;3,11,9/25/19,0,0,0,0,0,0,0;1;1;748,1;3;2;52,0;1;1;15,0;0;0;15,m;m
4811,ICLR,2020,Compressed Sensing with Deep Image Prior and Learned Regularization,Dave Van Veen;Ajil Jalal;Mahdi Soltanolkotabi;Eric Price;Sriram Vishwanath;Alexandros G. Dimakis,davemvanveen@gmail.com;ajiljalal@utexas.edu;soltanol@usc.edu;ecprice@cs.utexas.edu;sriram@austin.utexas.edu;dimakis@austin.utexas.edu,3;6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,"University of Texas, Austin;University of Texas, Austin;University of Southern California;University of Texas, Austin;University of Texas, Austin;University of Texas, Austin",22;22;31;22;22;22,38;38;62;38;38;38,5;1,6/17/18,51,25,13,2,50,3,51;396;2984;2786;9324;12219,2;11;56;63;283;204,1;5;20;22;38;50,3;66;354;399;793;1439,m;m
4812,ICLR,2020,Unsupervised Hierarchical Graph Representation Learning with Variational Bayes,Shashanka Ubaru;Jie Chen,shashanka.ubaru@ibm.com;chenjie@us.ibm.com,3;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,International Business Machines;International Business Machines,-1;-1,-1;-1,1;10,9/25/19,1,0,0,0,0,0,163;143,23;103,8;5,15;4,m;f
4813,ICLR,2020,Learning Algorithmic Solutions to Symbolic Planning Tasks with a Neural Computer,Daniel Tanneberg;Elmar Rueckert;Jan Peters,daniel@robot-learning.de;rueckert@rob.uni-luebeck.de;mail@jan-peters.net,6;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,TU Darmstadt;University of Luebeck;TU Darmstadt,64;266;64,289;1397;289,8,9/25/19,0,0,0,0,0,0,52;379;15743,12;24;461,4;5;61,3;33;1278,m;m
4814,ICLR,2020,Multi-Step Decentralized Domain Adaptation,Akhil Mathur;Shaoduo Gan;Anton Isopoussu;Fahim Kawsar;Nadia Berthouze;Nicholas D. Lane,akhilmathurs@gmail.com;sgan@inf.ethz.ch;anton.isopoussu@gmail.com;fahim.kawsar@gmail.com;nadia.berthouze@gmail.com;nicholasd.lane@gmail.com,6;3;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0,yes,9/25/19,University College London;Swiss Federal Institute of Technology;;;;University of Oxford,50;10;-1;-1;-1;50,15;13;-1;-1;-1;1,4,9/25/19,0,0,0,0,0,0,25;74;43;2901;145;10358,14;7;15;175;28;170,3;3;3;21;7;45,2;16;2;188;7;704,m;m
4815,ICLR,2020,Robust saliency maps with distribution-preserving decoys,Yang Young Lu;Wenbo Guo;Xinyu Xing;William Stafford Noble,ylu465@uw.edu;wzg13@ist.psu.edu;xxing@ist.psu.edu;william-noble@uw.edu,3;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,"University of Washington, Seattle;Pennsylvania State University;Pennsylvania State University;University of Washington, Seattle",6;41;41;6,26;78;78;26,4,9/25/19,0,0,0,0,0,0,393;0;53;35054,16;12;9;402,7;0;2;76,37;0;0;3116,m;m
4816,ICLR,2020,Towards Interpreting Deep Neural Networks via Understanding Layer Behaviors,Jiezhang Cao;Jincheng Li;Xiping Hu;Peilin Zhao;Mingkui Tan,secaojiezhang@mail.scut.edu.cn;sejinchengli@mail.scut.edu.cn;huxp@lzu.edu.cn;peilinzhao@hotmail.com;mingkuitan@scut.edu.cn,3;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,4,0,yes,9/25/19,South China University of Technology;South China University of Technology;Tsinghua University;;South China University of Technology,481;481;8;-1;481,501;501;23;-1;501,1,9/25/19,0,0,0,0,0,0,87;0;1913;3602;1889,17;3;101;133;113,5;0;23;31;25,2;0;57;393;171,m;m
4817,ICLR,2020,Robust Graph Representation Learning via Neural Sparsification,Cheng Zheng;Bo Zong;Wei Cheng;Dongjin Song;Jingchao Ni;Wenchao Yu;Haifeng Chen;Wei Wang,chengzheng@cs.ucla.edu;bzong@nec-labs.com;weicheng@nec-labs.com;dsong@nec-labs.com;jni@nec-labs.com;yuwenchao@ucla.edu;haifeng@nec-labs.com;weiwang@cs.ucla.edu,6;1;8,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,"University of California, Los Angeles;NEC-Labs;NEC-Labs;NEC-Labs;NEC-Labs;University of California, Los Angeles;NEC-Labs;University of California, Los Angeles",20;-1;-1;-1;-1;20;-1;20,17;-1;-1;-1;-1;17;-1;17,10,9/25/19,1,1,0,0,0,0,1739;611;48;789;229;418;2635;106,204;53;39;40;31;37;156;137,22;11;4;13;8;11;30;5,56;64;2;77;15;31;231;10,m;f
4818,ICLR,2020,Good Semi-supervised VAE Requires Tighter Evidence Lower Bound,Haozhe Feng;Kezhi Kong;Tianye Zhang;Siyue Xue;Wei Chen,fenghz@zju.edu.cn;kong@cs.umd.edu;zhangtianye1026@zju.edu.cn;3160104527@zju.edu.cn;chenwei@cad.zju.edu.cn,3;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,4,0,yes,9/25/19,"Zhejiang University;University of Maryland, College Park;Zhejiang University;Zhejiang University;Zhejiang University",56;12;56;56;56,107;91;107;107;107,5,9/25/19,0,0,0,0,0,0,26;4;50;0;228,7;3;5;1;175,3;1;2;0;9,0;0;4;0;9,m;m
4819,ICLR,2020,EDUCE: Explaining model Decision through Unsupervised Concepts Extraction,Diane Bouchacourt;Ludovic Denoyer,dianeb@fb.com;denoyer@fb.com,6;3;8,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,9,0,yes,9/25/19,Facebook;Facebook,-1;-1,-1;-1,,5/28/19,8,4,1,0,0,0,197;3075,16;129,5;22,20;536,f;m
4820,ICLR,2020,Projected Canonical Decomposition for Knowledge Base Completion,Timothée Lacroix;Guillaume Obozinski;Joan Bruna;Nicolas Usunier,timothee.lax@gmail.com;guillaume.obozinski@epfl.ch;bruna@cims.nyu.edu;usunier@fb.com,3;8;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Facebook;Swiss Federal Institute of Technology Lausanne;New York University;Facebook,-1;481;25;-1,-1;38;29;-1,,9/25/19,0,0,0,0,0,0,197;5381;1;6244,5;65;9;109,3;29;1;30,41;581;0;1186,m;m
4821,ICLR,2020,Lossless Data Compression with Transformer,Gautier Izacard;Armand Joulin;Edouard Grave,gizacard@gmail.com;ajoulin@fb.com;egrave@fb.com,3;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,3,0,yes,9/25/19,Ecole polytechnique;Facebook;Facebook,481;-1;-1,93;-1;-1,3,9/25/19,0,0,0,0,0,0,24;10653;7711,5;74;57,2;32;23,0;1533;1136,m;m
4822,ICLR,2020,Stochastically Controlled Compositional Gradient for the Composition problem,Liu Liu;Ji Liu;Cho-Jui Hsieh;Dacheng Tao,liu.liu1@sydney.edu.au;ji.liu.uwisc@gmail.com;chohsieh@cs.ucla.edu;dacheng.tao@sydney.edu.au,6;3;3,I have read many papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0,yes,9/25/19,"University of Sydney;University of Rochester;University of California, Los Angeles;University of Sydney",86;100;20;86,60;173;17;60,,9/25/19,0,0,0,0,0,0,652;19;12827;80,109;38;168;29,12;2;41;5,39;0;1746;4,f;m
4823,ICLR,2020,Domain Adaptation via Low-Rank Basis Approximation,Christoph Raab;Frank-Michael Schleif,christoph.raab@fhws.de;frank-michael.schleif@fhws.de,1;3;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,University of Applied Sciences Würzburg-Schweinfurt;University of Applied Sciences Würzburg-Schweinfurt,481;481,1397;1397,,7/2/19,1,0,0,0,0,0,72;1162,34;183,5;16,0;35,m;m
4824,ICLR,2020,SGD with Hardness Weighted Sampling for Distributionally Robust Deep Learning,Lucas Fidon;Sebastien Ourselin;Tom Vercauteren,lucas.fidon@kcl.ac.uk;sebastien.ourselin@kcl.ac.uk;tom.vercauteren@kcl.ac.uk,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0,yes,9/25/19,King's College London;King's College London;King's College London,172;172;172,36;36;36,1,9/25/19,0,0,0,0,0,0,454;1564;698,12;100;38,6;17;7,30;109;30,m;m
4825,ICLR,2020,Fast Training of Sparse Graph Neural Networks on Dense Hardware,Matej Balog;Bart van Merriënboer;Subhodeep Moitra;Yujia Li;Daniel Tarlow,matej.balog@gmail.com;bartvm@google.com;smoitra@google.com;yujiali@google.com;dtarlow@google.com,3;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,12,0,yes,9/25/19,University of Cambridge;Google;Google;Google;Google,71;-1;-1;-1;-1,3;-1;-1;-1;-1,10,6/27/19,3,0,0,0,0,0,274;12971;416;268;2565,8;14;12;23;69,4;8;6;5;23,41;2189;58;39;310,m;m
4826,ICLR,2020,CGT: Clustered Graph Transformer for Urban Spatio-temporal Prediction,Xu Geng;Lingyu Zhang;Shulin Li;Yuanbo Zhang;Lulu Zhang;Leye Wang;Qiang Yang;Hongtu Zhu;Jieping Ye,xgeng@connect.ust.hk;zhanglingyu@didiglobal.com;lishulin_i@didiglobal.com;bozhangyuanbo_i@didiglobal.com;zhanglulululu@didiglobal.com;leyewang@pku.edu.cn;qyang@cse.ust.hk;zhuhongtu@didiglobal.com;yejieping@didiglobal.com,3;3;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,"The Hong Kong University of Science and Technology;DiDi AI Labs, Didi Chuxing;DiDi AI Labs, Didi Chuxing;DiDi AI Labs, Didi Chuxing;DiDi AI Labs, Didi Chuxing;Peking University;The Hong Kong University of Science and Technology;DiDi AI Labs, Didi Chuxing;DiDi AI Labs, Didi Chuxing",39;-1;-1;-1;-1;22;39;-1;-1,47;-1;-1;-1;-1;24;47;-1;-1,10,9/25/19,1,0,0,0,0,0,46;1;39;201;52;1735;1407;2;521,28;3;18;13;36;82;159;6;44,4;1;4;6;4;22;17;1;8,1;0;1;2;0;115;49;0;46,m;m
4827,ICLR,2020,Regularizing Black-box Models for Improved Interpretability,Gregory Plumb;Maruan Al-Shedivat;Eric Xing;Ameet Talwalkar,gdplumb@andrew.cmu.edu;alshedivat@cs.cmu.edu;epxing@cs.cmu.edu;talwalkar@cmu.edu,6;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,5,0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1,27;27;27;27,,2/18/19,6,4,3,1,0,1,42;750;25065;6515,26;35;605;79,3;13;77;34,3;79;2695;768,m;m
4828,ICLR,2020,AlgoNet: $C^\infty$ Smooth Algorithmic Neural Networks,Felix Petersen;Christian Borgelt;Oliver Deussen,felix.petersen@uni.kn;christian@borgelt.net;oliver.deussen@uni.kn,1;3;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,;;,-1;-1;-1,-1;-1;-1,,5/16/19,0,0,0,0,0,0,25;3368;6707,22;204;288,2;27;41,1;269;427,m;m
4829,ICLR,2020,CAPACITY-LIMITED REINFORCEMENT LEARNING: APPLICATIONS IN DEEP ACTOR-CRITIC METHODS FOR CONTINUOUS CONTROL,Tyler James Malloy;Matthew Riemer;Miao Liu;Tim Klinger;Gerald Tesauro;Chris R. Sims,mallot@rpi.edu;mdriemer@us.ibm.com;miao.liu1@ibm.com;tklinger@us.ibm.com;gtesauro@us.ibm.com;simsc3@rpi.edu,3;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,Rensselaer Polytechnic Institute;International Business Machines;International Business Machines;International Business Machines;International Business Machines;Rensselaer Polytechnic Institute,172;-1;-1;-1;-1;172,438;-1;-1;-1;-1;438,8,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,0;0;0;0;0;0,m;m
4830,ICLR,2020,AdvCodec: Towards A Unified Framework for Adversarial Text Generation,Boxin Wang;Hengzhi Pei;Han Liu;Bo Li,boxinw2@illinois.edu;hzpei16@fudan.edu.cn;hanliu@northwestern.edu;lbo@illinois.edu,3;6;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,7,0,yes,9/25/19,"University of Illinois, Urbana Champaign;Fudan University;Northwestern University;University of Illinois, Urbana Champaign",3;79;44;3,48;109;22;48,3;4,9/25/19,4,3,1,0,0,1,41;10;299;246,7;5;115;147,3;2;10;9,8;2;19;26,m;f
4831,ICLR,2020,Function Feature Learning of Neural Networks,Guangcong Wang;Jianhuang Lai;Guangrun Wang;Wenqi Liang,wanggc3@mail2.sysu.edu.cn;stsljh@mail.sysu.edu.cn;wanggrun@mail2.sysu.edu.cn;liangwq8@mail2.sysu.edu.cn,3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,3,1,yes,9/25/19,SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY,481;481;481;481,299;299;299;299,,9/25/19,0,0,0,0,0,0,107;331;107;29,12;61;12;10,6;10;6;2,8;28;8;1,m;m
4832,ICLR,2020,Unsupervised domain adaptation with imputation,Matthieu Kirchmeyer;Patrick Gallinari;Alain Rakotomamonjy;Amin Mantrach,m.kirchmeyer@criteo.com;patrick.gallinari@lip6.fr;a.rakotomamonjy@criteo.com;a.mantrach@criteo.com,8;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Criteo;LIP6;Criteo;Criteo,-1;-1;-1;-1,-1;-1;-1;-1,,9/25/19,0,0,0,0,0,0,13;75;4682;579,5;24;137;35,2;5;28;13,1;6;561;60,m;m
4833,ICLR,2020,Path Space for Recurrent Neural Networks with ReLU Activations,Yue Wang;Qi Meng;Wei Chen;Yuting Liu;Zhi-Ming Ma;Tie-Yan Liu,11271012@bjtu.edu.cn;meq@microsoft.com;wche@microsoft.com;ytliu@bjtu.edu.cn;mazm@amt.ac.cn;tie-yan.liu@microsoft.com,3;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Beijing jiaotong univercity;Microsoft;Microsoft;Beijing jiaotong univercity;Chinese Academy of Sciences;Microsoft,481;-1;-1;481;59;-1,952;-1;-1;952;1397;-1,1;10,9/25/19,0,0,0,0,0,0,-1;301;228;427;871;13552,-1;58;175;49;59;369,-1;11;9;11;14;51,0;7;9;27;81;1723,m;m
4834,ICLR,2020,Fast Machine Learning with Byzantine Workers and Servers,El-Mahdi El-Mhamdi;Rachid Guerraoui;Arsany Guirguis,elmahdi.elmhamdi@epfl.ch;rachid.guerraoui@epfl.ch;arsany.guirguis@epfl.ch,3;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne,481;481;481,38;38;38,1,9/25/19,1,1,1,0,0,0,351;15462;72,25;579;14,9;53;5,64;1508;6,m;m
4835,ICLR,2020,Topology-Aware Pooling via Graph Attention,Hongyang Gao;Shuiwang Ji,hongyang.gao@tamu.edu;sji@tamu.edu,3;6;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0,yes,9/25/19,Texas A&M;Texas A&M,44;44,177;177,3;2;10,9/25/19,0,0,0,0,0,0,289;8754,21;136,8;35,39;728,m;m
4836,ICLR,2020,Stochastic Latent Actor-Critic: Deep Reinforcement Learning with a Latent Variable Model,Alex X. Lee;Anusha Nagabandi;Pieter Abbeel;Sergey Levine,alexlee_gk@cs.berkeley.edu;nagaban2@berkeley.edu;pabbeel@cs.berkeley.edu;svlevine@eecs.berkeley.edu,8;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5,13;13;13;13,5,7/1/19,25,14,12,3,0,5,1360;616;37294;24893,18;16;438;310,15;9;94;74,153;56;4481;3235,m;m
4837,ICLR,2020,Localised Generative Flows,Rob Cornish;Anthony Caterini;George Deligiannidis;Arnaud Doucet,rcornish@robots.ox.ac.uk;anthony.caterini@stats.ox.ac.uk;deligian@stats.ox.ac.uk;doucet@stats.ox.ac.uk,3;3;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,6,0,yes,9/25/19,University of Oxford;University of Oxford;University of Oxford;University of Oxford,50;50;50;50,1;1;1;1,5,9/25/19,6,3,1,0,0,1,11;80;449;21104,5;13;31;246,2;5;8;52,2;7;60;2619,m;m
4838,ICLR,2020,Guided Adaptive Credit Assignment for Sample Efficient Policy Optimization,Hao Liu;Richard Socher;Caiming Xiong,lhao499@gmail.com;rsocher@salesforce.com;cxiong@salesforce.com,3;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,University of California Berkeley;SalesForce.com;SalesForce.com,5;-1;-1,13;-1;-1,3,9/25/19,0,0,0,0,0,0,144;54922;6431,78;180;156,6;49;31,6;9057;1071,m;m
4839,ICLR,2020,Proactive Sequence Generator via Knowledge Acquisition,Qing Sun;James Cross;Dmitriy Genzel,qingsun@fb.com;jcross@fb.com;dgenzel@fb.com,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,Facebook;Facebook;Facebook,-1;-1;-1,-1;-1;-1,3,9/25/19,0,0,0,0,0,0,70;2522;369,68;129;15,4;17;6,2;155;32,f;m
4840,ICLR,2020,Reinforcement Learning without Ground-Truth State,Xingyu Lin;Harjatin Singh Baweja;David Held,xlin3@cs.cmu.edu;dheld@andrew.cmu.edu;harjatis@andrew.cmu.edu,3;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1,27;27;27,,5/20/19,1,0,1,0,0,0,285;38;834,32;11;43,10;3;7,6;2;72,m;m
4841,ICLR,2020,Generalized Inner Loop Meta-Learning,Edward Grefenstette;Brandon Amos;Denis Yarats;Phu Mon Htut;Artem Molchanov;Franziska Meier;Douwe Kiela;Kyunghyun Cho;Soumith Chintala,egrefen@gmail.com;brandon.amos.cs@gmail.com;denisyarats@cs.nyu.edu;pmh330@nyu.edu;a.molchanov86@gmail.com;fmeier@fb.com;dkiela@fb.com;kyunghyun.cho@nyu.edu;soumith@gmail.com,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,10,0,yes,9/25/19,Facebook;Facebook;New York University;New York University;;Facebook;Facebook;New York University;Facebook,-1;-1;25;25;-1;-1;-1;25;-1,-1;-1;29;29;-1;-1;-1;29;-1,1;6,9/25/19,12,5,4,0,0,1,7175;1981;1668;106;183;119;3454;46450;19496,57;40;12;13;18;54;80;272;31,25;19;7;5;9;5;29;52;18,859;221;251;13;11;5;588;6610;2886,m;m
4842,ICLR,2020,Channel Equilibrium Networks,Wenqi Shao;Shitao Tang;Xingang Pan;Ping Tan;Xiaogang Wang;Ping Luo,weqish@link.cuhk.edu.hk;shitaot@sfu.ca;px117@ie.cuhk.edu.hk;pingtan@sfu.ca;xgwang@ee.cuhk.edu.hk;pluo.lhi@gmail.com,3;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,The Chinese University of Hong Kong;Simon Fraser University;The Chinese University of Hong Kong;Simon Fraser University;The Chinese University of Hong Kong;The University of Hong Kong,59;64;59;64;59;92,35;272;35;272;35;35,,9/25/19,1,0,0,0,0,0,1;25;208;335;2598;80,3;9;23;29;138;33,1;3;4;5;21;4,0;0;37;60;389;6,m;m
4843,ICLR,2020,Zeroth Order Optimization by a Mixture of Evolution Strategies,Jun-Kun Wang;Xiaoyun Li;Ping Li,jimwang@gatech.edu;xl374@scarletmail.rutgers.edu;liping11@baidu.com,3;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,Georgia Institute of Technology;Rutgers University;Baidu,13;34;-1,38;168;-1,,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1,-1;-1;-1,-1;-1;-1,0;0;0,m;m
4844,ICLR,2020,Limitations for Learning from Point Clouds,Christian Bueno;Alan G. Hylton,christianbueno@ucsb.edu;alan.g.hylton@nasa.gov,8;3;3,I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,4,0,yes,9/25/19,UC Santa Barbara;NASA,38;-1,57;-1,1,9/25/19,0,0,0,0,0,0,0;80,2;6,0;2,0;4,m;m
4845,ICLR,2020,WORD SEQUENCE PREDICTION FOR AMHARIC LANGUAGE,Nuniyat Kifle;Ermias Abebe,nunukifle2@gmail.com;ermiasabebe@gmail.com,1;1;1,I have published in this field for several years.:N/A:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:N/A:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,Addis Ababa University;,481;-1,1397;-1,3,10/1/14,0,0,0,0,0,0,-1;-1,-1;-1,-1;-1,0;0,m;m
4846,ICLR,2020,Learning Underlying Physical Properties From Observations For Trajectory Prediction,Ekaterina Nikonova;Jochen Renz,ekaterina.nikonova@anu.edu.au;jochen.renz@anu.edu.au,1;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,4,0,yes,9/25/19,Australian National University;Australian National University,108;108,50;50,,9/25/19,0,0,0,0,0,0,51;2639,34;111,3;25,0;296,f;m
4847,ICLR,2020,Provable Representation Learning for Imitation Learning via Bi-level Optimization,Sanjeev Arora;Simon S. Du;Sham Kakade;Yuping Luo;Nikunj Saunshi,arora@cs.princeton.edu;ssdu@ias.edu;sham@cs.washington.edu;yupingl@cs.princeton.edu;nsaunshi@cs.princeton.edu,3;3;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,"Princeton University;Institue for Advanced Study, Princeton;University of Washington;Princeton University;Princeton University",31;-1;6;31;31,6;-1;26;6;6,1,9/25/19,0,0,0,0,0,0,2317;81;13740;0;145,82;14;198;2;8,14;5;58;0;4,254;4;1986;0;32,m;m
4848,ICLR,2020,Anchor & Transform: Learning Sparse Representations of Discrete Objects,Paul Pu Liang;Manzil Zaheer;Yuan Wang;Amr Ahmed,pliang@cs.cmu.edu;manzilzaheer@google.com;yuanwang@google.com;amra@google.com,3;3;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,5,0,yes,9/25/19,Carnegie Mellon University;Google;Google;Google,1;-1;-1;-1,27;-1;-1;-1,3,9/25/19,0,0,0,0,0,0,672;1647;-1;5221,44;63;-1;148,13;17;-1;29,109;275;0;535,m;m
4849,ICLR,2020,EgoMap: Projective mapping and structured egocentric memory for Deep RL,Edward Beeching;Christian Wolf;Jilles Dibangoye;Olivier Simonin,edward.beeching@inria.fr;christian.wolf@insa-lyon.fr;jilles.dibangoye@inria.fr;olivier.simonin@inria.fr,6;3;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,5,0,yes,9/25/19,INRIA;INSA de Lyon;INRIA;INRIA,-1;481;-1;-1,-1;1397;-1;-1,8,9/25/19,0,0,0,0,0,0,9;2941;364;1050,2;113;49;190,1;28;10;16,0;236;38;50,m;m
4850,ICLR,2020,Few-shot Learning by Focusing on Differences,Muhammad Rizki Maulana;Lee Wee Sun,maulana@comp.nus.edu.sg;leews@comp.nus.edu.sg,3;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,National University of Singapore;National University of Singapore,16;16,25;25,6,9/25/19,0,0,0,0,0,0,3;0,17;1,1;0,0;0,m;m
4851,ICLR,2020,On the Parameterization of Gaussian Mean Field Posteriors in Bayesian Neural Networks,Jakub Świątkowski;Kevin Roth;Bastiaan S. Veeling;Linh Tran;Joshua V. Dillon;Jasper Snoek;Stephan Mandt;Tim Salimans;Rodolphe Jenatton;Sebastian Nowozin,kuba.swiatkowski@gmail.com;kevin.roth@inf.ethz.ch;basveeling@gmail.com;linh.tran@imperial.ac.uk;jvdillon@google.com;jaspersnoek@gmail.com;stephan.mandt@gmail.com;salimans@google.com;rjenatton@google.com;nowozin@google.com,3;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,3,0,yes,9/25/19,";Swiss Federal Institute of Technology;Google;Imperial College London;Google;Google;University of California, Irvine;Google;Google;Google",-1;10;-1;73;-1;-1;35;-1;-1;-1,-1;13;-1;10;-1;-1;96;-1;-1;-1,11,9/25/19,1,1,0,0,0,0,21;33;99;2491;1011;4985;1300;6754;3562;18,6;8;14;94;28;61;67;35;39;9,3;4;5;21;13;18;17;14;20;3,4;6;19;214;166;512;119;1054;334;1,m;m
4852,ICLR,2020,Deep unsupervised feature selection,Ian Covert;Uygar Sumbul;Su-In Lee,icovert@cs.washington.edu;uygars@alleninstitute.org;suinlee@cs.washington.edu,3;1;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,University of Washington;Allen Institute;University of Washington,6;-1;6,26;-1;26,1,9/25/19,0,0,0,0,0,0,27;1;189,4;3;10,2;1;2,5;0;20,m;f
4853,ICLR,2020,Efficient Bi-Directional Verification of ReLU Networks via Quadratic Programming,Aleksei Kuvshinov;Stephan Guennemann,kuvshino@in.tum.de;guennemann@in.tum.de,3;8;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Technical University Munich;Technical University Munich,53;53,43;43,4;1,9/25/19,0,0,0,0,0,0,1;2528,7;139,1;28,0;291,m;m
4854,ICLR,2020,AdaScale SGD: A Scale-Invariant Algorithm for Distributed Training,Tyler B. Johnson;Pulkit Agrawal;Haijie Gu;Carlos Guestrin,tbjohns@apple.com;pulkit_agrawal@apple.com;jaygu@apple.com;guestrin@apple.com,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,2,4,0,yes,9/25/19,Apple;Apple;Apple;Apple,-1;-1;-1;-1,-1;-1;-1;-1,3;2,9/25/19,0,0,0,0,0,0,117;3022;1338;28796,26;53;12;217,5;16;3;66,11;252;321;4005,m;m
4855,ICLR,2020,Small-GAN: Speeding up GAN Training using Core-Sets,Samarth Sinha;Han Zhang;Anirudh Goyal;Yoshua Bengio;Hugo Larochelle;Augustus Odena,samarth.sinha@mail.utoronto.ca;zhanghan@google.com;anirudhgoyal9119@gmail.com;yoshua.bengio@mila.quebec;hugolarochelle@google.com;augustusodena@google.com,3;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,9,1,yes,9/25/19,Toronto University;Google;University of Montreal;University of Montreal;Google;Google,18;-1;128;128;-1;-1,18;-1;85;85;-1;-1,5;4,9/25/19,4,0,0,0,0,0,115;801;1137;208566;25332;3293,15;104;46;807;124;25,4;15;12;147;44;13,23;38;130;24297;2884;477,m;m
4856,ICLR,2020,AUGMENTED POLICY GRADIENT METHODS FOR EFFICIENT REINFORCEMENT LEARNING,Kai Lagemann;Gregor Roering;Christoph Henke;Rene Vossen;Frank Hees,kai.lagemann@rwth-aachen.de;gregor.roering@rwth-aachen.de;christoph.henke@ifu.rwth-aachen.de;rene.vossen@ifu.rwth-aachen.de;hees.office@ima-ifu.rwth-aachen.de,1;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,0,0,yes,9/25/19,RWTH Aachen University;RWTH Aachen University;RWTH Aachen University;RWTH Aachen University;RWTH Aachen University,95;95;95;95;95,98;98;98;98;98,,9/25/19,0,0,0,0,0,0,0;0;9;73;1,1;1;22;39;14,0;0;2;3;1,0;0;1;5;0,m;m
4857,ICLR,2020,Neural Approximation of an Auto-Regressive Process through Confidence Guided Sampling,YoungJoon Yoo;Sanghyuk Chun;Jaejun Yoo;Sangdoo Yun;Jung Woo Ha,youngjoon.yoo@navercorp.com;sanghyuk.c@navercorp.com;jaejun.yoo@navercorp.com;sangdoo.yun@navercorp.com;jungwoo.ha@navercorp.com,6;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,NAVER;NAVER;NAVER;NAVER;NAVER,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,,9/25/19,0,0,0,0,0,0,118;153;206;1032;1945,9;18;13;38;67,3;5;6;11;15,31;43;17;204;376,m;m
4858,ICLR,2020,Certifiably Robust Interpretation in Deep Learning,Alexander Levine;Sahil Singla;Soheil Feizi,alevine0@cs.umd.edu;ssingla@cs.umd.edu;sfeizi@cs.umd.edu,3;1;3,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,6,1,yes,9/25/19,"University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park",12;12;12,91;91;91,4,5/28/19,10,4,2,0,4,0,126;427;3362,28;41;12,5;11;6,5;35;233,m;m
4859,ICLR,2020,Towards More Realistic Neural Network Uncertainties,Joachim Sicking;Alexander Kister;Matthias Fahrland;Stefan Eickeler;Fabian Hueger;Stefan Rueping;Peter Schlicht;Tim Wirtz,joachim.sicking@iais.fraunhofer.de;alexander.kister@iais.fraunhofer.de;matthias.fahrland@iav.de;stefan.eickeler@iais.fraunhofer.de;fabian.hueger@volkswagen.de;stefan.rueping@iais.fraunhofer.de;peter.schlicht@volkswagen.de;tim.wirtz@iais.fraunhofer.de,1;3;1,I have published one or two papers in this area.:I did not assess the experiments.:N/A:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,"Fraunhofer IIS;Fraunhofer IIS;;Fraunhofer IIS;Data Lab, Volkswagen Group;Fraunhofer IIS;Data Lab, Volkswagen Group;Fraunhofer IIS",-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1,,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1,0;0;0;0;0;0;0;0,m;m
4860,ICLR,2020,Adversarial Training: embedding adversarial perturbations into the parameter space of a neural network to build a robust system,Shixian Wen;Laurent Itti,shixianw@usc.edu;itti@usc.edu,1;3;3,I have published in this field for several years.:N/A:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,University of Southern California;University of Southern California,31;31,62;62,4,9/25/19,0,0,0,0,0,0,19;26830,7;266,2;55,0;2974,m;m
4861,ICLR,2020,Neural Linear Bandits: Overcoming Catastrophic Forgetting through Likelihood Matching,Tom Zahavy;Shie Mannor,tomzahavy@gmail.com;shiemannor@gmail.com,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Technion;Technion,26;26,412;412,,1/24/19,4,3,1,0,7,0,496;11682,31;418,10;50,29;1230,m;m
4862,ICLR,2020,Neural Non-additive Utility Aggregation,Markus Zopf,mzopf@ke.tu-darmstadt.de,3;1;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Reject,0,5,0,yes,9/25/19,TU Darmstadt,64,289,,9/25/19,-1,-1,-1,-1,-1,-1,-1,-1,-1,0,m;m
4863,ICLR,2020,Understanding the functional and structural differences across excitatory and inhibitory neurons,Sun Minni;Li Ji-An;Theodore Moskovitz;Grace Lindsay;Kenneth Miller;Mario Dipoppa;Guangyu Robert Yang,sunminni1031@gmail.com;jian.li.acad@gmail.com;thmoskovitz@gmail.com;gracewlindsay@gmail.com;kendmiller@gmail.com;mario.dipoppa@gmail.com;gyyang.neuro@gmail.com,6;6;8,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,11,0,yes,9/25/19,Columbia University;University of Science and Technology of China;University College London;University College London;;;Columbia University,15;481;50;50;-1;-1;15,16;80;15;15;-1;-1;16,,6/25/19,0,0,0,0,0,0,0;898;19;286;8350;419;550,2;998;7;17;183;17;27,0;10;2;6;37;7;11,0;44;5;16;908;33;36,f;m
4864,ICLR,2020,Skew-Explore: Learn faster in continuous spaces with sparse rewards,Xi Chen;Yuan Gao;Ali Ghadirzadeh;Marten Bjorkman;Ginevra Castellano;Patric Jensfelt,xi8@kth.se;gaoyuankidult@gmail.com;algh@kth.se;celle@csc.kth.se;ginevra.castellano@it.uu.se;patric@kth.se,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,5,0,yes,9/25/19,"KTH Royal Institute of Technology, Stockholm, Sweden;Uppsala University;KTH Royal Institute of Technology, Stockholm, Sweden;KTH Royal Institute of Technology, Stockholm, Sweden;Uppsala University;KTH Royal Institute of Technology, Stockholm, Sweden",128;154;128;128;154;128,222;102;222;222;102;222,,9/25/19,0,0,0,0,0,0,58;671;244;1005;556;4945,127;83;25;58;8;178,4;12;9;16;2;41,3;49;7;49;23;271,m;m
4865,ICLR,2020,Self-Supervised Speech Recognition via Local Prior Matching,Wei-Ning Hsu;Ann Lee;Gabriel Synnaeve;Awni Hannun,wnhsu@mit.edu;annl@fb.com;gab@fb.com;awni@fb.com,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Massachusetts Institute of Technology;Facebook;Facebook;Facebook,2;-1;-1;-1,5;-1;-1;-1,3,9/25/19,3,2,1,1,0,0,764;3411;73;121,34;77;13;18,15;20;4;6,68;226;3;9,m;m
4866,ICLR,2020,Learning Reusable Options for Multi-Task Reinforcement Learning,Francisco M. Garcia;Chris Nota;Philip S. Thomas,fmaxgarcia@gmail.com;cnota@cs.umass.edu;pthomas@cs.umass.edu,3;6;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,"Amazon;University of Massachusetts, Amherst;University of Massachusetts, Amherst",-1;28;28,-1;209;209,,9/25/19,0,0,0,0,0,0,6;12;1166,8;8;73,1;3;16,0;3;106,m;m
4867,ICLR,2020,A Unified framework for randomized smoothing based certified defenses,Tianhang Zheng;Di Wang;Baochun Li;Jinhui Xu,th.zheng@mail.utoronto.ca;dwang45@buffalo.edu;bli@ece.toronto.edu;jinhui@buffalo.edu,3;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,3,4,0,yes,9/25/19,"Toronto University;State University of New York, Buffalo;University of Toronto;State University of New York, Buffalo",18;84;18;84,18;263;18;263,4,9/25/19,1,1,0,0,0,0,224;5439;14528;1884,30;484;413;226,8;36;68;19,14;241;1029;108,m;m
4868,ICLR,2020,Closed loop deep Bayesian inversion:  Uncertainty driven acquisition for fast MRI,Thomas Sanchez;Igor Krawczuk;Zhaodong Sun;Volkan Cevher,thomas.sanchez@epfl.ch;igor.krawczuk@epfl.ch;zhaodong.sun@epfl.ch;volkan.cevher@epfl.ch,3;3;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne,481;481;481;481,38;38;38;38,5;4;11,9/25/19,0,0,0,0,0,0,1448;5;0;6382,119;4;2;288,22;1;0;40,96;0;0;484,m;m
4869,ICLR,2020,Scalable Differentially Private Data Generation via Private  Aggregation  of  Teacher Ensembles,Yunhui Long;Suxin Lin;Zhuolin Yang;Carl A. Gunter;Han Liu;Bo Li,ylong4@illinois.edu;linsuxin28@gmail.com;lucas110550@sjtu.edu.cn;cgunter@illinois.edu;hanliu@northwestern.edu;lbo@illinois.edu,3;1;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,4,0,yes,9/25/19,"University of Illinois, Urbana Champaign;University of Electronic Science and Technology of China;Shanghai Jiao Tong University;University of Illinois, Urbana Champaign;Northwestern University;University of Illinois, Urbana Champaign",3;481;53;3;44;3,48;628;157;48;22;48,5;4;1,9/25/19,0,0,0,0,0,0,167;3;44;6755;13;15,16;2;6;271;28;22,3;1;3;43;2;2,22;1;13;555;0;0,f;f
4870,ICLR,2020,Adaptive Generation of Unrestricted Adversarial Inputs,Isaac Dunn;Hadrien Pouget;Tom Melham;Daniel Kroening,isaac.dunn@cs.ox.ac.uk;hadrien.pouget@cs.ox.ac.uk;tom.melham@cs.ox.ac.uk;kroening@cs.ox.ac.uk,3;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,9,0,yes,9/25/19,University of Oxford;University of Oxford;University of Oxford;University of Oxford,50;50;50;50,1;1;1;1,5;4,9/25/19,2,0,1,0,0,0,9;2;1729;7591,4;2;63;366,2;1;9;41,0;0;232;779,m;m
4871,ICLR,2020,GPNET: MONOCULAR 3D VEHICLE DETECTION BASED ON LIGHTWEIGHT WHEEL GROUNDING POINT DETECTION NETWORK,zizhang.wu,wuzizhang87@gmail.com,1;1;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,,,,,9/25/19,-1,-1,-1,-1,-1,-1,-1,-1,-1,0,m
4872,ICLR,2020,Training Neural Networks for and by Interpolation,Leonard Berrada;Andrew Zisserman;Pawan M. Kumar,lberrada@robots.ox.ac.uk;az@robots.ox.ac.uk;pawan@robots.ox.ac.uk,1;6;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,5,0,yes,9/25/19,University of Oxford;University of Oxford;University of Oxford,50;50;50,1;1;1,,6/13/19,5,2,2,1,3,1,54;141121;2663,5;797;83,4;141;24,6;20187;248,m;m
4873,ICLR,2020,On the expected running time of nonconvex optimization with early stopping,Thomas Flynn;Kwang Min Yu;Abid Malik;Shinjae Yoo;Nicholas D'Imperio,thomasflynn918@gmail.com;kyu@bnl.gov;amalik@bnl.gov;sjyoo@bnl.gov;dimperio@bnl.gov,3;6;3,I have published one or two papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I did not assess the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,Brookhaven National Laboratory;Brookhaven National Laboratory;Brookhaven National Laboratory;Brookhaven National Laboratory;Brookhaven National Laboratory,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,1;9;8,9/25/19,0,0,0,0,0,0,643;40;48;697;7,91;31;9;79;5,11;4;2;13;2,34;1;7;51;0,m;m
4874,ICLR,2020,Interpreting video features: a comparison of 3D convolutional networks and convolutional LSTM networks,Joonatan Mänttäri*;Sofia Broomé*;John Folkesson;Hedvig Kjellström,sbroome@kth.se;manttari@kth.se;johnf@kth.se;hedvig@kth.se,6;1;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,4,0,yes,9/25/19,"KTH Royal Institute of Technology, Stockholm, Sweden;KTH Royal Institute of Technology, Stockholm, Sweden;KTH Royal Institute of Technology, Stockholm, Sweden;KTH Royal Institute of Technology, Stockholm, Sweden",128;128;128;128,222;222;222;222,2,9/25/19,1,1,0,0,0,0,8;6;1421;3313,4;6;83;106,2;2;19;26,0;1;83;199,m;f
4875,ICLR,2020,Deep Multiple Instance Learning for Taxonomic Classification of Metagenomic read sets,Andreas Georgiou;Vincent Fortuin;Harun Mustafa;Gunnar Rätsch,geandrea@ethz.ch;fortuin@inf.ethz.ch;harun.mustafa@inf.ethz.ch;raetsch@inf.ethz.ch,3;3;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0,yes,9/25/19,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10;10,13;13;13;13,,9/25/19,0,0,0,0,0,0,256;79;57;9598,35;18;16;244,5;6;4;52,19;5;7;784,m;m
4876,ICLR,2020,Learning with Long-term Remembering: Following the Lead of Mixed Stochastic Gradient,Yunhui Guo;Mingrui Liu;Tianbao Yang;Tajana Rosing,yug185@eng.ucsd.edu;mingrui-liu@uiowa.edu;tianbao-yang@uiowa.edu;tajana@ucsd.edu,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0,yes,9/25/19,"University of California, San Diego;University of Iowa;University of Iowa;University of California, San Diego",11;154;154;11,31;227;227;31,,9/25/19,0,0,0,0,0,0,104;238;3236;6245,19;29;187;246,5;9;29;42,15;10;351;454,m;f
4877,ICLR,2020,Skew-Fit: State-Covering Self-Supervised Reinforcement Learning,Vitchyr H. Pong;Murtaza Dalal;Steven Lin;Ashvin Nair;Shikhar Bahl;Sergey Levine,vitchyr@berkeley.edu;mdalal@berkeley.edu;stevenlin598@berkeley.edu;anair17@berkeley.edu;shikharbahl@berkeley.edu;svlevine@eecs.berkeley.edu,6;3;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,9,0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5;5;5,13;13;13;13;13;13,,3/8/19,44,16,11,2,6,5,489;310;922;810;218;24893,11;9;25;16;11;310,7;4;8;7;6;74,51;43;79;72;25;3235,m;m
4878,ICLR,2020,Uncertainty - sensitive learning and planning with ensembles,Piotr Miłoś;Łukasz Kuciński;Konrad Czechowski;Piotr Kozakowski;Maciej Klimek,pmilos@mimuw.edu.pl;lukasz.kucinski@gmail.com;konrad.czechowski@gmail.com;p.kozakowski@mimuw.edu.pl;maciej.klimek@gmail.com,3;6;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,3,0,yes,9/25/19,"University of Washington, Seattle;Institute of Mathematics Polish Academy of Sciences;University of Washington, Seattle;University of Washington, Seattle;",6;-1;6;6;-1,26;-1;26;26;-1,,9/25/19,0,0,0,0,0,0,437;0;137;109;0,46;2;15;4;1,10;0;4;1;0,37;0;11;7;0,m;m
4879,ICLR,2020,Gradient-based training of Gaussian Mixture Models in High-Dimensional Spaces,Alexander Gepperth;Benedikt Pfülb,alexander.gepperth@cs.hs-fulda.de;benedikt.pfuelb@cs.hs-fulda.de,3;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,HS Fulda;HS Fulda,-1;-1,-1;-1,1,9/25/19,1,0,1,0,0,1,507;29,70;5,11;2,29;3,m;m
4880,ICLR,2020,Counterfactual Regularization for Model-Based Reinforcement Learning,Lawrence Neal;Li Fuxin;Xiaoli Fern,nealla@oregonstate.edu;fuxin.li@oregonstate.edu;xiaoli.fern@oregonstate.edu,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,1,0,yes,9/25/19,Oregon State University;Oregon State University;Oregon State University,77;77;77,373;373;373,,9/25/19,0,0,0,0,0,0,321;4;2799,12;25;125,4;1;26,24;0;299,m;f
4881,ICLR,2020,Three-Head Neural Network Architecture for AlphaZero Learning,Chao Gao;Martin Mueller;Ryan Hayward;Hengshuai Yao;Shangling Jui,cgao3@ualberta.ca;mmueller@ualberta.ca;hayward@ualberta.ca;hengshuai.yao@huawei.com;jui.shangling@huawei.com,6;3;6,I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,University of Alberta;University of Alberta;University of Alberta;Huawei Technologies Ltd.;Huawei Technologies Ltd.,100;100;100;-1;-1,136;136;136;-1;-1,8,9/25/19,0,0,0,0,0,0,9;345;966;97;9,19;39;91;42;10,2;9;15;6;2,1;19;83;9;0,m;m
4882,ICLR,2020,GATO: Gates Are Not the Only Option,Mark Goldstein*;Xintian Han*;Rajesh Ranganath,goldstein@nyu.edu;xh1007@nyu.edu;rajeshr@cims.nyu.edu,3;8;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,5,0,yes,9/25/19,New York University;New York University;New York University,25;25;25,29;29;29,1,9/25/19,0,0,0,0,0,0,1015;0;5012,263;2;77,11;0;27,33;0;511,m;m
4883,ICLR,2020,Collaborative Inter-agent Knowledge Distillation for Reinforcement Learning,Zhang-Wei Hong;Prabhat Nagarajan;Guilherme Maeda,williamd4112@gapp.nthu.edu.tw;prabhat@preferred.jp;gjmaeda@preferred.jp,3;8;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,7,0,yes,9/25/19,"National Tsing Hua University;Preferred Networks, Inc.;Preferred Networks, Inc.",172;-1;-1,365;-1;-1,,9/25/19,1,0,0,0,0,0,189;66;510,13;19;44,4;4;13,16;2;29,m;m
4884,ICLR,2020,THE EFFECT OF ADVERSARIAL TRAINING: A THEORETICAL CHARACTERIZATION,Mingyang Yi;Huishuai Zhang;Wei Chen;Zhi-Ming Ma;Tie-Yan Liu,yimingyang17@mails.ucas.edu.cn;huzhang@microsoft.com;wche@microsoft.com;mazm@amt.ac.cn;tie-yan.liu@microsoft.com,1;1;1,I have read many papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,4,0,yes,9/25/19,University of Chinese Academy of Sciences;Microsoft;Microsoft;Chinese Academy of Sciences;Microsoft,59;-1;-1;59;-1,1397;-1;-1;1397;-1,4;1,9/25/19,0,0,0,0,0,0,4;421;464;871;13552,6;42;128;59;369,1;10;11;14;51,0;49;13;81;1723,m;m
4885,ICLR,2020,Flexible and Efficient Long-Range Planning Through Curious Exploration,Aidan Curtis;Minjian Xin;Kevin Feigelis;Dan Yamins,southpawac@gmail.com;xinminjian@sjtu.edu.cn;feigelis@stanford.edu;yamins@stanford.edu,3;1;6,I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,16,0,yes,9/25/19,Rice University;Shanghai Jiao Tong University;Stanford University;Stanford University,84;53;4;4,105;157;4;4,,9/25/19,0,0,0,0,0,0,1;0;0;602,3;2;5;42,1;0;0;9,0;0;0;36,m;m
4886,ICLR,2020,Learning Latent Dynamics for Partially-Observed Chaotic Systems,Said ouala;Duong Nguyen;Lucas Drumetz;Bertrand Chapron;Ananda Pascual;Fabrice Collard;Lucile Gaultier;Ronan Fablet,said.ouala@imt-atlantique.fr;van.nguyen1@imt-atlantique.fr;lucas.drumetz@imt-atlantique.fr;bertrand.chapron@ifremer.fr;ananda.pascual@imedea.uib-csic.es;dr.fab@oceandatalab.com;lucile.gaultier@oceandatalab.com;ronan.fablet@imt-atlantique.fr,3;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0,yes,9/25/19,IMT Atlantique;IMT Atlantique;IMT Atlantique;;Spanish National Research Council;Oceandatalab;Oceandatalab;IMT Atlantique,481;481;481;-1;-1;-1;-1;481,393;393;393;-1;-1;-1;-1;393,1,7/4/19,4,3,0,0,0,0,52;253;289;21;1554;853;149;1570,15;42;41;23;119;77;19;205,4;7;8;2;21;17;6;22,3;12;25;2;111;122;7;77,m;m
4887,ICLR,2020,Make Lead Bias in Your Favor: A Simple and Effective Method for News Summarization,Chenguang Zhu;Ziyi Yang;Robert Gmyr;Michael Zeng;Xuedong Huang,chezhu@microsoft.com;zy99@stanford.edu;rogmyr@microsoft.com;nzeng@microsoft.com;xdh@microsoft.com,6;1;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,6,0,yes,9/25/19,Microsoft;Stanford University;Microsoft;Microsoft;Microsoft,-1;4;-1;-1;-1,-1;4;-1;-1;-1,,9/25/19,4,2,1,0,0,0,869;38;333;85;6037,47;17;36;21;151,11;3;10;5;35,96;3;16;20;530,m;m
4888,ICLR,2020,High performance RNNs with spiking neurons,Manu V Nair;Giacomo Indiveri,mnair@ini.uzh.ch;giacomo@ini.uzh.ch,6;6;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,5,1,yes,9/25/19,University of Zurich;University of Zurich,143;143,90;90,,9/25/19,0,0,0,0,0,0,83;7032,21;283,5;39,1;290,m;m
4889,ICLR,2020,Universal Learning Approach for Adversarial Defense,Uriya Pesso;Koby Bibas;Meir Feder,uriyapes@gmail.com;kobybibas@gmail.com;meir@eng.tau.ac.il,3;1;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,3,0,yes,9/25/19,Tel Aviv University;Tel Aviv University;Tel Aviv University,35;35;35,188;188;188,4,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1,-1;-1;-1,-1;-1;-1,0;0;0,m;m
4890,ICLR,2020,Re-Examining Linear Embeddings for High-dimensional Bayesian Optimization,Benjamin Letham;Roberto Calandra;Akshara Rai;Eytan Bakshy,bletham@fb.com;rcalandra@fb.com;akshararai@fb.com;ebakshy@fb.com,3;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,Facebook;Facebook;Facebook;Facebook,-1;-1;-1;-1,-1;-1;-1;-1,11,9/25/19,0,0,0,0,0,0,822;1023;196;5079,31;58;20;37,12;15;8;18,86;102;8;420,m;m
4891,ICLR,2020,Salient Explanation for Fine-grained Classification,Kanghan Oh;Sungchan Kim;Il-Seok Oh,blastps@gmail.com;s.k@jbnu.ac.kr;iosh@jbnu.ac.kr,1;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,0,0,yes,9/25/19,Chonbuk National University;Chonbuk National University;Chonbuk National University,-1;-1;-1,-1;-1;-1,1,9/25/19,0,0,0,0,0,0,30;729;3,7;64;12,3;15;1,5;52;0,f;m
4892,ICLR,2020,Deep Coordination Graphs,Wendelin Boehmer;Vitaly Kurin;Shimon Whiteson,wendelin.boehmer@cs.ox.ac.uk;vitaly.kurin@cs.ox.ac.uk;shimon.whiteson@cs.ox.ac.uk,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,University of Oxford;University of Oxford;University of Oxford,50;50;50,1;1;1,10;8,9/25/19,6,3,3,1,0,1,166;182;5445,18;11;203,8;6;38,13;13;588,m;m
4893,ICLR,2020,Adversarial Training Generalizes Data-dependent Spectral Norm Regularization,Kevin Roth;Yannic Kilcher;Thomas Hofmann,kevin.roth@inf.ethz.ch;yannic.kilcher@inf.ethz.ch;thomas.hofmann@inf.ethz.ch,1;3;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,11,0,yes,9/25/19,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10,13;13;13,4;1,6/4/19,2,1,2,1,3,1,531;114;23015,28;14;173,10;4;53,72;26;3393,m;m
4894,ICLR,2020,TriMap: Large-scale Dimensionality Reduction Using Triplets,Ehsan Amid;Manfred K. Warmuth,eamid@ucsc.edu;manfred@ucsc.edu,3;1;3,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,University of Southern California;University of Southern California,31;31,62;62,,9/25/19,6,2,0,0,0,0,135;11348,24;251,7;53,13;1221,m;m
4895,ICLR,2020,Unsupervised Temperature Scaling: Robust Post-processing Calibration for Domain Shift,Azadeh Sadat Mozafari;Hugo Siqueira Gomes;Christian Gagne,azadeh-sadat.mozafari.1@ulaval.ca;hugo.siqueira-gomes.1@ulaval.ca;christian.gagne@gel.ulaval.ca,3;1;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,10,0,yes,9/25/19,Laval university;Laval university;Laval university,481;481;481,272;272;272,,9/25/19,0,0,0,0,0,0,44;6;187,15;7;25,3;1;5,4;0;12,f;m
4896,ICLR,2020,Uncertainty-Aware Prediction for Graph Neural Networks,Xujiang Zhao;Feng Chen;Shu Hu;jin-Hee Cho,xxz190020@utdallas.edu;feng.chen@utdallas.edu;shu2@albany.edu;jicho@vt.edu,3;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0,yes,9/25/19,"University of Texas, Dallas;University of Texas, Dallas;State University of New York, Albany;Virginia Tech",86;86;266;79,319;319;350;240,11;10,9/25/19,0,0,0,0,0,0,6;120;79;530,8;31;23;32,2;5;5;7,2;14;2;46,m;f
4897,ICLR,2020,Hydra: Preserving Ensemble Diversity for Model Distillation,Linh Tran;Bastiaan S. Veeling;Kevin Roth;Jakub Świątkowski;Joshua V. Dillon;Jasper Snoek;Stephan Mandt;Tim Salimans;Sebastian Nowozin;Rodolphe Jenatton,linh.tran@imperial.ac.uk;basveeling@gmail.com;kevin.roth@inf.ethz.ch;kuba.swiatkowski@gmail.com;jvdillon@google.com;jaspersnoek@gmail.com;stephan.mandt@gmail.com;salimans@google.com;nowozin@google.com;rjenatton@google.com,6;3;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,5,0,yes,9/25/19,"Imperial College London;Google;Swiss Federal Institute of Technology;;Google;Google;University of California, Irvine;Google;Google;Google",73;-1;10;-1;-1;-1;35;-1;-1;-1,10;-1;13;-1;-1;-1;96;-1;-1;-1,,9/25/19,4,1,2,0,0,1,232;99;33;21;1011;4985;39;6754;7062;3562,56;14;8;6;28;61;13;35;134;39,9;5;4;3;13;18;4;14;39;20,4;19;6;4;166;512;6;1054;941;334,f;m
4898,ICLR,2020,Finding Deep Local Optima Using Network Pruning,Yangzi Guo;Yiyuan She;Ying Nian Wu;Adrian Barbu,yguo@math.fsu.edu;yshe@stat.fsu.edu;ywu@stat.ucla.edu;abarbu@stat.fsu.edu,3;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,"SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;University of California, Los Angeles;SUN YAT-SEN UNIVERSITY",481;481;20;481,299;299;17;299,,9/25/19,0,0,0,0,0,0,2;1085;5865;30,10;48;276;30,1;14;38;3,0;143;596;6,m;m
4899,ICLR,2020,Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning,Xue Bin Peng;Aviral Kumar;Grace Zhang;Sergey Levine,xbpeng@berkeley.edu;aviralkumar2907@gmail.com;grace.zhang@berkeley.edu;svlevine@eecs.berkeley.edu,6;3;6,I have published one or two papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,31,0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5;5,13;13;13;13,,9/25/19,12,6,8,2,0,4,1116;143;18;24893,23;24;10;310,11;6;2;74,84;32;4;3235,m;m
4900,ICLR,2020,Generative Teaching Networks: Accelerating Neural Architecture Search by Learning  to Generate Synthetic Training Data,Felipe Petroski Such;Aditya Rawal;Joel Lehman;Kenneth Stanley;Jeff Clune,felipe.such@uber.com;aditya.rawal@uber.com;joel.lehman@uber.com;kstanley@uber.com;jeffclune@uber.com,3;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,13,0,yes,9/25/19,Uber;Uber;Uber;Uber;Uber,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,5,9/25/19,4,4,1,1,0,1,723;724;344;12177;11069,14;48;27;260;113,7;13;10;53;36,86;38;26;1529;777,m;m
4901,ICLR,2020,Compression without Quantization,Gergely Flamich;Marton Havasi;José Miguel Hernández-Lobato,gf332@cam.ac.uk;mh740@cam.ac.uk;jmh233@cam.ac.uk,3;3;6;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,5,0,yes,9/25/19,University of Cambridge;University of Cambridge;University of Cambridge,71;71;71,3;3;3,,9/25/19,1,0,0,0,0,0,57;39;3824,2;7;114,1;3;28,10;10;420,m;m
4902,ICLR,2020,CLAREL: classification via retrieval loss for zero-shot learning,Boris N. Oreshkin;Negar Rostamzadeh;Pedro O. Pinheiro;Christopher Pal,boris@elementai.com;negar@elementai.com;pedro@elementai.com;christopher.pal@elementai.com,1;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,6,0,yes,9/25/19,Element AI;Element AI;Element AI;Element AI,-1;-1;-1;-1,-1;-1;-1;-1,6,5/31/19,0,0,0,0,0,0,768;389;2454;8489,46;32;36;120,12;10;13;33,89;52;239;764,m;m
4903,ICLR,2020,Generating Semantic Adversarial Examples with Differentiable Rendering,Lakshya Jain;Steven Chen;Wilson Wu;Uyeong Jang;Varun Chandrasekaran;Sanjit Seshia;Somesh Jha,lakshya.jain@berkeley.edu;scchen@berkeley.edu;wilswu@berkeley.edu;wjang@cs.wisc.edu;vchandrasek4@wisc.edu;sseshia@eecs.berkeley.edu;jha@cs.wisc.edu,3;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley;University of Southern California;University of Southern California;University of California Berkeley;University of Southern California,5;5;5;31;31;5;31,13;13;13;62;62;13;62,4;10,9/25/19,4,2,1,0,0,0,4;2531;14;57;64;10056;18974,3;135;9;8;18;320;331,1;25;2;4;4;49;67,0;106;1;5;2;820;1824,m;m
4904,ICLR,2020,Constant Time Graph Neural Networks,Ryoma Sato;Makoto Yamada;Hisashi Kashima,r.sato@ml.ist.i.kyoto-u.ac.jp;myamada@i.kyoto-u.ac.jp;kashima@i.kyoto-u.ac.jp,6;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Meiji University;Meiji University;Meiji University,481;481;481,332;332;332,10,1/23/19,2,2,1,0,5,0,35;1663;4194,15;245;192,3;20;32,3;130;375,m;m
4905,ICLR,2020,SSE-PT: Sequential Recommendation Via Personalized Transformer,Liwei Wu;Shuqing Li;Cho-Jui Hsieh;James Sharpnack,liwu@ucdavis.edu;qshli@ucdavis.edu;chohsieh@cs.ucla.edu;jsharpna@ucdavis.deu,1;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,"University of California, Davis;University of California, Davis;University of California, Los Angeles;",79;79;20;-1,55;55;17;-1,3,9/25/19,0,0,0,0,0,0,85;2;12827;516,19;8;168;42,5;1;41;13,8;0;1746;63,m;m
4906,ICLR,2020,Adaptive Learned Bloom Filter (Ada-BF): Efficient Utilization of the Classifier,Zhenwei Dai;Anshumali Shrivastava,zd11@rice.edu;anshumali@rice.edu,3;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Rice University;Rice University,84;84,105;105,,9/25/19,3,1,0,0,0,1,202;1135,15;101,4;16,10;111,m;m
4907,ICLR,2020,Classification Attention for Chinese NER,Yuchen Ge;FanYang;PeiYang,geyc2@lenovo.com;yangfan24@lenovo.com;yangpei4@lenovo.com,3;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,0,0,yes,9/25/19,Lenovo Research;Lenovo Research;Lenovo Research,-1;-1;-1,-1;-1;-1,3,9/25/19,0,0,0,0,0,0,14;47;0,4;8;1,1;2;0,1;2;0,m;u
4908,ICLR,2020,Global-Local Network for Learning Depth with Very Sparse Supervision,Antonio Loquercio;Alexey Dosovitskiy;Davide Scaramuzza,loquercio@ifi.uzh.ch;adosovitskiy@google.com;sdavide@ifi.uzh.ch,6;1;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,University of Zurich;Google;University of Zurich,143;-1;143,90;-1;90,,9/25/19,0,0,0,0,0,0,383;2042;14672,19;13;243,8;5;54,41;338;1214,m;m
4909,ICLR,2020,Localized Generations with Deep Neural Networks for Multi-Scale Structured Datasets,Yoshihiro Nagano;Shiro Takagi;Yuki Yoshida;Masato Okada,nagano@mns.k.u-tokyo.ac.jp;takagi@mns.k.u-tokyo.ac.jp;yoshida@mns.k.u-tokyo.ac.jp;okada@edu.k.u-tokyo.ac.jp,3;3;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,The University of Tokyo;The University of Tokyo;The University of Tokyo;The University of Tokyo,56;56;56;56,36;36;36;36,5;6,9/25/19,0,0,0,0,0,0,21;9;393;172,9;7;91;96,2;1;11;7,7;0;28;8,m;m
4910,ICLR,2020,CrossNorm: On Normalization for Off-Policy Reinforcement Learning,Aditya Bhatt;Max Argus;Artemij Amiranashvili;Thomas Brox,aditya@bhatts.org;argus.max@gmail.com;amiranas@cs.uni-freiburg.de;brox@cs.uni-freiburg.de,3;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,TU Berlin;Universität Freiburg;Universität Freiburg;Universität Freiburg,108;118;118;118,149;85;85;85,,2/14/19,5,0,0,0,3,0,15;86;30;38737,11;10;10;257,3;3;4;73,0;6;0;5989,m;m
4911,ICLR,2020,Diagonal Graph Convolutional Networks with Adaptive Neighborhood Aggregation,Jie Zhang;Yuxiao Dong;Jie Tang,zhangjie.exe@gmail.com;yuxdong@microsoft.com;jietang@tsinghua.edu.cn,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,webank;Microsoft;Tsinghua University,-1;-1;8,-1;-1;23,10,9/25/19,0,0,0,0,0,0,346;1855;1552,181;80;68,9;16;9,13;223;182,m;m
4912,ICLR,2020,Variance Reduced Local SGD with Lower Communication Complexity,Xianfeng Liang;Shuheng Shen;Jingchang Liu;Zhen Pan;Yifei Cheng;Enhong Chen,zeroxf@mail.ustc.edu.cn;vaip@mail.ustc.edu.cn;jliude@cse.ust.hk;pzhen@mail.ustc.edu.cn;chengyif@mail.ustc.edu.cn;cheneh@ustc.edu.cn,3;1;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,University of Science and Technology of China;University of Science and Technology of China;The Hong Kong University of Science and Technology;University of Science and Technology of China;University of Science and Technology of China;University of Science and Technology of China,481;481;39;481;481;481,80;80;47;80;80;80,1,9/25/19,6,2,4,0,0,1,13;5;26;194;12;7904,6;4;11;53;12;433,2;1;4;8;2;41,1;0;2;9;0;546,m;m
4913,ICLR,2020,White Box Network: Obtaining a right composition ordering of functions,Eun saem Lee;Hyung Ju Hwang,dmstoa2502@postech.ac.kr;hjhwang@postech.ac.kr,1;1;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,6,0,yes,9/25/19,POSTECH;POSTECH,118;118,146;146,,9/25/19,0,0,0,0,0,0,0;487,2;75,0;13,0;39,f;f
4914,ICLR,2020,Neural Embeddings for Nearest Neighbor Search Under Edit Distance,Xiyuan Zhang;Yang Yuan;Piotr Indyk,zhangxiyuan@zju.edu.cn;yuanyang@tsinghua.edu.cn;indyk@mit.edu,6;6;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0,yes,9/25/19,Zhejiang University;Tsinghua University;Massachusetts Institute of Technology,56;8;2,107;23;5,,9/25/19,2,1,1,0,0,1,33;248;24320,14;28;286,3;5;66,10;53;3234,m;m
4915,ICLR,2020,Accelerating First-Order Optimization Algorithms,Ange Tato;Roger Nkambou,angetato@gmail.com;nkambou@gmail.com,3;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,;,-1;-1,-1;-1,1,9/27/18,0,0,0,0,0,0,52;1654,21;209,4;20,1;94,f;m
4916,ICLR,2020,A SIMPLE AND EFFECTIVE FRAMEWORK FOR PAIRWISE DEEP METRIC LEARNING,Qi Qi;Yan Yan;Zixuan Wu;Xiaoyu Wang;Tianbao Yang,qi-qi@uiowa.edu;yanyan.tju@gmail.com;wuzu@bc.edu;fanghuaxue@gmail.com;tianbao-yang@uiowa.edu,6;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,University of Iowa;University of Iowa;Boston College;Snap Inc.;University of Iowa,154;154;266;-1;154,227;227;323;-1;227,2,9/25/19,0,0,0,0,0,0,100;63;21;5461;3236,29;57;10;221;187,3;4;2;35;29,3;4;0;428;351,f;m
4917,ICLR,2020,Local Label Propagation for Large-Scale Semi-Supervised Learning,Chengxu Zhuang;Chaofei Fan;Xuehao Ding;Divyanshu Murli;Daniel Yamins,chengxuz@stanford.edu;stfan@stanford.edu;xhding@stanford.edu;divymurli@gmail.com;yamins@stanford.edu,3;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,Stanford University;Stanford University;Stanford University;Kount Inc;Stanford University,4;4;4;-1;4,4;4;4;-1;4,,5/28/19,2,1,0,0,0,0,143;246;15;55;3458,11;27;5;7;30,4;8;2;4;20,11;13;1;0;257,m;m
4918,ICLR,2020,Sparse Skill Coding: Learning Behavioral Hierarchies with Sparse Codes,Sophia Sanborn;Michael Chang;Sergey Levine;Thomas Griffiths,sanborn@berkeley.edu;mbchang@berkeley.edu;svlevine@eecs.berkeley.edu;tomg@princeton.edu,1;6;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,1,0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley;Princeton University,5;5;5;31,13;13;13;6,,9/25/19,0,0,0,0,0,0,27;41;959;21772,9;22;34;441,3;3;9;70,2;2;103;2194,f;m
4919,ICLR,2020,"JAX MD: End-to-End Differentiable, Hardware Accelerated, Molecular Dynamics in Pure Python",Samuel S. Schoenholz;Ekin D. Cubuk,schsam@google.com;cubuk@google.com,6;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I do not know much about this area.:N/A:I made a quick assessment of this paper.:N/A,Reject,0,3,0,yes,9/25/19,Google;Google,-1;-1,-1;-1,,9/25/19,6,3,1,0,0,0,3133;1032,70;29,21;13,388;133,m;m
4920,ICLR,2020,ROS-HPL: Robotic Object Search with Hierarchical Policy Learning and Intrinsic-Extrinsic Modeling,Xin Ye;Shibin Zheng;Yezhou Yang,xinye1@asu.edu;szheng31@asu.edu;yz.yang@asu.edu,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,3,0,yes,9/25/19,Arizona State University;Arizona State University;Arizona State University,95;95;95,155;155;155,,9/25/19,0,0,0,0,0,0,4;18;17,25;4;15,1;3;2,0;3;1,m;m
4921,ICLR,2020,Fractional Graph Convolutional Networks (FGCN) for Semi-Supervised Learning,Yuzhou Chen;Yulia R. Gel;Konstantin Avrachenkov,yuzhouc@smu.edu;ygl@utdallas.edu;konstentin.avratchankov@inria.fr,6;3;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,1,0,0,yes,9/25/19,"Singapore Management University;University of Texas, Dallas;INRIA",92;86;-1,1397;319;-1,10,9/25/19,0,0,0,0,0,0,19;815;3567,7;86;308,2;14;29,0;54;267,m;m
4922,ICLR,2020,Graph Neural Networks For Multi-Image Matching,Stephen Phillips;Kostas Daniilidis,stephi@seas.upenn.edu;kostas@seas.upenn.edu,3;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0,yes,9/25/19,University of Pennsylvania;University of Pennsylvania,19;19,11;11,2;10,9/25/19,0,0,0,0,0,0,166;10204,62;343,8;54,12;939,m;m
4923,ICLR,2020,Cost-Effective Testing of a Deep Learning Model through Input Reduction,Jianyi Zhou;Feng Li;Jinhao Dong;Hongyu Zhang;Dan Hao,zhoujianyi@pku.edu.cn;lifeng2014@pku.edu.cn;xdu_jhdong@163.com;hongyu.zhang@newcastle.edu.au;haod@sei.pku.edu.cn,8;6;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,"Peking University;Peking University;Xidian University;University of Newcastle, Australia;Peking University",22;22;481;390;22,24;24;919;311;24,,9/25/19,0,0,0,0,0,0,61;223;1;43;6,16;155;2;24;11,4;7;1;2;2,1;6;0;3;0,u;f
4924,ICLR,2020,Decoupling Weight Regularization from Batch Size for Model Compression,Dongsoo Lee;Se Jung Kwon;Byeongwook Kim;Yongkweon Jeon;Baeseong Park;Jeongin Yun;Gu-Yeon Wei,dslee3@gmail.com;mogndrewk@gmail.com;quddnr145@gmail.com;dragwon.jeon@gmail.com;qkrqotjd91@gmail.com;yji6373@naver.com;gywei@g.harvard.edu,8;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0,yes,9/25/19,;Samsung;Samsung;Samsung;;Samsung;Harvard University,-1;-1;-1;-1;-1;-1;39,-1;-1;-1;-1;-1;-1;7,,9/25/19,0,0,0,0,0,0,60;23;23;32;0;0;5285,14;22;10;9;5;6;180,2;3;3;3;0;0;37,6;0;1;2;0;0;456,m;m
4925,ICLR,2020,Amharic Text Normalization with Sequence-to-Sequence Models,Seifedin Shifaw Mohamed;Solomon Teferra Abate (PhD),seifedin28@gmail.com;solomon_teferra_7@yahoo.com,1;1;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,Addis Ababa University;,481;-1,1397;-1,3,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1,-1;-1,-1;-1,0;0,m;m
4926,ICLR,2020,Star-Convexity in Non-Negative Matrix Factorization,Johan Bjorck;Carla Gomes;Kilian Weinberger,njb225@cornell.edu;gomes@cs.cornell.edu;kilianweinberger@cornell.edu,3;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,7,0,yes,9/25/19,Cornell University;Cornell University;Cornell University,7;7;7,19;19;19,,9/25/19,0,0,0,0,0,0,176;0;24393,11;2;166,5;0;54,11;0;3878,m;m
4927,ICLR,2020,Learning Compact Reward for Image Captioning,Nannan Li;Zhenzhong Chen,live@whu.edu.cn;zzchen@whu.edu.cn,6;1;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Wuhan University;Wuhan University,266;266,354;354,4;9,9/25/19,1,1,0,0,0,0,10;2781,13;221,2;28,0;209,f;m
4928,ICLR,2020,Learning Entailment-Based Sentence Embeddings from Natural Language Inference,Rabeeh Karimi Mahabadi*;Florian Mai*;James Henderson,rkarimi@idiap.ch;florian.mai@idiap.ch;james.henderson@idiap.ch,6;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:N/A:I assessed the sensibility of the derivations and theory.,Reject,0,3,1,yes,9/25/19,Idiap Research Institute;Idiap Research Institute;Idiap Research Institute,-1;-1;-1,-1;-1;-1,3,9/25/19,0,0,0,0,0,0,85;28;222,10;11;69,5;3;9,6;0;14,f;m
4929,ICLR,2020,Word embedding re-examined: is the symmetrical factorization optimal?,Zhichao Han;Jia Li;Xu Li;Hong Cheng,zchan@se.cuhk.edu.hk;lijia@se.cuhk.edu.hk;xuli@se.cuhk.edu.hk;hcheng@se.cuhk.edu.hk,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,The Chinese University of Hong Kong;The Chinese University of Hong Kong;The Chinese University of Hong Kong;The Chinese University of Hong Kong,59;59;59;59,35;35;35;35,,9/25/19,0,0,0,0,0,0,48;13;136;449,11;16;60;50,2;2;6;12,0;2;5;40,m;f
4930,ICLR,2020,Structured consistency loss for semi-supervised semantic segmentation,JongMok Kim;Joo Young Jang;Hyunwoo Park,win98man1@gmail.com;jyjang1090@gmail.com;phw08132@gmail.com,1;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,;Hyundai Mobis;,-1;-1;-1,-1;-1;-1,2,9/25/19,0,0,0,0,0,0,43;50;214,9;13;49,2;4;7,6;0;11,m;m
4931,ICLR,2020,Attention on Abstract Visual Reasoning,Lukas Hahne;Timo Lüddecke;Florentin Wörgötter;David Kappel,l.hahne@stud.uni-goettingen.de;timo.lueddecke@phys.uni-goettingen.de;worgott@gwdg.de;david.kappel@phys.uni-goettingen.de,3;3;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,2,0,yes,9/25/19,University of Goettingen;University of Goettingen;;University of Goettingen,323;323;-1;323,123;123;-1;123,,9/25/19,3,0,2,0,0,0,2;11;5249;93,1;14;379;22,1;2;35;6,0;1;307;3,m;m
4932,ICLR,2020,Event extraction from unstructured Amharic text,Ephrem Tadesse;Rosa Tsegaye;Kuulaa Qaqqabaa,ephe11ta@gmail.com;rosatsegaye@gmail.com;kuulaa@gmail.com,3;1;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,;;Addis Ababa Science and Technology University,-1;-1;-1,-1;-1;-1,,9/25/19,0,0,0,0,0,0,0;0;0,1;2;1,0;0;0,0;0;0,m;m
4933,ICLR,2020,Discovering Topics With Neural Topic Models Built From PLSA Loss,sileye ba,sileye.ba@outlook.com,1;1;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,2,0,0,yes,9/25/19,,,,,9/25/19,0,0,0,0,0,0,792,54,14,45,m;m
4934,ICLR,2020,Exploiting Excessive Invariance caused by Norm-Bounded Adversarial Robustness,Jörn-Henrik Jacobsen;Jens Behrmann;Nicholas Carlini;Florian Tramèr;Nicolas Papernot,j.jacobsen@vectorinstitute.ai;jensb@uni-bremen.de;nicholas@carlini.com;tramer@cs.stanford.edu;nicolas.papernot@utoronto.ca,3;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Vector Institute;Universität Bremen;Carlini;Stanford University;Toronto University,-1;154;-1;4;18,-1;360;-1;4;18,4,3/25/19,20,10,1,1,3,3,420;276;5947;2402;9462,19;13;44;36;66,9;5;20;18;27,38;44;1020;333;1100,m;m
4935,ICLR,2020,WikiMatrix: Mining 135M Parallel Sentences in 1620 Language Pairs from Wikipedia,Holger Schwenk;Vishrav Chaudhary;Shuo Sun;Hongyu Gong;Francisco Guzmán,schwenk@fb.com;vishrav@fb.com;ssun32@jhu.edu;hgong6@illinois.edu;fguzman@fb.com,3;6;8;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:N/A:N/A:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,"Facebook;Facebook;Johns Hopkins University;University of Illinois, Urbana Champaign;Facebook",-1;-1;73;3;-1,-1;-1;12;48;-1,,7/10/19,37,21,20,0,0,6,14380;221;96;126;59,152;9;28;17;5,37;6;4;6;4,2262;50;8;12;5,m;m
4936,ICLR,2020,ProtoAttend: Attention-Based Prototypical Learning,Sercan O. Arik;Tomas Pfister,soarik@google.com;tpfister@google.com,3;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Google;Google,-1;-1,-1;-1,,9/25/19,2,0,0,0,0,0,1391;2335,54;47,17;16,119;285,m;m
4937,ICLR,2020,Discourse-Based Evaluation of Language Understanding,Damien Sileo;Tim Van-De-Cruys;Camille Pradel;Philippe Muller,damien.sileo@irit.fr;tim.vandecruys@irit.fr;camille.pradel@synapse-fr.com;philippe.muller@irit.fr,6;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,5,0,yes,9/25/19,"IRIT, University of Toulouse;IRIT, University of Toulouse;Synapse-fr;IRIT, University of Toulouse",-1;-1;-1;-1,-1;-1;-1;-1,3,7/19/19,1,0,1,1,0,0,12;588;126;1290,9;60;30;315,2;13;8;18,3;31;7;97,m;m
4938,ICLR,2020,SINGLE PATH ONE-SHOT NEURAL ARCHITECTURE SEARCH WITH UNIFORM SAMPLING,Zichao Guo;Xiangyu Zhang;Haoyuan Mu;Wen Heng;Zechun Liu;Yichen Wei;Jian Sun,guozichao@megvii.com;zhangxiangyu@megvii.com;muhy17@mails.tsinghua.edu.cn;hengwen@megvii.com;zliubq@connect.ust.hk;weiyichen@megvii.com;sunjian@megvii.com,6;8;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,6,1,yes,9/25/19,Megvii Technology Inc.;Megvii Technology Inc.;Tsinghua University;Megvii Technology Inc.;The Hong Kong University of Science and Technology;Megvii Technology Inc.;Megvii Technology Inc.,-1;-1;8;-1;39;-1;-1,-1;-1;23;-1;47;-1;-1,,3/31/19,110,53,52,3,0,20,188;4427;228;539;323;8840;-1,10;136;12;24;18;80;-1,4;21;6;4;6;36;-1,36;720;39;59;61;1556;0,m;m
4939,ICLR,2020,Inducing Stronger Object Representations in Deep Visual Trackers,Ross Goroshin;Jonathan Tompson;Debidatta Dwibedi,goroshin@google.com;tompson@google.com;debidatta@google.com,3;3;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,1,4,0,yes,9/25/19,Google;Google;Google,-1;-1;-1,-1;-1;-1,,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1,-1;-1;-1,-1;-1;-1,0;0;0,m;m
4940,ICLR,2020,Automatically Learning Feature Crossing from Model Interpretation for Tabular Data,Zhaocheng Liu;Qiang Liu;Haoli Zhang,zhaocheng.liu@realai.ai;qiang.liu@realai.ai;haoli.zhang@realai.ai,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,1,13,0,yes,9/25/19,RealAI;RealAI;RealAI,-1;-1;-1,-1;-1;-1,,9/25/19,0,0,0,0,0,0,540;60;2048,31;70;228,12;3;20,18;9;122,m;m
4941,ICLR,2020,Semi-Supervised Boosting via Self Labelling,Akul Goyal;Yang Liu,akulg2@illinois.edu;yangliu@ucsc.edu,1;1;3,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,"University of Illinois, Urbana Champaign;University of Southern California",3;31,48;62,8,9/25/19,0,0,0,0,0,0,0;2,1;15,0;1,0;0,m;m
4942,ICLR,2020,Depth creates no more spurious local minima in linear networks,Li Zhang,liqzhang@google.com,3;6;3,I have read many papers in this area.:N/A:N/A:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,Google,-1,-1,1,9/25/19,0,0,0,0,0,0,598,82,11,37,m
4943,ICLR,2020,Learning by shaking: Computing policy gradients by physical forward-propagation,Arash Mehrjou;Ashkan Soleymani;Stefan Bauer;Bernhard Schölkopf,amehrjou@tuebingen.mpg.de;soli.ashkan98@gmail.com;stefan.bauer@tuebingen.mpg.de;bs@tuebingen.mpg.de,1;1;3;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,"Max Planck Institute for Intelligent Systems, Max-Planck Institute;Sharif University of Technology;Max Planck Institute for Intelligent Systems, Max-Planck Institute;Max Planck Institute for Intelligent Systems, Max-Planck Institute",-1;323;-1;-1,-1;564;-1;-1,,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1;-1,-1;-1;-1;-1,-1;-1;-1;-1,0;0;0;0,m;m
4944,ICLR,2020,Consistent Meta-Reinforcement Learning via Model Identification and Experience Relabeling,Russell Mendonca;Xinyang Geng;Chelsea Finn;Sergey Levine,russellm@berkeley.edu;young.geng@berkeley.edu;cbfinn@cs.stanford.edu;svlevine@eecs.berkeley.edu,3;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0,yes,9/25/19,University of California Berkeley;University of California Berkeley;Stanford University;University of California Berkeley,5;5;4;5,13;13;4;13,,9/25/19,0,0,0,0,0,0,94;313;7879;68,9;8;101;31,2;4;34;4,6;36;1060;4,m;m
4945,ICLR,2020,Accelerating Monte Carlo Bayesian Inference via Approximating Predictive Uncertainty over the Simplex,Yufei Cui;Wuguannan Yao;Qiao Li;Antoni Chan;Chun Jason Xue,yufeicui92@gmail.com;satie.yao@my.cityu.edu.hk;qiaoli045@gmail.com;abchan@cityu.edu.hk;jasonxue@cityu.edu.hk,3;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,6,0,yes,9/25/19,City University of Hong Kong;City University of Hong Kong;;City University of Hong Kong;City University of Hong Kong,92;92;-1;92;92,35;35;-1;35;35,4;11,5/29/19,2,1,1,1,0,0,25;2;6366;5093;3057,13;1;687;139;326,3;1;40;33;27,1;0;380;568;142,f;m
4946,ICLR,2020,Trajectory growth through random deep ReLU networks,Ilan Price;Jared Tanner,ilan.price@maths.ox.ac.uk;tanner@maths.ox.ac.uk,3;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0,yes,9/25/19,University of Oxford;University of Oxford,50;50,1;1,1,9/25/19,0,0,0,0,0,0,1;2921,4;73,1;25,0;272,m;m
4947,ICLR,2020,Ellipsoidal Trust Region Methods for Neural Network Training,Leonard Adolphs;Jonas Kohler;Aurelien Lucchi,ladolphs@inf.ethz.ch;jonas.kohler@inf.ethz.ch;aurelien.lucchi@inf.ethz.ch,3;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10,13;13;13,8,5/22/19,4,2,0,0,0,0,38;-1;7077,6;-1;68,3;-1;24,6;0;1118,m;m
4948,ICLR,2020,Defensive Tensorization: Randomized Tensor Parametrization for Robust Neural Networks,Adrian Bulat;Jean Kossaifi;Sourav Bhattacharya;Yannis Panagakis;Georgios Tzimiropoulos;Nicholas D.  Lane;Maja Pantic,adrian@adrianbulat.com;jean.kossaifi@gmail.com;bsourav@gmail.com;i.panagakis@imperial.ac.uk;georgios.t@samsung.com;nic.lane@samsung.com;maja.pantic@gmail.com,6;6;6,I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,1,5,0,yes,9/25/19,Samsung;Imperial College London;;Imperial College London;Samsung;Samsung;Imperial College London,-1;73;-1;73;-1;-1;73,-1;10;-1;10;-1;-1;10,4;2,9/25/19,0,0,0,0,0,0,1373;1176;2064;1069;4733;10358;2439,29;30;150;89;102;170;71,11;10;22;18;32;45;20,175;128;180;65;565;704;265,m;f
4949,ICLR,2020,The Dual Information Bottleneck,Zoe Piran;Naftali Tishby,zoe.piran@mail.huji.ac.il;tishby@cs.huji.ac.il,3;6;3,I have published one or two papers in this area.:N/A:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,Hebrew University of Jerusalem;Hebrew University of Jerusalem,67;67,216;216,,9/25/19,0,0,0,0,0,0,2;12240,2;262,1;45,0;1231,f;m
4950,ICLR,2020,ADAPTIVE GENERATION OF PROGRAMMING PUZZLES,Ashwin Kalyan;Oleksandr Polozov;Adam Tauman Kalai,ashwinkv@gatech.edu;alex.polozov@microsoft.com;adam.kalai@microsoft.com,3;8;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,4,0,yes,9/25/19,Georgia Institute of Technology;Microsoft;Microsoft,13;-1;-1,38;-1;-1,5,9/25/19,0,0,0,0,0,0,6;559;5147,6;22;126,2;9;33,0;47;694,m;m
4951,ICLR,2020,Feature-map-level Online Adversarial Knowledge Distillation,Inseop Chung;SeongUk Park;Jangho Kim;Nojun Kwak,jis3613@snu.ac.kr;swpark0703@snu.ac.kr;kjh91@snu.ac.kr;nojunk@snu.ac.kr,3;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,Seoul National University;Seoul National University;Seoul National University;Seoul National University,41;41;41;41,64;64;64;64,4,9/25/19,3,2,1,1,0,1,9;40;212;148,12;5;81;25,2;3;9;5,4;8;22;17,m;m
4952,ICLR,2020,Graph convolutional networks for learning with few clean and many noisy labels,Ahmet Iscen;Giorgos Tolias;Yannis Avrithis;Ondrej Chum;Cordelia Schmid,iscen@google.com;giorgos.tolias@cmp.felk.cvut.cz;yannis@avrithis.net;chum@cmp.felk.cvut.cz;cordelias@google.com,6;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,3,0,yes,9/25/19,Google;Czech Technical University in Prague;INRIA;Czech Technical University in Prague;Google,-1;323;-1;323;-1,-1;956;-1;956;-1,6;10,9/25/19,1,1,1,0,0,0,414;2236;3566;14507;73934,26;56;198;102;430,9;20;32;37;114,56;369;243;2138;10183,m;f
4953,ICLR,2020,Visual Interpretability Alone Helps Adversarial Robustness,Akhilan Boopathy;Sijia Liu;Gaoyuan Zhang;Pin-Yu Chen;Shiyu Chang;Luca Daniel,akhilan@mit.edu;sijia.liu@ibm.com;gaoyuan.zhang@ibm.com;pin-yu.chen@ibm.com;shiyu.chang@ibm.com;dluca@mit.edu,3;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,9,0,yes,9/25/19,Massachusetts Institute of Technology;International Business Machines;International Business Machines;International Business Machines;International Business Machines;Massachusetts Institute of Technology,2;-1;-1;-1;-1;2,5;-1;-1;-1;-1;5,4,9/25/19,0,0,0,0,0,0,18;298;37;194;2991;62,3;52;22;44;111;19,1;11;3;6;28;4,0;24;1;22;397;4,m;m
4954,ICLR,2020,Linguistic Embeddings as a Common-Sense Knowledge Repository: Challenges and Opportunities,Nancy Fulda,nfulda@byu.edu,1;1;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,0,0,yes,9/25/19,Brigham Young University,-1,-1,3;1,9/25/19,0,0,0,0,0,0,102,22,4,12,f
4955,ICLR,2020,Starfire: Regularization-Free Adversarially-Robust Structured Sparse Training,Noah Gamboa;Kais Kudrolli;Anand Dhoot;Ardavan Pedram,ngamboa@stanford.edu;kudrolli@stanford.edu;anandd@stanford.edu;perdavan@stanford.edu,1;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Reject,0,4,0,yes,9/25/19,Stanford University;Stanford University;Stanford University;Stanford University,4;4;4;4,4;4;4;4,4,9/25/19,0,0,0,0,0,0,9;33;1;1590,3;5;8;28,2;2;1;12,1;0;0;165,m;m
4956,ICLR,2020,Pixel Co-Occurence Based Loss Metrics for Super Resolution Texture Recovery,Ying Da Wang;Pawel Swietojanski;Ryan T Armstrong;Peyman Mostaghimi,yingda.wang@unsw.edu.au;p.swietojanski@unsw.edu.au;ryan.armstrong@unsw.edu.au;peyman@unsw.edu.au,1;1;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,1,0,yes,9/25/19,University of New South Wales;University of New South Wales;University of New South Wales;University of New South Wales,481;481;481;481,1397;1397;1397;1397,5;4,9/25/19,0,0,0,0,0,0,-1;9;1726;2100,-1;3;120;101,-1;2;25;21,0;1;54;53,m;m
4957,ICLR,2020,Cross-Iteration Batch Normalization,Zhuliang Yao;Yue Cao;Shuxin Zheng;Gao Huang;Stephen Lin;Jifeng Dai,yaozhuliang13@gmail.com;yuecao@microsoft.com;shuxin.zheng@microsoft.com;gaohuang@tsinghua.edu.cn;stevelin@microsoft.com;jifdai@microsoft.com,6;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,3,9,0,yes,9/25/19,Tsinghua University;Microsoft;Microsoft;Tsinghua University;Microsoft;Microsoft,8;-1;-1;8;-1;-1,23;-1;-1;23;-1;-1,2,9/25/19,0,0,0,0,0,0,132;4;11;12600;99;7431,5;20;9;61;26;46,3;1;1;22;5;21,32;0;0;2097;10;1080,m;m
4958,ICLR,2020,Exploration via Flow-Based Intrinsic Rewards,Hsuan-Kung Yang;Po-Han Chiang;Min-Fong Hong;Chun-Yi Lee,hellochick@gapp.nthu.edu.tw;ymmoy999@gapp.nthu.edu.tw;romulus@gapp.nthu.edu.tw;cylee@gapp.nthu.edu.tw,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,10,0,yes,9/25/19,National Tsing Hua University;National Tsing Hua University;National Tsing Hua University;National Tsing Hua University,172;172;172;172,365;365;365;365,2,5/24/19,2,2,0,0,0,0,57;70;3;889,6;6;2;57,2;3;1;16,5;4;0;56,m;m
4959,ICLR,2020,Embodied Multimodal Multitask Learning,Devendra Singh Chaplot;Lisa Lee;Ruslan Salakhutdinov;Devi Parikh;Dhruv Batra,chaplot@cs.cmu.edu;lslee@cs.cmu.edu;rsalakhu@cs.cmu.edu;parikh@gatech.edu;dbatra@gatech.edu,3;3;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,3,0,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Georgia Institute of Technology;Georgia Institute of Technology,1;1;1;13;13,27;27;27;38;38,,2/4/19,2,2,0,0,0,0,787;713;69005;15347;8589,30;64;254;185;147,11;9;82;55;42,100;71;7875;2411;1166,m;m
4960,ICLR,2020,Towards Principled Objectives for Contrastive Disentanglement,Anwesa Choudhuri;Ashok Vardhan Makkuva;Ranvir Rana;Sewoong Oh;Girish Chowdhary;Alexander Schwing,anwesac2@illinois.edu;makkuva2@illinois.edu;rbrana2@illinois.edu;sewoong@cs.washington.edu;girishc@illinois.edu;aschwing@illinois.edu,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;University of Washington;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",3;3;3;6;3;3,48;48;48;26;48;48,,9/25/19,0,0,0,0,0,0,2;41;90;101;2595;3818,3;10;5;24;208;118,1;3;2;5;23;32,0;0;9;4;165;353,f;m
4961,ICLR,2020,Hebbian Graph Embeddings,Shalin Shah;Venkataramana Kini,shalin.shah@target.com;venkataramana.kini@target.com,1;1;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Johns Hopkins University;Target,73;-1,12;-1,10,8/21/19,0,0,0,0,0,0,42;29,8;10,2;3,5;1,m;m
4962,ICLR,2020,Probabilistic modeling the hidden layers of deep neural networks,Xinjie Lan;Kenneth E. Barner,lxjbit@udel.edu;barner@udel.edu,6;6;8,I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,8,0,yes,9/25/19,University of Delaware;University of Delaware,233;233,295;295,11;8,9/25/19,0,0,0,0,0,0,21;3225,10;237,3;31,0;257,m;m
4963,ICLR,2020,Empirical confidence estimates for classification by deep neural networks,Chris Finlay;Adam M. Oberman,christopher.finlay@gmail.com;adam.oberman@mcgill.ca,1;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,McGill University;McGill University,86;86,42;42,1,9/25/19,3,3,0,0,2,0,158;1689,48;88,8;22,10;157,m;m
4964,ICLR,2020,TED: A Pretrained Unsupervised Summarization Model with Theme Modeling and Denoising,Ziyi Yang;Chenguang Zhu;Michael Zeng;Xuedong Huang;Eric Darve,ziyi.yang@stanford.edu;chezhu@microsoft.com;nzeng@microsoft.com;xdh@microsoft.com;darve@stanford.edu,8;8;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:N/A:N/A,Reject,0,4,0,yes,9/25/19,Stanford University;Microsoft;Microsoft;Microsoft;Stanford University,4;-1;-1;-1;4,4;-1;-1;-1;4,,9/25/19,0,0,0,0,0,0,38;869;85;6037;4744,17;47;21;151;207,3;11;5;35;31,3;96;20;530;242,m;m
4965,ICLR,2020,Biologically Plausible Neural Networks via Evolutionary Dynamics and Dopaminergic Plasticity,Sruthi Gorantla;Anand Louis;Christos H. Papadimitriou;Santosh Vempala;Naganand Yadati,sruthi@comp.nus.edu.sg;anandl@iisc.ac.in;christos@columbia.edu;vempala@gatech.edu;y.naganand@gmail.com,1;3;3,I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,National University of Singapore;Indian Institute of Science;Columbia University;Georgia Institute of Technology;Indian Institute of Science,16;95;15;13;95,25;301;16;38;301,1,9/11/19,0,0,0,0,0,0,46;331;41569;11507;54,4;39;577;303;11,2;12;96;55;4,10;18;4277;1253;4,f;m
4966,ICLR,2020,Meta-Learning Initializations for Image Segmentation,Sean M. Hendryx;Andrew B. Leach;Paul D. Hein;Clayton T. Morrison,seanmhendryx@gmail.com;imaleach@gmail.com;pauldhein@email.arizona.edu;claytonm@email.arizona.edu,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0,yes,9/25/19,;Google;University of Arizona;University of Arizona,-1;-1;172;172,-1;-1;103;103,6;2;8,9/25/19,2,1,0,0,0,0,31;3;332;641,7;4;31;81,2;1;9;13,0;0;30;38,m;m
4967,ICLR,2020,Sequence-level Intrinsic Exploration Model for Partially Observable Domains,Haiyan Yin;Jianda Chen;Sinno Jialin Pan,yinhaiyan@outlook.com;jianda001@e.ntu.edu.sg;sinnopan@ntu.edu.sg,6;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Reject,0,4,0,yes,9/25/19,National Taiwan University;National Taiwan University;National Taiwan University,86;86;86,120;120;120,,9/25/19,0,0,0,0,0,0,413;50;12967,55;13;110,13;5;34,15;4;1205,f;m
4968,ICLR,2020,EINS: Long Short-Term Memory with Extrapolated Input Network Simplification,Nicholas I-Hsien Kuo;Mehrtash T. Harandi;Nicolas Fourrier;Gabriela Ferraro;Christian Walder;Hanna Suominen,u6424547@anu.edu.au;mehrtash.harandi@monash.edu;nicolas.fourrier@devinci.fr;gabriela.ferraro@csiro.au;gabriela.ferraro@data61.csiro.au;christian.walder@data61.csiro.au;hanna.suominen@anu.edu.au,1;1;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,"Australian National University;Monash University;Ecole Superieur d'Ingenieurs Leonard de Vinci;CSIRO;, CSIRO;, CSIRO;Australian National University",108;118;-1;-1;233;233;108,50;75;-1;-1;-1;-1;50,3;5,9/25/19,0,0,0,0,0,0,0;3039;206;270;281;4,1;100;15;39;37;7,0;27;6;10;8;1,0;342;24;18;12;0,m;f
4969,ICLR,2020,Connectivity-constrained interactive annotations for panoptic segmentation,Ruobing Shen;Bo Tang;Ismail Ben Ayed;Andrea Lodi;Thomas Guthier,ruobing.shen@gmobis.com;lucastang1994@gmail.com;ismail.benayed@etsmtl.ca;andrea.lodi@polymtl.ca;thomas.guthier@gmobis.com,1;3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Reject,0,5,0,yes,9/25/19,Hyundai Mobis;Northeastern University;École de technologie supérieure;Polytechnique Montreal;Hyundai Mobis,-1;16;481;390;-1,-1;906;1397;1397;-1,2;10,9/25/19,0,0,0,0,0,0,9;112;2255;729;44,9;45;150;59;12,2;4;26;9;4,0;3;180;95;0,m;m
4970,ICLR,2020,Frustratingly easy quasi-multitask learning,Gábor Berend;Norbert Kis-Szabó,berendg@inf.u-szeged.hu;kis-szabo.norbert@stud.u-szeged.hu,1;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,2,0,yes,9/25/19,University of Szeged;University of Szeged,323;323,874;874,8,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1,-1;-1,-1;-1,0;0,m;m
4971,ICLR,2020,Towards Effective 2-bit Quantization: Pareto-optimal Bit Allocation for Deep CNNs Compression,Zhe Wang;Jie Lin;Mohamed M. Sabry Aly;Sean I Young;Vijay Chandrasekhar;Bernd Girod,mark.wangzhe@gmail.com;lin-j@i2r.a-star.edu.sg;msabry@ntu.edu.sg;sean.i.young@stanford.edu;vijay@i2r.a-star.edu.sg;bgirod@stanford.edu,8;6;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,9,0,yes,9/25/19,Stanford University;A*STAR;National Taiwan University;Stanford University;A*STAR;Stanford University,4;-1;86;4;-1;4,4;-1;120;4;-1;4,,9/25/19,0,0,0,0,0,0,231;74;33;-1;1121;22603,102;52;16;-1;36;662,8;4;3;-1;13;73,30;5;2;0;175;1896,m;m
4972,ICLR,2020,Representing Unordered Data Using Multiset Automata and Complex Numbers,Justin DeBenedetto;David Chiang,jdebened@nd.edu;dchiang@nd.edu,6;3;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,University of Notre Dame;University of Notre Dame,118;118,157;157,,9/25/19,0,0,0,0,0,0,5;5806,10;102,1;29,0;848,m;m
4973,ICLR,2020,Domain-invariant Learning using Adaptive Filter Decomposition,Ze Wang;Xiuyuan Cheng;Guillermo Sapiro;Qiang Qiu,ze.w@duke.edu;xiuyuan.cheng@duke.edu;guillermo.sapiro@duke.edu;qiang.qiu@duke.edu,6;1;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Duke University;Duke University;Duke University;Duke University,47;47;47;47,20;20;20;20,,9/25/19,0,0,0,0,0,0,42;405;44214;31,12;52;650;16,2;12;86;3,1;35;3902;5,m;m
4974,ICLR,2020,Gradient-free Neural Network Training by Multi-convex Alternating Optimization,Junxiang Wang;Fuxun Yu;Xiang Chen;Liang Zhao,jwang40@gmu.edu;fyu2@gmu.edu;xchen26@gmu.edu;lzhao9@gmu.edu,1;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,2,0,yes,9/25/19,George Mason University;George Mason University;George Mason University;George Mason University,100;100;100;100,282;282;282;282,1;9,9/25/19,0,0,0,0,0,0,58;448;124;229,21;49;68;67,4;11;6;7,3;23;11;5,m;m
4975,ICLR,2020,GDP: Generalized Device Placement for Dataflow Graphs,Yanqi Zhou;Sudip Roy;Amirali Abdolrashidi;Daniel Wong;Peter C. Ma;Qiumin Xu;Ming Zhong;Hanxiao Liu;Anna Goldie;Azalia Mirhoseini;James Laudon,yanqiz@google.com;sudipr@google.com;abdolrashidi@google.com;wonglkd@google.com;pcma@google.com;qiuminxu@google.com;mingzhong@google.com;hanxiaol@google.com;agoldie@google.com;azalia@google.com;jlaudon@google.com,6;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,Google;Google;Google;Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,10,9/25/19,4,2,0,0,0,0,914;366;255;15;46;368;532;2015;517;1083;5854,34;59;21;6;5;25;57;35;18;56;55,13;9;9;2;3;9;7;12;6;14;20,92;14;16;0;2;30;26;522;45;79;689,f;m
4976,ICLR,2020,Policy Optimization In the Face of Uncertainty,Tung-Long Vuong;Han Nguyen;Hai Pham;Kenneth Tran,longvt94@vnu.edu.vn;hann1@andrew.cmu.edu;htpham@cs.cmu.edu;ktran@microsoft.com,3;3;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.,Reject,0,3,0,yes,9/25/19,Australian National University;Carnegie Mellon University;Carnegie Mellon University;Microsoft,108;1;1;-1,50;27;27;-1,10;8,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1;-1,-1;-1;-1;-1,-1;-1;-1;-1,0;0;0;0,m;m
4977,ICLR,2020,Learning relevant features for statistical inference,Cédric Bény,cedric.beny@gmail.com,3;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Hanyang University,233,393,11,4/23/19,0,0,0,0,0,0,417,34,12,40,m;m
4978,ICLR,2020,Accelerating Reinforcement Learning Through GPU Atari Emulation,Steven Dalton;Michael Garland;Iuri Frosio,sdalton@nvidia.com;mgarland@nvidia.com;ifrosio@nvidia.com,1;8;8,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,NVIDIA;NVIDIA;NVIDIA,-1;-1;-1,-1;-1;-1,,9/25/19,0,0,0,0,0,0,1526;13270;64,29;137;10,12;47;3,122;1585;5,m;m
4979,ICLR,2020,Discrete InfoMax Codes for Meta-Learning,Yoonho Lee;Wonjae Kim;Seungjin Choi,einet89@gmail.com;dandelin.kim@kakaocorp.com;seungjin.choi.mlg@gmail.com,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,;Kakao;BARO,-1;-1;-1,-1;-1;-1,8;1;6,9/25/19,2,2,1,0,0,0,271;40;6552,34;14;321,8;2;38,31;5;640,m;m
4980,ICLR,2020,Efficient Saliency Maps for Explainable AI,T. Nathan Mundhenk;Barry Chen;Gerald Friedland,mundhenk1@llnl.gov;chen52@llnl.gov;fractor@eecs.berkeley.edu,6;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,22,0,yes,9/25/19,Lawrence Livermore National Labs;Lawrence Livermore National Labs;University of California Berkeley,-1;-1;5,-1;-1;13,,9/25/19,0,0,0,0,0,0,337;684;8517,19;39;403,8;13;42,31;49;522,m;m
4981,ICLR,2020,Monte Carlo Deep Neural Network Arithmetic,Julian Faraone;Philip Leong,julian.faraone@sydney.edu.au;philip.leong@sydney.edu.au,3;3;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,University of Sydney;University of Sydney,86;86,60;60,,9/25/19,0,0,0,0,0,0,87;3391,9;201,3;31,11;296,m;m
4982,ICLR,2020,Language-independent Cross-lingual Contextual Representations,Xiao Zhang;Song Wang;Dejing Dou;Xien Liu;Thien Huu Nguyen;Ji Wu,xzhang19@mails.tsinghua.edu.cn;wangsong16@mails.tsinghua.edu.cn;dou@cs.uoregon.edu;xeliu@mail.tsinghua.edu.cn;thien@cs.uoregon.edu;wuji_ee@mail.tsinghua.edu.cn,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,1,0,yes,9/25/19,Tsinghua University;Tsinghua University;University of Oregon;Tsinghua University;University of Oregon;Tsinghua University,8;8;205;8;205;8,23;23;288;23;288;23,3;6,9/25/19,0,0,0,0,0,0,462;121;2318;230;1160;67,65;39;153;30;52;80,5;5;24;7;16;5,22;12;182;16;129;1,m;m
4983,ICLR,2020,Deep Nonlinear Stochastic Optimal Control for Systems with Multiplicative Uncertainties,Marcus Pereira;Ziyi Wang;Tianrong Chen;Evangelos Theodorou,mpereira30@gatech.edu;zwang450@gatech.edu;tianrong.chen@gatech.edu;evangelos.theodorou@gatech.edu,3;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology,13;13;13;13,38;38;38;38,,9/25/19,0,0,0,0,0,0,29;84;0;207,13;32;5;53,2;6;0;8,0;3;0;16,m;m
4984,ICLR,2020,Causal Induction from Visual Observations for Goal Directed Tasks,Suraj Nair;Yuke Zhu;Silvio Savarese;Li Fei-Fei,surajn@stanford.edu;yukez@cs.stanford.edu;ssilvio@stanford.edu;feifeili@cs.stanford.edu,6;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0,yes,9/25/19,Stanford University;Stanford University;Stanford University;Stanford University,4;4;4;4,4;4;4;4,6;10,9/25/19,7,3,0,0,0,0,139;2411;16959;80554,11;56;284;450,6;20;66;95,12;294;2529;11761,m;f
4985,ICLR,2020,Towards Understanding the Spectral Bias of Deep Learning,Yuan Cao;Zhiying Fang;Yue Wu;Ding-Xuan Zhou;Quanquan Gu,yuancao@cs.ucla.edu;zyfang4-c@my.cityu.edu.hk;ywu@cs.ucla.edu;mazhou@cityu.edu.hk;qgu@cs.ucla.edu,3;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,"University of California, Los Angeles;City University of Hong Kong;University of California, Los Angeles;City University of Hong Kong;University of California, Los Angeles",20;92;20;92;20,17;35;17;35;17,1;9;8,9/25/19,11,9,0,1,0,2,147;138;181;3948;3895,48;9;25;168;174,7;5;8;32;34,15;8;11;398;411,m;m
4986,ICLR,2020,Reweighted Proximal Pruning for Large-Scale Language Representation,Fu-Ming Guo;Sijia Liu;Finlay S. Mungall;Xue Lin;Yanzhi Wang,elphinkuo@gmail.com;sijia.liu@ibm.com;fmungall@gmail.com;xue.lin@northeastern.edu;yanz.wang@northeastern.edu,6;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,1,yes,9/25/19,Northeastern University;International Business Machines;;Northeastern University;Northeastern University,16;-1;-1;16;16,906;-1;-1;906;906,3;6,9/25/19,3,1,0,1,0,0,109;298;2;121;3901,38;52;1;18;262,7;11;1;6;31,2;24;0;5;227,m;m
4987,ICLR,2020,Distilled embedding: non-linear embedding factorization using knowledge distillation,Vasileios Lioutas;Ahmad Rashid;Krtin Kumar;Md Akmal Haidar;Mehdi Rezagholizadeh,vasileios.lioutas@carleton.ca;ahmad.rashid@huawei.com;krtin.kumar@huawei.com;md.akmal.haidar@huawei.com;mehdi.rezagholizadeh@huawei.com,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,Carleton University;Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.,154;-1;-1;-1;-1,535;-1;-1;-1;-1,3,9/25/19,0,0,0,0,0,0,8;2;7;79;132,6;4;3;25;32,2;1;1;6;6,0;0;0;7;5,m;m
4988,ICLR,2020,Switched linear projections and inactive state sensitivity for deep neural network interpretability,Lech Szymanski;Brendan McCane;Craig Atkinson,lechszym@cs.otago.ac.nz;mccane@cs.otago.ac.nz;atkcr398@student.otago.ac.nz,1;3;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,17,0,yes,9/25/19,University of Otago;University of Otago;University of Otago,390;390;390,218;218;218,,9/25/19,0,0,0,0,0,0,105;1208;27,34;124;34,6;15;2,6;73;5,m;m
4989,ICLR,2020,SpectroBank: A filter-bank convolutional layer for CNN-based audio applications,Helena Peic Tukuljac;Benjamin Ricaud;Nicolas Aspert;Pierre Vandergheynst,helena.peictukuljac@epfl.ch;benjamin.ricaud@epfl.ch;nicolas.aspert@epfl.ch;pierre.vandergheynst@epfl.ch,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0,yes,9/25/19,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne,481;481;481;481,38;38;38;38,,9/25/19,1,0,1,0,0,0,1;550;1214;46,3;50;25;9,1;10;10;4,0;37;63;3,f;m
4990,ICLR,2020,Stochastic Latent Residual Video Prediction,Jean-Yves Franceschi;Edouard Delasalles;Mickael Chen;Sylvain Lamprier;Patrick Gallinari,jean-yves.franceschi@lip6.fr;edouard.delasalles@lip6.fr;mickael.chen@lip6.fr;sylvain.lamprier@lip6.fr;patrick.gallinari@lip6.fr,6;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,1,yes,9/25/19,LIP6;LIP6;LIP6;LIP6;LIP6,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,,9/25/19,0,0,0,0,0,0,34;20;1;254;75,5;7;2;57;24,3;1;1;7;5,5;1;0;35;6,m;m
4991,ICLR,2020,Large-scale Pretraining for Neural Machine Translation with Tens of Billions of Sentence Pairs,Yuxian Meng;Xiangyuan Ren;Zijun Sun;Xiaoya Li;Arianna Yuan;Fei Wu;Jiwei Li,yuxian_meng@shannonai.com;xiangyuan_re@shannonai.com;zijun_sun@shannonai.com;xiaoya_li@shannonai.com;xfyuan@stanford.edu;wufei@zju.edu.cn;jiwei_li@shannonai.com,3;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,1,4,0,yes,9/25/19,Shannon.AI;Shannon.AI;Shannon.AI;Shannon.AI;Stanford University;Zhejiang University;Shannon.AI,-1;-1;-1;-1;4;56;-1,-1;-1;-1;-1;4;107;-1,3,9/25/19,2,0,0,0,0,0,54;51;103;66;81;71;5854,14;2;10;9;17;60;100,4;2;4;4;5;5;30,4;0;7;7;7;11;844,m;m
4992,ICLR,2020,Random Matrix Theory Proves that Deep Learning Representations of GAN-data Behave as Gaussian Mixtures,Mohamed El Amine Seddik;Cosme Louart;Mohamed Tamaazousti;Romain Couillet,melaseddik@gmail.com;cosme.louart@gmail.com;mohamed.tamaazousti@cea.fr;romain.couillet@gmail.com,6;3;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,CEA;CEA;CEA;,233;233;233;-1,1027;1027;1027;-1,5;4,9/25/19,7,2,1,0,0,0,49;78;206;2914,11;10;41;225,4;4;7;26,2;7;14;251,m;m
4993,ICLR,2020,The advantage of using Student's t-priors in variational autoencoders,Najmeh Abiri;Mattias Ohlsson,najmeh@thep.lu.se;mattias@thep.lu.se,1;1;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,0,0,yes,9/25/19,Lund University;Lund University,390;390,98;98,5,9/25/19,1,0,0,0,0,0,2;23,4;14,1;1,0;2,f;m
4994,ICLR,2020,Neural Communication Systems with Bandwidth-limited Channel,Karen Ullrich;Fabio Viola;Danilo J. Rezende,mail.karen.ullrich@gmail.com;fviola@google.com;danilor@google.com,3;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,University of Amsterdam;Google;Google,172;-1;-1,62;-1;-1,,9/25/19,0,0,0,0,0,0,532;190;8814,10;6;63,6;1;27,73;32;1132,f;m
4995,ICLR,2020,Unified Probabilistic Deep Continual Learning through Generative Replay and Open Set Recognition,Martin Mundt;Sagnik Majumder;Iuliia Pliushch;Visvanathan Ramesh,mmundt@em.uni-frankfurt.de;majumder@ccc.cs.uni-frankfurt.de;pliushch@em.uni-frankfurt.de;ramesh@fias.uni-frankfurt.de,6;3;3,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Goethe University;Goethe University;Goethe University;Goethe University,233;233;233;233,305;305;305;305,5;11,5/28/19,2,0,0,0,4,0,240;28;17;9835,21;9;6;106,6;3;2;33,8;2;2;894,m;m
4996,ICLR,2020,Optimizing Loss Landscape Connectivity via Neuron Alignment,N. Joseph Tatro;Pin-Yu Chen;Payel Das;Igor Melnyk;Prasanna Sattigeri;Rongjie Lai,tatron@rpi.edu;pin-yu.chen@ibm.com;daspa@us.ibm.com;igor.melnyk@ibm.com;psattig@us.ibm.com;lair@rpi.edu,1;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,Rensselaer Polytechnic Institute;International Business Machines;International Business Machines;International Business Machines;International Business Machines;Rensselaer Polytechnic Institute,172;-1;-1;-1;-1;172,438;-1;-1;-1;-1;438,,9/25/19,0,0,0,0,0,0,0;194;545;258;610;951,2;44;67;33;59;61,0;6;11;11;12;18,0;22;19;17;73;55,m;m
4997,ICLR,2020,Adaptive Online Planning for Continual Lifelong Learning,Kevin Lu;Igor Mordatch;Pieter Abbeel,kzl@berkeley.edu;imordatch@google.com;pabbeel@cs.berkeley.edu,1;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,5,0,yes,9/25/19,University of California Berkeley;Google;University of California Berkeley,5;-1;5,13;-1;13,,9/25/19,2,1,1,0,0,0,15;3078;37294,12;49;438,2;27;94,2;351;4481,m;m
4998,ICLR,2020,Adversarially Robust Generalization Just Requires More Unlabeled Data,Runtian Zhai;Tianle Cai;Di He;Chen Dan;Kun He;John E. Hopcroft;Liwei Wang,zhairuntian@pku.edu.cn;caitianle1998@pku.edu.cn;dihe@microsoft.com;cdan@cs.cmu.edu;brooklet60@hust.edu.cn;jeh17@cornell.edu;wanglw@cis.pku.edu.cn,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,Peking University;Peking University;Microsoft;Carnegie Mellon University;Hong Kong University of Science and Technology;Cornell University;Peking University,22;22;-1;1;39;7;22,24;24;-1;27;47;19;24,4;1;8,6/3/19,25,7,2,2,0,1,35;58;2719;73;620;28945;293,2;10;259;68;38;303;36,2;4;27;5;15;60;10,4;5;110;6;63;2742;31,m;m
4999,ICLR,2020,Filling the Soap Bubbles: Efficient Black-Box Adversarial Certification with Non-Gaussian Smoothing,Dinghuai Zhang*;Mao Ye*;Chengyue Gong*;Zhanxing Zhu;Qiang Liu,zhangdinghuai@pku.edu.cn;lushleaf21@gmail.com;cygong@cs.utexas.edu;zhanxing.zhu@pku.edu.cn;lqiang@cs.utexas.edu,1;1;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,2,5,0,yes,9/25/19,"Peking University;University of Electronic Science and Technology of China;University of Texas, Austin;Peking University;University of Texas, Austin",22;481;22;22;22,24;628;38;24;38,4,9/25/19,1,0,0,0,0,0,82;122;130;891;60,5;50;14;80;70,2;5;5;14;3,14;16;15;112;9,m;m
5000,ICLR,2020,Zeno++: Robust Fully Asynchronous SGD,Cong Xie;Oluwasanmi Koyejo;Indranil Gupta,cx2@illinois.edu;sanmi@illinois.edu;indy@illinois.edu,3;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,9,0,yes,9/25/19,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",3;3;3,48;48;48,1,9/25/19,2,2,1,0,0,1,68;1232;4390,13;98;210,5;17;34,7;126;364,m;m
5001,ICLR,2020,LARGE SCALE REPRESENTATION LEARNING FROM TRIPLET COMPARISONS,Siavash Haghiri;Leena Chennuru Vankadara;Ulrike von Luxburg,siyavash.haghiri@gmail.com;leena.chennuru-vankadara@uni-tuebingen.de;luxburg@informatik.uni-tuebingen.de,3;1;3;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,9,0,yes,9/25/19,University of Tuebingen;University of Tuebingen;University of Tuebingen,154;154;154,91;91;91,,9/25/19,0,0,0,0,0,0,45;4;8861,11;4;94,4;1;31,3;0;916,m;f
5002,ICLR,2020,A Mean-Field Theory for Kernel Alignment with Random Features in Generative Adverserial Networks,Masoud Badiei Khuzani;Liyue Shen;Shahin Shahrampour;Lei Xing,mbadieik@stanford.edu;liyues@stanford.edu;shahin@tamu.edu;lei@stanford.edu,1;6;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Stanford University;Stanford University;Texas A&M;Stanford University,4;4;44;4,4;4;177;4,5;4;1,9/25/19,0,0,0,0,0,0,114;1368;628;1657,23;21;42;199,5;6;12;20,5;483;75;76,m;m
5003,ICLR,2020,Homogeneous Linear Inequality Constraints for Neural Network Activations,Thomas Frerix;Matthias Nießner;Daniel Cremers,thomas.frerix@tum.de;niessner@tum.de;cremers@tum.de,1;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,4,0,yes,9/25/19,Technical University Munich;Technical University Munich;Technical University Munich,53;53;53,43;43;43,5,2/5/19,1,0,0,0,0,0,146;21875;25570,7;140;494,4;39;78,21;1787;2751,m;m
5004,ICLR,2020,Distributed Online Optimization with Long-Term Constraints,Deming Yuan;Alexandre Proutiere;Guodong Shi,dmyuan1012@gmail.com;alepro@kth.se;guodong.shi@anu.edu.au,3;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,";KTH Royal Institute of Technology, Stockholm, Sweden;Australian National University",-1;128;108,-1;222;50,9;10,9/25/19,0,0,0,0,0,0,583;99;2105,40;18;147,14;5;23,24;9;103,m;m
5005,ICLR,2020,Semi-supervised Pose Estimation with Geometric Latent Representations,Luis A. Perez Rey;Dmitri Jarnikov;Mike Holenderski,l.a.perez.rey@tue.nl;d.s.jarnikov@tue.nl;m.holenderski@tue.nl,3;3;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Eindhoven University of Technology;Eindhoven University of Technology;Eindhoven University of Technology,205;205;205,185;185;185,5;2,9/25/19,0,0,0,0,0,0,17;136;98,5;25;16,2;5;5,0;7;10,m;m
5006,ICLR,2020,Parallel Scheduled Sampling,Daniel Duckworth;Arvind Neelakantan;Ben Goodrich;Lukasz Kaiser;Samy Bengio,duckworthd@google.com;aneelakantan@google.com;bgoodrich@google.com;lukaszkaiser@google.com;bengio@google.com,6;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0,yes,9/25/19,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,,6/11/19,2,1,1,0,3,1,209;1501;2640;22913;26795,24;24;30;75;332,5;14;11;23;67,17;161;403;3899;3497,m;m
5007,ICLR,2020,ADAPTING PRETRAINED LANGUAGE MODELS FOR LONG DOCUMENT CLASSIFICATION,Matthew Lyle Olson;Lisa Zhang;Chun-Nam Yu,olsomatt@oregonstate.edu;lisa.zhang@nokia-bell-labs.com;cnyu@cs.cornell.edu,3;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Reject,0,3,0,yes,9/25/19,Oregon State University;Nokia Bell Labs;Cornell University,77;-1;7,373;-1;19,3,9/25/19,0,0,0,0,0,0,72;241;277,12;21;15,4;9;6,2;22;23,m;m
5008,ICLR,2020,Composable Semi-parametric Modelling for Long-range Motion Generation,Jingwei Xu;Huazhe Xu;Bingbing Ni;Xiaokang Yang;Trevor Darrell,xjwxjw@sjtu.edu.cn;huazhe_xu@eecs.berkeley.edu;nibingbing@sjtu.edu.cn;xkyang@sjtu.edu.cn;trevor@eecs.berkeley.edu,3;6;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Shanghai Jiao Tong University;University of California Berkeley;Shanghai Jiao Tong University;Shanghai Jiao Tong University;University of California Berkeley,53;5;53;53;5,157;13;157;157;13,,9/25/19,0,0,0,0,0,0,655;815;4304;689;90979,62;14;171;218;559,15;7;36;14;112,25;99;334;47;11527,m;m
5009,ICLR,2020,Finding Mixed Strategy Nash Equilibrium for Continuous Games through Deep Learning,Zehao Dou;Xiang Yan;Dongge Wang;Xiaotie Deng,zehaodou@pku.edu.cn;yxghost@sjtu.edu.cn;dgwang96@pku.edu.cn;xiaotie@pku.edu.cn,6;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,9,0,yes,9/25/19,Peking University;Shanghai Jiao Tong University;Peking University;Peking University,22;53;22;22,24;157;24;24,5;4,9/25/19,0,0,0,0,0,0,12;399;36;49,5;95;11;23,2;11;4;4,0;27;0;2,m;m
5010,ICLR,2020,Omnibus Dropout for Improving The Probabilistic Classification Outputs of ConvNets,Zhilu Zhang;Adrian V. Dalca;Mert R. Sabuncu,zz452@cornell.edu;adalca@mit.edu;msabuncu@cornell.edu,3;1;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,3,0,yes,9/25/19,Cornell University;Massachusetts Institute of Technology;Cornell University,7;2;7,19;5;19,11,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1,-1;-1;-1,-1;-1;-1,0;0;0,m;m
5011,ICLR,2020,Online Learned Continual Compression with Stacked Quantization Modules,Lucas Caccia;Eugene Belilovsky;Massimo Caccia;Joelle Pineau,lucas.page-caccia@mail.mcgill.ca;belilovsky.eugene@gmail.com;massimo.p.caccia@gmail.com;jpineau@cs.mcgill.ca,3;3;6;3;6,I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,1,4,0,yes,9/25/19,McGill University;University of Montreal;University of Montreal;McGill University,86;128;128;86,42;85;85;42,,9/25/19,5,2,2,0,0,0,89;305;2362;11328,9;30;234;267,3;10;21;46,19;32;193;1235,m;f
5012,ICLR,2020,Yet another but more efficient black-box adversarial attack: tiling and evolution strategies,Laurent Meunier;Jamal Atif;Olivier Teytaud,laurent.meunier1995@gmail.com;jamal.atif@dauphine.fr;oteytaud@fb.com,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Univeristé Paris-Dauphine;Univeristé Paris-Dauphine;Facebook,481;481;-1,1397;1397;-1,4,9/25/19,5,3,3,0,0,1,356;906;2697,19;114;273,6;14;25,44;38;187,m;m
5013,ICLR,2020,AlignNet: Self-supervised Alignment Module,Antonia Creswell;Luis Piloto;David Barrett;Kyriacos Nikiforou;David Raposo;Marta Garnelo;Peter Battaglia;Murray Shanahan,tonicreswell@google.com;piloto@google.com;peterbattaglia@google.com;knikiforou@google.com;barrettdavid@google.com;garnelo@google.com;mshanahan@google.com;draposo@google.com,6;3;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Google;Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1,10,9/25/19,0,0,0,0,0,0,469;78;1209;32;41;952;4598;4905,15;5;24;12;5;23;88;166,7;4;11;3;2;11;29;38,34;8;162;1;8;116;424;460,f;m
5014,ICLR,2020,A Simple Recurrent Unit with Reduced Tensor Product Representations,Shuai Tang;Paul Smolensky;Virginia R. de Sa,shuaitang93@ucsd.edu;paul.smolensky@gmail.com;desa@ucsd.edu,6;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,1,0,yes,9/25/19,"University of California, San Diego;Microsoft;University of California, San Diego",11;-1;11,31;-1;31,3;1,10/29/18,0,0,0,0,0,0,80;10024;1070,24;206;81,5;40;17,4;933;55,m;f
5015,ICLR,2020,A Hierarchy of Graph Neural Networks Based on Learnable Local Features,Michael Lingzhi Li;Meng Dong;Jiawei Zhou;Alexander M. Rush,mlli@mit.edu;mengdong@g.harvard.edu;jzhou02@g.harvard.edu;srush@cornell.edu,3;8;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,6,0,yes,9/25/19,Massachusetts Institute of Technology;Harvard University;Harvard University;Cornell University,2;39;39;7,5;7;7;19,10,9/25/19,1,1,0,0,0,0,76;138;19;7049,20;31;15;86,4;4;2;32,8;3;1;941,m;m
5016,ICLR,2020,Discriminability Distillation in Group Representation Learning,Manyuan Zhang，Guanglu Song，Yu Liu，Hang Zhou,zhangmanyuan@sensetime.com;songguanglu@sensetime.com;yuliu@ee.cuhk.edu.hk;zhouhang@link.cuhk.edu.hk,3;1;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,SenseTime Group Limited;SenseTime Group Limited;The Chinese University of Hong Kong;The Chinese University of Hong Kong,-1;-1;59;59,-1;-1;35;35,2,9/25/19,0,0,0,0,0,0,65,12,4,0,f;m
5017,ICLR,2020,Blurring Structure and Learning to Optimize and Adapt Receptive Fields,Evan Shelhamer;Dequan Wang;Trevor Darrell,shelhamer@cs.berkeley.edu;dqwang@eecs.berkeley.edu;trevor@eecs.berkeley.edu,3;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,13;13;13,2,4/25/19,14,4,3,0,0,0,27530;1001;90979,26;23;559,14;11;112,4172;199;11527,m;m
5018,ICLR,2020,Conditional Flow Variational Autoencoders for Structured Sequence Prediction,Apratim Bhattacharyya;Michael Hanselmann;Mario Fritz;Bernt Schiele;Christoph-Nikolas Straehle,abhattac@mpi-inf.mpg.de;michael.hanselmann@de.bosch.com;fritz@cispa.saarland;schiele@mpi-inf.mpg.de;christoph-nikolas.straehle@de.bosch.com,6;6;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,"Saarland Informatics Campus, Max-Planck Institute;Bosch;CISPA Helmholtz Center for Information Security;Saarland Informatics Campus, Max-Planck Institute;Bosch",-1;-1;143;-1;-1,-1;-1;1397;-1;-1,5,8/24/19,7,4,2,0,0,3,152;287;7800;41307;42,18;29;198;503;3,7;7;46;99;2,18;24;1012;5119;5,m;m
5019,ICLR,2020,LAVAE: Disentangling Location and Appearance,Andrea Dittadi;Ole Winther,adit@dtu.dk;olwi@dtu.dk,1;1;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,2,0,yes,9/25/19,Technical University of Denmark;Technical University of Denmark,481;481,182;182,5,9/25/19,0,0,0,0,0,0,2;6191,5;202,1;35,0;714,m;m
5020,ICLR,2020,BETANAS: Balanced Training and selective drop for Neural Architecture Search,Muyuan Fang;Qiang Wang;Jian Zhang;Zhao Zhong,fangmuyuan@huawei.com;wangqiang168@huawei.com;zhangjian157@huawei.com;zorro.zhongzhao@huawei.com,3;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.,-1;-1;-1;-1,-1;-1;-1;-1,7,9/25/19,1,0,0,0,0,0,5;139;-1;702,3;95;-1;254,2;6;-1;11,0;8;0;68,f;m
5021,ICLR,2020,Unifying Graph Convolutional Neural Networks and Label Propagation,Hongwei Wang;Jure Leskovec,wanghongwei55@gmail.com;jure@cs.stanford.edu,3;6;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Stanford University;Stanford University,4;4,4;4,10,9/25/19,5,3,1,0,0,0,93;48502,35;302,5;93,10;6080,m;m
5022,ICLR,2020,Defective Convolutional Layers Learn Robust CNNs,Tiange Luo;Tianle Cai;Xiaomeng Zhang;Siyu Chen;Di He;Liwei Wang,luotg@pku.edu.cn;caitianle1998@pku.edu.cn;zhan147@usc.edu;siyuchen@pku.edu.cn;dihe@microsoft.com;wanglw@cis.pku.edu.cn,3;1;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Peking University;Peking University;University of Southern California;Peking University;Microsoft;Peking University,22;22;31;22;-1;22,24;24;62;24;-1;24,4,9/25/19,0,0,0,0,0,0,104;58;114;19;40;308,8;10;27;21;29;36,4;4;7;2;4;10,23;5;4;0;10;32,m;m
5023,ICLR,2020,Unsupervised Progressive Learning and the STAM Architecture,James Smith;Constantine Dovrolis,jamessealesmith@gatech.edu;constantine@gatech.edu,8;3;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,2,yes,9/25/19,Georgia Institute of Technology;Georgia Institute of Technology,13;13,38;38,,4/3/19,1,1,0,0,0,0,13;8595,17;143,1;45,0;903,m;m
5024,ICLR,2020,Asymptotic learning curves of kernel methods: empirical data v.s. Teacher-Student paradigm,Stefano Spigler;Mario Geiger;Matthieu Wyart,stefano.spigler@epfl.ch;mario.geiger@epfl.ch;matthieu.wyart@epfl.ch,3;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne,481;481;481,38;38;38,8,5/26/19,5,2,1,0,0,0,201;603;3256,12;25;148,6;10;35,15;92;243,m;m
5025,ICLR,2020,Sticking to the Facts: Confident Decoding for Faithful Data-to-Text Generation,Ran Tian;Shashi Narayan;Thibault Sellam;Ankur P. Parikh,tianran@google.com;shashinarayan@google.com;tsellam@google.com;aparikh@google.com,6;3;8;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,9,0,yes,9/25/19,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,,9/25/19,4,4,2,0,0,1,19;638;163;1519,29;47;29;43,3;13;7;17,2;84;4;191,m;m
5026,ICLR,2020,Evidence-Aware Entropy Decomposition For  Active Deep Learning,Weishi Shi;Xujiang Zhao;Feng Chen;Qi Yu,ws7586@rit.edu;xujiang.zhao@utdallas.edu;feng.chen@utdallas.edu;qi.yu@rit.edu,6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,"Rochester Institute of Technology;University of Texas, Dallas;University of Texas, Dallas;Rochester Institute of Technology",128;86;86;128,843;319;319;843,,9/25/19,0,0,0,0,0,0,18;0;320;5,12;2;16;12,2;0;4;1,1;0;32;0,m;m
5027,ICLR,2020,Meta-Learning by Hallucinating Useful Examples,Yu-Xiong Wang;Yuki Uchiyama;Martial Hebert;Karteek Alahari,yuxiongw@cs.cmu.edu;braverthan2@gmail.com;hebert@ri.cmu.edu;karteek.alahari@inria.fr,3;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,5,0,yes,9/25/19,Carnegie Mellon University;;Carnegie Mellon University;INRIA,1;-1;1;-1,27;-1;27;-1,6,9/25/19,0,0,0,0,0,0,1430;56;1955;2976,34;35;47;76,16;5;12;26,119;1;174;381,m;m
5028,ICLR,2020,PDP: A General Neural Framework for Learning SAT Solvers,Saeed Amizadeh;Sergiy Matusevych;Markus Weimer,saamizad@microsoft.com;sergiym@microsoft.com;markus.weimer@microsoft.com,1;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,3,0,yes,9/25/19,Microsoft;Microsoft;Microsoft,-1;-1;-1,-1;-1;-1,10,9/25/19,0,0,0,0,0,0,263;98;2408,23;9;80,6;5;19,30;14;275,m;m
5029,ICLR,2020,Consistency-Based Semi-Supervised Active Learning: Towards Minimizing Labeling Budget,Mingfei Gao;Zizhao Zhang;Guo Yu;Sercan O. Arik;Larry S. Davis;Tomas Pfister,mgao@cs.umd.edu;zizhaoz@google.com;gy63@uw.edu;soarik@google.com;lsd@umiacs.umd.edu;tpfister@google.com,6;6,I have published in this field for several years.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,"University of Maryland, College Park;Google;University of Washington, Seattle;Google;University of Maryland, College Park;Google",12;-1;6;-1;12;-1,91;-1;26;-1;91;-1,,9/25/19,5,1,2,0,0,1,373;978;110;1391;38252;2335,29;59;110;54;752;47,7;15;5;17;99;16,51;102;16;119;3269;285,m;m
5030,ICLR,2020,TabNet: Attentive Interpretable Tabular Learning,Sercan O. Arik;Tomas Pfister,soarik@google.com;tpfister@google.com,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Reject,1,8,0,yes,9/25/19,Google;Google,-1;-1,-1;-1,,8/20/19,10,5,3,2,0,2,1391;2335,54;47,17;16,119;285,m;m
5031,ICLR,2020,Adapting Behaviour for Learning Progress,Tom Schaul;Diana Borsa;David Ding;David Szepesvari;Georg Ostrovski;Will Dabney;Simon Osindero,schaul@google.com;borsa@google.com;fding@google.com;dsz@google.com;ostrovski@google.com;wdabney@google.com;osindero@google.com,6;3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,6,0,yes,9/25/19,Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,,9/25/19,4,1,1,0,0,0,8594;197;849;332;11001;1790;14818,83;16;152;4;16;25;38,30;9;16;3;10;13;21,1150;8;92;52;1889;340;1866,m;m
5032,ICLR,2020,Improving Differentially Private Models with Active Learning,Zhengli Zhao;Nicolas Papernot;Sameer Singh;Neoklis Polyzotis;Augustus Odena,zhengliz@uci.edu;papernot@google.com;sameer@uci.edu;npolyzotis@google.com;augustusodena@google.com,3;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,1,5,0,yes,9/25/19,"University of California, Irvine;Google;University of California, Irvine;Google;Google",35;-1;35;-1;-1,96;-1;96;-1;-1,,9/25/19,0,0,0,0,0,0,432;9462;5682;4254;3293,7;66;117;138;25,4;27;27;39;13,28;1100;775;447;477,m;m
5033,ICLR,2020,Style-based Encoder Pre-training for Multi-modal Image Synthesis,Moustafa Meshry;Yixuan Ren;Ricardo Martin-Brualla;Larry Davis;Abhinav Shrivastava,mmeshry@cs.umd.edu;yxren@cs.umd.edu;rmbrualla@google.com;lsd@umiacs.umd.edu;abhinav@cs.umd.edu,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,"University of Maryland, College Park;University of Maryland, College Park;Google;University of Maryland, College Park;University of Maryland, College Park",12;12;-1;12;12,91;91;-1;91;91,5,9/25/19,0,0,0,0,0,0,45;84;215;38252;161,10;16;17;752;29,3;4;7;99;4,4;3;12;3269;9,m;m
5034,ICLR,2020,GRAPH ANALYSIS AND GRAPH POOLING IN THE SPATIAL DOMAIN,Mostafa Rahmani;Ping Li,mostafarahmani@baidu.com;liping11@baidu.com,6;6;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,9,1,yes,9/25/19,Baidu;Baidu,-1;-1,-1;-1,10,9/25/19,0,0,0,0,0,0,392;305,47;107,9;9,23;22,m;m
5035,ICLR,2020,Cover Filtration and Stable Paths in the Mapper,Dustin L. Arendt;Matthew Broussard;Bala Krishnamoorthy;Nathaniel Saul,dustin.arendt@pnnl.gov;matthew.broussard@wsu.edu;kbala@wsu.edu;nat@riverasaul.com,1;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0,yes,9/25/19,Pacific Northwest National Laboratory;SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;Riverasaul,-1;481;481;-1,-1;299;299;-1,8,9/25/19,0,0,0,0,0,0,180;2;379;500,46;6;77;10,8;1;9;3,10;0;26;34,m;m
5036,ICLR,2020,Rigging the Lottery: Making All Tickets Winners,Utku Evci;Erich Elsen;Pablo Castro;Trevor Gale,ue225@nyu.edu;eriche@google.com;tgale@google.com;psc@google.com,3;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,13,4,yes,9/25/19,New York University;Google;Google;Google,25;-1;-1;-1,29;-1;-1;-1,,9/25/19,12,7,5,1,0,3,153;4689;847;2344,8;53;30;197,3;21;14;26,17;493;53;279,m;m
5037,ICLR,2020,Disentangled GANs for Controllable Generation of High-Resolution Images,Weili Nie;Tero Karras;Animesh Garg;Shoubhik Debhath;Anjul Patney;Ankit B. Patel;Anima Anandkumar,wn8@rice.edu;tkarras@nvidia.com;garg@cs.toronto.edu;shoubhikdn@gmail.com;anjul.patney@gmail.com;abp4@rice.edu;animakumar@gmail.com,3;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,5,0,yes,9/25/19,"Rice University;NVIDIA;Department of Computer Science, University of Toronto;NVIDIA;Facebook;Rice University;University of California-Irvine",84;-1;18;-1;-1;84;35,105;-1;18;-1;-1;105;96,5;4;8,9/25/19,0,0,0,0,0,0,9;4781;1040;0;937;378;5451,3;42;79;2;41;30;187,2;21;20;0;16;6;38,1;859;66;0;75;39;761,m;f
5038,ICLR,2020,UNIVERSAL MODAL EMBEDDING OF DYNAMICS IN VIDEOS AND ITS APPLICATIONS,Israr Ul Haq;Yoshinobu Kawahara,israr.haq@riken.jp;kawahara@imi.kyushu-u.ac.jp,3;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,RIKEN;Meiji University,-1;481,-1;332,2,9/25/19,0,0,0,0,0,0,65;24,36;12,4;2,3;0,m;m
5039,ICLR,2020,First-Order Preconditioning via Hypergradient Descent,Ted Moskovitz;Rui Wang;Janice Lan;Sanyam Kapoor;Thomas Miconi;Jason Yosinski;Aditya Rawal,thmoskovitz@gmail.com;ruiwang@uber.com;janlan@uber.com;sanyam@uber.com;tmiconi@uber.com;yosinski@uber.com;aditya.rawal@uber.com,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,University College London;Uber;Uber;Uber;Uber;Uber;Uber,50;-1;-1;-1;-1;-1;-1,15;-1;-1;-1;-1;-1;-1,,9/25/19,0,0,0,0,0,0,0;189;73;39;458;8488;724,1;78;7;4;42;52;48,0;7;3;2;13;22;13,0;7;12;5;55;655;38,m;m
5040,ICLR,2020,Zero-shot task adaptation by homoiconic meta-mapping,Andrew K. Lampinen;James L. McClelland,lampinen@stanford.edu;jlmcc@stanford.edu,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A,Reject,0,8,0,yes,9/25/19,Stanford University;Stanford University,4;4,4;4,6,5/23/19,1,0,0,0,0,0,91;43165,22;424,5;82,6;3307,m;m
5041,ICLR,2020,MLModelScope: A Distributed Platform for ML Model Evaluation and Benchmarking at Scale,Cheng Li;Abdul Dakkak;Jinjun Xiong;Wen-mei Hwu,cli99@illinois.edu;dakkak@illinois.edu;jinjun@us.ibm.com;w-hwu@illinois.edu,1;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,13,0,yes,9/25/19,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;International Business Machines;University of Illinois, Urbana Champaign",3;3;-1;3,48;48;-1;48,,9/25/19,0,0,0,0,0,0,17;98;1661;11456,74;28;171;448,2;6;21;54,0;3;158;860,m;m
5042,ICLR,2020,Efficient Multivariate Bandit Algorithm with Path Planning,Keyu Nie;Zezhong Zhang;Ted Tao Yuan;Rong Song;Pauline Berry Burke,keyunie@google.com;zezzhang@ebay.com;teyuan@ebay.com;rsong@ebay.com;pmburke10@gmail.com,3;1;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,2,0,yes,9/25/19,Google;eBay;eBay;eBay;,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,10,9/6/19,0,0,0,0,0,0,1;47;6;1030;7,8;14;8;127;7,1;4;1;18;1,0;2;0;29;0,m;f
5043,ICLR,2020,Statistical Adaptive Stochastic Optimization,Pengchuan Zhang;Hunter Lang;Qiang Liu;Lin Xiao,penzhan@microsoft.com;hjl@mit.edu;lqiang@cs.utexas.edu;lin.xiao@microsoft.com,6;8;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,"Microsoft;Massachusetts Institute of Technology;University of Texas, Austin;Microsoft",-1;2;22;-1,-1;5;38;-1,,9/25/19,0,0,0,0,0,0,614;14;60;206,29;9;70;66,9;3;3;10,104;0;9;14,m;m
5044,ICLR,2020,The Benefits of Over-parameterization at Initialization in Deep ReLU Networks,Devansh Arpit;Yoshua Bengio,devansharpit@gmail.com;yoshua.bengio@mila.quebec,1;3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,SalesForce.com;University of Montreal,-1;128,-1;85,1,1/11/19,6,6,2,0,19,2,921;208566,42;807,12;147,118;24297,m;m
5045,ICLR,2020,Multichannel Generative Language Models,Harris Chan;Jamie Kiros;William Chan,hchan@cs.toronto.edu;kiros@google.com;williamchan@google.com,3;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0,yes,9/25/19,"Department of Computer Science, University of Toronto;Google;Google",18;-1;-1,18;-1;-1,3;5,9/25/19,0,0,0,0,0,0,23;1435;131,13;18;29,3;9;5,4;177;5,m;m
5046,ICLR,2020,Set Functions for Time Series,Max Horn;Michael Moor;Christian Bock;Bastian Rieck;Karsten Borgwardt,max.horn@bsse.ethz.ch;michael.moor@bsse.ethz.ch;christian.bock@bsse.ethz.ch;bastian.rieck@bsse.ethz.ch;karsten.borgwardt@bsse.ethz.ch,6;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,6,0,yes,9/25/19,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10;10;10,13;13;13;13;13,,9/25/19,1,0,0,0,0,0,60;47;42;233;12602,10;11;25;39;170,5;4;3;9;41,3;3;1;11;1829,m;m
5047,ICLR,2020,HIPPOCAMPAL NEURONAL REPRESENTATIONS IN CONTINUAL LEARNING,Samia Mohinta;Rui Ponte Costa;Stephane Ciocchi,dc18393@bristol.ac.uk;rui.costa@bristol.ac.uk,1;1;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,University of Bristol;University of Bristol,128;128,87;87,,9/25/19,0,0,0,0,0,0,0;388;257,1;45;4,0;10;2,0;22;26,f;m
5048,ICLR,2020,Augmenting Transformers with KNN-Based Composite Memory,Angela Fan;Claire Gardent;Chloe Braud;Antoine Bordes,angelafan@fb.com;claire.gardent@loria.fr;chloe.braud@loria.fr;abordes@fb.com,6;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Facebook;University of Lorraine;University of Lorraine;Facebook,-1;481;481;-1,-1;624;624;-1,5,9/25/19,1,0,0,0,0,1,395;2334;216;16259,54;229;24;76,11;23;8;32,36;220;19;2389,f;m
5049,ICLR,2020,Winning Privately: The Differentially Private Lottery Ticket Mechanism,Lovedeep Gondara;Ke Wang;Ricardo Silva Carvalho,lgondara@sfu.ca;wang@sfu.ca;ricardo_silva_carvalho@sfu.ca,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,3,0,yes,9/25/19,Simon Fraser University;Simon Fraser University;Simon Fraser University,64;64;64,272;272;272,,9/25/19,1,1,0,0,0,0,503;581;17,83;104;13,10;7;3,18;32;0,m;m
5050,ICLR,2020,Localizing and Amortizing: Efficient Inference for Gaussian Processes,Linfeng Liu;Liping Liu,linfeng.liu@tufts.edu;liping.liu@tufts.edu,1;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,1,5,0,yes,9/25/19,Tufts University;Tufts University,172;172,139;139,,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1,-1;-1,-1;-1,0;0,m;m
5051,ICLR,2020,BAIL: Best-Action Imitation Learning for Batch Deep Reinforcement Learning,Xinyue Chen;Zijian Zhou;Zheng Wang;Che Wang;Yanqiu Wu;Qing Deng;Keith Ross,xc1305@nyu.edu;zz1435@nyu.edu;zw1454@nyu.edu;cw1681@nyu.edu;yanqiu.wu@nyu.edu;qd319@nyu.edu;keithwross@nyu.edu,3;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,3,0,yes,9/25/19,New York University;New York University;New York University;New York University;New York University;New York University;New York University,25;25;25;25;25;25;25,29;29;29;29;29;29;29,,9/25/19,2,1,1,1,0,1,71;373;524;14;102;492;12156,10;33;68;13;19;67;295,2;11;9;2;5;11;56,8;45;28;0;5;33;1289,f;m
5052,ICLR,2020,Entropy Penalty: Towards Generalization Beyond the IID Assumption,Devansh Arpit;Caiming Xiong;Richard Socher,devansharpit@gmail.com;cxiong@salesforce.com;rsocher@salesforce.com,1;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,SalesForce.com;SalesForce.com;SalesForce.com,-1;-1;-1,-1;-1;-1,8,9/25/19,0,0,0,0,0,0,921;6301;53531,42;156;180,12;31;49,118;1045;8917,m;m
5053,ICLR,2020,A Causal View on Robustness  of Neural Networks,Cheng Zhang;Yingzhen Li,cheng.zhang@microsoft.com;yingzhen.li@microsoft.com,3;8;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,2,0,yes,9/25/19,Microsoft;Microsoft,-1;-1,-1;-1,,9/25/19,1,1,0,0,0,0,658;1035,98;43,13;14,27;145,m;f
5054,ICLR,2020,Improved Image Augmentation for Convolutional Neural Networks by Copyout and CopyPairing,Philip May,eniak.info@gmail.com,1;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,0,0,yes,9/25/19,T-Systems on site services GmbH,-1,-1,,9/1/19,0,0,0,0,0,0,178,7,4,9,m
5055,ICLR,2020,Model-free Learning Control of Nonlinear Stochastic Systems with Stability Guarantee,Minghao Han;Yuan Tian;Lixian Zhang;Jun Wang;Wei Pan,mhhan@hit.edu.cn;yuantian013@163.com;lixianzhang@hit.edu.cn;jun.wang@cs.ucl.ac.uk;wei.pan@tudelft.nl,1;6;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0,yes,9/25/19,Harbin Institute of Technology;Swiss Federal Institute of Technology;Harbin Institute of Technology;University College London;Delft University of Technology,172;10;172;50;89,424;13;424;15;67,,9/25/19,0,0,0,0,0,0,19;461;9209;189;1770,10;71;251;122;121,2;8;51;8;21,2;10;314;5;157,m;m
5056,ICLR,2020,A Theoretical Analysis of  Deep Q-Learning,Zhuoran Yang;Yuchen Xie;Zhaoran Wang,zy6@princeton.edu;yuchenxie2020@u.northwestern.edu;zhaoranwang@gmail.com,8;3;3,I have read many papers in this area.:N/A:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,Princeton University;Northwestern University;Northwestern University,31;44;44,6;22;22,,1/1/19,54,29,18,3,137,10,534;261;1193,48;26;78,12;8;20,71;25;132,m;m
5057,ICLR,2020,On the Pareto Efficiency of Quantized CNN,Ting-Wu Chin;Pierce I-Jen Chuang;Vikas Chandra;Diana Marculescu,tingwuc@cmu.edu;pichuang@fb.com;vchandra@fb.com;dianam@cmu.edu,3;6;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Carnegie Mellon University;Facebook;Facebook;Carnegie Mellon University,1;-1;-1;1,27;-1;-1;27,2,9/25/19,0,0,0,0,0,0,133;208;1416;5136,14;14;24;290,7;6;9;37,7;51;134;369,f;m
5058,ICLR,2020,Robust Reinforcement Learning via Adversarial Training with  Langevin Dynamics,Huang Yu-Ting;Parameswaran Kamalaruban;Paul Rolland;Ya-Ping Hsieh;Volkan Cevher,yu.huang@epfl.ch;kamalaruban.parameswaran@epfl.ch;paul.rolland@epfl.ch;ya-ping.hsieh@epfl.ch;volkan.cevher@epfl.ch,3;3;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne;Swiss Federal Institute of Technology Lausanne,481;481;481;481;481,38;38;38;38;38,8,9/25/19,1,0,1,0,0,0,-1;85;115;208;14,-1;18;44;32;10,-1;4;5;9;1,0;10;9;9;3,f;m
5059,ICLR,2020,Robust Reinforcement Learning with Wasserstein Constraint,Linfang Hou;Liang Pang;Xin Hong;Yanyan Lan;Zhiming Ma;Dawei Yin,houlinfang09@gmail.com;pangliang@ict.ac.cn;hongxin19b@ict.ac.cn;lanyanyan@ict.ac.cn;mazm@amt.ac.cn;yindawei@acm.org,3;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,"JD AI Research;Institute of Computing Technology, Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences;Chinese Academy of Sciences;",-1;59;59;59;59;-1,-1;1397;1397;1397;1397;-1,,9/25/19,0,0,0,0,0,0,10;923;58;2633;871;1875,6;46;15;120;59;113,1;12;4;25;14;25,0;151;4;438;81;149,u;m
5060,ICLR,2020,Representation Learning for Remote Sensing: An Unsupervised Sensor Fusion Approach,Aidan M. Swope;Xander H. Rudelis;Kyle T. Story,aidanswope@gmail.com;xander@descarteslabs.com;kyle@descarteslabs.com,3;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,"California Institute of Technology;Descartes Labs, Inc;Descartes Labs, Inc",143;-1;-1,2;-1;-1,,9/25/19,0,0,0,0,0,0,0;6;3813,1;3;105,0;2;32,0;0;282,m;m
5061,ICLR,2020,Learning to Combat Compounding-Error in Model-Based Reinforcement Learning,Chenjun Xiao;Yifan Wu;Chen Ma;Dale Schuurmans;Martin Müller,chenjun@ualberta.ca;yw4@andrew.cmu.edu;chenchloem@gmail.com;daes@ualberta.ca;mmueller@ualberta.ca,1;6;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,University of Alberta;Carnegie Mellon University;;University of Alberta;University of Alberta,100;1;-1;100;100,136;27;-1;136;136,,9/25/19,2,0,1,0,0,0,36;527;12;193;612,12;113;42;37;46,3;13;1;8;13,3;44;0;27;40,u;m
5062,ICLR,2020,Recurrent Neural Networks are Universal Filters,Wenjie Xu;Xiuqiong Chen;Stephen S.-T. Yau,1155118056@link.cuhk.edu.hk;cxq0828@tsinghua.edu.cn;yau@uic.edu,3;3;6,I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,4,0,yes,9/25/19,"The Chinese University of Hong Kong;Tsinghua University;University of Illinois, Chicago",59;8;56,35;23;254,11;1,9/25/19,0,0,0,0,0,0,15;6;32,18;10;5,2;1;1,0;0;2,m;m
5063,ICLR,2020,Soft Token Matching for Interpretable Low-Resource Classification,Federico Errica;Fabrizio Silvestri;Bora Edizel;Sebastian Riedel;Ludovic Denoyer;Vassilis Plachouras,federico.errica@phd.unipi.it;fabrizio.silvestri@gmail.com;b.edizel@gmail.com;sebastian.riedel@gmail.com;denoyer@fb.com;vplachouras@fb.com,3;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,6,0,yes,9/25/19,University of Pisa;;Facebook;University College London;Facebook;Facebook,233;-1;-1;50;-1;-1,366;-1;-1;15;-1;-1,,9/25/19,0,0,0,0,0,0,-1;557;33;409;3075;2292,-1;32;6;60;129;102,-1;9;3;10;22;25,0;54;4;23;536;176,m;m
5064,ICLR,2020,A Mention-Pair Model of Annotation with Nonparametric User Communities,Silviu Paun;Juntao Yu;Jon Chamberlain;Udo Kruschwitz;Massimo Poesio,s.paun@qmul.ac.uk;juntao.yu@qmul.ac.uk;jchamb@essex.ac.uk;udo@essex.ac.uk;m.poesio@qmul.ac.uk,3;8;6;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Queen Mary University London;Queen Mary University London;University of Sussex;University of Sussex;Queen Mary University London,233;233;233;233;233,110;110;146;146;110,3,9/25/19,0,0,0,0,0,0,65;49;1;1283;1,13;15;10;173;6,4;4;1;19;1,6;5;0;85;0,m;m
5065,ICLR,2020,Towards Finding Longer Proofs,Zsolt Zombori;Adrián Csiszárik;Henryk Michalewski;Cezary Kaliszyk;Josef Urban,zombori@renyi.hu;csadrian@renyi.hu;henrykmichalewski@gmail.com;cezary.kaliszyk@uibk.ac.at;josef.urban@gmail.com,6;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,4,0,yes,9/25/19,Alfréd Rényi Institute of Mathematics;Alfréd Rényi Institute of Mathematics;;University of Innsbruck;Czech Technical University in Prague,-1;-1;-1;481;323,-1;-1;-1;415;956,1,5/30/19,1,0,0,0,0,0,37;14;368;1725;2791,14;4;53;123;156,3;2;9;23;29,1;0;26;96;167,m;m
5066,ICLR,2020,iWGAN: an Autoencoder WGAN for Inference,Yao Chen;Qingyi Gao;Xiao Wang,chen2037@purdue.edu;gao424@purdue.edu;wangxiao@purdue.edu,8;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Purdue University;Purdue University;Purdue University,27;27;27,88;88;88,5;4;1;8,9/25/19,0,0,0,0,0,0,415;11;20,46;9;42,4;1;3,52;1;1,m;m
5067,ICLR,2020,Teacher-Student Compression with Generative Adversarial Networks,Ruishan Liu;Nicolo Fusi;Lester Mackey,ruishan@stanford.edu;lmackey@stanford.edu;fusi@microsoft.com,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,Stanford University;Stanford University;Microsoft,4;4;-1,4;4;-1,5;4,9/27/18,11,7,3,0,8,2,90;929;516,17;31;61,5;10;10,3;146;48,u;m
5068,ICLR,2020,Characterize and Transfer Attention in Graph Neural Networks,Mufei Li;Hao Zhang;Xingjian Shi;Minjie Wang;Yixing Guan;Zheng Zhang,limufe@amazon.com;sufeidechabei@gmail.com;xshiab@connect.ust.hk;wmjlyjemaine@gmail.com;guayixin@amazon.com;zz@nyu.edu,3;6;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,7,0,yes,9/25/19,Amazon;;The Hong Kong University of Science and Technology;;Amazon;New York University,-1;-1;39;-1;-1;25,-1;-1;47;-1;-1;29,6;10,9/25/19,0,0,0,0,0,0,321;368;23;1;8;-1,13;88;4;4;6;-1,6;7;2;1;2;-1,31;39;4;0;0;0,m;m
5069,ICLR,2020,Benchmarking Model-Based Reinforcement Learning,Tingwu Wang;Xuchan Bao;Ignasi Clavera;Jerrick Hoang;Yeming Wen;Eric Langlois;Shunshi Zhang;Guodong Zhang;Pieter Abbeel;Jimmy Ba,tingwuwang@cs.toronto.edu;xuchan.bao@mail.utoronto.ca;iclavera@berkeley.edu;jhoang@cs.toronto.edu;ywen@cs.toronto.edu;edl@cs.toronto.edu;matthew.zhang@mail.utoronto.ca;gdzhang@cs.toronto.edu;pabbeel@cs.berkeley.edu;jba@cs.toronto.edu,1;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:N/A:N/A,Reject,0,5,0,yes,9/25/19,"Department of Computer Science, University of Toronto;Toronto University;University of California Berkeley;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Toronto University;Department of Computer Science, University of Toronto;University of California Berkeley;Department of Computer Science, University of Toronto",18;18;5;18;18;18;18;18;5;18,18;18;13;18;18;18;18;18;13;18,,7/3/19,58,18,12,2,0,4,190;86;437;48;144;279;48;1335;37294;52924,8;3;15;2;10;40;2;16;438;56,5;3;8;1;4;10;1;11;94;22,17;2;57;2;13;24;3;225;4481;8625,m;m
5070,ICLR,2020,Gradient Perturbation is Underrated for Differentially Private Convex Optimization,Da Yu;Huishuai Zhang;Wei Chen;Tie-yan Liu;Jian Yin,yuda3@mail2.sysu.edu.cn;huishuai.zhang@microsoft.com;wche@microsoft.com;tie-yan.liu@microsoft.com;issjyin@mail.sysu.edu.cn,6;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,SUN YAT-SEN UNIVERSITY;Microsoft;Microsoft;Microsoft;SUN YAT-SEN UNIVERSITY,481;-1;-1;-1;481,299;-1;-1;-1;299,9,9/25/19,0,0,0,0,0,0,26;421;228;13552;5235,5;42;175;369;371,2;10;9;51;33,1;49;9;1723;280,m;m
5071,ICLR,2020,BasisVAE: Orthogonal Latent Space for Deep Disentangled Representation,Jin-Young  Kim;Sung-Bae Cho,seago0828@yonsei.ac.kr;sbcho@yonsei.ac.kr,1;1;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,1,20,0,yes,9/25/19,Yonsei University;Yonsei University,481;481,196;196,5,9/25/19,0,0,0,0,0,0,84;308,28;43,3;8,7;17,m;m
5072,ICLR,2020,How many weights are enough : can tensor factorization learn efficient policies ?,Pierre H. Richemond;Arinbjorn Kolbeinsson;Yike Guo,phr17@ic.ac.uk;ak711@imperial.ac.uk;y.guo@imperial.ac.uk,3;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Imperial College London;Imperial College London;Imperial College London,73;73;73,10;10;10,,9/25/19,0,0,0,0,0,0,13;-1;5381,11;-1;339,2;-1;33,2;0;321,m;m
5073,ICLR,2020,Superseding Model Scaling by Penalizing Dead Units and Points with Separation Constraints,Carles Riera;Camilo Rey-Torres;Eloi Puertas;Oriol Pujol,blauigris@gmail.com;camilorey@gmail.com;epuertas@ub.edu;oriol_pujol@ub.edu,3;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,13,1,yes,9/25/19,Universitat de Barcelona;Universidad Sergio Arboleda;Universitat de Barcelona;Universitat de Barcelona,481;481;481;481,213;1397;213;213,,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1;-1,-1;-1;-1;-1,-1;-1;-1;-1,0;0;0;0,m;m
5074,ICLR,2020,AHash: A Load-Balanced One Permutation Hash,Chenxingyu Zhao;Jie Gui;Yixiao Guo;Jie Jiang;Tong Yang;Bin Cui;Gong Zhang,dkzcxy@pku.edu.cn;guisj2017@pku.edu.cn;1700016637@pku.edu.cn;jie.jiang@pku.edu.cn;yangtongemail@gmail.com;bin.cui@pku.edu.cn;nicholas.zhang@huawei.com,1;6;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,0,0,yes,9/25/19,Peking University;Peking University;Peking University;Peking University;;Peking University;Huawei Technologies Ltd.,22;22;22;22;-1;22;-1,24;24;24;24;-1;24;-1,,9/25/19,0,0,0,0,0,0,108;418;50;0;175;154;51,7;24;12;7;59;41;50,3;6;4;0;8;3;4,8;39;0;0;5;7;0,m;m
5075,ICLR,2020,Representation Quality Explain Adversarial Attacks,Danilo Vasconcellos Vargas;Shashank Kotyan;Moe Matsuki,vargas@inf.kyushu-u.ac.jp;shashankkotyan@gmail.com;matsuki.sousisu@gmail.com,1;3;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Meiji University;Meiji University;,481;481;-1,332;332;-1,4;6,6/15/19,1,0,0,0,0,0,769;9;4,48;10;7,9;2;1,46;0;0,m;m
5076,ICLR,2020,Explaining A Black-box By Using A Deep Variational Information Bottleneck Approach,Seojin Bang;Pengtao Xie;Heewook Lee;Wei Wu;Eric Xing,seojinb@cs.cmu.edu,6;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,12,0,yes,9/25/19,Carnegie Mellon University,1,27,,2/19/19,9,6,4,0,2,4,43;1361;-1;14471;25065,12;72;-1;940;605,3;17;-1;55;77,6;157;0;1362;2695,f;m
5077,ICLR,2020,Neural networks are a priori biased towards Boolean functions with low entropy,Chris Mingard;Joar Skalse;Guillermo Valle-Pérez;David Martínez-Rubio;Vladimir Mikulik;Ard A. Louis,christopher.mingard@hertford.ox.ac.uk;joar.skalse@hertford.ox.ac.uk;guillermo.valle@dtc.ox.ac.uk;david.martinez@cs.ox.ac.uk;vladimir.mikulik@hertford.ox.ac.uk;ard.louis@physics.ox.ac.uk,3;6;6,I do not know much about this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,4,0,yes,9/25/19,University of Oxford;University of Oxford;University of Oxford;University of Oxford;University of Oxford;University of Oxford,50;50;50;50;50;50,1;1;1;1;1;1,1,9/25/19,2,0,0,0,0,0,2;3;45;78;3;2878,2;3;7;6;2;166,1;1;2;3;1;28,0;1;1;13;1;82,m;m
5078,ICLR,2020,Learning Structured Communication for Multi-agent Reinforcement Learning,Junjie Sheng;Xiangfeng Wang;Bo Jin;Junchi Yan;Wenhao Li;Tsung-Hui Chang;Jun Wang;Hongyuan Zha,52194501003@stu.ecnu.edu.cn;xfwang@sei.ecnu.edu.cn;bjin@cs.ecnu.edu.cn;yanjunchi@sjtu.edu.cn;52194501026@stu.ecnu.edu.cn;changtsunghui@cuhk.edu.cn;jwang@sei.ecnu.edu.cn;zha@sei.ecnu.edu.cn,6;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Australian National University;Australian National University;Australian National University;Shanghai Jiao Tong University;Australian National University;Tsinghua University;Australian National University;Australian National University,108;108;108;53;108;8;108;108,50;50;50;157;50;23;50;50,10,9/25/19,0,0,0,0,0,0,201;1102;115;2284;11;3290;106;14480,24;69;51;148;14;140;60;408,8;18;6;27;2;31;4;62,2;104;10;150;1;301;13;1225,m;m
5079,ICLR,2020,Lipschitz Lifelong Reinforcement Learning,Erwan Lecarpentier;David Abel;Kavosh Asadi;Yuu Jinnai;Emmanuel Rachelson;Michael L. Littman,erwan.lecarpentier@isae-supaero.fr;david_abel@brown.edu;k8@brown.edu;yuu_jinnai@brown.edu;emmanuel.rachelson@isae-supaero.fr;michael_littman@brown.edu,6;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Institut Supérieur de l'Aéronautique et de l'Espace;Brown University;Brown University;Brown University;Institut Supérieur de l'Aéronautique et de l'Espace;Brown University,481;67;67;67;481;67,1397;53;53;53;1397;53,,9/25/19,0,0,0,0,0,0,12;281;393;92;147;27570,5;34;19;23;39;369,2;9;8;6;7;68,3;25;48;7;6;2826,m;m
5080,ICLR,2020,Graph Neural Networks for Reasoning 2-Quantified Boolean Formulas,Fei Wang;Zhanfu Yang;Ziliang Chen;Guannan Wei;Tiark Rompf,wang603@purdue.edu;yang1676@purdue.edu;c.ziliang@yahoo.com;wei220@purdue.edu;tiark@purdue.edu,6;8;3,I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,Purdue University;Purdue University;SUN YAT-SEN UNIVERSITY;Purdue University;Purdue University,27;27;481;27;27,88;88;299;88;88,10,9/25/19,0,0,0,0,0,0,180;10;198;71;2512,122;4;28;27;111,7;2;6;5;28,8;0;23;6;234,m;m
5081,ICLR,2020,Semi-Supervised Few-Shot Learning with a Controlled Degree of Task-Adaptive Conditioning,Sung Whan Yoon;Jun Seo;Jaekyun Moon,shyoon8@kaist.ac.kr;tjwns0630@kaist.ac.kr;jmoon@kaist.edu,3;6;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;KAIST,481;481;20,110;110;110,6,9/25/19,0,0,0,0,0,0,80;43;1627,16;36;197,5;4;19,9;5;136,m;m
5082,ICLR,2020,Deep Lifetime Clustering,S Chandra Mouli;Leonardo Teixeira;Jennifer Neville;Bruno Ribeiro,chandr@purdue.edu;lteixeir@purdue.edu;ribeiro@cs.purdue.edu;neville@cs.purdue.edu,6;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0,yes,9/25/19,Purdue University;Purdue University;Purdue University;Purdue University,27;27;27;27,88;88;88;88,1,9/25/19,1,0,1,0,0,0,15;1412;75;20,4;116;32;4,2;18;5;2,1;45;4;3,m;m
5083,ICLR,2020,PassNet: Learning pass probability surfaces from single-location labels. An architecture for visually-interpretable soccer analytics,Javier Fernández;Luke Bornn,javier.fernandezr@fcbarcelona.cat;lbornn@kings.com,3;8;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Universitat Politècnica de Catalunya;Kings,266;-1,612;-1,,9/25/19,0,0,0,0,0,0,1;858,6;79,1;16,0;75,m;m
5084,ICLR,2020,Few-Shot One-Class Classification via Meta-Learning,Ahmed Frikha;Denis Krompaß;Hans-Georg Koepken;Volker Tresp,ahmed.frikha@siemens.com;denis.krompass@siemens.com;hans-georg.koepken@siemens.com;volker.tresp@siemens.com,1;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,13,0,yes,9/25/19,Siemens Corporate Research;Siemens Corporate Research;Siemens Corporate Research;Siemens Corporate Research,-1;-1;-1;-1,-1;-1;-1;-1,6,9/25/19,0,0,0,0,0,0,135;1390;24;8302,25;23;25;287,6;12;1;45,5;128;1;807,m;m
5085,ICLR,2020,Simple is Better: Training an End-to-end Contract Bridge Bidding Agent without Human Knowledge,Qucheng Gong;Yu Jiang;Yuandong Tian,qucheng@fb.com;tinayujiang@fb.com;yuandong@fb.com,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,3,0,yes,9/25/19,Facebook;Facebook;Facebook,-1;-1;-1,-1;-1;-1,,9/25/19,0,0,0,0,0,0,126;30;2498,8;28;85,4;2;25,5;0;293,m;m
5086,ICLR,2020,Rethinking deep active learning: Using unlabeled data at model training,Oriane Siméoni;Mateusz Budnik;Yannis Avrithis;Guillaume Gravier,oriane.simeoni@inria.fr;mateusz.budnik@inria.fr;yannis@avrithis.net;guig@irisa.fr,3;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,7,0,yes,9/25/19,"INRIA;INRIA;INRIA;IRISA, Université Bretagne Sud",-1;-1;-1;481,-1;-1;-1;1397,,9/25/19,4,1,0,0,0,0,22;27;3566;3771,5;7;198;233,3;4;32;28,3;0;243;359,f;m
5087,ICLR,2020,Why Convolutional Networks Learn Oriented Bandpass Filters: A Hypothesis,Richard P. Wildes,wildes@cse.yorku.ca,3;3;1;3,I have published in this field for several years.:N/A:N/A:N/A;I do not know much about this area.:N/A:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published in this field for several years.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,York University,172,416,,9/25/19,0,0,0,0,0,0,3399,87,30,308,m
5088,ICLR,2020,Deep Expectation-Maximization in Hidden Markov Models via Simultaneous Perturbation Stochastic Approximation,Chong Li;Dan Shen;C.J. Richard Shi;Hongxia Yang,chongli@uw.edu;dshen@alibaba-inc.com;cjshi@uw.edu;yang.yhx@alibaba-inc.com,3;3,I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,2,0,yes,9/25/19,"University of Washington, Seattle;Alibaba Group;University of Washington, Seattle;Alibaba Group",6;-1;6;-1,26;-1;26;-1,1,9/25/19,0,0,0,0,0,0,363;61;1158;614,41;19;131;89,7;5;19;12,24;1;56;74,m;f
5089,ICLR,2020,Learning to Contextually Aggregate Multi-Source Supervision for Sequence Labeling,Ouyu Lan*;Xiao Huang*;Bill Yuchen Lin;He Jiang;Xiang Ren,olan@usc.edu;huan183@usc.edu;yuchen.lin@usc.edu;jian567@usc.edu;xiangren@usc.edu,6;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,University of Southern California;University of Southern California;University of Southern California;University of Southern California;University of Southern California,31;31;31;31;31,62;62;62;62;62,3;6,9/25/19,3,1,1,0,0,1,31;7;143;324;1380,5;9;19;60;121,3;2;6;9;20,1;0;10;24;57,f;m
5090,ICLR,2020,Disentangled Cumulants Help Successor Representations Transfer to New Tasks,Chris Grimm;Irina Higgins;Andre Barreto;Denis Teplyashin;Markus Wulfmeier;Tim Hertweck;Raia Hadsell;Satinder Singh,crgrimm@umich.edu;irinah@google.com;andrebarreto@google.com;teplyashin@google.com;mwulfmeier@google.com;thertweck@google.com;raia@google.com;baveja@google.com,6;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,University of Michigan;Google;Google;Google;Google;Google;Google;Google,8;-1;-1;-1;-1;-1;-1;-1,21;-1;-1;-1;-1;-1;-1;-1,,9/25/19,1,1,0,0,0,0,45;2018;437;313;615;17;8331;88,22;25;70;9;27;5;63;28,4;11;11;7;12;2;26;6,5;302;24;33;51;1;804;8,m;m
5091,ICLR,2020,Adversarial training with perturbation generator networks,Hyeungill Lee;Sungyeob Han;Jungwoo Lee,hyungil0113@snu.ac.kr;yubise7en@snu.ac.kr;junglee@snu.ac.kr,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,9,0,yes,9/25/19,Seoul National University;Seoul National University;Seoul National University,41;41;41,64;64;64,4,9/25/19,0,0,0,0,0,0,82;68;732,6;3;180,3;1;12,4;4;49,u;m
5092,ICLR,2020,Batch Normalization has Multiple Benefits: An Empirical Study on Residual Networks,Soham De;Samuel L Smith,sohamde@google.com;slsmith@google.com,3;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,4,0,yes,9/25/19,Google;Google,-1;-1,-1;-1,,9/25/19,2,0,0,0,0,0,442;1268,24;26,12;11,39;111,m;m
5093,ICLR,2020,A TWO-STAGE FRAMEWORK FOR MATHEMATICAL EXPRESSION RECOGNITION,Jin Zhang;Weipeng Ming;Pengfei Liu,zhangjin9@100tal.com;mingweipeng@100tal.com;liupengfei1@100tal.com,1;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,100tal;Chinese Academy of Sciences;100tal,-1;59;-1,-1;1397;-1,2;8,9/25/19,0,0,0,0,0,0,39;0;7,48;1;12,4;0;2,3;0;1,m;m
5094,ICLR,2020,The Sooner The Better: Investigating Structure of Early Winning Lottery Tickets,Shihui Yin;Kyu-Hyoun Kim;Jinwook Oh;Naigang Wang;Mauricio Serrano;Jae-Sun Seo;Jungwook Choi,shihui.yin@ibm.com;kimk@us.ibm.com;ohj@us.ibm.com;nwang@us.ibm.com;mserrano@us.ibm.com;jaesun.seo@asu.edu;choij@hanyang.ac.kr,6;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,6,0,yes,9/25/19,International Business Machines;International Business Machines;International Business Machines;International Business Machines;International Business Machines;Arizona State University;Hanyang University,-1;-1;-1;-1;-1;95;233,-1;-1;-1;-1;-1;155;393,,9/25/19,1,1,0,0,0,1,78;1092;504;954;1471;1858;46,21;168;47;49;57;118;23,3;17;12;16;19;20;3,5;63;26;59;145;173;2,m;m
5095,ICLR,2020,How Well Do WGANs Estimate the Wasserstein Metric?,Anton Mallasto;Guido Montúfar;Augusto Gerolin,anton.mallasto@gmail.com;montufar@math.ucla.edu;augustogerolin@gmail.com,1;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,3,0,yes,9/25/19,"University of Copenhagen;University of California, Los Angeles;",100;20;-1,101;17;-1,5;4,9/25/19,1,0,0,0,0,0,31;1193;69,9;60;15,2;14;5,2;77;0,m;m
5096,ICLR,2020,A Group-Theoretic Framework for Knowledge Graph Embedding,Tong Yang;Long Sha;Pengyu Hong,yangto@bc.edu;longsha@brandeis.edu;hongpeng@brandeis.edu,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,4,4,0,yes,9/25/19,Boston College;Brandeis University;Brandeis University,266;323;323,323;244;244,10,9/25/19,0,0,0,0,0,0,175;318;1805,59;32;92,8;10;22,5;18;97,m;m
5097,ICLR,2020,Accelerated Information Gradient flow,Yifei Wang;Wuchen Li,zackwang24@pku.edu.cn;wcli@math.ucla.edu,3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,"Peking University;University of California, Los Angeles",22;20,24;17,11;1,9/4/19,4,3,3,0,0,0,-1;443,-1;57,-1;12,0;16,m;m
5098,ICLR,2020,Time2Vec: Learning a Vector Representation of Time,Seyed Mehran Kazemi;Rishab Goel;Sepehr Eghbali;Janahan Ramanan;Jaspreet Sahota;Sanjay Thakur;Stella Wu;Cathal Smyth;Pascal Poupart;Marcus Brubaker,mehran.kazemi@borealisai.com;rishab.goel@borealisai.com;sepehr.eghbali@rbc.com;janahan.ramanan@borealisai.com;jaspreet.sahota@borealisai.com;sttsanjay@gmail.com;stella.wu@borealisai.com;cathal.smyth@rbc.com;pascal.poupart@borealisai.com;marcus.brubaker@borealisai.com,6;1;3;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Borealis AI;Borealis AI;Borealis AI;Borealis AI;Borealis AI;;Borealis AI;Borealis AI;Borealis AI;Borealis AI,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1;-1;-1;-1,,7/11/19,6,2,1,0,0,0,302;36;56;9;46;123;54;96;4273;89,28;8;16;3;9;27;14;11;172;10,8;3;4;2;4;6;5;6;35;4,50;6;4;0;3;5;2;2;460;8,m;m
5099,ICLR,2020,MGP-AttTCN: An Interpretable Machine Learning Model for the Prediction of Sepsis,Margherita Rosnati;Vincent Fortuin,mrosnati@ethz.ch;fortuin@inf.ethz.ch,3;1;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10,13;13,,9/25/19,0,0,0,0,0,0,0;79,1;18,0;6,0;5,f;m
5100,ICLR,2020,NESTED LEARNING FOR MULTI-GRANULAR TASKS,Raphaël Achddou;J. Matias Di Martino;Guillermo Sapiro,raphael.achddou@telecom-paristech.fr;matiasdm@fing.edu.uy;guillermo.sapiro@duke.edu,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,3,0,yes,9/25/19,Télécom ParisTech;Facultad de Ingeniería;Duke University,481;-1;47,187;-1;20,,9/25/19,0,0,0,0,0,0,0;30;44214,1;16;650,0;4;86,0;0;3902,m;m
5101,ICLR,2020,Attack-Resistant Federated Learning with Residual-based Reweighting,Shuhao Fu;Chulin Xie;Bo Li;Qifeng Chen,sfuab@connect.ust.hk;chulinxie@zju.edu.cn;lbo@illinois.edu;chenqifeng22@gmail.com,3;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,1,0,yes,9/25/19,"The Hong Kong University of Science and Technology;Zhejiang University;University of Illinois, Urbana Champaign;Hong Kong University of Science and Technology",39;56;3;39,47;107;48;47,4,9/25/19,0,0,0,0,0,0,23;11;235;1785,9;4;147;30,3;2;9;15,1;1;25;302,m;m
5102,ICLR,2020,Self-Supervised Policy Adaptation,Christopher Mutschler;Sebastian Pokutta,christopher.mutschler@iis.fraunhofer.de;pokutta@zib.de,1;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,1,0,yes,9/25/19,Fraunhofer IIS;,-1;-1,-1;-1,,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1,-1;-1,-1;-1,0;0,m;m
5103,ICLR,2020,Efficient generation of structured objects with Constrained Adversarial Networks,Jacopo Gobbi;Luca Di Liello;Pierfrancesco Ardino;Paolo Morettin;Stefano Teso;Andrea Passerini,jacopo.gobbi@studenti.unitn.it;luca.diliello@studenti.unitn.it;pierfrancesco.ardino@unitn.it;paolo.morettin@unitn.it;stefano.teso@gmail.com;andrea.passerini@unitn.it,3;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,University of Trento;University of Trento;University of Trento;University of Trento;;University of Trento,18;18;18;18;-1;18,307;307;307;307;-1;307,5;4;10,9/25/19,0,0,0,0,0,0,0;0;0;47;15;524,2;1;1;15;13;62,0;0;0;4;2;11,0;0;0;4;0;31,m;f
5104,ICLR,2020,Subgraph Attention for Node Classification and Hierarchical Graph Pooling,Sambaran Bandyopadhyay;Manasvi Aggarwal;M. N. Murty,sambaran.ban89@gmail.com;manasvia@iisc.ac.in;mnm@iisc.ac.in,3;6;1,I do not know much about this area.:N/A:N/A:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,5,0,yes,9/25/19,International Business Machines;Indian Institute of Science;Indian Institute of Science,-1;95;95,-1;301;301,10,9/25/19,0,0,0,0,0,0,82;0;1,27;2;10,6;0;1,2;0;1,m;m
5105,ICLR,2020,Learning from Partially-Observed Multimodal Data with Variational Autoencoders,Yu Gong;Hossein Hajimirsadeghi;Jiawei He;Megha Nawhal;Thibaut Durand;Greg Mori,yu_gong@sfu.ca;hossein.hajimirsadeghi@gmail.com;jha203@sfu.ca;mnawhal@sfu.ca;thibaut.p.durand@borealisai.com;mori@cs.sfu.ca,3;3;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,7,0,yes,9/25/19,Simon Fraser University;Borealis AI;Simon Fraser University;Simon Fraser University;Borealis AI;Simon Fraser University,64;-1;64;64;-1;64,272;-1;272;272;-1;272,5,9/25/19,0,0,0,0,0,0,81;317;207;43;354;9711,30;29;42;15;21;197,5;9;8;3;6;45,3;27;22;6;36;817,m;m
5106,ICLR,2020,A Simple Approach to the Noisy Label Problem Through the Gambler's Loss,Liu Ziyin;Ru Wang;Paul Pu Liang;Ruslan Salakhutdinov;Louis-Philippe Morency;Masahito Ueda,zliu@cat.phys.s.u-tokyo.ac.jp;wangru1994305@gmail.com;pliang@cs.cmu.edu;rsalakhu@cs.cmu.edu;morency@cs.cmu.edu;ueda@phys.s.u-tokyo.ac.jp,6;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,7,0,yes,9/25/19,The University of Tokyo;;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;The University of Tokyo,56;-1;1;1;1;56,36;-1;27;27;27;36,8,9/25/19,0,0,0,0,0,0,1;3;658;69005;10810;6,5;11;44;254;325;10,1;1;13;82;54;2,0;0;104;7875;1129;0,u;m
5107,ICLR,2020,Contextual Temperature for Language Modeling,Pei-Hsin Wang;Sheng-Iou Hsieh;Shieh-Chieh Chang;Jia-Yu Pan;Yu-Ting Chen;Wei Wei;Da-Cheng Juan,peihsin@gapp.nthu.edu.tw;steins1111@gapp.nthu.edu.tw;scchang@cs.nthu.edu.tw;jypan@google.com;yutingchen@google.com;wewei@google.com;dacheng@google.com,3;6;3,I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,5,0,yes,9/25/19,National Tsing Hua University;National Tsing Hua University;National Tsing Hua University;Google;Google;Google;Google,172;172;172;-1;-1;-1;-1,365;365;365;-1;-1;-1;-1,3,9/25/19,0,0,0,0,0,0,5;0;0;68;104;109;33,4;1;1;16;35;46;10,1;0;0;3;5;5;3,2;0;0;1;16;11;7,u;m
5108,ICLR,2020,MULTI-LABEL METRIC LEARNING WITH BIDIRECTIONAL REPRESENTATION DEEP NEURAL NETWORKS,Tao Zheng;Ivor Tsang;Xin Yao,tao.zheng@student.uts.edu.au;ivor.tsang@uts.edu.au;xiny@sustech.edu.cn,1;3;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,University of Technology Sydney;University of Technology Sydney;Tsinghua University,108;108;8,193;193;23,,9/25/19,0,0,0,0,0,0,5;10966;544,5;253;88,1;51;6,1;1283;33,m;u
5109,ICLR,2020,Unsupervised Data Augmentation for Consistency Training,Qizhe Xie;Zihang Dai;Eduard Hovy;Minh-Thang Luong;Quoc V. Le,qizhex@cs.cmu.edu;dzihang@cs.cmu.edu;hovy@cs.cmu.edu;thangluong@google.com;qvl@google.com,8;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,11,1,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Google;Google,1;1;1;-1;-1,27;27;27;-1;-1,6,4/29/19,108,33,36,2,0,16,737;2445;24246;3111;48901,18;27;579;33;193,8;14;76;20;81,163;442;2524;359;6080,m;m
5110,ICLR,2020,Representation Learning Through Latent Canonicalizations,Or Litany;Ari Morcos;Srinath Sridhar;Leonidas Guibas;Judy Hoffman,orlitany@gmail.com;arimorcos@gmail.com;ssrinath@cs.stanford.edu;guibas@cs.stanford.edu;judy@gatech.edu,3;8;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Stanford University;Facebook;Stanford University;Stanford University;Georgia Institute of Technology,4;-1;4;4;13,4;-1;4;4;38,8,9/25/19,0,0,0,0,0,0,596;1034;533;46189;9487,34;32;28;700;60,12;12;7;98;32,58;116;64;5581;1123,m;f
5111,ICLR,2020,Molecular Graph Enhanced Transformer for Retrosynthesis Prediction,Kelong Mao;Peilin Zhao;Tingyang Xu;Yu Rong;Xi Xiao;Junzhou Huang,mkl18@mails.tsinghua.edu.cn;masonzhao@tencent.com;tingyangxu@tencent.com;yu.rong@hotmail.com;xiaox@sz.tsinghua.edu.cn;joehhuang@tencent.com,3;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Tsinghua University;Tencent AI Lab;Tencent AI Lab;Tencent AI Lab;Tsinghua University;Tencent AI Lab,8;-1;-1;-1;8;-1,23;-1;-1;-1;23;-1,3;10,9/25/19,1,0,0,0,0,0,1;168;147;105;2795;7,1;23;29;37;367;7,1;6;6;5;30;1,0;21;18;10;113;0,m;m
5112,ICLR,2020,Frequency Pooling: Shift-Equivalent and Anti-Aliasing Down Sampling,Zhendong Zhang;Cheolkon Jung,zhd.zhang.ai@gmail.com;zhengzk@xidian.edu.cn,1;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0,yes,9/25/19,Tsinghua University;Tsinghua University,8;8,23;23,,9/25/19,0,0,0,0,0,0,224;965,54;183,9;15,0;87,m;m
5113,ICLR,2020,Where is the Information in a Deep Network?,Alessandro Achille;Stefano Soatto,achille@cs.ucla.edu;soatto@cs.ucla.edu,6;8;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,8,1,yes,9/25/19,"University of California, Los Angeles;University of California, Los Angeles",20;20,17;17,1;8,5/29/19,14,9,1,1,0,3,655;15761,31;457,12;62,78;1445,m;m
5114,ICLR,2020,FleXOR: Trainable Fractional Quantization,Dongsoo Lee;Se Jung Kwon;Byeongwook Kim;Yongkweon Jeon;Baeseong Park;Jeongin Yun;Gu-Yeon Wei,dslee3@gmail.com;mogndrewk@gmail.com;quddnr145@gmail.com;dragwon.jeon@gmail.com;qkrqotjd91@gmail.com;yji6373@naver.com;gywei@g.harvard.edu,3;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,;Samsung;Samsung;Samsung;;Samsung;Harvard University,-1;-1;-1;-1;-1;-1;39,-1;-1;-1;-1;-1;-1;7,,9/25/19,0,0,0,0,0,0,60;23;23;32;0;0;5285,14;22;10;9;5;6;180,2;3;3;3;0;0;37,6;0;1;2;0;0;456,m;m
5115,ICLR,2020,Mixture Distributions for Scalable Bayesian Inference,Pranav Poduval;Hrushikesh Loya;Rajat Patel;Sumit Jain,pranav97.poduval@gmail.com;loyahrushikesh@gmail.com;prajat5232@iitb.ac.in;sumitjain3033@gmail.com,1;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,13,0,yes,9/25/19,Indian Institute of Technology Bombay;Indian Institute of Technology Bombay;Indian Institute of Technology Bombay;,118;118;118;-1,480;480;480;-1,4;11,9/25/19,1,0,0,0,0,0,1;2;4;39,5;5;5;18,1;1;1;4,0;0;0;1,m;m
5116,ICLR,2020,On the Invertibility of Invertible Neural Networks,Jens Behrmann;Paul Vicol;Kuan-Chieh Wang;Roger B. Grosse;Jörn-Henrik Jacobsen,jensb@uni-bremen.de;pvicol@cs.toronto.edu;wangkua1@cs.toronto.edu;rgrosse@cs.toronto.edu;j.jacobsen@vectorinstitute.ai,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,"Universität Bremen;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto;Vector Institute",154;18;18;18;-1,360;18;18;18;-1,5,9/25/19,3,2,0,0,0,0,276;146;180;189;420,13;19;11;50;19,5;6;3;8;9,44;17;24;12;38,m;m
5117,ICLR,2020,Meta Learning via Learned Loss,Sarah Bechtle;Artem Molchanov;Yevgen Chebotar;Edward Grefenstette;Ludovic Righetti;Gaurav Sukhatme;Franziska Meier,sbechtle@tuebingen.mpg.de;molchano@usc.edu;ychebota@usc.edu;egrefen@gmail.com;ludovic.righetti@nyu.edu;gaurav@usc.edu;fmeier@fb.com,3;3;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,"Max Planck Institute for Intelligent Systems, Max-Planck Institute;University of Southern California;University of Southern California;Facebook;New York University;University of Southern California;Facebook",-1;31;31;-1;25;31;-1,-1;62;62;-1;29;62;-1,6,6/12/19,11,4,2,2,8,2,19;183;878;7382;45;265;507,5;18;26;58;17;57;44,3;9;12;26;4;9;13,2;11;64;888;5;9;39,f;f
5118,ICLR,2020,Demonstration Actor Critic,Guoqing Liu;Li Zhao;Pushi Zhang;Jiang Bian;Tao Qin;Nenghai Yu;Tie-Yan Liu,lgq1001@mail.ustc.edu.cn;lizo@microsoft.com;zpschang@gmail.com;jiang.bian@microsoft.com;taoqin@microsoft.com;ynh@ustc.edu.cn;tyliu@microsoft.com,6;6;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,9,0,yes,9/25/19,University of Science and Technology of China;Microsoft;Tsinghua University;Microsoft;Microsoft;University of Science and Technology of China;Microsoft,481;-1;8;-1;-1;481;-1,80;-1;23;-1;-1;80;-1,1,9/25/19,0,0,0,0,0,0,15;235;1;1031;2245;589;13552,16;200;2;41;44;55;369,2;6;1;14;8;7;51,3;20;0;101;317;74;1723,m;m
5119,ICLR,2020,Moniqua: Modulo Quantized Communication in Decentralized SGD,Yucheng Lu;Christopher De Sa,yl2967@cornell.edu;cdesa@cs.cornell.edu,3;3;3;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,10,0,yes,9/25/19,Cornell University;Cornell University,7;7,19;19,1;10,9/25/19,3,2,0,0,0,0,4;1523,4;69,1;18,0;158,m;m
5120,ICLR,2020,Testing For Typicality with Respect to an Ensemble of Learned Distributions,Forrest Laine;Claire Tomlin,forrest.laine@berkeley.edu;tomlin@eecs.berkeley.edu,3;1;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,University of California Berkeley;University of California Berkeley,5;5,13;13,,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1,-1;-1,-1;-1,0;0,m;f
5121,ICLR,2020,Min-Max Optimization without Gradients: Convergence and Applications to Adversarial ML,Sijia Liu;Songtao Lu;Xiangyi Chen;Yao Feng;Kaidi Xu;Abdullah Al-Dujaili;Minyi Hong;Una-May Obelilly,sijia.liu@ibm.com;songtao@ibm.com;chen5719@umn.edu;feng-y16@mails.tsinghua.edu.cn;xu.kaid@husky.neu.edu;aldujail@mit.edu;mhong@umn.edu;unamay@csail.mit.edu,3;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0,yes,9/25/19,"International Business Machines;International Business Machines;University of Minnesota, Minneapolis;Tsinghua University;Northeastern University;Massachusetts Institute of Technology;University of Minnesota, Minneapolis;Massachusetts Institute of Technology",-1;-1;59;8;16;2;59;2,-1;-1;79;23;906;5;79;5,4;9,9/25/19,8,2,3,0,0,1,298;378;21;42;255;155;5275;7,52;84;7;11;27;22;207;1,11;10;3;4;10;7;33;1,24;14;4;2;18;20;540;1,m;f
5122,ICLR,2020,Wyner VAE: A Variational Autoencoder with Succinct Common Representation Learning,J. Jon Ryu;Yoojin Choi;Young-Han Kim;Mostafa El-Khamy;Jungwon Lee,jongha.ryu@gmail.com;yoojin.c@samsung.com;yhk@ucsd.edu;mostafa.e@samsung.com;jungwon2.lee@samsung.com,6;6;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,15,0,yes,9/25/19,"University of California, San Diego;Samsung;University of California, San Diego;Samsung;Samsung",11;-1;11;-1;-1,31;-1;31;-1;-1,5,9/25/19,0,0,0,0,0,0,16;462;122;1105;2326,11;35;39;102;166,2;9;4;17;25,0;48;13;110;238,m;m
5123,ICLR,2020,Improving Confident-Classifiers For Out-of-distribution Detection,Sachin Vernekar;Ashish Gaurav;Vahdat Abdelzad;Taylor Denouden;Rick Salay;Krzysztof Czarnecki,sverneka@uwaterloo.ca;a5gaurav@uwaterloo.ca;vabdelza@gsd.uwaterloo.ca;tadenoud@uwaterloo.ca;rsalay@gsd.uwaterloo.ca;kczarnec@gsd.uwaterloo.ca,3;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,University of Waterloo;University of Waterloo;University of Waterloo;University of Waterloo;University of Waterloo;University of Waterloo,28;28;28;28;28;28,235;235;235;235;235;235,,9/25/19,0,0,0,0,0,0,42;17;88;62;1028;14034,17;9;24;8;86;328,4;3;6;4;18;54,4;0;5;3;66;1356,m;m
5124,ICLR,2020,FINBERT:  FINANCIAL SENTIMENT ANALYSIS   WITH PRE-TRAINED LANGUAGE MODELS,Dogu Araci;Zulkuf Genc,dogu.araci@naspers.com;zulkuf.genc@naspers.com,3;3;1;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0,yes,9/25/19,Naspers;Prosus,-1;-1,-1;-1,3;6,8/27/19,2,2,2,0,0,0,5;80,1;4,1;3,0;2,m;m
5125,ICLR,2020,Rethinking Neural Network Quantization,Qing Jin;Linjie Yang;Zhenyu Liao,jinqingking@gmail.com;yljatthu@gmail.com;liaozhenyu2004@gmail.com,3;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0,yes,9/25/19,Bytedance;Bytedance;Kuaishou,-1;-1;-1,-1;-1;-1,,9/25/19,1,0,0,0,0,0,25;40;1,13;10;5,3;3;1,1;12;0,u;m
5126,ICLR,2020,A Simple and Scalable Shape Representation for 3D Reconstruction,Mateusz Michalkiewicz;Eugene Belilovsky;Mahsa Baktashmotagh;Anders Eriksson,78lhar@gmail.com;belilovsky.eugene@gmail.com;m.baktashmotlagh@uq.edu.au;a.eriksson@uq.edu.au,6;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,University of Queensland;University of Montreal;University of Queensland;University of Queensland,205;128;205;205,66;85;66;66,,9/25/19,1,1,0,0,0,0,34;305;583;89,6;30;32;19,2;10;11;4,0;32;60;6,m;m
5127,ICLR,2020,Learning Cross-Context Entity Representations from Text,Jeffrey Ling;Nicholas FitzGerald;Zifei Shan;Livio Baldini Soares;Thibault Févry;David Weiss;Tom Kwiatkowski,jeffreyling@google.com;nfitz@google.com;zifeis@google.com;liviobs@google.com;tfevry@google.com;djweiss@google.com;tomkwiat@google.com,3;3;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:N/A:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,Google;Google;Google;Google;Google;Google;Google,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,3;6,9/25/19,6,4,2,0,0,0,184;722;84;923;155;143;2367,19;36;10;24;7;18;40,6;11;6;11;4;5;18,31;64;2;79;13;12;214,m;m
5128,ICLR,2020,Unsupervised Few Shot Learning via Self-supervised Training,Zilong Ji;Xiaolong Zou;Tiejun Huang;Si Wu,jizilong@mail.bnu.edu.cn;xiaolz@pku.edu.cn;tjhuang@pku.edu.cn;siwu@pku.edu.cn,1;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:N/A:N/A:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Australian National University;Peking University;Peking University;Peking University,108;22;22;22,50;24;24;24,6,9/25/19,3,1,1,0,0,1,19;66;558;172,8;19;74;31,3;3;13;3,2;3;20;23,m;f
5129,ICLR,2020,Confidence-Calibrated Adversarial Training: Towards Robust Models Generalizing Beyond the Attack Used During Training,David Stutz;Matthias Hein;Bernt Schiele,david.stutz@mpi-inf.mpg.de;schiele@mpi-inf.mpg.de;matthias.hein@uni-tuebingen.de,3;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,9,0,yes,9/25/19,"Saarland Informatics Campus, Max-Planck Institute;Saarland Informatics Campus, Max-Planck Institute;University of Tuebingen",-1;-1;154,-1;-1;91,4,9/25/19,0,0,0,0,0,0,712;524;41307,17;53;503,8;8;99,74;43;5119,m;m
5130,ICLR,2020,Optimal Unsupervised Domain Translation,Emmanuel de Bézenac;Ibrahim Ayed;Patrick Gallinari,emmanuel.de-bezenac@lip6.fr;ayedibrahim@gmail.com;patrick.gallinari@lip6.fr,3;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,LIP6;LIP6;LIP6,-1;-1;-1,-1;-1;-1,,6/4/19,3,2,0,0,0,0,90;33;4901,9;7;450,3;3;33,10;0;380,m;m
5131,ICLR,2020,A Uniform Generalization Error Bound for Generative Adversarial Networks,Hao Chen;Zhanfeng Mo;Qingyi Gao;Zhouwang Yang;Xiao Wang,ch330822@mail.ustc.edu.cn;oscarmzf@mail.ustc.edu.cn;gao424@purdue.edu;yangzw@ustc.edu.cn;wangxiao@purdue.edu,1;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,University of Science and Technology of China;University of Science and Technology of China;Purdue University;University of Science and Technology of China;Purdue University,481;481;27;481;27,80;80;88;80;88,5;4;1;8,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,0;0;0;0;0,m;m
5132,ICLR,2020,Efficient Wrapper Feature Selection using Autoencoder and Model Based Elimination,Sharan Ramjee;Aly El Gamal,sramjee@purdue.edu;elgamala@purdue.edu,1;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,1,5,0,yes,9/25/19,Purdue University;Purdue University,27;27,88;88,,5/28/19,1,1,0,0,5,0,35;427,7;61,2;12,4;30,m;m
5133,ICLR,2020,Training Data Distribution Search with Ensemble Active Learning,Kashyap Chitta;Jose M. Alvarez;Elmar Haussmann;Clement Farabet,kashyap.chitta@tue.mpg.de;josea@nvidia.com;ehaussmann@nvidia.com;cfarabet@nvidia.com,6;6;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,3,0,yes,9/25/19,"Max Planck Institute for Intelligent Systems, Max-Planck Institute;NVIDIA;NVIDIA;NVIDIA",-1;-1;-1;-1,-1;-1;-1;-1,,9/25/19,2,0,0,0,0,0,36;976;471;5914,15;51;21;28,4;15;11;17,2;99;59;420,m;m
5134,ICLR,2020,Coresets for Accelerating Incremental Gradient Methods,Baharan Mirzasoleiman;Jeff Bilmes;Jure Leskovec,baharanm@cs.stanford.edu;bilmes@uw.edu;jure@cs.stanford.edu,8;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,"Stanford University;University of Washington, Seattle;Stanford University",4;6;4,4;26;4,1,9/25/19,1,1,1,0,0,0,945;13972;48502,28;350;302,12;55;93,145;1283;6080,f;m
5135,ICLR,2020,Gaussian MRF Covariance Modeling for Efficient Black-Box Adversarial Attacks,Anit Kumar Sahu;J. Zico Kolter;Satya Narayan Shukla,anit.sahu@gmail.com;zkolter@cs.cmu.edu;snshukla@cs.umass.edu,8;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,";Carnegie Mellon University;University of Massachusetts, Amherst",-1;1;28,-1;27;209,4;11,9/25/19,0,0,0,0,0,0,488;7776;55,39;107;22,10;35;4,56;1073;4,m;m
5136,ICLR,2020,How can we generalise learning distributed representations of graphs?,Paul M Scherer;Pietro Lio,pms69@cam.ac.uk;pl219@cam.ac.uk,3;6;1,I have published one or two papers in this area.:I carefully checked the experiments.:N/A:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,8,0,yes,9/25/19,University of Cambridge;University of Cambridge,71;71,3;3,10,9/25/19,0,0,0,0,0,0,9;767,6;96,1;8,0;81,m;m
5137,ICLR,2020,Graph Residual Flow for Molecular Graph Generation,Shion Honda;Hirotaka Akita;Katsuhiko Ishiguro;Toshiki Nakanishi;Kenta Oono,26x.orc.ed5.1hs@gmail.com;akita714@preferred.jp;k.ishiguro.jp@ieee.org;nakanishi@preferred.jp;oono@preferred.jp,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,"The University of Tokyo;Preferred Networks, Inc.;Preferred Networks, Inc.;Preferred Networks, Inc.;Preferred Networks, Inc.",56;-1;-1;-1;-1,36;-1;-1;-1;-1,5;10,9/25/19,2,2,1,0,0,1,5;605;628;91;664,4;76;74;17;11,2;14;14;6;5,3;31;42;3;69,m;m
5138,ICLR,2020,Regularizing Deep Multi-Task Networks using Orthogonal Gradients,Mihai Suteu;Yi-ke Guo,m.suteu16@imperial.ac.uk;y.guo@imperial.ac.uk,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,3,0,yes,9/25/19,Imperial College London;Imperial College London,73;73,10;10,,9/25/19,3,1,1,0,0,0,21;157,16;48,3;8,3;9,m;m
5139,ICLR,2020,Blending Diverse Physical Priors with Neural Networks,Yunhao Ba;Guangyuan Zhao;Achuta Kadambi,yhba@ucla.edu;zhaoguangyuan@ucla.edu;achuta@ee.ucla.edu,6;3;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,6,0,yes,9/25/19,"University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles",20;20;20,17;17;17,8,9/25/19,3,3,0,0,0,0,7;29;837,4;17;43,2;3;12,0;1;30,m;m
5140,ICLR,2020,The Detection of Distributional Discrepancy for Text Generation,Xingyuan Chen;Ping Cai;Peng Jin;Haokun Du;Hongjun Wang;Xinyu Dai;Jiajun Chen,1045258214@qq.com;1061185275@qq.com;jandp@pku.edu.cn;626913553@qq.com;wanghongjun@swjtu.edu.cn;daixinyu@nju.edu.cn;chenjj@nju.edu.cn,1;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0,yes,9/25/19,;Tsinghua University;Peking University;;Tsinghua University;Zhejiang University;Zhejiang University,-1;8;22;-1;8;56;56,-1;23;24;-1;23;107;107,3;4;5,9/25/19,0,0,0,0,0,0,128;301;38;107;1073;117;975,61;73;44;10;86;13;129,6;10;4;5;16;3;16,18;20;0;5;42;7;84,m;f
5141,ICLR,2020,DG-GAN: the GAN with the duality gap,Cheng Peng;Hao Wang;Xiao Wang;Zhouwang Yang,pch0051@mail.ustc.edu.cn;wh001@mail.ustc.edu.cn;wangxiao@purdue.edu;yangzw@ustc.edu.cn,1;1;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,University of Science and Technology of China;University of Science and Technology of China;Purdue University;University of Science and Technology of China,481;481;27;481,80;80;88;80,5;4;1;8,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1;-1,-1;-1;-1;-1,-1;-1;-1;-1,0;0;0;0,m;m
5142,ICLR,2020,Samples Are Useful? Not Always: denoising policy gradient updates using variance explained,Yannis Flet-Berliac;Philippe Preux,yannis.flet-berliac@inria.fr;philippe.preux@inria.fr,3;6;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,INRIA;INRIA,-1;-1,-1;-1,,9/25/19,1,0,0,0,0,0,5;103,3;17,1;5,0;7,m;m
5143,ICLR,2020,Continual Learning via Principal Components Projection,Gyuhak Kim;Bing Liu,gkim87@uic.edu;liub@uic.edu,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,"University of Illinois, Chicago;University of Illinois, Chicago",56;56,254;254,,9/25/19,0,0,0,0,0,0,0;453,1;34,0;12,0;36,u;m
5144,ICLR,2020,Iterative Deep Graph Learning for Graph Neural Networks,Yu Chen;Lingfei Wu;Mohammed J. Zaki,cheny39@rpi.edu;lwu@email.wm.edu;zaki@cs.rpi.edu,3;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Rensselaer Polytechnic Institute;College of William and Mary;Rensselaer Polytechnic Institute,172;154;172,438;235;438,10,9/25/19,0,0,0,0,0,0,163;93;17324,67;33;353,9;5;58,11;8;1882,f;m
5145,ICLR,2020,Training Deep Neural Networks by optimizing over nonlocal paths in hyperparameter space,Vlad Pushkarov;Yonathan Efroni;Mykola Maksymenko;Maciej Koch-Janusz,vladpush@icloud.com;jonathan.efroni@gmail.com;mmaks@softserveinc.com;maciejk@ethz.ch,6;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Icloud;Technion;SoftServe Inc.;Swiss Federal Institute of Technology,-1;26;-1;10,-1;412;-1;13,,9/9/19,1,0,0,0,0,0,1;16;29;151,1;2;14;15,1;1;3;5,0;0;1;8,m;m
5146,ICLR,2020,An Optimization Principle Of Deep Learning?,Cheng Chen;Junjie Yang;Yi Zhou,u0952128@utah.edu;yang.4972@buckeyemail.osu.edu;yi.zhou@utah.edu,3;1;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,University of Utah;Ohio State University;University of Utah,233;77;233,366;373;366,9,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1,-1;-1;-1,-1;-1;-1,0;0;0,f;m
5147,ICLR,2020,Amata: An Annealing Mechanism for Adversarial Training Acceleration,Nanyang Ye;Qianxiao Li;Zhanxing Zhu,yn272@cam.ac.uk;qianxiao@nus.edu.sg;zhanxing.zhu@pku.edu.cn,6;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,University of Cambridge;National University of Singapore;Peking University,71;16;22,3;25;24,4;1,9/25/19,0,0,0,0,0,0,51;410;891,17;29;80,4;9;14,2;40;112,m;m
5148,ICLR,2020,Sparsity Meets Robustness: Channel Pruning for the Feynman-Kac Formalism Principled Robust Deep Neural Nets,Thu Dinh*;Bao Wang*;Andrea L. Bertozzi;Stanley J. Osher;Jack Xin,thud2@uci.edu;wangbaonj@gmail.com;bertozzi@math.ucla.edu;sjo@math.ucla.edu;jxin@math.uci.edu,3;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,"University of California, Irvine;University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles;University of California, Irvine",35;20;20;20;35,96;17;17;17;96,4,9/25/19,1,1,0,0,0,0,1141;1010;10302;60202;2568,37;403;405;561;180,18;16;52;96;24,99;38;750;5077;189,m;m
5149,ICLR,2020,Perceptual Generative Autoencoders,Zijun Zhang;Ruixiang Zhang;Zongpeng Li;Yoshua Bengio;Liam Paull,zijun.zhang@ucalgary.ca;sodabeta7@gmail.com;zongpeng@whu.edu.cn;yoshua.bengio@mila.quebec;paulll@iro.umontreal.ca,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,University of Calgary;MILA;Wuhan University;University of Montreal;University of Montreal,172;-1;266;128;128,210;-1;354;85;85,5,3/27/19,3,2,1,0,3,0,361;570;58;208566;865,31;58;19;807;59,9;10;3;147;19,9;62;7;24297;42,m;m
5150,ICLR,2020,Deep symbolic regression,Brenden K. Petersen,petersen33@llnl.gov,3;6;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Lawrence Livermore National Labs,-1,-1,,9/25/19,0,0,0,0,0,0,72,19,6,2,m
5151,ICLR,2020,Task-Relevant Adversarial Imitation Learning,Konrad Zolna;Scott Reed;Alexander Novikov;Ziyu Wang;Sergio Gómez;David Budden;Serkan Cabi;Misha Denil;Nando de Freitas,konrad.zolna@gmail.com;reedscot@google.com;anovikov@google.com;ziyu@google.com;sergomez@google.com;budden@google.com;cabi@google.com;mdenil@google.com;nandodefreitas@google.com,8;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,1,5,0,yes,9/25/19,Jagiellonian University;Google;Google;Google;Google;Google;Google;Google;Google,481;-1;-1;-1;-1;-1;-1;-1;-1,610;-1;-1;-1;-1;-1;-1;-1;-1,5;4,9/25/19,4,1,1,1,0,0,108;11248;62;4319;1271;1305;181;3379;19467,22;28;7;51;15;55;12;38;184,7;16;3;21;10;15;7;20;55,6;2075;1;492;141;137;10;285;1854,m;m
5152,ICLR,2020,"Imitation Learning of Robot Policies using Language, Vision and Motion",Simon Stepputtis;Joseph Campbell;Mariano Phielipp;Chitta Baral;Heni Ben Amor,sstepput@asu.edu;jacampb1@asu.edu;mariano.j.phielipp@intel.com;chitta@asu.edu;hbenamor@asu.edu,6;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Reject,0,6,0,yes,9/25/19,Arizona State University;Arizona State University;Intel;Arizona State University;Arizona State University,95;95;-1;95;95,155;155;-1;155;155,3;8,9/25/19,0,0,0,0,0,0,38;376;37;4822;1363,12;45;17;314;113,3;10;3;34;20,2;23;2;425;66,m;m
5153,ICLR,2020,Why Learning of Large-Scale Neural Networks Behaves Like Convex Optimization,Hui Jiang,hj@cse.yorku.ca,1;1;1;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,York University,172,416,9,3/6/19,1,1,0,0,8,0,867,112,14,41,m
5154,ICLR,2020,Information Theoretic Model Predictive Q-Learning,Mohak Bhardwaj;Ankur Handa;Dieter Fox;Byron Boots,mohakb@cs.washington.edu;ahanda@nvidia.com;fox@cs.washington.edu;bboots@cs.washington.edu,3;6;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,9,0,yes,9/25/19,University of Washington;NVIDIA;University of Washington;University of Washington,6;-1;6;6,26;-1;26;26,,9/25/19,0,0,0,0,0,0,94;2220;40659;2468,6;34;375;135,3;16;98;27,4;266;3215;252,m;m
5155,ICLR,2020,Learning to Optimize via Dual space Preconditioning,Sélim Chraibi;Adil Salim;Samuel Horváth;Filip Hanzely;Peter Richtárik,selimsepthuit@gmail.com;adil.salim@kaust.edu.sa;samuel.horvath@kaust.edu.sa;filip.hanzely@kaust.edu.sa;richtarik@gmail.com,3;3;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0,yes,9/25/19,CNRS;KAUST;KAUST;KAUST;KAUST,-1;128;128;128;128,-1;1397;1397;1397;1397,,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,0;0;0;0;0,m;m
5156,ICLR,2020,HaarPooling: Graph Pooling with Compressive Haar Basis,Yu Guang Wang;Ming Li;Zheng Ma;Guido Montufar;Xiaosheng Zhuang;Yanan Fan,yuguang.wang@unsw.edu.au;ming.li.ltu@gmail.com;mzheng@princeton.edu;montufar@math.ucla.edu;xzhuang7@cityu.edu.hk;y.fan@unsw.edu.au,3;3;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,9,0,yes,9/25/19,"University of New South Wales;Australian National University;Princeton University;University of California, Los Angeles;City University of Hong Kong;University of New South Wales",481;108;31;20;92;481,1397;50;6;17;35;1397,10,9/25/19,0,0,0,0,0,0,-1;980;280;1193;94;506,-1;166;77;60;22;60,-1;6;9;14;6;12,0;156;11;77;6;74,m;f
5157,ICLR,2020,Neural-Guided Symbolic Regression with Asymptotic Constraints,Li Li;Minjie Fan;Rishabh Singh;Patrick Riley,leeley@google.com;mjfan@google.com;rising@google.com;pfr@google.com,3;3;8,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:N/A:N/A,Reject,0,12,0,yes,9/25/19,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,,9/25/19,1,1,1,0,0,0,258;48;913;40,108;19;53;26,7;5;12;4,4;4;96;3,m;m
5158,ICLR,2020,Handwritten Amharic Character Recognition System Using Convolutional Neural Networks,Fetulhak Abdurahman,afetulhak@yahoo.com,1;1;1,I have published one or two papers in this area.:I did not assess the experiments.:N/A:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,1,1,0,yes,9/25/19,Zhejiang University,56,107,,4/20/19,0,0,0,0,0,0,2,7,1,0,m
5159,ICLR,2020,Deep Relational Factorization Machines,Hongchang Gao;Gang Wu;Ryan Rossi;Viswanathan Swaminathan;Heng Huang,hongchanggao@gmail.com;gawu@adobe.com;ryrossi@adobe.com;vishy@adobe.com;henghuanghh@gmail.com,1;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,1,0,yes,9/25/19,University of Pittsburgh;Adobe Systems;Adobe Systems;Adobe Systems;University of Pittsburgh,79;-1;-1;-1;79,113;-1;-1;-1;113,10,9/25/19,0,0,0,0,0,0,334;1739;31;68;319,18;98;12;14;36,7;17;2;3;10,44;100;0;4;47,m;m
5160,ICLR,2020,Dirichlet Wrapper to Quantify Classification Uncertainty in Black-Box Systems,José Mena Roldán;Oriol Pujol Vila;Jordi Vitrià Marca,jmenarol7@alumnes.ub.edu;oriol_pujol@ub.edu;jordi.vitria@ub.edu,6;1;1;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,Universitat de Barcelona;Universitat de Barcelona;Universitat de Barcelona,481;481;481,213;213;213,3;2,9/25/19,0,0,0,0,0,0,0;10;0,2;10;4,0;2;0,0;4;0,m;m
5161,ICLR,2020,DO-AutoEncoder: Learning and Intervening Bivariate Causal Mechanisms in Images,Tianshuo Cong;Dan Peng;Furui Liu;Zhitang Chen,cts17@mails.tsinghua.edu.cn;lepangdan@outlook.com;liufurui2@huawei.com;chenzhitang2@huawei.com,1;1;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,3,0,yes,9/25/19,Tsinghua University;Harbin Institute of Technology;Huawei Technologies Ltd.;Huawei Technologies Ltd.,8;172;-1;-1,23;424;-1;-1,4;10,9/25/19,0,0,0,0,0,0,0;66;570;212,3;12;13;39,0;3;4;8,0;5;52;16,u;m
5162,ICLR,2020,Forecasting Deep Learning Dynamics with Applications to Hyperparameter Tuning,Piotr Kozakowski;Łukasz Kaiser;Afroz Mohiuddin,p.kozakowski@mimuw.edu.pl;lukaszkaiser@google.com;afrozm@google.com,3;6;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,"University of Washington, Seattle;Google;Google",6;-1;-1,26;-1;-1,,9/25/19,0,0,0,0,0,0,109;84;0,4;11;1,1;2;0,7;11;0,m;m
5163,ICLR,2020,Disentangling Trainability and Generalization in Deep Learning,Lechao Xiao;Jeffrey Pennington;Sam Schoenholz,xlc@google.com;jpennin@google.com;schsam@google.com,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,Google;Google;Google,-1;-1;-1,-1;-1;-1,8,9/25/19,4,1,0,0,0,0,453;16550;3133,19;51;70,8;20;21,63;2640;388,m;m
5164,ICLR,2020,Versatile Anomaly Detection with Outlier Preserving Distribution Mapping Autoencoders,Walter Gerych;Elke Rundensteiner;Emmanuel Agu,wgerych@wpi.edu;rundenst@wpi.edu;emmanuel@wpi.edu,6;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Worcester Polytechnic Institute;Worcester Polytechnic Institute;Worcester Polytechnic Institute,172;172;172,628;628;628,,9/25/19,0,0,0,0,0,0,0;7902;1072,7;596;107,0;44;14,0;515;63,m;m
5165,ICLR,2020,DeepPCM: Predicting Protein-Ligand Binding using Unsupervised Learned Representations,Paul Kim;Robin Winter;Djork-Arné Clevert,paul.kim@bayer.com;robin.winter@bayer.com;djork-arne.clevert@bayer.com,3;1;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,2,0,0,yes,9/25/19,Bayer Ag;Bayer Ag;Bayer Ag,-1;-1;-1,-1;-1;-1,,9/25/19,0,0,0,0,0,0,37;3560;10,9;112;5,1;25;1,1;193;0,m;m
5166,ICLR,2020,Learning transitional skills with intrinsic motivation,Qiangxing Tian;Jinxin Liu;Donglin Wang,11821087@zju.edu.cn;liujinxin@westlake.edu.cn;wangdonglin@westlake.edu.cn,3;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Zhejiang University;Westlake University;Westlake University,56;-1;-1,107;-1;-1,,9/25/19,0,0,0,0,0,0,0;9;16,2;7;14,0;2;2,0;0;0,m;m
5167,ICLR,2020,Preventing Imitation Learning with Adversarial Policy Ensembles,Albert Zhan;Pieter Abbeel;Stas Tiomkin,albertzhan@berkeley.edu;pabbeel@cs.berkeley.edu;stasti@gmail.com,3;1;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,13;13;13,4,9/25/19,0,0,0,0,0,0,0;37294;78,1;438;14,0;94;4,0;4481;3,m;m
5168,ICLR,2020,Noisy Machines: Understanding noisy neural networks and enhancing robustness to analog hardware errors using distillation,Chuteng Zhou;Prad Kadambi;Matthew Mattina;Paul N. Whatmough,chu.zhou@arm.com;pkadambi@asu.edu;matthew.mattina@arm.com;paul.whatmough@arm.com,3;6;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,5,0,yes,9/25/19,arm;Arizona State University;arm;arm,-1;95;-1;-1,-1;155;-1;-1,,9/25/19,1,0,1,0,0,0,81;2;1604;73,19;3;35;17,5;1;9;4,3;0;165;5,m;m
5169,ICLR,2020,Mutual Information Maximization for Robust Plannable Representations,Yiming Ding;Ignasi Clavera;Pieter Abbeel,dingyiming0427@berkeley.edu;iclavera@berkeley.edu;pabbeel@cs.berkeley.edu,3;1;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,13;13;13,,9/25/19,0,0,0,0,0,0,118;437;37294,10;15;438,2;8;94,12;57;4481,f;m
5170,ICLR,2020,Removing input features via a generative model to explain their attributions to classifier's decisions,Chirag Agarwal;Dan Schonfeld;Anh Nguyen,chiragagarwall12@gmail.com;dans@uic.edu;anh.ng8@gmail.com,1;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,8,0,yes,9/25/19,"University of Illinois, Chicago;University of Illinois, Chicago;Auburn University",56;56;390,254;254;651,5,9/25/19,5,3,2,0,0,0,65;2776;3787,20;295;24,5;25;12,3;166;224,m;m
5171,ICLR,2020,Gauge Equivariant Spherical CNNs,Berkay Kicanaoglu;Pim de Haan;Taco Cohen,b.kicanaoglu@uva.nl;pimdehaan@gmail.com;taco.cohen@gmail.com,8;8;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,University of Amsterdam;University of Amsterdam;University of Amsterdam,172;172;172,62;62;62,2;10,9/25/19,0,0,0,0,0,0,71;83;1706,8;22;32,3;4;17,8;11;251,m;m
5172,ICLR,2020,Relation-based Generalized Zero-shot Classification with the Domain Discriminator on the shared representation,Masahiro Suzuki;Yutaka Matsuo,masa@weblab.t.u-tokyo.ac.jp;matsuo@weblab.t.u-tokyo.ac.jp,6;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,6,0,yes,9/25/19,The University of Tokyo;The University of Tokyo,56;56,36;36,5;6,9/25/19,0,0,0,0,0,0,795;7676,316;381,12;34,78;512,m;m
5173,ICLR,2020,Machine Truth Serum,Tianyi Luo;Yang Liu,tluo6@ucsc.edu;yangliu@ucsc.edu,6;1;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,University of Southern California;University of Southern California,31;31,62;62,11,9/25/19,0,0,0,0,0,0,97;1552,19;211,5;22,16;99,m;m
5174,ICLR,2020,A Gradient-Based Approach to Neural Networks Structure Learning,Amir Ali Moinfar;Amirkeivan Mohtashami;Mahdieh Soleymani;Ali Sharifi-Zarchi,moinfar@ce.sharif.edu;mohtashami@ce.sharif.edu;soleymani@sharif.edu;sharifi@sharif.edu,6;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Sharif University of Technology;Sharif University of Technology;Sharif University of Technology;Sharif University of Technology,323;323;323;323,564;564;564;564,,9/25/19,0,0,0,0,0,0,0;0;5;10,1;1;5;11,0;0;1;1,0;0;0;1,m;m
5175,ICLR,2020,Learning Generative Models using Denoising Density Estimators,Siavash Bigdeli;Geng Lin;Tiziano Portenier;Andrea Dunbar;Matthias Zwicker,siavash.bigdeli@csem.ch;geng@cs.umd.edu;tiziano.portenier@vision.ee.ethz.ch;andrea.dunbar@csem.ch;zwicker@cs.umd.edu,1;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,7,0,yes,9/25/19,"Swiss Center for Electronics and Micro Electronics;University of Maryland, College Park;Swiss Federal Institute of Technology;Swiss Center for Electronics and Micro Electronics;University of Maryland, College Park",-1;12;10;-1;12,-1;91;13;-1;91,5;4;1,9/25/19,2,2,2,0,0,0,164;306;133;455;5593,17;73;14;55;162,4;9;5;13;40,16;16;10;18;407,m;m
5176,ICLR,2020,NPTC-net: Narrow-Band Parallel Transport Convolutional Neural Network on Point Clouds,Pengfei Jin;Tianhao Lai;Rongjie Lai;Bin Dong,jinpf@pku.edu.cn;howeverlth@pku.edu.cn;lair@rpi.edu;dongbin@math.pku.edu.cn,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,3,0,yes,9/25/19,Peking University;Peking University;Rensselaer Polytechnic Institute;Peking University,22;22;172;22,24;24;438;24,2;8,5/29/19,1,0,1,0,2,0,24;8;951;1836,10;7;61;138,3;1;18;20,0;0;55;125,u;m
5177,ICLR,2020,Attributed Graph Learning with 2-D Graph Convolution,Qimai Li;Xiaotong Zhang;Han Liu;Xiao-Ming Wu,csqmli@comp.polyu.edu.hk;zxt.dut@hotmail.com;liu.han.dut@gmail.com;xiao-ming.wu@polyu.edu.hk,3;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,4,0,yes,9/25/19,The Hong Kong Polytechnic University;The Hong Kong Polytechnic University;The Hong Kong Polytechnic University;The Hong Kong Polytechnic University,172;172;172;172,171;171;171;171,10,9/25/19,1,0,1,0,0,0,317;245;305;58,11;44;74;24,4;9;9;4,50;19;8;5,m;f
5178,ICLR,2020,Pareto Optimality in No-Harm Fairness,Natalia Martinez;Martin Bertran;Guillermo Sapiro,natalia.martinez@duke.edu;martin.bertran@duke.edu;guillermo.sapiro@duke.edu,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Duke University;Duke University;Duke University,47;47;47,20;20;20,1;7,9/25/19,0,0,0,0,0,0,0;13;44214,4;12;650,0;2;86,0;1;3902,f;m
5179,ICLR,2020,Conditional generation of molecules from disentangled representations,Amina Mollaysa;Brooks Paige;Alexandros  Kalousis,amina.mollaysa@gmail.com;tbpaige@gmail.com;alexandros.kalousis@hesge.ch,3;1;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,"University of Geneva, Switzerland;University College London;Geneva School of Business Administration, HES-SO University of Applied Sciences of Western Switzerland;",481;50;481;-1,144;15;1397;-1,,9/25/19,0,0,0,0,0,0,12;845;2131,4;39;98,2;13;24,2;88;215,f;m
5180,ICLR,2020,Overparameterized Neural Networks Can Implement Associative Memory,Adityanarayanan Radhakrishnan;Mikhail Belkin;Caroline Uhler,aradha@mit.edu;mbelkin@cse.ohio-state.edu;cuhler@mit.edu,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,9,0,yes,9/25/19,Massachusetts Institute of Technology;Ohio State University;Massachusetts Institute of Technology,2;77;2,5;373;5,,9/25/19,3,1,1,0,0,0,40;17243;168,14;120;17,4;40;4,2;2069;6,m;f
5181,ICLR,2020,TinyBERT: Distilling BERT for Natural Language Understanding,Xiaoqi Jiao;Yichun Yin;Lifeng Shang;Xin Jiang;Xiao Chen;Linlin Li;Fang Wang;Qun Liu,jiaoxiaoqi@hust.edu.cn;yinyichun@huawei.com;shang.lifeng@huawei.com;jiang.xin@huawei.com;chen.xiao2@huawei.com;lynn.lilinlin@huawei.com;wangfang@hust.edu.cn;qun.liu@huawei.com,8;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,1,9,0,yes,9/25/19,Hong Kong University of Science and Technology;Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.;Hong Kong University of Science and Technology;Huawei Technologies Ltd.,39;-1;-1;-1;-1;-1;39;-1,47;-1;-1;-1;-1;-1;47;-1,3,9/23/19,61,27,33,3,0,13,70;59;1733;663;290;826;115;360,3;3;41;23;59;158;22;89,3;1;15;9;7;14;5;9,14;12;203;88;17;33;13;43,u;m
5182,ICLR,2020,Cross Domain Imitation Learning,Kun Ho Kim;Yihong Gu;Jiaming Song;Shengjia Zhao;Stefano Ermon,khkim@cs.stanford.edu;gyh15@mails.tsinghua.edu.cn;jiaming.tsong@gmail.com;sjzhao@stanford.edu;ermon@cs.stanford.edu,3;6;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,5,0,yes,9/25/19,Stanford University;Tsinghua University;Stanford University;Stanford University;Stanford University,4;8;4;4;4,4;23;4;4;4,5;4;6,9/25/19,2,2,0,0,0,0,403;44;811;592;4975,79;4;44;29;203,7;3;14;13;31,17;2;127;98;664,m;m
5183,ICLR,2020,Multi-source Multi-view Transfer Learning in Neural Topic Modeling with Pretrained Topic and Word Embeddings,Pankaj Gupta;Yatin Chaudhary;Hinrich Schütze,pankaj_gupta96@yahoo.com;yatinchaudhary91@gmail.com;hinrich@hotmail.com,6;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,Siemens Corporate Research;Siemens Corporate Research;Institut für Informatik,-1;-1;-1,-1;-1;-1,3;6;8,9/25/19,0,0,0,0,0,0,71;16;1,26;8;4,3;2;1,9;0;0,m;m
5184,ICLR,2020,Natural Image Manipulation for Autoregressive Models Using Fisher Scores,Wilson Yan;Jonathan Ho;Pieter Abbeel,wilson1.yan@berkeley.edu;jonathanho@berkeley.edu;pabbeel@cs.berkeley.edu,3;8;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,4,0,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,13;13;13,5,9/25/19,0,0,0,0,0,0,65;2743;37294,8;31;438,3;12;94,7;408;4481,m;m
5185,ICLR,2020,SPECTRA: Sparse Entity-centric Transitions,Rim Assouel;Yoshua Bengio,rim.assouel@hotmail.fr;yoshua.bengio@mila.quebec,3;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,1,0,yes,9/25/19,University of Montreal;University of Montreal,128;128,85;85,,9/25/19,0,0,0,0,0,0,14;208566,3;807,1;147,1;24297,f;m
5186,ICLR,2020,Attentive Sequential Neural Processes,Jaesik Yoon;Gautam Singh;Sungjin Ahn,jaesik817@gmail.com;singh.gautam.iitg@gmail.com;sjn.ahn@gmail.com,6;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,6,0,yes,9/25/19,;Rutgers University;Rutgers University,-1;34;34,-1;168;168,,9/25/19,0,0,0,0,0,0,366;645;1389,44;50;41,8;12;12,21;40;161,m;m
5187,ICLR,2020,Generative Latent Flow,Zhisheng Xiao;Qing Yan;Yali Amit,zxiao@uchicago.edu;yanq@uchicago.edu;amit@marx.uchicago.edu,3;3;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,University of Chicago;University of Chicago;University of Chicago,48;48;48,9;9;9,5,5/24/19,6,0,1,1,0,0,17;14;3474,11;11;92,3;3;25,1;1;235,m;m
5188,ICLR,2020,"Farkas layers: don't shift the data, fix the geometry",Aram-Alexandre Pooladian;Chris Finlay;Adam M Oberman,aram-alexandre.pooladian@mail.mcgill.ca;christopher.finlay@gmail.com;adam.oberman@mcgill.ca,1;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,McGill University;McGill University;McGill University,86;86;86,42;42;42,,9/25/19,0,0,0,0,0,0,9;158;1689,6;48;88,1;8;22,0;10;157,m;m
5189,ICLR,2020,STABILITY AND CONVERGENCE THEORY FOR LEARNING RESNET: A FULL CHARACTERIZATION,Huishuai Zhang;Da Yu;Mingyang Yi;Wei Chen;Tie-yan Liu,huishuai.zhang@microsoft.com;yuda3@mail2.sysu.edu.cn;v-minyi@microsoft.com;wche@microsoft.com;tie-yan.liu@microsoft.com,3;1;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,1,4,0,yes,9/25/19,Microsoft;SUN YAT-SEN UNIVERSITY;Microsoft;Microsoft;Microsoft,-1;481;-1;-1;-1,-1;299;-1;-1;-1,9,9/25/19,0,0,0,0,0,0,421;384;4;464;13552,42;55;6;128;369,10;12;1;11;51,49;23;0;13;1723,m;m
5190,ICLR,2020,Universal approximations of permutation invariant/equivariant functions by deep neural networks,Akiyoshi Sannai;Yuuki Takai;Matthieu Cordonnier,akiyoshi.sannai@riken.jp;yuuki.takai@riken.jp;matthieu.cordonnier@ens-paris-saclay.fr,3;3;3,I do not know much about this area.:N/A:N/A:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:N/A:N/A:I did not assess the derivations or theory.;I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Reject,0,3,0,yes,9/25/19,RIKEN;RIKEN;Ecole Normale Superieure,-1;-1;100,-1;-1;45,,3/5/19,18,12,0,2,0,4,92;28;17,22;9;1,4;3;1,8;4;4,m;m
5191,ICLR,2020,Context-aware Attention Model for Coreference Resolution,Yufei Li;Xiangyu Zhou;Jie Ma;Yu Long;Xuan Wang;Chen Li,vermouthtarot@gmail.com;zxy951005@stu.xjtu.edu.cn;majack@stu.xjtu.edu.cn;longyu95@stu.xjtu.edu.cn;wangxuan8888@stu.xjtu.edu.cn;cli@xjtu.edu.cn,1;1;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,;Xi'an Jiaotong University;Xi'an Jiaotong University;Xi'an Jiaotong University;Xi'an Jiaotong University;Xi'an Jiaotong University,-1;481;481;481;481;481,-1;555;555;555;555;555,,9/25/19,0,0,0,0,0,0,11;5;10;945;4023;13,12;8;30;164;91;39,2;1;2;15;21;2,0;1;0;29;565;0,u;m
5192,ICLR,2020,A Random Matrix Perspective on Mixtures of Nonlinearities in High Dimensions,Ben Adlam;Jake Levinson;Jeffrey Pennington,adlam@google.com;jpennin@google.com;jlev@google.com,8;3;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Google;Google;Google,-1;-1;-1,-1;-1;-1,,9/25/19,0,0,0,0,0,0,210;53;16550,20;19;51,6;5;20,13;7;2640,m;m
5193,ICLR,2020,CaptainGAN: Navigate Through Embedding Space For Better Text Generation,Chun-Hsing Lin;Alvin Chiang;Chi-Liang Liu;Chien-Fu Lin;Po-Hsien Chu;Siang-Ruei Wu;Yi-En Tsai;Chung-Yang (Ric) Huang,jsaon92@gmail.com;alvin.chiang.180@gmail.com;liangtaiwan1230@gmail.com;gblin75468@gmail.com;cph@yoctol.com;raywu0@gmail.com;ypiheyn.imm02g@g2.nctu.edu.tw;cyhuang@ntu.edu.tw,6;6;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,12,0,yes,9/25/19,;;National Taiwan University;;Yoctol;;National Chiao Tung University;National Taiwan University,-1;-1;86;-1;-1;-1;143;86,-1;-1;120;-1;-1;-1;564;120,3;5,9/25/19,0,0,0,0,0,0,0;25;27;2;0;0;1978;269,1;8;7;4;1;1;45;18,0;3;3;1;0;0;19;10,0;0;3;0;0;0;164;12,m;m
5194,ICLR,2020,Policy Optimization with Stochastic Mirror Descent,Long Yang;Gang Zheng;Zavier Zhang;Yu Zhang;Qian Zheng;Jun Wen;Gang Pana sample efficient policy gradient method with stochastic mirror descent.,yanglong@zju.edu.cn;gang_zheng@zju.edu.cn;21721269@zju.edu.cn;hzzhangyu@zju.edu.cn;csqianzheng@gmail.com;junwen@zju.edu.cn;gpan@zju.edu.cn,3;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Zhejiang University;Zhejiang University;Zhejiang University;Zhejiang University;;Zhejiang University;Zhejiang University,56;56;56;56;-1;56;56,107;107;107;107;-1;107;107,9,6/25/19,3,1,0,0,3,1,10;2201;1519;-1;-1;-1;-1,26;198;113;-1;-1;-1;-1,2;23;18;-1;-1;-1;-1,3;135;142;0;0;0;0,m;u
5195,ICLR,2020,A Novel Analysis Framework of Lower Complexity Bounds for Finite-Sum Optimization,Guangzeng Xie;Luo Luo;Zhihua Zhang,smsxgz@pku.edu.cn;rickyluoluo@gmail.com;zhzhang@math.pku.edu.cn,3;8;6,I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,4,0,yes,9/25/19,Peking University;Shanghai Jiao Tong University;Peking University,22;53;22,24;157;24,1,8/22/19,0,0,0,0,0,0,6;219;163,6;42;45,2;8;6,1;20;22,m;m
5196,ICLR,2020,Reducing Computation in Recurrent Networks by Selectively Updating State Neurons,Thomas Hartvigsen;Cansu Sen;Xiangnan Kong;Elke Rundensteiner,twhartvigsen@wpi.edu;csen@wpi.edu;xkong@wpi.edu;rundenst@wpi.edu,6;6;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,Worcester Polytechnic Institute;Worcester Polytechnic Institute;Worcester Polytechnic Institute;Worcester Polytechnic Institute,172;172;172;172,628;628;628;628,,9/25/19,0,0,0,0,0,0,17;0;2375;7902,12;9;120;596,3;0;27;44,2;0;192;515,m;f
5197,ICLR,2020,Selective Brain Damage: Measuring the Disparate Impact of Model Pruning,Sara Hooker;Yann Dauphin;Aaron Courville;Andrea Frome,shooker@google.com;ynd@google.com;aaron.courville@gmail.com;onepinkfairyarmadillo@gmail.com,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,4,0,yes,9/25/19,Google;Google;University of Montreal;,-1;-1;128;-1,-1;-1;85;-1,,9/25/19,2,1,0,0,0,0,327;8765;62509;3565,6;43;203;13,4;27;65;8,40;1037;7971;409,f;f
5198,ICLR,2020,Deep exploration by novelty-pursuit with maximum state entropy,Zi-Niu Li;Xiong-Hui Chen;Yang Yu,liziniu1997@gmail.com;chenxh@lamda.nju.edu.cn;yuy@lamda.nju.edu.cn,1;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,4,0,yes,9/25/19,Xi'an Jiaotong University;Zhejiang University;Zhejiang University,481;56;56,555;107;107,,9/25/19,0,0,0,0,0,0,9;13;382,5;3;61,2;2;9,1;4;20,m;m
5199,ICLR,2020,Smart Ternary Quantization,Gregoire Morin;Ryan Razani;Vahid Partovi Nia;Eyyub Sari,gregoire.morin@huawei.com;ryan.razani@huawei.com;vahid.partovinia@huawei.com;eyyub.sari@huawei.com,3;6;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.,Reject,0,3,0,yes,9/25/19,Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.,-1;-1;-1;-1,-1;-1;-1;-1,,9/25/19,0,0,0,0,0,0,2;1;31;6,4;3;17;7,1;1;2;2,0;0;0;0,m;m
5200,ICLR,2020,Alleviating Privacy Attacks via Causal Learning,Shruti Tople;Amit Sharma;Aditya Nori,t-shtopl@microsoft.com;amshar@microsoft.com;adityan@microsoft.com,3;6;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Reject,0,14,0,yes,9/25/19,Microsoft;Microsoft;Microsoft,-1;-1;-1,-1;-1;-1,11;4;1,9/25/19,0,0,0,0,0,0,390;186;3056,33;68;97,9;8;28,44;6;214,f;m
5201,ICLR,2020,Generalized Domain Adaptation with Covariate and Label Shift CO-ALignment,Shuhan Tan;Xingchao Peng;Kate Saenko,tanshh@mail2.sysu.edu.cn;xpeng@bu.edu;saenko@bu.edu,6;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,1,5,0,yes,9/25/19,SUN YAT-SEN UNIVERSITY;Boston University;Boston University,481;67;67,299;61;61,,9/25/19,1,1,1,0,0,1,5;678;17431,4;19;178,1;11;56,3;81;2403,m;f
5202,ICLR,2020,Deeper Insights into Weight Sharing in Neural Architecture Search,Yuge Zhang;Quanlu Zhang;Junyang Jiang;Zejun Lin;Yujing Wang,scottyugochang@gmail.com;quanlu.zhang@microsoft.com;jyjiang97@gmail.com;gdzejlin@gmail.com;yujing.wang@microsoft.com,3;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Microsoft;Microsoft;Fudan University;Columbia University;Microsoft,-1;-1;79;15;-1,-1;-1;109;16;-1,,9/25/19,4,3,0,0,0,0,4;118;14;7;699,1;14;9;5;92,1;4;2;1;13,0;11;1;0;86,m;u
5203,ICLR,2020,Combining MixMatch and Active Learning for Better Accuracy with Fewer Labels,Shuang Song;David Berthelot;Afshin Rostamizadeh,shuangsong@google.com;dberth@google.com;rostami@google.com,3;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,3,0,yes,9/25/19,Google;Google;Google,-1;-1;-1,-1;-1;-1,,9/25/19,2,1,1,0,0,0,446;1454;4296,40;30;61,8;11;23,36;239;608,f;m
5204,ICLR,2020,Perceptual Regularization: Visualizing and Learning Generalizable Representations,Hongzhou Lin;Joshua Robinson;Stefanie Jegelka,hongzhou@mit.edu;joshrob@mit.edu;stefje@csail.mit.edu,6;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,6,0,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2,5;5;5,4,9/25/19,0,0,0,0,0,0,18;1139;3398,4;114;115,2;14;28,0;17;547,m;f
5205,ICLR,2020,Attention Forcing for Sequence-to-sequence Model Training,Qingyun Dou;Yiting Lu;Joshua Efiong;Mark J.F. Gales,qd212@cam.ac.uk;ytl28@cam.ac.uk;je369@cam.ac.uk;mjfg@cam.ac.uk,1;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,2,4,0,yes,9/25/19,University of Cambridge;University of Cambridge;University of Cambridge;University of Cambridge,71;71;71;71,3;3;3;3,3,9/25/19,0,0,0,0,0,0,9;26;0;10544,7;13;1;369,1;2;0;48,1;2;0;1047,m;m
5206,ICLR,2020,Transferable Recognition-Aware Image Processing,Zhuang Liu;Tinghui Zhou;Zhiqiang Shen;Bingyi Kang;Trevor Darrell,zhuangl@berkeley.edu;tinghuiz@eecs.berkeley.edu;zhiqians@andrew.cmu.edu;kang@u.nus.edu;trevor@eecs.berkeley.edu,8;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,8,0,yes,9/25/19,University of California Berkeley;University of California Berkeley;Carnegie Mellon University;National University of Singapore;University of California Berkeley,5;5;1;16;5,13;13;27;25;13,,9/25/19,1,1,1,0,0,0,37577;8114;1116;869;90979,341;24;31;41;559,90;13;10;15;112,2383;1641;176;68;11527,m;m
5207,ICLR,2020,NeuralUCB: Contextual Bandits with Neural Network-Based Exploration,Dongruo Zhou;Lihong Li;Quanquan Gu,drzhou@cs.ucla.edu;lihongli.cs@gmail.com;qgu@cs.ucla.edu,3;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,10,0,yes,9/25/19,"University of California, Los Angeles;Google;University of California, Los Angeles",20;-1;20,17;-1;17,1,9/25/19,0,0,0,0,0,0,349;300;3895,18;40;174,7;8;34,35;22;411,m;m
5208,ICLR,2020,Quantized Reinforcement Learning (QuaRL),Srivatsan Krishnan;Sharad Chitlangia;Maximilian Lam;Zishen Wan;Aleksandra Faust;Vijay Janapa Reddi,srivatsan@seas.harvard.edu;f20170472@goa.bits-pilani.ac.in;maxlam@g.harvard.edu;zishenwan@g.harvard.edu;sandrafaust@google.com;vj@eecs.harvard.edu,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Reject,0,3,0,yes,9/25/19,"Harvard University;BITS Pilani, BITS Pilani;Harvard University;Harvard University;Google;Harvard University",39;480;39;39;-1;39,7;1397;7;7;-1;7,,9/25/19,1,0,0,0,0,0,60;1;477;5;392;5884,16;2;10;7;39;116,4;1;6;1;9;24,5;0;78;0;10;922,m;m
5209,ICLR,2020,CNAS: Channel-Level Neural Architecture Search,Heechul Lim;Min-Soo Kim;Jinjun Xiong,skyde1021@dgist.ac.kr;mskim@dgist.ac.kr;jinjun@us.ibm.com,3;1;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,0,0,yes,9/25/19,Gwangju Institute of Science and Technology;Gwangju Institute of Science and Technology;International Business Machines,-1;-1;-1,-1;-1;-1,,9/25/19,0,0,0,0,0,0,-1;-1;1775,-1;-1;171,-1;-1;23,0;0;155,m;m
5210,ICLR,2020,Measuring Numerical Common Sense: Is A Word Embedding Approach Effective?,Hiroaki Yamane;Chin-Yew Lin;Tatsuya Harada,hiroaki.yamane@riken.jp;cyl@microsoft.com;harada@mi.t.u-tokyo.ac.jp,1;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Reject,0,0,0,yes,9/25/19,RIKEN;Microsoft;The University of Tokyo,-1;-1;56,-1;-1;36,3,9/25/19,1,1,0,0,0,0,55;12122;2594,31;189;213,4;44;26,3;1764;327,m;m
5211,ICLR,2020,Semi-supervised Semantic Segmentation using Auxiliary Network,Wei-Hsu Chen;Hsueh-Ming Hang,qoososola520.ee06g@nctu.edu.tw;hmhang@nctu.edu.tw,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,0,0,yes,9/25/19,National Chiao Tung University;National Chiao Tung University,143;143,564;564,4;2,9/25/19,0,0,0,0,0,0,0;1644,2;207,0;18,0;125,m;m
5212,ICLR,2020,DeepSFM: Structure From Motion Via Deep Bundle Adjustment,Xingkui Wei;Yinda Zhang;Zhuwen Li;Yanwei Fu;Xiangyang Xue,xkwei19@fudan.edu.cn;yindaz@cs.princeton.edu;lzhuwen@gmail.com;yanweifu@fudan.edu.cn;xyxue@fudan.edu.cn,6;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,"Fudan University;Princeton University;Nuro, Inc;Fudan University;Fudan University",79;31;-1;79;79,109;6;-1;109;109,2,9/25/19,2,0,1,0,0,0,2;1744;669;1915;895,1;35;26;90;114,1;15;12;22;16,0;311;122;254;51,m;m
5213,ICLR,2020,A Base Model Selection Methodology for Efficient Fine-Tuning,Yosuke Ueno;Masaaki Kondo,ueno@hal.ipc.i.u-tokyo.ac.jp;kondo@hal.ipc.i.u-tokyo.ac.jp,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,4,0,yes,9/25/19,The University of Tokyo;The University of Tokyo,56;56,36;36,6,9/25/19,0,0,0,0,0,0,160;44,47;33,7;4,8;2,m;m
5214,ICLR,2020,Localized Meta-Learning: A PAC-Bayes Analysis for Meta-Leanring Beyond Global Prior,Chenghao Liu;Tao Lu;Doyen Sahoo;Yuan Fang;Steven C.H. Hoi.,chliu@smu.edu.sg;lutaott@zju.edu.cn;doyensahoo@gmail.com;yfang@smu.edu.sg;shoi@salesforce.com,3;3;8;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,9,0,yes,9/25/19,Singapore Management University;Zhejiang University;;Singapore Management University;SalesForce.com,92;56;-1;92;-1,1397;107;-1;1397;-1,6,9/25/19,0,0,0,0,0,0,11;52;368;20;9066,10;9;27;81;295,2;4;10;2;50,0;1;50;3;849,m;m
5215,ICLR,2020,Searching to Exploit Memorization Effect in Learning from Corrupted Labels,Hansi Yang;Quanming Yao;Bo Han;Gang Niu,yhs17@mails.tsinghua.edu.cn;qyaoaa@connect.ust.hk;bo.han@riken.jp;gang.niu@riken.jp,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,Tsinghua University;The Hong Kong University of Science and Technology;RIKEN;RIKEN,8;39;-1;-1,23;47;-1;-1,,9/25/19,0,0,0,0,0,0,0;935;700;1198,2;58;101;80,0;15;11;17,0;106;20;148,u;m
5216,ICLR,2020,Attention over Phrases,Wanyun Cui,cui.wanyun@sufe.edu.cn,3;1;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,2,0,yes,9/25/19,Shanghai University of Finance and Economics,266,1397,3,9/25/19,0,0,0,0,0,0,396,14,6,61,m
5217,ICLR,2020,Retrospection: Leveraging the Past for Efficient Training of Deep Neural Networks,Ayush Chopra;Surgan Jandial;Mausoom Sarkar;Balaji Krishnamurthy;Vineeth Balasubramanian,ayuchopr@adobe.com;cs17btech11038@iith.ac.in;msarkar@adobe.com;kbalaji@adobe.com;vineethnb@iith.ac.in,8;3,I have read many papers in this area.:I carefully checked the experiments.:N/A:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,Adobe Systems;Indian Institute of Technology Hyderabad;Adobe Systems;Adobe Systems;Indian Institute of Technology Hyderabad,-1;205;-1;-1;205,-1;713;-1;-1;713,,9/25/19,0,0,0,0,0,0,9;6;18;4;1050,12;6;13;7;116,2;1;3;1;15,1;1;2;0;135,m;m
5218,ICLR,2020,Improving Dirichlet Prior Network for Out-of-Distribution Example Detection,Jay Nandy,a0123886@u.nus.edu,3;3;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Reject,0,5,0,yes,9/25/19,National University of Singapore,16,25,,9/25/19,0,0,0,0,0,0,20,5,2,3,m
5219,ICLR,2020,Continual Density Ratio Estimation (CDRE): A new method for evaluating generative models in continual learning,Yu Chen;Song Liu;Tom Diethe;Peter Flach,yc14600@bristol.ac.uk;song.liu@bristol.ac.uk;tdiethe@amazon.com;peter.flach@bristol.ac.uk,1;1;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,5,0,yes,9/25/19,University of Bristol;University of Bristol;Amazon;University of Bristol,128;128;-1;128,87;87;-1;87,5,9/25/19,0,0,0,0,0,0,-1;-1;637;8304,-1;-1;75;321,-1;-1;11;44,0;0;35;689,m;m
5220,ICLR,2020,Prune or quantize? Strategy for Pareto-optimally low-cost and accurate CNN,Kengo Nakata;Daisuke Miyashita;Asuka Maki;Fumihiko Tachibana;Shinichi Sasaki;Jun Deguchi,kengo1.nakata@toshiba.co.jp;daisuke1.miyashita@toshiba.co.jp;asuka.maki@toshiba.co.jp;fumihiko.tachibana@toshiba.co.jp;shinichi8.sasaki@toshiba.co.jp;jun.deguchi@toshiba.co.jp,3;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Reject,0,11,0,yes,9/25/19,Toshiba Memory;Toshiba Memory;Toshiba Memory;Toshiba Memory;Toshiba Memory;Toshiba Memory,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,,9/25/19,0,0,0,0,0,0,2;26;20;95;1;208,6;10;12;21;10;53,1;2;4;6;1;9,0;4;2;15;0;19,m;m
5221,ICLR,2020,Concise Multi-head Attention Models,Srinadh Bhojanapalli;Chulhee Yun;Ankit Singh Rawat;Sashank Reddi;Sanjiv Kumar,bsrinadh@google.com;chulheey@mit.edu;ankitsrawat@google.com;sashank@google.com;sanjivk@google.com,1;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,3,0,yes,9/25/19,Google;Massachusetts Institute of Technology;Google;Google;Google,-1;2;-1;-1;-1,-1;5;-1;-1;-1,3,9/25/19,0,0,0,0,0,0,1642;202;1431;2194;941,28;22;84;53;142,16;6;19;20;13,199;14;140;418;113,m;m
5222,ICLR,2020,A Finite-Time Analysis of  Q-Learning with Neural Network Function Approximation,Pan Xu;Quanquan Gu,panxu@cs.ucla.edu;qgu@cs.ucla.edu,3;6;6,I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Reject,4,6,0,yes,9/25/19,"University of California, Los Angeles;University of California, Los Angeles",20;20,17;17,1;9,9/25/19,5,2,1,0,0,0,295;3895,31;174,10;34,42;411,m;m
5223,ICLR,2020,SCELMo: Source Code Embeddings from Language Models,Rafael - Michael Karampatsis;Charles Sutton,mpatsis13@gmail.com;charlessutton@google.com,3;3;8,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,4,0,yes,9/25/19,University of Edinburgh;Google,33;-1,30;-1,3,9/25/19,1,0,0,0,0,0,0;50,1;18,0;3,0;4,m;m
5224,ICLR,2020,GraphMix: Regularized Training of Graph Neural Networks for Semi-Supervised Learning,Vikas Verma;Meng Qu;Alex Lamb;Yoshua Bengio;Juho Kannala;Jian Tang,vikasverma.iitm@gmail.com;meng.qu@umontreal.ca;lambalex@iro.umontreal.ca;yoshua.bengio@mila.quebec;juho.kannala@aalto.fi;jian.tang@hec.ca,3;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,1,5,0,yes,9/25/19,;University of Montreal;University of Montreal;University of Montreal;Aalto University;HEC Montreal,-1;128;128;128;143;128,-1;85;85;85;182;85,10,9/25/19,9,3,5,0,0,2,202;504;2136;208566;3645;346,25;40;22;807;127;94,4;5;8;147;24;8,40;100;203;24297;499;27,m;m
5225,ICLR,2020,Neural networks with motivation,Sergey A. Shuvaev;Ngoc B. Tran;Marcus Stephenson-Jones;Bo Li;Alexei A. Koulakov,sshuvaev@cshl.edu;ntran@cshl.edu;mstephen@cshl.edu;bli@cshl.edu;akula@cshl.edu,1;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Reject,0,11,0,yes,9/25/19,Cold Spring Harbor Laboratory;Cold Spring Harbor Laboratory;Cold Spring Harbor Laboratory;Cold Spring Harbor Laboratory;Cold Spring Harbor Laboratory,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,5,6/23/19,0,0,0,0,0,0,16;94;563;997;1816,8;12;14;67;83,3;3;9;9;16,0;6;27;114;140,m;m
5226,ICLR,2020,Measuring causal influence with back-to-back regression: the linear case,Jean-Remi King;Francois Charton;Maxime Oquab;David Lopez-Paz,jeanremi@fb.com;fcharton@fb.com;qas@fb.com;dlp@fb.com,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Reject,0,6,0,yes,9/25/19,Facebook;Facebook;Facebook;Facebook,-1;-1;-1;-1,-1;-1;-1;-1,,9/25/19,1,0,1,0,0,0,1273;1;2971;2483,45;14;20;46,16;1;7;19,90;0;222;428,m;m
5227,ICLR,2020,Longitudinal Enrichment of Imaging Biomarker Representations for Improved Alzheimer's Disease Diagnosis,Saad Elbeleidy;Lyujian Lu;L. Zoe Baker;Hua Wang;Feiping Nie,selbeleidy@mymail.mines.edu;lyujianlu@mines.edu;laurenzoebaker@mymail.mines.edu;huawangcs@gmail.com;feipingnie@gmail.com,1;3;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Reject,0,0,0,yes,9/25/19,Colorado School of Mines;Colorado School of Mines;Colorado School of Mines;Colorado School of Mines;,172;172;172;172;-1,343;343;343;343;-1,,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,0;0;0;0;0,m;m
5228,ICLR,2020,RefNet: Automatic Essay Scoring by Pairwise Comparison,Jiaxin Li;Jinan Zhou,jiaxin.li@link.cuhk.edu.hk;jinan.zhou@link.cuhk.edu.hk,1;3;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,The Chinese University of Hong Kong;The Chinese University of Hong Kong,59;59,35;35,,9/25/19,0,0,0,0,0,0,499;6,62;5,10;1,58;0,m;m
5229,ICLR,2020,Topology of deep neural networks,Gregory Naitzat;Andrey Zhitnikov;Lek-Heng Lim,gregn@uchicago.edu;andreyz@technion.ac.il;lekheng@galton.uchicago.edu,1;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,University of Chicago;Technion;University of Chicago,48;26;48,9;412;9,8,9/25/19,0,0,0,0,0,0,31;18;4069,5;5;92,2;1;24,3;0;399,m;m
5230,ICLR,2020,Provable Convergence and Global Optimality  of Generative Adversarial Network,Qi Cai;Zhuoran Yang;Jason D. Lee;Shaolei S. Du;Zhaoran Wang,qicai2022@u.northwestern.edu;zy6@princeton.edu;jasonlee@princeton.edu;ssdu@ias.edu;zhaoranwang@gmail.com,3;3;3,I do not know much about this area.:N/A:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:N/A:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,"Northwestern University;Princeton University;Princeton University;Institue for Advanced Study, Princeton;Northwestern University",44;31;31;-1;44,22;6;6;-1;22,5;4;1;9,9/25/19,0,0,0,0,0,0,2;194;5042;2;126,9;42;121;2;26,1;7;36;1;7,0;23;620;0;16,f;m
5231,ICLR,2020,Improving Irregularly Sampled Time Series Learning with Dense Descriptors of Time,Rafael Teixeira Sousa;Lucas Araújo Pereira;Anderson da Silva Soares,rafaelts777@gmail.com;apereiral@outlook.com;engsoares@gmail.com,1;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Universidade Federal de Goiàs;;,-1;-1;-1,-1;-1;-1,,9/25/19,0,0,0,0,0,0,12;35;4,5;10;7,3;3;2,0;3;0,m;m
5232,ICLR,2020,Understanding and Training Deep Diagonal Circulant Neural Networks,Alexandre Araujo;Benjamin Negrevergne;Yann Chevaleyre;Jamal Atif,alexandre.araujo@dauphine.eu;benjamin.negrevergne@dauphine.psl.eu;yann.chevaleyre@lamsade.dauphine.fr;jamal.atif@lamsade.dauphine.fr,1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,2,,yes,9/25/19,Univeristé Paris-Dauphine;Univeristé Paris-Dauphine;Univeristé Paris-Dauphine;Univeristé Paris-Dauphine,481;481;481;481,1397;1397;1397;1397,,1/29/19,0,0,0,0,0,0,38;278;2407;991,9;30;102;114,4;9;26;14,7;23;200;42,m;m
5233,ICLR,2020,DP-LSSGD: An Optimization Method to Lift the Utility in Privacy-Preserving ERM,Bao Wang;Quanquan Gu;March Boedihardjo;Farzin Barekat;Stanley J. Osher,wangbaonj@gmail.com;qgu@cs.ucla.edu;march@math.ucla.edu;fbarekat@math.ucla.edu;sjo@math.ucla.edu,3;6;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,"University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles",20;20;20;20;20,17;17;17;17;17,,9/25/19,0,0,0,0,0,0,2;4160;29;56;145,11;175;22;21;25,1;35;4;5;5,0;416;3;2;15,m;m
5234,ICLR,2020,On Global Feature Pooling for Fine-grained Visual Categorization,Pei Guo;Connor Anderson;Ryan Farrell,peiguo@cs.byu.edu;thecatalystak@gmail.com;farrell@cs.byu.edu,6;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,2,0,,yes,9/25/19,Brigham Young University;Brigham Young University;Brigham Young University,-1;-1;-1,-1;-1;-1,8,9/25/19,0,0,0,0,0,0,241;101;1,54;14;8,8;2;1,14;5;0,m;m
5235,ICLR,2020,Quantifying Exposure Bias for Neural Language Generation,Tianxing He;Jingzhao Zhang;Zhiming Zhou;James Glass,cloudygoose@csail.mit.edu;jzhzhang@mit.edu;heyohai@apex.sjtu.edu.cn;glass@mit.edu,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,6,,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Shanghai Jiao Tong University;Massachusetts Institute of Technology,2;2;53;2,5;5;157;5,3,5/25/19,5,0,0,0,0,0,151;439;243;94,18;41;29;43,5;7;8;5,20;17;21;8,m;m
5236,ICLR,2020,Mix-review: Alleviate Forgetting in the Pretrain-Finetune Framework for Neural Language Generation Models,Tianxing He;Jun Liu;Kyunghyun Cho;Myle Ott;Bing Liu;James Glass;Fuchun Peng,tianxing@mit.edu;junliu@fb.com;kyunghyuncho@fb.com;myleott@fb.com;bingl@fb.com;glass@mit.edu;fuchunpeng@fb.com,3;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,Massachusetts Institute of Technology;Facebook;Facebook;Facebook;Facebook;Massachusetts Institute of Technology;Facebook,2;-1;-1;-1;-1;2;-1,5;-1;-1;-1;-1;5;-1,,9/25/19,2,0,0,0,0,0,151;419;47561;4157;304;11517;3179,18;134;274;39;86;342;68,5;10;53;20;10;57;23,20;37;6747;818;15;966;299,m;m
5237,ICLR,2020,INVOCMAP: MAPPING METHOD NAMES TO METHOD INVOCATIONS VIA MACHINE LEARNING,Hung Phan;Ali Jannesari,hungphd@iastate.edu;jannesar@iastate.edu,1;1;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,4,,yes,9/25/19,Iowa State University;Iowa State University,172;172,399;399,3,9/25/19,0,0,0,0,0,0,533;933,38;99,13;18,48;20,m;m
5238,ICLR,2020,Fix-Net: pure fixed-point representation of deep neural networks,Lukas Enderich;Fabian Timm;Lars Rosenbaum;Wolfram Burgard,lukas.enderich@de.bosch.com;fabian.timm@de.bosch.com;lars.rosenbaum@de.bosch.com;burgard@informatik.uni-freiburg.de,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,0,,yes,9/25/19,Bosch;Bosch;Bosch;Universität Freiburg,-1;-1;-1;118,-1;-1;-1;85,,9/25/19,0,0,0,0,0,0,4;356;567;53280,3;20;30;783,1;6;11;100,0;40;25;3906,m;m
5239,ICLR,2020,Random Partition Relaxation for Training Binary and Ternary Weight Neural Network,Lukas Cavigelli;Luca Benini,cavigelli@iis.ee.ethz.ch;benini@iis.ee.ethz.ch,1;3;1;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10,13;13,,9/25/19,2,0,0,0,0,0,1013;35085,49;1313,12;86,92;1959,m;m
5240,ICLR,2020,Emergent Communication in Networked Multi-Agent Reinforcement Learning,Shubham Gupta;Rishi Hazra;Amebdkar Dukkipati,shubhamg@iisc.ac.in;rishihazra@iisc.ac.in;ambedkar@iisc.ac.in,3;1;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,Indian Institute of Science;Indian Institute of Science;Indian Institute of Science,95;95;95,301;301;301,,9/25/19,0,0,0,0,0,0,1084;0;0,130;4;1,16;0;0,57;0;0,m;m
5241,ICLR,2020,Recognizing Plans by Learning Embeddings from Observed Action Distributions,Yantian Zha;Yikang Li;Sriram Gopalakrishnan;Hankz Hankui Zhuo;Baoxin Li;Subbarao Kambhampati,yantian.zha@asu.edu;yikangli@asu.edu;sgopal28@asu.edu;zhuohank@mail.sysu.edu.cn;baoxin.li@asu.edu;rao@asu.edu,1;3;1;3,I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,5,,yes,9/25/19,Arizona State University;Arizona State University;Arizona State University;SUN YAT-SEN UNIVERSITY;Arizona State University;Arizona State University,95;95;95;481;95;95,155;155;155;299;155;155,,12/5/17,5,0,0,0,0,0,38;690;18;631;3794;8142,6;73;16;54;227;419,3;12;3;14;29;44,0;86;0;29;294;542,m;m
5242,ICLR,2020,Cancer homogeneity in single cell revealed by Bi-state model and Binary matrix factorization,Changlin Wan;Wennan Chang;Sha Cao;Xiao Wang;Chi Zhang,wan82@purdue.edu;chang534@purdue.edu;shacao@iu.edu;wangxiao@purdue.edu;czhang87@iu.edu,1;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Withdrawn,0,0,,yes,9/25/19,"Purdue University;Purdue University;Indiana University, Bloomington;Purdue University;Indiana University, Bloomington",27;27;73;27;73,88;88;134;88;134,,9/25/19,0,0,0,0,0,0,116;0;164;12174;176,26;2;23;370;86,6;0;6;65;6,3;0;6;364;6,m;m
5243,ICLR,2020,Residual EBMs: Does Real vs. Fake Text Discrimination Generalize?,Anton Bakhtin;Sam Gross;Myle Ott;Yuntian Deng;Marc'Aurelio Ranzato;Arthur Szlam,yolo@fb.com;sgross@fb.com;myleott@fb.com;dengyuntian@g.harvard.edu;ranzato@fb.com;aszlam@fb.com,1;3;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Facebook;Facebook;Facebook;Harvard University;Facebook;Facebook,-1;-1;-1;39;-1;-1,-1;-1;-1;7;-1;-1,3;8,9/25/19,0,0,0,0,0,0,178;7820;4157;1513;56;9245,14;17;39;31;17;87,6;11;20;13;4;35,27;954;818;234;7;969,m;m
5244,ICLR,2020,EnsembleNet: A novel architecture for Incremental Learning,Suri Bhasker Sri Harsha;Y Kalidas,cs18s506@iittp.ac.in;ykalidas@iittp.ac.in,1;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,Indian Institute of Technology Tirupati;Indian Institute of Technology Tirupati,-1;-1,-1;-1,,9/25/19,0,0,0,0,0,0,0;75,1;6,0;1,0;6,f;m
5245,ICLR,2020,Dual Sequential Monte Carlo: Tunneling Filtering and Planning in Continuous POMDPs,Yunbo Wang;Bo Liu;Jiajun Wu;Yuke Zhu;Simon Shaolei Du;Li Fei-Fei;Joshua B. Tenenbaum,yunbo.thu@gmail.com;bliu@cs.utexas.edu;jiajunw@stanford.edu;yukez@cs.stanford.edu;ssdu@ias.edu;feifeili@cs.stanford.edu;jbt@mit.edu,1;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,4,,yes,9/25/19,"Tsinghua University;University of Texas, Austin;Stanford University;Stanford University;Institue for Advanced Study, Princeton;Stanford University;Massachusetts Institute of Technology",8;22;4;4;-1;4;2,23;38;4;4;-1;4;5,4,9/25/19,3,0,0,0,0,0,654;6805;4151;2473;2135;84787;33543,29;627;91;57;56;452;599,11;34;30;20;20;99;88,52;414;394;301;322;11922;2752,m;m
5246,ICLR,2020,Fast Bilinear Matrix Normalization via  Rank-1 Update,Tan Yu;Yunfeng Cai;Ping Li,tanyu01@baidu.com;caiyunfeng@baidu.com;liping11@baidu.com,6;3;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,7,,yes,9/25/19,Baidu;Baidu;Baidu,-1;-1;-1,-1;-1;-1,2,9/25/19,0,0,0,0,0,0,289;30;332,67;15;109,8;1;10,26;3;19,m;m
5247,ICLR,2020,Task-Mediated Representation Learning,Sergei Bugrov;Ron Sun,bugros@rpi.edu;rsun@rpi.edu,1;1;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,0,,yes,9/25/19,Rensselaer Polytechnic Institute;Rensselaer Polytechnic Institute,172;172,438;438,5,9/25/19,0,0,0,0,0,0,0;1204,1;66,0;15,0;105,m;m
5248,ICLR,2020,Universality Theorems for Generative Models,Valentin Khrulkov;Ivan Oseledets,khrulkov.v@gmail.com;i.oseledets@skoltech.ru,3;1;3,I have read many papers in this area.:N/A:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:N/A:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:N/A:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Skolkovo Institute of Science and Technology;Skolkovo Institute of Science and Technology,-1;-1,-1;-1,5;1,5/27/19,1,0,0,0,0,0,165;4783,12;200,7;31,17;413,m;m
5249,ICLR,2020,Embodied Language Grounding with Implicit 3D Visual Feature Representations,Mihir Prabhudesai;Hsiao-Yu Fish Tung;Syed Ashar Javed;Maximilian Sieb;Adam W. Harley;Katerina Fragkiadaki,mprabhud@cs.cmu.edu;htung@cs.cmu.edu;sajaved@andrew.cmu.edu;aharley@cs.cmu.edu;katef@cs.cmu.edu,3;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,0,,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1;1,27;27;27;27;27,3;2,9/25/19,3,0,0,0,0,0,3;457;19;8;523;1992,3;25;6;6;20;55,1;8;3;2;8;15,0;44;3;0;59;205,m;f
5250,ICLR,2020,Geometry-Aware Visual Predictive Models of Intuitive Physics,Hsiao-Yu Fish Tung;Zhou Xian;Mihir Prabhudesai;Katerina Fragkiadaki,htung@cs.cmu.edu;zhouxian@cmu.edu;mprabhud@cs.cmu.edu;katef@cs.cmu.edu,3;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University,1;1;1;1,27;27;27;27,2,9/25/19,0,0,0,0,0,0,457;8;3;1992,25;31;3;55,8;2;1;15,44;2;0;205,f;f
5251,ICLR,2020,PNEN: Pyramid Non-Local Enhanced Networks,Feida Zhu;Chaowei Fang;Kai-Kuang Ma,feida.zhu@ntu.edu.sg;chwfang@connect.hku.hk;ekkma@ntu.edu.sg,1;1;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,National Taiwan University;The University of Hong Kong;National Taiwan University,86;92;86,120;35;120,,9/25/19,0,0,0,0,0,0,7;48;0,10;12;7,2;4;0,0;7;0,m;m
5252,ICLR,2020,"Being Bayesian, Even Just a Bit, Fixes Overconfidence in ReLU Networks",Agustinus Kristiadi;Matthias Hein;Philipp Hennig,agustinus.kristiadi@uni-tuebingen.de;matthias.hein@uni-tuebingen.de;philipp.hennig@uni-tuebingen.de,1;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,University of Tuebingen;University of Tuebingen;University of Tuebingen,154;154;154,91;91;91,11,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1,-1;-1;-1,-1;-1;-1,0;0;0,m;m
5253,ICLR,2020,Auto Network Compression with Cross-Validation Gradient,Nannan Tian;Yong Liu,tiannannan@iie.ac.cn;liuyong@iie.ac.cn,1;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,"Institute of information engineering, CAS;Institute of information engineering, CAS",-1;-1,-1;-1,1,9/25/19,0,0,0,0,0,0,140;3225,15;255,7;30,2;140,f;m
5254,ICLR,2020,Feature-based Augmentation for Semi-Supervised Learning,Min-Hye Oh;Byung-Gook Park,listogato3@gmail.com;bgpark@snu.ac.kr,3;1;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,;Seoul National University,-1;41,-1;64,8,9/25/19,0,0,0,0,0,0,3;110,10;49,1;3,0;0,m;m
5255,ICLR,2020,Capsule Networks without Routing Procedures,Zhenhua Chen;Xiwen Li;Chuhua Wang;David Crandall,chen478@iu.edu;xiwenli@wustl.edu;cw234@iu.edu;djcran@indiana.edu,1;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,6,3,,yes,9/25/19,"Indiana University, Bloomington;Washington University, St. Louis;Indiana University, Bloomington;University of Arizona",73;100;73;172,134;52;134;103,4,9/25/19,1,0,0,0,0,0,105;4;3;5948,39;9;3;186,6;1;1;32,4;0;0;457,m;m
5256,ICLR,2020,Noisy $\ell^{0}$-Sparse Subspace Clustering on Dimensionality Reduced Data,Yingzhen Yang,superyyzg@gmail.com,3;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Withdrawn,0,0,,yes,9/25/19,Independent Researcher,-1,-1,,9/25/19,0,0,0,0,0,0,2130,83,20,175,m
5257,ICLR,2020,Side-Tuning: Network Adaptation via Additive Side Networks,Alexander Sax;Jeffrey Zhang;Amir Zamir;Silvio Savarese;Jitendra Malik,sax@berkeley.edu;jozhang@berkeley.edu;zamir@cs.stanford.edu;ssilvio@cs.stanford.edu;malik@eecs.berkeley.edu,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,5,,yes,9/25/19,University of California Berkeley;University of California Berkeley;Stanford University;Stanford University;University of California Berkeley,5;5;4;4;5,13;13;4;4;13,3;6,9/25/19,3,0,0,0,0,0,455;1108;5450;18240;12683,9;28;51;285;36,4;14;22;68;13,47;57;1162;2603;1267,m;m
5258,ICLR,2020,Deep Multivariate Mixture of Gaussians for Object Detection under Occlusion,Yihui He;Jianren Wang,he2@andrew.cmu.edu;jianrenw@andrew.cmu.edu,1;1;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,0,,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University,1;1,27;27,,9/25/19,0,0,0,0,0,0,1342;122,36;28,10;5,204;8,m;m
5259,ICLR,2020,Dynamic Graph Message Passing Networks,Li Zhang;Dan Xu;Anurag Arnab;Philip H.S. Torr,lz@robots.ox.ac.uk;danxu@robots.ox.ac.uk;anurag.arnab@gmail.com;phst@robots.ox.ac.uk,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,3,,yes,9/25/19,University of Oxford;University of Oxford;Google;University of Oxford,50;50;-1;50,1;1;-1;1,2;10,8/19/19,11,0,0,0,0,0,323;108;683;30614,98;50;25;360,9;5;11;87,23;6;59;3973,m;m
5260,ICLR,2020,POLYNOMIAL ACTIVATION FUNCTIONS,Vikas Gottemukkula,vikas11187@gmail.com,3;1;1,I have published one or two papers in this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Zoloz,-1,-1,,9/25/19,0,0,0,0,0,0,124,11,6,9,m
5261,ICLR,2020,CopyCAT: Taking Control of Neural Policies with Constant Attacks,Léonard Hussenot;Matthieu Geist;Olivier Pietquin,hussenot@google.com;mfgeist@google.com;pietquin@google.com,1;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,4,,yes,9/25/19,Google;Google;Google,-1;-1;-1,-1;-1;-1,4,5/29/19,0,0,0,0,0,0,4;100;408,7;18;42,1;3;6,0;7;50,m;m
5262,ICLR,2020,Influence-aware Memory for Deep Reinforcement Learning,Miguel Suau;Elena Congeduti;Rolf A.N. Starre;Aleksander Czechowski;Frans A. Oliehoek,m.suaudecastro@tudelft.nl;e.congeduti@tudelft.nl;a.t.czechowski@tudelft.nl;r.a.n.starre@tudelft.nl;f.a.oliehoek@tudelft.nl,1;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,1,4,,yes,9/25/19,Delft University of Technology;Delft University of Technology;Delft University of Technology;Delft University of Technology;Delft University of Technology,89;89;89;89;89,67;67;67;67;67,,9/25/19,0,0,0,0,0,0,0;4;0;42;1794,1;2;2;23;113,0;1;0;4;23,0;0;0;2;154,m;m
5263,ICLR,2020,CurricularFace: Adaptive Curriculum Learning Loss for Deep Face Recognition,Yuge Huang;Yuhan Wang;Ying Tai;Xiaoming Liu;Pengcheng Shen;Shaoxin Li;Jilin Li;Feiyue Huang,huangyg@zju.edu.cn;wang_yuhan@zju.edu.cn;yingtai@tencent.com;liuxm@cse.msu.edu;quantshen@tencent.com;darwinli@tencent.com;jerolinli@tencent.com;garyhuang@tencent.com,3;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Withdrawn,0,3,,yes,9/25/19,Zhejiang University;Zhejiang University;Tencent AI Lab;SUN YAT-SEN UNIVERSITY;Tencent AI Lab;Tencent AI Lab;Tencent AI Lab;Tencent AI Lab,56;56;-1;481;-1;-1;-1;-1,107;107;-1;299;-1;-1;-1;-1,2,9/25/19,4,0,0,0,0,0,76;-1;1797;58;161;7;134;726,12;-1;52;24;12;4;35;84,4;-1;15;4;5;2;6;15,2;0;332;2;4;0;7;104,f;m
5264,ICLR,2020,Compressive Hyperspherical Energy Minimization,Rongmei Lin;Weiyang Liu;Zhen Liu;Chen Feng;Zhiding Yu;James M. Rehg;Li Xiong;Le Song,rongmei.lin@emory.edu;wyliu@gatech.edu;zhen.liu.2@umontreal.ca;cfeng@nyu.edu;zhidingy@nvidia.com;rehg@gatech.edu;lxiong@emory.edu;lsong@cc.gatech.edu,6;3;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,9,,yes,9/25/19,Emory University;Georgia Institute of Technology;University of Montreal;New York University;NVIDIA;Georgia Institute of Technology;Emory University;Georgia Institute of Technology,172;13;128;25;-1;13;172;13,416;38;85;29;-1;38;416;38,4;8,9/25/19,2,0,0,0,0,0,65;2127;1409;1011;2618;16263;4582;10105,11;62;112;25;66;302;139;329,4;15;21;13;15;62;32;55,6;326;31;111;350;1554;382;1133,f;m
5265,ICLR,2020,Testing Robustness Against Unforeseen Adversaries,Daniel Kang*;Yi Sun*;Dan Hendrycks;Tom Brown;Jacob Steinhardt,ddkang@stanford.edu;yisun@math.columbia.edu;hendrycks@berkeley.edu;tom@openai.com;jsteinhardt@berkeley.edu,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,1,4,,yes,9/25/19,Stanford University;Columbia University;University of California Berkeley;OpenAI;University of California Berkeley,4;15;5;-1;5,4;16;13;-1;13,4,8/21/19,21,0,0,0,0,0,129;673;1622;386;2405,22;95;27;27;58,5;13;14;7;19,16;47;309;27;215,m;m
5266,ICLR,2020,Reasoning-Aware Graph Convolutional Network for Visual Question Answering,Yangyang Cheng;Chun Yuan,chengyang317@gmail.com;yuanc@sz.tsinghua.edu.cn,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Tsinghua University;Tsinghua University,8;8,23;23,10,9/25/19,0,0,0,0,0,0,26;1712,14;187,4;18,0;78,f;m
5267,ICLR,2020,Text Embedding Bank Module for Detailed Image Paragraph Caption,Zengming Shen;Arjun Gupta;Thomas S. Huang,zshen5@illinois.edu;arjung2@illinois.edu;t-huang1@illinois.edu,1;1;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,"University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",3;3;3,48;48;48,3,9/25/19,0,0,0,0,0,0,223;9;1088,12;14;33,5;2;4,0;0;99,m;m
5268,ICLR,2020,FNNP: Fast Neural Network Pruning Using Adaptive Batch Normalization,Bailin Li;Bowen Wu;Jiang Su;Guangrun Wang,bl-zorro@163.com;wubw6@mail2.sysu.edu.cn;sujiang@dm-ai.cn;wangguangrun@dm-ai.cn,3;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Withdrawn,1,5,,yes,9/25/19,SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;DMAI Inc.;DMAI Inc.,481;481;-1;-1,299;299;-1;-1,,9/25/19,0,0,0,0,0,0,11;11;1286;1050,15;14;120;34,2;1;17;12,1;0;115;67,m;m
5269,ICLR,2020,Boosting Ticket: Towards Practical Pruning for Adversarial Training with Lottery Ticket Hypothesis,Bai Li;Shiqi Wang;Yunhan Jia;Yantao Lu;Zhenyu Zhong;Lawrence Carin;Suman Jana,bai.li@duke.edu;tcwangshiqi@cs.columbia.edu;jack0082010@gmail.com;ylu25@syr.edu;edwardzhong@baidu.com;lcarin@duke.edu;suman@cs.columbia.edu,3;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Duke University;Columbia University;University of Michigan;Syracuse University;Baidu;Duke University;Columbia University,47;15;8;233;-1;47;15,20;16;21;292;-1;20;16,4;9,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,0;0;0;0;0;0;0,m;m
5270,ICLR,2020,Stabilizing Neural ODE Networks with Stochasticity,Xuanqing Liu;Tesi Xiao;Si Si;Qin Cao;Sanjiv Kumar;Cho-Jui Hsieh,xqliu@cs.ucla.edu;texiao@ucdavis.edu;sisidaisy@google.com;qincao@google.com;sanjivk@google.com;chohsieh@cs.ucla.edu,6;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,"University of California, Los Angeles;University of California, Davis;Google;Google;Google;University of California, Los Angeles",20;79;-1;-1;-1;20,17;55;-1;-1;-1;17,4;8,9/25/19,0,0,0,0,0,0,262;15;336;8;1096;319,20;4;10;12;142;23,6;1;2;1;15;4,38;2;48;0;115;59,m;m
5271,ICLR,2020,Robustness and/or Redundancy Emerge in Overparametrized Deep Neural Networks,Stephen Casper;Xavier Boix;Vanessa D'Amario;Christopher Rodriguez;Ling Guo;Kasper Vinken;Gabriel Kreiman,scasper@college.harvard.edu;xboix@mit.edu;vanedamario@gmail.com;chrizrodz@gmail.com;kasper.vinken@kuleuven.be;gabriel.kreiman@childrens.harvard.edu,8;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,0,,yes,9/25/19,Harvard University;Massachusetts Institute of Technology;Università degli Studi di Genova;;KU Leuven;Harvard University,39;2;-1;-1;118;39,7;5;-1;-1;45;7,,9/25/19,0,0,0,0,0,0,96;1582;1;48;55;121;12557,100;52;6;20;24;20;184,6;16;1;3;4;7;34,3;236;0;5;2;8;919,m;m
5272,ICLR,2020,Interpretable Deep Neural Network Models: Hybrid of Image Kernels and Neural Networks,Mr. Jay Hoon Jung;and Prof. YoungMin Kwon,jay.jung@stonybrook.edu;youngmin.kwon@sunykorea.ac.kr,1;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,"State University of New York, Stony Brook;Korea University",41;323,304;179,,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1,-1;-1,-1;-1,0;0,m;m
5273,ICLR,2020,EnsembleNet: End-to-End Optimization of Multi-headed Models,Hanhan Li;Joe Ng;Apostol (Paul) Natsev,mirror.haha@gmail.com;yhng@google.com;natsev@google.com,3;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,6,,yes,9/25/19,Google;Google;Google,-1;-1;-1,-1;-1;-1,,5/24/19,2,0,0,0,0,0,2;1823;4502,6;10;102,1;6;26,0;154;480,m;m
5274,ICLR,2020,PatchVAE: Learning Local Latent Codes for Recognition,Kamal Gupta;Saurabh Singh;Abhinav Shrivastava,kamalgupta308@gmail.com;saurabhsingh@google.com;abhinav@cs.umd.edu,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,0,,yes,9/25/19,"University of Maryland, College Park;Google;University of Maryland, College Park",12;-1;12,91;-1;91,5,9/25/19,1,0,0,0,0,0,100;129;193,8;41;32,4;6;4,11;4;10,m;m
5275,ICLR,2020,Improving One-Shot NAS By Suppressing The Posterior Fading,Xiang Li*;Chen Lin*;Chuming Li;Ming Sun;Wei Wu;Junjie Yan;Wanli Ouyang,xiang_li_1@brown.edu;linchen@sensetime.com;lichuming@sensetime.com;sunming1@sensetime.com;wuwei@sensetime.com;yanjunjie@sensetime.com;wanli.ouyang@sydney.edu.au,3;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,Brown University;SenseTime Group Limited;SenseTime Group Limited;SenseTime Group Limited;SenseTime Group Limited;SenseTime Group Limited;University of Sydney,67;-1;-1;-1;-1;-1;86,53;-1;-1;-1;-1;-1;60,11;2,9/25/19,7,0,0,0,0,0,883;221;21;715;16282;7431;11391,143;98;10;44;940;169;152,17;8;3;8;58;44;54,93;15;5;27;1391;1035;1238,m;m
5276,ICLR,2020,Min-max Entropy for Weakly Supervised Pointwise Localization,Belharbi Soufiane;Rony Jérôme;Dolz Jose;Ben Ayed Ismail;McCaffrey Luke;Granger Eric,soufiane.belharbi.1@etsmtl.net;jerome.rony.1@etsmtl.net;jose.dolz@etsmtl.ca;ismail.benayed@etsmtl.ca;luke.mccaffrey@mcgill.ca;eric.granger@etsmtl.ca,3;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,École de technologie supérieure;École de technologie supérieure;École de technologie supérieure;École de technologie supérieure;McGill University;École de technologie supérieure,481;481;481;481;86;481,1397;1397;1397;1397;42;1397,,9/25/19,2,0,0,0,0,0,69;7;863;2511;968;99,18;5;69;151;37;29,4;2;16;27;15;4,4;0;61;185;48;7,m;m
5277,ICLR,2020,Context-Gated Convolution,Xudong Lin;Lin Ma;Wei Liu;Shih-Fu Chang,xudong.lin@columbia.edu;forest.linma@gmail.com;wl2223@columbia.edu;shih.fu.chang@columbia.edu,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,3,,yes,9/25/19,Columbia University;Tencent AI Lab;Columbia University;Columbia University,15;-1;15;15,16;-1;16;16,3,9/25/19,1,0,0,0,0,0,27;68;8671;280,15;43;346;28,3;5;44;6,3;6;872;22,m;m
5278,ICLR,2020,FAKE CAN BE REAL IN GANS,Song Tao;Jia Wang,taosong@sjtu.edu.cn;jiawang@sjtu.edu.cn,1;3;8,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Withdrawn,0,4,,yes,9/25/19,Shanghai Jiao Tong University;Shanghai Jiao Tong University,53;53,157;157,5;4;1;8,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1,-1;-1,-1;-1,0;0,m;f
5279,ICLR,2020,WHAT ILLNESS OF LANDSCAPE CAN OVER-PARAMETERIZATION ALONE CURE?,Dawei Li;Tian Ding;Ruoyu Sun,dawei2@illinois.edu;dt016@ie.cuhk.edu.hk;ruoyus@illinois.edu,3;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,9,,yes,9/25/19,"University of Illinois, Urbana Champaign;The Chinese University of Hong Kong;University of Illinois, Urbana Champaign",3;59;3,48;35;48,1,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1,-1;-1;-1,-1;-1;-1,0;0;0,m;m
5280,ICLR,2020,Representational Disentanglement for Multi-Domain Image Completion,Liyue Shen;Wentao Zhu;Xiaosong Wang;Lei Xing;John Pauly;Baris Turkbey;Stephanie Harmon;Thomas Sanford;Sherif Mehralivand;Peter L. Choyke;Bradford J. Wood;Daguang Xu,liyues@stanford.edu;wentaoz@nvidia.com;xiaosongw@nvidia.com;lei@stanford.edu;pauly@stanford.edu;ismail.turkbey@nih.gov;stephanie.harmon@nih.gov;thomas.sanford@nih.gov;sherif.mehralivand@nih.gov;pchoyke@mail.nih.gov;bwood@nih.gov;daguangx@nvidia.com,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,Stanford University;NVIDIA;NVIDIA;Stanford University;Stanford University;National Institutes of Health;National Institutes of Health;National Institutes of Health;National Institutes of Health;National Institutes of Health;National Institutes of Health;NVIDIA,4;-1;-1;4;4;-1;-1;-1;-1;-1;-1;-1,4;-1;-1;4;4;-1;-1;-1;-1;-1;-1;-1,5;4;2,9/25/19,0,0,0,0,0,0,1454;133;54;1965;16463;2458;317;3697;343;43302;14455;344,22;24;17;199;316;119;100;126;92;1280;577;44,6;5;4;23;60;21;10;34;10;95;63;10,500;14;2;77;975;65;11;399;11;1368;459;21,f;m
5281,ICLR,2020,SCL: Towards Accurate Domain Adaptive Object Detection via Gradient Detach Based Stacked Complementary Losses,Zhiqiang Shen;Harsh Maheshwari;Weichen Yao;Marios Savvides,zhiqians@andrew.cmu.edu;harshmaheshwari135@gmail.com;wyao2@andrew.cmu.edu;marioss@andrew.cmu.edu,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,1,5,,yes,9/25/19,Carnegie Mellon University;Indian Institute of Technology Kharagpur;Carnegie Mellon University;Carnegie Mellon University,1;266;1;1,27;476;27;27,2,9/25/19,6,0,0,0,0,0,1137;13;9;6320,31;6;8;286,10;2;1;40,180;4;3;603,m;m
5282,ICLR,2020,Relevant-features based Auxiliary Cells for Robust and Energy Efficient Deep Learning,Aparna Aketi;Priyadarshini Panda;Kaushik Roy,saketi@purdue.edu;priya.panda@yale.edu;kaushik@purdue.edu,1;3;6,I have published in this field for several years.:I carefully checked the experiments.:I made a quick assessment of this paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,5,,yes,9/25/19,Purdue University;Yale University;Purdue University,27;64;27,88;8;88,,9/25/19,0,0,0,0,0,0,0;674;243,1;66;72,0;15;10,0;45;22,f;m
5283,ICLR,2020,Learning to Transfer Learn,Linchao Zhu;Sercan O. Arik;Yi Yang;Tomas Pfister,zhulinchao7@gmail.com;soarik@google.com;yi.yang@uts.edu.au;tpfister@google.com,3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,4,,yes,9/25/19,University of Technology Sydney;Google;University of Technology Sydney;Google,108;-1;108;-1,193;-1;193;-1,6,9/25/19,1,0,0,0,0,0,409;1537;3637;2426,44;53;201;47,10;17;32;16,45;122;288;292,u;m
5284,ICLR,2020,Variational lower bounds on mutual information based on nonextensive statistical mechanics,Valeriu Balaban;Yang Zikun;Paul Bogdan,vbalaban@usc.edu;yangzikun@buaa.edu.cn;pbogdan@usc.edu,3;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,University of Southern California;Beihang University;University of Southern California,31;118;31,62;594;62,1;8,9/25/19,0,0,0,0,0,0,9;0;122,2;1;15,1;0;5,0;0;1,m;m
5285,ICLR,2020,Instance adaptive adversarial training: Improved accuracy tradeoffs in neural nets,Yogesh Balaji;Tom Goldstein;Judy Hoffman,yogesh@cs.umd.edu;tomg@cs.umd.edu;judy@gatech.edu,3;6;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,5,,yes,9/25/19,"University of Maryland, College Park;University of Maryland, College Park;Georgia Institute of Technology",12;12;13,91;91;38,4;8,9/25/19,12,0,0,0,0,0,517;260;9856,19;62;60,7;10;32,64;27;1159,m;f
5286,ICLR,2020,Robust Few-Shot Learning with Adversarially Queried Meta-Learners,Micah Goldblum;Liam Fowl;Tom Goldstein,goldblumcello@gmail.com;lhfowl@gmail.com;tomg@cs.umd.edu,3;6;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,4,,yes,9/25/19,"University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park",12;12;12,91;91;91,4;6,9/25/19,1,0,0,0,0,0,25;22;260,11;10;62,3;2;10,2;1;27,m;m
5287,ICLR,2020,Mixture Density Networks Find Viewpoint the Dominant Factor for Accurate Spatial Offset Regression,Ali Varamesh;Tinne Tuytelaars,ali.varamesh@kuleuven.be;tinne.tuytelaars@esat.kuleuven.be,3;1;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,KU Leuven;KU Leuven,118;118,45;45,2,9/25/19,0,0,0,0,0,0,25;34164,8;340,4;56,2;4070,m;f
5288,ICLR,2020,Learning Adversarial Grammars for Future Prediction,AJ Piergiovanni;Alexander Toshev;Anelia Angelova;Michael Ryoo,ajpiergi@indiana.edu;toshev@google.com;anelia@google.com;mryoo@google.com,1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,2,,yes,9/25/19,University of Arizona;Google;Google;Google,172;-1;-1;-1,103;-1;-1;-1,4,9/25/19,0,0,0,0,0,0,190;8790;265;3161,33;41;30;103,8;19;3;25,12;954;18;275,m;m
5289,ICLR,2020,Dual-Component Deep Domain Adaptation: A New Approach for Cross Project Software Vulnerability Detection,Van Nguyen;Trung Le;Olivier de Vel;Paul Montague;John C Grundy;Dinh Phung,van.nk@monash.edu;trunglm@monash.edu;olivier.devel@dst.defence.gov.au;paul.montague@dst.defence.gov.au;john.grundy@monash.edu;dinh.phung@monash.edu,1;6;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Monash University;Monash University;;;Monash University;Monash University,118;118;-1;-1;118;118,75;75;-1;-1;75;75,5;4;6,9/25/19,0,0,0,0,0,0,1283;75;1578;6;5013;5128,73;26;87;9;385;302,13;5;17;2;35;34,91;1;105;0;266;419,f;m
5290,ICLR,2020,FAN: Focused Attention Networks,Chu Wang;Babak Samari;Vladimir Kim;Siddhartha Chaudhuri;Kaleem Siddiqi,chuwang@cim.mcgill.ca;babak@cim.mcgill.ca;vokim@adobe.com;sidch@adobe.com;siddiqi@cim.mcgill.ca,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,8,,yes,9/25/19,McGill University;McGill University;Adobe Systems;Adobe Systems;McGill University,86;86;-1;-1;86,42;42;-1;-1;42,,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,0;0;0;0;0,m;m
5291,ICLR,2020,State2vec: Off-Policy Successor Feature Approximators,Sephora Madjiheurem;Laura Toni,sephora.madjiheurem.17@ucl.ac.uk;l.toni@ucl.ac.uk,1;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:N/A:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,University College London;University College London,50;50,15;15,10,9/25/19,1,0,0,0,0,0,7;497,4;75,1;12,0;25,f;f
5292,ICLR,2020,Semi-supervised Autoencoding Projective Dependency Parsing,Xiao Zhang;Dan Goldwasser,zhang923@purdue.edu;dgoldwas@purdue.edu,1;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Purdue University;Purdue University,27;27,88;88,5;10,9/25/19,0,0,0,0,0,0,203;1279,84;73,5;17,23;100,m;m
5293,ICLR,2020,Progressive Knowledge Distillation For Generative Modeling,Yu-Xiong Wang;Adrien Bardes;Ruslan Salakhutdinov;Martial Hebert,yuxiongw@cs.cmu.edu;adrien.bardes@dbmail.com;rsalakhu@cs.cmu.edu;hebert@ri.cmu.edu,3;3;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,0,,yes,9/25/19,Carnegie Mellon University;Ecole Normale Superieure;Carnegie Mellon University;Carnegie Mellon University,1;100;1;1,27;45;27;27,5;6,9/25/19,0,0,0,0,0,0,1448;0;72367;2054,34;1;256;47,16;0;83;13,120;0;8008;173,m;m
5294,ICLR,2020,Lyceum: An efficient and scalable ecosystem for robot learning,Colin X. Summers;Kendall Lowrey;Aravind Rajeswaran;Emanuel Todorov;Siddhartha Srinivasa,colinxs@cs.washington.edu;klowrey@cs.washington.edu;todorov@cs.washington.edu;siddh@cs.washington.edu;aravraj@cs.washington.edu,1;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,1,,yes,9/25/19,University of Washington;University of Washington;University of Washington;University of Washington;University of Washington,6;6;6;6;6,26;26;26;26;26,,9/25/19,1,0,0,0,0,0,1879;501;920;5617;94,24;10;24;33;17,10;7;13;14;2,113;34;86;438;7,m;m
5295,ICLR,2020,Adversarial Attribute Learning by  Exploiting negative correlated attributes,Satoshi Tsutsui;Yanwei Fu;David Crandall,stsutsui@indiana.edu;yanweifu@fudan.edu.cn;djcran@indiana.edu,1;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,University of Arizona;Fudan University;University of Arizona,172;79;172,103;109;103,4,9/25/19,0,0,0,0,0,0,107;2119;5948,38;92;186,6;23;32,10;268;457,m;m
5296,ICLR,2020,VISUALIZING POINT CLOUD CLASSIFIERS BY MORPHING POINT CLOUDS INTO POTATOES,Ziwen Chen;Wenxuan Wu;Zhongang Qi;Fuxin Li,chenziwe@grinnell.edu;wuwen@oregonstate.edu;qiz@oregonstate.edu;lif@oregonstate.edu,3;3;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,4,,yes,9/25/19,Grinnell College;Oregon State University;Oregon State University;Oregon State University,-1;77;77;77,-1;373;373;373,,9/25/19,0,0,0,0,0,0,6;3;223;2587,8;7;18;82,2;1;7;25,1;1;31;323,f;m
5297,ICLR,2020,Learning Multi-Agent Communication Through Structured Attentive Reasoning,Murtaza Rangwala;Ryan Williams,murtazar@vt.edu;rywilli1@vt.edu,3;1;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,Virginia Tech;Virginia Tech,79;79,240;240,,9/25/19,0,0,0,0,0,0,-1;5056,-1;206,-1;33,-1;530,m;m
5298,ICLR,2020,The Power of  Semantic Similarity based Soft-Labeling for Generalized Zero-Shot Learning,Shabnam Daghaghi;Tharun Medini;Anshumali Shrivastava,shabnam.daghaghi@rice.edu;tharun.medini@rice.edu;anshumali@rice.edu,6;1;3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,5,,yes,9/25/19,Rice University;Rice University;Rice University,84;84;84,105;105;105,6,9/25/19,0,0,0,0,0,0,0;6;1205,3;13;102,0;2;16,0;0;116,f;m
5299,ICLR,2020,Spline Templated Based Handwriting Generation,Daniel Clothiaux;Ravi Starzl,dclothia@andrew.cmu.edu;rstarzl@cs.cmu.edu,6;3;1,I do not know much about this area.:I carefully checked the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University,1;1,27;27,,9/25/19,0,0,0,0,0,0,297;73,4;17,2;3,26;0,m;m
5300,ICLR,2020,Improving the robustness of ImageNet classifiers using elements of human visual cognition,Emin Orhan;Brenden Lake,aeminorhan@gmail.com;brenden@nyu.edu,3;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,New York University;New York University,25;25,29;29,4,6/20/19,2,0,0,0,0,0,283;3160,20;47,9;16,15;307,m;m
5301,ICLR,2020,Going Deeper with Lean Point Networks,Eric-Tuan Le;Iasonas Kokkinos;Niloy J. Mitra,eric-tuan.le.18@ucl.ac.uk;i.kokkinos@cs.ucl.ac.uk;n.mitra@cs.ucl.ac.uk,1;6;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,University College London;University College London;University College London,50;50;50,15;15;15,2,7/1/19,4,0,0,0,0,0,4;12313;12190,3;130;249,1;37;56,1;1694;921,m;m
5302,ICLR,2020,Newton Residual Learning,Grigorios Chrysos;Jiankang Deng;Yannis Panagakis;Stefanos Zafeiriou,g.chrysos@imperial.ac.uk;j.deng16@imperial.ac.uk;i.panagakis@imperial.ac.uk;s.zafeiriou@imperial.ac.uk,8;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,1,1,,yes,9/25/19,Imperial College London;Imperial College London;Imperial College London;Imperial College London,73;73;73;73,10;10;10;10,2,9/25/19,0,0,0,0,0,0,163;1445;1153;1197,18;42;89;68,5;16;18;13,13;306;69;82,m;m
5303,ICLR,2020,Training-Free Uncertainty Estimation for Neural Networks,Lu Mi;Hao Wang;Yonglong Tian;Nir Shavit,lumi@mit.edu;hoguewang@gmail.com;yonglong@mit.edu;shanir@csail.mit.edu,1;6;6;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,5,,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2;2;2,5;5;5;5,11;2,9/25/19,2,0,0,0,0,0,56;856;1788;10312,63;89;22;223,4;8;13;51,2;45;190;1276,m;m
5304,ICLR,2020,AMUSED: A Multi-Stream Vector Representation Method for Use In Natural Dialogue,Gaurav Kumar;Rishabh Joshi;Jaspreet Singh;Promod Yenigalla,gaurav.k1@samsung.com;rjoshi2@andrew.cmu.edu;jaspreet.ahluwalia@stonybrook.edu;promod.y@samsung.com,6;1;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A,Withdrawn,0,3,,yes,9/25/19,"Samsung;Carnegie Mellon University;State University of New York, Stony Brook;Samsung",-1;1;41;-1,-1;27;304;-1,6;10,9/25/19,0,0,0,0,0,0,0;55;253;60,1;8;54;9,0;1;7;3,0;7;14;6,m;m
5305,ICLR,2020,PAD-Nets: Learning Dynamic Receptive Fields via Pixel-Wise Adaptive Dilation,Dongdong Wang;Hao Hu;Jie Yao;Zihang Zou;Liqiang Wang,daniel.wang@knights.ucf.edu;hhu@fxpal.com;17112098@bjtu.edu.cn;zzz@knights.ucf.edu;lwang@cs.ucf.edu,1;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,"University of Central Florida;FX Palo Alto Laboratory, Inc.;Beijing jiaotong univercity;University of Central Florida;University of Central Florida",77;-1;481;77;77,609;-1;952;609;609,,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,0;0;0;0;0,m;m
5306,ICLR,2020,Posterior Sampling: Make Reinforcement Learning Sample Efficient Again,Calvin Seward;Urs Bergmann;Roland Vollgraf;Sepp Hochreiter,seward@bioinf.jku.at;urs.bergmann@zalando.de;roland.vollgraf@zalando.de;hochreit@bioinf.jku.at,3;3;6;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,4,,yes,9/25/19,Johannes Kepler University Linz;Zalando Research;Zalando Research;Johannes Kepler University Linz,481;-1;-1;481,620;-1;-1;620,11,9/25/19,0,0,0,0,0,0,15;246;2409;37413,7;21;35;112,2;6;12;29,0;33;617;6816,m;m
5307,ICLR,2020,Unifying Part Detection And Association For Multi-person Pose Estimation,Rania Briq;Andreas Doering;Juergen Gall,briq@iai.uni-bonn.de;doering@iai.uni-bonn.de;gall@iai.uni-bonn.de,3;6;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,0,,yes,9/25/19,University of Bonn;University of Bonn;University of Bonn,128;128;128,106;106;106,2,9/25/19,0,0,0,0,0,0,8;566;9655,4;34;177,1;14;54,0;49;985,f;m
5308,ICLR,2020,Slow Thinking Enables Task-Uncertain Lifelong and Sequential Few-Shot Learning,Rosalie Dolor;Hsin-Chi Chu;Shan-Hung Wu,rosalie@ghtinc.com;hcchu@datalab.cs.nthu.edu.tw;shwu@cs.nthu.edu.tw,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Ghtinc;National Tsing Hua University;National Tsing Hua University,-1;172;172,-1;365;365,6,9/25/19,0,0,0,0,0,0,0;0;349,1;1;33,0;0;12,0;0;44,u;m
5309,ICLR,2020,How Aggressive Can Adversarial Attacks Be: Learning Ordered Top-k Attacks,Zekun Zhang;Tianfu Wu,zzhang56@ncsu.edu;tianfu_wu@ncsu.edu,3;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,North Carolina State University;North Carolina State University,86;86,310;310,4,9/25/19,0,0,0,0,0,0,301;1233,25;90,10;18,17;59,m;m
5310,ICLR,2020,PolyGAN: High-Order Polynomial Generators,Grigorios Chrysos;Stylianos Moschoglou;Yannis Panagakis;Stefanos Zafeiriou,g.chrysos@imperial.ac.uk;s.moschoglou@imperial.ac.uk;i.panagakis@imperial.ac.uk;s.zafeiriou@imperial.ac.uk,1;3;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,Imperial College London;Imperial College London;Imperial College London;Imperial College London,73;73;73;73,10;10;10;10,5;4,8/19/19,6,0,0,0,0,0,278;133;1153;1197,39;14;89;68,7;4;18;13,30;25;69;82,m;m
5311,ICLR,2020,Generative Multi Source Domain Adaptation,Subhankar Roy;Aliaksandr Siarohin;Enver Sangineto;Moin Nabi;Tassilo Klein;Nicu Sebe;Elisa Ricci,subhankar.roy@unitn.it;aliaksandr.siarohin@unitn.it;enver.sangineto@unitn.it;m.nabi@sap.com;tassilo.klein@sap.com;niculae.sebe@unitn.it;eliricci@fbk.eu,3;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,5,,yes,9/25/19,University of Trento;University of Trento;University of Trento;SAP;SAP;University of Trento;Fondazione Bruno Kessler,18;18;18;323;323;18;-1,307;307;307;258;258;307;-1,5;4,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,0;0;0;0;0;0;0,m;f
5312,ICLR,2020,Semi-Supervised Named Entity Recognition with CRF-VAEs,Thomas Effland;Michael Collins,teffland@cs.columbia.edu;mcollins@cs.columbia.edu;mc3354@columbia.edu,3;3;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,1,3,,yes,9/25/19,Columbia University;Columbia University;Columbia University,15;15;15,16;16;16,5,9/25/19,0,0,0,0,0,0,18;1535,6;63,2;9,0;137,m;m
5313,ICLR,2020,Understanding the (Un)interpretability of Natural Image Distributions Using Generative Models,Ryen Krusinga;Sohil Shah;Matthias Zwicker;Tom Goldstein;David Jacobs,krusinga@cs.umd.edu;sohilas@umd.edu;zwicker@inf.unibe.ch;tom@cs.umd.edu;djacobs@umiacs.umd.edu,1;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,1,0,,yes,9/25/19,"University of Maryland, College Park;University of Maryland, College Park;University of Bern;University of Maryland, College Park;University of Maryland, College Park",12;12;390;12;12,91;91;113;91;91,5,1/6/19,5,0,0,0,0,0,4;134;6035;6632;13044,1;19;164;98;216,1;6;42;29;52,0;13;413;741;1625,m;m
5314,ICLR,2020,Depth-Recurrent Residual Connections for Super-Resolution of Real-Time Renderings,Erik Franz;Mengyu Chu;Rüdiger Westermann;Nils Thuerey,franzer@in.tum.de;mengyu.chu@tum.de;westermann@tum.de;nils.thuerey@tum.de,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Technical University Munich;Technical University Munich;Technical University Munich;Technical University Munich,53;53;53;53,43;43;43;43,5;4,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1;-1,-1;-1;-1;-1,-1;-1;-1;-1,0;0;0;0,m;m
5315,ICLR,2020,The Blessing of Dimensionality: An Empirical Study of Generalization,W. Ronny Huang;Zeyad Emam;Micah Goldblum;Liam Fowl;Justin K. Terry;Furong Huang;Tom Goldstein,wrhuang@umd.edu;zeyad@math.umd.edu;goldblum@math.umd.edu;lfowl@math.umd.edu;jkterry@cs.umd.edu;furongh@cs.umd.edu;tomg@cs.umd.edu,3;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,"University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park",12;12;12;12;12;12;12,91;91;91;91;91;91;91,8,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,0;0;0;0;0;0;0,m;m
5316,ICLR,2020,Strong Baseline Defenses Against Clean-Label Poisoning Attacks,Neal Gupta;W. Ronny Huang;Liam Fowl;Chen Zhu;Soheil Feizi;Tom Goldstein;John Dickerson,ngupta@cs.umd.edu;wronnyhuang@gmail.com;lhfowl@gmail.com;chenzhu@cs.umd.edu;sfeizi@cs.umd.edu;tomg@cs.umd.edu;john@cs.umd.edu,3;3;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,"University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park",12;12;12;12;12;12;12,91;91;91;91;91;91;91,4,9/25/19,1,0,0,0,0,0,119;344;22;966;3697;260;1228,11;22;10;42;13;62;117,4;7;2;11;6;10;18,11;28;1;65;242;27;67,m;m
5317,ICLR,2020,MetaPoison:   Learning to craft adversarial poisoning examples via meta-learning,W. Ronny Huang;Jonas Geiping;Liam Fowl;Gavin Taylor;Tom Goldstein,wronnyhuang@gmail.com;jonas.geiping@uni-siegen.de;lfowl@math.umd.edu;taylor@usna.edu;tomg@cs.umd.edu,3;3;1,I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,"University of Maryland, College Park;University of Siegen;University of Maryland, College Park;University of Arizona;University of Maryland, College Park",12;323;12;172;12,91;570;91;103;91,4;6,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,0;0;0;0;0,m;m
5318,ICLR,2020,Domain-Relevant Embeddings for Question Similarity,Clara McCreery;Namit Katariya;Anitha Kannan;Manish Chablani;Xavier Amatriain,mccreery@stanford.edu;namit@curai.com;anitha@curai.com;manish@curai.com;xavier@curai.com,3;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,0,,yes,9/25/19,Stanford University;Curai;Curai;Curai;Curai,4;233;233;233;233,4;-1;-1;-1;-1,,9/25/19,0,0,0,0,0,0,45;14;439;3;3073,4;5;14;7;81,1;1;6;1;24,0;2;19;0;195,f;m
5319,ICLR,2020,Divide-and-Conquer Adversarial Learning for High-Resolution Image Enhancement,Zhiwu Huang;Danda Pani Paudel;Guanju Li;Jiqing Wu;Radu Timofte;Luc Van Gool,zhiwu.huang@vision.ee.ethz.ch;paudel@vision.ee.ethz.ch;ligua@student.ethz.ch;jwu@vision.ee.ethz.ch;radu.timofte@vision.ee.ethz.ch;vangool@vision.ee.ethz.ch,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:N/A:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10;10;10;10,13;13;13;13;13;13,5;4,9/25/19,0,0,0,0,0,0,2152;207;4;340;8118;91677,242;43;4;23;181;1271,25;8;1;8;37;127,238;21;0;38;1032;10396,m;m
5320,ICLR,2020,Learning Out-of-distribution Detection without Out-of-distribution Data,Yen-Chang Hsu;Yilin Shen;Hongxia Jin;Zsolt Kira,yenchang.hsu@gatech.edu;yilin.shen@samsung.com;hongxia.jin@samsung.com;zkira@gatech.edu,3;3;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,3,1,,yes,9/25/19,Georgia Institute of Technology;Samsung;Samsung;Georgia Institute of Technology,13;-1;-1;13,38;-1;-1;38,,9/25/19,0,0,0,0,0,0,201;1129;1709;1233,14;100;179;70,9;19;23;16,39;77;119;179,m;m
5321,ICLR,2020,Domain Adaptation Through Label Propagation: Learning Clustered and Aligned Features,Changhwa Park;Jaeyoon Yoo;Youngjun Hong;Sungroh Yoon,omega6464@snu.ac.kr;yjy765@snu.ac.kr;youngjun.hong@enerzai.com;sryoon@snu.ac.kr,3;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,1,10,,yes,9/25/19,Seoul National University;Seoul National University;Enerzai;Seoul National University,41;41;-1;41,64;64;-1;64,1,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1;-1,-1;-1;-1;-1,-1;-1;-1;-1,0;0;0;0,m;m
5322,ICLR,2020,ManiGAN: Text-Guided Image Manipulation,Bowen Li;Xiaojuan Qi;Thomas Lukasiewicz;Philip H. S. Torr,bowen.li@cs.ox.ac.uk;xiaojuan.qi@eng.ox.ac.uk;thomas.lukasiewicz@cs.ox.ac.uk;philip.torr@eng.ox.ac.uk,1;6;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,University of Oxford;University of Oxford;University of Oxford;University of Oxford,50;50;50;50,1;1;1;1,3;4;5,9/25/19,3,0,0,0,0,0,57;4681;5504;1943,38;52;296;30,3;19;35;10,3;776;446;441,m;m
5323,ICLR,2020,Empirical observations pertaining to learned priors for deep latent variable models,Rogan Morrow;Wei-Chen Chiu,rogan.o.morrow@gmail.com;walon@cs.nctu.edu.tw,3;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,4,,yes,9/25/19,National Chiao Tung University;National Chiao Tung University,143;143,564;564,5;4;2,9/25/19,0,0,0,0,0,0,3;262,3;29,1;7,0;33,m;m
5324,ICLR,2020,Variational inference of latent hierarchical dynamical systems in neuroscience: an application to calcium imaging data,Luke Y. Prince;Blake A. Richards,luke.prince@utoronto.ca;blake.richards@mcgill.ca,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Toronto University;McGill University,18;86,18;42,5,9/25/19,0,0,0,0,0,0,31;1856,7;56,3;18,1;110,m;m
5325,ICLR,2020,Characterizing convolutional neural networks with one-pixel signature,Shanjiaoyang Huang;Weiqi Peng;Zhuowen Tu,shh236@ucsd.edu;wep012@ucsd.edu;ztu@ucsd.edu,3;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,"University of California, San Diego;University of California, San Diego;University of California, San Diego",11;11;11,31;31;31,4,9/25/19,0,0,0,0,0,0,0;189;205,1;25;11,0;7;2,0;0;25,m;m
5326,ICLR,2020,Increasing batch size through instance repetition improves generalization,Elad Hoffer;Tal Ben-Nun;Itay Hubara;Niv Giladi;Torsten Hoefler;Daniel Soudry,elad.hoffer@gmail.com;talbn@inf.ethz.ch;itayhubara@gmail.com;giladiniv@campus.technion.ac.il;htor@inf.ethz.ch;daniel.soudry@gmail.com,3;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,2,3,,yes,9/25/19,Technion;Swiss Federal Institute of Technology;;Technion;Swiss Federal Institute of Technology;Technion,26;10;-1;26;10;26,412;13;-1;412;13;412,8,9/25/19,0,0,0,0,0,0,1526;124;3161;23;7030;5173,27;12;21;5;339;77,12;6;11;2;40;26,178;0;451;2;576;656,m;m
5327,ICLR,2020,"Mix & Match: training convnets with mixed image sizes for improved accuracy, speed and scale resiliency",Elad Hoffer;Berry Weinstein;Itay Hubara;Tal Ben-Nun;Torsten Hoefler;Daniel Soudry,elad.hoffer@gmail.com;bweinstein@habana.ai;itayhubara@gmail.com;talbn@inf.ethz.ch;htor@inf.ethz.ch;daniel.soudry@gmail.com,6;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,3,,yes,9/25/19,Technion;interdisciplinary center herzliya;;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Technion,26;-1;-1;10;10;26,412;-1;-1;13;13;412,,8/12/19,2,1,1,0,0,0,1533;1;3182;484;7098;5180,27;3;21;31;340;77,12;1;11;10;40;26,178;0;453;50;574;657,m;m
5328,ICLR,2020,Revisit Knowledge Distillation: a Teacher-free Framework,Li Yuan;Francis EH Tay;Guilin Li;Tao Wang;Jiashi Feng,ylustcnus@gmail.com;mpetayeh@nus.edu.sg;liguilin2@huawei.com;twangnh@gmail.com;elefjia@nus.edu.sg,6;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,5,,yes,9/25/19,National University of Singapore;National University of Singapore;Huawei Technologies Ltd.;National University of Singapore;National University of Singapore,16;16;-1;16;16,25;25;-1;25;25,1,9/25/19,12,0,0,0,0,0,-1;12;122;27;9932,-1;7;65;16;333,-1;2;6;2;52,0;3;9;1;1286,m;m
5329,ICLR,2020,On learning visual odometry errors,Andrea De Maio;Simon Lacroix,andrea.de-maio@laas.fr;simon.lacroix@laas.fr,1;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,4,,yes,9/25/19,LAAS / CNRS;LAAS / CNRS,-1;-1,-1;-1,,9/25/19,0,0,0,0,0,0,1273;130,88;25,23;5,22;5,f;m
5330,ICLR,2020,Mitigating Posterior Collapse in Strongly Conditioned Variational Autoencoders,Mohammad Sadegh Aliakbarian;Fatemeh Sadat Saleh;Mathieu Salzmann;Lars Petersson;Stephen Gould,sadegh.aliakbarian@anu.edu.au;fatemehsadat.saleh@anu.edu.au;mathieu.salzmann@epfl.ch;lars.petersson@data61.csiro.au;stephen.gould@anu.edu.au,3;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,4,,yes,9/25/19,"Australian National University;Australian National University;Swiss Federal Institute of Technology Lausanne;, CSIRO;Australian National University",108;108;481;233;108,50;50;38;-1;50,5,9/25/19,0,0,0,0,0,0,81;101;5938;2420;9812,15;16;202;131;434,4;4;44;29;39,6;6;629;139;677,m;m
5331,ICLR,2020,Unsupervised  Video-to-Video Translation via Self-Supervised Learning,Kangning Liu;Shuhang Gu;Radu Timofte,kl3141@nyu.edu;shuhang.gu@vision.ee.ethz.ch;radu.timofte@vision.ee.ethz.ch,3;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,4,,yes,9/25/19,New York University;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,25;10;10,29;13;13,,9/25/19,0,0,0,0,0,0,23;3166;8118,7;59;181,3;19;37,0;562;1032,m;m
5332,ICLR,2020,ROBUST SINGLE-STEP ADVERSARIAL TRAINING,B.S. Vivek;R. Venkatesh Babu,svivek@iisc.ac.in;venky@iisc.ac.in,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,3,,yes,9/25/19,Indian Institute of Science;Indian Institute of Science,95;95,301;301,4;2,9/25/19,0,0,0,0,0,0,105;4255,11;228,4;34,14;401,m;m
5333,ICLR,2020,Attentive Weights Generation for Few Shot Learning via Information Maximization,Yiluan Guo;Ngai-Man Cheung,guoyl1990@outlook.com;ngaiman_cheung@sutd.edu.sg,1;6;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,5,,yes,9/25/19,Singapore University of Technology and Design;Singapore University of Technology and Design,481;481,1397;1397,1;6,9/25/19,1,0,0,0,0,0,71;2986,6;187,3;30,3;181,m;m
5334,ICLR,2020,DropGrad: Gradient Dropout Regularization for Meta-Learning,Hung-Yu Tseng;Yi-Wen Chen;Yi-Hsuan Tsai;Sifei Liu;Yen-Yu Lin;Ming-Hsuan Yang,htseng6@ucmerced.edu;ychen319@ucmerced.edu;wasidennis@gmail.com;sifeil@nvidia.com;lin@cs.nctu.edu.tw;mhyang@ucmerced.edu,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,0,,yes,9/25/19,University of California at Merced;University of California at Merced;NEC-Labs;NVIDIA;National Chiao Tung University;University of California at Merced,481;481;-1;-1;143;481,354;354;-1;-1;564;354,6;8,9/25/19,0,0,0,0,0,0,568;7;1224;1449;184;5994,25;12;60;43;37;132,10;2;13;15;8;27,105;0;242;234;18;1287,m;m
5335,ICLR,2020,Hierarchical Image-to-image Translation with Nested Distributions Modeling,Shishi Qiao;Ruiping Wang;Shiguang Shan;Xilin Chen,qiaoshishi14@mails.ucas.ac.cn;wangruiping@ict.ac.cn;sgshan@ict.ac.cn;xlchen@ict.ac.cn,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,"Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences",59;59;59;59,1397;1397;1397;1397,,9/25/19,0,0,0,0,0,0,6;3805;489;17134,5;111;61;515,2;27;7;61,0;548;78;1819,m;m
5336,ICLR,2020,Deep Neural Forests: An Architecture for Tabular Data,Ami Abutbul;Gal Elidan;Liran Katzir;Ran El-Yaniv,amramabutbul@cs.technion.ac.il;elidan@google.com;lirank@google.com;elyaniv@google.com,3;3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,4,,yes,9/25/19,Technion;Google;Google;Google,26;-1;-1;-1,412;-1;-1;-1,,9/25/19,0,0,0,0,0,0,0;2534;851;111,3;73;33;10,0;21;12;4,0;223;113;16,m;m
5337,ICLR,2020,Is my Deep Learning Model Learning more than I want it to?,Naveen Panwar;Tarun Tater;Anush Sankaran;Senthil Mani,naveen.panwar@in.ibm.com;anussank@in.ibm.com;taruntater3@gmail.com;sentmani@in.ibm.com,3;1;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,International Business Machines;International Business Machines;International Business Machines;International Business Machines,-1;-1;-1;-1,-1;-1;-1;-1,8,9/25/19,0,0,0,0,0,0,33;7;435;3,16;5;46;17,3;2;12;1,1;1;33;1,m;m
5338,ICLR,2020,Mixing Up Real Samples and Adversarial Samples for Semi-Supervised Learning,Yun Ma;Xudong Mao;Yangbin Chen;Qing Li,mayun371@gmail.com;xudong.xdmao@gmail.com;robinchen2-c@my.cityu.edu.hk;qing-prof.li@polyu.edu.hk,1;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:N/A:N/A,Withdrawn,0,1,,yes,9/25/19,The Hong Kong Polytechnic University;The Hong Kong Polytechnic University;City University of Hong Kong;The Hong Kong Polytechnic University,172;172;92;172,171;171;35;171,4,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1;-1,-1;-1;-1;-1,-1;-1;-1;-1,0;0;0;0,m;m
5339,ICLR,2020,ADASAMPLE: ADAPTIVE SAMPLING OF HARD POSITIVES FOR DESCRIPTOR LEARNING,Xin-Yu Zhang;Jia-Wang Bian;Le Zhang;Zao-Yi Zheng;Yun Liu;Ming-Ming Cheng;Ian Reid,xinyuzhang@mail.nankai.edu.cn;jiawang.bian@gmail.com;zhangleuestc@gmail.com;roymarssss@gmail.com;nk12csly@mail.nankai.edu.cn;cmm@nankai.edu.cn;ian.reid@adelaide.edu.au,3;6;3,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,5,,yes,9/25/19,Nankai University;The University of Adelaide;A*STAR;;Nankai University;Nankai University;The University of Adelaide,481;128;-1;-1;481;481;128,366;120;-1;-1;366;366;120,,9/25/19,1,0,0,0,0,0,790;362;200;0;153;11446;-1,105;12;42;1;49;158;-1,13;4;5;0;7;42;-1,70;58;7;0;15;1605;0,m;m
5340,ICLR,2020,Better Optimization for Neural Architecture Search with Mixed-Level Reformulation,Chaoyang He;Haishan Ye;Tong Zhang,chaoyang.he@usc.edu;yhs12354123@163.com;tongzhang@tongzhang-ml.org,3;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,University of Southern California;163;The Hong Kong University of Science and Technology,31;-1;39,62;-1;47,,9/25/19,0,0,0,0,0,0,258;58;206,26;21;81,7;5;6,22;6;22,m;m
5341,ICLR,2020,Amharic Light Stemmer,Girma Neshir;Andeas Rauber;and Solomon Atnafu,girma1978@gmail.com;rauber@ifs.tuwien.ac.at;solomon.atnafu@aau.edu.et,1;1;1,I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Withdrawn,0,0,,yes,9/25/19,Addis Ababa University;TU Wien Vienna University of Technology;Addis Ababa University,481;100;481,1397;360;1397,,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1,-1;-1;-1,-1;-1;-1,0;0;0,m;m
5342,ICLR,2020,Neural Reverse Engineering of Stripped Binaries,Yaniv David;Uri Alon;Eran Yahav,yanivd@cs.technion.ac.il;urialon@cs.technion.ac.il;yahave@cs.technion.ac.il,6;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:N/A:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,7,,yes,9/25/19,Technion;Technion;Technion,26;26;26,412;412;412,10,2/25/19,8,0,0,0,0,0,189;361;4324,10;10;144,5;5;40,31;56;368,m;m
5343,ICLR,2020,Cost-Effective Interactive Neural Attention Learning,Jay Heo;Junhyeon Park;Hyewon Jeong;Wuhyun Shin;Kwang Joon Kim;juho Lee;Eunho Yang;Sung Ju Hwang,jayheo@kaist.ac.kr;pjh2941@kaist.ac.kr;jhw162@kaist.ac.kr;wuhyun.shin@kaist.ac.kr;preppie@yuhs.ac.kr;juho@aitrics.com;eunhoy@kaist.ac.kr;sjhwang82@kaist.ac.kr,3;3;3;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,4,,yes,9/25/19,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Kyung Hee;AITRICS;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,481;481;481;481;481;-1;481;481,110;110;110;110;319;-1;110;110,,9/25/19,0,0,0,0,0,0,11;0;161;-1;4;529;1137;1185,3;4;28;-1;9;54;81;75,1;0;6;-1;1;5;16;17,1;0;9;0;0;44;171;124,m;m
5344,ICLR,2020,Gated Channel Transformation for Visual Recognition,Zongxin Yang;Linchao Zhu;Yu Wu;Yi Yang,zongxin.yang@student.uts.edu.au;zhulinchao7@gmail.com;yu.wu-3@student.uts.edu.au;yi.yang@uts.edu.au,3;1;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,University of Technology Sydney;University of Technology Sydney;University of Technology Sydney;University of Technology Sydney,108;108;108;108,193;193;193;193,2,9/25/19,1,0,0,0,0,0,-1;409;18;-1,-1;44;7;-1,-1;10;2;-1,0;45;2;0,m;m
5345,ICLR,2020,Masked Translation Model,Arne Nix;Yunsu Kim;Jan Rosendahl;Shahram Khadivi;Hermann Ney,nix@i6.informatik.rwth-aachen.de;kim@i6.informatik.rwth-aachen.de;rosendahl@i6.informatik.rwth-aachen.de;skhadivi@ebay.com;ney@i6.informatik.rwth-aachen.de,3;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,1,1,,yes,9/25/19,RWTH Aachen University;RWTH Aachen University;RWTH Aachen University;eBay;RWTH Aachen University,95;95;95;-1;95,98;98;98;-1;98,3,9/25/19,0,0,0,0,0,0,20;109;28;826;39271,4;33;9;89;942,3;6;3;15;86,4;11;3;63;2900,m;m
5346,ICLR,2020,Target-directed Atomic Importance Estimation via Reverse Self-attention,Gyoung S. Na;Hyun Woo Kim,ngs0726@gmail.com;ahwk@krict.re.kr,1;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,POSTECH;,118;-1,146;-1,10,9/25/19,0,0,0,0,0,0,16;164,5;36,2;8,1;3,m;m
5347,ICLR,2020,ProxNet: End-to-End Learning of  Structured Representation by Proximal Mapping,Mao Li;Yingyi Ma;Xinhua Zhang,mli206@uic.edu;yma36@uic.edu;zhangx@uic.edu,3;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,2,,yes,9/25/19,"University of Illinois, Chicago;University of Illinois, Chicago;University of Illinois, Chicago",56;56;56,254;254;254,4,9/25/19,0,0,0,0,0,0,43838;2;32,3566;7;30,89;1;3,1528;0;0,f;m
5348,ICLR,2020,Measure by Measure: Automatic Music Composition with Traditional Western Music Notation,Yujia Yan;Zhiyao Duan,yujia.yan.w@gmail.com;zhiyao.duan@rochester.edu,1;1;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,1,,yes,9/25/19,University of Rochester;University of Rochester,100;100,173;173,,9/25/19,0,0,0,0,0,0,9;40,5;18,1;4,1;0,u;m
5349,ICLR,2020,When Do Variational Autoencoders Know  What They Don't Know?,Bin Dai;David Wipf,daib13@mails.tsinghua.edu.cn;davidwipf@gmail.com,8;6;8,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,Tsinghua University;Microsoft,8;-1,23;-1,5,9/25/19,0,0,0,0,0,0,248;12,43;5,9;2,1;2,m;m
5350,ICLR,2020,Irrationality can help reward inference,Lawrence Chan;Andrew Critch;Anca Dragan,chanlaw@berkeley.edu;critch@berkeley.edu;anca@berkeley.edu,3;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,University of California Berkeley;University of California Berkeley;University of California Berkeley,5;5;5,13;13;13,,9/25/19,0,0,0,0,0,0,1913;149;3854,44;18;126,21;6;32,97;8;291,m;f
5351,ICLR,2020,Teaching GAN to generate per-pixel annotation,Danil Galeev;Konstantin Sofiyuk;Danila Rukhovich;Anton Konushin;Mikhail Romanov,denemmy@gmail.com;ksofiyuk@gmail.com;danrukh@gmail.com;a.konushin@samsung.com;m.romanov@samsung.com,3;1;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Samsung;Samsung;Lomonosov Moscow State University;Samsung;Samsung,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,5;2,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,0;0;0;0;0,m;m
5352,ICLR,2020,One Generation Knowledge Distillation by Utilizing Peer Samples,Xingjian Li;Haozhe An;Haoyi Xiong;Jun Huan;Dejing Dou;Chengzhong Xu,1762778193@qq.com;haozhe.an@yahoo.com,1;3;3,I have published one or two papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,;Baidu,-1;-1,-1;-1,,9/25/19,0,0,0,0,0,0,78;1;1607;3200;2546;232,15;6;95;195;158;51,6;1;21;24;25;8,0;0;85;249;178;14,m;m
5353,ICLR,2020,Guided variational autoencoder for disentanglement learning,Zheng Ding;Yifan Xu;Weijian Xu;Yang Yang;Max Welling;Zhuowen Tu,dingz16@mails.tsinghua.edu.cn;yix081@ucsd.edu;wex041@eng.ucsd.edu;yyangy@qti.qualcomm.com;welling.max@gmail.com;ztu@ucsd.edu,1;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,"Tsinghua University;University of California, San Diego;University of California, San Diego;Qualcomm Inc, QualComm;University of California - Irvine;University of California, San Diego",8;11;11;-1;35;11,23;31;31;-1;96;31,5;4,9/25/19,1,0,0,0,0,0,242;1301;33;2925;790;205,29;139;12;190;60;11,9;20;4;11;14;2,19;36;2;138;114;25,m;m
5354,ICLR,2020,Learning Sparsity and Quantization Jointly and Automatically for Neural Network Compression via Constrained Optimization,Haichuan Yang;Shupeng Gui;Yuhao Zhu;Ji Liu,h.yang@rochester.edu;sgui2@ur.rochester.edu;yzhu@rochester.edu;ji.liu.uwisc@gmail.com,3;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,University of Rochester;University of Rochester;University of Rochester;University of Rochester,100;100;100;100,173;173;173;173,,9/25/19,3,0,0,0,0,0,184;25;780;256,32;14;59;70,8;3;15;7,5;0;59;29,m;m
5355,ICLR,2020,FairFace: A Novel Face Attribute Dataset for Bias Measurement and Mitigation,Kimmo Kärkkäinen;Jungseock Joo,kimmo@cs.ucla.edu;jjoo@comm.ucla.edu,3;1;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,3,,yes,9/25/19,"University of California, Los Angeles;University of California, Los Angeles",20;20,17;17,7;2;8,9/25/19,0,0,0,0,0,0,27;302,10;28,3;10,4;20,m;m
5356,ICLR,2020,Pruning Depthwise Separable Convolutions for Extra Efficiency Gain of Lightweight Models,Cheng-Hao Tu;Jia-Hong Lee;Yi-Ming Chan;Chu-Song Chen,andytu28@iis.sinica.edu.tw;honghenry.lee@iis.sinica.edu.tw;yiming@iis.sinica.edu.tw;song@iis.sinica.edu.tw,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,1,0,,yes,9/25/19,Academia Sinica;Academia Sinica;Academia Sinica;Academia Sinica,-1;-1;-1;-1,-1;-1;-1;-1,,9/25/19,0,0,0,0,0,0,3;300;223;4885,7;20;28;219,1;4;7;34,0;33;6;535,m;m
5357,ICLR,2020,Benchmarking Adversarial Robustness,Yinpeng Dong;Qi-An Fu;Xiao Yang;Tianyu Pang;Hang Su;Jun Zhu,dyp17@mails.tsinghua.edu.cn;fqa19@mails.tsinghua.edu.cn;yangxiao19@mails.tsinghua.edu.cn;pty17@mails.tsinghua.edu.cn;suhangss@mail.tsinghua.edu.cn;dcszj@mail.tsinghua.edu.cn,1;3;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,0,,yes,9/25/19,Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University,8;8;8;8;8;8,23;23;23;23;23;23,4,9/25/19,3,0,0,0,0,0,1430;4;42;-1;886,28;11;19;-1;96,15;1;3;-1;17,212;0;6;-1;16,m;m
5358,ICLR,2020,Unsupervised Learning from Video with Deep Neural Embeddings,Chengxu Zhuang;Tianwei She;Alex Andonian;Daniel Yamins,chengxuz@stanford.edu;shetw@stanford.edu;aandonia@mit.edu;yamins@stanford.edu,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A,Withdrawn,0,0,,yes,9/25/19,Stanford University;Stanford University;Massachusetts Institute of Technology;Stanford University,4;4;2;4,4;4;5;4,,5/28/19,4,0,0,0,0,0,156;-1;422;658,11;-1;10;45,4;-1;5;9,11;0;83;37,m;m
5359,ICLR,2020,Towards Unifying Neural Architecture Space Exploration and Generalization,Kartikeya Bhardwaj;Radu Marculescu,bhardwajkartikeya@gmail.com;radum@cmu.edu,3;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,arm;Carnegie Mellon University,-1;1,-1;27,8,9/25/19,0,0,0,0,0,0,140;9746,26;302,5;50,8;621,m;m
5360,ICLR,2020,IEG: Robust neural net training with severe label noises,Zizhao Zhang;Han Zhang;Sercan Arik;Honglak Lee;Tomas Pfister,zizhaoz@google.com;zhanghan@google.com;soarik@google.com;honglak@google.com;tpfister@google.com,1;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,6,,yes,9/25/19,Google;Google;Google;Google;Google,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,,9/25/19,0,0,0,0,0,0,1007;1531;1537;25920;2426,59;72;53;169;47,15;8;17;62;16,108;174;122;2904;292,m;m
5361,ICLR,2020,Stochastic Geodesic Optimization for Neural Networks,Zana Rashidi;Aijun An;Xiaogang Wang,zrashidi@eecs.yorku.ca;aan@cse.yorku.ca;stevenw@mathstat.yorku.ca,1;3;3,I have read many papers in this area.:I did not assess the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,York University;York University;York University,172;172;172,416;416;416,,9/25/19,0,0,0,0,0,0,7;3205;2973,5;181;140,1;28;24,0;215;407,m;m
5362,ICLR,2020,WEEGNET: an wavelet based Convnet for Brain-computer interfaces,Mouad Riyad;Mohammed Khalil;Abdellah Adib,riyadmouad1@gmail.com;medkhalil87@gmail.com;adib@fstm.ma,1;3;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:N/A:N/A,Withdrawn,0,0,,yes,9/25/19,Faculty of Sciences and Technologies -Mohammedia;;,-1;-1;-1,-1;-1;-1,,9/25/19,0,0,0,0,0,0,3;113;663,3;38;100,1;6;14,0;2;35,m;m
5363,ICLR,2020,Classification as Decoder: Trading Flexibility for Control in Multi Domain Dialogue,Sam Shleifer;Manish Chablani;Namit Katariya;Anitha Kannan;Xavier Amatriain,sshleifer@gmail.com;manish@curai.com;namit@curai.com;anitha@curai.com;xavier@curai.com,1;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,3,,yes,9/25/19,Stanford University;Curai;Curai;Curai;Curai,4;233;233;233;233,4;-1;-1;-1;-1,5,9/25/19,0,0,0,0,0,0,8;3;14;439;326,7;7;5;14;10,2;1;1;6;6,0;0;2;19;30,m;m
5364,ICLR,2020,Anomaly Detection and Localization in Images using Guided Attention,Shashanka Venkataramanan;Rajat Vikram Singh;Kuan-Chuan Peng,shashankv@knights.ucf.edu;singh.rajat@siemens.com;kp388@cornell.edu,6;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,5,,yes,9/25/19,University of Central Florida;Siemens Corporate Research;Cornell University,77;-1;7,609;-1;19,5;4;2,9/25/19,0,0,0,0,0,0,94;25;383,14;8;21,4;2;9,3;5;63,m;m
5365,ICLR,2020,In-training Matrix Factorization for Parameter-frugal Neural Machine Translation,Zachary Kaden;Teven Le Scao;Raphael Olivier,kadenzack@gmail.com;tlescao@andrew.cmu.edu;rolivier@cs.cmu.edu,1;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,0,,yes,9/25/19,;Carnegie Mellon University;Carnegie Mellon University,-1;1;1,-1;27;27,3,9/25/19,0,0,0,0,0,0,43;0;22,5;3;9,2;0;1,4;0;1,m;m
5366,ICLR,2020,Dataset Distillation,Tongzhou Wang;Jun-Yan Zhu;Antonio Torralba;Alexei A. Efros,tongzhou.wang.1994@gmail.com;junyanz@mit.edu;torralba@mit.edu;efros@eecs.berkeley.edu,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,2,11,,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;University of California Berkeley,2;2;2;5,5;5;5;13,,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1;-1,-1;-1;-1;-1,-1;-1;-1;-1,0;0;0;0,m;m
5367,ICLR,2020,Spatial Information is Overrated for Image Classification,Yue Fan;Yongqin Xian;Max Maria Losch;Bernt Schiele,yfan@mpi-inf.mpg.de;yxian@mpi-inf.mpg.de;mlosch@mpi-inf.mpg.de;schiele@mpi-inf.mpg.de,6;1;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,"Saarland Informatics Campus, Max-Planck Institute;Saarland Informatics Campus, Max-Planck Institute;Saarland Informatics Campus, Max-Planck Institute;Saarland Informatics Campus, Max-Planck Institute",-1;-1;-1;-1,-1;-1;-1;-1,,9/25/19,0,0,0,0,0,0,9;1233;38;44100,25;9;12;505,2;6;4;103,0;295;2;5160,m;m
5368,ICLR,2020,GMM-UNIT: Unsupervised Multi-Domain and Multi-Modal Image-to-Image Translation via Attribute Gaussian Mixture Modelling,Yahui Liu;Marco De Nadai;Jian Yao;Nicu Sebe;Bruno Lepri;Xavier Alameda-Pineda,yahui.liu@unitn.it;denadai@fbk.eu;jian.yao@whu.edu.cn;niculae.sebe@unitn.it;lepri@fbk.eu;xavier.alameda-pineda@inria.fr,6;3;1,I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,University of Trento;Fondazione Bruno Kessler;Wuhan University;University of Trento;Fondazione Bruno Kessler;INRIA,18;-1;266;18;-1;-1,307;-1;354;307;-1;-1,,9/25/19,1,0,0,0,0,0,552;216;1195;17136;217;1022,78;21;69;548;14;85,12;6;10;69;4;18,26;8;59;1030;21;55,m;m
5369,ICLR,2020,Information lies in the eye of the beholder: The effect of representations on observed mutual information,Julian Zilly;Lorenz Hetzel;Andrea Censi;Emilio Frazzoli,jzilly@ethz.ch;hetzell@ethz.ch;acensi@ethz.ch;emilio.frazzoli@idsc.mavt.ethz.ch,3;1;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,2,,yes,9/25/19,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10;10,13;13;13;13,,9/25/19,0,0,0,0,0,0,465;0;179;18549,13;2;24;469,4;0;5;63,59;0;13;1340,m;m
5370,ICLR,2020,Quantitatively Disentangling and Understanding Part Information in CNNs,Quanshi Zhang;Yu Yang;Haotian Ma;Ying Nian Wu,zqs1022@sjtu.edu.cn;yy19970901@ucla.edu;11612807@mail.sustc.edu.cn;ywu@stat.ucla.edu,6;3;3,I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,"Shanghai Jiao Tong University;University of California, Los Angeles;University of Science and Technology of China;University of California, Los Angeles",53;20;481;20,157;17;80;17,,9/25/19,0,0,0,0,0,0,1229;368;0;6490,77;112;6;277,17;10;0;39,45;6;0;601,m;m
5371,ICLR,2020,Graph-based motion planning networks,Tai Hoang;Ngo Anh Vien,thobotics@gmail.com;v.ngo@qub.ac.uk,1;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,0,,yes,9/25/19,VNG Corporation;Queen's University Belfast,-1;266,-1;204,10;8,9/25/19,0,0,0,0,0,0,77;475,6;55,1;12,4;31,m;m
5372,ICLR,2020,Bridging ELBO objective and MMD,Talip Ucar,pilatracu@gmail.com,1;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,University College London,50,15,5,9/25/19,1,0,0,0,0,0,12,6,1,1,m
5373,ICLR,2020,Deep 3D-Zoom Net: Unsupervised Learning of Photo-Realistic 3D-Zoom,Juan Luis Gonzalez Bello;Munchurl Kim,juanluisgb@kaist.ac.kr;mkimee@kaist.ac.kr,3;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,481;481,110;110,6,9/20/19,4,0,0,0,0,0,10;14,8;10,2;3,0;1,m;m
5374,ICLR,2020,Through the Lens of Neural Network: Analyzing Neural QA Models via Quantized Latent Representation,Tsung-Han Wu;Chun-Cheng Hsieh;Yen-Hao Chen;Hung-yi Lee,ynnekuw@gmail.com;syasyunjyo@gmail.com;r07921112@ntu.edu.tw;hungyilee@ntu.edu.tw,3;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,0,,yes,9/25/19,National Taiwan University;;National Taiwan University;National Taiwan University,86;-1;86;86,120;-1;120;120,,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1;-1,-1;-1;-1;-1,-1;-1;-1;-1,0;0;0;0,m;m
5375,ICLR,2020,Hierarchical Complement Objective Training,Hao-Yun Chen;Li-Huang Tsai;Shih-Chieh Chang;Jia-Yu Pan;Yu-Ting Chen;Wei Wei;Da-Cheng Juan,haoyunchen@gapp.nthu.edu.tw;lihuangtsai@gapp.nthu.edu.tw;scchang@cs.nthu.edu.tw;jypan@google.com;yutingchen@google.com;wewei@google.com;dacheng@google.com,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,National Tsing Hua University;National Tsing Hua University;National Tsing Hua University;Google;Google;Google;Google,172;172;172;-1;-1;-1;-1,365;365;365;-1;-1;-1;-1,2,9/25/19,0,0,0,0,0,0,16;0;17;73;52;529;33,6;4;9;16;31;201;11,3;0;3;3;2;9;3,3;0;1;1;4;36;7,m;m
5376,ICLR,2020,Anomalous Pattern Detection in Activations and Reconstruction Error of Autoencoders,Celia Cintas;Skyler Speakman;Victor Akinwande;Srihari Sridharan;William Ogallo;Edward McFowland III,celia.cintas@ibm.com;skyler@ke.ibm.com;victor.akinwande1@ibm.com;sriharis.sridharan@ke.ibm.com;william.ogallo@ibm.com;mcfowland@umn.edu,3;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.,Withdrawn,0,3,,yes,9/25/19,"International Business Machines;International Business Machines;International Business Machines;International Business Machines;International Business Machines;University of Minnesota, Minneapolis",-1;-1;-1;-1;-1;59,-1;-1;-1;-1;-1;79,4,9/25/19,0,0,0,0,0,0,1;229;20;87;0;147,12;23;17;22;6;18,1;6;3;5;0;5,0;18;5;3;0;9,f;m
5377,ICLR,2020,Dynamically Balanced Value Estimates for Actor-Critic Methods,Nicolai Dorka;Joschka Boedecker;Wolfram Burgard,dorka@cs.uni-freiburg.de;jboedeck@cs.uni-freiburg.de;burgard@cs.uni-freiburg.de,3;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,0,,yes,9/25/19,Universität Freiburg;Universität Freiburg;Universität Freiburg,118;118;118,85;85;85,,9/25/19,0,0,0,0,0,0,13;1224;53280,4;51;783,2;17;100,0;87;3906,m;m
5378,ICLR,2020,Incorporating Perceptual Prior to Improve Model's Adversarial Robustness,B.S. Vivek;Arya Baburaj;Ashutosh B Sathe;R. Venkatesh Babu,svivek@iisc.ac.in;aryababuraj@iisc.ac.in;satheab16.mech@coep.ac.in;venky@iisc.ac.in,1;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,5,,yes,9/25/19,"Indian Institute of Science;Indian Institute of Science;College of Engineering, Pune;Indian Institute of Science",95;95;-1;95,301;301;-1;301,4;2,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1;-1,-1;-1;-1;-1,-1;-1;-1;-1,0;0;0;0,m;m
5379,ICLR,2020,MultiGrain: a unified image embedding for classes and instances,Maxim Berman;Hervé Jégou;Andrea Vedaldi;Iasonas Kokkinos;Matthijs Douze,maxim.berman@esat.kuleuven.be;rvj@fb.com;vedaldi@fb.com;iasonas.kokkinos@gmail.com;matthijs@fb.com,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Withdrawn,0,4,,yes,9/25/19,KU Leuven;Facebook;Facebook;Ariel AI;Facebook,118;-1;-1;-1;-1,45;-1;-1;-1;-1,,2/14/19,6,0,0,0,0,0,154;15159;36289;12313;11428,19;164;202;130;100,5;42;63;37;33,15;2396;4784;1694;1800,m;m
5380,ICLR,2020,A Simple Geometric Proof for the Benefit of Depth in ReLU Networks,Asaf Amrami;Yoav Goldberg,asaf.amrami@gmail.com;yoav.goldberg@gmail.com,3;3;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,;Bar-Ilan University,-1;95,-1;513,1,9/25/19,0,0,0,0,0,0,12;1504,3;40,2;10,1;189,m;m
5381,ICLR,2020,How does Lipschitz Regularization Influence GAN Training?,Yipeng Qin;Niloy Mitra;Peter Wonka,qinyipeng1991@gmail.com;niloym@gmail.com;pwonka@gmail.com,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,Cardiff University;University College London;KAUST,172;50;128,196;15;1397,5,11/23/18,0,0,0,0,0,0,81;12190;8064,14;249;220,4;56;44,14;921;622,m;m
5382,ICLR,2020,SIMULTANEOUS ATTRIBUTED NETWORK EMBEDDING AND CLUSTERING,Lazhar labiod;Mohamed Nadif,lazhar.labiod@parisdescartes.fr;mohamed.nadif@parisdescartes.fr,1;1;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,University Paris Descartes;University Paris Descartes,-1;-1,-1;-1,,9/25/19,0,0,0,0,0,0,124;0,41;6,6;0,6;0,m;m
5383,ICLR,2020,Tree-structured Attention Module for Image Classification,Gyungin Shin;Sung-Ho Bae;and Yong-Jae Moon,gishin@khu.ac.kr;shbae@khu.ac.kr;moonyj@khu.ac.kr,3;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Withdrawn,0,3,,yes,9/25/19,Kyung Hee University;Kyung Hee University;Kyung Hee University,481;481;481,319;319;319,2,9/25/19,0,0,0,0,0,0,14;191;0,6;16;1,2;3;0,0;11;0,m;m
5384,ICLR,2020,Task Level Data Augmentation for Meta-Learning,Jialin Liu;Fei Chao;Chih-Min Lin,31520171153232@stu.xmu.edu.cn;fchao@xmu.edu.cn;cml@saturn.yzu.edu.tw,3;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,1,,yes,9/25/19,"Xiamen University;Xiamen University;Department of Computer Science and Engineering, Yuan Ze University",64;64;-1,8;8;-1,1;6,9/25/19,0,0,0,0,0,0,112;807;80,24;125;35,6;16;3,14;9;3,f;m
5385,ICLR,2020,Learning Semantic Correspondences from Noisy Data-text Pairs by Local-to-Global Alignments,Feng Nie;Jinpeng Wang;Rong Pan;Chin-Yew Lin,fengniesysu@gmail.com;jinpwa@microsoft.com;panr@sysu.edu.cn;cyl@microsoft.com,3;8,I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,SUN YAT-SEN UNIVERSITY;Microsoft;SUN YAT-SEN UNIVERSITY;Microsoft,481;-1;481;-1,299;-1;299;-1,3,9/25/19,0,0,0,0,0,0,215;358;1111;8,39;39;43;11,10;11;8;2,2;27;117;2,u;m
5386,ICLR,2020,VideoEpitoma: Efficient Recognition of Long-range Actions,Noureldien Hussein;Babak Ehteshami Bejnordi;Mihir Jain,nhussein@uva.nl;behtesha@qti.qualcomm.com;mijain@qti.qualcomm.com,1;3;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I carefully checked the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,"University of Amsterdam;Qualcomm Inc, QualComm;Qualcomm Inc, QualComm",172;-1;-1,62;-1;-1,,9/25/19,0,0,0,0,0,0,63;3458;1452,7;27;27,4;12;11,7;110;158,m;m
5387,ICLR,2020,StacNAS: Towards Stable and Consistent  Optimization for Differentiable  Neural Architecture Search,Li Guilin;Zhang Xing;Wang Zitong;Li Zhenguo;Zhang Tong,hiliguilin@gmail.com;zhang.xing1@huawei.com;ztwang@math.cuhk.edu.hk;li.zhenguo@huawei.com;tongzhang@tongzhang-ml.org,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Withdrawn,1,3,,yes,9/25/19,;Huawei Technologies Ltd.;The Chinese University of Hong Kong;Huawei Technologies Ltd.;The Hong Kong University of Science and Technology,-1;-1;59;-1;39,-1;-1;35;-1;47,,9/25/19,4,0,0,0,0,0,113;2042;6288;12;14,60;59;253;11;20,6;23;33;2;2,8;299;520;1;0,f;m
5388,ICLR,2020,Construction of Macro Actions for Deep Reinforcement Learning,Yi-Hsiang Chang;Kuan-Yu	Chang;Henry Kuo;Chun-Yi Lee,shawn420@gapp.nthu.edu.tw;kychang@elsa.cs.nthu.edu.tw;hkuo@college.harvard.edu;cylee@gapp.nthu.edu.tw,1;1;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,1,,yes,9/25/19,National Tsing Hua University;National Tsing Hua University;Harvard University;National Tsing Hua University,172;172;39;172,365;365;7;365,,8/5/19,1,0,0,0,0,0,52;1;3;958,11;2;4;73,4;1;1;19,2;1;1;59,u;m
5389,ICLR,2020,Towards Understanding Generalization in Gradient-Based Meta-Learning,Simon Guiroy;Vikas Verma;Christopher J. Pal,simon.guiroy@umontreal.ca;vikasverma.iitm@gmail.com;christopher.pal@polymtl.ca,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,University of Montreal;;Polytechnique Montreal,128;-1;390,85;-1;1397,6;8,7/16/19,7,0,0,0,0,0,6;104;862,2;47;58,1;6;12,2;6;72,m;m
5390,ICLR,2020,Structural Multi-agent Learning,Kaiqian Han;Liangliang Ren;Jiwen Lu;Jie Zhou,hkg16@mails.tsinghua.edu.cn;renll16@mails.tsinghua.edu.cn;lujiwen@tsinghua.edu.cn;jzhou@tsinghua.edu.cn,1;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,4,,yes,9/25/19,Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University,8;8;8;8,23;23;23;23,10,9/25/19,0,0,0,0,0,0,0;11;5;493,1;5;17;134,0;2;1;13,0;1;0;42,m;m
5391,ICLR,2020,AdamT: A Stochastic Optimization with Trend Correction Scheme,Bingxin Zhou;Xuebin Zheng;Junbin Gao,bzho3923@uni.sydney.edu.au;xzhe2914@uni.sydney.edu.au;junbin.gao@sydney.edu.au,3;3;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,University of Sydney;University of Sydney;University of Sydney,86;86;86,60;60;60,,9/25/19,1,0,0,0,0,0,3;100;23,3;10;23,1;3;3,0;7;1,m;m
5392,ICLR,2020,Elastic-InfoGAN: Unsupervised Disentangled Representation Learning in Imbalanced Data,Utkarsh Ojha;Krishna Kumar Singh;Cho-Jui Hsieh;Yong Jae Lee,uojha@ucdavis.edu;krsingh@ucdavis.edu;chohsieh@cs.ucla.edu;yongjaelee@ucdavis.edu,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,"University of California, Davis;University of California, Davis;University of California, Los Angeles;University of California, Davis",79;79;20;79,55;55;17;55,5,9/25/19,0,0,0,0,0,0,68;80;13775;3268,6;23;168;84,3;4;42;26,13;2;1753;380,m;m
5393,ICLR,2020,Generalizing Deep Multi-task Learning with Heterogeneous Structured Networks,Ming Hou;Xinqi Chen;Shifeng Huang;Shengli Xie;Guoxu Zhou;Qibin Zhao,ming.hou@riken.jp;xinqicham@gmail.com;sfengmmin@163.com;shlxie@gdut.edu.cn;gx.zhou@gdut.edu.cn;qibin.zhao@riken.jp,3;1;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,0,,yes,9/25/19,RIKEN;South China University of Technology;163;South China University of Technology;South China University of Technology;RIKEN,-1;481;-1;481;481;-1,-1;501;-1;501;501;-1,,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1,0;0;0;0;0;0,m;m
5394,ICLR,2020,Rethinking Data Augmentation: Self-Supervision and Self-Distillation,Hankook Lee;Sung Ju Hwang;Jinwoo Shin,hankook.lee@kaist.ac.kr;sjhwang82@kaist.ac.kr;jinwoos@kaist.ac.kr,1;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,2,,yes,9/25/19,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,481;481;481,110;110;110,6;8,9/25/19,13,0,0,0,0,0,32;1185;1934,5;75;192,3;17;19,5;124;231,m;m
5395,ICLR,2020,Dynamical Clustering of Time Series Data Using Multi-Decoder RNN Autoencoder,Daisuke Kaji;Kazuho Watanabe;Masahiro Kobayashi,daisuke.kaji.j3a@jp.denso.com;wkazuho@cs.tut.ac.jp;kobayashi@lisl.cs.tut.ac.jp,3;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,"Jp.denso;Toyohashi University of Technology,;Toyohashi University of Technology,",-1;-1;-1,-1;-1;-1,,9/25/19,0,0,0,0,0,0,24;58;32,17;15;19,3;3;3,0;6;4,u;u
5396,ICLR,2020,Regularizing Predictions via Class-wise Self-knowledge Distillation,Sukmin Yun;Jongjin Park;Kimin Lee;Jinwoo Shin,sukmin.yun@kaist.ac.kr;jongjin.park@kaist.ac.kr;kiminlee@kaist.ac.kr;jinwoos@kaist.ac.kr,3;1;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Withdrawn,0,1,,yes,9/25/19,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,481;481;481;481,110;110;110;110,8,9/25/19,2,0,0,0,0,0,24;1021;540;1934,5;23;28;192,3;9;8;19,3;10;116;231,m;m
5397,ICLR,2020,Imbalanced Classification via Adversarial Minority Over-sampling,Jaehyung Kim;Jongheon Jeong;Jinwoo Shin,jaehyungkim@kaist.ac.kr;jongheonj@kaist.ac.kr;jinwoos@kaist.ac.kr,3;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,481;481;481,110;110;110,3;4;8,9/25/19,1,0,0,0,0,0,85;7;1934,45;8;192,5;2;19,4;0;231,m;m
5398,ICLR,2020,Neuron ranking - an informed way to compress convolutional neural networks,Kamil Adamczewski;Mijung Park,kamil.m.adamczewski@gmail.com;mijung.park@tuebingen.mpg.de,1;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,"Max Planck Institute for Intelligent Systems, Max-Planck Institute;Max Planck Institute for Intelligent Systems, Max-Planck Institute",-1;-1,-1;-1,,9/25/19,0,0,0,0,0,0,65;16,36;19,4;3,8;0,m;f
5399,ICLR,2020,Compressing Deep Neural Networks With Learnable Regularization,Yoojin Choi;Mostafa El-Khamy;Jungwon Lee,yoojin.c@samsung.com;mostafa.e@samsung.com;jungwon2.lee@samsung.com,3;6;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,Samsung;Samsung;Samsung,-1;-1;-1,-1;-1;-1,,9/1/18,0,0,0,0,0,0,513;1190;2634,35;102;196,9;17;26,45;111;244,f;m
5400,ICLR,2020,Image Classification Through Top-Down Image Pyramid Traversal,Athanasios Papadopoulos;Pawel Korus;Nasir Memon,ap4094@nyu.edu;pkorus@nyu.edu;memon@nyu.edu,3;1;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,4,,yes,9/25/19,New York University;New York University;New York University,25;25;25,29;29;29,,9/25/19,0,0,0,0,0,0,203;365;14702,14;30;390,7;10;61,24;32;1138,m;m
5401,ICLR,2020,PAC-Bayes Few-shot Meta-learning with Implicit Learning of Model Prior Distribution,Cuong Nguyen;Thanh-Toan Do;Gustavo Carneiro,cuong.nguyen@adelaide.edu.au;thanh-toan.do@liverpool.ac.uk;gustavo.carneiro@adelaide.edu.au,6;1;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,The University of Adelaide;University of Liverpool;The University of Adelaide,128;-1;128,120;-1;120,11;5;6,9/25/19,0,0,0,0,0,0,887;887;5418,129;64;167,13;16;35,61;95;395,m;m
5402,ICLR,2020,CRAP: Semi-supervised Learning via Conditional Rotation Angle Prediction,Hai-Ming Xu;Lingqiao Liu,hai-ming.xu@adelaide.edu.au;lingqiao.liu@adelaide.edu.au,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Withdrawn,0,2,,yes,9/25/19,The University of Adelaide;The University of Adelaide,128;128,120;120,,9/25/19,0,0,0,0,0,0,3;40,5;25,1;4,0;2,m;m
5403,ICLR,2020,A novel text representation which enables image classifiers to perform text classification,Stephen M. Petrie;T'Mir D. Julius,spetrie@swin.edu.au;tdjempire@gmail.com,1;1;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,0,,yes,9/25/19,Swinburne University of Technology;,-1;-1,-1;-1,3,8/19/19,0,0,0,0,0,0,279;5,13;6,6;1,11;0,m;f
5404,ICLR,2020,FACE SUPER-RESOLUTION GUIDED BY 3D FACIAL PRIORS,xiaobin hu;wenqi ren;jiaolong yang;xiaochun cao;Xiaoming Li;John LaMaster;Bjoern Menze;wei liu,xiaobin.hu@tum.de;rwq.renwenqi@gmail.com;jiaoyan@microsoft.com;caoxiaochun@iie.ac.cn;hit.xmshr@gmail.com;jlamaste@gmail.com;bjoern.menze@tum.de;wl2223@columbia.edu,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,"Technical University Munich;Institute of information engineering, CAS;Microsoft;Institute of information engineering, CAS;Harbin Institute of Technology;Technical University Munich;Technical University Munich;Columbia University",53;-1;-1;-1;172;53;53;15,43;-1;-1;-1;424;43;43;16,,9/25/19,0,0,0,0,0,0,56;1437;5820;13;2791;0;2;407,15;44;258;15;118;1;5;107,3;17;40;3;25;0;1;10,1;251;616;0;184;0;0;16,m;m
5405,ICLR,2020,Doubly Normalized Attention,Nan Ding;Xinjie Fan;Zhenzhong Lan;Dale Schuurmans;Radu Soricut,dingnan@google.com;fan.xinjiebuaa@gmail.com;lanzhzh@google.com;schuurmans@google.com;rsoricut@google.com,3;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,"Google;University of Texas, Austin;Google;Google;Google",-1;22;-1;-1;-1,-1;38;-1;-1;-1,,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,0;0;0;0;0,m;m
5406,ICLR,2020,Correctness Verification of Neural Network,Yichen Yang;Martin Rinard,yicheny@csail.mit.edu;rinard@csail.mit.edu,1;3;1,I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;2,5;5,1,6/3/19,4,0,0,0,0,0,6;13157,7;354,1;61,0;1025,m;m
5407,ICLR,2020,Diversely Stale Parameters for Efficient Training of Deep Convolutional Networks,An Xu;Zhouyuan Huo;Heng Huang,an.xu@pitt.edu;zhouyuan.huo@pitt.edu;heng.huang@pitt.edu,3;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Withdrawn,0,0,,yes,9/25/19,University of Pittsburgh;University of Pittsburgh;University of Pittsburgh,79;79;79,113;113;113,1;8,9/25/19,0,0,0,0,0,0,1751;446;355,113;40;36,23;12;10,48;51;49,m;m
5408,ICLR,2020,WHAT DATA IS USEFUL FOR MY DATA: TRANSFER LEARNING WITH A MIXTURE OF SELF-SUPERVISED EXPERTS,Xi Yan;David Acuna;Sanja Fidler,xi.yan@mail.utoronto.ca;davidj@cs.toronto.edu;fidler@cs.toronto.edu,1;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,"Toronto University;Department of Computer Science, University of Toronto;Department of Computer Science, University of Toronto",18;18;18,18;18;18,6;2,9/25/19,0,0,0,0,0,0,0;604;11171,18;30;160,0;11;49,0;73;1429,f;f
5409,ICLR,2020,Classification Logit Two-sample Testing by Neural Networks,Xiuyuan Cheng;Alexander Cloninger,xiuyuan.cheng@duke.edu;acloninger@ucsd.edu,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,"Duke University;University of California, San Diego",47;11,20;31,5;4;1,9/25/19,2,0,0,0,0,0,544;292,51;34,13;7,37;22,m;m
5410,ICLR,2020,Frontal low-rank random tensors for high-order feature representation,Yan Zhang;Krikamol Muandet;Qianli Ma;Heiko Neumann;Siyu Tang,yan.zhang@tuebingen.mpg.de;krikamol@tuebingen.mpg.de;qianli.ma@tue.mpg.de;heiko.neumann@uni-ulm.de;stang@tuebingen.mpg.de,6;3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,"Max Planck Institute for Intelligent Systems, Max-Planck Institute;Max Planck Institute for Intelligent Systems, Max-Planck Institute;Max Planck Institute for Intelligent Systems, Max-Planck Institute;Ulm University;Max Planck Institute for Intelligent Systems, Max-Planck Institute",-1;-1;-1;172;-1,-1;-1;-1;416;-1,1;2,9/25/19,0,0,0,0,0,0,1771;1223;2105;220;2022,137;45;68;22;109,20;14;11;7;17,94;141;153;6;167,f;f
5411,ICLR,2020,Interpretability Evaluation Framework for Deep Neural Networks,Junxiang Wang;Liang Zhao;Yanfang Ye;Houman Homayoun,jwang40@gmu.edu;lzhao9@gmu.edu;yanfang.ye@mail.wvu.edu;hhomayou@gmu.edu,1;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,George Mason University;George Mason University;West Virginia University;George Mason University,100;100;-1;100,282;282;-1;282,1,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1;-1,-1;-1;-1;-1,-1;-1;-1;-1,0;0;0;0,m;m
5412,ICLR,2020,CWAE-IRL: Formulating a supervised approach to Inverse Reinforcement Learning problem,Arpan Kusari,arpan.kusari@gmail.com,3;1;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,,,,5,9/25/19,0,0,0,0,0,0,83,8,3,4,m
5413,ICLR,2020,"Study of a Simple, Expressive and Consistent Graph Feature Representation",Pineau Edouard,pineau.edouard@gmail.com,1;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,Télécom ParisTech,481,187,10,9/25/19,0,0,0,0,0,0,0,1,0,0,m
5414,ICLR,2020,Input Alignment along Chaotic directions increases Stability in Recurrent Neural Networks,Priyadarshini Panda;Kaushik Roy,priya.panda@yale.edu;kaushik@purdue.edu,6;1;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Yale University;Purdue University,64;27,8;88,,12/26/17,0,0,0,0,0,0,674;22262,66;765,15;76,45;1551,f;m
5415,ICLR,2020,All Neural Networks are Created Equal,Guy Hacohen;Leshem Choshen;Daphna Weinshall,guy.hacohen@mail.huji.ac.il;leshem.choshen@mail.huji.ac.il;daphna@cs.huji.ac.il,3;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Withdrawn,0,1,,yes,9/25/19,Hebrew University of Jerusalem;Hebrew University of Jerusalem;Hebrew University of Jerusalem,67;67;67,216;216;216,10;8,9/25/19,1,0,0,0,0,0,35;117;5527,4;17;216,1;7;37,3;14;529,m;f
5416,ICLR,2020,Hyperbolic Image Embeddings,Valentin Khrulkov;Leyla Mirvakhabova;Evgeniya Ustinova;Ivan Oseledets;Victor Lempitsky,khrulkov.v@gmail.com;leyla.mirvakhabova@skoltech.ru;evgeniya.ustinova@skoltech.ru;i.oseledets@skoltech.ru;v.lempitsky@samsung.com,1;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,Skolkovo Institute of Science and Technology;Skolkovo Institute of Science and Technology;Skolkovo Institute of Science and Technology;Skolkovo Institute of Science and Technology;Samsung,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,6;2,4/3/19,7,0,0,0,0,0,165;20;2055;4783;15700,12;4;9;200;133,7;2;4;31;51,17;4;469;413;2290,m;m
5417,ICLR,2020,Instant Quantization of Neural Networks using Monte Carlo Methods,Gonçalo Mordido;Matthijs Van Keirsbilck;Alexander Keller,goncalo.mordido@hpi.de;matthijsv@nvidia.com;akeller@nvidia.com,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.,Withdrawn,0,0,,yes,9/25/19,Hasso Plattner Institute;NVIDIA;NVIDIA,266;-1;-1,1397;-1;-1,,5/29/19,1,0,0,0,0,0,0;5;5807,3;3;182,0;2;35,0;0;484,m;m
5418,ICLR,2020,Gating Revisited: Deep Multi-layer RNNs That Can Be Trained,Mehmet Ozgur Turkoglu;Stefano D'Aronco;Jan Dirk Wegner;Konrad Schindler,ozgur.turkoglu@geod.baug.ethz.ch;stefano.daronco@geod.baug.ethz.ch;jan.wegner@geod.baug.ethz.ch;schindler@geod.baug.ethz.ch,1;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology,10;10;10;10,13;13;13;13,,9/25/19,1,0,0,0,0,0,13;78;1332;12873,6;22;51;234,2;5;15;60,0;7;100;1332,m;m
5419,ICLR,2020,ILS-SUMM: Iterated Local Search for Unsupervised Video Summarization,Yair Shemer;Daniel Rotman;Nahum Shimkin,sy@campus.technion.ac.il;danieln@il.ibm.com;shimkin@ee.technion.ac.il,1;3;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,Technion;International Business Machines;Technion,26;-1;26,412;-1;412,,9/25/19,1,0,0,0,0,0,1;27;3959,1;11;130,1;4;28,0;3;366,m;m
5420,ICLR,2020,Quadratic GCN for graph classification,Omer Nagar;Yoram Louzoun,ovednagar@hotmail.com;louzouy@math.biu.ac.il,1;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,;Bar Ilan University,-1;95,-1;513,10,9/25/19,0,0,0,0,0,0,0;2772,1;173,0;28,0;108,m;m
5421,ICLR,2020,Topological based classification using graph convolutional networks,Roy Abel;Idan Benami;Yoram Louzoun,royabel10@gmail.com;louzouy@math.biu.ac.il,3;1;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,0,,yes,9/25/19,Bar Ilan University;Bar Ilan University,95;95,513;513,10,9/25/19,0,0,0,0,0,0,212;8;2772,11;3;173,4;2;28,24;1;108,m;m
5422,ICLR,2020,Quantifying Layerwise Information Discarding of Neural Networks and Beyond,Haotian Ma;Yinqing Zhang;Fan Zhou;Quanshi Zhang,11612807@mail.sustc.edu.cn;zhangyinqing@sjtu.edu.cn;zhoufan98@sjtu.edu.cn;zqs1022@sjtu.edu.cn,3;3;3,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Withdrawn,0,3,,yes,9/25/19,University of Science and Technology of China;Shanghai Jiao Tong University;Shanghai Jiao Tong University;Shanghai Jiao Tong University,481;53;53;53,80;157;157;157,7,9/25/19,0,0,0,0,0,0,65;420;10;1229,22;24;18;77,5;11;2;17,1;8;0;45,m;m
5423,ICLR,2020,Real or Fake: An Empirical Study and Improved Model for Fake Face Detection,Zhengzhe Liu;Xiaojuan Qi;Jiaya Jia;Philip H. S. Torr,liuzhengzhelzz@gmail.com;xiaojuan.qi@eng.ox.ac.uk;leojia@cse.cuhk.edu.hk;philip.torr@eng.ox.ac.uk,3;3;3,I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,;University of Oxford;The Chinese University of Hong Kong;University of Oxford,-1;50;59;50,-1;1;35;1,5,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1;-1,-1;-1;-1;-1,-1;-1;-1;-1,0;0;0;0,m;m
5424,ICLR,2020,Minimizing Change in Classifier Likelihood to Mitigate Catastrophic Forgetting,Ashish Gaurav;Sachin Vernekar;Sean Sedwards;Jaeyoung Lee;Vahdat Abdelzad;Krzysztof Czarnecki,ashish.gaurav@uwaterloo.ca;sachin.vernekar@uwaterloo.ca;sean.sedwards@uwaterloo.ca;jaeyoung.lee@uwaterloo.ca;vabdelza@gsd.uwaterloo.ca;kczarnec@gsd.uwaterloo.ca,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,1,1,,yes,9/25/19,University of Waterloo;University of Waterloo;University of Waterloo;University of Waterloo;University of Waterloo;University of Waterloo,28;28;28;28;28;28,235;235;235;235;235;235,,9/25/19,0,0,0,0,0,0,17;44;962;220;96;15632,9;17;73;61;24;328,3;4;18;8;6;58,0;4;31;9;5;1340,m;m
5425,ICLR,2020,A Memory-augmented Neural Network by Resembling Human Cognitive Process of Memorization,Dongjing Shan;Xiongwei Zhang;Chao Zhang;Limin Wang,shandongjing@pku.edu.cn;xwzhang9898@163.com;chzhang@cis.pku.edu.cn;07wanglimin@gmail.com,3;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Peking University;163;Peking University;Zhejiang University,22;-1;22;56,24;-1;24;107,,9/25/19,0,0,0,0,0,0,0;423;351;555,3;155;75;21,0;10;9;6,0;18;16;95,u;m
5426,ICLR,2020,Molecule Property Prediction and Classification with Graph Hypernetworks,Eliya Nachmani;Lior Wolf,enk100@gmail.com;wolf@fb.com,1;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Facebook;Facebook,-1;-1,-1;-1,10,9/25/19,2,0,0,0,0,0,576;496,13;74,8;11,40;56,m;m
5427,ICLR,2020, Sparsity Learning in Deep Neural Networks,Amirsina Torfi;Rouzbeh A. Shirvani;Sobhan Soleymani;Nasser M. Nasrabadi,atorfi@vt.edu;rouzbeh.asghari@gmail.com;ssoleyma@mix.wvu.edu;nasser.nasrabadi@mail.wvu.edu,3;1;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,0,,yes,9/25/19,Virginia Tech;;West Virginia University;West Virginia University,79;-1;-1;-1,240;-1;-1;-1,,1/7/19,4,1,1,0,4,1,129;57;129;15106,22;9;23;371,6;4;6;40,10;4;5;872,m;m
5428,ICLR,2020,A Harmonic Structure-Based Neural Network Model for Musical Pitch Detection,Xian Wang;Lingqiao Liu;Qinfeng Shi,xian.wang01@adelaide.edu.au;lingqiao.liu@adelaide.edu.au;javen.shi@adelaide.edu.au,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:N/A:N/A,Withdrawn,0,0,,yes,9/25/19,The University of Adelaide;The University of Adelaide;The University of Adelaide,128;128;128,120;120;120,,9/25/19,0,0,0,0,0,0,99;2658;4084,9;93;102,2;25;26,9;315;385,m;m
5429,ICLR,2020,SEERL : Sample Efficient Ensemble Reinforcement Learning,Rohan Saphal;Balaraman Ravindran;Dheevatsa Mudigere;Sasikanth Avancha;Bharat Kaul,rohansaphal@gmail.com;ravi@cse.iitm.ac.in;dheevatsa@fb.com;sasikanth.avancha@intel.com;bharat.kaul@intel.com,1;1;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,0,,yes,9/25/19,University of Oxford;Indian Institute of Technology Madras;Facebook;Intel;Intel,50;154;-1;-1;-1,1;641;-1;-1;-1,1,9/25/19,1,0,0,0,0,0,1;2738;1529;3999;477,3;237;40;62;34,1;28;15;19;11,0;211;153;419;49,m;m
5430,ICLR,2020,Exploring by Exploiting Bad Models in Model-Based Reinforcement Learning,Yixin Lin;Sarah Bechtle;Ludovic Righetti;Akshara Rai;Franziska Meier,yixinlin@fb.com;sbechtle@tuebingen.mpg.de;ludovic.righetti@nyu.edu;akshararai@fb.com;fmeier@fb.com,3;1;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,0,,yes,9/25/19,"Facebook;Max Planck Institute for Intelligent Systems, Max-Planck Institute;New York University;Facebook;Facebook",-1;-1;25;-1;-1,-1;-1;29;-1;-1,,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,0;0;0;0;0,m;f
5431,ICLR,2020,Defensive Quantization Layer For Convolutional Network Against Adversarial Attack,Sirui Song;Qinglong Wang;Derek Yang;Yan Song;Xue Liu;Tong Zhang,siruisong97@gmail.com;qinglong.wang@mail.mcgill.ca;dyang1206@gmail.com;songyan@chuangxin.com;xueliu@cs.mcgill.ca;tongzhang0@gmail.com,3;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,"Tsinghua University;McGill University;University of California, San Diego;Sinovation Ventures;McGill University;The Hong Kong University of Science and Technology",8;86;11;-1;86;39,23;42;31;-1;42;47,4,9/25/19,0,0,0,0,0,0,9;65;4697;273;178;206,7;12;29;113;62;81,2;4;21;10;4;6,0;1;242;27;11;22,m;m
5432,ICLR,2020,Learnable Higher-order Representation for Action Recognition,Kai Hu;Bhiksha Raj,kaihu@cmu.edu;bhiksha@cs.cmu.edu,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,1,,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University,1;1,27;27,1,9/25/19,0,0,0,0,0,0,4633;8826,492;318,36;46,112;784,u;u
5433,ICLR,2020,Meta Module Network for Compositional Visual Reasoning,Wenhu Chen;Zhe Gan;Linjie Li;Yu Cheng;William Wang;Jingjing Liu,wenhuchen@ucsb.edu;zhe.gan@microsoft.com;lindsey.li@microsoft.com;yu.cheng@microsoft.com;william@cs.ucsb.edu;jingjl@microsoft.com,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,UC Santa Barbara;Microsoft;Microsoft;Microsoft;UC Santa Barbara;Microsoft,38;-1;-1;-1;38;-1,57;-1;-1;-1;57;-1,10;8,9/25/19,3,0,0,0,0,0,516;76;116;194;17;319,42;6;8;31;7;41,10;3;3;6;3;7,68;15;21;30;3;32,m;f
5434,ICLR,2020,Multi-Task Adapters for On-Device Audio Inference,M. Tagliasacchi;F. de Chaumont Quitry;D. Roblek,mtagliasacchi@google.com;fcq@google.com;droblek@google.com,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,0,,yes,9/25/19,Google;Google;Google,-1;-1;-1,-1;-1;-1,,9/25/19,0,0,0,0,0,0,4005;10;78,229;5;23,34;2;6,215;0;6,m;m
5435,ICLR,2020,Learning audio representations with self-supervision,M. Tagliasacchi;B. Gfeller;F. de Chaumont Quitry;D. Roblek,mtagliasacchi@google.com;beatg@google.com;fcq@google.com;droblek@google.com,3;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,3,9/25/19,0,0,0,0,0,0,4005;529;10;78,229;37;5;23,34;9;2;6,215;48;0;6,m;m
5436,ICLR,2020,Scholastic-Actor-Critic For Multi Agent Reinforcement Learning,Weiying Chen，Ruize Hou,dissolution@126.com;fsszns@163.com,1;1;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,4,,yes,9/25/19,Netaji Subhas Institute of Technology;Peking University,-1;22,-1;24,,9/25/19,0,0,0,0,0,0,178,10,6,4,m;m
5437,ICLR,2020,Non-Gaussian processes and neural networks at finite widths,Sho Yaida,shoyaida@fb.com,3;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,Facebook,-1,-1,11,9/25/19,4,0,0,0,0,0,1067,41,12,65,m
5438,ICLR,2020,Uncertainty-aware Variational-Recurrent Imputation Network for Clinical Time Series,Ahmad Wisnu Mulyadi;Eunji Jun;Heung-Il Suk,wisnumulyadi@korea.ac.kr;ejjun92@korea.ac.kr;hisuk@korea.ac.kr,1;3;6;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Korea University;Korea University;Korea University,323;323;323,179;179;179,5,9/25/19,0,0,0,0,0,0,4;3;3550,6;5;109,1;1;27,0;0;182,m;m
5439,ICLR,2020,An Information Theoretic Perspective on Disentangled Representation Learning,Xiaojiang Yang;Wendong Bi;Yu Cheng;Junchi Yan,yangxiaojiang@sjtu.edu.cn;biwendong1997@gmail.com;yu.cheng@microsoft.com;yanjunchi@sjtu.edu.cn,1;1;3,I have published one or two papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Withdrawn,0,4,,yes,9/25/19,Shanghai Jiao Tong University;Microsoft;Microsoft;Shanghai Jiao Tong University,53;-1;-1;53,157;-1;-1;157,5;1,9/25/19,0,0,0,0,0,0,12;0;11277;2405,12;3;859;152,2;0;49;28,0;0;512;154,m;m
5440,ICLR,2020,Towards a Unified Evaluation of Explanation Methods without Ground Truth,Hao Zhang;Jiayi Chen;Haotian Xue;Quanshi Zhang,1603023-zh@sjtu.edu.cn;miracle3310@sjtu.edu.cn;xavihart@sjtu.edu.cn;zqs1022@sjtu.edu.cn,3;1;6,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,Shanghai Jiao Tong University;Shanghai Jiao Tong University;Shanghai Jiao Tong University;Shanghai Jiao Tong University,53;53;53;53,157;157;157;157,,9/25/19,3,0,0,0,0,0,-1;303;30;1229,-1;40;10;77,-1;7;3;17,0;10;1;45,m;m
5441,ICLR,2020,Fast Sparse ConvNets,Erich Elsen;Marat Dukhan;Trevor Gale;Karen Simonyan,eriche@google.com;maratek@google.com;tgale@google.com;simonyan@google.com,3;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A,Withdrawn,0,3,,yes,9/25/19,Google;Google;Google;Google,-1;-1;-1;-1,-1;-1;-1;-1,,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1;-1,-1;-1;-1;-1,-1;-1;-1;-1,0;0;0;0,m;m
5442,ICLR,2020,Locally adaptive activation functions with slope recovery term for deep and physics-informed neural networks,Ameya D. Jagtap;Kenji Kawaguchi;George Em Karniadakis,ameya_jagtap@brown.edu;kawaguch@mit.edu;george_karniadakis@brown.edu,1;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Withdrawn,0,0,,yes,9/25/19,Brown University;Massachusetts Institute of Technology;Brown University,67;2;67,53;5;53,1,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1,-1;-1;-1,-1;-1;-1,0;0;0,m;m
5443,ICLR,2020,GumbelClip: Off-Policy Actor-Critic Using Experience Replay,Norman Tasfi;Miriam Capretz,ntasfi@gmail.com,1;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,University of Western Ontario,-1,-1,,9/25/19,0,0,0,0,0,0,9;1706,3;127,1;19,0;88,m;f
5444,ICLR,2020,Adversarial Neural Pruning,Divyam Madaan;Jinwoo Shin;Sung Ju Hwang,dmadaan@kaist.ac.kr;jinwoos@kaist.ac.kr;sjhwang82@kaist.ac.kr,1;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,481;481;481,110;110;110,4;11,9/25/19,1,0,0,0,0,0,4;1934;1185,5;193;75,1;19;17,0;231;124,m;m
5445,ICLR,2020,Auto-Encoding Explanatory Examples,César Ojeda;David Biesner;Ramses Sanchez;Kostadin Cvejoski;Jannis Schuecker;Christian Bauckhage;Bodgan Georgiev,cesarali07@gmail.com;david.biesner@iais.fraunhofer.de;sanchez@bit.uni-bonn.de;kostadin.cvejoski@iais.fraunhofer.de;jannis.schuecker@iais.fraunhofer.de;christian.bauckhage@iais.fraunhofer.de;bogdan.georgiev@iais.fraunhofer.de,3;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,Fraunhofer IIS;Fraunhofer IIS;University of Bonn;Fraunhofer IIS;Fraunhofer IIS;Fraunhofer IIS;Fraunhofer IIS,-1;-1;128;-1;-1;-1;-1,-1;-1;106;-1;-1;-1;-1,,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,-1;-1;-1;-1;-1;-1;-1,0;0;0;0;0;0;0,m;m
5446,ICLR,2020,Affine Self Convolution,Nichita Diaconu;Daniel E. Worrall,diacon995@gmail.com;d.e.worrall@uva.nl,1;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,University of Amsterdam;University of Amsterdam,172;172,62;62,,9/25/19,2,0,0,0,0,0,7;466,2;19,2;6,0;50,m;m
5447,ICLR,2020,Universal Source-Free Domain Adaptation,Jogendra Nath Kundu;Naveen Venkat;Rahul M V;R. Venkatesh Babu,jogendrak@iisc.ac.in;nav.naveenvenkat@gmail.com;rmvenkat@andrew.cmu.edu;venky@iisc.ac.in,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Withdrawn,0,4,,yes,9/25/19,Indian Institute of Science;Indian Institute of Science;Carnegie Mellon University;Indian Institute of Science,95;95;1;95,301;301;27;301,5;4,9/25/19,0,0,0,0,0,0,106;0;1;4255,12;2;4;228,4;0;1;34,8;0;0;401,m;m
5448,ICLR,2020,Learning an off-policy predictive state representation for deep reinforcement learning for vision-based steering in autonomous driving,Daniel Graves,dgraves@ualberta.ca,1;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,4,,yes,9/25/19,University of Alberta,100,136,8,9/25/19,-1,-1,-1,-1,-1,-1,-1,-1,-1,0,m
5449,ICLR,2020,$\textrm{D}^2$GAN: A Few-Shot Learning Approach with Diverse and Discriminative Feature Synthesis,Kai Li;Yulun Zhang;Kunpeng Li;Yun Fu,li.kai.gml@gmail.com;yulun100@gmail.com;kunpengli@ece.neu.edu;yunfu@ece.neu.edu,1;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Northeastern University;Northeastern University;Northeastern University;Northeastern University,16;16;16;16,906;906;906;906,5;4;6,9/25/19,0,0,0,0,0,0,19389;69;224;1263,92;24;25;164,14;2;8;22,3603;2;29;50,m;m
5450,ICLR,2020,Hardware-aware One-Shot Neural Architecture Search in Coordinate Ascent Framework,Li Lyna Zhang;Yuqing Yang;Yuhang Jiang;Wenwu Zhu;Yunxin Liu,lzhani@microsoft.com;yuqing.yang@microsoft.com;jyh17@mails.tsinghua.edu.cn;wwzhu@tsinghua.edu.cn;yunxin.liu@microsoft.com,3;1;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Microsoft;Microsoft;Tsinghua University;Tsinghua University;Microsoft,-1;-1;8;8;-1,-1;-1;23;23;-1,,9/25/19,0,0,0,0,0,0,134;28;28;422;1966,36;13;10;36;132,6;2;2;9;24,7;1;2;36;109,f;m
5451,ICLR,2020,Multi-task Network Embedding with Adaptive Loss Weighting,Fatemeh Salehi Rizi;Michael Granitzer,fatemeh.salehirizi@uni-passau.de;michael.granitzer@uni-passau.de,3;1;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,University of Passau;University of Passau,233;233,146;146,,9/25/19,0,0,0,0,0,0,54;109,12;15,4;4,3;6,f;m
5452,ICLR,2020,Perception-Driven Curiosity with Bayesian Surprise,Bernadette Bucher;Anton Arapin;Ramanan Sekar;Marc Badger;Feifei Duan;Oleh Rybkin;Kostas Daniilidis,bucherb@seas.upenn.edu;aarapin@fandm.edu;ramanans@seas.upenn.edu;mbadger@seas.upenn.edu;feifeid@seas.upenn.edu;oleh@seas.upenn.edu;kostas@seas.upenn.edu,1;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,University of Pennsylvania;Franklin & Marshall College;University of Pennsylvania;University of Pennsylvania;University of Pennsylvania;University of Pennsylvania;University of Pennsylvania,19;-1;19;19;19;19;19,11;-1;11;11;11;11;11,,9/25/19,1,0,0,0,0,0,62;1;116;97;1;24;10820,21;1;19;9;5;12;343,3;1;6;4;1;3;55,2;0;3;6;0;1;937,f;m
5453,ICLR,2020,Global reasoning network for image super-resolution,Jiahui Zhang;Bin Zhou;Qingchang Tao;Deqiang Wang,jhzhang988@gmail.com;binzhou@sdu.edu.cn;taoqingchang@mail.tsinghua.edu.cn;wdq_sdu@sdu.edu.cn,1;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,1,2,,yes,9/25/19,Shandong University;Shandong University;Tsinghua University;Shandong University,154;154;8;154,658;658;23;658,10,9/25/19,0,0,0,0,0,0,202;21;14;18,52;22;8;20,8;3;3;2,26;0;0;1,u;u
5454,ICLR,2020,MUSE: Multi-Scale Attention Model for Sequence to Sequence Learning,Guangxiang Zhao;Xu Sun;Jingjing Xu;Zhiyuan Zhang;Liangchen Luo,1701214310@pku.edu.cn;xusun@pku.edu.cn;jingjingxu@pku.edu.cn;zzy1210@pku.edu.cn;luolc@pku.edu.cn,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:N/A:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,6,,yes,9/25/19,Peking University;Peking University;Peking University;Peking University;Peking University,22;22;22;22;22,24;24;24;24;24,3,9/25/19,0,0,0,0,0,0,14;3257;74;65;176,8;128;22;48;7,3;19;4;5;4,1;55;0;1;36,m;m
5455,ICLR,2020,Utility Analysis of Network Architectures for 3D Point Cloud Processing,Shikun Huang;Binbin Zhang;Wen Shen;Zhihua Wei;Quanshi Zhang,hsk@tongji.edu.cn;0206zbb@tongji.edu.cn;1810068@tongji.edu.cn;zhihua_wei@tongji.edu.cn;zqs1022@sjtu.edu.cn,6;3;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University;Shanghai Jiao Tong University,8;8;8;8;53,23;23;23;23;157,4;1,9/25/19,1,0,0,0,0,0,26;137;3419;405;1229,6;11;197;102;77,2;6;30;10;17,1;4;170;27;45,m;m
5456,ICLR,2020,Reducing Sentiment Bias in Language Models via Counterfactual Evaluation,Po-Sen Huang;Huan Zhang;Ray Jiang;Robert Stanforth;Johannes Welbl;Jack Rae;Vishal Maini;Dani Yogatama;Pushmeet Kohli,posenhuang@google.com;huan@huan-zhang.com;rayjiang@google.com;stanforth@google.com;j.welbl@cs.ucl.ac.uk;jwrae@google.com;vmaini@google.com;dyogatama@google.com;pushmeet@google.com,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Withdrawn,0,6,,yes,9/25/19,"Google;University of California, Los Angeles;Google;Google;University College London;Google;Google;Google;Google",-1;20;-1;-1;50;-1;-1;-1;-1,-1;17;-1;-1;15;-1;-1;-1;-1,3;7,9/25/19,7,0,0,0,0,0,1847;2162;64;543;1034;688;39;3801;24235,59;31;11;19;20;19;2;41;315,17;20;5;9;9;11;2;21;71,257;297;4;63;275;84;3;417;2804,m;m
5457,ICLR,2020,BERT for Sequence-to-Sequence Multi-Label Text Classification,Ramil Yarullin;Pavel Serdyukov,ramly@ya.ru;pavel.serdyukov@gmail.com,3;3;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,4,,yes,9/25/19,Yandex;,-1;-1,-1;-1,,9/25/19,1,0,0,0,0,0,12;3221,9;151,1;28,0;240,m;m
5458,ICLR,2020,Boosting Generative Models by Leveraging Cascaded Meta-Models,Fan Bao;Hang Su;Jun Zhu,bf19@mails.tsinghua.edu.cn;suhangss@mail.tsinghua.edu.cn;dcszj@mail.tsinghua.edu.cn,1;6;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Tsinghua University;Tsinghua University;Tsinghua University,8;8;8,23;23;23,5;1,5/11/19,1,0,0,0,0,0,231;7151;5025,28;412;203,6;34;37,5;518;530,m;m
5459,ICLR,2020,RAT-SQL: Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers,Bailin Wang*;Richard Shin*;Xiaodong Liu;Oleksandr Polozov;Matthew Richardson,bailin.wang@ed.ac.uk;ricshin@cs.berkeley.edu;xiaodl@microsoft.com;polozov@microsoft.com;mattri@microsoft.com,6;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,4,,yes,9/25/19,University of Edinburgh;University of California Berkeley;Microsoft;Microsoft;Microsoft,33;5;-1;-1;-1,30;13;-1;-1;-1,3;8,9/25/19,6,0,0,0,0,0,92;298;1839;577;528,7;29;171;22;25,4;10;20;9;7,17;29;243;47;49,m;m
5460,ICLR,2020,Shape Features Improve General Model Robustness,Chaowei Xiao;Mingjie Sun;Haonan Qiu;Han Liu;Mingyan Liu;Bo Li,xiaocw@umich.edu;mingjies@andrew.cmu.edu;haonanqiu@link.cuhk.edu.cn;hanliu@northwestern.edu;mingyan@umich.ed;lxbosky@gmail.com,1;1;6,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,University of Michigan;Carnegie Mellon University;Tsinghua University;Northwestern University;;University of California Berkeley,8;1;8;44;-1;5,21;27;23;22;-1;13,5;4,9/25/19,0,0,0,0,0,0,1449;458;38;13;234;266,31;34;10;28;30;147,13;9;3;2;5;9,151;55;2;0;12;32,m;f
5461,ICLR,2020,The Secret Revealer: Generative Model Inversion Attacks Against Deep Neural Networks,Yuheng Zhang;Ruoxi Jia;Hengzhi Pei;Wenxiao Wang;Bo Li;Dawn Song,16307130075@fudan.edu.cn;ruoxijia@berkeley.edu;hzpei16@fudan.edu.cn;wangwx16@mails.tsinghua.edu.cn;lxbosky@gmail.com,6;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Fudan University;University of California Berkeley;Fudan University;Tsinghua University;University of California Berkeley,79;5;79;8;5,109;13;109;23;13,5;4;1;2,9/25/19,6,0,0,0,0,0,84;424;10;178;266;40714,27;36;5;19;147;279,6;13;2;6;9;100,5;27;2;4;32;3986,m;f
5462,ICLR,2020,Common sense and Semantic-Guided Navigation via Language in Embodied Environments,Dian Yu;Chandra Khatri;Alexandros Papangelis;Mahdi Namazifar;Andrea Madotto;Huaixiu Zheng;Gokhan Tur,dian.yu@uber.com;chandrak@uber.com;apapangelis@uber.com;mahdin@uber.com;amadotto@connect.ust.hk;huaixiu.zheng@uber.com;gokhan@uber.com,1;1;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,0,,yes,9/25/19,Uber;Uber;Uber;Uber;The Hong Kong University of Science and Technology;Uber;Uber,-1;-1;-1;-1;39;-1;-1,-1;-1;-1;-1;47;-1;-1,,9/25/19,0,0,0,0,0,0,51;324;290;95;398;1054;1318,12;29;48;17;42;41;58,3;8;9;4;10;14;14,3;22;13;6;53;38;106,f;m
5463,ICLR,2020,TransINT: Embedding Implication Rules in Knowledge Graphs with Isomorphic Intersections of Linear Subspaces,So Yeon Min;Preethi Raghavan;Peter Szolovits,symin95@mit.edu;praghav@us.ibm.com;psz@mit.edu,3;3;1,I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,2,3,,yes,9/25/19,Massachusetts Institute of Technology;International Business Machines;Massachusetts Institute of Technology,2;-1;2,5;-1;5,10,9/25/19,0,0,0,0,0,0,39;433;9683,14;27;247,4;9;43,0;17;600,u;m
5464,ICLR,2020,Meta Label Correction for Learning with Weak Supervision,Guoqing Zheng;Ahmed Hassan Awadallah;Susan Dumais,zheng@microsoft.com;hassanam@microsoft.com;sdumais@microsoft.com,3;3;8;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,1,,yes,9/25/19,Microsoft;Microsoft;Microsoft,-1;-1;-1,-1;-1;-1,6,9/25/19,1,0,0,0,0,0,753;2356;36306,41;121;318,11;26;82,67;178;3499,m;f
5465,ICLR,2020,Towards Effective and Efficient Zero-shot Learning by Fine-tuning with  Task Descriptions,Tian Jin*;Zhun Liu*;Shengjia Yan;Alexandre Eichenberger;Louis-Philippe Morency,tian.jin1@ibm.com;zhunl@andrew.cmu.edu;sjyan@nyu.edu;alexe@us.ibm.com;morency@cs.cmu.edu,3;1;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,International Business Machines;Carnegie Mellon University;New York University;International Business Machines;Carnegie Mellon University,-1;1;25;-1;1,-1;27;29;-1;27,3;6,9/25/19,0,0,0,0,0,0,0;85;5;1438;11542,0;33;3;57;327,0;5;1;21;55,0;1;1;113;1147,u;m
5466,ICLR,2020,Multi-hop Question Answering via Reasoning Chains,Jifan Chen;Shih-ting Lin;Greg Durrett,jfchen@cs.utexas.edu;j0717lin@gmail.com;gdurrett@cs.utexas.edu,6;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,4,,yes,9/25/19,"University of Texas, Austin;;University of Texas, Austin",22;-1;22,38;-1;38,,9/25/19,8,0,0,0,0,0,214;167;1306,15;55;49,6;7;17,21;6;162,m;m
5467,ICLR,2020,Factorized Multimodal Transformer for Multimodal Sequential Learning,Amir Zadeh;Chengfeng Mao;Jiaxin Shi;Yiwei Zhang;Paul Pu Liang;Soujanya Poria;Louis-Philippe Morency,abagherz@andrew.cmu.edu;chengfem@andrew.cmu.edu;jiaxins1@andrew.cmu.edu;yiweizh2@andrew.cmu.edu;pliang@cs.cmu.edu;sporia@ntu.edu.sg;morency@cs.cmu.edu,1;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;National Taiwan University;Carnegie Mellon University,1;1;1;1;1;86;1,27;27;27;27;27;120;27,8,9/25/19,0,0,0,0,0,0,1547;0;0;78;672;378;11542,40;2;1;14;44;11;327,16;0;0;4;13;2;55,255;0;0;1;109;26;1147,m;m
5468,ICLR,2020,Faster and Just As Accurate: A Simple Decomposition for Transformer Models,Qingqing Cao;Harsh Trivedi;Aruna Balasubramanian;Niranjan Balasubramanian,qicao@cs.stonybrook.edu;hjtrivedi@cs.stonybrook.edu;arunab@cs.stonybrook.edu;niranjan@cs.stonybrook.edu,6;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Withdrawn,1,3,,yes,9/25/19,"State University of New York, Stony Brook;State University of New York, Stony Brook;State University of New York, Stony Brook;State University of New York, Stony Brook",41;41;41;41,304;304;304;304,3,9/25/19,3,0,0,0,0,0,112;214;6612;1760,23;41;66;63,5;8;19;15,10;8;749;186,m;m
5469,ICLR,2020,Learning Function-Specific Word Representations,Daniela Gerz;Ivan Vulić;Marek Rei;Roi Reichart;Anna Korhonen,dsg40@cam.ac.uk;iv250@cam.ac.uk;marek.rei@cl.cam.ac.uk;roiri@technion.ac.il;alk23@cam.ac.uk,3;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,University of Cambridge;University of Cambridge;University of Cambridge;Technion;University of Cambridge,71;71;71;26;71,3;3;3;412;3,,9/25/19,0,0,0,0,0,0,286;2401;768;2892;431,14;139;42;105;50,7;27;13;24;9,52;291;69;353;42,f;f
5470,ICLR,2020,Attention over Parameters for Dialogue Systems,Andrea Madotto;Zhaojiang Lin;Chien-Sheng Wu;Jamin Shin;Pascale Fung,amadotto@connect.ust.hk;zlinao@connect.ust.hk;wu.jason@salesforce.com;jay.shin@connect.ust.hk;pascale@ece.ust.hk,1;3;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,The Hong Kong University of Science and Technology;The Hong Kong University of Science and Technology;SalesForce.com;The Hong Kong University of Science and Technology;The Hong Kong University of Science and Technology,39;39;-1;39;39,47;47;-1;47;47,,9/25/19,0,0,0,0,0,0,398;102;420;102;235,42;20;30;15;50,10;6;10;5;8,53;8;61;9;35,f;f
5471,ICLR,2020,"RL-ST: Reinforcing Style, Fluency and Content Preservation for Unsupervised Text Style Transfer",Bhargav Upadhyay;Akhilesh Sudhakar;Arjun Maheswaran,bhargav@agaralabs.com;akhilesh@agaralabs.com;arjun@agaralabs.com,3;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:N/A:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Agara;Agara;Agara,-1;-1;-1,-1;-1;-1,3,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1,-1;-1;-1,-1;-1;-1,0;0;0,m;m
5472,ICLR,2020,Extreme Language Model Compression with Optimal Subwords and Shared Projections,Sanqiang Zhao;Raghav Gupta;Yang Song;Denny Zhou,sanqiang.zhao@pitt.edu;raghavgupta@google.com;yangso@google.com;dennyzhou@google.com,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,3,,yes,9/25/19,University of Pittsburgh;Google;Google;Google,79;-1;-1;-1,113;-1;-1;-1,3,9/25/19,17,0,0,0,0,0,1238;475;141;74,33;44;31;11,12;10;6;5,154;54;12;10,m;m
5473,ICLR,2020,Distilling the Knowledge of BERT for Text Generation,Yen-Chun Chen;Zhe Gan;Yu Cheng;Jingzhou Liu;Jingjing Liu,yen-chun.chen@microsoft.com;zhe.gan@microsoft.com;yu.cheng@microsoft.com;jingzhol@andrew.cmu.edu;jingjl@microsoft.com,1;6;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Microsoft;Microsoft;Microsoft;Carnegie Mellon University;Microsoft,-1;-1;-1;1;-1,-1;-1;-1;27;-1,3,9/25/19,10,0,0,0,0,0,308;2385;1145;186;186,11;85;21;7;7,6;26;11;3;3,51;330;37;34;34,m;f
5474,ICLR,2020,DCTD: Deep Conditional Target Densities for Accurate Regression,Fredrik K. Gustafsson;Martin Danelljan;Goutam Bhat;Thomas B. Schön,fredrik.gustafsson@it.uu.se;martin.danelljan@vision.ee.ethz.ch;goutam.bhat@vision.ee.ethz.ch;thomas.schon@it.uu.se,3;6;1,I do not know much about this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Uppsala University;Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;Uppsala University,154;10;10;154,102;13;13;102,2,9/25/19,3,0,0,0,0,0,21;7882;1581;4699,5;68;23;234,2;23;10;34,4;1860;445;352,m;m
5475,ICLR,2020,A Gradient-based Architecture HyperParameter Optimization Approach,Zechun Liu;Xiangyu Zhang;Zhe Li;Yichen Wei;Kwang-Ting Cheng;Jian Sun,zliubq@connect.ust.hk;zhangxiangyu@megvii.com;lizhe@megvii.com;weiyichen@megvii.com;timcheng@ust.hk;sunjian@megvii.com,3;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Withdrawn,0,0,,yes,9/25/19,The Hong Kong University of Science and Technology;Megvii Technology Inc.;Megvii Technology Inc.;Megvii Technology Inc.;The Hong Kong University of Science and Technology;Megvii Technology Inc.,39;-1;-1;-1;39;-1,47;-1;-1;-1;47;-1,,9/25/19,0,0,0,0,0,0,343;66943;103;9307;353;4376,18;330;26;80;43;231,7;46;2;38;11;28,63;12294;8;1582;24;304,f;m
5476,ICLR,2020,Generalizing Natural Language Analysis through Span-relation Representations,Zhengbao Jiang;Wei Xu;Jun Araki;Graham Neubig,zhengbaj@cs.cmu.edu;weixu@cse.ohio-state.edu;jun.araki@us.bosch.com;gneubig@cs.cmu.edu,3;3;6,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,1,,yes,9/25/19,Carnegie Mellon University;;Bosch;Carnegie Mellon University,1;-1;-1;1,27;-1;-1;27,3,9/25/19,0,0,0,0,0,0,70;1700;218;5539,12;199;26;443,5;15;9;38,9;152;23;566,m;m
5477,ICLR,2020,On the Distribution of Penultimate Activations of Classification Networks,Minkyo Seo;Yoonho Lee;Suha Kwak,mkseo@postech.ac.kr;einet89@gmail.com;suha.kwak@postech.ac.kr,1;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,POSTECH;;POSTECH,118;-1;118,146;-1;146,5,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1,-1;-1;-1,-1;-1;-1,0;0;0,m;m
5478,ICLR,2020,Learning to Sit: Synthesizing Human-Chair Interactions via Hierarchical Control,Yu-Wei Chao;Jimei Yang;Weifeng Chen;Jia Deng,ychao@nvidia.com;jimyang@adobe.com;wfchen@umich.edu;jiadeng@princeton.edu,3;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,NVIDIA;Adobe Systems;University of Michigan;Princeton University,-1;-1;8;31,-1;-1;21;6,,8/20/19,1,0,0,0,0,0,974;5727;1119;95,23;75;97;26,12;36;18;5,139;872;87;1,m;m
5479,ICLR,2020,"Unsupervised Few-shot Object Recognition by Integrating Adversarial, Self-supervision, and Deep Metric Learning of Latent Parts",Khoi Nguyen;Sinisa Todorovic,nguyenkh@oregonstate.edu;sinisa@oregonstate.edu,1;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,0,,yes,9/25/19,Oregon State University;Oregon State University,77;77,373;373,5;4;6,9/25/19,0,0,0,0,0,0,930;4795,62;152,17;39,51;355,m;m
5480,ICLR,2020,BERT Wears GloVes: Distilling Static Embeddings from Pretrained Contextual Representations,Rishi Bommasani;Kelly Davis;Claire Cardie,rb724@cornell.edu;kdavis@mozilla.com;cardie@cs.cornell.edu,3;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Cornell University;Mozilla;Cornell University,7;-1;7,19;-1;19,3,9/25/19,0,0,0,0,0,0,2;158;15913,4;17;224,1;6;57,0;7;1481,m;f
5481,ICLR,2020,Mixed Setting Training Methods for Incremental Slot-Filling Tasks,Daniel C. Michelin;Jonathan K. Kummerfeld;Kevin Leach;Stefan Larson;Yunqi Zhang;Joeseph J. Peper,daniel@clinc.com;jkk@clinc.com;kevin.leach@clinc.com;slars@clinc.com;yunqi@clinc.com;joe@clinc.com,1;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Clinc;Clinc;Clinc;Clinc;Clinc;Clinc,233;233;233;233;233;233,-1;-1;-1;-1;-1;-1,,9/25/19,0,0,0,0,0,0,0;620;269;13;788;0,3;39;19;5;62;1,0;15;6;2;11;0,0;65;14;5;62;0,m;m
5482,ICLR,2020,PLEX: PLanner and EXecutor for Embodied Learning in Navigation,Gil Avraham;Yan Zuo;Tom Drummond,gil.avraham@monash.edu;yan.zuo@monash.edu;tom.drummond@monash.edu,3;1;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Monash University;Monash University;Monash University,118;118;118,75;75;75,,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1,-1;-1;-1,-1;-1;-1,0;0;0,m;m
5483,ICLR,2020,UniLoss: Unified Surrogate Loss by Adaptive Interpolation,Lanlan Liu;Mingzhe Wang;Jia Deng,llanlan@umich.edu;mingzhew@cs.princeton.edu;jiadeng@princeton.edu,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,University of Michigan;Princeton University;Princeton University,8;31;31,21;6;6,,9/25/19,1,0,0,0,0,0,6;22;3,4;9;6,2;3;1,0;4;0,f;m
5484,ICLR,2020,MobileBERT: Task-Agnostic Compression of BERT by Progressive Knowledge Transfer,Zhiqing Sun;Hongkun Yu;Xiaodan Song;Renjie Liu;Yiming Yang;Denny Zhou,zhiqings@andrew.cmu.edu;hongkuny@google.com;xiaodansong@google.com;renjieliu@google.com;yiming@cs.cmu.edu;dennyzhou@google.com,6;3;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,8,,yes,9/25/19,Carnegie Mellon University;Google;Google;Google;Carnegie Mellon University;Google,1;-1;-1;-1;1;-1,27;-1;-1;-1;27;-1,3,9/25/19,5,0,0,0,0,0,201;96;4464;14;23549;74,14;15;48;8;273;11,6;4;17;2;51;5,72;12;307;4;2656;10,m;m
5485,ICLR,2020,Interactive Classification by Asking Informative Questions,Lili Yu;Howard Chen;Sida I. Wang;Yoav Artzi;Tao Lei,liliyu@asapp.com;hchen@asapp.com;sidaw@cs.princeton.edu;yoav@cs.cornell.edu;tao@asapp.com,6;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,ASAPP Inc;ASAPP Inc;Princeton University;Cornell University;ASAPP Inc,-1;-1;31;7;-1,-1;-1;6;19;-1,3,9/25/19,0,0,0,0,0,0,118;66;0;2288;1306,31;4;5;45;34,6;1;0;23;14,12;18;0;334;224,f;m
5486,ICLR,2020,Cross-Lingual Vision-Language Navigation,An Yan;Xin Wang;Jiangtao Feng;Lei Li;William Wang,ayan@ucsd.edu;xwang@cs.ucsb.edu;fengjiangtao@bytedance.com;lileilab@bytedance.com;william@cs.ucsb.edu,6;3;1,I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,"University of California, San Diego;UC Santa Barbara;Bytedance;Bytedance;UC Santa Barbara",11;38;-1;-1;38,31;57;-1;-1;57,3;4;6,9/25/19,0,0,0,0,0,0,809;529;31;-1;2423,32;165;11;-1;128,5;10;3;-1;29,53;40;1;0;289,f;m
5487,ICLR,2020,Single Deep Counterfactual Regret Minimization,Eric Steinberger,ericsteinberger.est@gmail.com,3;6;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,Massachusetts Institute of Technology,2,5,,1/22/19,8,0,0,0,0,0,10,3,2,2,m
5488,ICLR,2020,I love your chain mail! Making knights smile in a fantasy game world,Shrimai Prabhumoye;Margaret Li;Jack Urbanek;Emily Dinan;Douwe Kiela;Jason Weston;Arthur Szlam,sprabhum@cs.cmu.edu;margaretli@fb.com;jju@fb.com;edinan@fb.com;dkiela@fb.com;jase@fb.com;aszlam@fb.com,3;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,0,,yes,9/25/19,Carnegie Mellon University;Facebook;Facebook;Facebook;Facebook;Facebook;Facebook,1;-1;-1;-1;-1;-1;-1,27;-1;-1;-1;-1;-1;-1,,9/25/19,3,0,0,0,0,0,281;79;369;606;3572;49317;9288,22;6;8;19;80;242;87,5;4;5;8;30;79;35,55;19;78;127;604;5926;971,f;m
5489,ICLR,2020,DOUBLE-HARD DEBIASING: TAILORING WORD EMBEDDINGS FOR GENDER BIAS MITIGATION,Tianlu Wang;Xi Victoria Lin;Nazneen Fatema Rajani;Vicente Ordonez;Caimng Xiong,tianlu@virginia.edu;xilin@salesforce.com;nazneen.rajani@salesforce.com;vicente@virginia.edu;cxiong@salesforce.com,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Withdrawn,0,0,,yes,9/25/19,University of Virginia;SalesForce.com;SalesForce.com;University of Virginia;SalesForce.com,59;-1;-1;59;-1,107;-1;-1;107;-1,3;7,9/25/19,1,0,0,0,0,0,706;260;148;4226;1,40;11;20;56;1,13;8;6;20;1,72;36;11;492;0,f;m
5490,ICLR,2020,Joint text classification on multiple levels with multiple labels,Miruna Pîslar;Marek Rei,miruna.pislar@gmail.com;marek.rei@cl.cam.ac.uk,1;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,University of Cambridge;University of Cambridge,71;71,3;3,3;6,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1,-1;-1,-1;-1,0;0,f;m
5491,ICLR,2020,Discrete Transformer,Jambay Kinley;Yuntian Deng;Alexander M. Rush,j_kinley@college.harvard.edu;dengyuntian@seas.harvard.edu;arush@cornell.edu,3;3;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,Harvard University;Harvard University;Cornell University,39;39;7,7;7;19,3,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1,-1;-1;-1,-1;-1;-1,0;0;0,m;m
5492,ICLR,2020,Pragmatic Evaluation of Adversarial Examples in Natural Language,John Morris;Eli Lifland;Ji Gao;Jack Lanchantin;Yangfeng Ji;Yanjun Qi,jm8wx@virginia.edu;edl9cy@virginia.edu;jg6yd@virginia.edu;jjl5sw@virginia.edu;yj3fs@virginia.edu;yq2h@virginia.edu,6;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Withdrawn,1,3,,yes,9/25/19,University of Virginia;University of Virginia;University of Virginia;University of Virginia;University of Virginia;University of Virginia,59;59;59;59;59;59,107;107;107;107;107;107,3;4,9/25/19,0,0,0,0,0,0,-1;0;512;930;1978;1143,-1;5;22;25;33;24,-1;0;5;8;17;8,0;0;15;35;218;42,m;f
5493,ICLR,2020,Question Generation from Paragraphs: A Tale of Two Hierarchical Models,Vishwajeet Kumar;Raktim Chaki;Sai Teja Talluri;Ganesh Ramakrishnan;Yuan-Fang Li;Gholamreza Haffari,vishwajeet@cse.iitb.ac.in;raktimchaki@cse.iitb.ac.in;saiteja.talluri@gmail.com;ganesh@cse.iitb.ac.in;yuanfang.li@monash.edu;gholamreza.haffari@monash.edu,1;1;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Indian Institute of Technology Bombay;Indian Institute of Technology Bombay;Indian Institute of Technology Bombay;Indian Institute of Technology Bombay;Monash University;Monash University,118;118;118;118;118;118,480;480;480;480;75;75,,9/25/19,0,0,0,0,0,0,35;7;0;1237;1004;1889,14;2;1;140;102;132,4;1;0;16;15;24,3;2;0;90;84;156,m;m
5494,ICLR,2020,Should All Cross-Lingual Embeddings Speak English?,Antonios Anastasopoulos;Graham Neubig,aanastas@andrew.cmu.edu;gneubig@cs.cmu.edu,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Carnegie Mellon University;Carnegie Mellon University,1;1,27;27,3,9/25/19,1,0,0,0,0,0,648;5539,41;443,11;38,53;566,m;m
5495,ICLR,2020,Couple-VAE: Mitigating the Encoder-Decoder Incompatibility in Variational Text Modeling with Coupled Deterministic Networks,Chen Wu;Prince Zizhuang Wang;William Yang Wang,wu-c16@mails.tsinghua.edu.cn;zizhuang_wang@ucsb.edu;william@cs.ucsb.edu,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,Tsinghua University;UC Santa Barbara;UC Santa Barbara,8;38;38,23;57;57,5,9/25/19,0,0,0,0,0,0,44;408;10,34;42;23,3;11;2,0;25;3,m;m
5496,ICLR,2020,Task-agnostic Continual Learning via Growing Long-Term Memory Networks,Germán Kruszewski;Ionut Teodor Sorodoc;Tomas Mikolov,germank@gmail.com;ionutteodor.sorodoc@upf.edu;tmikolov@fb.com,6;6;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:N/A:N/A,Withdrawn,0,0,,yes,9/25/19,;Universitat Pompeu Fabra;Facebook,-1;481;-1,-1;141;-1,3;2,9/25/19,0,0,0,0,0,0,1782;27;-1,26;8;-1,13;3;-1,189;0;0,m;m
5497,ICLR,2020,Exploring the Pareto-Optimality between Quality and Diversity in Text Generation,Jianing Li;Yanyan Lan;Jiafeng Guo;Xueqi Cheng,lijianing@ict.ac.cn;lanyanyan@ict.ac.cn;guojiafeng@ict.ac.cn;cxq@ict.ac.cn,1;3;3,I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,"Institute of Computing Technology, Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences;Institute of Computing Technology, Chinese Academy of Sciences",59;59;59;59,1397;1397;1397;1397,1,9/25/19,0,0,0,0,0,0,15;2797;1470;89,13;124;66;26,3;25;13;4,1;449;150;4,u;m
5498,ICLR,2020,An Empirical Study of Encoders and Decoders in Graph-Based Dependency Parsing,Ge Wang;Ziyuan Hu;Zechuan Hu;Kewei Tu,wangge@shanghaitech.edu.cn;huzy@shanghaitech.edu.cn;huzch@shanghaitech.edu.cn;tukw@shanghaitech.edu.cn,3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,0,,yes,9/25/19,ShanghaiTech University;ShanghaiTech University;ShanghaiTech University;ShanghaiTech University,481;481;481;481,1397;1397;1397;1397,,9/25/19,0,0,0,0,0,0,46;23;1;504,18;5;2;58,4;1;1;11,0;0;0;29,m;m
5499,ICLR,2020,Hierarchical Summary-to-Article Generation,Wangchunshu Zhou;Tao Ge;Ke Xu;Furu Wei;Ming Zhou,v-waz@microsoft.com;tage@microsoft.com;kexu@nlsde.buaa.edu.cn;fuwei@microsoft.com;mingzhou@microsoft.com,3;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,4,,yes,9/25/19,Microsoft;Microsoft;Beihang University;Microsoft;Microsoft,-1;-1;118;-1;-1,-1;-1;594;-1;-1,3,9/25/19,0,0,0,0,0,0,22;99;335;7663;2100,12;39;119;165;204,3;6;9;45;19,2;4;16;809;56,m;m
5500,ICLR,2020,Anomaly Detection by Deep Direct Density Ratio Estimation,Masahiro Abe;Masashi Sugiyama,masahiro.abe@d2c.co.jp;sugi@k.u-tokyo.ac.jp,3;3;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,;The University of Tokyo,-1;56,-1;36,,9/25/19,1,0,0,0,0,0,1;13178,6;710,1;57,0;1306,m;m
5501,ICLR,2020,Generating Biased Datasets for Neural Natural Language Processing,Alvin Chan;Yi Tay;Yew Soon Ong;Aston Zhang,guoweial001@e.ntu.edu.sg;ytay017@e.ntu.edu.sg;asysong@ntu.edu.sg;astonz@amazon.com,3;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,National Taiwan University;National Taiwan University;National Taiwan University;Amazon,86;86;86;-1,120;120;120;-1,3;5;7,9/25/19,0,0,0,0,0,0,1772;1455;9656;667,133;69;358;42,19;19;48;13,111;171;489;51,m;m
5502,ICLR,2020,3D-SIC: 3D Semantic Instance Completion for RGB-D Scans,Ji Hou;Angela Dai;Matthias Niessner,ji.hou@tum.de;angela.dai@tum.de;niessner@tum.de,6;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,0,,yes,9/25/19,Technical University Munich;Technical University Munich;Technical University Munich,53;53;53,43;43;43,,9/25/19,3,0,0,0,0,0,80;2818;22340,11;36;142,3;16;39,9;438;1810,m;m
5503,ICLR,2020,Towards Holistic and Automatic Evaluation of Open-Domain Dialogue Generation,Bo Pang;Erik Nijkamp;Wenjuan Han;Alex Zhou,bopang@g.ucla.edu;erik.nijkamp@gmail.com;hanwj0309@gmail.com;alexzhou907@gmail.com,1;1;3,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,"University of California, Los Angeles;;;",20;-1;-1;-1,17;-1;-1;-1,3,9/25/19,0,0,0,0,0,0,17;783;49;64,16;25;14;8,2;9;4;3,0;90;3;4,m;m
5504,ICLR,2020,Semi-Supervised Semantic Dependency Parsing Using CRF Autoencoders,Zixia Jia;Youmi Ma;Jiong Cai;Kewei Tu,jiazx@shanghaitech.edu.cn;maym@shanghaitech.edu.cn;caijiong@shanghaitech.edu.cn;tukw@shanghaitech.edu.cn,3;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,ShanghaiTech University;ShanghaiTech University;ShanghaiTech University;ShanghaiTech University,481;481;481;481,1397;1397;1397;1397,5;10,9/25/19,0,0,0,0,0,0,2;0;2;504,2;1;7;58,1;0;1;11,0;0;0;29,m;m
5505,ICLR,2020,A Syntax-Aware Approach for Unsupervised Text Style Transfer,Yun Ma;Yangbin Chen;Xudong Mao;Qing Li,mayun371@gmail.com;robinchen2-c@my.cityu.edu.hk;xudong.xdmao@gmail.com;qing-prof.li@polyu.edu.hk,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,The Hong Kong Polytechnic University;City University of Hong Kong;The Hong Kong Polytechnic University;The Hong Kong Polytechnic University,172;92;172;172,171;35;171;171,,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1;-1,-1;-1;-1;-1,-1;-1;-1;-1,0;0;0;0,m;m
5506,ICLR,2020,Contextualized Sparse Representation with Rectified N-Gram Attention for Open-Domain Question Answering,Jinhyuk Lee;Minjoon Seo;Hannaneh Hajishirzi;Jaewoo Kang,jinhyuk_lee@korea.ac.kr;minjoon@cs.washington.edu;hannaneh@washington.edu;kangj@korea.ac.kr,6;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,4,,yes,9/25/19,Korea University;University of Washington;University of Washington;Korea University,323;6;6;323,179;26;26;179,,9/25/19,0,0,0,0,0,0,416;34;2999;2953,12;8;98;164,5;2;25;24,104;9;607;311,m;m
5507,ICLR,2020,Recurrent Layer Attention Network,Eunseok Kim;Inwook Shim,eunseok1117@gmail.com;inugi00@gmail.com,1;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Withdrawn,2,1,,yes,9/25/19,Agency for Defense Development;,-1;-1,-1;-1,2,9/25/19,0,0,0,0,0,0,161;221,77;25,7;7,11;15,u;m
5508,ICLR,2020,Super-AND: A Holistic Approach to Unsupervised Embedding Learning,Sungwon Han;Yizhan Xu;Sungwon Park;Meeyoung Cha;Cheng-Te Li,lion4151@kaist.ac.kr;re6071020@gs.ncku.edu.tw;psw0416@kaist.ac.kr;mcha@ibs.re.kr;chengte@mail.ncku.edu.tw,1;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,1,,yes,9/25/19,Korea Advanced Institute of Science and Technology;Peking University;Korea Advanced Institute of Science and Technology;Institute for Basic Science;Peking University,481;22;481;-1;22,110;24;110;-1;24,,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,0;0;0;0;0,m;m
5509,ICLR,2020,Natural Language State Representation for Reinforcement Learning,Erez Schwartz;Guy Tennenholtz;Chen Tessler;Shie Mannor,erezschwartz@campus.technion.ac.il;sguyt@campus.technion.ac.il;chen.tessler@gmail.com;shiemannor@gmail.com,1;1;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,1,,yes,9/25/19,Technion;Technion;Technion;Technion,26;26;26;26,412;412;412;412,3,9/25/19,0,0,0,0,0,0,0;34;262;12549,6;11;10;420,0;4;5;51,0;3;17;1227,m;m
5510,ICLR,2020,BEAN: Interpretable Representation Learning with Biologically-Enhanced Artificial Neuronal Assembly Regularization,Yuyang Gao;Giorgio Ascoli;Liang Zhao,ygao13@gmu.edu;ascoli@gmu.edu;lzhao9@gmu.edu,3;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,George Mason University;George Mason University;George Mason University,100;100;100,282;282;282,,9/25/19,0,0,0,0,0,0,246;46;-1,34;19;-1,9;4;-1,11;0;0,m;m
5511,ICLR,2020,Revisiting Fine-tuning for Few-shot Learning,Akihiro Nakamura;Tatsuya Harada,nakamura@mi.t.u-tokyo.ac.jp;harada@mi.t.u-tokyo.ac.jp,1;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,The University of Tokyo;The University of Tokyo,56;56,36;36,6,9/25/19,4,0,0,0,0,0,544;2786,105;216,12;28,23;336,m;m
5512,ICLR,2020,Unrestricted Adversarial Attacks For Semantic Segmentation,Guangyu Shen;Chengzhi Mao;Junfeng Yang;Baishakhi Ray,shen447@purdue.edu;cm3797@columbia.edu;junfeng@cs.columbia.edu;rayb@cs.columbia.edu,6;6;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Purdue University;Columbia University;Columbia University;Columbia University,27;15;15;15,88;16;16;16,5;4;2,9/25/19,1,0,0,0,0,0,106;29;24;2139,17;10;12;65,6;3;2;21,6;2;2;195,m;f
5513,ICLR,2020,End-to-End Multi-Domain Task-Oriented Dialogue Systems with Multi-level Neural Belief Tracker,Hung Le;Doyen Sahoo;Chenghao Liu;Nancy F. Chen;Steven C.H. Hoi,l.hung1610@gmail.com;dsahoo@salesforce.com;chliu@smu.edu.sg;nfychen@i2r.a-star.edu.sg;shoi@salesforce.com,3;3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,0,,yes,9/25/19,Singapore Management University;SalesForce.com;Singapore Management University;A*STAR;SalesForce.com,92;-1;92;-1;-1,1397;-1;1397;-1;-1,,9/25/19,2,0,0,0,0,0,292;386;908;711;9999,35;28;50;95;298,7;11;12;15;55,19;53;73;24;859,m;m
5514,ICLR,2020,Accelerate DNN Inference  By Inter-Operator Parallelization,Yaoyao Ding;Ligeng Zhu;Zhihao Jia;Song Han,yyding@mit.edu;ligeng@mit.edu;zhihao@cs.stanford.edu;songhan@mit.edu,3;1;3,I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,0,,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Stanford University;Massachusetts Institute of Technology,2;2;4;2,5;5;4;5,,9/25/19,0,0,0,0,0,0,197;510;800;0,13;16;70;11,6;5;15;0,13;140;47;0,m;m
5515,ICLR,2020,Posterior Control of Blackbox Generation ,Xiang Lisa Li;Alexander M. Rush,xli150@jhu.edu;srush@seas.harvard.edu,3;3;6,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Johns Hopkins University;Harvard University,73;39,12;7,3,9/25/19,0,0,0,0,0,0,2205;7262,206;87,26;33,76;964,f;m
5516,ICLR,2020,Learning to Generate 3D Training Data through Hybrid Gradient,Dawei Yang;Jia Deng,ydawei@umich.edu;jiadeng@princeton.edu,8;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,University of Michigan;Princeton University,8;31,21;6,10,6/29/19,1,0,0,0,0,0,26;16159,15;81,2;29,2;2735,m;m
5517,ICLR,2020,Learning to Learn via Gradient Component Corrections,Christian Simon;Piotr Koniusz;Richard Nock;Mehrtash Harandi,christian.simon@anu.edu.au;peter.koniusz@data61.csiro.au;richard.nock@data61.csiro.au;mehrtash.harandi@monash.edu,1;3;3,I do not know much about this area.:I did not assess the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,"Australian National University;, CSIRO;, CSIRO;Monash University",108;233;233;118,50;-1;-1;75,6,9/25/19,0,0,0,0,0,0,197;947;3820;3271,29;52;239;101,6;17;29;28,11;82;329;351,m;m
5518,ICLR,2020,Counting the Paths in Deep Neural Networks as a Performance Predictor,Michele Sasdelli;Ian Reid;Gustavo Carneiro,michele.sasdelli@adelaide.edu.au;ian.reid@adelaide.edu.au;gustavo.carneiro@adelaide.edu.au,3;1;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,The University of Adelaide;The University of Adelaide;The University of Adelaide,128;128;128,120;120;120,10,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1;-1,-1;-1;-1,-1;-1;-1,0;0;0,m;m
5519,ICLR,2020,VUSFA:Variational Universal Successor Features Approximator ,Shamane Siriwardhana;Rivindu Weerasakera;Denys J.C. Matthies;Suranga Nanayakkara,shamane@ahlab.org;rivindu@ahlab.org;denys@ahlab.org;suranga@ahlab.org,1;1;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,3,,yes,9/25/19,University of Auckland;;;,266;-1;-1;-1,177;-1;-1;-1,6,8/18/19,1,0,0,0,0,0,1;0;347;838,4;2;49;102,1;0;12;14,0;0;15;43,m;m
5520,ICLR,2020,Data Annealing Transfer learning Procedure for Informal Language Understanding Tasks,Jing Gu;Yu Zhou,jkgu@ucdavis.edu;joyu@ucdavis.edu,3;3;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,"University of California, Davis;University of California, Davis",79;79,55;55,3;6,9/25/19,0,0,0,0,0,0,-1;85,-1;86,-1;5,0;0,f;f
5521,ICLR,2020,Building Hierarchical Interpretations in Natural Language via Feature Interaction Detection,Hanjie Chen;Guangtao Zheng;Yangfeng Ji,hc9mx@virginia.edu;gz5hp@virginia.edu;yangfeng@virginia.edu,3;1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,University of Virginia;University of Virginia;University of Virginia,59;59;59,107;107;107,3,9/25/19,0,0,0,0,0,0,1;3;7,5;7;4,1;1;1,0;0;1,f;m
5522,ICLR,2020,Mem2Mem: Learning to Summarize Long Texts with Memory-to-Memory Transfer,Jaehong Park;Jonathan Pilault;Christopher Pal,jaehong.park@elementai.com;jonathan.pilault@elementai.com;christopher.pal@elementai.com,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,Element AI;Element AI;Element AI,-1;-1;-1,-1;-1;-1,,9/25/19,0,0,0,0,0,0,117;10;862,36;5;58,3;1;12,6;2;72,m;m
5523,ICLR,2020,Improving Neural Abstractive Summarization Using Transfer Learning and Factuality-Based Evaluation: Towards Automating Science Journalism,Rumen Dangovski*;Michelle Shen*;Dawson Byrd*;Li Jing*;Preslav Nakov;Marin Soljacic,rumenrd@mit.edu;mcshen99@mit.edu;dbyrd@exeter.edu;ljing@mit.edu;pnakov@qf.org.qa;soljacic@mit.edu,1;1;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,Massachusetts Institute of Technology;Massachusetts Institute of Technology;Phillips Exeter Academy;Massachusetts Institute of Technology;QCRI;Massachusetts Institute of Technology,2;2;-1;2;-1;2,5;5;-1;5;-1;5,6,9/25/19,0,0,0,0,0,0,18;0;0;96;7941;487,16;2;1;79;262;50,3;0;0;4;47;9,3;0;0;3;788;32,m;m
5524,ICLR,2020,Higher-order Weighted Graph Convolutional Networks,Songtao Liu;Lingwei Chen;Hanze Dong;Zihao Wang;Dinghao Wu;Zengfeng Huang,stliu15@fudan.edu.cn;lvc5613@psu.edu;hdongaj@ust.hk;zzw166@psu.edu;duw12@psu.edu;huangzf@fudan.edu.cn,3;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,Fudan University;Pennsylvania State University;The Hong Kong University of Science and Technology;Pennsylvania State University;Pennsylvania State University;Fudan University,79;41;39;41;41;79,109;78;47;78;78;109,10,9/25/19,1,0,0,0,0,0,392;255;18;6;2145;411,38;27;6;21;104;39,7;9;3;1;25;11,61;9;1;0;184;32,m;m
5525,ICLR,2020,Distilling Neural Networks for Faster and Greener Dependency Parsing,Mark Anderson;Carlos Gómez-Rodríguez,mark.anderson.nlp@gmail.com;carlos.gomez@udc.es,3;6;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,5,,yes,9/25/19,Universidade da Coruña;Universidade da Coruña,-1;-1,-1;-1,3,9/25/19,0,0,0,0,0,0,91;1171,68;112,6;19,4;65,m;m
5526,ICLR,2020,From Here to There: Video Inbetweening Using Direct 3D Convolutions,Yunpeng Li;Dominik Roblek;Marco Tagliasacchi,yunpeng@google.com;droblek@google.com;mtagliasacchi@google.com,3;3;3,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,Google;Google;Google,-1;-1;-1,-1;-1;-1,4,5/24/19,7,0,0,0,0,0,1823;78;4005,104;23;229,21;6;34,175;6;215,m;m
5527,ICLR,2020,ON SOLVING COOPERATIVE DECENTRALIZED MARL PROBLEMS WITH SPARSE REINFORCEMENTS,Rajiv Ranjan Kumar;Pradeep Varakantham,rajivrk.2017@phdis.smu.edu.sg;pradeepv@smu.edu.sg,1;6;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Withdrawn,0,0,,yes,9/25/19,Singapore Management University;Singapore Management University,92;92,1397;1397,,9/25/19,0,0,0,0,0,0,195;2131,87;142,7;24,6;192,m;m
5528,ICLR,2020,On the Anomalous Generalization of GANs,Jinchen Xuan;Yunchang Yang;Ze Yang;Di He;Liwei Wang,1600012865@pku.edu.cn;1500010650@pku.edu.cn;yangze@pku.edu.cn;dihe@microsoft.com;wanglw@cis.pku.edu.cn,1;3;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Withdrawn,0,0,,yes,9/25/19,Peking University;Peking University;Peking University;Microsoft;Peking University,22;22;22;-1;22,24;24;24;-1;24,5;4;8,9/25/19,0,0,0,0,0,0,0;68;5;626;103,1;9;3;29;25,0;4;2;7;6,0;2;0;68;21,m;m
5529,ICLR,2020,Bridging the domain gap in cross-lingual document classification,Guokun Lai;Barlas Oguz;Yiming Yang;Veselin Stoyanov,guokun@cs.cmu.edu;barlaso@fb.com;yiming@cs.cmu.edu;ves@fb.com,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:N/A:N/A;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Carnegie Mellon University;Facebook;Carnegie Mellon University;Facebook,1;-1;1;-1,27;-1;27;-1,3,9/16/19,2,0,0,0,0,0,830;112;23549;4297,14;17;273;45,6;5;51;24,152;19;2656;715,m;m
5530,ICLR,2020,NAMSG: An Efficient Method for Training Neural Networks,Yushu Chen;Hao Jing;Wenlai Zhao;Zhiqiang Liu;Ouyi Li;Liang Qiao;Haohuan Fu;Wei Xue;Guangwen Yang,yschen11@126.com;jinghao0320@gmail.com;cryinlaugh@gmail.com;gt_liuzq@163.com;18801087946@163.com;qiaoliang6363@163.com;haohuan@tsinghua.edu.cn;xuewei@mail.tsinghua.edu.cn;ygw@mail.tsinghua.edu.cn,3;3;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Withdrawn,2,4,,yes,9/25/19,126;;;163;163;163;Tsinghua University;Tsinghua University;Tsinghua University,-1;-1;-1;-1;-1;-1;8;8;8,-1;-1;-1;-1;-1;-1;23;23;23,1,9/25/19,3,0,0,0,0,0,643;105;123;244;-1;716;2654;1637;3342,128;45;24;52;-1;114;181;195;307,15;4;5;7;-1;13;24;20;26,31;2;9;25;0;34;141;76;164,f;m
5531,ICLR,2020,INTERPRETING CNN  PREDICTION THROUGH  LAYER - WISE SELECTED DISCERNIBLE NEURONS,Md Tauhid Bin Iqbal;Abdul Muqeet;Sung-Ho Bae,tauhidiq@khu.ac.kr;amuqeet@khu.ac.kr;shbae@khu.ac.kr,1;3;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Kyung Hee University;Kyung Hee University;Kyung Hee University,481;481;481,319;319;319,,9/25/19,0,0,0,0,0,0,5;96;191,5;23;16,1;4;3,0;8;11,m;m
5532,ICLR,2020,Toward Controllable Text Content Manipulation,Shuai Lin;Wentao Wang;Zhiting Hu;Zichao Yang;Xiaodan Liang;Haoran Shi;Frank Xu;Eric Xing,shuailin97@gmail.com;wwt.cpp@gmail.com;zhitinghu@gmail.com;yangtze2301@gmail.com;xdliang328@gmail.com;haoranshi97@gmail.com;eric.xing@petuum.com,3;1;6,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,0,,yes,9/25/19,SUN YAT-SEN UNIVERSITY;Peking University;Carnegie Mellon University;;SUN YAT-SEN UNIVERSITY;Carnegie Mellon University;Petuum Inc.,481;22;1;-1;481;1;-1,299;24;27;-1;299;27;-1,,9/25/19,0,0,0,0,0,0,24;7;3398;134;2;12;112;26593,8;20;64;5;6;2;19;606,2;1;29;3;1;1;6;80,1;0;371;1;0;0;6;2726,m;m
5533,ICLR,2020,Learning Low-rank Deep Neural Networks via Singular Vector Orthogonality Regularization and Singular Value Sparsification,Huanrui Yang;Minxue Tang;Wei Wen;Feng Yan;Daniel Hu;Ang Li;Hai Li,huanrui.yang@duke.edu;tangmx16@mails.tsinghua.edu.cn;wei.wen@duke.edu;fyan@unr.edu;danielhu2003@gmail.com;ang.li630@duke.edu;hai.li@duke.edu,3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,"Duke University;Tsinghua University;Duke University;University of Nevada, Reno;Newport High School;Duke University;Duke University",47;8;47;266;-1;47;47,20;23;20;1397;-1;20;20,,9/25/19,0,0,0,0,0,0,77;-1;5;65;231;152,16;-1;12;55;26;79,6;-1;1;4;7;8,10;0;0;4;3;5,m;f
5534,ICLR,2020,FAST LEARNING VIA EPISODIC MEMORY: A PERSPECTIVE FROM ANIMAL DECISION-MAKING,Xiaohan Zhang;Lu Liu;Guodong Long;jing jiang;Shenquan Liu,xh1315255662@gmail.com;lu.liu.cs@icloud.com;guodong.long@uts.edu.au;jing.jiang@uts.edu.au;mashqliu@scut.edu.cn,1;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,0,,yes,9/25/19,South China University of Technology;University of Technology Sydney;University of Technology Sydney;University of Technology Sydney;South China University of Technology,481;108;108;108;481,501;193;193;193;501,5,9/25/19,0,0,0,0,0,0,66;164;1879;3;17,23;35;80;4;11,5;7;17;1;3,1;9;162;0;0,f;u
5535,ICLR,2020,Improving and Stabilizing Deep Energy-Based Learning,Lifu Tu;Richard Yuanzhe Pang;Kevin Gimpel,lifu@ttic.edu;yzpang@nyu.edu;kgimpel@ttic.edu,6;1;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Toyota Technological Institute at Chicago;New York University;Toyota Technological Institute at Chicago,-1;25;-1,-1;29;-1,,9/25/19,0,0,0,0,0,0,141;12;6132,13;10;99,5;2;31,29;0;843,m;m
5536,ICLR,2020,INTERPRETING CNN COMPRESSION USING INFORMATION BOTTLENECK,Hui Xiang;Feifei Shi;Peng Wang;Qigang Wang;Zhongchao Shi,xianghui1@lenovo.com;shiff3@lenovo.com;wangpeng31@lenovo.com;wangqg1@lenovo.com;shizc2@lenovo.com,1;1;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Lenovo Research;Lenovo Research;Lenovo Research;Lenovo Research;Lenovo Research,-1;-1;-1;-1;-1,-1;-1;-1;-1;-1,1;8,9/25/19,0,0,0,0,0,0,70;61;274;0;277,15;14;122;4;64,5;5;5;0;10,2;0;12;0;10,m;m
5537,ICLR,2020,Conversation Generation with Concept Flow,Houyu Zhang;Zhenghao Liu;Chenyan Xiong;Zhiyuan Liu,houyu_zhang@brown.edu;liu-zh16@mails.tsinghua.edu.cn;chenyan.xiong@microsoft.com;liuzy@tsinghua.edu.cn,6;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,0,,yes,9/25/19,Brown University;Tsinghua University;Microsoft;Tsinghua University,67;8;-1;8,53;23;-1;23,10,9/25/19,0,0,0,0,0,0,706;86;901;7353,44;14;52;134,13;4;14;37,4;9;107;1171,m;m
5538,ICLR,2020,iSOM-GSN: An Integrative Approach for Transforming Multi-omic Data into Gene Similarity Networks via Self-organizing Maps,Nazia Fatima;Johan Fernandes;Luis Rueda,fatiman@uwindsor.ca;ferna11i@uwindsor.ca;lrueda@uwindsor.ca,3;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,University of Windsor;University of Windsor;University of Windsor,-1;-1;-1,-1;-1;-1,10,9/25/19,0,0,0,0,0,0,2;-1;1047,8;-1;177,1;-1;14,0;0;65,f;m
5539,ICLR,2020,Task-Agnostic Robust Encodings for Combating Adversarial Typos,Erik Jones;Robin Jia;Aditi Raghunathan;Percy Liang,erik.jones313@gmail.com;robinjia@stanford.edu;aditir@stanford.edu;pliang@cs.stanford.edu,3;3;3,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,4,,yes,9/25/19,Stanford University;Stanford University;Stanford University;Stanford University,4;4;4;4,4;4;4;4,3;4,9/25/19,0,0,0,0,0,0,61;1539;677;195,53;16;19;27,2;9;8;7,6;293;83;11,m;m
5540,ICLR,2020,Natural Language Adversarial Attack and Defense in Word Level,Xiaosen Wang;Hao Jin;Kun He,xiaosen@hust.edu.cn;mailtojinhao@hust.edu.cn;brooklet60@hust.edu.cn,6;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,2,,yes,9/25/19,Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology,39;39;39,47;47;47,3;4;2,9/15/19,6,0,0,0,0,0,55;1120;-1,11;184;-1,5;15;-1,2;54;0,m;f
5541,ICLR,2020,Randomness in Deconvolutional Networks for Visual Representation,Kun He;Jingbo Wang;Haochuan Li;Yao Shu;Liwei Wang;John E. Hopcroft,brooklet60@hust.edu.cn;jingbow@usc.edu;lhchuan@pku.edu.cn;shuyao95@gmail.com;wanglw@cis.pku.edu.cn;jeh@cs.cornell.edu,1;3;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Hong Kong University of Science and Technology;University of Southern California;Peking University;National University of Singapore;Peking University;Cornell University,39;31;22;16;22;7,47;62;24;25;24;19,,4/2/17,1,0,0,0,0,0,66;2814;761;131;201;32135,18;178;52;29;31;303,3;26;10;7;6;64,3;171;68;4;20;2770,f;m
5542,ICLR,2020,An Inter-Layer Weight Prediction and Quantization for Deep Neural Networks based on Smoothly Varying Weight Hypothesis,Kang-Ho Lee;JoonHyun Jung;Sung-Ho Bae,ho7719@khu.ac.kr;doublejtoh@khu.ac.kr;shbae@khu.ac.kr,3;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Kyung Hee University;Kyung Hee University;Kyung Hee University,481;481;481,319;319;319,,7/16/19,1,0,0,0,0,0,21;1;452,12;3;80,1;1;13,0;0;35,m;m
5543,ICLR,2020,Generalized Transformation-based Gradient,Anbang Wu,wab@zju.edu.cn,3;1;6,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,Zhejiang University,56,107,,9/25/19,0,0,0,0,0,0,84,29,5,4,u
5544,ICLR,2020,Learning Multi-facet Embeddings of Phrases and Sentences using Sparse Coding for Unsupervised Semantic Applications,Haw-Shiuan Chang;Amol Agrawal;Andrew McCallum,hschang@cs.umass.edu;amolagrawal@cs.umass.edu;mccallum@cs.umass.edu,3;3;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,1,,yes,9/25/19,"University of Massachusetts, Amherst;University of Massachusetts, Amherst;University of Massachusetts, Amherst",28;28;28,209;209;209,3,9/25/19,0,0,0,0,0,0,315;24;53087,19;5;434,8;2;99,30;3;5393,m;m
5545,ICLR,2020,Making DenseNet Interpretable: A Case Study in Clinical Radiology,Kwun Ho Ngan;Artur d'Avila Garcez;Karen M. Knapp;Andy Appelboam;Constantino Carlos Reyes-Aldasoro,kwun-ho.ngan@city.ac.uk;a.garcez@city.ac.uk;k.m.knapp@exeter.ac.uk;andy.appelboam@nhs.net;reyes@city.ac.uk,1;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,"City, University of London;City, University of London;University of Exeter;;City, University of London",266;266;390;-1;266,422;422;146;-1;422,2,9/25/19,0,0,0,0,0,0,96;1750;724;37;1620,9;145;91;9;132,5;23;16;2;21,5;99;35;4;66,m;m
5546,ICLR,2020,Learning Classifier Synthesis for Generalized Few-Shot Learning,Han-Jia Ye;Hexiang Hu;De-Chuan Zhan;Fei Sha,yehj@lamda.nju.edu.cn;hexiang.frank.hu@gmail.com;zhandc@nju.edu.cn;feisha@usc.edu,3;3;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,1,,yes,9/25/19,Zhejiang University;University of Southern California;Zhejiang University;University of Southern California,56;31;56;31,107;62;107;62,6,6/7/19,5,0,0,0,0,0,140;539;710;9720,27;28;60;118,7;11;10;41,13;67;56;1389,m;m
5547,ICLR,2020,Restoration of Video Frames from a Single Blurred Image with Motion Understanding,Dawit Mureja Argaw;Junsik Kim;Francois Rameau;Chaoning Zhang;In so Kweon,dawitmureja@kaist.ac.kr;mibastro@gmail.com;rameau.fr@gmail.com;chaoningzhang1990@gmail.com;iskweon77@kaist.ac.kr,3;6;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology;Korea Advanced Institute of Science and Technology,481;481;481;481;481,110;110;110;110;110,,9/25/19,0,0,0,0,0,0,0;241;225;49;28,3;49;30;18;18,0;7;8;4;2,0;20;12;2;1,m;m
5548,ICLR,2020,Open-Set Domain Adaptation with Category-Agnostic Clusters,Yingwei Pan;Ting Yao;Yehao Li;Chong-Wah Ngo;Tao Mei,panyw.ustc@gmail.com;tingyao.ustc@gmail.com;yehaoli.sysu@gmail.com;cscwngo@cityu.edu.hk;tmei@live.com,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,JD AI Research;JD AI Research;SUN YAT-SEN UNIVERSITY;City University of Hong Kong;JD AI Research,-1;-1;481;92;-1,-1;-1;299;35;-1,8,9/25/19,0,0,0,0,0,0,1618;742;723;245;642,41;36;22;26;90,16;8;10;5;15,179;165;89;46;25,m;m
5549,ICLR,2020,Understanding Distributional Ambiguity via Non-robust Chance Constraint,Shumin MA;LEUNG Cheuk Hang;Qi WU;Wei Liu,shuminma@cityu.edu.hk;chleung87@cityu.edu.hk;qiwu55@cityu.edu.hk;wl2223@columbia.edu,3;1;3,I do not know much about this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I did not assess the derivations or theory.,Withdrawn,0,0,,yes,9/25/19,City University of Hong Kong;City University of Hong Kong;City University of Hong Kong;Columbia University,92;92;92;15,35;35;35;16,,9/25/19,0,0,0,0,0,0,-1;0;52;64382,-1;1;24;4203,-1;0;4;89,0;0;0;5267,m;m
5550,ICLR,2020,Simple but effective techniques to reduce dataset biases,Rabeeh Karimi Mahabadi;James Henderson,rkarimi@idiap.ch;james.henderson@idiap.ch,3;6;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,Idiap Research Institute;Idiap Research Institute,-1;-1,-1;-1,3,9/13/19,11,0,0,0,0,0,93;2595,10;210,5;28,6;195,f;m
5551,ICLR,2020,EfferenceNets for latent space planning,Hlynur Davíð Hlynsson;Merlin Schüler;Robin Schiewer;Laurenz Wiskott,hlynurd@gmail.com;merlin.schueler@ini.rub.de;robin.schiewer@ini.rub.de;laurenz.wiskott@ini.rub.de,1;1;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:N/A:N/A,Withdrawn,0,0,,yes,9/25/19,;Ruhr-Universtät Bochum;Ruhr-Universtät Bochum;Ruhr-Universtät Bochum,-1;-1;-1;-1,-1;-1;-1;-1,10,9/25/19,0,0,0,0,0,0,4;6;0;8081,4;4;1;134,1;2;0;34,0;0;0;639,m;m
5552,ICLR,2020,Mining GANs for knowledge transfer to small domains,Yaxing Wang;Abel Gonzalez-Garcia;David Berga;Luis Herranz;Fahad Shahbaz Khan;Joost van de Weijer,yaxing@cvc.uab.es;agonzalez@cvc.uab.es;dberga@cvc.uab.es;lherranz@cvc.uab.es;fahad.khan@liu.se;joost@cvc.uab.es,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:N/A:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,"Computer Vision Center, Universitat Autònoma de Barcelona;Computer Vision Center, Universitat Autònoma de Barcelona;Computer Vision Center, Universitat Autònoma de Barcelona;Computer Vision Center, Universitat Autònoma de Barcelona;Linköping University;Computer Vision Center, Universitat Autònoma de Barcelona",-1;-1;-1;-1;481;-1,-1;-1;-1;-1;407;-1,5,9/25/19,0,0,0,0,0,0,-1;310;28;195;8946;8983,-1;19;18;28;131;183,-1;7;4;7;33;46,0;26;2;20;1811;823,m;m
5553,ICLR,2020,"EMS: End-to-End Model Search for Network Architecture, Pruning and Quantization",Tianzhe Wang;Kuan Wang;Han Cai;Ji Lin;Yujun Lin;Zhijian Liu;Song Han,usedtobe@mit.edu;wangkuan15@mails.tsinghua.edu.cn;hancai@mit.edu;jilin@mit.edu;yujunlin@mit.edu;zhijian@mit.edu;songhan@mit.edu,3;1;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:N/A:I assessed the sensibility of the derivations and theory.,Withdrawn,1,0,,yes,9/25/19,Massachusetts Institute of Technology;Tsinghua University;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology,2;8;2;2;2;2;2,5;23;5;5;5;5;5,,9/25/19,0,0,0,0,0,0,10;1807;112;356;499;609;0,10;191;33;73;18;20;11,2;23;4;7;6;7;0,0;69;9;17;85;117;0,m;m
5554,ICLR,2020,Deflecting Adversarial Attacks,Yao Qin;Nicholas Frosst;Colin Raffel;Garrison Cottrell;Geoffrey Hinton,yaq007@eng.ucsd.edu;frosst@google.com;craffel@google.com;gary@eng.ucsd.edu;geoffhinton@google.com,3;3;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,2,,yes,9/25/19,"University of California, San Diego;Google;Google;University of California, San Diego;Google",11;-1;-1;11;-1,31;-1;-1;31;-1,4,9/25/19,1,0,0,0,0,0,580;2000;4996;12250;229323,27;12;62;311;415,2;6;23;45;131,69;494;533;1124;21940,f;m
5555,ICLR,2020,One Demonstration Imitation Learning,Bradly C. Stadie;Siyan Zhao;Qiqi Xu;Bonnie Li;Lunjun Zhang,bstadie@berkeley.edu;siyan.zhao@mail.utoronto.ca;frances.xu@mail.utoronto.ca;bonnieli20010901@gmail.com;lunjun.zhang@mail.utoronto.ca,1;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,University of California Berkeley;Toronto University;Toronto University;McGill University;Toronto University,5;18;18;86;18,13;18;18;42;18,,9/25/19,0,0,0,0,0,0,794;298;264;116;30,18;21;12;9;3,7;9;5;3;1,51;24;23;8;2,m;u
5556,ICLR,2020,SDNet: Contextualized Attention-based Deep Network for Conversational Question Answering,Chenguang Zhu;Michael Zeng;Xuedong Huang,chezhu@microsoft.com;nzeng@microsoft.com;xdh@microsoft.com,3;3;3,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,0,,yes,9/25/19,Microsoft;Microsoft;Microsoft,-1;-1;-1,-1;-1;-1,,12/10/18,42,0,0,0,0,0,937;85;6589,47;21;151,11;5;36,91;20;522,m;m
5557,ICLR,2020,Differentially Private Survival Function Estimation,Lovedeep Gondara;Ke Wang,lgondara@sfu.ca;wang@sfu.ca,1;1;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,Simon Fraser University;Simon Fraser University,64;64,272;272,,9/25/19,0,0,0,0,0,0,516;35,83;13,11;3,18;2,m;m
5558,ICLR,2020,GPU Memory Management for Deep Neural Networks Using Deep Q-Network,Shicheng Chen,coder.chen.shi.cheng@gmail.com,3;3;1,I do not know much about this area.:I assessed the sensibility of the experiments.:N/A:N/A;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,0,,yes,9/25/19,,,,,9/25/19,0,0,0,0,0,0,11,13,2,1,u
5559,ICLR,2020,Meta Decision Trees for Explainable Recommendation Systems,Eyal Shulman;Lior Wolf,shulmaneyal@gmail.com;wolf@fb.com,3;8;3,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,1,,yes,9/25/19,Tel Aviv University;Facebook,35;-1,188;-1,,9/25/19,0,0,0,0,0,0,0;496,1;74,0;11,0;56,m;m
5560,ICLR,2020,Parameterized Action Reinforcement Learning for Inverted Index Match Plan Generation,Linfeng Zhao;Lifei Zhu;Qi Chen;Hui Xue;Haidong Wang;Chuanjie Liu;Yuan Liu;Lawson Wong;Lintao Zhang,zhao.linf@husky.neu.edu;v-lifzh@microsoft.com;cheqi@microsoft.com;xuehui@microsoft.com;haidwa@microsoft.com;chuanli@microsoft.com;yuanliu@neu.edu.cn;lawsonlsw@northeastern.edu;lintaoz@microsoft.com,3;3;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A;I do not know much about this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:I did not assess the derivations or theory.;I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I did not assess the derivations or theory.,Withdrawn,0,5,,yes,9/25/19,Northeastern University;Microsoft;Microsoft;Microsoft;Microsoft;Microsoft;Northeastern University;Northeastern University;Microsoft,16;-1;-1;-1;-1;-1;16;16;-1,906;-1;-1;-1;-1;-1;906;906;-1,,9/25/19,0,0,0,0,0,0,42;544;614;1492;80;5;49;461;6920,21;55;147;62;5;5;23;36;86,3;14;7;14;2;1;5;11;28,1;25;33;59;2;0;1;20;848,m;m
5561,ICLR,2020,Fault Tolerant Reinforcement Learning via A Markov Game of Control and Stopping,David Mguni,davidmguni@hotmail.com,3;1;3,I have read many papers in this area.:N/A:N/A:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:N/A:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Prowler.io,-1,-1,4;1,9/25/19,0,0,0,0,0,0,26,11,2,1,m
5562,ICLR,2020,Fuzzing-Based Hard-Label Black-Box Attacks Against Machine Learning Models,Yi Qin;Chuan Yue,yiqin@mines.edu;chuanyue@mines.edu,3;1;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,5,,yes,9/25/19,Colorado School of Mines;Colorado School of Mines,172;172,343;343,4,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1,-1;-1,-1;-1,0;0,u;m
5563,ICLR,2020,"Read, Highlight and Summarize: A Hierarchical Neural Semantic Encoder-based Approach",Rajeev Bhatt Ambati;Saptarashmi Bandyopadhyay;Prasenjit Mitra,rajeev24811@gmail.com;sbandyo20@gmail.com;pum10@psu.edu,3;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Pennsylvania State University;Pennsylvania State University;Pennsylvania State University,41;41;41,78;78;78,3;8,9/25/19,0,0,0,0,0,0,0;7;767,2;7;51,0;2;12,0;1;68,m;m
5564,ICLR,2020,Generalization Puzzles in Deep Networks,Qianli Liao;Brando Miranda;Lorenzo Rosasco;Andrzej Banburski;Robert Liang;Jack Hidary;Tomaso Poggio,lql@mit.edu;miranda9@illinois.edu;lrosasco@mit.edu;kappa666@mit.edu;bobliang345@gmail.com;hidary@google.com;tp@csail.mit.edu,1;1;6,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,3,,yes,9/25/19,"Massachusetts Institute of Technology;University of Illinois, Urbana Champaign;Massachusetts Institute of Technology;Massachusetts Institute of Technology;;Google;Massachusetts Institute of Technology",2;3;2;2;-1;-1;2,5;48;5;5;-1;-1;5,8,9/25/19,0,0,0,0,0,0,1143;431;5581;126;2046;166;2689,58;30;207;25;236;38;50,16;10;37;7;22;7;14,71;20;512;4;113;13;605,m;m
5565,ICLR,2020,Recurrent Chunking Mechanisms for Conversational Machine Reading Comprehension,Hongyu Gong;Yelong Shen;Dian Yu;Jianshu Chen;Dong Yu,hgong6@illinois.edu;yelongshen@tencent.com;yudian@tencent.com;jianshuchen@tencent.com;dyu@tencent.com,6;3;1,I have read many papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,8,,yes,9/25/19,"University of Illinois, Urbana Champaign;Tencent AI Lab;Tencent AI Lab;Tencent AI Lab;Tencent AI Lab",3;-1;-1;-1;-1,48;-1;-1;-1;-1,,9/25/19,0,0,0,0,0,0,132;2288;-1;3087;-1,17;50;-1;107;-1,6;17;-1;27;-1,12;229;0;243;0,f;m
5566,ICLR,2020,Rethinking Generalized Matrix Factorization for Recommendation: The Importance of Multi-hot Encoding,Lei Feng;Hongxin Wei;Qingyu Guo;Zhuoyi Lin;Bo An,feng0093@e.ntu.edu.sg;owenwei@ntu.edu.sg;qguo005@e.ntu.edu.sg;zhuoyi001@e.ntu.edu.sg;boan@ntu.edu.sg,3;3;1,I have published in this field for several years.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,National Taiwan University;National Taiwan University;National Taiwan University;National Taiwan University;National Taiwan University,86;86;86;86;86,120;120;120;120;120,,9/25/19,0,0,0,0,0,0,967;19;27;1;177,80;9;14;7;22,12;3;3;1;4,17;0;0;0;3,m;m
5567,ICLR,2020,Extractor-Attention Network: A New Attention Network with Hybrid Encoders for Chinese Text Classification,Junhao Qiu;Ronghua Shi;Fangfang Li (the corresponding author);Jinjing Shi;Wangmin Liao,qiujunhao@csu.edu.cn;shirh@csu.edu.cn;lifangfang@csu.edu.cn;shijinjing@csu.edu.cn;0909123117@csu.edu.cn,1;6;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY,481;481;481;481;481,299;299;299;299;299,,9/25/19,0,0,0,0,0,0,57;4;13;192;11,4;9;18;40;5,2;2;3;7;1,3;0;0;4;0,m;u
5568,ICLR,2020,Fooling Pre-trained Language Models: An Evolutionary Approach to Generate Wrong Sentences with High Acceptability Score,Marco Di Giovanni;Marco Brambilla,marco.digiovanni@polimi.it;marco.brambilla@polimi.it,3;3;1,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I made a quick assessment of this paper.:N/A,Withdrawn,0,6,,yes,9/25/19,Politecnico di Milano;Politecnico di Milano,128;128,347;347,3;4;8,9/25/19,-1,-1,-1,-1,-1,-1,-1;-1,-1;-1,-1;-1,0;0,m;m
5569,ICLR,2020,The Convex Information Bottleneck Lagrangian,Borja Rodríguez Gálvez;Ragnar Thobaben;Mikael Skoglund,borjarg@kth.se;ragnart@kth.se;skoglund@kth.se,3;3;1,I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I did not assess the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,5,,yes,9/25/19,"KTH Royal Institute of Technology, Stockholm, Sweden;KTH Royal Institute of Technology, Stockholm, Sweden;KTH Royal Institute of Technology, Stockholm, Sweden",128;128;128,222;222;222,1,9/25/19,3,0,0,0,0,0,3;1066;6211,5;81;583,1;19;37,0;52;416,m;m
5570,ICLR,2020,Deep Learning-Based Average Consensus,Masako Kishida;Masaki Ogura;Yuichi Yoshida;Tadashi Wadayama,kishida@nii.ac.jp;oguram@is.naist.jp;yyoshida@nii.ac.jp;wadayama@nitech.ac.jp,1;1;3,I have published in this field for several years.:I carefully checked the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper thoroughly.:I carefully checked the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,4,,yes,9/25/19,"Meiji University;Nara Institute of Science and Technology, Japan;Meiji University;Meiji University",481;481;481;481,332;1397;332;332,10,9/25/19,0,0,0,0,0,0,34;8;12;30,14;4;9;19,3;2;2;3,0;0;1;4,u;m
5571,ICLR,2020,STYLE EXAMPLE-GUIDED TEXT GENERATION USING GENERATIVE ADVERSARIAL TRANSFORMERS,Kuo-Hao Zeng;Mohammad Shoeybi;Ming-Yu Liu,khzeng@cs.washington.edu;mshoeybi@nvidia.com;sean.mingyu.liu@gmail.com,3;3;1,I have published in this field for several years.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,University of Washington;NVIDIA;NVIDIA,6;-1;-1,26;-1;-1,5,9/25/19,0,0,0,0,0,0,186;754;7496,12;34;93,6;11;32,25;58;1285,m;m
5572,ICLR,2020,GraphFlow: Exploiting Conversation Flow with Graph Neural Networks for Conversational Machine Comprehension,Yu Chen;Lingfei Wu;Mohammed J. Zaki,cheny39@rpi.edu;lwu@email.wm.edu;zaki@cs.rpi.edu,1;1,I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I carefully checked the derivations and theory.;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.,Withdrawn,0,0,,yes,9/25/19,Rensselaer Polytechnic Institute;College of William and Mary;Rensselaer Polytechnic Institute,172;154;172,438;235;438,10,7/31/19,14,0,0,0,0,0,76;691;19551,10;64;355,4;16;60,5;63;1872,f;m
5573,ICLR,2020,Revisiting the Information Plane,Martin Schiemer;Juan Ye,martin.schiemer@web.de;juan.ye@st-andrews.ac.uk,3;3;1,I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I have read many papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A,Withdrawn,0,0,,yes,9/25/19,University of St Andrews;University of St Andrews,-1;-1,-1;-1,8,9/25/19,1,0,0,0,0,0,1;20,2;12,1;2,1;2,m;f
5574,ICLR,2020,Fully Quantized Transformer for Improved Translation,Gabriele Prato;Ella Charlaix;Mehdi Rezagholizadeh,prato.gab@gmail.com;ella.charlaix@huawei.com;mehdi.rezagholizadeh@huawei.com,3;1;3,I have published one or two papers in this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:I assessed the sensibility of the derivations and theory.;I do not know much about this area.:I assessed the sensibility of the experiments.:I read the paper at least twice and used my best judgement in assessing the paper.:N/A;I have published one or two papers in this area.:I carefully checked the experiments.:I read the paper thoroughly.:N/A,Withdrawn,0,3,,yes,9/25/19,University of Montreal;Huawei Technologies Ltd.;Huawei Technologies Ltd.,128;-1;-1,85;-1;-1,3,08/17/19,5,0,0,0,0,0,5;5;163,4;2;32,1;1;32,0;0;6,f;m
